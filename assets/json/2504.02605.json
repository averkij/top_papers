{
    "paper_title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "authors": [
        "Daoguang Zan",
        "Zhirong Huang",
        "Wei Liu",
        "Hanwu Chen",
        "Linhao Zhang",
        "Shulin Xin",
        "Lu Chen",
        "Qi Liu",
        "Xiaojian Zhong",
        "Aoyan Li",
        "Siyao Liu",
        "Yongsheng Xiao",
        "Liangqiang Chen",
        "Yuyu Zhang",
        "Jing Su",
        "Tianyu Liu",
        "Rui Long",
        "Kai Shen",
        "Liang Xiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI."
        },
        {
            "title": "Start",
            "content": "Multi-SWE-bench: Multilingual Benchmark for Issue Resolving"
        },
        {
            "title": "Abstract",
            "content": "The task of issue resolving is to modify codebase to generate patch that addresses given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes total of 1, 632 high-quality instances, which were carefully annotated from 2, 456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present comprehensive analysis with key empirical insights. In addition, we launch Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release set of 4, 723 well-structured instances spanning seven programming languages, laying solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI. 5 2 0 2 3 ] . [ 1 5 0 6 2 0 . 4 0 5 2 : r Figure 1. Resolved rate (%) on Multi-SWE-bench (Claude-3.5-Sonnet). Author contributions listed at end of paper. Correspondence to: {zandaoguang, shen.kai}@bytedance.com"
        },
        {
            "title": "Contents",
            "content": ""
        },
        {
            "title": "3.1 Benchmark Construction .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.2 Evaluation Metrics .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.1.1 Performance across Programming Languages",
            "content": ". . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.1.3 Performance across Different Repositories",
            "content": ". . . . . . . . . . . . . . . . . . 6.2 Influencing Factors of Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Issue Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.3 Case Study .",
            "content": ". . . . . . ."
        },
        {
            "title": "6.5 Troubleshooting .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Conclusions and Future Works",
            "content": "2 3 4 5 5 6 6 7 7 9 11 11 12 13 13 15 17 19 20 21 23 25 26 27 1. Introduction Automating software engineering tasks with large language models (LLMs) has gained considerable attention [Zan et al., 2023, Zheng et al., 2023b, Jiang et al., 2024, Jelodar et al., 2025] recently. Beyond code generation, the issue resolving task proposed by SWE-bench [Jimenez et al., 2023] changes the role of LLMs from code assistants to fully autonomous AI programmers. SWE-bench contains 2, 294 issues from 12 widely-used open-sourced Python libraries. LLMs are tasked to generate patch based on the issue description along with the buggy code repository. SWE-bench Verified is subset of 500 human-validated issues selected from SWE-bench, chosen for appropriately scoped unit tests and well-specified issue descriptions. Within less than one year, the resolving rate on SWE-bench Verified increased from 0.40% [Jimenez et al., 2023] (for RAG+GPT3.5) to 65.40% [augment code, 2025] (for Augment Agent v0). Although existing works based on SWE-bench demonstrate significant progress in Pythonbased issue resolving, the diversity of programming languages in real-world repositories presents additional challenges that remain unexplored. In particular, repositories in different languages follow distinct programming paradigms, idiomatic patterns, and runtime behaviors, which may impact the effectiveness of current approaches. This raises the question of whether the impressive performance of existing agents on Python issues can be generalized to other widely used languages, such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. To answer this question, we introduce Multi-SWE-bench, multilingual benchmark for issue resolving, consisting of 1, 632 issues across 7 widely used programming languages: Java, TypeScript, JavaScript, Go, Rust, C, and C++. To construct reliable benchmark for evaluating the ability of agents to resolve real-world software issues, we employ systematic five-phase pipeline. First, we select high-quality repositories from GitHub based on star ratings and runnability counts to ensure both popularity and practical usability. Second, we collect issue-related pull requests (PRs) along with their corresponding metadata. Third, we build Dockerized environments for each PR by extracting dependencies from CI/CD workflows and documentation to ensure reproducible execution. Fourth, we validate PRs by analyzing test outcomes across patch configurations, retaining only those with clear bug-fixing effects and no regressions. Fifth, we perform rigorous manual verification through dual annotation and cross-review, ensuring high-quality ground truth aligned with SWE-bench verified standards. By ensuring diversity, executability, and human-verified correctness, Multi-SWE-bench sets high standard for evaluating LLMs on realistic and non-trivial issue-resolving tasks. With its wide coverage of languages and issue types, Multi-SWE-bench introduces realistic challenges that push the boundaries of LLM-based software agents. We use Multi-SWE-bench to evaluate the generalizability of 3 representative methods (i.e., Agentless [Xia et al., 2024], SWE-agent [Yang et al., 2024], and OpenHands +CodeAct v2.1 [Wang et al., 2024b]) based on 9 top-performing frontier models (i.e., GPT-4o, OpenAI-o1, OpenAI-o3-mini-high, Claude-3.5Sonnet, Claude-3.7-Sonnet, DeepSeek-V3, DeepSeek-R1, Qwen2.5-72B-Instruct, and Doubao1.5-Pro). Our evaluation provides comparative analysis of the overall effectiveness of these methods across seven programming languages, along with Python, offering insights into their cross-language capabilities. Furthermore, we conduct fine-grained analysis of the key factors influencing model performance and investigate failure cases for each language to identify underlying challenges and limitations. Through comprehensive analysis and comparison, we provide good understanding of existing models and shed light on future directions and further progress. For example, our findings show that models perform generally better when issue descriptions are longer, indicating strong reliance on rich contextual grounding; in contrast, resolved rates drop sharply when fix patches exceed 600 tokens or touch more than one file, 3 exposing weaknesses in long-context retention and multi-file reasoning. Together, these findings delineate the current boundary of LLM capabilities in software engineering and define the key obstacles to real-world deployment. Beyond Multi-SWE-bench, we launch the Multi-SWE-RL open-source community to address the pressing need for scalable, high-quality RL environments in software engineering. Recent models such as DeepSeek-R1 [Guo et al., 2025], OpenAI-o1 [Jaech et al., 2024], and OpenAI-o3 [OpenAI, 2025] have demonstrated the potential of RL even with simplistic reward signals. These advances reinforce our belief that \"scaling RL in real-world software environments is key pathway toward human-level intelligence\". However, the creation of realistic, interactive environments remains major bottleneck. As first step toward scalable RL in software engineering, Multi-SWE-RL therefore launches collaborative initiative to build training data and environment from real-world tasks. As the initial contribution to the Multi-SWE-RL community, we release dataset of 4, 723 containerized issue-resolving instances spanning 7 programming languages. Each instance is equipped with reproducible execution environment, enabling plug-and-play training for RL agents in realistic software contexts. We envision this release as sparkigniting broader community interest in the construction of RL training data and paving the way toward fully autonomous agent systems. In summary, our main contributions are: Multi-SWE-bench, multilingual benchmark for issue resolving, consisting of 1, 632 human-validated GitHub issues on 7 widely used programming language. It serves as reliable support for comprehensively evaluating the performance of agents in real-world software development scenarios. large-scale evaluation of 9 state-of-the-art LLMs across 3 representative methods on Multi-SWE-bench, yielding diagnostic insights to guide future research. Multi-SWE-RL, community-driven open-source effort that initiates scalable RL data creation from real-world software tasks, laying the groundwork for long-term progress in multilingual software agent development. 2. Related Work The remarkable performance of LLMs in code-related tasks has motivated substantial research to study their role in automating software engineering. To evaluate the capabilities and limitations of existing approaches, wide range of benchmarks for code-related tasks has been developed. Early efforts in this domain focused on primarily evaluating models in monolingual programlevel evaluations [Allamanis and Sutton, 2013, Raychev et al., 2016, Iyer et al., 2018, Chen et al., 2021b, Austin et al., 2021, Wang et al., 2023]. As LLMs advanced, benchmarks evolved in two key dimensions to better align with real-world software engineering scenarios. First, benchmarks shift from monolingual to multilingual tasks, with growing interest and practical needs in evaluating LLMs performance across multiple programming languages. Examples include Multilingual-HumanEval [Athiwaratkun et al., 2023] and HumanEval-X [Zheng et al., 2023a], which extend the HumanEval [Chen et al., 2021a] benchmark to multiple languages, and MBXP [Athiwaratkun et al., 2022], which extends MBPP to multilingual scenarios. Second, benchmarks shift from program-level to repository-level tasks, focusing on more complex scenarios such as library-oriented code generation [Zan et al., 2022], repository-level code completion [Zhang et al., 2023, Liu et al., 2024a, Ding et al., 2024, Liu et al., 2024b, Yu et al., 2024], and bug fix [MÃ¼ndler et al., 2024, Ouyang et al., 2024, Sun et al., Saavedra et al., 2024]. These evolving benchmarks aim to provide more comprehensive evaluation of LLMs in real-world 4 Figure 2. Construction of Multi-SWE-bench. software development environments. In addition to existing benchmarks, SWE-bench [Jimenez et al., 2023] has gained significant attention since its release. Instead of focusing on isolating code subtasks into separate datasets, SWE-bench addresses broader range of tasks through repository-level issue resolving. These issue resolving tasks, including bug fixing, new feature requests, and optimization, which provide more comprehensive evaluation of LLMs ability to automating software development. While SWE-bench is limited to textual context, SWE-bench Multimodal [Yang et al., 2025] and Visual SWE-bench [Zhang et al., 2024] extend evaluation to systems fixing bugs in visually-oriented and user-facing applications. SWE-Lancer [Miserendino et al., 2025] focuses on JavaScript and TypeScript, featuring over 1, 400 freelance tasks from Upwork, including technical and managerial tasks. Despite these advancements, the performance of LLMs on other widely used programming languages remains underexplored. Our work aims to bridge this gap with Multi-SWE-bench, large-scale multilingual benchmark for issue resolving, with 1, 632 human-validated GitHub issues across 7 widely used languages. 3. Multi-SWE-bench Multi-SWE-bench consists of 1, 632 issue-resolving tasks spanning 7 programming languages: Java, TypeScript, JavaScript, Go, Rust, C, and C++. This section will provide the construction process of Multi-SWE-bench, along with an analysis of its key features. 3.1. Benchmark Construction To evaluate the generalizability of LLMs as issue resolvers, seven programming languages are selected to construct Multi-SWE-bench through five phases. As shown in Fig. 2, the first four phases create large pool of candidate data for each language, while the fifth phase finalizes the Multi-SWE-bench through manual verification. 5 3.1.1. Phase 1: Repository Selection We carefully curate diverse set of high-quality GitHub repositories for each of the seven target programming languages. The selection process is guided by the following criteria: Popularity and Maintenance: Repositories must have over 500 GitHub stars and demonstrate active maintenance for at least six months. In addition, we prioritize repositories frequently recommended in Google searches using keywords such as \"high-quality\", \"well-maintained\", and \"popular\". CI/CD Support: Selected repositories are required to include CI/CD configurations (e.g., workflows under \".github/workflows/\") to ensure automated testing and reproducibility. Build Viability: After minimal manual setup, the latest commit must be buildable and testable in clean environment, ensuring compatibility with modern tooling and infrastructure. This filtering process results in robust foundation of repositories that are representative of real-world, production-level codebases, laying the groundwork for subsequent phases. 3.1.2. Phase 2: Pull Request Crawling This phase aims to crawl issue-resolving pull requests (PRs) for each repository selected in phase 1. All PRs from the repository are collected and then filtered based on the following criteria: Linked with at least one GitHub issue: The PR must be linked to at least one issue to ensure it addresses clearly defined bug report or feature request. Modified test files: The PR must include changes to test files, guaranteeing proper testing is in place to verify the correctness of the fix patches. Merged into the main branch: The PR must be merged into the main branch, indicating it has been accepted by the repositorys maintainers and fully integrated. After filtering, detailed information is gathered for each PR, including attributes such as issue description, base commit, fix.patch, and test.patch. 3.1.3. Phase 3: Environment Determination To enable faithful execution and evaluation of issue-resolving tasks, each PR must be reproducibly built and executed in an isolated environment. In this phase, we construct Dockerbased runtime environment for every PR by automatically identifying and provisioning its necessary dependencies. The process begins with manual inspection of environment-related artifacts, including CI/CD configuration files (e.g., GitHub Actions), repository documentation (e.g., README files), and exploratory trial runs. Through this analysis, we extract environment dependencies and classify them into two categories: repo-common dependencies, which are shared across the entire repository, and PR-specific dependencies, which are introduced or modified by the target PR. Using the extracted dependency information, we generate tailored Dockerfile and attempt to build corresponding Docker image. If the build fails, we examine the error logs to identify missing dependencies, misconfigurations, or version conflicts. For fixable errors, we iteratively patch the Dockerfile or supporting scripts; otherwise, we discard the PR to ensure reliability. Once the image is successfully built, we verify whether the repository can be launched at the specific commit associated with the PR. This step ensures that all required services, packages, 6 and configurations are functional. If the launch fails, we again attempt corrective actions; if successful, we obtain validated and executable containerized environment for downstream evaluation. This phase ensures that each PR is equipped with clean and functional containerized environment, laying necessary foundation for subsequent testing and analysis. 3.1.4. Phase 4: Pull Request Filtering With both the PR metadata and working runtime environment established in the previous phases, we now perform semantic validation to ensure each PR meets the requirements of issue resolving. This is done by analyzing test behaviors under controlled patch configurations. For each PR, unlike SWE-bench which runs only relevant tests, we run the full test suit under the following three settings: Run.log: Tests are executed on the base commit. Test.log: The test.patch is applied to the base commit before execution. Fix.log: Both the test.patch and the fix.patch are applied to the base commit before execution. We then extract the execution status of each test case from these logs. Unlike SWE-bench, which considers only PASSED and FAILED, we also track NONE and SKIPPED status, as some test cases may be conditionally disabled or omitted after applying patchesresulting in inconsistent test counts across the three logs. Each test case is summarized by its status transition across the three settings. For instance, test case with PASSED, FAILED, and PASSED statuses in run.log, test.log, and fix.log, respectively, is represented as PASSEDFAILEDPASSED. We apply the following filtering rules to determine eligible PRs: PRs with any ANYPASSEDFAILED transitions are discarded to ensure that no potential regressions are introduced by the fix.patch. PRs without at least one ANYFAILEDPASSED transition are discarded, as they do not demonstrate any effective bug fix. PRs exhibiting abnormal transitions such as PASSEDNONE/SKIPPEDFAILED are discarded to eliminate ambiguous test behaviors. After applying these criteria, we retain 2, 456 issue-resolving instances spanning 39 repositories across 7 languages. For each instance, we extract test cases exhibiting transitions of the form AnyFAILED/PASSED/SKIPPED/NONEPASSED, and include them in the dataset to enable fine-grained and reliable evaluation. 3.1.5. Phase 5: Manual Verification To ensure the reliability of Multi-SWE-bench in evaluating the issue-resolving capabilities of LLMs, we conduct comprehensive manual verification on the 2, 456 issue-resolving instances retained from the previous phase. Our verification process follows the annotation guidelines of the recently released SWE-bench-verified1. In detail, we recruit 68 annotators through outsourcing, with the number per language proportional to the remaining annotation workload. All annotators were screened based on their qualifications, including at least two years of experience in the target language and relevant bachelors degree or higher. Before annotation, each annotator undergoes one-hour training session covering the background, objectives, procedures, deliverables, and quality standards of the task. To further 1https://openai.com/index/introducing-swe-bench-verified 7 Table 1. Statistics of the Multi-SWE-bench. #A2P2P, #A2F2P, and #A2N2P represent the average counts of AnyPASSED&FAILED&NONEPASSED unit tests."
        },
        {
            "title": "Repository",
            "content": "Org/Repo #Files #LoC Instance #Num Issue description Avg. #Tokens Fix patches Avg. #Lines Avg. #Hunks Avg. #Files Unit tests #A2P2P #A2F2P #A2N2P alibaba/fastjson2 elastic/logstash mockito/mockito apache/dubbo fasterxml/j-core fasterxml/j-dbind fasterxml/j-dfmt-xml google/gson google-ct/jib 4244 562 986 3939 366 1230 206 261 604 443.8k 59.9k 84.0k 402.1k 105.7k 217.5k 23.0k 48.0k 75.5k darkreader/darkreader mui/material-ui vuejs/core 189 27632 26.2k 698.6k 128.2k ag/gh-rdme-stats axios/axios expressjs/express iamkun/dayjs Kong/insomnia sveltejs/svelte cli/cli grpc/grpc-go zeromicro/go-zero BurntSushi/ripgrep clap-rs/clap nushell/nushell rayon-rs/rayon serde-rs/serde sharkdp/bat sharkdp/fd tokio-rs/bytes tokio-rs/tokio tokio-rs/tracing facebook/zstd jqlang/jq ponylang/ponyc catchorg/Catch2 fmtlib/fmt nlohmann/json simdjson/simdjson yhirose/cpp-httplib 69 166 142 324 526 2800 737 981 960 98 321 1479 191 188 83 24 33 727 241 276 80 285 399 25 477 455 33 11.8k 21.0k 17.3k 17.1k 182.0k 105.9k 165.1k 260.8k 117.6k 45.4k 70.4k 264.2k 36.9k 36.5k 22.0k 6.7k 11.9k 141.5k 60.9k 119.8k 43.0k 80.2k 58.0k 36.4k 124.7k 229.7k 50.9k 6 38 6 3 18 42 5 5 5 2 174 19 4 4 56 1 272 397 16 15 14 132 14 2 2 10 14 5 25 21 29 17 82 12 41 55"
        },
        {
            "title": "Java",
            "content": "10.5 212.3 92.5 9.3 33.8 35.1 98.4 35.8 15."
        },
        {
            "title": "TypeScript",
            "content": "13.0 331.2 22."
        },
        {
            "title": "JavaScript",
            "content": "123.6 179.5 7.2 21.7 1.0 72.0 103.8 81.8 52.4 1604.9 147.1 155.0 637.5 72.5 239.5 55.8 45.0 139.8 597.2 67.6 26.1 205.4 469.0 36.8 405.8 768.5 1.0 Go"
        },
        {
            "title": "Rust",
            "content": "C C++ 459.2 1600.4 315.2 774.0 304.7 621.5 1071.8 365.8 1094.6 749.5 508.6 694.8 287.1 490.8 177.5 325.6 709.0 618.9 347.6 276.1 205. 553.7 987.0 795.6 153.5 171.5 638.2 167.8 188.0 590.0 472.0 496.6 429.8 480.2 357.3 397.7 905.5 320.2 240.0 1.3 10.0 10.3 3.0 4.8 3.9 10.4 4.6 3.2 2.0 20.2 3.5 13.5 7.8 2.2 2.7 1.0 8. 9.0 7.7 4.9 21.9 15.7 10.6 5.5 3.0 14.1 7.8 5.6 10.6 39.3 10.9 2.7 15.6 15.4 3.0 27.9 35.5 1.0 1.2 4.6 4.7 1.3 2.1 2.1 3.2 1.8 2.6 1.5 12.0 1. 4.8 4.0 1.5 2.0 1.0 4.0 3.9 2.8 2.7 7.5 4.7 4.3 2.0 3.0 5.9 4.5 1.8 3.5 7.1 3.0 1.8 5.7 8.2 1.1 6.5 11.0 1.0 1243.5 554.7 97.2 2.0 2.0 2.0 2.0 2.0 2. 41.0 5001.3 2920.4 108.9 68.5 808.2 60.4 105.0 4904.2 1997.0 230.4 1318.9 233.2 489.5 798.6 113.5 0.0 152.7 186.5 23.2 26.6 30.8 0.8 27.2 997.6 19.9 9.3 26.5 18.6 272. 0.8 1.9 1.0 57.0 85.6 73.8 94.2 62.6 96.2 3.5 2.3 3.0 3.5 1.2 1.5 1.2 1.0 5.5 2.9 0.6 0.3 1.1 3.1 2.6 0.5 0.0 1.7 1.1 0.4 0.0 0.2 0.5 1.0 1. 0.7 0.0 0.0 0.0 1.0 1020.5 256.2 3.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 836.8 0.0 3.4 0.0 65.2 3.2 0.0 0.0 31.0 6.6 43.9 8.1 378.8 336.6 171.0 294.5 33.6 0.0 91.6 287.4 182. 5.6 0.1 388.8 17.6 9.3 42.9 41.5 0.0 support consistency and correctness during the annotation process, we establish dedicated discussion channels to provide real-time guidance and handle edge cases collaboratively. Each instance is independently labeled by two annotators. Upon completion, the two annotations are cross-reviewed to produce single, agreed-upon final label. To ensure high annotation quality, we additionally form an internal quality assessment team of 14 experienced engineers. This team produces reference answers and verifies that the outsourced annotations for each language reach minimum accuracy threshold of 80%. After rigorous manual verification, total of 1, 632 high-quality instances are retained as the final Multi-SWE-bench, filtered by annotation criteria from the verification questionnaire2: Q2.1=0 & Q3.1{2,3} & Q4.1{2,3}. All annotation results have been made publicly available to ensure the transparency of the dataset3. 2https://github.com/multi-swe-bench/multi-swe-bench/blob/main/docs/manual-verificat ion/questionnaire-demo.pdf 3https://github.com/multi-swe-bench/multi-swe-bench/tree/main/docs/manual-verificat ion/annotation-results 8 Figure 3. Distribution of estimated time consumption of issues in Multi-SWE-bench. 3.2. Features of Multi-SWE-bench Tab. 1 presents an overview of the key statistics of Multi-SWE-bench, highlighting its coverage across wide range of programming languages and repositories. It includes 1, 632 issueresolving instances sourced from 39 diverse repositories, spanning 7 popular languages: Java, TypeScript, JavaScript, Go, Rust, C, and C++. These repositories vary significantly in size and complexity, with the number of files ranging from 24 to 27, 632, and lines of code from 6.7k to 698.6k. This diversity ensures that Multi-SWE-bench reflects realistic and heterogeneous software development scenarios. In terms of issue descriptions, the complexity varies notably across repositories and languages. Java and Rust projects generally present longer and more detailed issue reports (e.g., \"elastic/logstash\" with 1600.4 tokens), suggesting more contextdependent reasoning is required. In contrast, JavaScript, Go, and issues are typically brief and focused (e.g., \"expressjs/express\" with 177.5 tokens), implying simpler or more localized fixes. This variation in description length highlights the need for LLMs to adapt to both underand over-specified problem statements. Similarly, patch complexity also differs significantly across languages. Rust and C++ projects frequently require large-scale edits, with some instances modifying over 200 lines and 7 files per patch (e.g., \"BurntSushi/ripgrep\" and \"simdjson/simdjson\"). Conversely, JavaScript, and TypeScript patches tend to be more localized and atomic, often involving under 3 hunks and fewer than 2 files. These contrasts emphasize the importance of handling both high-granularity refactoring and precision editing. Moreover, all repositories come with strong test coverage, providing reliable signals for verifying patch correctness, as confirmed by the manual verification in Sec. 3.1.5. We further display the manual verification results in Tab. 3, which show that most instances have no serious issues and receive high scores, confirming the overall quality of the repositories selected in Sec. 3.1.1. As part of the manual annotation process in Multi-SWE-bench, we recorded the estimated time required to resolve each issue, categorized into four buckets: 15 minutes, 15 minutes1 hour, 14 hours, and 4 hours  (Fig. 3)  . Unlike SWE-Bench, we use this timebased annotation to define difficulty levels across all languages: easy (15 mins), medium (15 mins1h), and hard (1h). Tab. 2 summarizes the distribution of difficulty levels by language. We observe clear trends across these categories: As difficulty increases, issues tend to have longer descriptions, and the corresponding patches involve more lines, hunks, and files. Interestingly, certain easy instances exhibit large-scale edits (e.g., Rust), which are typically due to highly repetitive and pattern-consistent changes. This highlights the advantage of time-based difficulty categorization over superficial metrics like token count or file span. Such categorization provides more realistic measure of problem complexity and can better guide the development and evaluation of LLMs. 9 Table 2. Feature distribution of Multi-SWE-bench instances by difficulty and language."
        },
        {
            "title": "Difficulty",
            "content": "Instance #Num Issue description Avg. #Tokens Fix patches Avg. #Lines Avg. #Hunks Avg. #Files Unit tests #A2P2P #A2F2P #A2N2P"
        },
        {
            "title": "Easy\nMedium\nHard",
            "content": "194 261 45 27 65 36 72 88 64 10 105 241 141 153 134 66 126 30 54 44 28 59 42 417.9 555.9 589.8 733.8 843.3 1039.0 600.1 566.9 472.8 282.4 505.8 578. 411.7 331.4 274.0 808.2 814.7 599.4 551.4 449.9 460.2 494.5 427.5 904."
        },
        {
            "title": "Java",
            "content": "1.4 2.5 6.8 2.6 4.6 11."
        },
        {
            "title": "TypeScript",
            "content": "2.1 8.8 43."
        },
        {
            "title": "JavaScript",
            "content": "Go"
        },
        {
            "title": "Rust",
            "content": "C C++ 1.8 2.6 10.1 4.0 6.9 16.0 7.0 10.6 45.2 3.7 5.5 28. 4.4 7.6 47.2 5.0 14.1 55.8 12.4 36.2 246.1 8.3 74.3 806.6 4.7 15.5 92.2 26.6 49.6 238. 318.7 113.6 629.0 16.4 36.7 381.1 25.2 204.2 763.7 1.0 1.3 2.0 1.8 2.1 5.4 1.5 4.3 26. 1.6 2.1 4.5 2.7 2.6 6.6 3.3 3.7 10.3 2.2 2.5 8.7 2.2 3.3 11.1 116.2 115.4 166. 126.8 182.3 389.1 4806.8 4854.6 3706.1 616.8 3161.0 4169.9 2181.0 1832.5 1704.2 465.2 343.0 232.3 424.8 715.5 702. 45.0 18.2 9.3 3.9 2.4 2.9 58.0 58.6 21.8 2.0 2.8 2.7 1.2 3.6 5.2 2.6 2.2 3. 3.2 1.8 1.1 0.8 1.0 2.4 0.1 0.1 0.0 0 0 0 76.1 136.9 136.9 0.0 214.3 1980. 35.1 0.8 0.3 20.4 25.7 46.7 212.0 300.5 334.0 208.2 228.2 306.3 15.7 23.0 47.2 Table 3. Scoring statistics for Multi-SWE-bench from the verification questionnaire."
        },
        {
            "title": "Languages",
            "content": "Java TypeScript JavaScript Go Rust C++ Q2.1 Serious Issue Flag #Score 0 146 382 586 579 328 200 162 #Score 1 10 8 4 26 11 6 7 Q3.1 Clarity of Issue Description Q4.1 Coverage of Unit Tests #Score 0 2 5 0 5 4 2 #Score 1 2 56 6 10 20 4 6 #Score 2 44 121 13 276 165 115 96 #Score 3 98 200 567 288 139 79 60 #Score 0 10 31 55 44 23 13 7 #Score 1 5 76 172 100 50 55 21 #Score 2 17 133 305 151 74 83 #Score 3 114 142 54 284 181 49 89 4. Multi-SWE-RL Open-Source Community Community Introduction. Multi-SWE-RL is an open-source community aimed at developing high-quality RL training datasets for complex software engineering tasks. Its purpose is to serve as the foundational infrastructure for training fully autonomous agents capable of addressing real-world software engineering challenges, paving the way toward achieving AGI. The need for such community has become increasingly urgent as the potential of RL continues to expand. Notable models such as DeepSeek-R1 [Guo et al., 2025], OpenAI o1 [Jaech et al., 2024], and o3 [OpenAI, 2025] have demonstrated the power of RL, even with simple, rule-based reward signals. In light of these advancements, we are firmly convinced that scaling RL in real-world environments is the path toward human-like intelligence. However, the creation of such interactive environments and data trajectories is extremely challenging. For instance, the development of our Multi-SWE-bench took about one year to produce just high-quality 1, 632 instances. 10 Therefore, we launched the Multi-SWE-RL community to harness the power of open-source collaborative contributions for building diverse RL environments. Community Initialization. To bootstrap the Multi-SWE-RL community, we release an initial dataset comprising 4, 723 issue-resolving instances spanning 76 widely-used open-source repositories and 7 programming languages: Java, TypeScript, JavaScript, Go, Rust, C, and C++. Each instance is equipped with fully containerized execution environment to ensure reproducibility and ease of integration. This dataset was constructed using the same pipeline as Multi-SWEbench, excluding the manual verification process described in Sec. 3.1.5. Details about this release are available at Hugging Face dataset and Multi-SWE-RL contribution board. We envision this initial release as sparkigniting broader community collaboration and fueling the construction of scalable, high-quality RL environments for real-world software engineering. Contribution Guidelines and Recognition. We welcome contributions from the community to expand the Multi-SWE-bench and Multi-SWE-RL. To help new contributors get started, we provide detailed demo that walks through the process of creating an issue-resolving instance, available at Contribution-demo.md. To recognize and incentivize community contributions, we maintain rolling update schedule through periodic arXiv updates or follow-up technical reports, with new versions released every three months. Each update may include: Newly added benchmarks for additional programming languages in Multi-SWE-bench, with new authors and contributors; Newly contributed data to Multi-SWE-RL, with new authors and contributors; Newly reported performance results from RL trials on Multi-SWE-bench using Multi-SWERL data, with new authors and contributors; Newly open-sourced RL models with significantly enhanced performance, with new authors and contributors. Our contribution incentive policy is detailed at Incentive-plan.md. We are committed to continuously refining our contribution strategy to encourage sustained open-source engagement, and we warmly invite the community to take part in shaping and scaling this collaborative effort. 5. Experimental Setups 5.1. Evaluated LLMs and Methods Methods. We evaluate three representative methods for issue resolving: Agentless [Xia et al., 2024], SWE-agent [Yang et al., 2024], and OpenHands + CodeAct v2.1 [Wang et al., 2024b]. These methods were specifically designed for Python as used in SWE-Bench [Jimenez et al., 2023]. We extended the methods to accommodate the multilingual nature of Multi-SWE-bench4. Agentless5MagentLess6: Agentless addresses the issue resolving task through multistage fixed workflow, including hierarchical fault localization, code repair, and candidate patch selection via regression and reproduction tests. In MagentLess, we made the following key modifications to support multilingual adaptation and improve scalability: 1. We revised all prompts to accommodate the newly added languages. 4MagentLess and MopenHands are pronounced as /\"mA:dZ@nt.l@s/ and /\"moUp@n.hAndz/, respectively. 5https://github.com/OpenAutoCoder/Agentless 6https://github.com/multi-swe-bench/MagentLess 11 2. We replaced all file skeleton inputs with full file content, as extracting file skeletons is challenging in some programming languages. 3. We implemented function and class extraction for all languages using Tree-sitter7. 4. We pruned the extracted repository structures by retaining only files and directories with specific extensions, as repositories in certain languages (e.g., TypeScript) often contain an excessive number of files that may exceed LLM context limits. 5. We removed the candidate patch selection stage and retained only fault localization and code repair, as regression and reproduction testing is cumbersome to implement across languages and falls outside the scope of this work. SWE-agent8MSWE-agent9: SWE-agent is an agent-based approach that solves issues through multi-turn interactions via predefined agent-computer interface (ACI). To support Multi-SWE-bench, we developed MSWE-agent with the following modifications: 1. We revised all prompts to accommodate the newly added languages. 2. We truncated overly long environment observations to ensure stable agent execution. 3. We added \".gitignore\" to exclude compiled artifacts (e.g., \".o\", \".bin\") in languages like C/C++, which could otherwise interfere with \"git apply\". 4. We fixed language-specific commands that caused crashes or non-terminating behavior during execution to ensure stable agent execution. OpenHands10MopenHands11: OpenHands is widely adopted platform for building software development agents. In MopenHands, we made the following key modifications to support multilingual adaptation: 1. We revised all prompts to support the newly added programming languages. 2. We added \".gitignore\" to exclude compiled artifacts, as also done in MSWE-agent. 3. We fixed several implementation bugs, including an issue where \"CmdRunAction\" incorrectly rendered tab characters (t) as spaces in \"git diff\" outputs, making patches unapplicable. To resolve this, we redirected the diff to file and read it using \"FileReadAction\", which proved especially important in languages like Go. We have systematically extended the above methods to support the multilingual setting of MultiSWE-bench. Still, there remains substantial room for improvement, particularly in languagespecific adaptation and overall robustness. We welcome community collaboration to further advance their capabilities. LLMs. We evaluated 9 popular LLMs across the above three methods: GPT-4o (gpt-4o-2024-1120), OpenAI-o1 (o1-2024-12-17), OpenAI-o3-mini-high (o3-mini-2025-01-31 high), Claude-3.5Sonnet (claude-3-5-sonnet-20241022), Claude-3.7-Sonnet (claude-3-7-sonnet-20250219), DeepSeekV3, DeepSeek-R1, Qwen2.5-72B-Instruct, and Doubao-1.5-pro. 5.2. Evaluation Metrics Following SWE-Bench [Jimenez et al., 2023] and SWE-Lancer [Miserendino et al., 2025], we adopt Resolved Rate (%) as our primary evaluation metric, measuring the percentage of issues resolved. In addition, we report several other metrics to provide more detailed analysis: 7https://tree-sitter.github.io 8https://github.com/SWE-agent/SWE-agent 9https://github.com/multi-swe-bench/MSWE-agent 10https://github.com/All-Hands-AI/OpenHands 11https://github.com/multi-swe-bench/MopenHands 12 Success Location (%) the accuracy of fault localization at file level; and Average Cost ($) the average cost per issue. 6. Experimental Results 6.1. Performance on Multi-SWE-bench In this subsection, we conduct systematic evaluation of issue resolving performance on MultiSWE-bench along three key dimensions: (1) language-specific performance, examining variations in effectiveness across programming languages; (2) LLMs and agents comparison, evaluating the differential capabilities of various LLMs and methods; and (3) repository-level performance, analyzing the impact of repository characteristics on resolved rate. 6.1.1. Performance across Programming Languages Tab. 4 presents the overall performance of the agents across eight programming languages, and Tab. 5 further details the results across three distinct difficulty levels. Based on these tables, several key observations can be drawn and outlined below. Table 4. Resolved rate (%) of different models on Multi-SWE-bench."
        },
        {
            "title": "Python",
            "content": "GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro 36.20 48.20 46.40 42.40 44.60 41.00 42.20 26.80 26.20 18.80 28.80 28.60 24.80 45.80 4.20 2.00 8.60 12.40 25.60 16.00 20.40 39.00 52.20 27.80 26.00 4.40 8."
        },
        {
            "title": "JS Go Rust",
            "content": "C C++ 1.40 5.06 2.81 1.97 1.97 3.37 4.49 0.84 1.12 0.84 4.21 4.21 4.21 4.78 2.53 1.40 0.56 1.40 1.97 3.65 3.37 1.97 5.06 1.12 2.53 0.84 1.12 2.80 4.44 3.97 5.14 5.84 5.37 3.74 1.40 2. 2.34 4.67 3.97 5.84 5.37 4.44 2.10 0.47 2.10 3.50 3.74 2.34 6.78 7.48 0.70 0.00 1.40 1.64 5.86 7.11 7.95 5.02 5.44 5.02 6.69 2.51 4.18 2.09 4.18 5.02 6.69 6.69 5.86 2.09 0.42 1.67 3.35 2.51 5.02 12.13 15.90 4.60 4.60 1.67 0.84 1.56 1.56 3.91 1.56 2.34 3.13 0.78 0.78 0. 1.56 3.91 2.34 4.69 8.59 2.34 0.78 1.56 2.34 0.00 3.13 1.56 3.13 8.59 3.13 2.34 0.78 0.00 6.98 5.43 1.55 3.88 3.10 1.55 3.10 0.78 0.00 2.33 3.88 5.43 6.98 11.63 7.75 6.20 0.00 6.20 3.88 3.88 6.98 12.40 14.73 7.75 4.65 2.33 3.10 Java TS MagentLess 2.23 11.72 5.80 21.09 0.45 5.47 4.91 14.84 3.57 14.06 6.70 7.03 6.25 22.66 4.46 10.94 2.23 5. MSWE-agent 12.50 21.88 16.41 20.31 23.44 11.72 9.38 2.34 7.03 0.45 4.02 4.91 8.04 11.16 2.68 5.80 0.00 1."
        },
        {
            "title": "MopenHands",
            "content": "9.38 3.91 10.16 14.84 21.88 9.38 8.59 3.13 0.78 0.00 0.45 0.45 11.61 2.23 1.34 0.45 0.00 0.00 13 - 4 o"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 1 p k - 1 p k - 3 b - 1 . 5 - Q 2 . 5 - 7 2 - t t u - 3 . 7 - n C d - 3 . 5 - n t"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 3 - i - h - 4 o"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 1 p k - 1 p k - 3 b - 1 . 5 - Q 2 . 5 - 7 2 - t t u - 3 . 7 - n C d - 3 . 5 - n t"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 3 - i - h - 4 o"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 1 p k - 1 p k - 3 b - 1 . 5 - Q 2 . 5 - 7 2 - t t u - 3 . 7 - n C d - 3 . 5 - n t"
        },
        {
            "title": "O\np\ne\nn\nA",
            "content": "I - 3 - i - h"
        },
        {
            "title": "M\no\nd\ne\nl\ns",
            "content": "1 5 . 4 6 6 . 7 0 4 1 . 2 4 4 1 . 2 4 7 1 . 6 5 4 8 . 9 3 1 . 4 4 1 8 . 5 6 3 8 . 6 6 1 7 . 5 3 1 5 . 4 6 2 . 5 7 . 2 2 6 1 . 8 6 2 8 . 3 5 4 2 . 7 8 4 0 . 7 2 2 5 . 7 3 9 . 1 8 4 4 . 3 3 5 8 . 7 6 5 7 . 7 3 6 4 . 4 3 6 1 . 8 6 7 . 0 1 6 8 . 0 4 5 5 . 1 5 2 5 . 9 3 1 8 . 0 6 1 0 . 0 1 2 . 7 7 2 4 . 2 4 0 . 0 0 7 . 4 1 1 4 . 8 1 1 8 . 5 4 8 . 1 5 0 . 0 0 0 . 0 0 1 . 3 9 2 . 7 8 2 . 7 1 0 . 0 0 2 0 . 0 0 1 0 . 0 0 0 . 0 0 4 . 9 6 2 . 1 0 . 0 0 2 . 1 3 1 . 5 2 1 . 5 2 1 3 . 6 4 6 . 0 0 . 0 0 0 . 0 0 6 . 6 7 6 . 6 7 1 4 . 2 9 1 0 . 7 1 4 . 2 9 1 7 . 8 6 3 0 . 0 0 1 1 . 3 5 2 1 . 2 1 1 3 . 3 3 2 . 1 4 2 2 . 2 2 7 . 4 1 2 9 . 6 3 1 1 . 1 1 7 . 4 1 4 . 8 1 3 3 . 3 3 1 . 3 9 1 . 3 9 0 . 0 0 2 . 7 0 . 0 0 9 . 7 2 5 . 5 6 4 4 . 4 4 2 0 . 8 3 4 8 . 1 1 5 . 2 8 4 0 . 0 0 3 0 . 0 0 0 . 0 0 1 0 . 0 0 0 . 0 1 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 3 3 . 3 3 1 1 . 1 2 0 . 0 0 4 8 . 1 5 2 2 . 2 2 4 . 1 7 0 . 0 0 1 0 . 0 0 . 0 0 1 4 . 8 1 3 3 . 3 3 1 . 3 9 6 . 9 4 1 0 . 0 2 0 . 0 0 5 1 . 8 5 1 1 . 1 1 3 0 . 0 0 1 8 . 5 2 1 1 . 1 3 0 . 0 0 9 . 2 2 8 . 5 1 5 . 6 7 3 . 5 5 3 . 5 7 . 8 0 9 . 9 3 1 0 . 6 1 6 . 0 6 1 5 . 1 5 1 5 . 1 4 . 9 6 8 . 5 1 8 . 5 1 5 . 6 7 1 . 4 2 6 . 3 9 . 2 2 1 3 . 6 4 3 . 0 3 6 . 0 6 1 . 5 2 1 . 5 4 . 5 5 1 0 . 6 1 6 . 6 7 6 . 6 7 6 . 6 7 0 . 0 3 . 3 3 0 . 0 0 3 . 3 3 0 . 0 0 3 2 . 1 4 1 4 . 2 7 . 1 4 1 0 . 7 1 1 7 . 8 6 0 . 0 0 1 7 . 8 6 1 0 . 7 1 0 . 6 4 1 3 . 6 4 2 0 . 0 0 2 8 . 5 7 1 3 . 4 8 1 3 . 6 1 3 . 3 3 1 7 . 8 6 1 2 . 1 2 3 . 3 3 1 4 . 2 9 6 . 0 1 . 5 2 1 0 . 0 0 6 . 6 7 7 . 1 4 7 . 1 4 0 . 0 0 . 0 0 0 . 0 0 6 . 6 7 3 . 3 3 3 . 3 3 6 . 6 3 . 3 3 3 . 3 3 0 . 0 0 3 . 5 7 3 . 5 7 0 . 0 7 . 1 4 1 0 . 7 1 3 . 5 7 1 4 . 2 9 1 7 . 8 6 3 3 . 3 5 . 5 6 2 0 . 0 0 1 3 . 4 8 1 0 . 6 1 3 7 . 0 4 1 1 . 1 3 0 . 0 0 1 1 . 3 5 9 . 0 9 1 4 . 8 1 1 . 3 9 3 0 . 0 4 0 . 7 4 1 1 . 1 1 2 0 . 0 0 2 2 . 2 2 4 . 1 7 2 0 . 0 9 . 9 3 6 . 3 8 6 . 3 8 2 2 . 7 3 1 6 . 6 7 1 3 . 6 4 . 9 8 3 . 4 5 1 9 . 1 6 2 1 . 4 6 4 4 . 8 3 3 6 . 0 1 4 . 5 6 1 6 . 4 8 1 9 . 5 4 1 0 . 7 3 4 . 9 8 1 . 9 2 . 6 8 4 0 . 6 1 2 5 . 6 7 2 1 . 4 6 2 4 . 1 4 1 6 . 0 2 0 . 3 1 1 8 . 3 9 3 6 . 0 2 3 4 . 8 7 3 5 . 6 3 3 4 . 1 3 8 . 3 1 4 0 . 2 3 2 7 . 9 7 1 . 5 4 3 . 0 8 1 0 . 7 1 0 . 7 7 2 3 . 0 8 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 2 . 2 7 1 8 . 4 6 1 3 . 6 4 1 0 . 7 7 4 . 6 2 6 . 1 0 . 0 0 0 . 0 0 0 ."
        },
        {
            "title": "M\no\np\ne\nn\nH\na\nn\nd\ns",
            "content": "7 . 6 9 1 . 5 4 1 2 . 3 1 9 . 2 3 2 7 . 6 9 2 0 . 0 1 8 . 4 6 2 3 . 0 8 1 5 . 3 8 2 . 2 7 0 . 0 0 6 . 8 2 . 2 7 9 . 0 9 5 . 6 8 3 . 4 1 4 . 5 5 1 . 1 W - n 4 . 6 2 7 . 6 9 2 3 . 0 8 6 . 1 5 1 3 . 8 1 3 . 8 5 4 . 6 2 2 4 . 6 2 1 3 . 8 5 3 . 4 1 4 . 5 6 . 8 2 6 . 8 2 3 . 4 1 2 . 2 7 0 . 0 0 4 . 5 1 ."
        },
        {
            "title": "M\na\ng\ne\nn\nt\nL\ne\ns\ns",
            "content": "0 . 9 5 0 . 9 5 2 . 8 6 1 . 9 0 7 . 6 2 3 . 8 3 . 8 1 6 . 6 7 2 . 8 6 1 . 9 0 0 . 9 5 1 . 9 3 . 8 1 7 . 6 2 7 . 6 2 3 . 8 1 6 . 6 7 0 . 9 0 . 0 0 0 . 0 0 7 . 6 2 4 . 7 6 3 . 8 1 1 . 9 4 . 7 6 9 . 5 2 1 . 9 0 0 . 0 0 1 . 3 1 0 . 0 0 . 0 0 9 . 1 5 5 . 2 3 1 . 9 6 1 . 9 6 1 . 9 0 . 6 5 0 . 0 0 0 . 0 0 3 . 2 7 4 . 5 8 2 . 6 2 . 6 1 3 . 9 2 1 . 3 1 2 . 6 1 0 . 6 5 1 . 9 4 . 5 8 3 . 9 2 3 . 2 7 1 . 3 1 5 . 8 8 1 . 9 0 . 0 0 0 . 7 9 1 . 5 9 3 . 9 7 0 . 0 0 1 . 8 1 . 8 5 3 . 7 0 0 . 0 0 0 . 0 0 3 . 3 9 8 . 4 1 3 . 4 9 1 1 . 1 1 1 5 . 2 5 7 . 1 4 0 . 7 9 2 . 3 2 . 3 8 0 . 7 9 0 . 0 0 0 . 7 9 4 . 7 6 2 . 3 4 . 7 6 2 . 3 8 3 . 9 7 3 . 1 7 1 . 5 9 1 . 5 3 . 9 7 0 . 7 9 3 . 1 7 3 . 1 7 2 . 3 8 3 . 1 1 . 5 9 3 . 7 0 0 . 0 0 3 . 7 0 0 . 0 0 3 . 7 3 . 7 0 0 . 0 0 3 . 7 0 7 . 4 1 3 . 7 0 3 . 7 3 . 7 0 0 . 0 0 0 . 0 0 1 . 8 5 1 . 8 5 1 . 8 1 . 8 5 1 . 8 5 1 . 8 5 1 . 8 5 1 . 8 5 1 0 . 1 6 . 7 8 5 . 0 8 3 . 3 9 3 . 3 9 0 . 0 0 5 . 0 1 0 . 1 7 1 1 . 8 6 6 . 7 8 5 . 0 8 5 . 0 8 1 . 6 0 . 0 0 0 . 0 0 5 . 0 8 3 . 3 9 3 . 3 9 3 . 3 1 . 6 9 5 . 0 8 6 . 7 8 2 . 2 2 0 . 0 0 0 . 0 6 . 6 7 1 1 . 1 1 1 3 . 3 3 6 . 6 7 2 . 2 2 4 . 4 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 6 . 6 7 4 . 4 8 . 8 9 4 . 4 4 4 . 4 4 4 . 4 4 0 . 0 0 6 . 6 4 . 4 4 1 1 . 1 1 6 . 6 7 4 . 4 4 8 . 8 9 2 . 2 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 2 . 7 8 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 1 . 5 6 1 . 5 6 1 . 5 6 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 3 . 1 3 3 . 1 0 . 0 0 3 . 1 3 0 . 0 0 1 . 5 6 1 . 5 6 0 . 0 1 . 5 6 1 . 5 6 1 . 5 6 0 . 0 0 1 . 5 6 1 . 5 0 . 8 3 0 . 0 0 2 . 0 7 0 . 8 3 2 . 9 0 0 . 8 1 . 6 6 1 . 2 4 1 . 6 6 0 . 8 3 0 . 4 1 0 . 8 2 . 0 7 3 . 7 3 2 . 9 0 3 . 7 3 2 . 9 0 0 . 8 1 . 2 4 0 . 4 1 2 . 0 7 1 . 6 6 0 . 4 1 0 . 8 0 . 8 3 2 . 4 9 0 . 4 1 0 . 0 0 0 . 7 5 0 . 0 0 . 0 0 1 . 4 9 2 . 2 4 0 . 0 0 0 . 7 5 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 7 5 0 . 7 5 1 . 4 0 . 0 0 1 . 4 9 0 . 0 0 0 . 0 0 0 . 0 0 1 . 4 1 . 4 9 0 . 0 0 0 . 7 5 0 . 7 5 0 . 7 5 0 . 0 2 . 1 3 4 . 2 6 0 . 0 0 4 . 2 6 1 4 . 8 9 8 . 5 4 . 2 6 2 . 1 3 2 . 1 3 4 . 2 6 0 . 0 0 2 . 1 2 . 1 3 8 . 5 1 2 . 1 3 2 . 1 3 2 . 1 3 0 . 0 2 . 1 3 0 . 0 0 2 . 1 3 2 . 1 3 4 . 2 6 4 . 2 2 . 1 3 4 . 2 6 6 . 3 8 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 2 . 2 7 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 2 . 2 7 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 2 . 2 7 2 . 2 7 0 . 0 0 4 . 5 5 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 2 . 3 8 2 . 3 2 . 3 8 0 . 0 0 0 . 0 0 2 . 3 8 0 . 0 0 0 . 0 2 . 3 8 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 0 . 0 0 . 0 0 0 . 0 0 0 . 0"
        },
        {
            "title": "T\na\nb\nl\ne",
            "content": "5 ."
        },
        {
            "title": "R\ne\ns\no\nl\nv\ne\nd\nr\na\nt\ne",
            "content": "( % ) i r m l M i - - c r v o i fi t v ."
        },
        {
            "title": "P\ny\nt\nh\no\nn",
            "content": "J a"
        },
        {
            "title": "T\nS",
            "content": "J S"
        },
        {
            "title": "R\nu\ns\nt",
            "content": "C + +"
        },
        {
            "title": "P\ny\nt\nh\no\nn",
            "content": "J a"
        },
        {
            "title": "T\nS",
            "content": "J S"
        },
        {
            "title": "R\nu\ns\nt",
            "content": "C + +"
        },
        {
            "title": "P\ny\nt\nh\no\nn",
            "content": "J a"
        },
        {
            "title": "T\nS",
            "content": "J S"
        },
        {
            "title": "R\nu\ns\nt",
            "content": "C + + Limited generalization beyond Python. From Tab. 4, it can be observed that existing LLMs and methods demonstrate strong performance in resolving Python issues but struggle to generalize effectively across other languages. For example, LLMs such as OpenAI-o1 and Claude-3.7-Sonnet achieve high resolved rates for Python but significantly lower effectiveness for most other languages. This performance disparity can be attributed to three main factors: (1) Benchmark difficulty: Multi-SWE-bench is inherently more challenging than SWE-Bench-Verified, with higher proportion of medium and hard issues (77.1% for Multi-SWE-bench compared to 61.2% for SWE-Bench-Verified, as calculated from Tab. 2). (2) Method optimization bias: The three methods are initially optimized for Python, resulting in performance bias that limits their effectiveness across other languages. (3) Language-specific complexity: Languages like TS and JS feature dynamic typing, asynchronous execution, and diverse runtime behaviors, while languages like C, C++, Rust, and Go involve manual memory management, complex build systems, and intricate type systems, which add to the difficulty for issue resolving. Performance variations across language domains. Languages in Multi-SWE-bench can be largely categorized into four domains: high-level general-purpose programming (Python, Java), web development (TS, JS), systems programming (Go, Rust), and low-level/high-performance computing (C, C++). Based on Tab. 4, the performance generally follows hierarchy, with highlevel general-purpose languages outperforming systems programming and low-level/highperformance computing languages, while web development languages perform the worst. Java ranks second after Python, though with noticeable gap. Go and Rust exhibit inconsistent performance across models, generally outperforming TS and JS but falling behind Java. and C++ show even greater variability, with some LLMs (e.g., Doubao-1.5-pro and Qwen2.572B-Instruct) struggling to handle them effectively due to the challenges posed by manual memory management and complex compilation pipelines. TS and JS consistently yield the lowest resolved rates, highlighting the difficulty LLMs face in handling their event-driven, asynchronous programming paradigms. High sensitivity to issue difficulty. As shown in Tab. 5, LLM-based agents exhibit performance that closely aligns with human-labeled difficulty, with resolved rates significantly decreasing as the issue difficulty increases from easy to hard. However, there are several exceptions. For example, the JS language on the SWE-agent and Claude-3.5-Sonnet exhibits zero resolved rate for easy tasks, compared to 7.62% for medium tasks. This suggests that the MSWE-agent is less effective for JS. In addition to human-assigned difficulty, other factors, such as the number of files requiring modification to resolve the issue, also influence performance. For hard-level issues, existing LLMs and agents are mostly ineffective, with resolved rates approaching zero. This phenomenon indicates the limitations of these LLMs and agents: they are primarily capable of addressing issues that human developers can resolve in under 15 minutes and are insufficient for handling more complex tasks requiring over one hour of human effort. This finding further underscores the need for RL techniques aimed at advancing agents towards more human-like intelligence, particularly for tackling real-world complex scenarios. 6.1.2. Performance across Various Methods and LLMs Variation in LLMs performance. From Tab. 4, significant variability is observed in the performance of different LLMs across programming languages. Specifically, LLMs such as OpenAI-o1, OpenAI-o3-mini-high, Claude-3.5-Sonnet, and Claude-3.7-Sonnet show relatively strong performance, particularly in languages like Python, Java, and C. In contrast, LLMs like Qwen2.572B-Instruct and Doubao-1.5-pro exhibit rather lower resolved rates, particularly for hard-level issues shown in Tab. 5. Furthermore, the LLMs exhibit distinct language-specific biases in 15 Figure 4. Issue flow from locating to resolving. 16 their performance. For example, models like OpenAI-o1 and OpenAI-o3-mini-high perform consistently well across languages such as Python and Java, whereas they struggle significantly with languages like and C++. On the one hand, this performance disparity is likely attributed to the models being better suited to handle higher-level languages like Python and Java, which are more prevalent in their training data. On the other hand, the challenges with and C++ may arise from the models limited exposure to low-level language features, such as memory management and pointer manipulation, which are less represented in the training data. Performance comparison of methods. Tab. 4 and Tab. 5 also provide comprehensive comparison of the resolved rates across three issue-resolving methods, including MagentLess, MSWE-agent, and MopenHands. Overall, MopenHands outperforms the others in most cases, achieving the highest resolved rate in five out of seven languages, while MSWE-agent wins twice and MagentLess wins once. The better performance of MopenHands and MSWE-agent can be attributed to their more flexible workflow, which is better suited to another language beyond Python compared to MagentLess. In contrast, MagentLess follows more rigid workflow optimized for Python, and the adaptation made to create the MagentLess limits its adaptability across other languages. However, notable exception to this general trend is observed in the performance of the models DeepSeek-R1 and Qwen2.5-72B-Instruct. For these two models, MagentLess generally provides better results than MSWE-agent for languages except and C++. This suggests that these models may be better suited to the fixed workflow of MagentLess. Prioritizing accurate locating over editing and reproducing. MagentLess, MSWE-agent, and MopenHands generally resolve issues through two key steps: issue location and code editing to resolve the issue. To provide more detailed analysis of how existing LLMs and methods perform across these steps, we present the issue flow in Fig. 4. An issue is considered successfully located if the fix patches generated by the LLMs hit the files of ground truth fix patches. As shown in Fig. 4, all three methods generally fail to locate issues more often than they succeed. Accurate issue localization is fundamental to the overall success of the resolution process, serving as prerequisite for effective code editing. Compared to MopenHands, MagentLess achieves more accurate issue localization but struggles more with the code editing step, leading to lower overall resolved rate. This disparity is particularly evident on Claude-3.7-Sonnet. This underscores the need for balanced method that not only prioritizes precise issue identification but also enhances the models ability to generate effective fixes. Number of turns required by MSWE-agent and MopenHands. Both MSWE-agent and MopenHands resolve the issue by multi-turn interactions. Fig. 5 shows the distribution of turns for successfully resolved an issue. The absence of corresponding box plot indicates cases where no issues were successfully resolved, such as MSWE-agent with Qwen2.5-72B-Instruct on C++. The number of interaction turns required by two methods differs across models and languages. Specifically, MopenHands resolves issues in fewer turns than MSWE-agent when using GPT-4o for Java, whereas MSWE-agent requires fewer turns when resolving Python issues. However, MopenHands exhibits rather higher degree of dispersion in the number of interaction turns compared to MSWE-agent, which is particularly evident on OpenAI-o3-mini-high. This suggests that MopenHands performance is less stable across different issues, requiring varying number of turns depending on the complexity or nature of the issue. 6.1.3. Performance across Different Repositories To understand how repository characteristics affect performance, we examine two factors: (1) repository quality, which includes the number of stars, forks, PRs, and issues, and (2) repository complexity, which includes the number of code lines and files, and the language entropy. 17 Figure 5. Number of turns required across different programming languages. Performance across repositories of varying quality. To assess repository quality, we examine key metrics including the number of stars, forks, PRs, and issues. Fig. 6 illustrates the average resolved rate across LLMs for the three methods in relation to the number of stars and forks. Similarly, Fig. 7 shows the average resolved rate in relation to the number of issues and PRs. Both Fig. 6 and Fig. 7 exhibit general positive correlation between #Stars and #Forks, as well as #Issues and #PRs across the majority of repositories. Furthermore, repositories with higher resolved rates tend to cluster in the upper-right quadrant of both Fig. 6 and Fig. 7, suggesting that repositories with greater activity and community engagement (i.e., higher counts of stars, forks, issues, and PRs) are typically associated with higher resolved rate. This trend is particularly evident for the MSWE-agent and MopenHands. In contrast, MagentLess exhibits relatively low variation in resolved rates across both Fig. 6 and Fig. 7, underscoring an important observation: while greater number of stars, forks, issues, and PRs tend to correlate with higher resolved rates, these metrics do not provide guarantee of repositorys issue-resolving effectiveness. Performance across repositories with different levels of complexity. To evaluate repository complexity, we consider several key metrics: the number of lines of code (#LoC), the number of files (#Files), and language entropy. Let ð¿ = {ð1, ð2, , ðð} represent the set of programming languages used in the repository, with corresponding proportions { ð1, ð2, , ðð}. The language entropy of the repository is then calculated as: ð» (ð¿) = ð ð=1 ðð log( ðð) where ðð denotes the proportion of the repository written in language ðð. The average resolved rate across nine base LLMs with different repository complexity is presented in Fig. 8. Fig. 8 shows consistent trend in the resolved rate across varied repository complexity: All 18 Figure 6. Relationship between resolved rate and the number of stars and forks of repository. Figure 7. Relationship between resolved rate and the number of issues and PRs of repository. three methods exhibit fluctuations in performance with changes in #LoC, #Files, and language entropy, generally decreasing as the repository complexity increases. For the impact of #LoC, as #LoC increases, the resolved rate tends to decrease. However, Java-based repositories, such as gson, jib, j-core, j-dbind, and dubbo, show higher resolved rates despite their larger size. This suggests that factors beyond code size, such as lower language entropy, modularity, welldocumented code, and adherence to standardized practices, play significant role in improving performance. For example, the gson repository demonstrates nearly-zero language entropy in Fig. 8. Similarly, the impact of #Files follows trend similar to #LoC. The impact of language entropy shows clearer trend than that of #LoC and #Files: repositories with lower entropy typically achieve higher resolved rates. This indicates that code simplicity and consistency play crucial role in improving issue-resolving effectiveness on repository. 6.2. Influencing Factors of Performance In this subsection, we investigate the factors influencing issue resolving performance, focusing on three key factors: (1) issue type, examining how different types of issues impact resolving effectiveness; (2) issue description characteristics, evaluating the role of description length in resolving issues; and (3) fix patch characteristics, analyzing how the length of fix patches and the number of involved files influence the resolving performance. 19 Figure 8. Relation between resolved rate and the repository complexity on Multi-SWE-bench. 6.2.1. Issue Type Tab. 6 lists the performance of the three methods on Multi-SWE-bench across different issue types and languages. Through meticulous manual analysis of the annotation results in Sec. 3.1.5, we categorized all instances in Multi-SWE-bench into three issue types: bug fix (Bug Fix), new feature (New Feat.), and feature optimization (Feat. Opt.). We observe consistent performance hierarchy across all methods and languages: bug fix issues are resolved with the highest success rates, followed by new features, with feature optimization being the most challenging. For instance, MSWE-agent achieves 17.97% on Java bug fixes but drops to 3.91% and 1.56% for new features and optimizations, respectively. MagentLess and MopenHands show similar trend in all languages. These results highlight fundamental limitation of current agent-based methods: 20 they are more effective at localized, symptom-driven repairs, but struggle with semantically demanding tasks such as implementing new functionality or refining existing behavior. The latter requires deeper intent understanding, multi-component reasoning, and cross-file context aggregation capabilities that remain underdeveloped in current LLM-based agents. Table 6. Resolved rate(%) on Multi-SWE-bench across different issue types (Claude-3.7-Sonnet)."
        },
        {
            "title": "Languages",
            "content": "Java TypeScript JavaScript Go Rust C++ MSWE-agent Bug Fix New Feat. Feat. Opt. Bug Fix New Feat. Feat. Opt. Bug Fix New Feat. Feat. Opt."
        },
        {
            "title": "MagentLess",
            "content": "10.94 2.68 1.97 3.74 4.60 6.25 2.33 2.34 0.45 0.00 0.93 0.42 0.00 0.78 0.78 0.45 0.00 1.17 0.42 0.00 0.00 17.97 9.38 4.21 3.27 5.44 7.81 7.75 3.91 1.34 0.56 0.70 1.26 0.78 3.1 1.56 0.45 0.00 1.40 0.00 0.00 0. 17.97 1.79 3.65 4.44 12.97 7.81 10.85 3.12 0.00 1.12 2.10 2.93 0.78 3.10 0.78 0.45 0.28 0.93 0.00 0.00 0.78 6.2.2. Characteristics of Issue Description We aim to examine the impact of issue description length on issue-resolving performance. Fig. 9 illustrates the distribution of issue lengths (in tokens) in Multi-SWE-bench, which follows power law, with the majority of issues being under 1,000 tokens. To explore the effect of description length, the issues are categorized into 5 intervals: <100, 100-400, 400-700, 700-1000, and >1000 tokens, as shown in Fig. 10. The absence of corresponding bars indicates cases where no issues are successfully resolved. Figure 9. Histogram of issue description length (#tokens). As shown in Fig. 10, there is no consistent relationship between issue description length and resolved rate. For example, in Python, issues with longer descriptions tend to have lower resolved rates, whereas in Go, longer descriptions are associated with higher rates. This discrepancy arises from two potential types of long issue descriptions: (1) detailed issues with precise issue position indications and resolving steps, and (2) complex issues that require extended descriptions to explain. These two possibilities have distinct impacts on the difficulty of resolving an issue, influencing the resolved rate in different ways. As for the performance among methods, compared with MSWE-agent and MopenHands, MagentLess generally performs better with longer, more detailed descriptions on average of the nine LLMs, especially for languages like Python, Java, and TS. 21 Figure 10. Influence of issue description length on resolved rate. 6.2.3. Characteristics of Fix Patches In this subsection, we investigate the impact of the ground-truth fix patches on the resolved rate, focusing on two key factors: (1) Fix patch length: We analyze how the length of fix patches affects performance, noticing that longer patches require more complex reasoning capabilities from LLMs. The fix patches are categorized into five intervals based on the length distribution shown in Fig. 11: <200, 200-600, 600-1000, 1000-1400, and >1400 tokens. (2) Number of files modified by fix patches: We examine how the cross-file nature of the fix patches influences performance, with more files requiring enhanced cross-file handling capabilities. The number of modified files is divided into four categories: 1, 1-5, 5-10, and >10, with the distribution shown in Fig.12. The absence of corresponding bars indicates cases where no issues are successfully resolved. Figure 11. Histogram of fix patches length (#tokens). Figure 12. Histogram of the number of files modified by fix patches. Performance drops as fix patch length increases. As shown in Fig. 13, the length of fix patches significantly impacts the resolved rate, with shorter patches generally leading to higher success rates. Specifically, in the majority of cases, issues with descriptions >600 tokens exhibit resolved rate approximately 50% lower than that of issues with descriptions <200 tokens. For most programming languages, the resolved rate for shorter fix patches is notably higher, 22 Figure 13. Influence of fix patch length on resolved rate. especially for MagentLess, which shows peak in this range for languages like Python, Java, and C. This suggests that shorter patches are easier to handle, as they require less reasoning and simpler edits. For all three methods, the resolved rate for very long fix patches (>1000 tokens) drops significantly, even reaching zero for Java. This indicates that long patches, which likely require handling larger scope of modifications, present greater challenges, especially for methods that may not be optimized for such complex tasks. Cross-file fix patches lead to reduced effectiveness. Fig. 14 illustrates the relationship between the number of files modified by fix patches and the resolved rate. Consistent with the observation in Fig. 13, resolved rate drops significantly as the number of modified files increases across all three methods. This trend highlights the potential challenge of understanding and resolving issues that require changes across multiple files, which may demand more intricate handling or coordination between different parts of the repository. For issues resolved by modifications in single file, MagentLess outperforms MSWE-agent and MopenHands in five out of seven programming languages. This suggests that MagentLess is more effective at resolving issues within the scope of single file. 6.3. Case Study In this subsection, we analyze representative cases that highlight the strengths of agents, common failure patterns, and language-specific challenges, providing insights for future directions."
        },
        {
            "title": "6.3.1 Language-General Case",
            "content": "MSWE-agent and MopenHands often failed by exhausting the 50-round interaction limit, sometimes without even triggering the submit action, as seen in cases like axios__axios5919.traj, clap-rs__clap-5520.traj, and cli__cli-513.traj. Future work may explore strategies that enable agents to solve more complex tasks within limited number of interaction rounds. significant number of failures across all three agent methods were due to incorrect fault localization, which led to an inability to identify and modify the relevant code, as seen in 23 Figure 14. Influence of the number of files modified by fix patches on resolved rate. cases such as elastic__logstash-14898.traj, alibaba__fastjson2-2285.traj, fasterxml__jacksondatabind-3560.traj, and apache__dubbo-7041.traj. This highlights the centrality of accurate fault localization and points to the potential of integrating software engineering techniques like SBFL [Abreu et al., 2007, Jones et al., 2002] into future agent designs. In cases such as astropy__astropy-12907.traj and django__django-11299.traj, the model generated multiple valid actions in single turn, but the hardcoded agent framework executed only the last, resulting in premature submission. This reveals structural bottleneck in current agent design, where rigid control logic overrides model intent. It calls for shift toward lightweight, model-centric agents with full decision autonomy delegated to the LLM. Bug reproduction plays critical role in successful repair. In cases such as nlohmann__json4537.traj, fmtlib__fmt-3248.traj, fasterxml__jackson-core-1142.traj, and google__gson-1093.traj, the model successfully reproduced the issue before producing an effective fix. In contrast, failure to reproduce often resulted in unresolved cases, as seen in catchorg__Catch2-1609.traj. However, reproduction is not always prerequisite for success. Claude-3.5-Sonnet and Claude-3.7-Sonnet occasionally bypass reproduction and edit the code directlyyet still resolve the issue successfully, as in nlohmann__json-3601.traj, fmtlib__fmt-3729.traj, and googlecontainertools__jib-4035.traj. These cases suggest that agents should selectively invoke reproduction based on factors such as error traceability, edit confidence, and execution cost."
        },
        {
            "title": "6.3.2 Language-Specific Case",
            "content": "For certain TypeScript projects, the length of the extracted repository structure often exceeds the models maximum context length, preventing MagentLess from performing fault localization (e.g., mui__material-ui-25852.traj and mui__material-ui-37850.traj). This reveals the limited generalizability of fixed workflows like MagentLess when confronted with structurally irregular and language-specific scenarios, indicating significant room for improvement in both robustness and adaptability. Tree-sitter fails to reliably extract code structures in JavaScript repositories that use loosely bound syntax such as arrow functions, preventing MagentLess from constructing contextual windows around candidate edits (e.g., iamkun__dayjs-2532.traj and iamkun__dayjs-2399.traj). 24 Table 7. Average token consumption on Multi-SWE-bench. In. represents the average number of input tokens (in thousands), and Out. is the average number of output tokens (in thousands)."
        },
        {
            "title": "Java",
            "content": "TS JS Go"
        },
        {
            "title": "Rust",
            "content": "C C++ In. Out. In. Out. In. Out. In. Out. In. Out. In. Out. In. Out. In. Out. GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro 36.15 34.43 31.38 39.13 27.99 39.97 31.35 28.60 42.75 GPT-4o 166.91 OpenAI-o1 243.44 OpenAI-o3-mini-high 240.23 33.30 Claude-3.5-Sonnet Claude-3.7-Sonnet 31.86 12.63 DeepSeek-V3 11.76 DeepSeek-R1 164.42 Qwen2.5-72B-Instruct 72.58 Doubao-1.5-pro GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro 25.35 19.27 21.52 32.35 26.04 18.97 11.25 27.28 23.16 4.20 3.76 4.50 5.38 6.54 4.26 2.80 3.46 2. 3.08 1.64 1.82 5.55 4.46 22.83 2.65 6.69 1.30 1.24 1.20 5.18 7.69 7.84 5.16 5.13 10.38 3.95 52.10 50.18 79.48 48.42 63.97 42.35 70.35 62.95 116.09 51.05 33.36 26.37 32.09 38.96 35.08 17.51 53.43 37.75 22.01 18.69 22.82 31.97 28.43 26.35 17.15 33.26 24.15 2.74 1.92 2.36 2.67 3.16 2.70 1.76 2.52 1. 4.54 2.99 3.64 3.89 4.79 4.14 2.69 9.26 3.73 1.32 1.27 3.88 5.35 7.86 3.65 5.04 11.80 3."
        },
        {
            "title": "MagentLess",
            "content": "241.18 240.47 245.39 239.93 248.36 244.32 249.02 243.98 249.51 2.01 1.21 1.54 1.86 2.08 1.92 1.10 1.65 1.55 29.48 36.53 38.28 28.46 26.66 26.44 28.23 26.11 36.37 2.53 1.26 1.55 2.39 3.17 2.51 1.30 2.14 2.07 MSWE-agent 4.63 3.56 3.39 3.10 4.92 2.15 1.66 7.82 2. 32.01 25.70 21.33 23.94 32.16 19.78 9.36 35.21 32.90 4.36 3.19 3.99 3.66 4.60 3.23 2.43 6.45 3.68 MopenHands 1.60 2.14 4.32 6.43 7.31 3.69 4.33 9.12 1.66 35.51 30.96 36.57 38.91 38.06 29.08 17.85 28.84 23.75 1.50 2.07 4.06 6.14 7.46 4.43 5.29 9.07 2.76 46.39 30.05 18.27 21.51 32.08 15.73 9.91 39.58 19. 35.76 27.28 30.70 35.88 31.06 26.60 12.71 36.86 18.34 25.14 24.51 31.58 22.80 22.79 22.78 21.69 24.67 29.38 36.73 37.71 26.46 21.06 33.79 15.34 10.47 22.53 25.39 23.96 21.09 25.44 27.31 30.05 15.42 12.65 21.17 18.21 2.72 1.70 1.67 2.79 3.63 2.65 1.79 3.36 3.15 4.71 3.49 3.41 3.06 4.56 2.43 1.85 8.38 3. 1.52 1.56 4.14 7.26 7.86 3.31 5.14 11.35 3.78 48.23 58.59 68.80 51.25 81.15 83.04 88.73 50.63 124.67 43.79 39.51 32.84 35.47 40.59 33.98 13.98 36.49 38.09 40.40 28.90 30.64 55.51 48.30 32.90 17.58 37.14 27.40 2.71 1.49 2.05 2.66 3.33 2.47 1.52 2.89 1.62 4.71 3.71 4.03 4.03 4.56 5.47 2.86 7.93 4. 1.45 1.55 3.15 6.23 7.00 5.05 6.95 10.99 2.07 50.26 48.08 73.48 49.13 50.52 92.53 100.99 55.93 121.94 39.47 34.05 23.24 31.16 38.41 16.26 11.34 28.90 29.03 34.80 27.18 23.98 55.79 35.25 21.77 24.16 35.02 26.54 2.64 1.57 1.99 2.55 3.19 2.38 1.39 2.78 1.52 4.57 3.17 3.78 3.44 4.34 2.07 2.44 5.76 3. 2.08 1.67 3.26 5.66 6.30 3.53 6.11 9.69 2.82 76.38 119.64 200.29 96.85 129.34 189.08 177.66 150.44 216.21 55.49 29.24 32.39 38.22 36.96 31.28 14.64 67.29 32.67 34.61 21.55 23.76 35.85 33.14 30.67 17.38 35.34 26.07 2.44 1.31 1.91 2.49 2.91 2.11 1.41 2.19 0.76 4.96 3.28 4.90 4.32 4.67 4.18 3.06 11.69 3. 1.66 1.57 4.26 6.74 7.76 5.36 7.62 10.88 3.44 This exposes structural brittleness in syntax-driven workflows when applied to syntactically permissive languages, motivating future extensions of MagentLess toward greater tolerance to parsing failure and language-specific irregularities. In some JavaScript projects, agents sometimes invoke pnpm to launch development servers as part of the repair routine. However, current agent frameworks lack support for managing long-lived, interactive processes, often resulting in premature termination or container crashes (e.g., sveltejs__svelte-12460.traj and sveltejs__svelte-10077.traj). Future agents should support persistent shell sessions and interactive service control, as enabled by frameworks like SWEReX [SWE-agent, 2025]. 6.4. Resource Consumption In this subsection, we analyze the resource consumption across different languages, focusing on two key metrics: (1) the average token consumption and (2) the average cost per issue. Average token consumption per issue. Tab. 7 compares the average token consumption for various languages using the GPT-4o tokenizer. Overall, token consumption varies between methods and languages. Among languages, TS exhibits the highest token consumption in MagentLess, whereas Python is the most token-intensive language in MSWE-agent. Notably, Go demonstrates relatively low token consumption in both input and output, likely due to its minimalistic syntax and clear conventions, which contribute to its compact representation and reduced token overhead. Additionally, in MSWE-agent for Python, we observe increased token usage on LLMs, including GPT-4o, OpenAI-o1, OpenAI-o3-mini-high, and Qwen2.5-72BInstruct. This is because we maintain the original SWE-agent implementation for Python, which does not incorporate the over-length truncation mechanism applied to other languages. 25 Table 8. Average cost ($) per issue of different models and methods on Multi-SWE-bench."
        },
        {
            "title": "Java",
            "content": "JS Go"
        },
        {
            "title": "Rust",
            "content": "C C++ GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro GPT-4o OpenAI-o1 OpenAI-o3-mini-high Claude-3.5-Sonnet Claude-3.7-Sonnet DeepSeek-V3 DeepSeek-R1 Qwen2.5-72B-Instruct Doubao-1.5-pro 0.1324 0.7417 0.0543 0.1981 0.1821 0.0075 0.0105 0.0051 0. 0.4480 3.7499 0.2722 0.1831 0.1626 0.0260 0.0075 0.0241 0.0083 0.0758 0.3608 0.0465 0.2124 0.1957 0.0070 0.0128 0.0077 0.0037 TS MagentLess 0.6230 3.6795 0.2767 0.7478 0.7763 0.0192 0.0373 0.0324 0.0279 0.0990 0.6233 0.0489 0.1213 0.1275 0.0046 0.0068 0.0042 0.0046 0.1576 0.8680 0.0978 0.1853 0.2393 0.0059 0.0137 0.0092 0.0132 MSWE-agent 0.1623 0.6644 0.0350 0.1110 0.1700 0.0035 0.0050 0.0083 0. 0.1236 0.5772 0.0410 0.1266 0.1654 0.0049 0.0066 0.0072 0.0046 0.1731 0.6797 0.0450 0.1546 0.1887 0.0070 0.0083 0.0106 0.0052 MopenHands 0.1054 0.5374 0.0528 0.2041 0.2028 0.0059 0.0113 0.0084 0.0025 0.1038 0.5885 0.0581 0.2089 0.2261 0.0069 0.0141 0.0074 0.0034 0.0682 0.3564 0.0422 0.1761 0.2032 0.0059 0.0134 0.0090 0.0036 0.0900 0.4698 0.0421 0.1102 0.1229 0.0045 0.0070 0.0046 0. 0.1390 0.7749 0.0441 0.1091 0.1698 0.0037 0.0055 0.0063 0.0039 0.0751 0.4099 0.0462 0.1908 0.2080 0.0047 0.0130 0.0073 0.0031 0.1476 0.9682 0.0847 0.1937 0.2933 0.0085 0.0158 0.0077 0.0142 0.1565 0.8151 0.0538 0.1669 0.1901 0.0084 0.0082 0.0079 0.0053 0.1155 0.5262 0.0476 0.2601 0.2500 0.0079 0.0177 0.0092 0.0036 0.1520 0.8153 0.0896 0.1856 0.1994 0.0091 0.0172 0.0084 0. 0.1444 0.7010 0.0422 0.1451 0.1803 0.0034 0.0069 0.0061 0.0042 0.1078 0.5081 0.0407 0.2523 0.2002 0.0054 0.0168 0.0084 0.0037 0.2153 1.8734 0.2287 0.3280 0.4317 0.0156 0.0280 0.0204 0.0240 0.1883 0.6353 0.0572 0.1794 0.1810 0.0068 0.0088 0.0134 0.0046 0.1031 0.4171 0.0449 0.2086 0.2158 0.0080 0.0191 0.0089 0.0038 Average cost per issue. Tab. 8 presents the average cost ($) per issue on Multi-SWE-bench. Notably, DeepSeek-V3, DeepSeek-R1, and Qwen2.5-72B-Instruct achieve the lowest cost per resolved issue, staying below $0.03, benefiting from their cost-efficient pricing (below $0.14 per million input tokens). In contrast, OpenAI-o1 is the most expensive model, due to its high token price ($15 per million input tokens). Overall, MagentLess exhibits lower costs compared to MSWE-agent by virtue of its fixed workflow. Conversely, the workflows of both MSWEagent and MopenHands are determined by LLMs, requiring more interaction turns with the environment, which results in higher costs. 6.5. Troubleshooting During the construction of Multi-SWE-bench and Multi-SWE-RL, we encountered range of practical and non-obvious challenges. We document the key issues below to facilitate reproducibility and guide future community contributions: Test log inconsistency. The number of test cases differs between Test.log and Fix.log, as fix.patch may optimize control flow, eliminate redundant coverage, or merge test paths, which is commonly observed in repositories such as preactjs/preact. Pre-fix build failures. Certain repositories fail to compile or execute tests before applying fix.patch, due to newly introduced symbols (e.g., functions or variables) in test.patch that are 26 undefined without the fix. Binary artifacts in C&C++. Agent runs may generate compiled binaries (e.g., \".o\", \".bin\") that block \"git apply\". We currently strip these via hard-coded filtering, though more robust handling is needed. Evaluation nondeterminism. Java and tests occasionally exhibit unstable behavior due to excessive thread concurrency, leading to inconsistent run.log outcomes. We mitigate this by reducing parallelism during evaluation. Name casing mismatches. Some test names appear in lowercase in test.log but in uppercase in fix.log. We normalize all test names to lowercase to ensure alignment. Unstable test identifiers. Some test names are dynamically generated with timestamps or random suffixes, making them non-deterministic. Such instances are excluded. Log interleaving in Java. In some Java projects, test outputs from concurrent threads are interleaved without delimiters, making rule-based log parsing infeasible. This is likely due to unsynchronized multi-threaded logging. 7. Conclusions and Future Works We introduce Multi-SWE-bench, multilingual benchmark for issue resolving, consisting of 1, 632 human-validated GitHub instances on 7 widely used programming languages. Based on this benchmark, we evaluate nine popular models using three agent methods and conduct thorough analysis of the results. Additionally, we establish the Multi-SWE-RL open-source community, aimed at building large-scale RL training datasets for issue-resolving tasks. To catalyze community involvement, we release 4, 723 validated instances along with the complete data construction pipeline, encouraging the open-source community to continuously contribute and expand the dataset. Looking ahead, we plan to scale Multi-SWE-bench and Multi-SWE-RL to more instances, languages, and modalities. Beyond issue resolving, we would like to incorporate broader range of software engineering tasks into our benchmark and RL community, such as end-to-end project generation [Zan et al., 2024, Starace et al., 2025], runtime environment setup [Zala et al., 2024, Hu et al., 2025, Eliseeva et al., 2025], bug reproduction [Wang et al., 2024a, 2025] and localization [Hossain et al., 2024], and software testing and maintenance [Lemner et al., 2024, Peng et al., 2025]. Overall, we envision our efforts as step toward establishing scalable and sustainable data-centric infrastructure that empowers future research."
        },
        {
            "title": "Contributions",
            "content": "Daoguang Zan"
        },
        {
            "title": "Core Contributors",
            "content": "Code Development Zhirong Huang Hanwu Chen Daoguang Zan"
        },
        {
            "title": "Kai Shen\nLiang Xiang",
            "content": "Names marked with denote equal contribution."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge all members of the Seed-Foundation-Code team. We thank Dong Chen, Ailun Yu, Shaoxin Lin, Yifan Shi, Bo Shen, Guangtai Liang, and Qianxiang Wang, former colleagues of Daoguang at Huawei Cloud, for their early discussion. We thank Changxin Pu and Xiang Gao at Bytedance for their support with data annotation. We thank Professors Jinxi Li from Shenzhen Technology University; Wei Zhang, Haiyan Zhao, and Zhi Jin from Peking University; and Xudong Lu and Lizhen Cui from Shandong University. We thank Shulin, Wei, Aoyan, Lu, and Qi for their dedication in the final sprint before the deadline. We are especially grateful to Zhirong, Hanwu, Linhao, and Daoguang for their countless late nights devoted to developing the dataset, without which this work would not have been possible."
        },
        {
            "title": "References",
            "content": "R. Abreu, P. Zoeteweij, and A. J. Van Gemund. On the accuracy of spectrum-based fault In Testing: Academic and industrial conference practice and research techniqueslocalization. MUTATION (TAICPART-MUTATION 2007), pages 8998. IEEE, 2007. M. Allamanis and C. Sutton. Mining source code repositories at massive scale using language modeling. In 2013 10th working conference on mining software repositories (MSR), pages 207216. IEEE, 2013. B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, et al. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868, 2022. B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, et al. Multi-lingual evaluation of code generation models. In ICLR, 2023. augment code. Augment swe-bench verified agent, 2025. URL https://www.augmentcode. com/blog/1-open-source-agent-on-swe-bench-verified-by-combining-claud e-3-7-and-o1. 2025-03-31. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021a. URL https: //arxiv.org/abs/2107.03374. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b. Y. Ding, Z. Wang, W. Ahmad, H. Ding, M. Tan, N. Jain, M. K. Ramanathan, R. Nallapati, P. Bhatia, D. Roth, et al. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36, 2024. A. Eliseeva, A. Kovrigin, I. Kholkin, E. Bogomolov, and Y. Zharov. Envbench: benchmark for automated environment setup, 2025. URL https://arxiv.org/abs/2503.14443. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. S. B. Hossain, N. Jiang, Q. Zhou, X. Li, W.-H. Chiang, Y. Lyu, H. Nguyen, and O. Tripp. deep dive into large language models for automated bug localization and repair. Proceedings of the ACM on Software Engineering, 1(FSE):14711493, 2024. 29 R. Hu, C. Peng, X. Wang, and C. Gao. An llm-based agent for reliable docker environment configuration. arXiv preprint arXiv:2502.13681, 2025. S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer. Mapping language to code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 16431652, 2018. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. H. Jelodar, M. Meymani, and R. Razavi-Far. Large language models (llms) for source code analysis: applications, models and datasets, 2025. URL https://arxiv.org/abs/2503.1 7502. J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim. survey on large language models for code generation, 2024. URL https://arxiv.org/abs/2406.00515. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test information to assist fault localization. In Proceedings of the 24th international conference on Software engineering, pages 467477, 2002. L. Lemner, L. Wahlgren, G. Gay, N. Mohammadiha, J. Liu, and J. Wennerberg. Exploring the integration of large language models in industrial test maintenance processes. arXiv preprint arXiv:2409.06416, 2024. T. Liu, C. Xu, and J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2024a. URL https://arxiv.org/abs/2306.03091. W. Liu, A. Yu, D. Zan, B. Shen, W. Zhang, H. Zhao, Z. Jin, and Q. Wang. GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model, 2024b. URL https://arxiv.org/abs/2406.07003. S. Miserendino, M. Wang, T. Patwardhan, and J. Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. N. MÃ¼ndler, M. MÃ¼ller, J. He, and M. Vechev. Swt-bench: Testing and validating real-world bug-fixes with code agents. Advances in Neural Information Processing Systems, 37:8185781887, 2024. OpenAI. Openai o3-mini, 2025. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-01-31. Y. Ouyang, J. Yang, and L. Zhang. Benchmarking automated program repair: An extensive study on both real-world and artificial bugs. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 440452, 2024. X. Peng, C. Wang, M. Liu, Y. Lou, and Y. Wu. Code digital twin: Empowering llms with tacit knowledge for complex software maintenance. arXiv preprint arXiv:2503.07967, 2025. V. Raychev, P. Bielik, and M. Vechev. Probabilistic model for code with decision trees. ACM SIGPLAN Notices, 51(10):731747, 2016. 30 N. Saavedra, A. Silva, and M. Monperrus. Gitbug-actions: Building reproducible bug-fix In Proceedings of the 2024 IEEE/ACM 46th International benchmarks with github actions. Conference on Software Engineering: Companion Proceedings, pages 15, 2024. G. Starace, O. Jaffe, D. Sherburn, J. Aung, C. J. Shern, L. Maksin, R. Dias, E. Mays, B. Kinsella, W. Thompson, J. Heidecke, M. Glaese, T. Patwardhan, and OpenAI. Paperbench: Evaluating ais ability to replicate ai research. 2025. URL https://openai.com/index/paperbench. T. Sun, Y. Yang, X. Cheng, J. Yang, Y. Huo, Z. Ye, R. Yang, X. Guan, W. Zhang, H. Ji, et al. Repofixeval: repository-level program repair benchmark from issue discovering to bug fixing. SWE-agent. Swe-agent remote execution framework, 2025. URL https://github.com/SWE -agent/SWE-ReX. D. Wang, Z. Zhang, S. Feng, W. G. Halfond, and T. Yu. An empirical study on leveraging images in automated bug report reproduction. arXiv preprint arXiv:2502.15099, 2025. X. Wang, P. Gao, X. Meng, C. Peng, R. Hu, Y. Lin, and C. Gao. Aegis: An agent-based framework for general bug reproduction from issue descriptions. arXiv preprint arXiv:2411.18015, 2024a. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, H. H. Tran, F. Li, R. Ma, M. Zheng, B. Qian, Y. Shao, N. Muennighoff, Y. Zhang, B. Hui, J. Lin, R. Brennan, H. Peng, H. Ji, and G. Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024b. URL https://arxiv.org/abs/2407.16741. Z. Wang, S. Zhou, D. Fried, and G. Neubig. Execution-based evaluation for open-domain code generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12711290, 2023. C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. SWEagent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. J. Yang, C. E. Jimenez, A. L. Zhang, K. Lieret, J. Yang, X. Wu, O. Press, N. Muennighoff, G. Synnaeve, K. R. Narasimhan, D. Yang, S. I. Wang, and O. Press. SWE-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=riTiq3i21b. H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y. Ma, G. Liang, Y. Li, Q. Wang, and T. Xie. Codereval: benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pages 112, 2024. A. Zala, J. Cho, H. Lin, J. Yoon, and M. Bansal. Envgen: Generating and adapting environments via llms for training embodied agents, 2024. URL https://arxiv.org/abs/2403.12014. D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y. Wang, W. Chen, and J. Lou. CERT: continual pre-training on sketches for library-oriented code generation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, pages 23692375, 2022. 31 D. Zan, B. Chen, F. Zhang, D. Lu, B. Wu, B. Guan, W. Yongji, and J.-G. Lou. Large language models meet nl2code: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74437464, 2023. D. Zan, A. Yu, W. Liu, D. Chen, B. Shen, W. Li, Y. Yao, Y. Gong, X. Chen, B. Guan, et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. arXiv preprint arXiv:2403.16443, 2024. F. Zhang, B. Chen, Y. Zhang, J. Keung, J. Liu, D. Zan, Y. Mao, J.-G. Lou, and W. Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 24712484, 2023. L. Zhang, D. Zan, Q. Yang, Z. Huang, D. Chen, B. Shen, T. Liu, Y. Gong, P. Huang, X. Lu, G. Liang, L. Cui, and Q. Wang. Codev: Issue resolving with visual data, 2024. URL https: //arxiv.org/abs/2412.17315. Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li, T. Su, Z. Yang, and J. Tang. Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 56735684, 2023a. Z. Zheng, K. Ning, Y. Wang, J. Zhang, D. Zheng, M. Ye, and J. Chen. survey of large language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372, 2023b."
        }
    ],
    "affiliations": [
        "bytedance.com"
    ]
}