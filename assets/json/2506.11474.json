{
    "paper_title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards",
    "authors": [
        "Jaehoon Yun",
        "Jiwoong Sohn",
        "Jungwoo Park",
        "Hyunjae Kim",
        "Xiangru Tang",
        "Yanjun Shao",
        "Yonghoe Koo",
        "Minhyeok Ko",
        "Qingyu Chen",
        "Mark Gerstein",
        "Michael Moor",
        "Jaewoo Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 4 7 4 1 1 . 6 0 5 2 : r Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards Jaehoon Yun1,4,5, Jiwoong Sohn1,2*, Jungwoo Park1,4, Hyunjae Kim3, Xiangru Tang3, Yanjun Shao3, Yonghoe Koo6, Minhyeok Ko5, Qingyu Chen3, Mark Gerstein3, Michael Moor 2, Jaewoo Kang1,4 1 Korea University, 2 ETH Zürich, 3 Yale University, 4 AIGEN Sciences, 5 Hanyang University College of Medicine, 6 University of Ulsan College of Medicine"
        },
        {
            "title": "Abstract",
            "content": "Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in plug-and-play fashion with strong policy models such as Meerkat, achieving over 80% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at Med-PRM.github.io."
        },
        {
            "title": "Introduction",
            "content": "Clinical decision making (CDM) is complex, multi-step process involving the assessment of patient symptoms, retrieval of relevant clinical evidence, and formulation of diagnostic and treatment strategies. Unlike simple factual recall, CDM requires integrating diverse clinical findings and dynamically refining hypotheses as new information becomes available. Effective CDM entails not only selecting the most probable differential diagnoses but also determining what additional information * Equal contribution. Corresponding authors. is needed to reduce uncertainty and guide the next best diagnostic and therapeutic steps in patients clinical trajectory (Moor et al., 2023a). While CDM spans broad sequence of clinical decisions, core subcomponent is the step-by-step reasoning that underlies each decision point. This sequential structure makes CDM reasoning wellsuited to process reward modeling (PRM) (Lightman et al., 2023; Uesato et al., 2022; Setlur et al., 2024), which evaluates and rewards intermediate steps of process rather than solely its final outcome. In medical practice, sound intermediate reasoning is critical to ensuring safety, reliability, and adherence to the standard of care. This creates strong clinical motivation for models that support stepwise verification and feedback. Recent advances in large language models (LLMs) have substantially improved performance in medical applications through pre-training (Chen et al., 2023; Moor et al., 2023b), post-training (Kim et al., 2025), retrieval (Zakka et al., 2024; Jeong et al., 2024; Sohn et al., 2025), tool augmentation, and agentic systems (Tang et al., 2024; Schmidgall et al., 2024). More recently, emerging reasoning models (OpenAI, 2024; DeepSeek-AI et al., 2025) have demonstrated the ability to decompose complex tasks into interpretable steps and exhibit metacognitive skills such as planning and error correction. However, such abilities are underexplored in clinical domains, where transparency, robustness, and alignment with medical standards are critical for delivering high-quality care. Despite PRMs potential for medical reasoning, its application to the medical domain poses key challenges. Chief among these is the need for high-quality, step-level supervision, which is both expensive and labor-intensive to obtain. Early studies (Lightman et al., 2023) relied on human annotation, which is not scalable (Setlur et al., 2024). More recent works employed automatic labeling strategies such as Monte Carlo Tree Search (MCTS) (Wang et al., 2024b), which estimate the quality of reasoning step quality based on the probability of reaching the correct final answer from that step. Notably, MedS3 (Jiang et al., 2025), domain-specific PRM for clinical QA, also adopts an MCTS-based approach. However, these strategies often undervalue early reasoning steps that are logically sound but fail to lead to the correct outcome. This limitation is especially problematic as penalizing valid early steps can distort the learning signal and ultimately hinder the models ability to evaluate intermediate reasoning accurately. Second, medical reasoning requires extensive domain knowledge that may not be fully captured within language models parameters alone. Thus, it necessitates robust method to incorporate medical knowledge to generate factual, evidence-based outcomes and prevent hallucinations. In particular, training reward models solely on labels without medical context is insufficient for learning the rationale behind those labels. To overcome this, it is essential to provide relevant medical information, such as clinical guidelines, during training, enabling more accurate interpretation of stepwise reward signals grounded in medical reasoning. To address these challenges, we propose MedPRM, retrieval-augmented process reward modeling framework for clinical reasoning. Our method employs RAG-AS-A-JUDGE approach to perform stepwise evaluation conditioned on both the clinical question and retrieved medical documents. This retrieval-augmented evaluation aligns more closely with expert physician annotations than sampling-based auto-labeling methods used during training. By incorporating relevant clinical knowledge at both the training and inference stages, MedPRM enables more accurate assessment of intermediate reasoning and outperforms existing PRM baselines by an average of 3.44% across seven medical benchmarks. Our experiments demonstrate that test-time scaling, where Med-PRM is used as verifier alongside fine-tuned policy model, achieves state-of-the-art performance. Notably, Med-PRM exhibits strong plug-and-play generality: when applied to topperforming models such as UltraMedical (Zhang et al., 2024b), which is trained on data costing approximately $20,000, our reward model, trained on curated dataset costing less than $20, further achieves superior performance, highlighting the cost-efficiency and scalability of our approach. Our contributions are as follows: 1. We propose Med-PRM, retrieval-augmented process reward modeling framework that evaluates each reasoning step in the context of both the clinical question and retrieved evidence, enabling fine-grained and evidencegrounded assessment. 2. We demonstrate that Med-PRM achieves stateof-the-art performance across six out of seven medical QA benchmarks, outperforming all baseline language, reasoning, and medical models. Our verifier improves base model performance by up to 13.50% at test time and reaches 80.35% on MedQA (4 options) using only 8B-parameter models. 3. Through in-depth qualitative analysis and collaboration with medical experts, we show that Med-PRM closely aligns with clinical experts, addressing key limitations of prior training methods of PRMs in both logical consistency and factual accuracy."
        },
        {
            "title": "2 Related Work",
            "content": "For more detailed overview of related work, refer to Appendix E. LLMs have shown increasing proficiency in medical reasoning, effectively handling domain-specific terminology and multimodal data. Using corpora like PubMed, MIMIC-III/IV (Johnson et al., 2023), and UMLS, recent models go beyond surface-level recall to complex inference. Med-PaLM (Singhal et al., 2023a) demonstrates promising performance on expert-level medical QA benchmarks, while methods such as CoT prompting (Wei et al., 2023; Xu et al., 2024; Kim et al., 2025), agentic frameworks (Kim et al., 2024; Tang et al., 2024; Schmidgall et al., 2024), and PRM (Jiang et al., 2025) further enhance reasoning. Reinforcement learning (Wang et al., 2024b) and verifier feedback (Chen et al., 2024) have been applied to refine reasoning traces, emphasizing the need for stepwise supervision. MedS3(Jiang et al., 2025) applies PRM using MCTS-based auto-labeling. Our approach, MedPRM, also leverages process-level rewards but differs by incorporating retrieval-augmented generation and an LLM-as-a-Judge framework. As shown in Section 6 and Section 7, this yields superior performance compared to MCTS-based methods. Hao et al. (2024) explore the use of LLMs as verifiers for CoT reasoning. Med-PRM adopts RAGAS-A-JUDGE for stepwise supervision, diverging from earlier PRM approaches that rely on automatic scoring. In mathematics, RAG-PRM (Zhu et al., 2025) has been proposed to retrieve similar QA pairs for few-shot prompting with PRM. In contrast, Med-PRM retrieves medical knowledge and evidence, enabling integration of diverse sources like textbooks or clinical guidelines."
        },
        {
            "title": "3.1 Reward Model",
            "content": "Reward models have emerged as central to advancing LLMs beyond pre-training and fine-tuning, driven by two key developments in reinforcement learning (RL) and test-time compute scaling. RL methods such as Proximal Policy Optimization (PPO) rely on reward functions to optimize model behavior in settings where ground-truth supervision is sparse or costly. Additionally, test-time strategies like best-of-N (Lightman et al., 2023) have proven effective as alternatives to majority voting, such as self-consistency (Wang et al., 2023), using reward models to rank and select high-quality outputs. These trends highlight the growing importance of accurate, context-aware reward models not only during training but also at inference time. Outcome Reward Model (ORM) Given question and model-generated reasoning trace the ORM assigns sigmoid score rS [0, 1] indicating the correctness of the entire trace. ORM is trained with the following cross-entropy loss: LORM = (yS log rS + (1 yS) log(1 rS)) , where yS is the gold label of the reasoning trace S, yS = 1 if is correct, and yS = 0 otherwise. Process Reward Model (PRM) Given reasoning trace = (s1, s2, , sK) where is the number of reasoning steps, PRM assigns score rsi [0, 1] for each step si. Gold labels ysi {0, 1} indicate whether each step is correct. To compute these scores, the model predicts logits for the special tokens + (correct) and - (incorrect) appended to each reasoning step. The confidence score rsi is defined as the softmax probability of the + token over the logits of both tokens. The model is trained to minimize the cross-entropy loss over all reasoning steps: PRM takes as input the concatenation of the question and the reasoning trace S, and produces stepwise confidence scores as follows: (rs1, rs2, , rsK ) And the minimum step score is defined as the score of the solution S: RM(q, S) = rS, where rS = min(rs1, rs2, , rsK ) PRM Auto-Labeling Wang et al. (2024b) proposed an auto-labeling method to address the cost of human annotations ysi. Inspired by MCTS, completer model generates subsequent reasoning processes from each partial trace up to step si, producing sequences of the form(cid:8)(si+1,j, , sKj ,j, aj)(cid:9)N j=1, where aj is the final answer of the j-th continuation. Let denote the gold answer to the question q. hard label is assigned to step si as follows: yHE si = (cid:40) 1 0 if such that aj = a, otherwise. soft label is computed as the empirical probability of reaching the correct answer: ySE si ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 I(aj = a) This auto-labeling approach enables scalable PRM training without human supervision. However, it is prone to false negativesparticularly in complex questionswhere factually correct and logically coherent intermediate steps are mislabeled as incorrect (or assigned low soft-label scores) simply because none of their sampled continuations lead to the correct final answer. To mitigate false negatives, we incorporate retrievalbased fact-checking into the labeling process, as described in Section 4."
        },
        {
            "title": "4.1 RAG-AS-A-JUDGE",
            "content": "We use RAG-AS-A-JUDGE with labeling strategy that differs from conventional PRMs. Given question q, golden answer a, set of retrieved documents D, and sequence of reasoning steps = (s1, . . . , sK), RAG-AS-A-JUDGE performs binary classification on each step si to determine whether it is correct, producing labels yRAG {0, 1}: si LPRM = (cid:80)K i=1 (ysi log rsi + (1 ysi) log(1 rsi)) RAG-AS-A-JUDGE(D, q, a, S) = yRAG Figure 1: Overview of the MED-PRM. (1) An LLM generates reasoning traces for medical questions, and relevant documents are retrieved from external corpora. (2) large LLM assigns stepwise labels (+/) for each reasoning step. (3) These labeled traces are used to train the Med-PRM reward model. (4) Med-PRM reward model is used for inference-time evaluation or further train the policy model. These labels yRAG are plugged in LPRM in order to train our PRM. Each score rsi reflects the likelihood that step si is correct, following Lightman et al. (2023). During both training and evaluation, PRM receives the same input: question q, retrieved documents D, and reasoning trace S. The key difference from prior PRM models is the inclusion of the retrieved documents as part of the input. RM(D, q, S) = rS To construct the retrieval query, we concatenate the question with the reasoning trace: = Retriever(q, S) During training, the correct reasoning trace among multiple inferences is used as the query. During inference, randomly sampled reasoning trace is used as the query instead. Further implementation details of document retrieval are provided in Appendix B. = {S1, S2, , SN } be the set of reasoning traces and {aS1, aS2, , aSN } the corresponding answers. Then, we have arm = aS, where = arg max Sj rSj , and where rSj is final score of reasoning trace Sj We also adopt hybrid method following Li et al. (2023) that combines self-consistency and reward scoring, noted as SC+RM (Self-Consistency + Reward Model). Traces are grouped by their final answers, and the answer with the highest total reward is selected: asc+rm = arg max (cid:88) j= I(aSj = a) RM(q, Sj), where RM(q, Sj) is the reward score of the j-th reasoning trace assigned by PRM for question q. strong reward model improves selection by assigning higher scores to correct reasoning traces. Scaling Test-time Computation Following Lightman et al. (2023), we define reasoning traces final score as the minimum reward across its steps. The best-of-N approach selects an answer among the traces with the highest final score. Let"
        },
        {
            "title": "4.2 Policy Model Fine-tuning",
            "content": "We fine-tune the policy model using rejection sampling guided by Med-PRM. For each question in the training set, multiple reasoning traces are generated, and Med-PRM assigns stepwise reward scores. Traces are ranked by their minimum step score, and only top-ranked traces are retained for supervised fine-tuning. Following Qwen et al. (2025), we exclude questions consistently answered correctly to concentrate training on more challenging examples. After finetuning, Med-PRM is again used at inference time to rescore multiple generations and select the best one. This bootstraps the policy model to produce reasoning paths aligned with Med-PRM, improving performance on complex medical QA tasks."
        },
        {
            "title": "5.1 Training of Med-PRM",
            "content": "Model We perform full fine-tuning of the Llama3.1-8B-Instruct model on single NVIDIA A100 (80GB VRAM) with maximum sequence length of 4096 tokens. We use the AdamW optimizer with learning rate of 2 106, cosine decay, and 5% warmup ratio. Training is performed in bfloat16 precision with gradient checkpointing and Flash Attention V2 with gradient accumulation to global batch size of 64. We designate the EOS token as the padding token and introduce special marker to segment reasoning steps for process-level supervision. More details on the hyperparameters used are described in Appendix C. Data Filtering and Labeling Training uses MedQA (Jin et al., 2020), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), and MMLU (Hendrycks et al., 2021). We use the full MedQA training set (10,178 questions) and sample 500 instances from each of the remaining datasets. For each question, we sample 16 candidate reasoning traces and filter out traces with fewer than three or more than nine reasoning steps to avoid overly shallow or degenerate reasoning. To maintain label balance, the number of correct reasoning traces per question was limited to no more than the number of incorrect traces or two, whichever is greater. To ensure the total number of tokens to not exceed 4096, 1024 tokens were reserved for the question and reasoning, and the remaining 3072 tokens were used to sequentially include relevant documents. Retrieved documents (truncated to 3072 tokens) are prepended to the question and reasoning trace with each step separated using special token to form the input. stepwise binary supervision is applied at each marker using labels from RAG-AS-A-JUDGE."
        },
        {
            "title": "5.2 Evaluation of Med-PRM",
            "content": "Benchmarks We evaluate Med-PRM on MedQA (4 and 5 options), MedMCQA (validation set), six medical MMLU subsets (Singhal et al., 2023a), DDXPlus (Tchango et al., 2022), and two openended AgentClinic variants (Schmidgall et al., 2024) based on NEJM and MedQA. AgentClinic adopts an open-ended format and is evaluated using Gemini-2.0-flash. Detailed descriptions of benchmarks are in Appendix F. Baselines We benchmark Med-PRM against proprietary and open-source models, including general-purpose, reasoning, medical, and process reward models. Table 1 summarizes performance across both multiple-choice and open-ended question answering benchmarks widely used in the medical domain. We further compare our method against internal baselines in the ablation study: (1) PRMsoft and (2) PRMhard, both using MCTS-style autolabeling (Wang et al., 2024b), and (3) Med-PRM without retrieval. Our final method, (4) Med-PRM with retrieval, is also included for comparison. Each baseline is evaluated under two strategies: Best-of-N and SC+RM. Scaling Test-Time Computation We gradually increase the number of reasoning traces generated by the policy model up to = 64 per question. The final answer is selected using two strategies: Best-of-N, which chooses the trace with the highest rSj score, and SC+RM, which selects the answer group with the highest sum total reward score."
        },
        {
            "title": "6.1 Main Results",
            "content": "Table 1 summarizes the performance of Med-PRM compared to several baselines across seven medical benchmarks. We evaluate Med-PRM using two test-time strategies: Best-of-N and SC+RM, achieving average accuracy of 72.59% and 73.50%, respectively. These results outperform all existing open-source language, reasoning, medical, and medical process reward models with fewer than 10 billion parameters. With SC+RM, Med-PRM achieves state-of-the-art results on 4 out of 7 benchmarks and on 2 out of 7 benchmarks under the Best-of-N setting. We observe larger performance improvements on benchmarks requiring complex clinical reasoning compared to knowledge-centric tasks such as"
        },
        {
            "title": "Size",
            "content": "MedQA-4 MedQA-5 MedMCQA MMLU-Med DDXPlus Agent Clinic NEJM * Agent Clinic MedQA *"
        },
        {
            "title": "Average",
            "content": "Multiple-Choice QA Open-Ended QA Proprietary Language Models Gemini Flash 2.0 GPT-4o-mini GPT-3.5 turbo Proprietary Reasoning Models o4-mini o3-mini Open-source Language Models Llama3.1 Gemma 2 Ministral Open-source Reasoning Models Open-source Medical Models Open-source Medical Process Reward Models DeepSeek-R1 QwQ Sky-T1-Preview R1-Distill-Llama R1-Distill-Qwen Sky-T1 Marco-o TX-Gemma Meditron3 Meerkat UltraMedical HuatuoGPT-o1 MedS3 Best-of-N SC+RM Med-PRM Best-of-N SC+RM 8B 9B 8B 671B 32B 32B 8B 7B 7B 7B 9B 8B 8B 8B 8B 8B 8B 8B 8B 87.51 79.03 69.91 93.95 92.69 70.93 64.73 56.17 90.34 85.31 77.77 34.96 24.82 34.09 39.36 41.56 59.94 71.25 72.66 72.19 71.56 75. 76.76 79.18 85.23 74.31 65.44 91.12 90.97 65.20 60.25 50.43 89.87 81.62 73.53 30.16 19.56 30.64 34.80 35.11 52.95 69.13 68.34 63. 68.42 71.41 72.43 75.49 72.60 68.20 57.00 79.60 75.50 61.60 53.00 49.20 78.80 70.20 66.20 43.60 36.40 36.20 49. 36.00 48.20 56.40 62.60 63.60 64.20 64.20 64.20 67.40 92.01 87.79 76.77 93.99 93.01 78.97 77.87 67. 94.40 88.89 88.34 64.19 47.47 53.17 69.15 52.34 67.86 76.40 79.61 75.30 80.18 81.79 82.37 83.29 75.00 76.00 73.80 79.80 79. 68.80 64.40 51.80 79.60 74.00 74.00 36.80 36.80 47.40 38.40 57.80 67.80 70.00 72.60 64.00 75.40 74.60 77.80 77.20 70.83 58.33 51. 76.67 78.33 35.83 41.67 34.17 79.83 63.33 53.33 30.83 8.33 6.67 30.83 20.00 42.50 43.33 45.83 40.00 52.50 55.00 54.17 52. 87.74 79.44 77.93 94.86 96.26 71.96 66.36 62.62 91.12 85.51 81.31 57.01 35.51 29.91 63.55 50.00 67.29 76.40 70.56 71.50 74.30 74. 80.37 79.44 81.56 74.73 67.51 87.14 86.65 64.76 61.18 53.09 86.28 78.41 73.50 42.51 29.84 34.01 46.47 41.83 58.08 66.13 67.46 64. 69.51 71.06 72.59 73.50 Table 1: Accuracy of proprietary and open-source models across multiple-choice and open-ended medical QA benchmarks. We use instruction-tuned models for Llama3.1, Gemma2, and Ministral. Best scores are shown in bold, and second-best scores are underlined among small-scale models (< 10B parameters). We report results on AgentClinic*, simplified variants of the original benchmarks (see Appendix for details). MedMCQA. Notably, on the AgentClinic benchmark, which closely mirrors real-world diagnostic workflows, Med-PRM achieves accuracy gains of 12.50% and 10.75% under the SC+RM and Bestof-N settings, respectively. Compared to MedS3, the previous state-of-theart process reward model at the 8B scale, MedPRM achieves an average improvement of 2.44% across all benchmarks using the SC+RM strategy. Even under the Best-of-N strategy, Med-PRM outperforms MedS3 by 3.08%. These results demonstrate the strong capability of Med-PRM in identifying clinically sound reasoning paths. We further explore its effectiveness when paired with stronger, fine-tuned language models such as Meerkat-8B and UltraMedical-8B in Section 6."
        },
        {
            "title": "6.2 Reward Model as Verifier",
            "content": "To assess the model-agnostic utility of Med-PRM, we apply it as plug-and-play verifier during inference across various policy models on MedQA. As shown in Table 2, Med-PRM consistently improves performance, regardless of the underlying base or fine-tuned model."
        },
        {
            "title": "Model",
            "content": "MedQA (4 options) Llama-3.1-8B-Instruct + SC + SC + RM (Med-PRM RM ) + Best-of-N (Med-PRM RM ) Llama-3.1-8B-Instruct* (Med-PRM π) + SC + SC + RM (Med-PRM RM ) + Best-of-N (Med-PRM RM ) UltraMedical-8B + SC + SC + RM (Med-PRM RM ) + Best-of-N (Med-PRM RM ) Meerkat-8B + SC + SC + RM (Med-PRM RM ) + Best-of-N (Med-PRM RM ) 68.79 74.86 (+6.07) 78.24 (+9.45) 76.98 (+8.19) 67.22 75.02(+7.80) 79.18 (+11.96) 76.76 (+9.54) 67.51 75.63 (+8.12) 79.87 (+12.36) 76.42 (+8.91) 66.65 76.04 (+9.39) 80.35 (+13.70) 79.95 (+13.30) Table 2: Performance improvements from using the Med-PRM reward model as verifier on MedQA (4 options). For each policy model, the first row shows the average score over 64 sampled solutions. Subsequent rows apply Self-Consistency (SC), SC with reward model verification (SC+RM), and Best-of-N using the same 64 solutions. To further demonstrate the effectiveness of MedPRM, we train policy model using supervised fine-tuning (SFT) on rejection-sampled dataset constructed with our reward model, following the Entropy-Regularized PRM (Zhang et al., 2024a). (a) Best-of-N (b) SC+RM Figure 2: Comparison of scaling test-time computation performance between Med-PRM and conventionally trained PRMs across overall medical benchmarks. This policy model, denoted as Med-PRM π, leverages high-quality reasoning traces selected by MedPRM and achieves 79.18% accuracy on MedQA with 10.39% improvement over the base Llama3.1-8B-Instruct model alone. Moreover, when Med-PRM is used as verifier on top of strong, fine-tuned models such as Meerkat-8B and UltraMedical-8B, we observe additional gains. Notably, pairing Med-PRM with Meerkat-8B yields 80.35% accuracy on MedQA, marking the first time that an 8B-scale model has surpassed the 80% threshold on this benchmark. These results highlight the generalizability of our reward model as plug-and-play component for enabling more accurate medical reasoning across diverse models. comprehensive table of full benchmark results, including CoT, SC, and PRM baselines (hard label, soft label, MedS3, and MedPRM) is provided in Appendix J."
        },
        {
            "title": "7.1 Ablation Study",
            "content": "We conduct comprehensive ablation study to evaluate the contribution of each component in MedPRM. The results are visualized in Figure 2. We assess performance improvements under test-time scaling using 64 sampled solutions from the Llama3.1-8B-Instruct model, evaluated with both Bestof-N and SC+RM strategies across the benchmark. We compare Med-PRM to Self-Consistency and two PRMs trained with conventional auto-labeling methods: PRMsoft and PRMhard. The results including MedS3 are provided in Appendix G. To analyze the impact of our design, we increFigure 3: Comparison of scaling test-time computation performance between Med-PRM and other PRMs on AgentClinic* under the Best-of-N setting. mentally add key components. First, we replace conventional labeling with using an LLM to directly evaluate the reasoning steps. This is used to train variant named Med-PRM without RAG. Second, we incorporate retrieval for the full MedPRM framework. Results show that each component yields consistent gains across both test-time strategies. Notably, Med-PRM without retrieval already outperforms conventional PRMs, and adding retrieval further boosts performance. This demonstrates the critical role of grounding in external knowledge."
        },
        {
            "title": "Under",
            "content": "the SC+RM setting, conventional PRMs achieve modest improvements over SelfConsistency. However, in the more stringent Bestof-N setting, where only the top solution is selected, conventional PRMs underperform relative to Self-Consistency. In contrast, Med-PRM consistently outperforms Self-Consistency in both settings, underscoring that LLM-based step-level suFigure 4: Case study comparison of labeling strategies for reward model training. This example illustrates how MED-PRM labeling yields more clinically accurate judgments than both rule-based and auto-completed annotations. pervision is more effective than sampling-based auto-labeling. These findings also suggest that Best-of-N serves as more discriminative setting for comparing reward model performance. Subset Med-PRM Soft label Hard label"
        },
        {
            "title": "Easy\nHard",
            "content": "0.74 0.71 0.64 0.34 0.70 0."
        },
        {
            "title": "7.2 Open-Ended Clinical Tasks",
            "content": "We evaluate Med-PRM on AgentClinic* with Bestof-N strategy, diagnostic benchmark designed in an open-ended QA format to closely resemble real-world clinical settings (Figure 3). Although this dataset was not included in the training phase, Med-PRM achieves an 11.81% improvement in accuracy through scaling test-time computation. It outperforms other baseline methods by significant margin, achieving 4.87% higher accuracy than SelfConsistency and 4.32% higher than PRMs trained with conventional approaches."
        },
        {
            "title": "7.3 Expert Alignment",
            "content": "To assess the alignment between Med-PRM and medical experts, we calculate the Pearson correlation between model-generated labels and expert annotations on step-level reasoning quality. We select three questions each from an easy subset (where Llama-3.1-8B-Instruct achieves over 10% accuracy) and hard subset (accuracy below 10%) from the PRM training set. For each question, five model-generated reasoning traces were annotated by human experts, resulting in 180 step-level annotations in total. As shown in Table 3, Med-PRM shows high correlation with human judgments across both easy and hard subsets (0.74 and 0.71, respectively). In Table 3: Pearson correlation between model-generated labels and human annotations on reasoning steps for easy and hard subsets. contrast, the performance of soft and hard labeling strategies drops significantly on hard examples, dropping from 0.64 to 0.34 and 0.70 to 0.31, respectively. This suggests that Med-PRM produces more robust and consistent labels even in more challenging reasoning scenarios, making it better suited for constructing high-quality training sets."
        },
        {
            "title": "7.4 Case Study",
            "content": "Training Data Labeling Figure 4 presents an example from the MedQA training dataset concerning patient suspected of having Graves ophthalmopathy. Although diplopia and ocular pain are commonly associated with Graves disease, they result from autoimmune orbitopathy rather than sympathetic overactivity. Step 1 and Step 2 of the policy models reasoning appropriately integrate the patients symptoms to suspect thyroid-related exophthalmos, demonstrating sound medical reasoning. However, in Step 5, the model incorrectly attributes ocular symptoms to sympathetic overactivity, ultimately leading to the selection of an incorrect final answer."
        },
        {
            "title": "The retrieved document clarifies that diplopia",
            "content": "Figure 5: Case study comparison of step-wise reward assignment between conventional PRM and Med-PRM. Med-PRM assigns more appropriate and clinically grounded rewards than conventional PRM by leveraging evidence from retrieved documents, enabling more accurate evaluation of intermediate reasoning steps. and conjunctival injection are characteristic only of autoimmune orbitopathy in Graves disease. This distinction is key to differentiating the incorrect choice (E) from the correct one (C). Leveraging this evidence, Med-PRM assigns step-level labels of 1 to Step 1 and Step 2, and 0 to Step 5 and Step 6, aligning exactly with expert annotations. In contrast, the ORM data labeling pipeline assigns an overall incorrect label to the entire reasoning trace, as it only considers the final answer, missing the valid reasoning steps in Steps 1 and 2. Similarly, the conventional PRM data labeling pipeline assigns low scores to Steps 1 and 2, incorrectly marking them as poor reasoning, since the final answer derived from these steps is incorrect. As result, both ORM and PRM fail to correctly evaluate the quality of intermediate reasoning steps, mislabeling valid reasoning as low quality. Test-Time Labeling Figure 5 presents an example from the MedQA dataset concerning patient suspected of polyhydramnios. Duodenal atresia can cause polyhydramnios, while posterior urethral valve (PUV) typically leads to oligohydramnios. Steps 1 to 4 of the reasoning trace appropriately integrate the patients clinical findings to analyze the polyhydramnios context. However, in Step 5, the model incorrectly reasons that posterior uretheral valve can cause polyhydramnios, ultimately leading to an incorrect answer in Step 6. The retrieved document explicitly states that PUV leads to oligohydramnios due to urinary outflow obstruction. This serves as clear rationale for why the incorrect answer choice (D) is invalid. Conventional PRM models assign relatively high rewards to Step 5 and Step 6 in the absence of helpful external documents. In contrast, Med-PRM uses the retrieved document to detect the error more precisely, providing high rewards up to Step 4 and then sharply reducing rewards from Step 5 onward. These examples show that Med-PRM can identify and localize errors in medical reasoning steps more effectively than auto-labeling trained PRM, highlighting the value of document-grounded, stepwise evaluation in capturing nuanced reasoning quality by referring to relevant clinical documents."
        },
        {
            "title": "8 Conclusion",
            "content": "In this study, we present Med-PRM, process reward modeling framework that advances medical reasoning in language models with RAG-AS-AJUDGE approach. Med-PRM addresses key limitations of existing PRMs by verifying intermediate reasoning steps against retrieved medical documents rather than relying on outcome-based autolabeling. Our experiments show that Med-PRM consistently outperforms prior methods across both multiple-choice and open-ended clinical benchmarks, with notable gains in complex diagnostic tasks. Furthermore, the retrieval-based step verification produces scalable, expert-aligned labels that support effective policy training and verifier in test-time scaling. Together, these contributions establish Med-PRM as robust and generalizable approach for improving the accuracy, transparency, and trustworthiness of medical AI systems."
        },
        {
            "title": "Limitations",
            "content": "Our work has few limitations that warrant discussion. First, our evaluation is currently confined to the medical domain, though the methodology could potentially generalize to other domains requiring stepwise reasoning verification and retrieval-augmented generation. Second, due to computational constraints, we limited our experiments to small language models like Llama 3.1 8B and Meerkat 8B, though there remains significant potential to explore scalability across different model architectures and sizes Third, while our reward model demonstrates strong performance, we did not extensively explore diverse reinforcement learning methods that could further enhance our methods capabilities. Future work should investigate these aspects through broader domain coverage, model scaling experiments, and more sophisticated reinforcement learning training strategies."
        },
        {
            "title": "References",
            "content": "Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. 2024. HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs. arXiv preprint. ArXiv:2412.18925 [cs]. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. 2023. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint. ArXiv:2311.16079 [cs]. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint. ArXiv:2501.12948 [cs]. Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, and Zhiting Hu. 2024. LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models. arXiv preprint. ArXiv:2404.05221 [cs]. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. arXiv preprint. ArXiv:2009.03300 [cs]. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and JaeImproving medical reasoning woo Kang. 2024. through retrieval and self-reflection with retrievalaugmented large language models. Bioinformatics, 40(Supplement_1):i119i129. Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, and Yu Wang. 2025. MedS$^3$: Towards Medical Small Language Models with arXiv preprint. Self-Evolved Slow Thinking. ArXiv:2501.12051 [cs] version: 2. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What Disease does this Patient Have? Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv preprint. ArXiv:2009.13081 [cs]. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567 2577, Hong Kong, China. Association for Computational Linguistics. Qiao Jin, Won Kim, Qingyu Chen, Donald Comeau, Lana Yeganova, John Wilbur, and Zhiyong Lu. 2023. MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zeroshot biomedical information retrieval. Bioinformatics (Oxford, England), 39(11):btad651. Publisher: Oxford University Press. Alistair E. W. Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J. Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, Liwei H. Lehman, Leo A. Celi, and Roger G. Mark. 2023. MIMIC-IV, freely accessible electronic health record dataset. Scientific Data, 10(1):1. Publisher: Nature Publishing Group. Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Jungwoo Park, Olga Reykhart, Thomas Fetherston, Donghee Choi, Soo Heon Kwak, Qingyu Chen, and Jaewoo Kang. 2025. Small language models learn enhanced reasoning skills from medical textbooks. npj Digital Medicine, 8(1):110. Publisher: Nature Publishing Group. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, and Hae Won Park. 2024. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making. arXiv preprint. ArXiv:2404.15155 [cs]. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025. LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! arXiv preprint. ArXiv:2502.07374 [cs]. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making Language Models Better Reasoners with Step-Aware Verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets Verify Step by Step. arXiv preprint. ArXiv:2305.20050 [cs]. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M. Krumholz, Jure Leskovec, Eric J. Topol, and Pranav Rajpurkar. 2023a. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. 2023b. Med-Flamingo: Multimodal Medical Few-shot Learner. In Proceedings of the 3rd Machine Learning for Health Symposium, pages 353367. PMLR. ISSN: 2640-3498. OpenAI. 2024. Introducing OpenAI o1. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA : Largescale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. arXiv preprint. ArXiv:2203.14371 [cs]. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv preprint. ArXiv:2412.15115 [cs]. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. 2024. AgentClinic: multimodal agent benchmark to evaluate AI in simulated clinical environments. arXiv preprint. ArXiv:2405.07960 [cs]. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. Rewarding Progress: Scaling Automated Process arXiv preprint. Verifiers for LLM Reasoning. ArXiv:2410.08146 [cs]. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, and others. 2023a. Large language models encode clinical knowledge. Nature, 620(7972):172180. Publisher: Nature Publishing Group. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023b. Towards Expert-Level Medical Question Answering with Large Language Models. arXiv preprint. ArXiv:2305.09617 [cs]. on Management of Data, SIGMOD 21, pages 2614 2627, New York, NY, USA. Association for Computing Machinery. Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang, Mujeen Sung, Hyunjae Kim, and Jaewoo Kang. 2025. Rationale-Guided Retrieval Augmented Generation for Medical Question Answering. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1273912753, Albuquerque, New Mexico. Association for Computational Linguistics. Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, and Mark Gerstein. 2025. MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning. arXiv preprint. ArXiv:2503.07459 [cs]. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2024. MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning. arXiv preprint. ArXiv:2311.10537 [cs]. Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. 2022. DDXPlus: New Dataset For Automatic Medical Diagnosis. arXiv preprint. ArXiv:2205.09148 [cs]. Qwen Team. 2024. QwQ: Reflect Deeply on the Boundaries of the Unknown. Section: blog. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. arXiv preprint. ArXiv:2211.14275 [cs]. Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, and Shekoofeh Azizi. 2025. TxGemma: Efficient and Agentic LLMs for Therapeutics. arXiv preprint. ArXiv:2504.06196 [cs]. Guanchu Wang, Junhao Ran, Ruixiang Tang, Chia-Yuan Chang, Chia-Yuan Chang, Yu-Neng Chuang, Zirui Liu, Vladimir Braverman, Zhandong Liu, and Xia Hu. 2024a. Assessing and Enhancing Large Language Models in Rare Disease Question-answering. arXiv preprint. ArXiv:2408.08422 [cs]. Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. 2021. Milvus: Purpose-Built Vector Data Management System. In Proceedings of the 2021 International Conference Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024b. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv preprint. ArXiv:2203.11171 [cs]. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint. ArXiv:2201.11903 [cs]. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking RetrievalAugmented Generation for Medicine. arXiv preprint. ArXiv:2402.13178 [cs]. Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K. Dey, and Dakuo Wang. 2024. Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1):132. ArXiv:2307.14385 [cs]. Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex Dalal, Jennifer Kim, Michael Moor, Robyn Fong, Curran Phillips, Kevin Alexander, Euan Ashley, and others. 2024. Almanacretrieval-augmented language models for clinical medicine. NEJM AI, 1(2):AIoa2300068. Publisher: Massachusetts Medical Society. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. 2024a. EntropyRegularized Process Reward Model. arXiv preprint. ArXiv:2412.11006 [cs]. Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou. 2024b. UltraMedical: Building Specialized Generalists in Biomedicine. arXiv preprint. ArXiv:2406.03949 [cs]. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions. arXiv preprint. ArXiv:2411.14405 [cs]. Qinyue Zheng, Salman Abdullah, Sam Rawal, Cyril Zakka, Sophie Ostmeier, Maximilian Purk, Eduardo Reis, Eric J. Topol, Jure Leskovec, and Michael Moor. 2025. MIRIAD: Augmenting LLMs with millions of medical query-response pairs. arXiv preprint. ArXiv:2506.06091 [cs]. Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, and Weinan Zhang. 2025. Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning. arXiv preprint. ArXiv:2502.14361 [cs]."
        },
        {
            "title": "Description",
            "content": "q Kj C Sj si si,j ySj ysi ySE si yHE si yRAG si ysi,j rSj rsi rsi,j aSj aRM RM (D, q, Sj) HE SE SC medical question retrieved documents number of reasoning steps number of reasoning steps in trace Sj step index (1, ... , K) number of reasoning traces trace index (1, ... , ) set of reasoning traces reasoning trace reasoning trace step of reasoning trace step of reasoning trace Sj gold label for trace gold label for step si soft label for step si hard label for step si label made by RAG-AS-A-JUDGE for step si gold label for step si,j of trace reward (sigmoid) score for trace reward (sigmoid) score for step si reward (sigmoid) score for step si,j of trace gold answer of decoded answer of trace answer selected by reward model reward score of trace Hard Label Soft Label Self Consistency Table 4: Summary of notations Table 4 summarizes the notations used throughout the manuscript for the convenience of readers based on Wang et al. (2024b)"
        },
        {
            "title": "B Retrieval",
            "content": "We use MedCPT (Jin et al., 2023) bi-encoder and cross-encoder for dense retrieval and reranking from four biomedical corpora: Clinical Guidelines (Chen et al., 2023) StatPearls (Xiong et al., 2024) Medical Textbooks (Singhal et al., 2023b) Rare Disease Corpus (Wang et al., 2024a) Retrieval is performed on AWS EC2 c5.9xlarge with Milvus (Wang et al., 2021) using Max Inner Product Search (MIPS); reranking uses an NVIDIA RTX 3090. For each query, 100 documents per corpus (400 total) are retrieved, with top 32 selected after reranking, following (Sohn et al., 2025)."
        },
        {
            "title": "Policy",
            "content": "1e-5 2e-6 cosine cosine 0.05 0.05 64 64 1 3 4096 4096 bfloat16 bfloat16 AdamW AdamW Table 5: Hyperparameters used for training Med-PRM Table 5 shows the hyperparameters used for training our models of Med-PRM framework"
        },
        {
            "title": "D API Usage and Cost",
            "content": "We utilized the Gemini-2.0-flash model via the Google Generative AI API. The model was employed as RAG-AS-A-JUDGE, specifically for generating training labels for the PRM and for scoring responses in the AgentClinic benchmark. All API calls were made using the official endpoint, with the temperature set to 0 and standard rate limits applied. The total API cost incurred for curating the training set of PRM was approximately $20."
        },
        {
            "title": "E Related Works",
            "content": "Medical Reasoning Large language models (LLMs) have shown growing competence in medical reasoning, which presents unique challenges beyond general language tasks, including specialized terminology, multimodal patient data, and high demands for factual accuracy. Leveraging biomedical corpora such as PubMed, MIMIC-III/IV (Johnson et al., 2023), and UMLS, recent models have progressed from surface-level recall to more complex diagnostic and therapeutic inference. Pioneered by Med-PaLM (Singhal et al., 2023a), demonstrates strong performance on expert-level medical questions. Advances in Chain-of-Thought (CoT) prompting and training (Wei et al., 2023; Xu et al., 2024; Kim et al., 2025), Retrieval-Augmented Generation (Zakka et al., 2024; Sohn et al., 2025; Zheng et al., 2025), agentic systems (Kim et al., 2024; Tang et al., 2024; Schmidgall et al., 2024; Tang et al., 2025) further extend this paradigm by orchestrating multiple specialized agents to collaboratively solve complex clinical tasks. PRM (Lightman et al., 2023; Jiang et al., 2025), and Reinforcement Learning (Wang et al., 2024b) have enabled LLMs to generate structured reasoning traces. Similarly, HuatuoGPT-o1 (Chen et al., 2024) incorporates verifier feedback to iteratively refine multi-step reasoning traces. These trends highlight the growing importance of process reward mechanisms. Like mathematical problem solving, clinical reasoning often requires multi-step inference, where single incorrect step can invalidate the final outcome. Thus, stepwise supervision is not only applicable to medicine but also critical for ensuring transparency and reliability in clinical decision-making. PRM in Medical Domain recent study has been made to apply PRM in the medical domain. MedS3 (Jiang et al., 2025) constructs PRM training dataset with similar approaches to existing PRM frameworks, using MCTS-based autolabeling. Med-PRM also provides process rewards for medical reasoning, but mainly differs in the way of constructing the training set, leveraging retrievalaugmented generation and LLM-as-a-Judge. Section 6 and Section 7 show that our approach is more effective than MCTS-based methods. LLM-as-a-Judge for Reasoning Evaluation Recent research actively explores the use of LLMas-a-Judge as PRM or as Process Explanation Model (PEM). In Hao et al. (2024), LLMs are employed to evaluate the quality of Chain-ofThought (CoT) reasoning. Med-PRM uses RAGAS-A-JUDGE as labeling strategy for reasoning steps, which differs in terms of how it is utilized, and this also contrasts with the automatic labeling approaches used in prior PRM research. Retrieval-Augmented PRM in Mathematics In the mathematical domain, recent work has explored combining retrieval with PRM (Zhu et al., 2025). RAG-PRM retrieves semantically similar sets of questions and answers to enable PRM to generalize against out-of-distribution questions. While both Med-PRM approaches incorporate retrieval, Med-PRM differs in that it retrieves medical evidence and knowledge rather than similar QA pairs for few-shot-style prompting. Moreover, our retrieval method supports scalable integration, ranging from curated sources such as medical textbooks and clinical guidelines to potentially broader corpora like PubMed in other deployments."
        },
        {
            "title": "F Benchmarks",
            "content": "Below are detailed descriptions of the medical benchmark datasets used in our study. MedQA MedQA is comprehensive medical question answering dataset derived from professional medical board examinations. The dataset spans three languages: English, simplified Chinese, and traditional Chinese. Our work focuses exclusively on the English subset, which contains 12,730 questions from the United States Medical Licensing Examination (USMLE). Our method was evaluated on questions with both four and five options. MedQA evaluates diverse aspects of medical knowledge, encompassing diagnostic procedures, treatment protocols, and fundamental medical concepts. The questions are designed to test both factual medical knowledge and clinical reasoning capabilities. MedMCQA MedMCQA (Pal et al., 2022) is an extensive multiple-choice dataset comprising over 194,000 high-quality questions from Indian medical entrance examinations (AIIMS and NEET PG). Our evaluation incorporates 500 questions from this dataset, with an average token length of 12.77 tokens per question. Each question presents four answer choices. The dataset demonstrates remarkable topical diversity, covering 2,400+ healthcare topics across 21 medical subjects. MedMCQAs comprehensive coverage and focus on entrance exam questions make it particularly valuable for assessing both theoretical knowledge and practical clinical reasoning abilities in medical problem-solving scenarios. MMLU (Medical Subset) The Massive Multitask Language Understanding benchmark (Hendrycks et al., 2021) contains specialized medical knowledge subsets that we incorporate into our evaluation. Our benchmark utilizes 1,089 medical-related questions from MMLU. Each question presents four multiple-choice options. The medical subsets encompass diverse domains including anatomy, clinical knowledge, college medicine, medical genetics, and professional medicine. MMLUs comprehensive coverage spans both fundamental and advanced medical concepts, testing knowledge across wide spectrum of difficulty levels from basic to professional expertise. The benchmarks standardized assessment format enables meaningful comparisons between medical reasoning capabilities and other knowledge domains. DDXPlus DDXPlus (Tchango et al., 2022) is large-scale synthetic dataset containing approximately 1.3 million patient cases, designed to advance research in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems. Unlike traditional medical datasets that only include binary symptoms and antecedents, DDXPlus incorporates categorical and multi-choice symptoms, along with hierarchical symptom organization. Each case includes comprehensive information such as differential diagnoses, ground truth pathologies, symptoms, and relevant antecedents. This dataset enables the development of more sophisticated medical reasoning systems that can interact with patients in logical manner and provide differential diagnoses, which is crucial for helping doctors understand the reasoning process of AI systems. AgentClinic* AgentClinic (Schmidgall et al., 2024) is multimodal agent benchmark designed to evaluate large language models (LLMs) in simulated clinical environments. Unlike traditional static question answering benchmarks, AgentClinic captures the complex, sequential nature of clinical decision making by integrating diverse clinical findings derived from patient interactions, multimodal data collection, and tool usage. The benchmark spans nine medical specialties and seven languages, providing comprehensive evaluation framework. Notably, when MedQA problems are presented in AgentClinics sequential decision making format, diagnostic accuracies can drop significantly compared to traditional formats. The benchmark enables novel patient-centric metrics and supports various tools including experiential learning, adaptive retrieval, and reflection cycles. AgentClinics interactive environment allows for in-depth evaluation of clinical reasoning capabilities through real-world electronic health records and clinical reader studies. In this work, we adopt simplified variant of the benchmark, referred to as AgentClinic*, which reformulates the task into single step inference problem. This adaptation is motivated by practical considerations: conducting multiple reasoning steps would incur excessive API calls and computational overhead in large-scale experiments. Moreover, techniques like self-consistency, which are important for evaluating model reliability, are less applicable in multi-turn settings due to nondeterministic agent trajectories. AgentClinic* thus strikes balance between realism and tractability while preserving the core challenge of evidencegrounded clinical reasoning. Training We train our model using four widely adopted medical datasets: MedQA, MedMCQA, PubMedQA, and MMLU-Med. For MedQA, we utilize the entire training set comprising 10,178 questions. For the remaining three datasets, we randomly sampled 500 examples from each training set to construct lightweight yet diverse training corpus, encompassing variety of question formats and clinical topics. This setup ensured data efficiency while enabling the model to learn from broad range of medical problem types. Evaluation Model performance was evaluated on MedQA, MedMCQA, PubMedQA, and six medical-related subsets of MMLU (Anatomy, Clinical Knowledge, College Biology, College Medicine, Medical Genetics, and Professional Medicine). Additionally, we conducted out-ofdomain evaluations that required more complex clinical reasoning. These included DDXPlus and two variants of AgentClinic based on NEJM case reports and MedQA. For DDXPlus, we randomly sampled 500 examples due to its extensive size, and reformulated the task to focus on differential diagnosis by providing supporting evidence and requiring the model to select the correct disease from list of up to five candidates. AgentClinic, in contrast, presented an open-ended question-answering format without predefined answer choices, simulating real-world clinical scenarios through the analysis of diverse clinical findings (multi-turn dialogues were not included). To evaluate responses in open-ended settings, we adopted an LLM-as-ajudge framework (Gemini-2.0-flash), following similar approach to HuatuoGPT-o1 (Chen et al., 2024)."
        },
        {
            "title": "G Full Ablation Study of PRMs vs SC",
            "content": "As shown in Figure 6, Med-PRM outperforms MedS3 when scaling test-time computation. System Message for Open-Ended Question Evaluation Using LLM-AS-A-JUDGE system_message The following presents short-answer question along with its Ground Truth and the Models Answer. Evaluate the Models Answer strictly based on its correctness. Your output must be either 1 or 0 only. Output 1 if the answer is correct, and 0 if it is incorrect. Generating PRM Training Data Labels Using RAG-AS-A-JUDGE system_message You are an evaluator responsible for assessing the quality of **wrong solutions** to medical questions in stepwise manner. Each question is accompanied by relevant documents, question, and the correct answer, and the quality of reasoning at each step must be evaluated. Give score of 0 if the response lacks logical coherence or is not based on medical evidence, and 1 if this is not the case. Please note that if the explanation does not match the provided ground truth, it must be scored as 0. Critically assess the reasoning at each step. At the end of your evaluation, you must include final summary of the scores in the following format: ## Step 1: 0 or 1 ## Step 2: 0 or 1 ## Step 3: 0 or 1 ... Med-PRM system_message You are an evaluator assessing the logicality and validity of the reasoning in each step of the given explanation. In order to support the evaluation, the relevant documents, the question, and the explanation are provided sequentially. If the reasoning contains errors, output - after that step. If the reasoning in step is logical and valid, output + after that step. PRM system_message You are an evaluator assessing the logicality and validity of the reasoning in each step of the given explanation. In order to support Figure 6: Comparison of scaling test-time computation performance between Med-PRM and PRMs trained with conventional approach across overall medical benchmarks(including MedS3 in the same setting)."
        },
        {
            "title": "H Prompts",
            "content": "Multiple Choice Questions CoT Prompt system_message Solve the following question step-by-step. Do not analyze individual options in single step. Each step of your explanation must start with Step number: format. You must provide the answer using the phrase the answer is (option alphabet) at the end of your step. Open-Ended Questions CoT Prompt system_message Solve the following question step-by-step. Each step of your explanation must start with Step number: format. The final answer must output concise and clearly defined diagnostic term. You must provide the final answer using the phrase ## Final Diagnosis: Disease name at the end of your final step. Please refer to the following example. ## Final Diagnosis: Multiple Sclerosis the evaluation, the question and the explanation are provided. If the reasoning contains errors, output - after that step. If the reasoning in step is logical and valid, output + after that step. - If step contains critical error, any subsequent steps that rely on or are influenced by that error should also be scored as 0."
        },
        {
            "title": "I Human Evaluation Details",
            "content": "The evaluation was conducted by one physician with four years of clinical experience and two senior medical students. Each of the two medical students independently annotated all reasoning steps, and all annotationsincluding those with disagreementswere subsequently reviewed and adjudicated by the physician. The evaluation followed the guidelines described below."
        },
        {
            "title": "Human Evaluation Guidelines",
            "content": "The following content presents stepwise explanation of medical problem. Provide critical evaluation of each step based on an integrated assessment of the following criteria. Evaluation Criteria - Factual Accuracy:Does the step accurately reflect established medical knowledge? Are there any inconsistencies or factual inaccuracies? - Problem-Solving Relevance: Does the step contribute meaningfully to solving the problem? Does it avoid diverging into irrelevant or tangential reasoning? - Logical Coherence: Is the reasoning based on appropriate medical knowledge and logically consistent within the clinical context? Scoring Method - Assign 1 point or 0 points to each step. - 1 point: Awarded when the step is generally factually accurate, contributes to solving the problem, and demonstrates coherent medical reasoning. - 0 points: Assigned when the step contains significant factual errors or involves reasoning that critically undermines the problemsolving process. Scaling Test-Time Computation with PRMs Across Multiple Models"
        },
        {
            "title": "Model",
            "content": "MedQA-4 MedQA-5 MedMCQA MMLU-Med DDXPlus Agent Clinic NEJM * Agent Clinic MedQA *"
        },
        {
            "title": "Average",
            "content": "Multiple-Choice QA Open-Ended QA Open-source Language Models with Scaling Test-time Computation (Best-of-N) Open-source Language Models with Scaling Test-time Computation (SC+RM) Llama3.1 CoT SC PRMsoft PRMhard MedS3 Med-PRM UltraMedical CoT SC PRMsoft PRMhard MedS3 Med-PRM Med-PRM π CoT SC PRMsoft PRMhard MedS3 Med-PRM Llama3.1 CoT SC PRMsoft PRMhard MedS3 Med-PRM UltraMedical CoT SC PRMsoft PRMhard MedS3 Med-PRM Med-PRM π CoT SC PRMsoft PRMhard MedS3 Med-PRM 70.93 74.86 72.19 71.09 70.15 76.98 72.66 75.63 75.65 76.28 74.00 76. 67.16 76.04 72.58 72.27 69.60 76.76 70.93 74.86 75.49 75.33 73.84 78.24 72.66 75.63 77.14 76.90 75.41 79.87 67.16 76.04 76.51 77.06 75.26 79.18 65.20 70.70 67.48 67.48 65.99 73.06 68.34 71.80 71.80 71.48 68.66 74. 64.26 71.80 68.81 69.05 67.09 73.06 65.20 70.70 72.19 71.09 70.15 73.53 68.34 71.80 72.43 72.19 72.03 75.26 64.26 71.80 72.51 72.58 72.03 75.49 61.60 63.40 64.40 61.80 62.80 63.40 62.60 64.20 63.00 63.40 63.00 64. 57.20 62.20 60.20 60.40 60.00 64.40 61.60 63.40 65.00 64.20 64.40 66.40 62.60 64.20 65.40 64.40 63.00 65.50 57.20 62.20 63.60 63.00 63.40 67.40 78.97 81.63 78.97 76.49 77.78 81.18 79.61 81.91 81.36 80.99 80.62 83. 75.48 82.00 78.97 78.33 78.79 82.46 78.97 81.63 82.28 81.73 81.36 83.29 79.61 81.91 82.46 82.37 81.36 82.83 75.48 82.00 82.55 82.19 81.82 83.29 68.80 75.20 74.40 70.60 77.60 78.00 72.60 76.20 73.60 72.80 75.80 75. 67.20 74.80 74.60 71.40 74.80 77.80 68.80 75.20 75.00 75.20 75.80 76.80 72.60 76.20 75.60 75.80 76.60 77.60 67.20 74.80 75.80 75.60 75.60 77.20 35.83 49.17 51.67 48.33 49.17 57.50 45.83 49.17 55.00 48.33 48.33 55. 40.00 50.83 49.17 45.83 48.33 54.17 35.83 49.17 50.83 51.67 54.17 53.33 45.83 49.17 51.67 50.00 54.17 52.50 40.00 50.83 54.17 53.33 52.50 52.50 71.96 75.23 73.83 76.64 78.04 76.64 70.56 76.64 77.57 74.77 74.77 77. 68.69 78.04 77.10 73.83 71.50 80.37 71.96 75.23 76.17 75.23 76.64 77.10 70.56 76.64 78.50 78.04 75.23 77.57 68.69 78.04 78.97 79.44 78.04 79.44 64.76 70.03 68.99 67.49 68.79 72.39 67.46 70.79 71.14 69.72 69.31 72. 62.86 70.82 68.78 67.30 67.16 72.72 64.76 70.03 70.99 70.64 70.91 72.67 67.46 70.79 71.89 71.39 71.11 73.02 62.86 70.82 72.02 71.89 71.24 73.50 Table 6: Accuracy of open-source language models with scaling test-time computation. Three models were evaluated: Llama 3.1 8B Instruct, Llama-3-8B-UltraMedical (best performing model below 10B parameters), and Med-PRM π, for solution sampling. The sampling outputs were assessed using Self-Consistency (SC) and various PRM methods under both Best-of-N and SC+RM settings. Across different scaling test-time computation strategies for each sampling, the best scores are shown in bold, and second-best scores are underlined. Med-PRM achieved the highest average score across test-time computation scaling methods."
        }
    ],
    "affiliations": [
        "AIGEN Sciences",
        "ETH Zürich",
        "Hanyang University College of Medicine",
        "Korea University",
        "University of Ulsan College of Medicine",
        "Yale University"
    ]
}