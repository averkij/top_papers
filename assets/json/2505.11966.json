{
    "paper_title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier",
    "authors": [
        "Jianyuan Zhong",
        "Zeju Li",
        "Zhijian Xu",
        "Xiangyu Wen",
        "Kezhi Li",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 6 9 1 1 . 5 0 5 2 : r Solve-Detect-Verify : Inference-Time Scaling with Flexible Generative Verifier Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, Kezhi Li, Qiang Xu The Chinese University of Hong Kong {jyzhong, zjli24, zjxu21, xywen22, kzli24, qxu}@cse.cuhk.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) reasoning for complex tasks inherently involves trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe , novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe , proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers scalable and effective solution to enhance LLM reasoning at test time. Figure 1: Performance Scaling Analysis. (Left) On the AIME2024 benchmark, our inference-time scaling framework, Solve-Detect-Verify , achieves higher accuracy while requiring approximately 4x fewer solutions compared to baseline approaches. Since DeepSeek-R1-Distill-Qwen-14B does not report performance from = 2...32, we connect two dots with dotted straight line. (Right) On the Math benchmark, our verifier FlexiVe (specifically with the Flex@8 configuration) attains higher F1 score while generating approximately 3x fewer tokens than the baseline. Equal contribution. Corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have significantly enhanced their capabilities in tackling complex reasoning tasks, primarily through the explicit generation of step-by-step reasoning traces [1, 2]. This shift towards deeper, more analytical \"System 2\" processes [37], while crucial for improving solution accuracy, inherently presents fundamental trade-off with computational efficiency. Models often produce verbose reasoning, including redundant steps or overthinking [8], where extensive intermediate computations needed for higher accuracy incur substantial costs, sometimes for only marginal gains. This landscape highlights the ongoing challenge of balancing accuracy and efficiency in LLM reasoning, necessitating more sophisticated mechanisms for both generating solutions and verifying their correctness. The need to ensure the reliability of these reasoning traces through verification further complicates the aforementioned accuracy-efficiency balance [9]. While robust verification is crucial for enhancing LLM capabilities, existing methods introduce their own challenging trade-offs. For example, Generative Reward Models (GenRMs) promise detailed step-level feedback [10, 11], but often at the cost of significant computational overhead or naive and expensive integration [12]. Conversely, highly token-efficient mechanisms like NoThinking [13], when adapted for verification, can achieve substantial token reduction (e.g., 27x-40x fewer tokens, see Figure 2) but suffer severe drop in error precision (e.g., to 39-56% on mathematical benchmarks), leading to unreliable judgments. This underscores the critical demand for verifiers that can effectively reconcile speed with high reliability. This initial efficiency challenge within the reasoning process itself is further exacerbated when LLMs exhibit prolonged self-correction behavior. Models frequently generate hesitation words or phrases (e.g., hmm, let me double check) and redundant internal verification steps even after correct intermediate solution might have been implicitly reached [8]. This continued generation, as models overthink problem, incurs substantial computational costs for little to no gain in final accuracy. An effective system must therefore also address these redundancies by intelligently discerning when solution is likely complete. This complex interplay of trade-offs in reasoning and verification reveals clear methodological gap: there is pressing need for (1) flexible verifier that can dynamically adapt its computational effort to the complexity of the verification task, balancing inference speed with accuracy, and (2) an intelligent inference-time pipeline that strategically deploys such verifier and streamlines the overall reasoning process by curtailing unnecessary computation. To address these compounded challenges, we introduce two main contributions: We propose FlexiVe (Flexible Generative Verifier), novel generative verification method that dynamically adjusts its computational resources. FlexiVe employs rapid, resource-efficient fast thinking mode, optimized for concise error diagnosis through techniques like Group Relative Policy Optimization (GRPO) [6, 14], and thorough, computationally-intensive slow thinking mode. The transition between these modes is governed by Flexible Allocation of Verification Budget strategy; this strategy first uses efficient, parallelizable assessments of the entire reasoning trace to gauge verification difficulty, escalating to deeper analysis only when initial consensus is low, thereby allowing FlexiVe to analyze entire reasoning traces efficiently and pinpoint errors with high precision, unlike verifiers that operate on per-step basis [15]. To effectively leverage FlexiVe , we introduce the Solve-Detect-Verify pipeline, novel inference-time scaling framework. This pipeline features lightweight assessment mechanism that continuously monitors the solver LLMs reasoning trace for cues of solution completeness. Upon detecting potentially complete solution, the pipeline pauses generation and invokes FlexiVe for targeted verification. If validated, the solution is finalized, saving further computation. If errors are found, FlexiVe focused feedback guides the solver towards refining its reasoning. Extensive experiments validate both contributions. FlexiVe demonstrates superior accuracy in identifying and pinpointing errors within reasoning traces compared to existing verification methods on benchmarks like ProcessBench [16]. The integrated Solve-Detect-Verify pipeline significantly outperforms widely-adopted inference-time strategies, such as self-consistency [17], in both reasoning accuracy and token efficiency on challenging mathematical reasoning benchmarks, including AIME 2024 [18], AIME 2025 [19]. Our work presents scalable and effective approach to enhance the reliability and efficiency of complex LLM reasoning at test time, illustrated in Figure 1. The remainder 2 of this paper details our methodology, presents comprehensive experimental results, discusses related work, and concludes with future directions. Figure 2: Empirical motivation for efficient verification and generation strategies. (Left) Comparison of error precision and token usage between NoThinking and Thinking verification on GSM8K and Math (ProcessBench). While NoThinking significantly reduces tokens, its error precision is substantially lower, sugguesting high false positive rate. (Right) Accuracy and token usage comparison between generating full solution (Full Thinking) and halting generation early upon detecting complete intermediate solution (First Solution) on AIME 2024 and AIME 2025. Early detection offers significant token reduction with comparable accuracy."
        },
        {
            "title": "2 Related Work",
            "content": "Inference-Time Scaling Strategies. To navigate the inherent accuracy-efficiency trade-off in LLM reasoning, various inference-time scaling strategies increase compute at test time, such as Bestof-N sampling, self-consistency [17], and tree-based searches [20, 21]. While often improving accuracy, these methods can be computationally intensive and may not optimally integrate verification, sometimes exacerbating inefficiencies like overthinking [8]. The need for robust verification within these scaled approaches [9, 22, 23] underscores that simply increasing generation is insufficient, calling for intelligent frameworks like Solve-Detect-Verify to strategically manage both generation and verification. Generative Process Verifiers. While crucial for accuracy, verifiers themselves can complicate the LLM reasoning tradeoff. Expressive generative verifiers like Generative Reward Models (GenRMs) and Process Reward Models (PRMs) [24 26, 11, 10] offer detailed feedback but are often computationally demanding [12], and SFT-based training may limit generalization [27]. Even dynamic approaches like Dyve [15], with \"fast\" and \"slow\" modes, face challenges, as per-step verification can accumulate significant overhead. In contrast, FlexiVe holistic trace analysis with dynamic budget allocation aims for more cost-effective balance, efficiently pinpointing errors with high precision. NoThinking User Query / Problem <beginning_of_thinking> Okay, think have finished thinking. </end_of_thinking> Final Answer Generation Thinking Fast and Slow in Reasoning Language Models. Kahnemans dual-process theory [3] informs approaches to balancing deliberate System 2-like reasoning with efficiency in LLMs [4]. While some methods target generation efficiency (e.g., adaptive computation [28, 29], pruning [30]), extreme token reduction like the NoThinking mechanism [13] highlights the verification dilemma: when applied to verification, such efficiency can lead to low precision (Figure 2). FlexiVe dual-mode \"fast\" and \"slow thinking\" is inspired by these concepts, but its \"fast thinking\" is specifically optimized for reliable error diagnosis via Reinforcement Learning [6]. This, combined with dynamic budget allocation, seeks more robust and efficient balance than verification strategies that are either consistently expensive or unreliably fast. Figure 3: The NoThinking mechanism bypasses explicit thought generation, using template to fill the thinking phase."
        },
        {
            "title": "3 Method",
            "content": "3.1 Problem Formulation System Components Our inference-time scaling framework uses two primary Large Language Model (LLM) components: solver LLM and FlexiVe , our specialized generative verifier. Both are reasoning-capable models. The solver, an off-the-shelf LLM, generates initial candidate solutions without modification. FlexiVe is specifically trained for verification, with its architecture and training detailed in Section 3.2. Reasoning Trace Segmentation reasoning trace Strace is parsed into an ordered sequence of Ns steps, Strace = (step1, . . . , stepNs ). Each stepi is contiguous text segment delineated by predefined \"hesitation keywords\" (e.g., \"hmm,\" as might be listed Figure 12 in the appendix), marking transitions between keywords or the traces start/end. This segmented trace forms the input for verification. Verifier Operation and Output The task of the verifier is to assess the correctness of the solvers reasoning trace Strace. Different verifier architectures approach this differently, as illustrated in Figure 4. For example, standard Generative Reward Model (GenRM) might perform single, comprehensive \"long thinking\" pass over the entire query and trace to output binary judgment. Processfocused variants like GenPRM often conduct sequential, step-by-step verification, which can be computationally intensive. In our framework, the steps from Strace are formatted using critic template [16] to create an input prompt for FlexiVe . Unlike per-step verifiers, FlexiVe evaluates the entire trace but employs dynamic strategy (detailed in Section 3.2) to modulate its computational effort. It outputs Vout = (F, idxpred), where is textual error analysis and idxpred is the predicted index of the first error. Consistent with its training, idxpred = 1 signifies no errors. This Vout informs decisions within our Solve-Detect-Verify . Figure 4: Comparison of verification mechanisms. Standard GenRMs holistically assess trace. GenPRMs often verify step-by-step. FlexiVe (Ours) uses an adaptive approach on the entire trace, with initial parallel fast evaluations deciding if deeper, slow verification is needed. 3.2 FlexiVe FlexiVe is generative verifier that dynamically modulates computational effort during test-time verification, operating in \"fast thinking\" and \"slow thinking\" modes. The fast thinking mode, inspired by Ma et al. [13] and enhanced with Reinforcement Finetuning, generates significantly shorter outputs 27x-40x in Figure 2) than the conventional slow thinking modes detailed trace. Our Flexible Allocation of Verification Budget scheme manages these modes to leverage fast thinkings efficiency. Reinforcement Training FlexiVe is trained using Group Relative Policy Optimization (GRPO) [6]. In this framework, base reasoning model fine-tuned on mistake detection task predicts either the index of the first error (idxgt) or returns 1 if the reasoning is correct. GRPO optimizes the models generation policy by maximizing composite reward defined as Ri = Rcorrect + Rlength. The correctness reward Rcorrect is defined by Rcorrect(idxpred, idxgt) = (cid:26)1.0 0.0 if idxpred = idxgt otherwise , (1) assigning binary score based on the match between the predicted and true error indices. The length adjustment reward Rlength modulates the response length relative to idxgt and is given by Rlength = (L, idxgt), where (L, idxgt) is the length penalty function. 4 (L, idxgt) = min (Pmax, cf ast max(0, Lf ast)) if idxgt = 1 min (Pmax, cunder max(0, Lslow_min L)) + min (Pmax, cover max(0, Lslow_max)) if idxgt = 1 (2) In Equation 2, when idxgt = 1, responses exceeding the target length Lf ast are penalized, thereby promoting fast thinking in the absence of errors. Conversely, if idxgt = 1, lengths outside the interval [Lslow_min, Lslow_max] are penalized to encourage detailed thinking during error analysis. Training involves sampling outputs per prompt, computing each reward, and calculating advantages relative to the groups average as in Shao et al. [6]. Flexible Allocation of Verification Budget FlexiVe dynamically allocates its verification budget. The core intuition is thus to leverage inexpensive, parallelizable probes to gauge verification difficulty upfront, and only escalate to more resource-intensive analysis when these probes indicate ambiguity or complexity, thereby tailoring computational effort to the specific needs of each verification instance. Initially, it performs \"fast thinking\" verification runs. The consensus among these runs is measured by the agreement ratio: (3) where ai is the count of the most frequent outcome (using fuzzy error index matching). If this ratio meets predefined threshold τ (Ragreement τ ), the consensus result from the fast phase, Vfast, is accepted. Otherwise, max(1, k/8) additional, resource-intensive \"slow thinking\" runs are triggered to produce the final outcome Vslow. The overall verification result is thus determined by: maxi ai Ragreement = , (cid:26)Vfast, = if Ragreement τ, Vslow, otherwise. (4) Algorithm 1 Solve-Detect Stage of Solve-Detect-Verify This adaptive strategy optimizes computational cost by reserving intensive verification only for cases where initial fast assessments lack sufficient agreement. Crucially, this decision logic and the subsequent verification (whether fast or slow) are applied to the reasoning trace as whole, rather than on per-step basis as in some prior dynamic verifiers like Dyve [15]. By evaluating the entire trace with dynamically chosen verification depth, FlexiVe aims to avoid the accumulated cost of per-step decisions, potentially offering better scalability and efficiency, especially for longer or more complex reasoning processes. The intuition is to use quick, broad assessment first, and only invest significant resources when this initial assessment signals higher uncertainty or difficulty. 3.3 Solve-Detect-Verify 1 ) 1 tk S1 stop_f lag false for = 1 to Lmax do tk Msolve(P, S(k1) 1 S(k1) S(k) if tk = EOS then Input: Problem , Solver Msolve Output: Candidate Solution S1 1: procedure SOLVEDETECT(P, Msolve) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: break S1 S(k) stop_f lag true if stop_f lag then return if S(k) 1 1 Lmax is max length ends with kw Khesitation then logpYes log pMsolve (YesPromptcomplete(S(k) 1 )) logpNo log pMsolve (NoPromptcomplete(S(k) 1 )) if logpYes > logpNo then stop_f lag true Compare log-probs Solution complete Solve-Detect-Verify is multi-component framework designed to enhance the reasoning accuracy and efficiency of Large Language Models (LLMs). The pipeline integrates distinct modules: an initial solution generation phase (Solve), mid-stream reasoning monitoring and management stage (Detect), and combined validation and conditional refinement process (Verify and Refine). The complete pipeline is summarized in Algorithm A.2, with detailed implementation provided in Appendix. The conceptual framework of the pipeline is as follows: Solve The Solve stage initiates the process, wherein the solver LLM is tasked with generating an initial, step-by-step candidate solution (S1) to given problem. This stage forms the foundational attempt at problem-solving, producing complete reasoning trace and final answer for subsequent evaluation. Detect The Detect module continuously monitors LLM output for predefined hesitation keywords (Figure 12 in the Appendix). Upon detecting keyword, generation pauses, and the LLM is prompted (Figure 13 in the Appendix) to assess solution completeness via log-probabilities ( log p(Yes) 5 vs. log p(No)). This check efficiently reuses over 90% of the generation prefix, preserving the Key-Value (KV) cache and minimizing computation overheads. If reasoning is deemed complete, the pipeline advances to Verify and Refine; otherwise, generation resumes. This adaptive monitoring reduces overhead and enables early verification. Verify and Refine Upon full generation or early completion detected by the Detect module, the candidate solution S1 is assessed by FlexiVe , which identifies any errors and their specific step idxpred. validated S1 directly becomes the final output. If an error is found in S1, FlexiVe diagnostic feedback (F1) guides the solver LLM to generate single new candidate solution, S2, aiming to correct the error by exploring an alternative reasoning path. This refined solution S2 is then accepted as the final output, without requiring an additional validation round. This integrated approach of validation followed by conditional, feedback-driven refinement ensures balance between rigorous solution assessment and efficient improvement."
        },
        {
            "title": "4 Experiments",
            "content": "Our experiments are designed to achieve two primary goals. First, we evaluate the performance and efficiency of FlexiVe as standalone generative verifier, analyzing its scaling properties compared to baseline approaches. Second, we assess the effectiveness of our Solve-Detect-Verify in enhancing the reasoning accuracy and computational efficiency of LLMs on complex mathematical tasks, comparing it against standard inference-time strategies. 4.1 Experimental Setup For detailed experimental configurations, including hyperparameter settings for all models and full dataset statistics, please refer to Appendix A.1. FlexiVe Training FlexiVe is initialized from DeepSeek-R1-Distill-Qwen-14B [6] and trained for mistake detection using Group Relative Policy Optimization (GRPO) [6] on 90% of the BIG-Bench Mistake dataset [31], with 10% for validation. All the NoThinking mechanism are activated for all input problem and reasoning traces pair to ensure that the training was performed in fast mode for targeted improvement. The objective, optimizing composite reward, uses LORA PEFT [32] (r = 16, α = 32) and AdamW [33]. Key GRPO parameters include = 14 samples per input and KL coefficient of 0.04. All experiments are conducted on 8 NVIDIA A800-SXM4-80GB GPUs. Evaluation Tasks and Datasets We assess FlexiVe step-level verification capability, measured by F1 score, on the comprehensive ProcessBench benchmark [16]. ProcessBench includes diverse mathematical reasoning datasets such as GSM8K, MATH, OlympiadBench, and OmniMATH. For the full Solve-Detect-Verify , we evaluate end-to-end task accuracy and token efficiency on particularly challenging mathematical datasets: AIME (2024, 2025) [18, 19], AMC, CNMO [34], and OlympiadBench. Especially, All token counts in the results refer exclusively to the output tokens generated by the LLM, in the entire testing dataset. Baselines On ProcessBench, FlexiVe performance is compared against established Process Reward Models (PRMs) from [16], as detailed in Table 4. We also include comparison with token-efficient NoThinking verification approach, similar to that described in [13], which represents simpler, non-deliberative verification strategy. For evaluating the Solve-Detect-Verify , DeepSeek-R1 14B and 32B models [6] serve as the base \"worker\" LLMs. The pipelines performance is benchmarked against: (1) the direct output of the worker LLM, representing standard prompting baseline, and (2) Self-Consistency with majority voting [17], widely recognized inference-time technique for enhancing LLM reasoning by sampling multiple solutions. 4.2 FlexiVe Performance and Scaling Analysis This section evaluates FlexiVe error identification accuracy on ProcessBench [16] and its efficiency on subsets GSM8K and MATH. We test FlexiVe in several configurations: FlexiVe (Flex@k) uses adaptive verification, starting with initial \"fast\" verification samples and dynamically deciding whether to escalate to more thorough verification; FlexiVe (Think@k) employs samples from FlexiVe deliberative \"slow thinking\" verification process with majority vote, designed for higher accuracy at typically higher computational cost; and FlexiVe (NoThinking@k) represents FlexiVe in purely \"fast thinking\" or non-deliberative mode using samples with majority vote, analogous to the NoThinking baseline but with FlexiVe architecture. The \"Moderate Compute\" and \"High 6 Table 1: ProcessBench results reported with F1 scores. Results for FlexiVe are highlighted . bold indicates the best in the sub category. All FlexiVe variants are trained on only 1526 samples. Model # Samples GSM8K MATH Olympiad Bench OmniMATH Avg. GPT-4o-0806 o1-mini Proprietary Models unk unk 79.2 93.2 Open Source Models (7-8B) Qwen2.5-Math-PRM-7B RetrievalPRM-7B Universal-PRM-7B Direct Generative PRM-7B GenPRM-7B w/ Code Exec (Pass@1) GenPRM-7B w/ Code Exec (Maj@8) 344K 404K unk 23K 23K 23K 82.4 74.6 85.8 63.9 78.7 81.0 63.6 88. 77.6 71.1 77.7 65.8 80.3 85.7 Open Source Models (14-32B) w/ Moderate Compute Dyve-14B GenPRM-32B w/o Code Exec (Maj@8) FlexiVe (Flex@32) FlexiVe (Flex@128) 117K 23K 1526 1526 68.5 78.8 82.8 83.0 58.3 85.1 83.3 85. Open Source Models (14-32B) w/ High Compute GenPRM-32B (Pass@1) w/ Code Exec GenPRM-32B (Maj@8) w/ Code Exec FlexiVe (Think@64) 23K 23K 1526 83.1 85.1 88.1 81.7 86.3 90.1 51.4 87. 67.5 60.2 67.6 54.5 72.2 78.4 49.0 78.7 79.2 80.0 72.8 78.9 86.7 53.5 82.4 66.3 57.3 66.4 55.9 69.8 76.8 47.2 74.9 73.4 75. 72.8 80.1 80.4 61.9 87.9 73.5 65.8 74.3 60.0 75.2 80.5 55.8 79.3 79.7 80.8 77.6 82.6 86.3 Compute\" categories in Table 4 are broadly defined by the number of verification samples or overall inference cost, with \"High Compute\" settings involving more extensive verification efforts. Verification Accuracy on ProcessBench Table 4 (with FlexiVe results highlighted in violet ) details the F1 scores for FlexiVe compared to various baselines. In the \"Moderate Compute\" setting, FlexiVe (Flex@128) achieves strong average F1 score of 80.8%, with notable 85.0% on the MATH dataset. This performance surpasses the GenPRM32B (Maj@8) model (without code execution), which scores 79.3% average F1, despite FlexiVe being trained on significantly fewer samples (1,526 vs. 23K). The FlexiVe (Flex@32) configuration also demonstrates competitive performance with 79.7% average F1 score. verification tokens on GSM8K Figure 5: F1 score vs. FlexiVe (Flex@k, green cir- (left) and MATH (right). cles) demonstrates higher F1 for similar token costs than DeepSeek-R1-Distill-Qwen-14B (blue triangles, baseline verifier), both outperforming the token-efficient FlexiVe (NoThinking variant, red squares). X-axis denote the number of token generated across the entire test set. In the \"High Compute\" setting, FlexiVe (Think@64), utilizing its deliberative \"slow thinking\" mode, achieves exceptional F1 scores of 88.1% on GSM8K and 90.1% on MATH. This performance notably exceeds that of the compute-intensive GenPRM-32B (Maj@8) model with code execution (which scores 85.1% on GSM8K and 86.3% on MATH). These results highlight that FlexiVe sophisticated deliberative verification (Think@64), despite its own computational demands, can achieve superior accuracy 7 compared to other large verifiers, even those augmented with code execution. This underscores the effectiveness of FlexiVe architecture and training, even when scaled to more intensive verification tasks. The significantly smaller training data requirement for FlexiVe across all its configurations further emphasizes its sample efficiency. Efficiency and Budget Scaling (GSM8K & MATH) Figure 5 depicts the accuracy-cost trade-off. On both GSM8K and MATH, FlexiVe (Flex@k) (green circles) provides better F1 score for comparable token usage than the baseline verifier, DeepSeek-R1-Distill-Qwen-14B (DS14B, blue triangles). While both FlexiVe (Flex@k) and DS14B reach higher peak F1 scores, the FlexiVe (NoThinking@k) variant (red squares) is considerably more token-frugal, albeit with lower F1 ceiling. 4.3 Scaling Solve-Detect-Verify for Enhanced Performance We evaluate Solve-Detect-Verify on AIME2024 [18], AIME2025 [19], and CNMO [34] to understand its scaling properties. We explore two primary scaling dimensions: first, varying FlexiVe verification budget within single pipeline execution, and second, generating multiple candidate solutions from the worker LLM, each processed by Solve-Detect-Verify . Scaling FlexiVe Verification Budget (Flex@N) in Single Pipeline Run We first analyze scaling FlexiVe internal verification budget (Flex@N, representing fast-thinking verification samples post-extraction) within single pipeline pass. In figure 6, the w/o Flex setup (Solve + Detect) significantly cuts token usage, token ratio 0.67 on AIME2024 and 0.43 on CNMO, but can reduce accuracy, notably on CNMO (44.4% vs. In55.5% baseline). tegrating FlexiVe verification, particularly Flex@8, substantially boosts accuracy over baseline on AIME2024 (73.3% vs. 56.6%) and AIME2025 (50.0% vs. 43.3%), and matches baseline accuracy on CNMO (55.5%). Crucially, these Flex@8 configurations use fewer tokens than the baseline (e.g., 0.96 AIME2024, 0.80 CNMO token ratio), demonstrating Solve-Detect-Verify token-efficient accuracy gains. However, CNMOs less consistent improvement with suggests that varying only the verifier budget might not universally ensure peak performance. Impact of scaling FlexiVe verification budget Figure 6: (Flex@N) within single Solve-Detect-Verify execution on Pass@1 Accuracy vs. Token Usage Ratio relative to DeepSeek R1 14B. Benchmarks are color/linestyle distinguished. Scaling Solver and Verifier via Multiple Solutions To achieve more consistent gains and higher peak accuracies, we scale compute by generating multiple solutions from the solver, each verified by FlexiVe . On the AIME2024 benchmark (Figure 1, left panel), this strategy yields significant and consistent accuracy improvements as more solutions are processed: accuracy climbs from 67.5% (1 solution) to over 83% (16 solutions). This approach effectively leverages increased solver compute, with FlexiVe identifying the correct solution among candidates, demonstrating robust path to superior performance, especially for top-tier accuracy. This underscores our takeaway in Figure 7: for optimal results with Solve-Detect-Verify , scaling solver LLMs compute is as important as scaling FlexiVe verification capabilities. Takeaway for Solve-Detect-Verify scaling With Solve-Detect-Verify , scaling solver LLMs compute is as important as scaling FlexiVe s. Figure 7: take away highlights the symbiotic relationship. 8 4.4 Extended Analysis Component Performance Comparison An ablation study assessed individual component impacts. For FlexiVe , we used Flex@4; for NoThinking, maj@8; and for both the DeepSeek-R1-Distill-Qwen-14B baseline and FlexiVe deliberative mode, Think@1, ensuring roughly comparable computational budgets. Figure 8 shows that FlexiVe Reinforcement Learning (RL) training not only matches or slightly exceeds the baseline verifiers performance under similar compute but also significantly outperforms when FlexiVe engages its \"thinking\" mode. This is crucial: though trained with RL primarily leveraging its efficient \"NoThinking\" (fast) mode, FlexiVe generalizes effectively to improve verification in its more deliberative \"thinking\" mode, underscoring its RL-trained robustness and adaptability. RL vs. SFT We compared our RL approach with traditional SFT for training verifiers. The SFT baseline used 10,000 reasoning paths with problems form OpenO1 [35] and generated by DeepSeek-R1-Distill-Qwen-14B. They are labeled via LLM-based judging as [16]. Findings (Figure 9) suggest SFT lack generalization. Reasoning traces in benchmarks like ProcessBench, often from weaker, non-thinking LLMs, are shorter and less complex. This led to performance drops for SFT verifier on more diverse processes. In contrast, FlexiVe , RL-trained on only 1,526 BIG-Bench Mistake [31] problems, showed strong generalization. This highlights RLs advantage in fostering robust verifiers with significantly less data than typical SFT."
        },
        {
            "title": "5 Limitations",
            "content": "Figure 8: Ablation: Component impact on GSM8K/MATH (%). FlexiVe (Think) excels; FlexiVe (Flex@4) also surpasses NoThinking (maj@8) and DS-R1-14B (Think@1). Figure 9: RL (FlexiVe ) vs. SFT (DeepSeek-R1-Distill-Qwen-14B) on GSM8K/MATH. RL-trained FlexiVe , especially in thinking mode, shows superior generalization over the SFT baseline. While FlexiVe and Solve-Detect-Verify demonstrate promising advancements, several avenues warrant future investigation to enhance their robustness and broaden their applicability. The generalization of FlexiVe is inherently linked to its training data diversity, and our current validation, primarily on mathematical reasoning due to computational constraints, invites further cross-domain exploration (e.g., in program synthesis or commonsense QA). The empirically-set parameters (k, τ ) for FlexiVe dynamic budget allocation would benefit from comprehensive sensitivity analysis and the development of automated tuning guidelines to maximize practical adoption. Furthermore, although Solve-Detect-Verify is designed for efficiencywith mechanisms like KV cache reuse in its heuristic Detect stageits multi-component nature and dynamic mode-switching introduce inherent computational overhead. We believe this overhead could be substantially mitigated, and overall performance significantly boosted, through optimized implementations, potentially leveraging advanced inference engines like vLLM [36] or SGLang [37]; advancing this represents valuable direction for community exploration to fully realize the benefits of such dynamic reasoning systems. Addressing these aspects will be key to the continued development and deployment of sophisticated, efficient, and widely applicable verified reasoning frameworks."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce FlexiVe , dynamic verifier balancing computational cost and accuracy, integrated into the Solve-Detect-Verify pipeline for efficient LLM reasoning enhancement. Experiments confirm that our pipeline, leveraging FlexiVe , achieves significant gains in both accuracy and token efficiency over baselines, highlighting flexible verification and intelligent pipeline design as scalable path toward more reliable and efficient complex reasoning in LLMs."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [2] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. [3] Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. [4] Zhong-Zhi Li, Haotian Wang, Kaiyan Zhang, Yancheng He, Yujia Xie, Yuxiang Huang, Zhengliang Shi, HongCheng Li, Wenxuan Wang, Zhiwei He, Dian Yu, Haitao Mi, Dong Yu, Jie Tang, and AnBo Zhang. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [5] OpenAI. Reasoning models. https://platform.openai.com/docs/guides/reasoning, 2024. Accessed: May 7, 2025. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. [7] Google. Gemini 2.5 pro preview: even better coding performance. https://developers. googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/, May 2025. Accessed: May 7, 2025. [8] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [9] Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Ö. Arık. SETS: Leveraging self-verification and self-correction for improved test-time scaling. arXiv preprint arXiv:2501.19306, 2025. [10] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling, 2025. URL https://arxiv.org/abs/ 2504.02495. [11] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction, 2025. URL https: //arxiv.org/abs/2408.15240. [12] Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning, 2025. URL https://arxiv.org/abs/2504. 01005. [13] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking, 2025. URL https://arxiv.org/abs/2504. 09858. [14] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300v3. [15] Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, and Qiang Xu. Dyve: Thinking fast and slow for dynamic process verification, 2025. URL https://arxiv.org/abs/2502.11157. [16] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2024. URL https://arxiv.org/abs/2412.06559. [17] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR), 2023. [18] Aime 2024 dataset card. 2024. URL https://huggingface.co/datasets/ HuggingFaceH4/aime_2024. [19] Aime 2025 dataset card. 2025. URL https://huggingface.co/datasets/opencompass/ AIME2025. [20] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Sha, Thomas Chen, Boyuan Rius, Yuxuan Du, Yang Liu, Zipeng Jiang, Tushar Han, et al. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023. [21] Noah Xie, AI AUTOdidax, Sarmad Parvez, Michael Song, Zhenqiao Zhang, Ziyu Chen, Shrimai Joshi, Robert Gmyr, Yufan Li, Siyuan Li, et al. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023. [22] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023. [23] Zhaofeng Gou, Zhibo Liu, Jiacheng Xu, Hong Beaver Zhou, Shwai Zhang, Keyan Zhao, Weize Wang, and Chang Liu. Critic: Large language models can self-critique and self-correct their own novice mistakes. In International Conference on Learning Representations (ICLR). [24] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cognome. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [25] Michal Pikwalez, Konrad Słowik, Thomas Zomphos, Piotr Michalak, Mateusz Błaszczyk, Emilia Kosson, Paweł Topolski, Piotr Stanczyk, Adam Zomphos, Jakub Błajdo, Jan Miłkowski, Kyriacos Szymanski, Sebastian Jaszczur, Konrad GALIAS, et al. Prover: Process-based rewardIn International Conference on Learning Representations model for verifiable reasoning. (ICLR), 2024. [26] Daniel Saunders, Kevin Stuhlmüller, Amanda Askell, Nelson Smith, Benjamin Dominé, Dylan Drain, Albert Chen, Catherine Olsson, Long Ouyang, Evan Hubinger, et al. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. [27] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 2773027744, 2022. [28] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. [29] Shuming Diao, Sendi Chen, Nathanael Schärli, and Ankur Bapna. Blackmamba: Bit-masking for sparse and efficient attention. arXiv preprint arXiv:2310.01409, 2023. [30] Penghao Zhou, Zialan Huang, Bei Chen, Qian Zhang, Yonatan Bisk, Baolin Peng, Jianfeng Wang, and Chen Zhu. Condensed composite cone (c3) geometric approach to pruning chainof-thought. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1312613141, 2023. 11 [31] Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. LLMs cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.826. URL https://aclanthology.org/2024.findings-acl.826. [32] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/ forum?id=Bkg6RiCqY7. [34] Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. [35] O1-OPEN Team. Openo1-sft-ultra dataset. https://huggingface.co/datasets/ O1-OPEN/OpenO1-SFT-Ultra, 2024. Accessed: May 14, 2025. [36] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Lee, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles (SOSP 23), page 10131029, New York, NY, USA, 2023. Association for Computing Machinery. doi: 10.1145/3600006.3613165. URL https://doi. org/10.1145/3600006.3613165. [37] Lianmin Zheng, Siyuan Zhuang, Zhuohan Li, Cody Hao Yu, Lequn Li, Haotian Chen, Joseph E. Gonzalez, Ion Stoica, and Jonathan Ragan-Kelley. SGLang: Efficient and expressive structured generation for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), pages 10531071, St. Julians, Malta, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.63. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theArt natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. [39] Leandro von Werra, Lewis Schmid, Thomas Wolf, and Lewis Tunstall. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020-2024. [40] Lukas Biewald. Experiment tracking with weights and biases. https://wandb.ai, 2020. URL https://www.wandb.com/. Software available from wandb.com. [41] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Extended Experimental Setup FlexiVe Training. We train FlexiVe using Group Relative Policy Optimization (GRPO) [6] on mistake detection task. The policy πθ is initialized from the DeepSeek-R1-Distill-Qwen-14B model [6]. We utilize the BIG-Bench Mistake dataset [31], reserving 90% for training and 10% for validation. The training objective is to predict the index of the first reasoning error (idxgt) or output -1 if the trace is correct, optimized using the composite reward detailed in Section 4 (main paper). Parameter-Efficient Fine-Tuning (PEFT) is employed via LoRA [32] with rank = 16 and α = 32, targeting the attention projection layers. Optimization is performed using AdamW [33] with learning rate of 5 106 and gradient checkpointing. For GRPO, we sample = 14 outputs per input, and the KL coefficient is set to 0.04. Training is managed using the transformers [38] and trl [39] libraries, with experiment tracking via Weights & Biases [40]. Evaluation Tasks and Datasets. For FlexiVe evaluation, to assess its step-level verification capabilities, we use the ProcessBench benchmark [16]. This includes diverse mathematical reasoning datasets such as GSM8K, MATH, OlympiadBench, and OmniMATH. Performance is measured using the F1 score for identifying the first erroneous step. For Solve-Detect-Verify pipeline evaluation, to assess end-to-end effectiveness, we use suite of challenging mathematical reasoning datasets: AIME (2024 and 2025) [18, 19], AMC (mentioned in main text, details can be added if necessary), CNMO [34], and OlympiadBench (also used for FlexiVe evaluation). AIME is prestigious high school mathematics competition known for its challenging mathematical problems, and contains problems from the American Invitational Mathematics Examination (AIME) 2024 and 2025. The CNMO Benchmark evaluates AI on Chinas National Mathematical Olympiad problems, focusing on advanced proof-based problem-solving. On these tasks, we measure final task accuracy and computational efficiency (e.g., total tokens). Baselines. For FlexiVe baselines on ProcessBench, we compare against state-of-the-art Process Reward Models (PRMs) as reported in [16] and the NoThinking verification approach adapted from [41] (or your specific citation, e.g., [13]). For our Solve-Detect-Verify pipeline, we use DeepSeek-R1 14B [6] as the base worker LLMs. The full pipeline is compared against: (1) the worker LM generating solutions directly (potentially with the Detect mechanism only), and (2) the Self-Consistency method [17] applied to the worker LM. A.2 Solve-Detect-Verify Pipeline Implementation Details The Solve-Detect-Verify pipeline is implemented with specific two-attempt strategy derived from our Python codebase, emphasizing adaptive verification and intelligent solution generation. Algorithm 2 outlines this refined flow. Key components like solution generation with hesitation detection and adaptive verification are encapsulated in helper functions for clarity. 13 Algorithm 2 Solve-Detect-Verify Pipeline (Reflecting Python Implementation Logic) Require: Problem , Verification Parameters ΘV = (kf ast, τagree, kslow), Best-of-N NBoN 1: Sf inal NIL 2: 3: rompt1 FormatInitialPrompt(P ) 4: S1 GenerateSolutionWithDetection(LLM, rompt1) Attempt 1: Initial Solve and Adaptive Verification Handles streaming, hesitation detection, and continuation Uses kf ast, τagree, kslow Attempt 1 failed or was deemed invalid by verification Attempt 2: Retry with Best-of-N (BoN) rompt2 FormatRetryPromptWithFeedback(P, S1, F1) Solutionscandidates []; Answerscandidates [] for = 1 to NBoN do 5: (is_valid1, error_step1, F1) AdaptiveVerify(P, S1, ΘV ) 6: if is_valid1 = True then Sf inal S1 7: 8: else 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: if Sf inal is NIL and S1 is not NIL then Scand GenerateSolutionWithDetection(LLM, rompt2) Add Scand to Solutionscandidates Anscand ExtractFinalAnswer(Scand) Add Anscand to Answerscandidates (Ansmajority, Svoted) MajorityVote(Answerscandidates, Solutionscandidates) Sf inal Svoted BoN result is used directly without re-verification as per Python code Fallback if BoN stage wasnt reached or produced nothing, but S1 exists Sf inal S1 20: 21: Evaluate Sf inal against ground truth. 22: return Sf inal, Evaluation Metrics, Total Compute Cost Key Helper Functions: GenerateSolutionWithDetection(LLM, Prompt): This function generates solution by streaming tokens from the LLM. It incorporates the hesitation detection mechanism (detailed below in the \"Detect\" section) to identify potential points of self-correction or solution completion. If hesitation is detected and the solution is deemed complete by an internal check, generation might be paused and then explicitly continued to ensure the full thought process is captured before final truncation. AdaptiveVerify(P, S, ΘV ): This function performs verification on solution for problem . It first conducts kf ast \"fast thinking\" verifications. If the agreement ratio among these (based on exact error step matching) meets or exceeds τagree, their consensus result is returned. Otherwise, it proceeds to kslow (e.g., kf ast/4) \"slow thinking\" verifications, and their consensus is returned. FormatInitialPrompt, FormatRetryPromptWithFeedback, ExtractFinalAnswer, MajorityVote: Standard utility functions for formatting prompts (see Figure 10 for initial prompt and Figure 11 for retry prompt), extracting answers, and performing majority voting. Solve Given math problem x, we employ DeepSeek-R1-14B as step-by-step solution proposer (the LLM in GenerateSolutionWithDetection) using an initial prompt like the one in Figure 10. The prompt is sent in streaming chat-completion mode, and tokens are appended sequentially to buffer. If the initial solution attempt requires refinement based on verification feedback, retry prompt like the one in Figure 11 is used. Detect The GenerateSolutionWithDetection function incorporates mechanism to detect hesitation during reasoning. LLMs often employ hesitation words (e.g., \"hmm\", \"let me verify\") to self-verify. We observe that models may continue generating redundant checks even after reaching solution. To decide when to truncate these overthinking situations and reduce redundant tokens, we use streaming detection framework. We first define the set of hesitation cues, shown in Figure 12. 14 LLM Initial Solver Prompt The following is math problem: [Math Problem] {question} Solve it step by step. For each step, you should use nn in the end. Please put your final answer (i.e., the index) in boxed{{}}. Figure 10: LLM Initial Solver Prompt (Appendix). This prompt structure is utilized by the FormatInitialPrompt helper function. LLM Retry Prompt with Feedback (Guided Solver) The following is math problem: [Math Problem] {question} You previously attempted to solve this problem, and your solution was: [Previous Solution] {previous_solution_S1} That solution was reviewed, and the feedback is: [Verification Feedback] {verifier_feedback_F1} Please carefully consider the feedback and correct your solution. Provide complete, new solution with clear reasoning steps. Please put your final answer (i.e., the index) in boxed{{}}. Figure 11: LLM Retry Prompt with Feedback (Appendix). This prompt structure is utilized by the FormatRetryPromptWithFeedback helper function. Placeholders such as {question}, {previous_solution_S1}, and {verifier_feedback_F1} are dynamically populated. As each token arrives during solution generation, if the end of the current reasoning sequence matches any hesitation keyword (where is the set from Figure 12), we suspend the primary LLM proposer and trigger detection process. Detector LLM (which can be the same base model with specific prompt) evaluates the current reasoning context using the prompt in Figure 13 to check whether complete solution (including the final answer) has been reached. For efficiency, the Detector LLM is prompted to respond with only one token (\"Yes\" or \"No\") and minimal internal thought, for example: think Okay, think have finished thinking. /think To improve decision robustness, we compare the log-probabilities of \"Yes\" and \"No\" from the Detector LLMs top token predictions. If log p(Yes) > log p(No), we conclude that the current reasoning contains complete solution. If hesitation was detected and the solution deemed complete, the generation might be explicitly continued (as per the Python codes continue-after-detected logic) to capture any final utterances before concluding that segment of generation. The overall generation process then decides whether to terminate or proceed based on the pipelines state. A.3 Scaling of FlexiVe Modes on ProcessBench This section details the performance and token usage of FlexiVe when operating in its deliberative \"With Thinking\" (Think@k) mode versus its efficient \"Without Thinking\" (NoThinking@k) mode. The experiments were conducted on subsets of the ProcessBench benchmark (GSM8K, MATH, OlympiadBench, and OmniMATH) across various sampling budgets (k). Table 2 shows the F1 scores and total token consumption for FlexiVe in \"With Thinking\" mode. This mode generally achieves higher F1 scores, especially as the sampling budget increases, but at significantly higher token cost. 15 LLM Detection Prompt Wait, double-check, Alternatively, Hmm, Let me check, Alright, make sure, Another way, Let me verify, to confirm, Looking back, But wait Figure 12: representative set of hesitation keywords monitored in the reasoning trace to detect potential solution completion (Appendix). (Requires customtakeaway environment definition.) LLM Detection Prompt You are solution completeness checker. Given current solution to math problem, determine if it is complete solution (i.e., contains final answer). Respond with exactly one word: Yes if complete, No otherwise. Figure 13: LLM Detection Prompt (Appendix). Conversely, Table 3 presents the results for the \"Without Thinking\" mode. This mode is significantly more token-efficient, though it generally results in lower F1 scores compared to the \"With Thinking\" mode at equivalent sampling budgets. The trade-off between accuracy and computational cost is evident when comparing these two modes. Token Efficiency Summary The \"Without Thinking\" mode demonstrates substantial token savings compared to the \"With Thinking\" mode: GSM8K: \"Without Thinking\" uses approximately 84.5% fewer tokens. MATH: \"Without Thinking\" uses approximately 71.0% fewer tokens. OlympiadBench: \"Without Thinking\" uses approximately 77.6% fewer tokens. OmniMATH: \"Without Thinking\" uses approximately 76.8% fewer tokens. Average: On average, the \"Without Thinking\" mode uses approximately 77.5% fewer tokens than the \"With Thinking\" mode across these datasets. This highlights the efficiency of the \"NoThinking@k\" approach for scenarios where computational budget is primary constraint, while \"Think@k\" is preferable for achieving higher accuracy when more resources are available. The adaptive FlexiVe (Flex@k) mode, discussed in the main paper (Section 4.2), aims to balance these two extremes. A.3.1 Supplementary Figures and Tables from Main Text Comments Table 2: Performance of FlexiVe \"With Thinking\" (Think@k) under different sampling budgets (k) on ProcessBench subsets. Tokens are total generated across the respective test set. GSM8K MATH OlympiadBench OmniMATH Voting Budget (k) F1 (%) Tokens F1 (%) Tokens F1 (%) Tokens F1 (%) Tokens 2 4 8 16 32 64 82.3 86.7 86.4 87.6 87.7 87.8 88.1 2,412,972 4,773,358 9,534,029 19,169,102 38,055,768 76,325,097 152,675,054 81.9 86.4 88.9 89.7 89.7 90.1 90.0 5,209,255 10,416,363 20,913,932 41,778,727 83,807,676 167,497,140 335,401,726 78.0 84.3 85.4 86.5 86.7 86.7 86.7 8,428,333 16,779,943 33,417,171 66,852,313 133,587,678 267,287,483 534,138, 71.3 76.9 78.9 80.1 80.6 80.4 80.5 7,055,913 14,283,830 28,633,370 57,096,638 114,215,045 228,408,308 456,401,199 16 Table 3: Performance of FlexiVe \"Without Thinking\" (NoThinking@k) under different sampling budgets (k) on ProcessBench subsets. Tokens are total generated across the respective test set. OmniMATH MATH OlympiadBench GSM8K Voting Budget (k) F1 (%) Tokens F1 (%) Tokens F1 (%) Tokens F1 (%) Tokens 2 4 8 16 32 64 128 61.5 66.8 66.7 66.8 66.5 66.8 66.7 362,849 737,332 1,490,192 2,973,364 5,936,588 11,833,305 23,715, 57.2 61.3 62.8 64.3 64.4 64.2 65.0 1,516,537 3,040,918 6,090,996 12,107,246 24,247,615 48,501,840 96,833,463 49.0 53.8 55.2 55.9 55.9 56.1 56.3 1,879,631 3,725,119 7,505,333 15,025,214 29,940,405 59,802,922 119,821,725 50.5 52.5 53.6 54.2 54.7 54.0 54.1 1,634,222 3,317,988 6,626,085 13,258,722 26,531,060 52,945,921 105,854, Table 4: ProcessBench results reported with F1 scores. Results for FlexiVe are highlighted . bold indicates the best in the sub category. All FlexiVe variants are trained on only 1526 samples. Model # Samples GSM8K MATH Olympiad Bench OmniMATH Avg. GPT-4o-0806 o1-mini Proprietary Models unk unk 79.2 93.2 Open Source Models (1.5B) Skywork-PRM-1.5B GenPRM-1.5B (Pass@1) w/ Code Exec unk 23K 59.0 52.8 Open Source Models (7-8B) Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B EurusPRM-Stage2 Qwen2.5-Math-PRM-7B RetrievalPRM-7B Universal-PRM-7B Direct Generative PRM-7B GenPRM-7B w/ Code Exec (Pass@1) GenPRM-7B w/ Code Exec (Maj@8) 445K 273K 30K 344K 404K unk 23K 23K 23K 47.9 50.4 47.3 82.4 74.6 85.8 63.9 78.7 81. 63.6 88.9 48.0 66.6 29.5 33.4 35.7 77.6 71.1 77.7 65.8 80.3 85.7 Open Source Models (14-32B) w/ Moderate Compute Dyve-14B GenPRM-32B w/o Code Exec (Maj@8) FlexiVe (Flex@32) FlexiVe (Flex@128) 117K 23K 1526 68.5 78.8 82.8 83.0 58.3 85.1 83.3 85.0 Open Source Models (14-32B) w/ High Compute GenPRM-32B (Pass@1) w/ Code Exec GenPRM-32B (Maj@8) w/ Code Exec FlexiVe (Think@64) 23K 23K 1526 83.1 85.1 88. 81.7 86.3 90.1 51.4 87.2 19.3 55.1 24.8 13.8 21.2 67.5 60.2 67.6 54.5 72.2 78.4 49.0 78.7 79.2 80.0 72.8 78.9 86. 53.5 82.4 19.2 54.5 23.8 15.8 20.9 66.3 57.3 66.4 55.9 69.8 76.8 47.2 74.9 73.4 75.2 72.8 80.1 80.4 61.9 87. 36.4 57.3 31.5 28.4 31.3 73.5 65.8 74.3 60.0 75.2 80.5 55.8 79.3 79.7 80.8 77.6 82.6 86.3 17 Figure 14: F1 score scaling with voting budget on GSM8K (left) and MATH (right). FlexiVe (Flex@k, green circles) improves with larger k, performing comparably or better than DS14B (blue triangles, baseline verifier), while both surpass the FlexiVe (NoThinking variant, red squares). (Previously commented out from main text)."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong"
    ]
}