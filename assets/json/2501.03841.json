{
    "paper_title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "authors": [
        "Mingjie Pan",
        "Jiyao Zhang",
        "Tianshu Wu",
        "Yinghao Zhao",
        "Wenlong Gao",
        "Hao Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 1 4 8 3 0 . 1 0 5 2 : r OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints Mingjie Pan1,2, Jiyao Zhang1,2, Tianshu Wu1, Yinghao Zhao3, Wenlong Gao3, Hao Dong1,2 1CFCS, School of CS, Peking University 2PKU-AgiBot Lab 3AgiBot https://omnimanip.github.io Figure 1. We proposed OmniManip, an open-vocabulary manipulation method that bridges the gap between the high-level reasoning of vision-language models (VLM) and the low-level precision, featuring closed-loop capabilities in both planning and execution."
        },
        {
            "title": "Abstract",
            "content": "The development of general robotic systems capable of manipulating in unstructured environments is significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the finegrained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose novel object-centric representation that bridges the gap between VLMs high-level reasoning and the low-level precision required for manipulation. Our key insight is that an objects canonical space, defined by its functional affordances, provides structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as bridge, translating VLMs commonsense reasoning into actionable 3D spatial In this context, we introduce dual closedconstraints. loop, open-vocabulary robotic manipulation system: one *: Equal contributions. : Corresponding author loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM finetuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation. 1. Introduction Developing general robotic manipulation system has long been challenging task, primarily due to the complexity and variability of real-world [26, 47, 48]. Inspired by the rapid advancements in Large Language Models (LLM)[1, 42] and Vision-Language Models (VLM) [25, 28, 34, 54], which leverage vast amounts of internet data to acquire rich commonsense knowledge, researchers have recently turned attention to exploring their application in robotics[14, 53]. Most existing works focus on utilizing this knowledge for high-level task planning, such as semantic reasoning [4, 31, 37]. Despite these advances, current VLMs, primarily trained on extensive 2D visual data, lack the 3D spatial understanding ability necessary for precise, low-level manipulation tasks. This limitation poses challenges in manipulations within unstructured environments. One approach to overcoming this limitation is to finetransforming tune VLM on large-scale robotic datasets, them into VLA [2, 3, 8, 19]. However, this faces two major challenges: 1) acquiring diverse, high-quality robotic data is costly and time-consuming, and 2) fine-tuning VLM into VLA results in agent-specific representations, which are tailored to specific robots, limiting their generalizability. promising alternative is to abstract robotic actions into interaction primitives (e.g., points or vectors) and leverage VLM reasoning to define the spatial constraints of these primitives, while traditional planning algorithms handle execution [13, 15, 27]. However, existing methods for defining and using primitives have several limitations: The process of generating primitive proposals is task-agnostic, which poses the risk of lacking suitable proposals. Additionally, relying on manually designed rules for post-processing proposals also introduces instability. This naturally leads to an important question: How can we develop more efficient and generalizable representations that bridge VLM high-level reasoning with precise, low-level robotic manipulation? To address this challenge, we propose novel objectcentric intermediate representation incorporating interaction points and directions within an objects canonical space. This representation bridges the gap between VLMs high-level commonsense reasoning and precise 3D spatial understanding. Our key insight is that an objects canonical space is typically defined based on its functional affordances. As result, we can describe an objects functionality in more structured and semantically meaningful way within its canonical space. Meanwhile, recent advancements in universal object pose estimation [7, 55, 56] make it feasible to canonicalize wide range of objects. Specifically, we employ universal 6D object pose estimation model [56] to canonicalize objects and describe their rigid transformations during interactions. In parallel, single-view 3D generation network generates detailed object meshes [29, 40]. Within the canonical space, interaction directions are initially sampled along the objects principal axes, providing coarse set of interaction possibilities. Meanwhile, the VLM predicts interaction points. Subsequently, the VLM identifies task-relevant primitives and estimates the spatial constraints between them. To address the hallucination issue in VLM reasoning, we introduce self-correction mechanism through interaction rendering and primitive resampling that enables closed-loop reasoning. Once the final strategy is determined, actions are computed through constrained optimization, with pose tracking ensuring robust, real-time control in closed-loop execution phase. Our method offers several key advantages: 1) Efficient and Effective Interaction Primitive Sampling: By leveraging the objects canonical space, our approach enables efficient and effective sampling of interaction primitives, enhancing the systems reasoning capabilities. 2) Dual Closed-Loop, Open-Vocabulary Robotic Manipulation System: Benefiting from the proposed object-centric intermediate representation, our method implements dual closed-loop system. The rendering and resampling process drives reasoning loop for decision-making, while pose tracking ensures closed loop for action execution. In summary, our contributions are threefold: We propose novel object-centric interaction representation that bridges the gap between VLMs high-level commonsense reasoning and low-level robotic manipulation. To the best of our knowledge, we are the first to present planning and execution dual closed-loop open-vocabulary manipulation system without VLM fine-tuning. Extensive experiments demonstrate our methods strong zero-shot generalization across diverse manipulation tasks, and we also highlight its potential for automating robotic manipulation data generation. 2. Related Work Foundation Models For Robotics The emergence of foundation models has significantly influenced the field of robotics[11, 18, 51], particularly in the application of vision-language models[1, 4, 12, 23, 28, 50], which excel in environment understanding and high-level commonsense reasoning. These models demonstrate the potential for controlling robots to perform general tasks in novel and unstructured environments. Some studies [2, 3, 19, 24] have fine-tuned VLM on robotics datasets to create VLA models that output robotic trajectories, but these efforts are limited by the high cost of data collection and issues with generalization. Other approaches attempt to extract operation primitives using visual foundation models [9, 13, 15, 21, 27, 33, 52], which are then used as visual or language prompts for VLM to perform high-level commonsense reasoning, combined with motion planners [38, 39, 41] for low-level control. However, these methods are constrained by the ambiguity of compressing 3D primitives into the 2D images or 1D text required by VLM and the hallucination tendencies of VLM themselves, making it difficult to ensure that the high-level plans generated by VLM are accurate. In this work, we demonstrate OmniManips unique advantages in addressing these challenges, particularly in fine-grained 3D understanding and mitigating large model hallucinations. Representations for Manipulation Structural representations determine the capabilities and effectiveness of manipulation methods. Among various types of representations, keypoints are popular choice due to their flexibility, generalization, and ability to model variability [32, 35, 36, 46]. However, these keypoints-based methods require Figure 2. Overview framework. Given instruction and RGB-D observation marked by VFM, VLM firstly filters task-related objects and partitions the task into stages. For each stage, VLM extracts object-centric canonical interaction primitives as spatial constraints in closed-loop manner. For execution, the trajectory is optimized by constraints and updated in closed loop using 6D Pose Tracker. manual task-specific annotations to generate actions. To enable zero-shot open-world manipulation, studies such as [15, 27, 33] have transformed keypoints into visual prompts for VLM, facilitating the automatic generation of high-level planning results. Despite their advantages, keypoints can be unstable; they struggle under occlusion and pose challenges in the extraction and selection of specific keypoints. Another common representation is the 6D pose, which efficiently defines long-range dependencies between objects for manipulation and offers degree of robustness to occlusion [16, 17, 44, 45]. However, these methods necessitate prior modeling of geometric relationships and, due to the sparse nature of poses, cannot provide fine-grained geometry. This limitation can lead to failures in manipulation strategies across different objects due to intra-class variations. To address these issues, OmniManip combines the fine-grained geometry of keypoints with the stability of the 6D pose. It automatically extracts detailed functional points and directions within the canonical coordinate system of objects using VLM, enabling precise manipulation. 3. Method Here we discuss: (1) How do we formulate robotic manipulation via interaction primitives as spatial constraints(Sec. 3.1)? (2) How to extract canonical interaction primitives in generic and open vocabulary way (Sec. 3.2)? (3) Why can OmniManip achieve dual closed-loop system (Sec. 3.3)? 3.1. Manipulation with Interaction Primitives In our formulation, complex robotic tasks are decomposed into stages, each defined by object interaction primitives with spatial constraints. This structured approach allows for the precise definition of task requirements and facilitates the execution of complex manipulation tasks. In this section, we detail how interaction primitives serve as the foundation for spatial constraints, enabling robust manipulation. Task Decomposition. As shown in Figure 2, given manipulation task (e.g., pouring tea into cup), we first utilize GroundingDINO[30] and SAM[20], two Visual Foundation Models (VFMs), to mark all foreground objects in the scene like [49] as visual prompt. Subsequently, VLM [1] is employed to filter task-relevant obi , Opassive Figure 3. Interaction points generation. jects and decompose the task into multiple stages = {S1, S2, . . . , Sn}, where each stage Si can be formalized as Si = {Ai, Oactive }, where Ai represents the action and Opassive to be performed (e.g., grasp, pour), and Oactive refer to the object initiating the interaction and the object being acted upon, respectively. For example, in Figure 2, the teapot is the passive object in the stage of grasping the teapot while the teapot is the active object and the cup is passive in the stage of pouring tea into the cup. Object-Centric Canonical Interaction Primitives. We propose novel object-centric representation with canonical interaction primitives to describe how objects interact during manipulation tasks. Specifically, an objects interaction primitives are characterized by its interaction point and direction in canonical space. The interaction point R3 denotes key location on the object where interaction occurs, while the interaction direction R3 represents the primary axis relevant to the task. Together, these form the interaction primitive = {p, v}, encapsulating the essential intrinsic geometric and functional properties required to meet task constraints. These canonical interaction primitives are defined relative to their canonical space, remaining consistent across different scenarios, enabling more generalized and reusable manipulation strategies. Interaction Primitives with Spatial Constraints. At each stage Si, set of spatial constraints Ci governs the spatial relationships between the active and passive objects. These constraints are divided into two categories: distance constraints di, which regulate the distance between interaction points, and angular constraints θi, which ensure proper alignment of interaction directions. Together, these constraints define the geometric rules necessary for precise spatial alignment and task execution. The overall spatial constraint for each stage Si is given by: Ci = (cid:110) Oactive , Opassive , di, θi (cid:111) (1) Once the constraints Ci have been defined, the task execution can be formulated as an optimization problem. Figure 4. Interaction directions extraction. 3.2. Primitives and Constraints Extraction In this section, we detail the process of extracting interaction primitives and their spatial constraints for each stage. As illustrated in Figure 2, we first obtain 3D object meshes for both the task-relevant active and passive objects via single-view 3D generation [29, 40, 57], followed by pose estimation with Omni6DPose[56] for object canonicalization. Next, we extract task-relevant interaction primitives and their corresponding constraints. Grounding Interaction Point. As shown in Figure 3, interaction points are categorized as Visible and Tangible (e.g., teapot handle) or Invisible or Intangible (e.g., the center of its opening). To enhance VLM for interaction points grounding, SCAFFOLD [22] visual prompting mechanism is employed, which overlays Cartesian grid onto the input image. Visible points are directly localized in the image plane, while invisible points are inferred through multi-view reasoning based on proposed canonical object representations, as illustrated in Figure 3. Reasoning begins from the primary viewpoint, with ambiguities resolved by switching to an orthogonal view. This approach enables more flexible and reliable interaction point grounding. For tasks like grasping, heatmaps are generated from multiple interaction points, improving the robustness of the grasping model. Sampling Interaction Direction. In the canonical space, the principal axes of an object are often functionally relevant. As illustrated in Figure 4, we treat the principal axes as candidate interaction directions. However, assessing the relevance of these directions to the task is challenging due to the limited spatial understanding of the current VLM. To address this, we propose VLM caption and LLM scoring mechanism: first, we use the VLM to generate semantic descriptions for each candidate axis, and then employ LLM to infer and score the relevance of these descriptions to the task. This process results in an ordered set of candidate directions that are most aligned with the task requirements. Ultimately, the interaction primitives with constraints are generated with VLM, yielding an ordered list of constrained interaction primitives for each stage Si, denoted as Ki = {C (1) , . . . , (N ) , (2) }. 3.3. Dual Closed-Loop System As outlined in Section 3.2, we obtain the interaction primitives of the active and passive objects, denoted as Oactive and Opassive, respectively, along with the spatial constraints that define their spatial relationships. However, this is an open-loop inference, which inherently limits the robustness and adaptability of the system. These limitations arise primarily from two sources: 1) the hallucination effect in large models, and 2) the dynamic nature of real-world environments. To overcome these challenges, we propose dual closed-loop system, as illustrated in Figure 2. (cid:111) (cid:110) C(1) , C(2) Algorithm 1 Self-Correction Algorithm via RRC Input: Task , Stage Si, Initial List of Primitives with Con- , . . . , C(N ) straints Ki = Output: Successful Constraints ˆCi or Task Failure 1: 1, maxSteps , ref ine False 2: while maxSteps do 3: 4: + 1 Render: Ii Render(C(k) Check: state VLM(T , Si, Ii, C(k) if state = Refine and ref ine = False then Resample: Update Ki Resample(C(k) ) 1, maxSteps , ref ine True , ref ine) 5: 6: ) i else if state = Success then 7: 8: 9: 10: end if 11: 12: end while 13: return Task Failed return C(k) Closed-loop Planning. To improve the accuracy of interaction primitives and mitigate hallucination issues in the VLM, we introduce self-correction mechanism based on Resampling, Rendering, and Checking (RRC). This mechanism uses real-time feedback from visual language model (VLM) to detect and correct interaction errors, ensuring precise task execution. The RRC process consists of two stages: the initial phase and the refinement phase. The overall RRC mechanism is outlined in Algorithm 1. In the initial phase, the system evaluates the interaction constraints Ki defined in Section 3.2, which specify the spatial relationships between active and passive objects. For each constraint C(k) , the system renders an interaction image Ii based on the current configuration and submits it to the VLM for validation. The VLM returns one of three outcomes: success, failure, or refinement. If success, the conIf failure, the straint is accepted, and the task proceeds. next constraint is evaluated. If refinement, the system enters the refinement phase for further optimization. In the refinei around vi and evaluates them. ment phase, the system performs fine-grained resampling around the predicted interaction direction vi to correct misalignments between the functional and geometric axes of objects. The system uniformly samples six refined directions v(j) Closed-loop Execution. Once the interaction primitives and the corresponding spatial constraints are defined for each stage, the task execution can be formulated as an optimization problem. The objective is to minimize the loss function to determine the target pose Pee of the endeffector. The optimization problem can be expressed as: (cid:88) Pee = arg min Pee Lj(Pee) , s.t. j= (2) = {LC, Lcollision, Lpath}, where the constraint loss LC ensures that the action adheres to the tasks spatial constraints C, and is defined as LC = ρ(C, Pactive , Ppassive ), where Pactive = Φ(Pee ) (3) Here, ρ() measures the deviation between the current spatial relationship of the active object Pactive and the passive object Ppassive from the desired constraint C, while Φ() maps the end-effector pose to the active objects pose. The collision loss Lcollision prevents the end-effector from colliding with obstacles in the environment and is defined as Lcollision = (cid:88) j= max (0, dmin d(Pee, Oj))2 , (4) where d(Pee, Oj) represents the distance between the end-effector and the obstacle Oj, and dmin is the minimum allowable safety distance. The path loss Lpath ensures smooth motion and is defined as Lpath = λ1dtrans(Pee , Pee) + λ2drot(Pee , Pee), (5) where dtrans() and drot() represent the translational and rotational displacements of the end-effector, respectively, and λ1 and λ2 are weighting factors that balance the influence of translation and rotation. By minimizing these loss functions, the system dynamically adjusts the end-effector pose Pee, ensuring successful task execution while avoiding collisions and maintaining smooth motion. While Equation 3 outlines how interaction primitives and their corresponding spatial constraints can be leveraged to optimize the executable end-effector pose, real-world task execution often involves significant dynamic factors. For instance, deviations in the grasp pose may result in unintended object movement during grasping task. Moreover, in certain dynamic environments, the target object may be displaced. These challenges highlight the critical importance of closed-loop execution in handling such uncertainties. To address these challenges, our system leverages the Tasks VoxPoser CoPa ReKep Auto OmniManip(Ours) Closed-loop Open-loop"
        },
        {
            "title": "Pour tea\nInsert flower into vase\nInsert the pen in holder\nRecycle the battery\nPick up the cup on the dish\nFit the lid onto the teapot",
            "content": "0/10 0/10 0/10 6/10 3/10 0/10 1/10 4/10 4/10 5/10 2/10 2/10 3/10 2/10 3/10 7/10 9/10 3/10 7/10 6/10 7/10 8/10 8/10 5/10 6/10 4/10 5/10 6/10 7/10 3/10 Total 15.0% 30.0% 45.0% 68.3% 51.7%"
        },
        {
            "title": "Open the drawer\nClose the drawer\nHammer the button\nPress the red button\nClose the lid of the laptop\nOpen the jar",
            "content": "1/10 3/10 0/10 0/10 4/10 2/10 4/10 3/10 3/10 3/10 3/10 0/10 Total 16.7% 26.7% - - - - - - - 6/10 8/10 4/10 7/10 6/10 6/10 4/10 6/10 2/10 6/10 4/10 5/10 61.7% 45.0% Table 1. Quantitative results across 12 real-world manipulation tasks. The first six tasks focus on rigid object manipulation, while the latter involves articulated object manipulation. - indicates that the method can not handle this task due to its underlying principles. proposed object-centric interaction primitives and directly employs an off-the-shelf 6D object pose tracking algorithm to continuously update the poses of both the active object Pactive in real-time, as ret quired in Equation 4. This real-time feedback allows for dynamic adjustments to the target pose of the end-effector, enabling robust and accurate closed-loop execution. and the passive object Ppassive 4. Experiment In this section, we aim to answer the following questions: (1) To what extent does OmniManip perform effectively in open-vocabulary manipulation tasks across diverse realworld scenarios (Section 4.2)? (2) What role do the systems critical features play in enhancing its overall performance (Section 4.3)? (3) How promising is OmniManip for automating the collection of robot manipulation trajectories to enable scalable imitation learning (Section 4.4)? 4.1. Experimental Setup Hardware Configuration. Our experimental platform is built around Franka Emika Panda robotic arm, with its parallel grippers fingers replaced by UMI fingers[6]. For perception, we employ two Intel RealSense D415 depth cameras. One camera is mounted at the gripper to provide first-person view of the manipulation area, while the second camera is positioned opposite the robot to offer thirdperson view of the workspace. Tasks and Metrics. As shown in Figure 1, We designed 12 tasks to evaluate models manipulation capabilities in realworld scenarios. Six of these involve rigid object manipulation (e.g., pour tea), while the others focus on articulated manipulation (e.g., open the drawer). These tasks cover diverse set of objects and are intended to assess the models ability to generalize and adapt in complex environments. For each task, 10 trials were performed for each approach, and the success rate was recorded. After each trial, the object layout was reconfigured to ensure robust evaluation. Baselines. We compare our approach with three baselines: 1) VoxPoser[14], which uses LLM and VLM to generate 3D value maps for synthesizing robot trajectories, excelling in zero-shot learning and closed-loop control; 2) CoPa[13], which introduces spatial constraints of object parts and combines with VLM to enable open-vocabulary manipulation; and 3) ReKep[15], which employs relational keypoint constraints and hierarchical optimization for realtime action generation from natural language instructions. Implement Details We use GPT-4O from OpenAI API as the vision-language model, leveraging small set of interaction examples as prompts to guide the models reasoning for manipulation tasks. The specific prompts used are detailed in the appendix. We employ off-the-shelf models [10, 43] for 6-DOF universal grasping and utilize GenPose++[56] for universal 6D pose estimation. 4.2. Open-Vocabulary Manipulation We conducted comprehensive evaluation of OmniManip on 12 open-vocabulary manipulation tasks, ranging from straightforward actions such as pick-and-place to more complex tasks involving object-object interactions with directional constraints and articulated object manipulation. As shown in Table 1, our method exhibits robust zero-shot generalization and superior performance across the board without task-specific training. This generalization capability can be attributed to the commonsense knowledge emMethod 0 25 45 75 90 ReKep OmniManip 0/10 7/ 1/10 8/10 3/10 8/10 5/10 7/10 7/10 7/10 Table 2. Quantitative analysis of the impact of viewpoints on the performance, using Recycle the battery as case study. bedded in VLM, while the proposed efficient object-centric interaction primitives facilitate precise 3D perception and execution. Additionally, we provide qualitative results in the appendix. OmniManip exhibits substantial performance advantage over baseline methods, primarily due to two key factors: 1) the efficiency and stability of the proposed object-centric canonical interaction primitives, as further validated through extensive experiments in Section 4.3, and 2) the advanced dual closed-loop system for planning and execution. By incorporating novel self-correction mechanism based on RRC, the system effectively mitigates hallucination issues of large models. As shown in Table 1, this closed-loop planning yields over 15% improvement in performance for both rigid and articulated object manipulation tasks. detailed qualitative analysis of the closed-loop reasoning and execution is provided in Section 4.3. 4.3. Core Attributes of OmniManip Reliability of OmniManip. To effectively bridge VLM with low-level manipulation, reliable interaction primitives are crucial. We evaluate this across two key dimensions: stability and viewpoint consistency. Stability indicates the reliable extraction of task-relevant interaction primitives. As shown in Figure 5, ReKep extracts keypoint proposals through semantic clustering but lacks sensitivity to spatial geometry and task, making it challenging to generate sufficient task-relevant keypoints. CoPa extracts parts via explicit pixel segmentation, exhibiting high sensitivity to image texture and part shape. In contrast, OmniManip, an object-centric interaction primitive, samples interaction points in canonical space aligned with the objects functionality, ensuring both robustness and task-specific precision. Consistency of primitive extraction across varying viewpoints is critical to ensuring the stability of manipulation. Both ReKep and CoPa exhibit difficulties in this regard due to their reliance on sampling points directly from the objects surface. Taking ReKep as an example, Figure 6 illustrates the planning results of ReKep and OmniManip for the Recycle battery task across different viewpoints. As shown, ReKep successfully identifies interaction points from 90 top-down view but fails under 0 frontal view, where the ideal target point is floating in the air. In contrast, OmniManip utilizes an object-centric primitive representation in canonical space, ensuring viewpoint invariance. Table 2 presents the quantitative comparison, demonstrating that OmniManips performance is nearly invariant across varying viewpoints, whereas ReKeps performance Figure 5. Stability analysis of interaction primitives. Visualization of planning and corresponding execution results across different methods, demonstrated using the Pour tea as case study. Figure 6. Qualitative analysis of the impact of viewpoints on the performance, using Recycle the battery as case study. is significantly affected by changes in viewpoint. Sampling Method Recycle Battery Iter. Suc. Rate Pour Tea Suc. Rate Iter. Uniform OmniManip 50% 80% 1.8 1. 30% 70% 3.4 1.8 Table 3. Quantitative analysis of the primitive sampling efficiency. Efficiency of OmniManip. Interaction direction proposals in OmniManip are driven by targeted sampling strategy. Compared with uniform sampling in SO(3), OmniManip samples along the principal axes of the objects canonical space. Since the canonical space is aligned with the objects functionality, this ensures both efficient and effective sampling. To evaluate this efficiency, we compared OmniManips sampling strategy with uniform sampling in SO(3) using two key metrics: the number of iterations and the corresponding task success rate. As shown in Table 3 OmniManip not only requires fewer iterations but also achieves superior task performance, demonstrating that aligning the sampling process with the objects functionality reduces sampling overhead while improving overall performance. Closed-Loop Planning. the planning component of VLM operates in an open-loop manIn current methods, Figure 7. Closed-planning. Self-correction mechanism via RRC. Figure 8. Two typical failure cases without closed-loop execution. ner, meaning it cannot verify the correctness of the plan before execution. While ReKep achieves closed-loop control through point tracking, this only functions at the execution stage and does not provide feedback on the planning results generated by the VLM. In contrast, OmniManip introduces unique self-correction mechanism via RRC, achieving closed-loop planning, which significantly reduces planning failures caused by VLM hallucinations, thereby offering more reliable planning. We report the results with closed-loop planning disabled in Table 1, where the task success rate decreases by over 15% in both rigid and articulated object manipulation tasks, demonstrating the effectiveness of the closed-loop planning approach. In Figure 7, we qualitatively illustrate the closed-loop planning results using the Insert the pen in holder task as an example. It is evident that OmniManip can effectively pre-render the planning outcomes and achieve self-correction through the RRC process, thereby enabling closed-loop planning. Task Success Rate Pick up the cup on the dish Recycle the battery Insert the pen in holder 95.24% 91.30% 86.36% Table 4. Behavior cloning with demonstrations from OmniManip. Closed-Loop Execution. Even with perfect planning, open-loop execution can still lead to task failure. Figure 8 illustrates two typical examples where planning succeeds, but open-loop execution causes failure. In the left image of Figure 8, the relative pose between the gripper and the object changes during the interaction, while the right image of Figure 8 shows scenario where the target pose is dynamic, such as when the object moves during the task. To address these challenges, OmniManip employs pose tracking to enable real-time closed-loop execution. Recent work, ReKep, uses point tracking for closed-loop control but suffers from occlusions, leading to 47% failure rate [15]. In contrast, OmniManip demonstrates greater robustness to occlusions caused by object movement. This is benefit of object-centric pose tracking, enabling continued tracking of canonical space interaction primitives based on the object pose, even when the primitives are no longer visible. 4.4. OmniManip for Demonstration Generation We employed OmniManip to generate automatic demonstration data. Unlike prior methods reliant on task-specific privileged information, OmniManip collects demonstration trajectories for new tasks in zero-shot manner, without needing task-specific details or prior object knowledge. To validate the effectiveness of OmniManip-generated data, we collected 150 trajectories per task to train behavior cloning policies [5]. These policies achieved high success rates, as shown in Table 4. Additional tasks and detailed results are provided in the appendix. 5. Conclusion In this work, we presented novel object-centric intermediate representation that effectively bridges the gap between VLM and the precise spatial reasoning required for robotic manipulation. We structured interaction primitives in object canonical space to translate high-level semantic reasoning into actionable 3D spatial constraints. The proposed dual closed-loop system ensures robust decision-making and execution, all without VLM fine-tuning. Our approach demonstrates strong zero-shot generalization across variety of manipulation tasks, highlighting its potential for automating robotic data generation and improving the efficiency of robotic systems in unstructured environments. This work provides promising foundation for future research into scalable, open-vocabulary robotic manipulation. Limitations. While advantageous, OmniManip also has limitations. It cannot model deformable objects due to pose representation. Its effectiveness also hinges on the mesh quality of 3D AIGC, which remains challenging despite progress. Additionally, multiple VLM calls present computational challenges, even with parallel processing."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Mingdong Wu and Tianhao Wu from PKU for their fruitful discussions, and Baifeng Xie from AgiBot for valuable technical support."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 3 [2] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 2 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2 [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 1, 2 [5] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 8 [6] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and ShuIn-the-wild ran Song. Universal manipulation interface: robot teaching without in-the-wild robots. arXiv preprint arXiv:2402.10329, 2024. 6 [7] Qiyu Dai, Jiyao Zhang, Qiwei Li, Tianhao Wu, Hao Dong, Ziyuan Liu, Ping Tan, and He Wang. Domain randomizationenhanced depth simulation and restoration for perceiving and grasping specular and transparent objects. In European Conference on Computer Vision, pages 374391. Springer, 2022. [8] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2 [9] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulateanything: Automating real-world robots using visionlanguage models. arXiv preprint arXiv:2406.18915, 2024. 2 [10] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 2023. 6 [11] Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, page 02783649241281508, 2023. 2 [12] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. [13] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv preprint arXiv:2403.08248, 2024. 2, 6 [14] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. 1, 6 [15] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. 2, 3, 6, 8 [16] Leslie Pack Kaelbling and Tomas Lozano-Perez. Hierarchical task and motion planning in the now. In 2011 IEEE International Conference on Robotics and Automation, pages 14701477. IEEE, 2011. 3 [17] Leslie Pack Kaelbling and Tomas Lozano-Perez. Integrated task and motion planning in belief space. The International Journal of Robotics Research, 32(9-10):11941227, 2013. 3 [18] Kento Kawaharazuka, Tatsuya Matsushima, Andrew Gambardella, Jiaxian Guo, Chris Paxton, and Andy Zeng. Realworld robot applications of foundation models: review. Advanced Robotics, pages 123, 2024. 2 [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3 [21] Olivia Lee, Annie Xie, Kuan Fang, Karl Pertsch, and Chelsea Finn. Affordance-guided reinforcement learning via visual prompting. arXiv preprint arXiv:2407.10341, 2024. 2 [22] Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. arXiv preprint arXiv:2402.12058, 2024. 4 [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [24] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. 2 [25] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 1 [26] Chang Liu, Kejian Shi, Kaichen Zhou, Haoxiao Wang, Jiyao Zhang, and Hao Dong. Rgbgrasp: Image-based object grasping by capturing multiple views during robot arm movement with neural radiance fields. IEEE Robotics and Automation Letters, 2024. [27] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. 2, 3 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2 [29] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10072 10083, 2024. 2, 4 [30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3 [31] Zeyi Liu, Arpit Bahety, and Shuran Song. Reflect: Summarizing robot experiences for failure explanation and correction. arXiv preprint arXiv:2306.15724, 2023. 1 [32] Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake. kpam: Keypoint affordances for category-level robotic manipulation. In The International Symposium of Robotics Research, pages 132157. Springer, 2019. [33] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. 2, 3 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [35] Tanner Schmidt, Richard Newcombe, and Dieter Fox. Selfsupervised visual descriptor learning for dense corresponIEEE Robotics and Automation Letters, 2(2):420 dence. 427, 2016. 2 [36] Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann. Neural descriptor fields: Se (3)- In equivariant object representations for manipulation. 2022 International Conference on Robotics and Automation (ICRA), pages 63946400. IEEE, 2022. [37] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1152311530. IEEE, 2023. 1 [38] Ioan Sucan, Mark Moll, and Lydia Kavraki. The open motion planning library. IEEE Robotics & Automation Magazine, 19(4):7282, 2012. 2 [39] Balakumar Sundaralingam, Siva Kumar Sastry Hari, Adam Fishman, Caelan Garrett, Karl Van Wyk, Valts Blukis, Alexander Millane, Helen Oleynikova, Ankur Handa, Fabio Ramos, et al. Curobo: Parallelized collision-free robot motion generation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 81128119. IEEE, 2023. 2 [40] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 2, 4 [41] Marc Toussaint, Jason Harris, Jung-Su Ha, Danny Driess, and Wolfgang Honig. Sequence-of-constraints mpc: Reactive timing-optimal control of sequential manipulation. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1375313760. IEEE, 2022. 2 [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [43] Chenxi Wang, Hao-Shu Fang, Minghao Gou, Hongjie Fang, Jin Gao, and Cewu Lu. Graspness discovery in clutters for In Proceedings of the fast and accurate grasp detection. IEEE/CVF International Conference on Computer Vision, pages 1596415973, 2021. 6 [44] Bowen Wen, Wenzhao Lian, Kostas Bekris, and Stefan Schaal. You only demonstrate once: Category-level manipulation from single visual demonstration. arXiv preprint arXiv:2201.12716, 2022. 3 [45] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17868 17879, 2024. 3 [46] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. 2 [47] Tianhao Wu, Jinzhou Li, Jiyao Zhang, Mingdong Wu, and Hao Dong. Canonical representation and force-based pretraining of 3d tactile for dexterous visuo-tactile policy learning. arXiv preprint arXiv:2409.17549, 2024. 1 [48] Tianhao Wu, Mingdong Wu, Jiyao Zhang, Yunchong Gan, and Hao Dong. Learning score-based grasping primitive for human-assisting dexterous grasping. Advances in Neural Information Processing Systems, 36, 2024. [49] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 3 [50] Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, and Shanghang Zhang. Lidar-llm: Exploring the potential of large language models for 3d lidar understanding. arXiv preprint arXiv:2312.14074, 2023. 2 [51] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023. 2 [52] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. 2 [53] Yiming Zeng, Mingdong Wu, Long Yang, Jiyao Zhang, Hao Ding, Hui Cheng, and Hao Dong. Lvdiffusor: Distilling functional rearrangement priors from large models into diffusor. IEEE Robotics and Automation Letters, 2024. 1 [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1 [55] Jiyao Zhang, Mingdong Wu, and Hao Dong. Generative category-level object pose estimation via diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [56] Jiyao Zhang, Weiyao Huang, Bo Peng, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, and Hao Dong. Omni6dpose: benchmark and model for universal 6d object pose estimation and tracking. In European Conference on Computer Vision, pages 199216. Springer, 2025. 2, 4, 6 [57] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view In Proceedings of 3d reconstruction with transformers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1032410335, 2024."
        }
    ],
    "affiliations": [
        "AgiBot",
        "CFCS, School of CS, Peking University",
        "PKU-AgiBot Lab"
    ]
}