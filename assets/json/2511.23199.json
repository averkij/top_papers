{
    "paper_title": "Vision Bridge Transformer at Scale",
    "authors": [
        "Zhenxiong Tan",
        "Zeqing Wang",
        "Xingyi Yang",
        "Songhua Liu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation."
        },
        {
            "title": "Start",
            "content": "Zhenxiong Tan1 Zeqing Wang1 Xingyi Yang2,1 Songhua Liu3,1 Xinchao Wang1 1National University of Singapore 2The Hong Kong Polytechnic University 3Shanghai Jiao Tong University https://Yuanshi9815.github.io/ViBT homepage 5 2 0 2 8 ] . [ 1 9 9 1 3 2 . 1 1 5 2 : r Figure 1. Results of vision bridge transformer on various vision translation tasks."
        },
        {
            "title": "Abstract",
            "content": "We introduce Vision Bridge Transformer (ViBT), largescale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating Corresponding author. (xinchao@nus.edu.sg) an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt Transformer architecture and propose variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation. 1 1. Introduction Generative models have advanced remarkably, beginning with Generative Adversarial Networks (GANs) [16, 25] that enabled high-quality image synthesis via adversarial training. More recently, probability-path methods, especially diffusion models [18, 34, 47], have further elevated generative capabilities. Transformer-based architectures trained at scale have significantly enhanced the fidelity and diversity of synthesized images [8, 15, 42, 60] and videos [9, 51, 54]. Building upon this success, extending these models to conditional vision generation tasks has become natural direction [49, 50, 64, 69]. Typically, these approaches inject visual conditions into the generative process by incorporating source images or videos as auxiliary inputs [64, 69]. Despite their success, the underlying noise-to-vision modeling paradigm widely adopted by these models [15, 15, 16] can be unnatural for conditional generation tasks. In this paradigm, models start from noise and gradually refine it toward the target [8, 29, 54, 60]. However, in many conditional scenarios (e.g., image editing, colorization, and frame interpolation), inputs already closely resemble the desired outputs, making this process unintuitive. Moreover, incorporating additional conditioning tokens introduces substantial computational overhead under transformer architectures [49, 50], especially in video settings [1, 22, 54]. In contrast, the vision-to-vision paradigm provides more intuitive alternative by directly modeling the transformation between structured source and target domains [31, 58]. Unlike the noise-to-vision approach, it explicitly models the direct path from conditioning inputs to target outputs, naturally capturing the strong correlations inherent in the data. Previous works demonstrated the feasibility of vision-to-vision modeling using Bridge Models [31, 35, 72], which construct stochastic processes connecting source and target data distributions. Although Brownian Bridge based formulations [31] have shown promising results, prior work has largely been limited to small-scale architectures and relatively simple tasks [5, 56, 67, 71], leaving their potential for complex vision translation scenarios underexplored . This work introduces Vision Bridge Transformer (ViBT), the first Brownian Bridge Model scaled to large-scale settings for complex vision translation tasks. ViBT leverages transformer architectures [42, 53] initialized from leading flow-matching models [54, 60], inheriting strong generative priors. By scaling ViBT to 20B and 1.3B parameters, we demonstrate the feasibility of applying Bridge Models to previously unexplored tasks within this framework. However, scaling Bridge Models to such large-scale architectures necessitates robust training objective. We observe that conventional displacement-style targets [31] disproportionately bias training toward early generation steps, while naive velocity-based objectives [5, 37] exhibit severe numerical instability, negatively affecting convergence and performance. To address these issues, we propose variancestabilized velocity matching objective, which maintains numerical stability and equally emphasizes learning across all timesteps, facilitating efficient training at scale. Extensive experiments demonstrate that ViBT effectively generalizes to wide range of complex vision translation tasks, including instruction-based image editing, instruction-guided video stylization, depth-to-video synthesis, image coloring, and video frame interpolation, achieving results competitive with traditional conditional diffusion methods while being significantly more efficient. Comprehensive ablation studies further verify the effectiveness of the variance-stabilized velocity matching objective. 2. Related Works Generative models Early generative models, such as Variational Autoencoders (VAEs) [44] and Generative Adversarial Networks (GANs) [16, 24], enabled high-quality synthesis through latent modeling and adversarial training. Diffusion models [18, 46] later introduced iterative denoising processes, significantly advancing image and video generation. Flow-matching models [15, 34] further reframed generation as learning deterministic or stochastic paths between distributions. More recently, transformer-based architectures trained at scale have further enhanced the fidelity and diversity of generative models [42, 54, 60]. Conditional generation Conditional diffusion models incorporate auxiliary signals such as text, images, poses, or depth maps through additional encoders, auxiliary branches, or cross-attention mechanisms. Representative methods include ControlNet [69], IP-Adapter [64], and T2IAdapters [41]. With the emergence of Diffusion Transformers (DiT) [42], recent approaches [29, 49, 60] incorporate conditions directly into transformer attention layers for stronger guidance. However, these methods introduce substantial computational overhead, especially in video tasks. Bridge models Bridge Models [31, 35, 72] construct stochastic processes directly connecting source and target distributions, providing an alternative to noise-driven Early approaches employed Schrodinger generation. bridges [13], stochastic interpolants [2], and entropic optimal transport [12]. Recent diffusion-based variants, such as Denoising Diffusion Bridge Models (DDBM) [72] and Brownian Bridge methods [31], demonstrated promising results for conditional generation and image translation tasks. Several recent works have highlighted the potential of Bridge Models in vision tasks, demonstrating improved structural and stylistic consistency in exemplar-guided image translation [30], enhanced temporal coherence in video synthesis [52], and increased efficiency during training and inference for basic image translation tasks [5, 21]. 2 3. Preliminaries 4. Methodology Probability path modeling [13, 34, 37] defines class of generative models that describe continuous-time processes transporting mass from prior distribution p0 to target distribution p1. Generally, these models are represented by stochastic differential equation (SDE): dXt = v(Xt, t) dt + σ(t) dWt, [0, 1], (1) with boundary conditions X0 p0 and X1 p1, velocity field : Rd [0, 1] Rd, diffusion coefficient σ : [0, 1] R0, and standard Brownian motion Wt. In practice, the velocity field is typically parameterized by neural network vθ, trained via velocity-matching objective [34, 37]: L(θ) = E(x0,x1), t, Xt (cid:2)vθ(Xt, t) ut(Xt x0, x1)2(cid:3), (2) where ut( x0, x1) denotes the instantaneous velocity induced by chosen teacher trajectory (deterministic or stochastic), and Xt is sampled accordingly. Throughout, we use uppercase Xt for stochastic states and lowercase xt for deterministic trajectories. Rectified Flow Rectified Flow [15, 37] is deterministic realization of probability path modeling obtained by setting σ(t) 0 in Eq. (1). It defines linear deterministic trajectories connecting endpoints x0 p0, typically sampled from standard Gaussian distribution (0, I), and x1 p1, drawn from the data distribution: xt = (1 t)x0 + tx1. (3) Under this linear interpolation, the instantaneous velocity target simplifies to constant vector: ut x1 x0. (4) Brownian Bridge In contrast to the deterministic Rectified Flow, the standard Brownian Bridge [2, 31] incorporates stochasticity via constant diffusion coefficient σ(t) 1. Given fixed endpoints (x0, x1), its conditional intermediate states follow Gaussian distribution: Xt (x0,x1) N(cid:0)(1 t)x0 + tx1 , (cid:125) (cid:124) t(1 t)I (cid:125) (cid:123)(cid:122) (cid:124) maximal variance at t=0.5 (cid:123)(cid:122) linear interpolation (cid:1). (5) Brownian Bridges are particularly suited to data-to-data transport tasks, such as denoising corrupted samples or translating data between structured image and video doPairs of endpoints (x0, x1) are sampled from mains. their respective source and target distributions. Under this stochastic formulation, the instantaneous velocity target used in velocity matching is expressed as: ut(Xt x0, x1) = x1 Xt 1 . (6) 3 Our method leverages transformer-based architecture to model vision translation tasks in latent space. Given paired source and target data (images or videos), we encode them into latent representations x0 psource and x1 ptarget using pre-trained VAE encoder [27], and apply the Brownian Bridge formulation to directly model the transformation from x0 to x1. 4.1. Stabilized velocity matching During training, given latent endpoint pairs (x0, x1) psource,target, we uniformly sample time [0, 1] and Gaussian noise ϵ (0, I). According to the conditional distribution of the Brownian bridge (5), the intermediate latent state xt is constructed as: xt = (1 t)x0 + tx1 + (cid:112)t(1 t) ϵ. (7) The velocity-based training target at this noisy state, derived from Eq. (6), is given by: ut(xtx1) = x1 xt 1 = (x1 x0) (cid:114) 1 ϵ. (8) Accordingly, the training objective is given by the mean squared error between the predicted and target velocities: Lvelocity = Et, ϵ, x0, x1 (cid:104) vθ(xt, t) ut(xtx1)2(cid:105) . (9) However, this objective faces critical issues as 1: the target velocity ut(xtx1) diverges as , causing instability, and the loss excessively focuses on these states, neglecting intermediate ones  (Fig. 2)  . (cid:16) 1 1t (cid:17) An alternative approach adopted in previous works [31] is to use displacement-based training target defined as dt(xtx1) = x1 xt. (10) Accordingly, the displacement-based training objective is given by the mean squared error: Ldisplacement = Et, ϵ, x0, x1 (cid:2)dθ(xt, t) dt(xtx1)2(cid:3) . (11) This displacement formulation naturally avoids numerical divergence, as it remains stable across all timesteps. However, its magnitude diminishes as 1 at the rate 1 t), causing the training loss to be dominated by O( samples at smaller values of t. Motivated by the above numerical instability and imbalanced loss across timesteps, we propose stabilized velocity matching, which introduces normalization factor α(x0, x1, t) to balance loss contributions across timesteps. We rescale the original velocity target as: ut(xtx1) = ut(xtx1) α(x0, x1, t) . (12) Algorithm 1: Training Algorithm 2: Inference Input: data pairs (x0, x1) psource,target, model vθ, Input: source-target latent pair (x0, x1), trained latent dimension 1 repeat 2 Sample latent pair (x0, x1), interpolation time (0, 1), and noise ϵ (0, I); Construct intermediate state xt = (1 t)x0 + tx1 + (cid:112)t(1 t) ϵ; Compute velocity target ut = (x1 xt)/(1 t); Compute normalization factor α2 = 1 + tD/[(1 t)x1 x02]; Compute stabilized velocity loss velocity = vθ(xt,t)ut Update model parameters θ by gradient descent on velocity; 2; α 3 4 5 7 8 until convergence; Figure 2. Instantaneous and cumulative target contributions. S(t) = Eτt2 with τt {dt, ut, ut}. C(t) = (cid:82) 0 S(s) ds . (cid:82) 0. 0 S(s) ds Specifically, we define α(x0, x1, t) based on the normalized root mean square magnitude of the velocity: (cid:2)ut(xtx1)2(cid:3) x1 x02 (1 t) x1 x02 , α(x0, x1, t)2 = = 1 + (13) (14) where denotes the latent dimensionality 1 . As shown in Fig. 2, this choice significantly reduces divergence and ensures balanced loss contributions throughout training. The resulting stabilized velocity-matching objective is: velocity = Et, ϵ, x0, x1 (cid:104)(cid:13) (cid:13)vθ(xt, t) ut(xtx1)(cid:13) (cid:13) 2(cid:105) , (15) where vθ(xt, t) = vθ(xt, t)/α(x0, x1, t) normalizes the network prediction for loss calculation only, while the network still directly predicts velocity. The complete training procedure is summarized in Algorithm 1. model vθ, latent dimension D, discretization steps , discretization schedule 0 = t0 < t1 < < tN = 1 1 Initialize x0; 2 for = 0, 1, . . . , 1 do 3 Compute step size tk+1 tk; 4 5 6 Compute scaling factor η Sample noise ϵ (0, I); Update latent state: (cid:113) 1tk+1 1tk ; + vθ(x, tk) + η ϵ 7 end Output: Final state approximating the target x1 4.2. Variance-corrected sampling To sample from the trained Brownian Bridge model with stabilized velocity matching, we discretize the continuoustime SDE defined in Eq. (1) using the Euler-Maruyama discretization [38]. Given schedule 0 = t0 < t1 < < tN = 1, the sampling process starts from the source x0 and iteratively updates the latent state towards the target x1. The standard EulerMaruyama discretization yields: (cid:112) xstandard k+1 = xk + tk vθ(xk, tk) + where tk = tk+1 tk and {ϵk}K1 k=0 are i.i.d. samples drawn from (0, I). This scheme assumes locally constant variance structure, i.e., the stochastic term scales purely with tk ϵk, (16) tk. However, in the Brownian Bridge process, the variance should gradually shrink as the trajectory approaches the target x1, reflecting decreasing uncertainty near = 1. Consequently, the noise magnitude in the naive discretization becomes overly large at late steps, leading to biased trajectories and degraded sample quality. To correct this mismatch, scaling factor can be applied to continuously modulate the variance across timesteps. In practice, the diffusion term is rescaled by the ratio 1tk+1 2, 1tk resulting in variance-corrected stochastic update [2, 31]: (cid:114) + tk xcorrected k+1 = xk + tk vθ(xk, tk) (cid:125) (cid:123)(cid:122) (cid:124) velocity toward target 1 tk+1 1 tk (cid:123)(cid:122) (cid:125) (cid:124) variance-corrected noise (17) This correction ensures that the variance decays smoothly as 1, aligning the discrete sampling dynamics with the intrinsic structure of the Brownian Bridge. The complete inference procedure is summarized in Algorithm 2. ϵk . 1Derivations of factor α() are in the Supplementary Material C. 2Derivations of the scaling ratio are in the Supplementary Material C. 4 5. Experiments We conduct extensive experiments to explore the effectiveness of scaling Brownian Bridge diffusion models across various complex vision conditional generation tasks. We begin with instruction-based image editing tasks in Section 5.1 to evaluate the models ability to perform finegrained and instruction-based content modification. Next, we extend our study to video stylization in Section 5.2, where input videos are transformed into target styles specified by textual instructions while preserving the original motion and structure. Finally, we examine video translation tasks focusing primarily on depth-to-video synthesis in Section 5.3. Additionally, detailed ablation studies in Section 5.4 validate the effectiveness of our proposed stabilized velocity-matching loss and explore key properties of the scaled Brownian Bridge diffusion process. Training and inference details For image and video modalities, we respectively initialize our models from stateof-the-art pre-trained models: Qwen-Image-Editing [60] with 20B parameters for image-based tasks, and Wan 2.1 [54] with 1.3B parameters for video-based tasks. During training, the image model employs LoRA [19] with rank of 128, while the video model undergoes full-parameter updates. We train our models using the Prodigy optimizer [39] with learning rate of 1 and set save warmup=True. By default, we train each model for 20,000 iterations on 1 NVIDIA H100 GPU with batch size of 1. 5.1. Instruction-based image editing We first evaluate our bridge models on the complex image editing task, which involves modifying specific content within an input image based on textual instructions while preserving other regions. In this task, the input image serves as the source domain psource, and the edited image represents the target domain ptarget. The brownian bridge model directly learns the transformation between the base image and the edited output. Dataset We create synthetic dataset for instructionediting based on the Open Images based image Dataset [28]. Specifically, we first randomly sample 5,000 images and generate corresponding editing instructions using the vision-language model Qwen3-VL [61]. We then produce edited images based on these instructions using the Qwen-Image-Editing model [60]. Additionally, we enrich our dataset by incorporating stylized image data generated by OmniConsistency [48]. Finally, we filter the generated instruction-image pairs with Qwen3-VL to ensure high alignment between the instructions and the image edits. The detailed dataset construction process is described in the Supplementary Material, Section B. Figure 3. Qualitative comparison on image editing. Model Add Adjust Extract Replace Remove Bg. Style Hybrid Action Avg. 2.84 MagicBrush 2.45 Ins.Pix2Pix 3.18 AnyEdit 3.88 Step1X-Edit 3.82 UniWorld-V1 4.20 ViBT FLUX Kontext 3.82 ViBT (s = 0.5)3 4.14 Qwen-Image-Edit 4.17 1.58 1.83 2.95 3.14 3.64 3.70 3.64 4.20 4. 1.51 1.44 1.88 1.76 2.27 2.31 2.27 2.64 2.44 1.97 2.01 2.47 3.40 3.47 3.86 3.47 3.72 4.30 1.58 1.50 2.23 2.41 3.24 2.91 3.24 3.03 3.90 1.75 2.38 1.44 3.55 2.24 2.85 3.16 4.63 2.99 4.21 3.92 4.85 2.99 4.21 4.06 4.87 4.15 4.00 1.62 1.20 1.56 2.64 2.96 2.72 2.96 3.19 3.32 1.22 1.46 2.65 2.52 2.74 3.52 2.74 3.95 4. 1.83 1.88 2.45 3.06 3.26 3.55 3.71 3.76 3.90 Table 1. Model ranking on ImgEdit-Bench based on average score. the ImgEditEvaluation and baselines We adopt Bench [65] as our evaluation benchmark, as it provides comprehensive assessment across multiple editing dimensions, including instruction-following accuracy, editing quality, and preservation of image details. All evaluations presented in this section strictly follow the official protocols defined by ImgEdit-Bench. We compare our bridge model with representative diffusion-based methods, including InstructPix2Pix [4], Qwen-Image-Editing [60], Step1Xedit [36], FLUX.1 Kontext [29], and several other notable approaches [32, 66, 68]. Results and analysis Table 1 reports the quantitative results on ImgEdit-Bench4. ViBT performs on similar level to current state-of-the-art methods across the different editing categories. In tasks such as object addition and style transfer, ViBT achieves notably stronger results, outperforming competing approaches by clear margin. The qualitative results in Figure 3 and 4 show that ViBT produces clear instruction-following edits while keeping the original scene content, achieving visual quality comparable to leading diffusion-based methods. 3The noise scale is set to = 0.5; details are given in Section 5.4. 4Results of baselines are reported from ImgEdit Bench [32, 33, 65]. Figure 4. Qualitative results of the image editing. Figure 5. Comparison of stylized videos under the Van Gogh style. Method NIQE TOPIQNR MUSIQ MANIQA CLIPIQA TokenFlow 4.767 4.268 Ins.V2V 6.514 RAVE 4.328 ViBT 0.376 0.467 0.351 0. 55.186 60.621 50.595 64.045 0.267 0.306 0.269 0.348 0.378 0.440 0.377 0.486 CLIP Score 0.683 0.827 0.683 0. Table 2. Quantitative results on the video stylization task. 5.2. Video stylization In the video domain, we first consider the instruction-based video stylization task, which aims to modify the visual style of an input video according to given textual instruction while preserving its original content and motion dynamics. Dataset and training We use the open-source Ditto-1M dataset [3] for training our bridge video model. Specifically, we randomly sample 10,000 video samples from the subset global style1 of Ditto-1M, which contains videos paired with style descriptions. We train bridge model on 4 NVIDIA H100 GPUs for 50,000 iterations in this task. Evaluation and baselines For evaluation, we construct benchmark comprising 100 videos generated by Wan 2.2 14B [54] using the first 100 prompts from MovieGen Bench [43], serving as inputs for the stylization task. These videos do not overlap with our training set. Each video is paired with randomly sampled textual style instruction. We stylize videos consisting of 81 frames each, and Figure 6. Qualitative comparisons of ViBT on the video stylization task with different styles. uniformly sample 5 frames per video for quality assessment. We quantitatively evaluate these sampled frames including using widely adopted image-quality metrics, NIQE [40], TOPIQ-NR [7], MUSIQ [26], MANIQA [63], CLIPIQA [55], and CLIPScore [17]. These metrics comprehensively measure perceptual image quality, aesthetic appeal, and visual-semantic alignment with textual instructions. We compare our method against three diffusionbased video stylization methods, Instruct Video-to-Video (InsV2V) [11], RAVE [23], and TokenFlow [45]. Results and analysis Quantitative results in Table 2 show that ViBT outperforms the baselines in most metrics, demonstrating its effectiveness in generating high-quality stylized videos that align well with the given instructions. Qualitative comparisons in Figure 5 further illustrate that ViBT can apply the desired style to the input video while preserving the original motion and structure. Figure 6 futher demonstrates that ViBT can effectively stylize videos across various artistic styles while preserving original content and motion. More stylized video examples are available in the Supplementary Material, Section A. 6 Method Base Model Perceptual quality Ground truth similarity CLIP Score VBench Score NIQE TOPIQNR MUSIQ MANIQA CLIPIQA SSIMc PSNR DISTS ControlVideo Control Video VideoComposer Wan Fun Control ViBT SD 1.5 SD 1.5 SD 2.1 Wan 2.1 Wan 2.1 6.641 5.102 6.750 5.346 4. 0.443 0.374 0.305 0.477 0.477 50.735 52.391 43.691 59.086 59.625 0.354 0.254 0.276 0.335 0.331 0.436 0.334 0.237 0.459 0.477 0.385 0.276 0.329 0.427 0.429 9.067 8.510 9.656 10.899 11. 0.465 0.348 0.457 0.281 0.230 0.732 0.715 0.722 0.776 0.781 0.62 0.59 0.59 0.69 0.71 Table 3. Quantitative comparison on the depth-to-video task. Method Subj. Cons. Bkgd. Cons. Aesth. Qual. Img. Qual. Obj. Class Multi Objs. Control Video Control Video Video Composer Wan Fun ViBT 0.899 0.791 0.873 0.913 0.907 0.94 0.88 0.92 0.93 0.93 0.54 0.48 0.44 0.60 0.63 0.52 0.59 0.48 0.57 0.63 0.57 0.59 0.67 0.87 0.91 0.26 0.25 0.23 0.65 0. Color 0.706 0.799 0.854 0.848 0.835 Spatial Rel. Scene Temp. Style Overall Cons. Human Action Temp. Flicker Motion Smooth. Dyn. Degree Appear. Style Avg. Score 0.46 0.44 0.32 0.70 0.74 0.29 0.43 0.29 0.46 0.54 0.20 0.21 0.22 0.24 0.25 0.24 0.24 0.24 0.26 0.27 0.80 0.83 0.91 1.00 1.00 0.991 0.982 0.963 0.989 0. 0.990 0.976 0.949 0.978 0.976 0.11 0.72 0.88 0.86 0.82 0.229 0.235 0.222 0.211 0.221 0.55 0.59 0.59 0.69 0.71 Table 4. Quantitative comparison on the VBench attribute breakdown for the depth-to-video task. 5.3. Video translation To verify the versatility and generalization capability of bridge model, we further explore its application to video translation tasks. We primarily investigate depth-to-video synthesis, fundamental yet challenging scenario. Dataset and training To create the training dataset, we first generate 1,003 videos using Wan 2.2 14B with prompts sourced from the MovieGen Bench [43]. We then transform these synthesized videos into depth maps using the Depth Anything V2 [62] model, forming depth-video pairs for training. Detailed generation procedures are provided in Supplementary Material, Section B. Evaluation and baselines We evaluate the brownian bridge model on the depth-to-video synthesis task, broadly adhering to the evaluation protocols outlined in VBench [20]. Specifically, we first generated 946 reference videos using Wan 2.2 14B based on the prompts provided by VBench, and subsequently converted these videos into corresponding depth maps. These depth maps were employed as conditioning inputs across all methods. Further details regarding the generation procedure are provided in the Supplementary Material, Section B. For comprehensive assessment, we initially applied the quality metrics discussed in Section 5.2. We then augmented this analysis by introducing reference-based metrics including SSIM [59], PSNR [6], and DISTS [14], to quantitatively measure similarity between generated outputs and ground-truth videos. Additionally, we included the VBench Score [20] as an extra criterion to capture finer-grained and interpretable dimensions of video quality. We compare ViBT against three representative diffusionbased controllable video generation models: Control-AFigure 7. Qualitative comparison on the depth-to-video task. Video [10], ControlVideo [70], and VideoComposer [57]. To provide direct baseline for evaluating the effectiveness of our proposed method, we also incorporate Wan-Fun Control [1], flow-matching-based method initialized from the same Wan 2.1 1.3B model as ViBT. Results Table 3 presents quantitative comparisons on video frame quality, condition-following accuracy, textfollowing accuracy, and the overall VBench Score. Across all metrics, ViBT consistently outperforms the baselines, indicating strong video generation quality and reliable conditioning behavior. To further examine specific aspects, Table 4 reports fine-grained attribute evaluations under VBench, where ViBT achieves leading performance on most attributes. Figure 7 provides qualitative examples, showing that ViBT produces richer and more detailed visuals that align more closely with the depth conditions. Additional experimental results, including qualitative evaluations on extra video translation tasks such as video interpolation and video colorization, can be found in the Supplementary Material, Section A. 7 Depth-to-Video Image Edit Training Objective SSIM PSNR NIQE DISTS CLIP Score VBench Score Add Adjust Extract Replace Remove Bg. Style Compose Action Avg. Displacement Velocity Stabilized velocity 0.409 0.428 0. 11.04 10.81 11.40 4.91 5.45 4.90 0.26 0.27 0.23 0.772 0.773 0.78 0.695 0.698 0.71 4.18 4.09 4. 3.79 3.89 3.70 2.23 2.19 2.31 3.57 3.34 3.86 2.65 2.13 2.91 3.97 3.90 3.92 4.847 4.897 4. 2.74 2.62 2.72 3.519 3.149 3.518 3.50 3.36 3.55 Table 5. Quantitative comparison of different training objectives. Depth-to-Video Image Edit Noise Scale (s) SSIM PSNR NIQE DISTS CLIP Score VBench Score Add Adjust Extract Replace Remove Bg. Style Compose Action Avg. = 0 = 0.1 = 0.5 = 1 (default) = 2 = 4 0.347 0.331 0.398 0.429 0.396 0.394 9.808 9.206 10.227 11.403 11.305 10. 5.432 5.413 5.185 4.896 4.499 5.912 0.3103 0.3452 0.2617 0.2304 0.2295 0.3820 0.717 0.675 0.752 0.781 0.784 0.670 0.604 0.536 0.666 0.709 0.711 0.482 3.91 3.43 4.15 4.20 4.14 3.70 4.29 4.00 4.20 3.70 3.49 2. 2.01 2.04 2.64 2.31 2.36 2.24 2.45 2.31 3.72 3.86 3.94 3.60 1.60 1.61 3.03 2.91 3.16 2.88 3.35 3.53 4.06 3.92 3.64 2.93 4.65 4.46 4.87 4.85 4.82 4.43 2.56 2.58 3.19 2.72 2.46 1. 3.07 3.29 3.95 3.52 2.98 2.50 3.10 3.03 3.76 3.55 3.44 2.97 Table 6. Quantitative comparison across different noise scales (s). To investigate its impact, we conduct experiments across different values of s, summarizing results in Table 6. The corresponding training and inference modifications for this generalized formulation are detailed in the Supplementary Material, Section C. Our findings indicate that moderate noise scales (s = 1 or = 2) achieve better performance for the depth-to-video task, with = 2 showing strong overall scores. For image editing tasks, smaller noise scale (s = 0.5) surprisingly achieves the highest average performance, notably outperforming the default = 1 setting. However, excessively small (s < 0.5) or large (s > 2) noise scales significantly degrade quality on both tasks. These observations highlight that optimal noise scales differ across tasks, contrasting with previous work [5] advocating an extremely small noise scale (s = 0.005). 6. Conclusion In this paper, we introduced the Visual Bridge Transformer, large-scale instantiation of Brownian Bridge models, effectively scaling this paradigm to 20B parameters for conditional image and video generation. By proposing stabilized velocity-matching objective, we addressed the numerical instability inherent in conventional training methods, significantly improving model stability and performance. Extensive experiments demonstrated that our framework consistently outperforms existing baselines across multiple challenging vision translation tasks, including instructionbased image editing and video translation tasks. 7. Limitations and future work While our Visual Bridge Transformer demonstrates strong results, we observed that adjusting the noise scale can further optimize performance across different vision tasks. Future work may explore adaptive or automated methods to select this parameter, potentially enhancing the versatility and effectiveness of Bridge Models. (a) Training loss curves. (b) Visualization results. Figure 8. Comparison of different training objectives in depth-tovideo synthesis task. 5.4. Ablation and analysis Training objectives We compare three training objectives to validate our proposed stabilized velocity matching objective defined in Eq. (15), along with displacement matching Eq. (11) and velocity matching Eq. (2). Table 5 shows that stabilized velocity matching consistently achieves the best performance on both depth-to-video and image editing tasks. Specifically, it surpasses other objectives on all evaluated metrics for depth-to-video generation, and it also attains the highest average scores in diverse image editing scenarios. Moreover, Figure 8 highlights its superior training stability and improved visual quality compared to alternative objectives. Noise scale Several previous works [5, 31] further extend the Brownian Bridge formulation by modifying the diffusion term in Eq. (1). Instead of fixing the diffusion coefficient as constant σ(t) 1, they introduce global noise scale parameter such that σ(t) s, leading to the generalized SDE: dXt = vθ(Xt, t) dt + dWt. (18)"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional experimental results Efficiency comparison The Brownian Bridge formulation in ViBT enables more efficient training and inference by reducing reliance on auxiliary conditional branches or additional conditioning tokens. To quantitatively illustrate this potential advantage, we perform theoretical inference latency comparisons between ViBT and conventional conditional diffusion transformer (DiT) variants, which inject conditions by introducing extra tokens into attention layers. For image translation, ViBT is instantiated from QwenImage-Editing [60], while for video translation, ViBT is built upon Wan 2.1 1.3B [54]. Corresponding conditional DiT variants derived from these models serve as our baselines. We measure the inference latency for single forward pass under single NVIDIA H200 GPU, ensuring clean architectural efficiency comparison independent from sampling schedules or runtime optimization. Tables S1 and S2 detail the raw data for this comparison, including exact token counts and per-step latencies under various image resolutions and video settings. Figure S1 further visualizes the latency comparisons, clearly demonstrating that ViBT consistently reduces inference latency across all evaluated image and video translation scenarios compared to the conditional DiT baselines. Additional video translation tasks Besides the depth-tovideo synthesis task presented in Section 5.3, we further evaluate ViBT on two additional video translation tasks: (1) video colorization and (2) video frame interpolation. For video colorization, we directly apply ViBT to transform grayscale videos into colored videos. Figure S3 shows qualitative examples of video colorization results, highlighting ViBTs strong generalization capability. For video frame interpolation, we first construct coarse video by repeating each original frame (except the first frame) times in pixel space. ViBT is then applied to refine this coarse video, enhancing both visual quality and temporal coherence. Figure S2 illustrates this interpolation pipeline clearly. In our experiments, we set = 4 to generate 4 interpolated frames between each original frame. This increases the frame rate of videos generated by Wan 2.1 from 15 FPS to 60 FPS, while maintaining high visual quality and temporal coherence. Qualitative results for this interpolation task are provided in Figure S4. Notably, ViBT is capable of producing high-quality and temporally coherent results within only few inference steps (e.g., 4 steps), demonstrating its efficiency. Resolution Conditional DiT ViBT Tokens Latency (ms) Tokens Latency (ms) Speedup 1024 1024 1328 1328 8,192 10,624 437 4,096 5,312 192 258 2.28 2.38 Table S1. Inference efficiency comparison (image). Resolution Conditional DiT ViBT Tokens Latency (ms) Tokens Latency (ms) Speedup 480P (5s) 480P (10s) 720P (5s) 720P (10s) 65,520 127,920 151,200 295,200 1,510 5,407 7,437 28,577 32,760 63,960 75,600 147,600 459 1,444 1,958 7,097 3.29 3.74 3.80 4.03 Table S2. Inference efficiency comparison (video). Figure S1. Comparison between Conditional DiT and ViBT. Figure S2. Illustration of video frame interpolation pipeline. Ablation study on variance-corrected sampling To validate the effectiveness variance-corrected sampling strategy described in Eq. (17), we perform an ablation study by comparing it with the standard Euler-Maruyama discretization method without variance correction. Figure S5 provides qualitative results for this comparison on the image editing task. We observe that the naive discretization method (without variance correction) introduces noticeable artifacts, leading to degraded visual quality. In contrast, the variance-corrected sampling generates cleaner and visually coherent image. 9 Figure S3. Qualitative results on video colorization task. Figure S4. Qualitative results on video frame interpolation task. Figure S5. Ablation study on variance-corrected sampling. Influence of inference steps and schedule We further investigate how the number of inference steps and the discretization schedule affect ViBTs performance. As illustrated in Figure S6, increasing the inference steps consistently improves the generation quality. Moreover, the choice of timestep scheduler significantly influences the performance. Specifically, we adopt the shifting strategy introduced in Stable Diffusion 3 [15], which uses shift coefficient γ to allocate more inference steps towards the earlier stages (t 0) of the diffusion process. This shifted schedule is formulated as: ti = γ + (γ 1) , (19) where denotes total steps and the step index. Figure S7 illustrates how increasing γ redistributes step density, placing more steps at earlier stages. Our experiments show that γ = 5 achieves significantly better visual quality than the linear schedule (γ = 1), especially with fewer inference steps (e.g., 4 or 8 steps). 10 Figure S6. Ablation on inference steps and timestep schedule. Figure S7. Step density and timestep schedule for different γ. Figure S8. Visualization of the intermediate stages in the ViBT bridge process. Additional visualizations We provide supplementary qualitative results: Figure S8 visualizes intermediate generation states at different timesteps in the ViBT bridge process, Figure S9 shows additional examples of image stylization tasks, Figure S10 presents further results on instruction-based image editing, and Figure S11 provides extra visualizations of video stylization outputs. 11 Figure S9. Additional examples of image stylization generated by ViBT. Figure S10. Additional qualitative results on instruction-based image editing. 13 Figure S11. Additional results of video stylization tasks. 14 B. Experimental details C. Theoretical analysis and extensions Image editing dataset construction We construct our image editing dataset by first randomly sampling 5,000 images from the Open Images Dataset [28]. These images are cropped and resized into resolutions supported by the Qwen-Image-Editing model, specifically: 1328 1328 (1:1), 1664 928 (16:9), 928 1664 (9:16), 1472 1104 (4:3), 11041472 (3:4), 15841056 (3:2), and 10561584 (2:3). Subsequently, we generate corresponding editing instructions for these images using the vision-language model Qwen3-VL [61]. We then produce edited images based on these instructions using Qwen-Image-Editing [60]. To ensure high-quality alignment, we further score the generated instruction-image pairs using Qwen3-VL, filtering out pairs with low alignment scores. This filtered set constitutes Part 1 of our training data, comprising approximately 3,335 validated samples. Additionally, we enrich the dataset by incorporating stylized images generated by OmniConsistency [48]. These images retain their original 1024 1024 resolution, with editing instructions uniformly formulated as Convert the image to [style] style image. This augmentation forms Part 2 of our dataset, introducing further diversity with approximately 2,605 samples. Depth-to-Video dataset construction To create the training dataset for depth-to-video synthesis, we first generate 1,003 videos using Wan 2.2 14B [54] with prompts sourced from the MovieGen Bench [43]. These videos are synthesized at resolution of 832 480 with 81 frames each, using classifier-free guidance (CFG) scale of 5 and 50 sampling steps. We then transform these synthesized videos into depth maps using the Depth Anything V2 [62] model, forming depth-video pairs for training. It should be noted that the generated depth maps utilize the default inferno colormap format provided by Depth Anything V2, rather than grayscale images. Normalization factor for stabilized velocity matching Conditioned on endpoints (x0, x1), the Brownian Bridge latent at time can be written as xt = (1 t)x0 + tx1 + (cid:112)t(1 t) ϵ, ϵ (0, I)."
        },
        {
            "title": "The velocity target is",
            "content": "ut(xt x1) = x1 xt 1 ."
        },
        {
            "title": "Substituting xt gives",
            "content": "x1 xt = (1 t)(x1 x0) (cid:112)t(1 t) ϵ, ut(xt x1) = (x1 x0) (cid:114) 1 ϵ. (20) (21) (22) (23) We define the normalization factor via the (conditional) expected squared normlknjugytfrd5e4s3w2aq1 fc gvbhnjm α(x0, x1, t)2 = Eϵ (cid:2)ut(xt x1)2(cid:3) x1 x02 , (24) where the expectation is taken over ϵ with (x0, x1) fixed. Using E[ϵ] = 0 and E[ϵ2] = D, we obtain Eϵ (cid:2)ut(xt x1)2(cid:3) = (cid:13) (cid:13)x1 x0 (cid:13) 2 (cid:13) + 1 D, (25) and hence α(x0, x1, t)2 = 1t x1 x02 + x1 x02 (1 t) x1 x02 . = 1 + (26) (27) This is the normalization factor used in the stabilized velocity loss in Eq. (15). Depth-to-Video evaluation details For evaluation on the depth-to-video synthesis task, we generate 946 reference videos using Wan 2.2 14B based on the prompts provided by VBench [20]. These videos are also synthesized at resolution of 832480 with 81 frames each, using CFG scale of 5 and 50 sampling steps. We then convert these videos into corresponding depth maps using the Depth Anything V2 [62] model, which are employed as conditioning inputs across all methods. The prompts used for generating the source videos are the extended versions [54]. However, for fair evaluation during testing, we use the original prompts provided by VBench. Variance-corrected noise scaling Write the process as deterministic interpolation plus zero-mean Brownian Bridge: Xt = (1 t)x0 + tx1 + Bt, (28) where {Bt}t[0,1] is Brownian Bridge from 0 to 0. For 0 t1 t2 1, its covariance satisfies E[Bt2] = 0, Var(Bt2) = t2(1 t2) I, Cov(Bt1 , Bt2) = t1(1 t2) I. (29) (30) (31)"
        },
        {
            "title": "Thus the conditional variance is",
            "content": "Var(Bt2 Bt1) = Var(Bt2) Cov(Bt2, Bt1) Var(Bt1 )1 Cov(Bt1, Bt2 ) t2 1(1 t2)2 t1(1 t1) = t2(1 t2) = (t2 t1)(1 t2) 1 I. Since the endpoints only affect the mean, Var(Xt2 Xt1) = (t2 t1)(1 t2) 1 t1 I."
        },
        {
            "title": "For a discretization schedule",
            "content": "0 = t0 < t1 < < tN = 1, (38) set t1 = tk, t2 = tk+1 and tk = tk+1 tk to obtain (32) (33) (34) Algorithm S1: Training with noise scale (35) Input: data pairs (x0, x1) psource,target, model vθ, latent dimension D, noise scale (36) (37) 1 repeat 2 Sample latent pair (x0, x1), interpolation time (0, 1), and noise ϵ (0, I); Construct intermediate state xt = (1 t)x0 + tx1 + s(cid:112)t(1 t) ϵ; Compute velocity target ut = (x1 xt)/(1 t); Compute normalization factor α2 = 1 + s2tD/[(1 t)x1 x02]; Compute stabilized velocity loss velocity = vθ(xt,t)ut Update model parameters θ by gradient descent on velocity; 2; α 3 5 6 7 Var(cid:0)Xtk+1 Xtk (cid:1) = tk 1 tk+1 1 tk I. (39) 8 until convergence; Therefore an increment of the form (cid:114) Xtk+1 = Xtk + tk 1 tk+1 1 tk ϵk, ϵk (0, I), (40) matches the Brownian Bridge conditional variance. When discretizing dXt = vθ(Xt, t) dt + dWt, (41) the EulerMaruyama update with variance correction becomes xcorrected k+1 = xk + tk vθ(xk, tk) (cid:114) + tk 1 tk+1 1 tk ϵk, (42) which is precisely the update in Eq. (17). Training and inference with noise scale Under the generalized Brownian Bridge SDE in Eq. (18), we keep the network architecture and stabilized velocity objective unchanged, and only rescale the stochastic terms by the global noise scale s. Concretely, the intermediate state construction in training and the variance-corrected noise in inference are both multiplied by s, as summarized below. Algorithm S2: Inference with noise scale Input: source-target latent pair (x0, x1), trained model vθ, latent dimension D, discretization steps , discretization schedule 0 = t0 < t1 < < tN = 1, noise scale 1 Initialize x0; 2 for = 0, 1, . . . , 1 do 3 Compute step size tk+1 tk; 4 5 6 Compute scaling factor η Sample noise ϵ (0, I); Update latent state: (cid:113) 1tk+1 1tk ; + vθ(x, tk) + η ϵ 7 end Output: Final state approximating the target x"
        },
        {
            "title": "Acknowledgment",
            "content": "This project is supported by NUS ITs Research Computing group under grant number NUSREC-HPC-00001. We thank Ruonan Yu and Sicheng Feng for helpful discussions."
        },
        {
            "title": "References",
            "content": "[1] aigc-apps. VideoX-Fun: more flexible framework that can generate videos at any resolution and create videos from images. https://github.com/aigc-apps/VideoXFun, 2025. VideoX-Fun is an open-source video generation pipeline supporting AI image and video generation and training baseline and LoRA models. 2, 7 [2] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 2, 3, 4 [3] Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, and Qifeng Chen. Scaling instruction-based video editing with highquality synthetic dataset. arXiv preprint arXiv:2510.15742, 2025. 6 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 5 Lbm: [5] Clement Chadebec, Onur Tasar, Sanjeev Sreetharan, Latent bridge matcharXiv preprint and Benjamin Aubin. ing for fast image-to-image translation. arXiv:2503.07535, 2025. 2, 8 [6] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch toolbox for image quality assessment. [Online]. Available: https : / / github . com / chaofengc / IQA - PyTorch, 2022. 7 [7] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2024. 6 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [9] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. 2 [10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023. [11] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoarXiv preprint to-video transfer using synthetic dataset. arXiv:2311.00213, 2023. 6 17 [12] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. 2 [13] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. Advances in neural information processing systems, 34:1769517709, 2021. 2, 3 [14] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 7 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 3, [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 75147528, 2021. 6 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 7, 15 [21] Haorui Ji, Tao Jun Lin, and Hongdong Li. Dpbridge: ArXiv, Latent diffusion bridge for dense prediction. abs/2412.20506, 2024. 2 [22] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2 [23] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6 [24] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 2 [25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. 2 [26] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [27] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [28] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 5, 15 [29] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 2, 5 [30] Eungbean Lee, Somi Jeong, and Kwanghoon Sohn. Ebdm: exemplar-guided image translation with brownian-bridge diffusion models. In European Conference on Computer Vision, pages 306323. Springer, 2024. 2 [31] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Imageto-image translation with brownian bridge diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, 2023. 2, 3, 4, 8 [32] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 5 [33] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [34] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [35] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. 2 sb: Image-to-image schr odinger bridge. arXiv preprint arXiv:2302.05872, 2023. 2 [36] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 5 [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3 Continuous markov processes and stochastic equations. Rendiconti del Circolo Matematico di Palermo, 4(1):4890, 1955. 4 [38] Gisiro Maruyama. [39] Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner. In Forty-first International Conference on Machine Learning, 2024. 5 [40] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. 6 [41] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. 2 [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 6, 7, 15 [44] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin. Variational autoencoder for deep learning of images, labels and captions. Advances in neural information processing systems, 29, 2016. 2 [45] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [47] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [48] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. 5, 15 [49] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1494014950, 2025. 2 [50] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 2 [51] Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, et al. Longcat-video technical report. arXiv preprint arXiv:2510.22200, 2025. 2 [52] Viacheslav Vasilev, Arseny Ivanov, Nikita Gushchin, Maria Kovaleva, and Alexander Korotin. Time-correlated video bridge matching. ArXiv, abs/2510.12453, 2025. 2 Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 5 [67] Conghan Yue, Zhengwei Peng, Junlong Ma, Shiyan Du, Pengxu Wei, and Dongyu Zhang. Image restoration through arXiv preprint generalized ornstein-uhlenbeck bridge. arXiv:2312.10299, 2023. 2 [68] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 5 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2 [70] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 7 [71] Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024. [72] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. ArXiv, abs/2309.16948, 2023. 2 [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [54] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5, 6, 9, 15 [55] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, pages 25552563, 2023. 6 [56] Peiyong Wang, Bohan Xiao, Qisheng He, Carri Glide-Hurst, Score-based image-to-image brownian and Ming Dong. bridge. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1076510773, 2024. 2 [57] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. 7 [58] Yuji Wang, Zehua Chen, Xiaoyu Chen, Yixiang Wei, Jun Improving imagearXiv preprint Zhu, and Jianfei Chen. Framebridge: to-video generation with bridge models. arXiv:2410.15371, 2024. 2 [59] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [60] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2, 5, 9, 15 [61] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 5, 15 [62] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 7, 15 [63] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11911200, 2022. [64] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [65] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 5 [66] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "The Hong Kong Polytechnic University"
    ]
}