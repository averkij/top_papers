{
    "paper_title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
    "authors": [
        "Zhengyi Wang",
        "Jonathan Lorraine",
        "Yikai Wang",
        "Hang Su",
        "Jun Zhu",
        "Sanja Fidler",
        "Xiaohui Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLaMA-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. LLaMA-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 5 9 5 9 0 . 1 1 4 2 : r LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models Zhengyi Wang1 * Jonathan Lorraine2 Yikai Wang1 Hang Su1 Jun Zhu1 Sanja Fidler Xiaohui Zeng2 https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh Tsinghua University1 NVIDIA2 Figure 1. An illustration of our method, LLAMA-MESH, which enables the generation of 3D meshes from human instructions via conversational interface. Users provide textual prompts, and the model responds with both text and 3D mesh outputs, facilitating interactive 3D content creation. LLAMA-MESH allows large language models to generate and interpret 3D meshes from text directly, seamlessly unifying language and 3D modalities within single model."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLAMA-MESH, novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in text-based format, effectively unifying the 3D and text modalities. LLAMA-MESH achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance. *Work completed during NVIDIA internship. Large Language Models (LLMs) [5, 52] have demonstrated remarkable capabilities in understanding and generating human-like text, achieving success in applications such as conversational agents, code generation, and visual content reasoning [1, 16, 30]. Despite these advances, their generative abilities have primarily been limited to the textual content, restricting their utility for broader tasks. Our work seeks to extend LLMs into new modality3D mesh generationunlocking significant potential for fields like computer graphics, engineering, robotics, and virtual/augmented reality. By enabling LLMs to generate 3D meshes from textual descriptions, we unify language understanding with 3D content creation, expanding the functional scope of LLMs. This approach paves the way for more intuitive and efficient workflows in 3D content creation driven by language-based instructions. However, integrating new modality into an LLM is challenging, particularly in the tokenization process for processing the new modality. To the best of our knowledge, there have been no attempts to unify 3D mesh and text generation in single framework. Some studies explored unifying image and text generation. Among these works [37, 55], common approach is to train new to1 Figure 2. Overview of our method. LLAMA-MESH unifies text and 3D mesh in uniform format by representing the numerical values of vertex coordinates and face definitions of 3D mesh as plain text. Our model is trained using text and 3D interleaved data end-to-end. Therefore, with single, unified model, we can generate both text and 3D meshes. kenizer such as vector-quantized variational autoencoder (VQ-VAE) [17, 51] to encode the new modality into discrete tokens, which are used in training. However, this requires vocabulary expansion, increasing the adaptations learning cost. Additionally, this method introduces information loss during the auto-encoding process. To tackle these challenges, we introduce LLAMA-MESH, novel framework that enables large language models (LLMs) to generate 3D meshes by representing them as plain text. Our approach uses the OBJ file format, widely adopted text-based standard for 3D models comprising vertex coordinates and face definitions, as shown in Figure 4. By treating these numerical values as sequence of text, we convert 3D meshes into format that LLMs can process directly, avoiding modifications to the tokenizer or the vocabulary, thus minimizing additional training overhead. This design capitalizes on the extensive knowledge embedded in pretrained LLMs. Figure 6 shows pretrained LLMs demonstrate native ability to represent 3D structures in text capability our framework harnesses. We construct supervised fine-tuning (SFT) dataset that includes text-3D pairs and interleaved text-3D dialogues. We fine-tune pretrained LLaMA-3.1-8B-Instruct [16] model on our curated dataset. We find that LLMs can acquire complex spatial knowledge by learning the numerical values of meshes in textual format. After fine-tuning, our model demonstrates the ability to (1) generate 3D meshes given text prompts, (2) produce interleaved outputs of text and 3D meshes in conversational setup, and (3) describe meshes in natural language. LLAMA-MESH is the first successful effort to empower an LLM to generate 3D content with language, unifying the 3D and text modalities in single large model. It achieves mesh generation quality comparable to models trained from scratch while maintaining strong text generation abilities. 2. Related Work We overview approaches to make LLMs multi-modal in Section 2.1, 3D object generation more broadly in Section 2.2, and then specifically generating meshes autoregressively in Section 2.3. 2.1. Enabling LLMs to be Multi-Modal Extending LLMs to process and generate multiple modalities, such as vision and language, in unified model is an active research area. Existing works allow an LLM to understand visual input for multimodal interactions [2, 3, 12, 21, 22, 26, 28, 31, 32, 67, 72] or unify image and text generation [37, 55, 61, 62, 68, 74] with new visual tokenizer. Instead, we avoid modifying tokenization and focus on 3D by simply outputting (the text of) an OBJ file. Closely related are works where LLMs wield tools to generate 3D scenes [66, 73] by generating layouts to compose predefined objects. However, these methods do not enable LLMs to produce 3D meshes directly. To the best of our knowledge, we are the first to allow LLMs to directly generate 3D meshes as text instead of just wielding 3D object generation tools. 2.2. 3D Object Generation DreamFusion [41], Magic3D [27], ProlificDreamer [56] and many other methods [6, 8, 11, 20, 25, 29, 35, 47, 48, 54, 63, 69, 75] use score-distillation to generate 3D objects from pretrained large-scale text-to-image diffusion model [42, 43]. Feed-forward methods including LRM [19, 23, 53, 65], CRM [57], InstantMesh [64], and other methods [24, 33, 34, 49, 60, 71, 76] generate 3D objects without test-time optimization. However, the above methods typically treat 3D objects as numerical fields and extract meshes using marching cubes or their variants [44, 45], which do not easily allow representation as discrete tokens. 2 Figure 3. Gallery of generations from LLAMA-MESH. We can generate high-quality and diverse meshes with artist-like created topology. 2.3. Auto-Regressive Mesh Generation Methods such as PolyGen [39], MeshGPT [46], MeshXL [7], instead model 3D object as discrete sequence of tokenized coordinates and use an auto-regressive to generate an object with artist-created transformer topology. MeshAnything [9, 10], PivotMesh [58] and EdgeRunner [50] take point cloud as input condition for better control. These works also treat meshes as discrete tokens generated using auto-regressive transformers, but they are trained from scratch and lack language capabilities. 3. Method We now introduce LLAMA-MESH. First, in Section 3.1, we explain why and how we represent 3D meshes as plain text for easy processing by LLMs. Then, in Section 3.2, we detail the pretrained LLaMA model [16], an effective initialization of LLAMA-MESH. Finally, Section 3.3 describes how we create 3D dialog SFT dataset to give LLMs 3D generation capabilities through fine-tuning. Our model structure is illustrated in Figure 2. 3.1. 3D Representation To enable large language models (LLMs) to generate 3D meshes directly, key challenge lies in tokenizing this new modality so that the LLM can process it effectively. We observe that pretrained LLMs can generate 3D objects in the OBJ file formata simple and widely used plain text formatin zero-shot manner, as illustrated in Figure 6. Although these generated shapes are simple and not immediately usable, they demonstrate that some 3D knowledge in OBJ format is inherently encoded in the LLMs. Additionally, since OBJ files describe 3D geometry in plain text format, they are ideal candidates for integration with LLMs without requiring modifications to the tokenizer or vocabulary. These insights motivate us to represent 3D objects using the OBJ file format. Figure 4. Illustration of our 3D representation approach. Left: snippet of an OBJ file represented as plain text, containing vertex (v) and face (f) definitions. Right: The 3D object rendered from the OBJ file. We enable the LLM to process and generate 3D meshes by converting the mesh data into textual format. An OBJ file consists of list of vertex coordinates and face definitions: Vertices (v): Each line starting with the letter defines vertex in 3D space with its x, y, and coordinates, e.g., 0.123 0.234 0.345. Faces (f): Each line starting with the letter defines face by listing vertex indices that form polygon (typically triangle or quadrilateral), e.g., 1 2 3. By treating these numerical values as plain text, we convert the 3D mesh into sequential text format that LLMs process natively. Figure 4 shows an example of simple OBJ file and its corresponding 3D object rendering. Note that OBJ files from the internet may vary slightly in format. We adopt standard that is widely used and straightforward. Figure 5. Illustration of our vertex quantization method. Top: The original OBJ file represents vertex coordinates in decimal values, splitting single coordinate into several tokens. Bottom: After quantization, we represent the vertices as integers containing fewer tokens and are processed by LLM more efficiently. Note that 3D mesh coordinates are typically stored as floating-point numbers. Using floating-point numbers for vertex coordinates directly leads to long token sequences, exceeding most LLMs context length limitations and increasing the computational cost. To address this, we quantize the vertex coordinates into fixed number of bins (64 per axis in our case). We scale the mesh to the range [0, 64] and quantize the coordinates to the nearest integer. Figure 5 shows our quantization, which slightly reduces the coordinates precision. However, it significantly decreases the token count, making it feasible for LLMs to handle longer sequences without sacrificing geometric fidelity. 3.2. Pretrained Models Pretrained LLMs, like LLaMA [16] variants, are natural candidates for generating text for meshes as they are (a) strong tools for modeling arbitrary sequences and (b) may have encountered similar data during pre-training. Specifically, we use LLaMA3.1-8B-Instruct [16] as our base model."
        },
        {
            "title": "This model",
            "content": "is chosen for its balance between perIt has been formance and computational efficiency. instruction-tuned to follow prompts and generate coherent responses, which is advantageous for our application, where the model needs to interpret text prompts and generate corresponding 3D meshes. Notably, the model can generate simple (but not perfect) OBJ files without fine-tuning, as shown in Figure 6, likely because there are publicly available examples, e.g., on GitHub. Despite its strengths, the pretrained LLaMA model performs poorly on mesh generation tasks without fine-tuning, underscoring the need for fine-tuning the model on specialized dataset that includes mesh representations in plain text. By fine-tuning LLaMA on our curated dataset of textmesh pairs, we enable the model to learn the patterns and semantics of the OBJ format, allowing it to generate valid 3D meshes directly from textual descriptions. Figure 6. Illustration of mesh generation capability from an LLM without finetuning. Left: results from ChatGPT-4o. Right: results from LLaMA 3.1 8B-Instruct. Pretrained LLMs can generate simple 3D objects in text format; however, mesh quality and complexity are often unsatisfactory. OBJ files from the internet may vary slightly in format. The [...] indicates omitted text. 3.3. 3D-task Finetuning To equip LLMs with 3D capabilities, we construct supervised fine-tuning (SFT) dataset for training. We use 3D meshes from Objaverse [14], comprehensive 3D dataset for general objects. To build the chat dataset, we employ (1) rule-based approach and (2) LLM-based augmentation. In the rule-based approach, we design several simple patterns, such as (user) {obj} What is this? (assistant) {caption}. for mesh understanding, and (user) Create 3D model of {caption}. (assistant) {obj}. for mesh generation. For each 3D object, we randomly select pattern and replace placeholders with the mesh definition and caption. Although these conversations are straightforward, they provide the LLM with foundational knowledge of the correspondence between text and 3D representations. To enable more sophisticated conversations, we create complex text-3D dialogues. We write sample dialogues in text-3D interleaved format and use in-context learning to prompt the pretrained LLM to generate dialogues for each 3D object based on its textual description. We use combi4 Figure 7. More dialog results. LLAMA-MESH achieves several new tasks, including mesh generation and understanding, while completing other tasks like the original LLM. [...]: we omit some text to make the snippet fit into the page. nation of rule-based and LLM augmentation methods. We randomly choose from the rule-based approach and LLMbased augmentation for each mesh to construct the dialog. Figure 8 shows examples of our training data. To preserve the LLMs language capabilities, we use UltraChat [15], general conversational dataset. Our final dataset is mix of mesh generation, mesh understanding, and general conversation data, using the ratio in Table 1. 4. Experiments We first provide implementation details in Section 4.1, including dataset preparation and training. Next, in Section 4.2, we include our methods results, showcasing the quality and diversity of generated meshes and the chatting ability preserved from the pretrained LLM. We compare LLAMA-MESH with baseline methods in Section 4.3. 4.1. Implementation Details Dataset preparation We filter the Objaverse dataset [14] to select meshes with maximum of 500 faces to maintain manageable computational complexity, resulting in 31k total meshes. Each is converted to the OBJ format, and vertex coordinates are quantized into 64 bins to reduce the token sequence length without significantly compromising geometric detail. We use captions generated by Cap3D [38] as text descriptions accompanying each mesh. To avoid overfitting, we randomly rotate the meshes with degrees from {0, 90, 180, 270}, resulting in around 125k meshes. Following prior works [58], we sort the vertices by z-y-x coordinates from lowest to highest. We also sort faces by If two faces have the same lowest lowest vertex indices. index, we sort by the next lowest, and so on. The LLMs context length is set to 8k tokens. 5 Figure 8. Training dataset curated for LLAMA-MESH. We use combination of rule-based methods in (a) and (b) and LLM-augmented methods in (c) and (d) to construct an SFT dataset for mesh generation and understanding. <start/end of mesh> is shown here for illustration only and does not appear in the training data. L n T Optimization Step Figure 9. Training loss of LLAMA-MESH. The model adapts quickly to the new modality. We do not observe loss instabilities during training. Total training time comparisons are in Table 2. Dataset Mesh Generation Mesh Understanding General Conversation [15]"
        },
        {
            "title": "Items",
            "content": "# Turns Prop. 125k 125k 1M 8 4 1 40% 20% 40% Table 1. Dataset Statistics. We list each datasets number of items, number of training turns per item, and the total sample proportions. Training is performed on combined dataset, with each dataset resampled according to the ratio. We use mix of mesh generation, mesh understanding, and general conversation data to equip LLMs with 3D capabilities while maintaining their language abilities. Datasets marked with are those we constructed. Training The model is trained on 32 A100 GPUs for 21k iterations. We conduct full parameter fine-tuning. We use the AdamW optimizer [36], with learning rate of 1e 5, warm-up of 30 steps with cosine scheduling, and global batch size of 128. The total training time is around 3 days. We visualize the training loss in Figure 9, which shows the model converges rapidly on the new modality, indicating fast adaptation of knowledge. We do not observe loss spiking or instabilities during training. Figure 10. Diversity of generations. LLAMA-MESH can generate diverse shapes given the same text prompt. 4.2. Results 4.2.1. Mesh Generation Results Figure 3 shows our method generates high-quality meshes. Similarly to previous auto-regressive mesh generation methods [9, 46], our approach produces artist-like topology, as it learns mesh topology during training. We evaluate the diversity of generated meshes by providing the same text prompt multiple times and observing the variations in the resulting meshes. Figure 10 demonstrates the model generates variety of unique meshes that all satisfy the prompt, highlighting our ability to produce diverse and creative outputs. This diversity is essential for applications requiring multiple design options or variations. 6 Figure 11. Comparison of LLAMA-MESH and baselines on text-to-mesh generation. Our method achieves competitive mesh quality."
        },
        {
            "title": "Method",
            "content": "MeshXL [7] LLAMA-MESH 4.3. Comparison with Existing Methods Qualitative Comparison To assess our 3D generation capability, we compare with state-of-the-art methods in 3D mesh generation, specifically MeshXL [7], an auto-regressive text-to-mesh generation approach, and Unique3D [59], 3D generation method based on multiview image diffusion. Since Unique3D produces dense meshes, we do not visualize their topology. Additionally, because Unique3D is originally designed for imageto-3D generation, we use SDXL [40] to generate an input image for Unique3D based on the text prompt. Figure 11 shows LLAMA-MESH generates meshes of comparable quality to existing methods when given the same text prompt, capturing fine details and complex geometries effectively. While Unique3D and MeshXL are specialized models trained exclusively for mesh generation, LLAMAMESH achieves similar results while maintaining robust language understanding capabilities within single model. Training Efficiency and Model Size Table 2 compares training time, computational resources, and model sizes across different methods. MeshXL [7] trains large transformer models entirely on mesh data, demanding substantial computational resources. In contrast, leveraging pretrained LLM makes our fine-tuning approach considerably more efficient, reducing training computational cost."
        },
        {
            "title": "Model Size\nGPU hours",
            "content": "350M 1.3B 6000 23 232 8B 2400 Table 2. Training time comparison. Compared to MeshXL [7], LLAMA-MESH uses far fewer GPU hours despite its larger model size, benefiting from using pretrained LLM weights. 4.2.2. Language and Conversational Abilities Qualitative Results After fine-tuning for mesh generation, we evaluate whether LLAMA-MESH retains its language understanding capabilities. Figures 1 and 7 show the model engages in coherent and contextually appropriate dialogues, comprehending complex instructions, asking clarifying questions, and providing detailed responses, demonstrating its language proficiency remains intact. Quantitative Results Table 3 presents quantitative results evaluating language abilities. We report the performance of our model, LLAMA-MESH (8B), and compare it with baseline models of various sizes: LLaMA3.1 (8B), LLaMA3.2 (3B), and LLaMA3.2 (1B). The metrics include MMLU [18] (5-shot), PIQA [4] (0-shot), HellaSwag [70] (0-shot), and GSM8K [13] (8-shot), which assess the models general knowledge, commonsense reasoning, and mathematical problem-solving skills. Our model, fine-tuned to generate OBJ files for 3D mesh generation, retains language understanding and reasoning capabilities comparable to the baseline models. This demonstrates that LLAMA-MESH successfully extends the LLMs functionality to 3D content generation while preserving its original language capabilities."
        },
        {
            "title": "Metric",
            "content": "LLaMA3.1 (8B) LLAMA-MESH (8B) LLaMA3.2 (3B) LLaMA3.2 (1B) MMLU (5-shot) PIQA (0-shot) Hellaswag (0-shot) GSM8K (8-shot) 66.07 81.01 79.19 77.18 61.74 79.16 77.35 62.09 59.44 75.52 70.47 66. 44.17 74.10 60.80 34.27 Table 3. Does LLAMA-MESH preserve language capabilities? We report the performance of LLAMA-MESH (8B) and compare it with base models of different sizes: LLaMA3.1 (8B), LLaMA3.2 (3B), and LLaMA3.2 (1B). The metrics include MMLU (5-shot), PIQA (0shot), HellaSwag (0-shot), and GSM8K (8-shot), which assess the models general knowledge, commonsense reasoning, and mathematical problem-solving abilities. Takeaway: Our method (in the blue column), after being fine-tuned to generate OBJ files, maintains language understanding and reasoning capabilities comparable to the base model while extending its functionality to 3D mesh generation. 5. Discussion 5.1. Limitations While LLAMA-MESH shows the LLMs potential for 3D mesh generation, there are several limitations to address in future work. Quantizing vertex coordinates into limited number of bins can lead to loss of geometric detail, affecting the generated meshes fidelity. Also, the context length constraints the models ability to generate highly complex or large-scale 3D structures. Currently, we only support maximum of 500 faces, limiting the mesh detail level. We observe slight degradation in language ability after fine-tuning as in Table 3. We conjecture this is due to relying solely on UltraChat as our text instruction dataset. Incorporating more diverse and high-quality text instruction datasets could help preserve the language capabilities of LLAMA-MESH. Also, we only use 3D meshes from Objaverse [14] dataset for training. We believe incorporating more datasets could enrich the generated results. Additionally, we use only an 8B model due to limited computational resources; we believe that using larger LLaMA model would further improve the results. 5.3. Future Work Future work could explore more efficient encoding schemes for 3D data within language models, methods to handle longer context lengths, and techniques to improve the geometric precision of generated meshes. Integrating additional modalities, such as textures or physical properties, and extending the models capabilities to handle dynamic scenes are promising directions. Integrating 3D mesh generation into LLMs opens up exciting possibilities for interactive design, where users can converse with model to create and manipulate 3D objects in real time. Such advancements could revolutionize virtual reality, gaming, education, and manufacturing by making 3D content creation more intuitive. We envision future where large language models are universal generative tools capable of seamlessly producing content across multiple modalities, including text, images, and 3D structures."
        },
        {
            "title": "Reproducibility",
            "content": "We provide comprehensive implementation details in Section 3, including data preprocessing steps, model architecture specifications, and training procedures. All hyperparameters, training configurations, and evaluation protocols are detailed in Section 4.1. 5.2. Conclusion"
        },
        {
            "title": "Ethics Statements",
            "content": "We introduced LLAMA-MESH, novel approach that unifies 3D mesh generation with large language models by representing meshes as plain text. By fine-tuning LLaMA on 3D dialog dataset we curated, we enabled it to generate 3D meshes directly from textual prompts without expanding the vocabulary or introducing new tokenizers. Our method preserves the language understanding capabilities of the base model while extending its generative abilities to the 3D domain. Experimental results show that LLAMAMESH achieves mesh generation quality comparable to specialized models trained from scratch on 3D data. This work represents significant step toward integrating multi-modal content generation within cohesive language model. Our work unifies 3D mesh generation with LLMs, and thus we may inherit ethical concerns from both language models and generative models in general. Potential risks include generating 3D content that could be misused for malicious purposes, reproducing biased or inappropriate content present in training data, and potentially impacting the livelihoods of 3D artists and designers due to automation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Jiahui Huang, Haoxiang Wang, Yiwen Chen, David Acuna and Amlan Kar for their helpful feedback."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 2 [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natIn Proceedings of the AAAI conference on ural language. artificial intelligence, pages 74327439, 2020. 7 [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 1 [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for arXiv preprint high-quality text-to-3d content creation. arXiv:2303.13873, 2023. [7] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, et al. Meshxl: Neural coordinate field for generative 3d foundation models. arXiv preprint arXiv:2405.20853, 2024. 3, 7 [8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved textto-3d generation with explicit view synthesis, 2023. 2 [9] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. 3, 6 [10] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. 3 [11] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585, 2023. 2 Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2 [13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 7 [14] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 4, 5, 8 [15] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. 5, [16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 2, 3, 4 [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. MeaarXiv suring massive multitask language understanding. preprint arXiv:2009.03300, 2020. 7 [19] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2 [20] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual synthesis, 2023. 2 [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 2 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [23] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 2 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, [24] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity 9 mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2 [25] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching, 2023. 2 [26] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv preprint arXiv:2409.09788, 2024. [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 2 [28] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 2 [29] Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, and Yueqi Duan. Sherpa3d: Boosting high-fidelity text-to-3d generation via coarse 3d prior, 2023. 2 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1 [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [33] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. 2 [34] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928, 2023. 2 [35] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, MingYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object synthesis, 2023. 2 [36] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [37] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models, 2023. URL https://arxiv. org/abs/2304.09842, 2023. 1, 2 [38] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. [39] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning, pages 72207229. PMLR, 2020. 3 [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 7 [41] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [44] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. [45] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 2 [46] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1961519625, 2024. 3, 6 [47] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior, 2023. 2 [48] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 2 [49] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation, 2024. 2 [50] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. 3 [51] Aaron van den Oord, Oriol Vinyals, and Koray Neural discrete representation learning, Kavukcuoglu. 2018. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. 1 10 [53] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Pf-lrm: Pose-free large reconstruction model Zhang. arXiv preprint for arXiv:2311.12024, 2023. 2 joint pose and shape prediction. [54] Xinzhou Wang, Yikai Wang, Junliang Ye, Zhengyi Wang, Fuchun Sun, Pengkun Liu, Ling Wang, Kai Sun, Xintong Wang, and Bin He. Animatabledreamer: Text-guided nonrigid 3d model generation and reconstruction with canonical score distillation, 2023. 2 [55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, [56] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillaIn Thirty-seventh Conference on Neural Information tion. Processing Systems, 2023. 2 [57] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh arXiv preprint with convolutional reconstruction model. arXiv:2403.05034, 2024. 2 [58] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. 3, 5 [59] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 7 [60] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 2 [61] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2 [63] Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, and Xiaohui Zeng. Latte3d: Large-scale amortized text-to-enhanced3d synthesis. arXiv preprint arXiv:2403.15385, 2024. 2 [64] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [65] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, 11 Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. [66] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1622716237, 2024. 2 [67] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2 [68] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36, 2024. 2 [69] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, SongHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation, 2023. 2 [70] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. 7 [71] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2 [72] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [73] Yunzhi Zhang, Zizhang Li, Matt Zhou, Shangzhe Wu, and The scene language: Representing scenes arXiv preprint Jiajun Wu. with programs, words, and embeddings. arXiv:2410.16770, 2024. 2 [74] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2 [75] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance, 2023. 2 [76] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable singleview 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147, 2023."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "NVIDIA"
    ]
}