{
    "paper_title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
    "authors": [
        "Sayan Deb Sarkar",
        "RÃ©mi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\\%$ and token usage by up to $93\\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding."
        },
        {
            "title": "Start",
            "content": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models Sayan Deb Sarkar1,2,, RÃ©mi Pautrat2, Ondrej Miksik2, Marc Pollefeys2,3, Iro Armeni1, Mahdi Rad2, Mihai Dusmanu2 1Stanford University, 2Microsoft Spatial AI Lab, 3ETH Zurich Part of work done at Microsoft Equal Supervision Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding. Correspondence: Sayan Deb Sarkar at sdsarkar@stanford.edu Project: https://cope.github.io 6 2 0 F 3 1 ] . [ 1 1 9 1 3 1 . 2 0 6 2 : r Figure 1 CoPE-VideoLM is codec-aware tokenization framework for Video Language Models that replaces dense RGB frame encoding with lightweight structured representations derived from codec primitives. Instead of treating every frame as full image, the model processes only sparse I-frames through vision encoder, while P-frames are converted into compact tokens using their motion vectors and residuals. By leveraging the inherent sparsity and structure of standard video codecs, CoPE-VideoLM avoids redundant RGB processing, reduces visual token usage by up to 93%, and cuts time-to-first-token by up to 86%, compared to standard dense VideoLMs."
        },
        {
            "title": "1 Introduction",
            "content": "Video Language Models (VideoLMs) represent major advancement in multi-modal AI [15], enhancing Vision Language Models (VLMs) with temporal reasoning that allows them to understand how visual narratives, objects, actions, and relationships evolve across video sequences. This enables wide range of downstream applications, from more natural human-computer interaction [6, 7] through video question-answering to robotics [8, 9], where agents must understand sequential actions [10, 11]. Their importance extends beyond this technical capability: they represent fundamental step toward AI systems that perceive and reason about the world more like humans do, processing information not as disconnected snapshots but as continuous, meaningful experiences. As video continues to dominate digital content, with platforms generating petabytes of video daily, models that can watch and understand video at scale [1214] become essential for the next generation of AI applications. VideoLMs have maximum context window limiting the amount of information that can be provided as input. This is not only training artifact, but also related to hardware constraints, as larger context windows require linearly more memory and quadratic compute [15]. To fit in the context window, existing VideoLMs select subset of video frames as keyframes either through some handcrafted heuristics (e.g., uniform temporal [16]) or learned methods (e.g., flexible frame selection [17]). In this setup, proprietary models with extended context lengths of up to 1 million tokens can take as input one hour of video at 1 FPS [16]. Open-source models [4] have much smaller budget and generally sample fixed number of frames (e.g., 64) regardless of video length. This is rather counterintuitive, as the information content naturally scales with video duration. there is generally high redundancy between consecutive frames (even when downsampled to 1 FPS); therefore, using the same token budget for each keyframe is suboptimal. Furthermore, treating each keyframe as full image also significantly increases the prefill time [18], delaying the timeto-first token (TTFT) of the VideoLM. Low TTFT is critical for user experience and essential for robotics applications requiring real-time responsiveness. To address these limitations, we propose to leverage video compression, decades-old field that addresses redundancy and sparsity in these streams [19, 20]. The main idea of video codecs is to encode what moves between frames as motion vectors and what changes between two frames as residuals, thus preserving the sparsity of the original stream. Their native structure minimizes redundancy by picking keyframes either at fixed interval or when large differences are detected. In typical streaming setup, these codecs provide keyframes every 510 seconds, encoding only the changes for all other frames. We propose to directly encode the motion vectors and residuals (i.e., codec primitives), by learning to represent them as aligned tokens compatible with image features, then concatenating them alongside keyframe image tokens during inference. An overview of our approach is shown in Fig. 1. This provides two major advantages: first, we avoid the costly full image encoding for most of the frames, and second, we can use much fewer tokens given the sparse nature of these primitives, thus reducing TTFT by significant margin. Optionally, the motion vectors and residuals of several subsequent frames can be grouped together to strike trade-off between fine-grained representation and total number of tokens. Note that this methodology is valuable beyond VideoLMs and can be used for other tasks such as video retrieval or action recognition. Moreover, any keyframe sampling approach fundamentally limits VideoLMs understanding by providing only sparse temporal coverage: this can miss both macro-level events crucial for high-level comprehension and micro-level details necessary for recognizing fine-grained actions. In addition, To this end, we propose to use transformer-based encoders to aggregate the information of all motion vectors and residuals from given frame. These encoders are first pre-trained to adapt them to the space of the image encoder and then, integrated with state-of-the-art open-source Vide2 oLM for end-to-end finetuning. The final model matches the performance of top VideoLMs and even surpasses them on several benchmarks despite using substantially fewer tokens. We extensively validate our approach across 14 benchmarks, demonstrating consistent gains across general video QA, temporal understanding, long-form reasoning, and spatial scene understanding tasks. Our main contributions are as follows: We propose novel way to encode videos for VideoLMs by leveraging the standardized compressed representation of video codecs. Codec primitives allows us to skip redundant RGB information, reducing the TTFT by up to 86% and token usage by up to 93%. We introduce two lightweight architectures for encoding codec primitives, achieving substantially higher compression rates / lower token counts than traditional image-based encoders. We propose pre-training strategy for codecprimitive encoders that aligns their representation space with that of image encoders, enabling faster training and quicker convergence when integrated with the VideoLM."
        },
        {
            "title": "Large",
            "content": "Recent advances Video Language Models. Language Models in Multimodal (MLLMs) [2132] have extended image-based architectures into the video domain, giving rise to VideoLMs capable of temporal reasoning over dynamic visual content. MLLMs typically comprise vision encoder (e.g., CLIP [33] or SigLIP [34]), modality alignment mechanism also known as adapter (e.g., linear projection, QFormer [35, 36], or gated cross-attention [37]), and an LLM backbone (e.g., LLaMA [38], Vicuna [39], or Qwen [14, 40, 41]) for mulEarly VideoLMs such as timodal decoding. Video-LLaMA [42] and VideoChat2 [1] relied on sparsely sampled keyframes and lightweight adapters, but were limited by short context and redundant tokenization. Subsequent models, including Video-LLaMA3 [5], improve efficiency through extended context windows and adaptive token compression, while others [4346] pool or merge tokens across spatial and temporal dimensions to fit longer clips within fixed budgets. Models such as TimeChat [47] and LLaVA-Video [4] further enhance temporal reasoning: TimeChat integrates timestamp-aware encoding with sliding video Q-Former for precise event localization, while LLaVA-Video leverages large-scale instruction tuning and optimized frame sampling for improved coherence. Closed-source systems like Gemini [13, 16, 48], GPT [12, 48] and Claude [49] demonstrate impressive fine-grained and long-context understanding but depend on proprietary data and undisclosed architectures. Despite these advances, open-source VideoLMs still process videos as dense RGB frame collections, overlooking the structured redundancy inherent in standard video codecs. Leveraging codec primitives like motion vectors and residuals, our approach directly encodes temporal dynamics, supporting efficient long-context understanding while retaining fine-grained detail. Token Compression. Compression techniques reduce the number of visual tokens fed into VideoLMs by removing redundancy while preserving semantic fidelity. Existing methods can be grouped into heuristic and learnable approaches. Heuristic methods apply rule-based feature reduction such as uniform downsampling [50], spatial or temporal pooling [4, 51, 52], or similarityguided merging [5355]. Learnable compression modules, including Q-Former [40, 56, 57], Perceiver Resampler [58], and memory-based mechanisms such as MovieChat [59, 60] and ChatUniVi [54], generate compact latent representations of video frames before integrating them to the LLM. Attention-based token compression methods [6164] leverage the inherent sparsity of visual feature attention [65] to guide token pruning. FrameFusion [62] removes visually redundant tokens, while PyramidDrop [63] and SparseVLM [64] dynamically modulate pruning ratios across model layers. FastV [61] first observed that visual tokens receive negligible attention after early layers and showed that pruning up to 50% of them preserves performance. On the other hand, 3 temporal pooling approaches [43, 51, 66, 67] exploit inter-frame redundancy by downsampling at the frame level. DyCoke [68] performs temporal pruning within grouped frames, and LLaVAScissor [69] leverages semantic connected components for spatio-temporal compression, achieving strong results particularly at low token retention ratios (See Supp. Sec for comparative study). More adaptive approaches such as AdaReTake [70] and FlexSelect [71] dynamically allocate compression budgets across layers or leverage cross-modal attention to filter tokens without retraining. However, these methods rely on dense RGB frame encodings and neglect intrinsic temporal redundancy. In contrast, our native codec representation inherently encodes only meaningful temporal changes rather than removing information post hoc. There Compressed Video Representation. has been growing interest in exploiting motion vectors and residuals directly from compressed video streams for visual understanding, particularly in action recognition, thus bypassing the costly full-frame processing. Early works such as CoViAR [72] pioneered this direction by training separate 2D CNNs on Iand P-frames and averaging their predictions, while TEAM-Net [73] introduced cross-modality interaction modules to better fuse compressed signals. However, these methods ignore inter-modal dependencies and temporal ordering, leading to high computational overheads and the need for multi-crop or multiclip inference. Later, CV-C3D [74] and DMCNet [75] extended compressed-domain learning to 3D CNNs and optical-flow-guided distillation, respectively, achieving improved accuracy at the cost of higher inference latency. Distillation-based approaches [76, 77] further align compresseddomain models with raw-domain teachers, but still require access to decoded RGB frames during training, reducing efficiency gains. Recent transformer-based models such as MMViT [78] attempt to leverage self-attention across codec primitives but incur significant inference costs. CompressedVideoMAE [79], introduces masked autoencoding pre-training in the compressed domain, demonstrating that spatial-temporal representations can be effectively learned using only motion vectors and residuals, achieving performance comparable to raw-video pretraining with far less compute. More recently, codec-based representations have been explored in VideoLMs. Video-LaVIT [80] discretizes motion vectors into language-like tokens, and EMA [81] discards residuals and aggregates I-frames and motion vectors within each GOP into fixed-length summary representation, similar to Video-VAE [82]. In contrast, CoPE-VideoLM treats both motion vectors and residuals as structured and unified codecnative representation and constructs variablelength, temporally ordered token sequence that adapts to content dynamics. By preserving finegrained motion and appearance signals instead of compressing them into fixed summaries or imagelike proxies, our method supports efficient and temporally consistent modeling across broader video language understanding."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Modern video codecs such as MPEG-4, H.264, and HEVC [83, 84] achieve high compression ratios by exploiting temporal redundancy across consecutive frames. Let video be sequence of frames (ğ¹ (1) , . . . , ğ¹ (ğ‘‡ ) ). Each of these frames can be an I-frame (intra-coded), P-frame (predictive), or optionally B-frame (bi-directional predictive). Consecutive frames are organized in Group of Pictures (GOP) structure as illustrated in Fig. 2. I-frames. An I-frame, ğ¼ (ğ‘¡) , is an RGB image encoded independently without the use of preceding or subsequent frames. It is used as reference point of the Group of Pictures and provides full visual representation. P-frame ğ‘ƒ (ğ‘¡) contains only the P-frames. changes from the previous frame, be it reference frame ğ¼ (ğ‘¡1) or some other P-frame ğ‘ƒ (ğ‘¡1) . The difference is defined by two components: Motion vectors ğœ(ğ‘¡) , which describe block-wise displacements from the reference to the tar4 Figure 2 Overview of our pipeline. Given video in its raw codec representation, our framework leverages the GOP structure for efficient, codec-aware tokenization. I-frames are processed by standard frozen vision encoder (ğœ™RGB) to produce dense RGB tokens. P-frames, however, bypass full RGB decoding. Their raw components, motion vectors and residuals, are instead fed into our lightweight Î”-Encoder (ğœ™Î”) to generate small set of highly compact Î”-tokens. The final token stream, an interleaved sequence of I-frame tokens and Î”-tokens, is consumed by the LLM, enabling dense temporal coverage at fraction of the standard token count and runtime. get frame, resembling coarse optical flow. Residuals ğ›¿(ğ‘¡) , which capture block-wise pixel corrections that remain after motion compensation. The recurrence [72, 79] for reconstructing Pframe is: ğ‘– = Ë†ğ¼ (ğ‘¡1) Ë†ğ¼ (ğ‘¡) ğ‘–ğœ(ğ‘¡) ğ‘– + ğ›¿(ğ‘¡) ğ‘– , (1) where ğ‘– is the pixel coordinate index and Ë†ğ¼ (ğ‘¡1) = ğ¼ (ğ‘¡1) ğ¹ (ğ‘¡) is an I-frame . (2) Hence, P-frames contain only incremental temporal information, and are therefore smaller in file size than I-frames. B-frames. B-frame leverages both preceding and subsequent frames to encode its differences. While this bidirectional prediction achieves the highest compression efficiency, it increases decoding complexity: B-frames must wait for future I/P-frames, creating mismatch between decode and display order. Consequently, they are less suited for streaming or real-time use. We therefore focus on P-frames, which depend only on past references and align naturally with the causal processing required by VideoLMs. Group of Pictures (GOP). GOP is cycle structure that comprises one I-frame together with any mixture of Pand/or B-frames, e.g., ğ¼ ğµ ğ‘ƒ ğ‘ƒ ğµ ğ‘ƒ ğ‘’ğ‘¡ğ‘. The GOP structure and length control the tradeoffs between the compression efficiency, quality, and the capability for random access. Typical applications use varying configurations: all three frame types are used by the H.264 codec, while early MPEG-4 uses mainly the Iand the P-frames (see Supp. Sec for detailed explanation). Implication for VideoLMs. Despite this rich structure, current VideoLMs discard codec information and fully decode and tokenize dense RGB frames, ignoring the inherent sparsity of Pand Bframes. This results in unnecessary computation and inflated token counts."
        },
        {
            "title": "3.2 CoPE-VideoLM",
            "content": "Our framework explicitly leverages the GOP structure and introduces codec-aware tokenization framework that seamlessly integrates with Vide5 oLMs. Instead of unnecessarily encoding each frame as RGB patches like prior work, we retain I-frames as full RGB tokens and encode P-frames into lightweight and compact Î”-tokens (delta for difference) obtained from motion vectors and residuals, thereby preserving temporal coverage without exceeding token budgets. An illustration of this process is presented in Fig. 2. Given video = (ğ¹ (1) , . . . , ğ¹ (ğ‘‡ ) ), each frame ğ¹ (ğ‘¡) is represented as: ğ¹ (ğ‘¡) = (cid:40)ğ¼ (ğ‘¡) , ğ‘ƒ (ğ‘¡) = (ğœ(ğ‘¡) , ğ›¿(ğ‘¡) ), if ğ¹ (ğ‘¡) is an I-frame, if ğ¹ (ğ‘¡) is P-frame, (3) where ğœ(ğ‘¡) is set of block-wise motion vectors (usually one per up to 16 16 region), and ğ›¿(ğ‘¡) is set of block-wise residuals. For our downstream processing, we represent these as tensors: ğœ(ğ‘¡) â„¤ğ»ğ‘Š 2 is the sparse tensor constructed from the block-wise motion vectors and ğ›¿(ğ‘¡) â„ğ»ğ‘Š ğ¶ is the sparse tensor of residuals. I-frames ğ¼ (ğ‘¡) are passed I-frame processing. through frozen vision encoder ğœ™RGB, producing ğ‘€ dense tokens: ğ‘‹ (ğ‘¡) ğ¼ = ğœ™RGB(ğ¼ (ğ‘¡) ) â„ğ‘€ğ‘‘ . (4) P-frame processing. Each P-frame ğ‘ƒ (ğ‘¡) is mapped into much more compact representation consisting of ğ‘ ğ‘€ tokens by the Î”-Encoder ğœ™Î”: ğ‘‹ (ğ‘¡) ğ‘ƒ = ğœ™Î” (ğœ(ğ‘¡) , ğ›¿(ğ‘¡) ) â„ğ‘ ğ‘‘ . (5) In the setup described so far, P-frame fusion. processing all frames at the native frame rate is mandatory as codec primitives are defined relative to previous frames and skipping frames invalidates these dependencies. For example, let us consider 30 FPS video with GOP size of 240 frames (8 seconds). Our method yields ğ‘€ + 239ğ‘ tokens per GOP, compared to 240ğ‘€ if all frames were encoded as RGB images. Fine-grained action recognition may require this exhaustive temporal coverage, but most video understanding tasks can often be done with sparser coverage. Rather than processing all frames at the native frame rate, we can fuse ğ‘  consecutive P-frames, encoding their combined changes relative to frame (ğ‘¡ğ‘ ) (rather than the immediately preceding frame). The maximum number of P-frames that can be fused is bounded by the GOP size. In the running example, using ğ‘  = 30 P-frames (effectively 1 FPS) for fusion reduces the token count to ğ‘€ +7ğ‘ ğ‘€ +239ğ‘ per GOP, instead of 8ğ‘€ when encoding RGB frames at 1 FPS. This fusion offers codec-native way to trade temporal resolution for efficiency and can be tuned to match available compute and task requirements. For clarity, we treat the temporal index (ğ‘¡) as already incorporating the P-frame fusion, so ğ¹ (ğ‘¡) always depends on ğ¹ (ğ‘¡1) , though (ğ‘¡) may no longer correspond to the raw frame indices. Î”-Encoder. The Î”-Encoder ğœ™Î”  (Fig. 3)  is designed to process the motion vectors ğœ(ğ‘¡) and residuals ğ›¿(ğ‘¡) through two specialized branches. The motion vectors ğœ(ğ‘¡) are processed via multi-layer MLP to extract local features over grid of size ğ»ğº ğ‘Šğº flattened as â„( ğ»ğºğ‘Šğº ) ğ‘‘. These features are then compressed via motion transformer ğœƒmotion with set of ğ¾ğœ learnable query tokens that can attend to all ğ»ğºğ‘Šğº input tokens and aggregate their information as: ğœ(ğ‘¡) tok = ğœƒmotion(MLP(ğœ(ğ‘¡) )) . (6) Only these ğ¾ğœ compressed motion tokens ğœ(ğ‘¡) â„ğ¾ğœ ğ‘‘ are used in the VideoLM. tok frame ğ›¿(ğ‘¡) is embedded by The residual lightweight ResNet-18 [85] module to extract local features over the same grid size ğ»ğº ğ‘Šğº. Similarly to above, residual transformer ğœƒresidual aggregates and compresses the raw features to set of ğ¾ğ›¿ compressed residual tokens ğ›¿(ğ‘¡) tok â„ğ¾ğ›¿ ğ‘‘ as: ğ›¿(ğ‘¡) tok = ğœƒresidual(ResNet(ğ›¿(ğ‘¡) )) . (7) In practice, we set Interleaved token stream. ğ¾ğœ = ğ¾ğ›¿ = 4 and thus, ğ‘ = 8 (see Supp. for an ablation study). The final visual sequence input to the LLM is an ordered concatenation of I-frame and P-frame tokens: 6 ğ‘‹ = (cid:2) ğ‘¥ (1) , ğ‘¥ (2) , . . . , ğ‘¥ (ğ‘‡ ) (cid:3) , where ğ‘¥ (ğ‘¡) = (cid:40)ğ‘‹ (ğ‘¡) , ğ¼ ğ‘‹ (ğ‘¡) ğ‘ƒ , if ğ¹ (ğ‘¡) if ğ¹ (ğ‘¡) is an I-frame is P-frame . (8) and ğ‘‹ (ğ‘¡) The LLM can consume ğ‘‹ (ğ‘¡) ğ‘ƒ alongside texğ¼ tual instructions, without any architectural modifications. This approach reduces the redundant data considerably while preserving accurate temporal coverage and enables smooth scalability to long videos by adjusting the I-frame density, Pframe grouping, and token allocation. Figure 3 Î”-encoder processes motion vectors and residuals through two lightweight branches designed to extract and compress codec-domain information. The resulting motion and residual tokens are concatenated to form the Î”-tokens used for P-frames, providing an efficient representation, which is projected to the RGB token space during pre-training."
        },
        {
            "title": "3.3 Training Paradigm",
            "content": "raw Ë†ğ¼ (ğ‘¡) . The process can be summarized as: The training is done in two stages. First the Î”encoder is pre-trained in order to render it compatible with the image encoder. Then, this encoder is integrated into VideoLM and the whole pipeline is fine-tuned end-to-end. Î”-encoder pre-training. The primary difficulty lies in ensuring that the Î”-tokens ğ‘‹ (ğ‘¡) ğ‘ƒ are aligned with the image tokens ğ‘‹ (ğ‘¡) . To achieve this, we ğ¼ pre-train the Î”-Encoder as modality adapter that enables codec-derived primitives (ğœ(ğ‘¡) , ğ›¿(ğ‘¡) ) to be compatible with the embedding space defined by vision encoder. By aligning the Î”-tokens with this space, P-frames can then be compactly represented and replace standard RGB tokens within VideoLM. For pre-training, two additional modules are used on top of the outputs of the Î”-encoder. First, reference transformer ğœƒref uses the image tokens from ğ¼ (ğ‘¡1) and the compressed motion vector tokens ğœ(ğ‘¡) , and its aim is to understand how the intok formation moved in the image and how it modified the original tokens. This is akin to the warping in Eq. 1. Second, warped transformer ğœƒwarped takes these enriched tokens and the residual tokens ğ›¿(ğ‘¡) in order to add the residual information emb as in Eq. 1. These final features Ë†ğ‘‹ (ğ‘¡) should be ğ‘ƒ similar to the image features extracted from the ğ‘‹ (ğ‘¡1) =ğœ™ğ‘…ğºğµ (ğ¼ (ğ‘¡1) ) ğ¼ warped =ğœƒref(ğ‘‹ (ğ‘¡1) Ë†ğ‘‹ (ğ‘¡1) , ğœ(ğ‘¡) tok) ğ‘ƒ =ğœƒwarped( Ë†ğ‘‹ (ğ‘¡1) Ë†ğ‘‹ (ğ‘¡) warped ğ¼ , ğ›¿(ğ‘¡) tok) . (9) (10) (11) To align Î”-tokens with image tokens, we apply patch-wise regression against the outputs of frozen vision encoder. Let ğ‘‹ (ğ‘¡) = ğœ™RGB(Ë†ğ¼ (ğ‘¡) ) â„ğ‘€ğ‘‘ denote the tokens of the ground-truth target frame. We minimize: ğ¼ LMSE = 1 ğ‘€ ğ‘€ ğ‘–=1 (cid:13) (cid:13)ğ‘‹ (ğ‘¡) ğ¼ (ğ‘–) Ë†ğ‘‹ (ğ‘¡) ğ‘ƒ (ğ‘–)(cid:13) (cid:13) 2 . (12) Unlike global contrastive losses, this fine-grained objective enforces spatially consistent alignment across patches. As result, the Î”-Encoder is encouraged to produce representations that are more closely aligned with the RGB token space, aiming to improve the integration of Iand Pframes during downstream VideoLM training (see Supp. Sec. G.2 to understand the effect of the pretraining phase). Integration into VideoLMs. After pre-training, we integrate the Î”-encoder ğœ™Î” into the VideoLM pipeline for full fine-tuning. We interleave the tokens coming from and frames as described in Eq. 8. Note that, the referenceconditioned branches (Reference / Warped) from 7 the Î”-encoder pre-training stage are not used at this stage, so no RGB reference frames are processed for the P-frames when training the language model. This yields substantial compute and memory reduction, as shown in Sec. 4.4. The LLM architecture and training objective remain unchanged (standard instruction tuning next-token prediction loss)."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Training Pipeline. For the sake of simplicity, we re-encode videos to the MPEG-4 codec at 30 FPS and choose GOP size of 240 frames. Furthermore, for the P-frames, we use fusion size of ğ‘  = 30 leading to 1 FPS. We use LLaVA-Video7B [4] as the base VideoLM, which consists of SigLIP [34] as the vision encoder and Qwen2 [86] as the language model. First, we pre-train the Î”encoder using 0 30 second videos from PerceptionTest [87] training set. Second, we fine-tune the base VideoLM with our input representation using LLaVA-Video-178K [4] comprising 1.39M question-answer samples. We use learning rate of 1e5, total batch size of 128, and train for 21K GPU hours (64 A100-80G GPUs for 14 days). Evaluation Benchmarks. We comprehensively evaluate our method across 14 video benchmarks spanning four categories: (i) general video QA: PerceptionTest [87], NextQA [88], ActivityNetQA [89], and VideoMME [90]; (ii) temporal reasoning: TempCompass [91], TOMATO [92], CVRR-ES [93] and MVBench [94]; (iii) long-form and instruction-following: LongVideoBench [95], LVBench [96], Video-TT [97], and VideoMMMU [98]; and (iv) spatial scene understanding: ScanQA [99] and SQA3D [100]. Our primary comparison is with LLaVA-Video-7B [4], which serves as our base model. We additionally compare with various similar open-source approaches. Following standard practice, we use lmms-eval [101] for evaluation. ActNet-QA [89] uses GPT-based evaluation; we report scores using GPT-4o1 in Tab. 1 and GPT-3.5-Turbo2 in Tab. 2 for fair comparison with prior work. Frame Sampling. Most open-source VideoLMs with 32K token window sample 64 frames per video, regardless of length [4, 102]. As result, unlike proprietary models, they cannot process videos at 1 FPS for videos longer than 64 seconds. Our framework, however, enables 1 FPS sampling (counting both Iand Pframes) for longer videos, by adjusting the number of keyframes per GOP. This allows the token budget to better scale with video duration, matching the natural growth of information content. In our experiments, for fair comparison, we sample frames at 1 FPS up to 64 GOPs unless otherwise specified. For videos longer than 64 GOPs (512s), we first perform uniform GOP sampling, and then encode only the I-/P-frames that are part of these sampled GOPs."
        },
        {
            "title": "4.2 Effectiveness of Î”-tokens",
            "content": "We first evaluate whether the proposed Î”-tokens derived from P-frames provide understanding capabilities comparable to dense RGB representations. Tab. 1 summarizes results on three benchmarks under different sampling configurations. Across all settings, our codec-aware representation achieves consistent improvements in accuracy compared to the same-sampling baseline, and helps close the gap to the next denser sampling while using significantly fewer tokens. Remarkably, even under aggressive compression (e.g., 1 keyframe per GOP), our model maintains strong performance with over an order-of-magnitude token reduction, demonstrating that the Î”-encoder successfully captures motion and appearance cues critical for temporal reasoning. At higher frame densities (e.g., 4 keyframe per GOP), our model not only matches but often surpasses the performance of the 64-frame LLaVA-Video-7B [4] baseline despite using only fraction of its tokens. These results confirm that our codec-aware tokenization preserves temporal semantics and finegrained dynamics. The largest relative gains are observed on PerceptionTest [87] (+6.9%) and 1gpt-4o-2024-11-20. 2Azure deployment of gpt-3.5-turbo-0613. 8 Model Sampling PerceptionTest NextQA ActNet-QA Token (%) Acc. (%) Token (%) Acc. (%) Token (%) Acc. (%) LLaVA-Video 1 keyframe / GOP + 7 P-frames / GOP Ours LLaVA-Video 2 keyframes / GOP + 6 P-frames / GOP Ours LLaVA-Video 4 keyframes / GOP + 4 P-frames / GOP Ours 005.3 006.8 009.7 011.0 018.6 019.5 60.4 64.7 62.1 67.8 63.6 70. +4.3 +5.7 +6.9 008.3 010.8 015.9 018.1 031.0 032.6 77.9 78.3 79.2 80.3 80.5 81.8 +0. +1.1 +1.3 021.6 028.8 042.5 048.8 084.3 088.7 61.7 62.3 62.9 63.3 63.6 64.1 +0.6 +0. +0.5 LLaVA-Video 64 keyframes total 100 67.9 100 83. 100 63.6 Table 1 Token Efficiency vs. Accuracy in Video QA. We report the performance of LLaVA-Video (7B) at different number of keyframes per GOP, as well as in the default setup of selecting 64 keyframes regardless of video length. For each setting, we also report the performance of our method using the same keyframes as I-frames and the remaining frames in the GOP as P-frames. We report accuracy (Acc., %) and the % of tokens used compared to the default setup (64 keyframes). Our method only adds low number of Î”-tokens compared to its associated baseline and these help close the gap compared to the next baseline that is using significantly larger number of tokens. Model PerceptionTest NextQA ActNet-QA val mc test VideoMME w/o sub sub Proprietary Models GPT-5 [48] Gemini 3 Pro [103] Gemini 2.5 Pro [13] Claude Sonnet 4.5 [49] Open-Source VideoLMs Video-LaVIT [80] EMA-7B [81] VILA-40B [50] LongVA-7B [27] IXC-2.5-7B [29] LLaVA-OV-7B [26] Apollo-7B [32] Oryx-7B [102] LLaVA-Video-7B [4] Ours-7B - - - - 47.9 - 54.0 - 34.4 57.1 67.3 68.6 67.9 70.5 86.3 84.3 85.3 79.2 - - 67.9 68.3 71.0 79.4 - 81.9 83.2 81.8 - - - - 50.1 52.1 58.0 50.0 52.8 56.6 - - 56.5 58.8 83.3 88.6 87.8 74.2 - 53.4 60.1 52.6 55.8 58.2 61.3 58.3 63.3 61.7 86.9 87.5 87.8 80.5 - 58.4 61.1 54.3 58.8 61.5 63.3 62.6 69.7 67. Table 2 General video understanding benchmarks. The best results among open-source methods are highlighted as first , second , and third . Our model achieves state-of-the-art performance among opensource video language models of comparable scale. These results demonstrate that our codec-aware tokenization framework effectively captures video semantics for diverse understanding tasks. NextQA [88] (+1.3%), indicating stronger reasoning under limited token budget. As shown in the plots, our method advances the Pareto-optimal frontier, achieving competitive accuracy with substantially reduced token counts, directly translating to lower TTFT and enabling more responsive video understanding systems."
        },
        {
            "title": "4.3 Comparison with Current Ap-",
            "content": "proaches We further benchmark CoPE-VideoLM against broad range of both open-source and proprietary VideoLMs. We organize the comparison into four groups: (i) general video QA (Tab. 2), (ii) temporal reasoning (Tab. 3), (iii) long-form and instruction-following (Tab. 4), and (iv) spatial scene understanding (Supp. Sec. B). General Video QA. As shown in Tab. 2, despite being trained on smaller corpus than most competing models, our codec-aware formulation achieves competitive or superior results across all major benchmarks. By leveraging motion vectors and residuals directly from the compressed stream, CoPE-VideoLM encodes substantially more frames within the same token budget, enhancing temporal coverage without sacrificing spatial fidelity. On PerceptionTest and ActivityNet-QA, our model yields the highest accuracy among all open-source models, indicating improved motion and appearance reasoning due to the Î”-encoders temporally grounded representation. We refer to Supp. for discussion on the performance gap on select benchmarks due to reduced training data scale. Temporal Reasoning. Tab. 3 evaluates benchmarks specifically designed to probe temporal understanding. On TempCompass [91], TOMATO [92], and CVRR-ES [93], CoPEVideoLM achieves the highest accuracy among all open-source models. This confirms that the explicit encoding of motion vectors and residuals provides stronger temporal signal than dense RGB frame processing. These gains are notable given that temporal reasoning requires precisely Model TempCompass Tomato CVRR-ES MVBench test MCQ test test test Model Video-TT Video-MMMU LVBench LongVideoBench mc test Proprietary Models GPT-5 [48] Gemini 3 Pro [103] Gemini 2.5 Pro [13] Claude Sonnet 4.5 [49] Open-Source VideoLMs LongVA-7B [27] VideoLLaMA2-7B [104] InternVL2-8B [31] VideoChat2-7B [94] VideoCCAM-9B [105] IXC-2.5-7B [29] LLaVA-OV-7B [26] Apollo-7B [32] LLaVA-Video-7B [4] Ours-7B 80.4 82.8 81.9 72.8 56.9 - 65.3 45.5 - 67.1 64.8 64.9 66.6 68.4 53.0 48.3 48.6 39.6 - 18.5 21.7 - 27.0 - 25.5 - 24. 28.3 - - - - - 21.6 - - - - 42.6 - 43.6 49.1 74.1 70.4 70.6 62.1 - 54.6 65.8 51.1 64.6 69.1 56.7 - 58. 61.6 Table 3 Motion understanding and temporal reasoning benchmarks. CoPE-VideoLM achieves the highest accuracy on TempCompass, TOMATO, and CVRR-ES, confirming that codec primitives provide strong inductive bias for temporal reasoning. the kind of fine-grained inter-frame dynamics that codec primitives natively capture. On MVBench, our model outperforms the LLaVA-Video-7B base model and remains competitive with other approaches. However, the relatively lower performance of LLaVA-Video-7B compared to other baselines illustrates that suboptimal alignment of its architecture and training data with MVBenchs appearance nature, which also helps explain our overall performance trends. Long-form and Instruction-following. Tab. 4 reports results on benchmarks requiring understanding of extended video sequences and complex instructions. CoPE-VideoLM achieves the best results among open-source models on Video-TT [97], Video-MMMU [98], and LVBench [96], while remaining competitive on LongVideoBench [95]. By compressing P-frames into compact Î”-tokens, our framework processes substantially more temporal content within the same token budget, directly benefiting long-form comprehension and instruction-following tasks. Overall, this highlights that codec-aware tokenization is practical and effective route to scaling VideoLMs, enabling fine-grained temporal reasoning and efficient usage of the context window. Proprietary Models GPT-5 [48] Gemini-3-Pro [103] Gemini-2.5-Pro [13] Claude-Sonnet-4.5 [49] Open-Source VideoLMs EMA-7B [81] LongVA-7B [27] Kangaroo-8B [106] mPLUG-Owl3-7B [107] PLLaVA-34B [51] InternVL2-8B [31] TimeMarker [30] LLaVA-OV-7B [26] LlAVA-Video-7B [4] Ours-7B - - - - - - - - - - - 44.0 41.8 44.3 - - - - - 23.9 - - - 37.4 - 33.9 36.1 37. test 68.8 78.0 78.4 50.5 - - 39.4 43.5 26.1 - 41.3 38.1 44.2 46.4 val 72.6 75.9 76.8 65. 47.0 - 54.8 52.1 53.2 54.6 56.3 56.5 58.2 56.9 Table 4 Long-form and instruction-following benchmarks. CoPE-VideoLM performs better than other open-source models on Video-TT, Video-MMMU, and LVBench, demonstrating compact Î”-tokens effectively scale to longer temporal contexts and complex instruction-following. during inference. Tab. 5 reports time-to-first-token (TTFT) and end-to-end-latency (E2EL) to generate 64 text tokens, measured on single RTX 4090 GPU at 1 FPS input. Compared to the 64-frame LLaVA-Video-7B baseline, our most compact configuration (1 keyframe per GOP) achieves 86.2% reduction in TTFT and 56.01% faster E2EL. This improvement stems from the reduced visual embedding load (since only I-frames require full RGB encoding) and the shorter overall sequence length processed by the LLM due to the Î”-tokens. We highlight the scalability of the computational advantage of the Î”-token formulation in Fig. 4. Standard dense RGB sampling saturates quickly, limiting coverage to short sequences as memory Model / Sampling TTFT (s) E2EL (s) LLaVA-Video-7B (64 keyframes) 2.39 3.78 Ours 1 keyframe per GOP (8 keyframes + 56 P-frames) 2 keyframes per GOP (16 keyframes + 48 P-frames) 4 keyframes per GOP (32 keyframes + 32 P-frames) 0. 1.66 0.51 1.71 0.90 2."
        },
        {
            "title": "4.4 Runtime and Memory",
            "content": "Beyond competitive performance in accuracy, CoPE-VideoLM provides critical efficiency gains Table 5 Runtime comparison. We report the Time-toFirst-Token (TTFT) and End-to-End Latency (E2EL) for generating 64 text tokens at several keyframe densities compared to the 64 keyframe baseline at 1 FPS. 10 tized DCT coefficients, which could offer even better computational and token efficiency. Finally, we use fixed P-frame fusion window, which is suboptimal for tasks with varying motion. Conclusion. Through extensive experiments on several benchmarks, we have demonstrated that codec-aware tokenization offers compelling alternative to traditional keyframe sampling for VideoLMs. By leveraging the information natively encoded by video compression algorithms (i.e., motion vectors and residuals), we achieve substantial efficiency gains while maintaining competitive performance. Notably, our approach reduces timeto-first-token by up to 86% (to as low as 0.33s), which is essential for real-time applications. As models scale towards larger context windows, our approach becomes increasingly valuable, enabling richer temporal representations with much lower computational overhead than traditional sparse keyframe sampling. This work opens new direction for video understanding, positioning codecbased methods as practical and efficient foundation for future VideoLMs."
        },
        {
            "title": "6 Acknowledgements",
            "content": "The authors would like to thank (in alphabetical order): Isar Meijer and Krzysztof Waraksa from Microsoft for help with training pipeline setup; Tao Sun and Jianhao Zheng from Stanford for feedback at different stages of the project."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide the following: 1. Scale of Training Data (Sec. A) 2. Spatial Video Question Answering (Sec. B) 3. Comparison with Token Pruning (Sec. C) 4. Details about Î”-encoder (Sec. D) 5. Additional Training Details (Sec. E) 6. Video Decoding Illustration (Sec. F) 7. Ablation Study (Sec. G) Varying the Number of Î”-tokens (Sec. G.1) Two-Stage Training (Sec. G.2) Are Î”-tokens used by the VideoLM? (Sec. G.3) Figure 4 Video length vs. token budget. Theoretical scaling plot showing token efficiency across configurations. The x-axis is logarithmic in token budget, and vertical dashed lines indicate evaluated budgets. Our Î”-token representation enables scaling to significantly longer videos without exceeding context limits. constraints are rapidly encountered. In contrast, CoPE-VideoLM exhibits highly efficient relationship between video length and token budget. The token efficiency enables scaling to sequences previously inaccessible to open-source models; our most compact configuration allows for the processing of videos up to 8 hours in duration (at 1 FPS) within 1M token context, demonstrating an order-of-magnitude increase in processing capability over the baseline. Together, these results confirm that codec-aware tokenization is not just semantically sound but is necessity for enabling fast inference and comprehensive long-form video coverage without needing architectural modifications or additional hardware."
        },
        {
            "title": "5 Conclusion",
            "content": "Limitations and Future Work. Our current approach focuses on Iand P-frames, lacking support for B-frames and their complex non-causal dependencies. One option to address this would be by using the decode order instead of the render order. Furthermore, we currently operate on tensorized version of the codec primitives. For future works, it would be interesting to stay closer to the raw codec primitives, by operating directly on sets of block-wise motion vectors and quan11 Benefits of Codec Primitives (Sec. G.4) Next-Frame Retrieval using the Î”-Encoder (Sec. G.6)"
        },
        {
            "title": "A Scale of Training Data",
            "content": "A consistent observation across our results is that CoPE-VideoLM exhibits performance gap on certain benchmarks, e.g, Video-MME [90] and LongVideoBench [95] relative to the fully-trained LLaVA-Video [4]. We attribute this primarily to the difference in training data scale and composition rather than to the codec-aware formulation itself. Unlike LLaVA-Video, which trains on 1.61M video samples and an additional vision-totext alignment stage with 1.1M image samples, our model is trained solely on 1.39M instructiontuning samples from LLaVA-Video-178K [4] and three QA datasets (NextQA, ActNet-QA, and PerceptionTest). To better understand the influence of training data scale, Tab. A.1 compares our results against different LLaVA-Video training configurations. We observe that LLaVA-Video experiences drop in VideoMME performance upon the addition of the three QA datasets, suggesting that this behavior is not codec-specific. Under matched training budgets, our approach consistently outperforms the LLaVA-Video-7B baseline across all benchmarks, underlining that structured temporal compression can yield stronger generalization even without larger training corpora. Training Data LLaVA-Video [4] LLaVA-Hound + LLaVA-Video-178K + 3 QA datasets + LlaVA-OV (Images) LLaVA-Video-178K (sampled) Total Data NextQA PercepTest VideoMME 0.25M 1.58M 1.61M 2.71M 1.08M 64.4 80.1 80.1 83.2 73.2 51.4 57.1 69.0 67.9 55.9 70.5 54.1 63.2 61.9 63.4 59.6 61.7 Ours LLaVA-Video-178K + 3 QA datasets 1.39M 81.8 Table A.1 Effect of training data scale. Our model achieves better performance compared to LLaVA-Video (7B) variants trained on similar data regimes. Furthermore, it is worth noting that we train using the QA datasets, which reduces the performance on VideoMME as remarked in the LLaVA-Video paper [4]."
        },
        {
            "title": "B Spatial VQA",
            "content": "We evaluate our method on two standard 3D QA benchmarks: (1) SQA3D [100] for situated reasoning, and (2) ScanQA [99], for spatial understanding. Both datasets require associating multi-view observations with 3D spatial structure, making them natural testbed for assessing whether compressed-domain video cues can support geometry-aware reasoning. Training. We follow the same training pipeline used in our video-language experiments. However, since these videos are comparatively shorter in duration ( 15s total), we re-encode them with GOP size of 120 at 30 FPS with accumulation size ğ‘  = 10. This leads to around 3-4 GOPs per video. Following standard practice in 3D LMMs, we finetune our base model on ScanQA and SQA3D training split, without any datasetspecific heuristics or architectural modifications. We train and evaluate with 6 keyframes + 6 Pframes per GOP to align better with 32-RGB frame setting of the other models. Results. We report the results in Tab. B.1. To maintain fairness with VideoLMs, we show results for both the original and the finetuned model. Despite using 1/4th of the number of tokens compared to LLaVA-Video-7B [4], the base CoPEVideoLM maintains comparable performance with state-of-the-art VideoLMs. With finetuning, our performance becomes comparable to the leading 3D VLMs, despite our method not having access to camera poses or 3D point-clouds. However, we note that other VideoLMs would also see such gains in performance as well. Comparison with Token Pruning Token compression and pruning methods such as FastV [61], DyCoke [68], PLLaVA [51], and LLaVA-Scissor [69] operate on dense vision tokens after full RGB encoding, selectively removing redundant tokens based on attention scores or semantic grouping. While effective at reducing the token count fed to the LLM, these approaches still Method Expert Models SQA3D [100] ScanQA [99] 3D-VLP [108] 3D-VisTA [109] 3D VLMs Chat-3D [110] 3D-LLM [111] Scene-LLM [112] LL3DA [113] LEO [114] ChatScene [115] Grounded 3D-LLM [116] LLaVA-3D [117] Video-3D-LLM [118] Ross3D [119] VideoLMs (Zero-shot) InternVL2-8B [31] Qwen2-VL-7B [41] LLaVA-Video-7B [4] 100% tokens Ours-7B (Zero-shot) 25.48% tokens Ours-7B (Fine-tuned) 25.48% tokens Point Encoder Vision Encoder SQA3Dtest EM EM-R CIDEr ScanQAval BLEU-4 METEOR ROUGE EM 46.6 53.6 50.0 54.6 55.6 58.6 63.0 33.0 40.7 48. 47.3 57.1 52.4 57.5 57.6 65.7 45.3 46.7 50.2 59.9 64.9 53.2 69.4 80.0 76.8 80.0 87.7 72.7 91.7 102.1 107.0 62.5 53.9 88.7 71.7 95.1 10.1 11.2 6.4 12.0 11.7 11.5 14.3 13.4 14.5 16.4 17.9 3.3 3.0 3. 9.7 14.1 13.1 13.5 13.9 11.9 14.5 15.8 15.9 16.2 18.0 20.7 20.0 20.9 14.5 11.4 17.7 14.6 18.7 33.3 34.5 35. 28.5 35.7 35.9 37.3 39.3 41.6 50.1 49.3 50.7 34.3 29.3 44.6 37.8 46.0 21.1 21.7 22.4 20.5 27.2 21.5 21.6 27.0 30.1 30.8 27.1 Table B.1 Evaluation of 3D question-answering on SQA3D [100] and ScanQA [99]. Expert models are customized for specific tasks with task-oriented decoders. EM stands for top-1 exact match and EM-R means the refined exact match following [114]. indicates the number is not available. We show results for our method in zero-shot setup where it performs comparable to state-of-the-art VideoLMs despite using only 25% of tokens. We further show results after fine-tuning where the performance increases significantly, even outperforming significant number of 3D VLMs which employ additional inputs (e.g., point-clouds, camera poses). require the costly image embedding step for every frame, which dominates the prefill latency. In contrast, CoPE-VideoLM avoids full image encoding for most frames entirely by leveraging codec-level sparsity, and token pruning methods could be applied as an additional compression mechanism on top of ours. Tab. C.1 compares CoPE-VideoLM against these methods at similar token budgets. Our approach consistently outperforms all token compression baselines across ActNet-QA [89], NextQA [88], and VideoMME [90], while additionally being 7 faster in time-to-first-token, as shown in Tab. 5), since the Î”-encoder bypasses the vision encoder entirely for P-frames. These results confirm that operating in the compressed domain provides fundamentally more efficient starting point than post-hoc pruning of dense RGB tokens. Details about Î”-Encoder The Î”-Encoder converts codec primitives, motion vectors and residuals, into compact set of ğ‘ = ğ¾ğœ + ğ¾ğ›¿ tokens aligned with the vision encoders embedding space. The entire module is lightweight (< 15M parameters) and operates purely in the compressed domain during VideMethod ActNet-QA Next-QA VideoMME w/o sub FastV [61] DyCoke [68] PLLaVA [51] VisionZip [120] LLaVA-Scissor [69] Ours 47.95 47.88 47.59 45.42 47.89 58.83 81.1 81.1 81.0 78.5 81.2 81.8 57.5 57.4 56.9 54.2 57.4 60.1 Table C.1 Comparison with token compression methods. CoPE-VideoLM outperforms post-hoc pruning approaches across all three benchmarks while being faster in TTFT, as P-frames bypass the vision encoder entirely. 13 oLM fine-tuning. Here, we provide additional architectural and implementation details. Motion Vector Branch. Given motion vectors ğœ(ğ‘¡) â„¤ğ»ğ‘Š 2, we first perform minmax normalization to map all values to [1, 1]. We then patchify the motion-field into non-overlapping 16 16 blocks, yielding grid of size ğ»ğº ğ‘Šğº where ğ»ğº = ğ»/16 and ğ‘Šğº = ğ‘Š/16. Each patch is flattened into vector of dimension 1622, and lightweight two-layer MLP with shared weights is applied independently to each patch, producing per-block embeddings with the same feature dimension as the vision encoder (ğ‘‘ = 1152). To aggregate these features, we employ transformer equipped with ğ¾ğœ learnable query tokens. These tokens are concatenated with the motion features along the sequence length dimension at the input of the transformer and then processed together using regular multi-headed attention layers. The transformer contains 4 layers with hidden dimension ğ‘‘, uses 9 attention heads, and adopts PreNorm residual blocks throughout. Residual Branch. Residuals ğ›¿(ğ‘¡) â„ğ»ğ‘Š ğ¶ are processed through truncated ResNet-18 backbone (all convolutional layers up to the final global pooling) [85]. The resulting spatial features share the same grid resolution as above ( ğ»ğº ğ‘Šğº). second transformer, architecturally identical to the motion branch but with its own learned ğ¾ğ›¿ queries, is used to compress these features. As mentioned Pre-training the Î”-Encoder. in Sec 3.3, the Î”-Encoder is augmented with two auxiliary transformer modules, reference and warped branches, that enable it to reconstruct the token representation of the target frame without decoding RGB pixels. These transformers are architecturally identical to the ones in the motion vector and residual branch."
        },
        {
            "title": "E Additional Training Details",
            "content": "per-GPU learning rate of 6.25 105, optimized with AdamW and cosine scheduler with warmup steps = 1000. VLM Training. To better adhere to the original RGB aligned latent space, we train the VideoLM with 4 keyframes + 4 P-frames per GOP, for 10.9K steps on 64A100 GPUs with global batch size of 128. We keep the same hyperparameter settings as LLaVA-Video [4]."
        },
        {
            "title": "F Video Decoding Illustration",
            "content": "As mentioned in Sec. 3.1, the size of each Group of Pictures (GOP) is usually decided adaptively by the codec and video encoder depending on the motion or change. At encoding time, the developer can provide an upper bound for the GOP size, or even fix it. For example, if the GOP size is fixed at 250, the first frame of each GOP is an Iframe and the rest 249 frames are P-frames, each consisting of motion vectors and residuals. During video decoding, the I-frame or reconstructed frame at the previous timestamp ğ‘¡1 is first moved using the motion vectors and then the residuals are added to get the RGB frame at the current timestep ğ‘¡, as described in Eq. 1. We visualize the process with an example in Fig. 5."
        },
        {
            "title": "G Ablation Study",
            "content": "To reduce carbon footprint and computation cost compared to the main high compute run, for the ablation experiments, we train our CoPE-VideoLM on the three QA datasets: PerceptionTest [87], NextQA [88], ActivityNet-QA [89], comprising 60K samples for 2 days on 16A100. Due to this, the results shown in Sec G.1, G.2, and G.3 are not directly comparable to these reported in Sec. 4. We perform evaluation on the val and test splits of PerceptionTest [87] and NextQA [88] respectively. Pre-training. We pre-train the Î”-encoder with the warped and reference branches together for two days using 16A100 GPUs, running 113K iterations with global batch size of 1024 and G.1 Varying the Number of Î”-Tokens The number of Î”-tokens emitted per P-frame controls the expressive capacity of the codec branch. Fewer tokens encourage more aggressively com14 Figure 5 Codec primer. We visualize from left to right: the previous frame, the motion vectors and residuals between previous and current frame, the intermediate reconstruction after motion compensation, and the final result after adding the residuals. pressed representation, whereas larger token budgets allow the Î”-Encoder to retain finer motion and appearance cues from the codec primitives. Tab. G.1 reports the effect of varying this value while keeping all other model and sampling configurations fixed. We observe consistent trend across both benchmarks. Moving from 2 to 4 Î”-tokens yields noticeable improvement, as the model benefits from an expanded latent space that can more faithfully capture motion and residual structure. Increasing to 8 Î”-tokens further improves performance, reaching the best overall results. Beyond this point, however, allocating more capacity produces diminishing returns: both NextQA and PerceptionTest remain nearly unchanged when increasing to 16 tokens. This indicates that the codec primitives contain limited amount of signal per fused Pframe, and that 8 tokens are sufficient to encode the relevant temporal variations. For this reason, we adopt 8 Î”-tokens per P-frame as the default configuration throughout, balancing accuracy and token efficiency. G.2 Two-Stage Training We ablate the contribution of each component in our two-stage training pipeline: (i) pre-training the Î”-Encoder to align codec primitives with the RGB embedding space, and (ii) end-to-end finetuning of the full VideoLM. The results are shown in Tab. G.2. Finetuning the VideoLM without any Î”-encoder pre-training yields reasonable performance, though noticeably worse than the full two-stage approach In this one-stage setting, the model must simultaneously learn motionresidual interpretation, feature alignment, and multimodal reasoning, which slows convergence and leads Î”-tokens per P-Frame PerceptionTest NextQA 1 Keyframe per GOP 2 4 8 16 63.26 65.68 67.33 67.69 +2.42 +1.65 +0.36 75.04 76.08 77.37 77.89 +1.04 +1.29 +0.52 Table G.1 Number of Î”-tokens per P-frame. We train several versions of our model with different number of Î”-tokens per P-frame. Performance significantly increases when going from aggressive compression (2 or 4 tokens) up to 8, which is the value we used for the results in the main paper. Going to 16 further improves performance, but the margin is much smaller and does not justify the significant increase in total number of tokens. to weaker temporal understanding, particularly on PerceptionTest. The best results are achieved when both stages are used. Pre-training ensures that Î”-tokens inhabit well-structured embedding space, while finetuning teaches the LLM how to infuse the Iand P-frame tokens together. This combination consistently provides the strongest performance across both datasets. While twostage training is clearly advantageous in our data regime, we note that the benefit may diminish when scaling to datasets exceeding 3M training samples. In such high-data settings, one-stage training may gradually compensate for the missing pre-training at the cost of more compute. G.3 Are Î”-tokens used by the VideoLM? natural question is whether the VideoLM actually uses the Î”-tokens during inference, rather than ignoring them and relying solely on sparse keyframes. To probe this, we conduct an ablation in an identical sampling regime, 1 keyframe + 7 P-frames per GOP and simply zero out all P-frame 15 Pre-train Î”-Encoder Finetune LLM PerceptionTest NextQA 1 Keyframe per GOP - 63.45 67.33 74.56 77.37 Table G.2 Two-stage training. We attempt to directly train the Î”-encoder together with the LLM without our initial pre-training scheme. This yields significantly lower performance than the two stage setup proposed in the main paper: Stage 1 Pre-train the Î”-encoder and Stage 2 Finetune pre-trained Î”-encoder & pretrained LLM together. Î”-tokens at inference time. This preserves the temporal structure of the input while removing all motion and residual information contributed by the Î”-encoder, isolating their functional role. As shown in Tab. G.3, performance drops substantially when Î”-tokens are removed, confirming that the VideoLM meaningfully leverages compresseddomain temporal cues for reasoning. These results demonstrate that the model is not merely interpolating between sparse I-frames but actively uses Î”-tokens to track motion, maintain temporal grounding, and refine fine-grained visual details. G.4 Benefits of Codec Primitives To isolate the benefit of our codec-aware training from the effect of Î”-tokens themselves, we conduct controlled experiment where all frames are encoded identically. Specifically, we compare three configurations on PerceptionTest: (i) LLaVAVideo with 8 I-frames per GOP, (ii) our model with 4 I-frames and 4 P-frames per GOP, and (iii) our model with 8 I-frames and no P-frames. As shown in Tab. G.4, when given identical input, our model Sampling Strategy PerceptionTest NextQA"
        },
        {
            "title": "1 Keyframe per GOP\n+ Î”-tokens = 0\n+ Î”-tokens",
            "content": "64.41 67.33 74.21 77.37 Table G.3 Ablation on the use of Î”-tokens. We compare our method with version where we set all Î”-tokens to zero, effectively not providing any useful information to the VideoLM. Zeroing out P-frame Î”-tokens leads to clear performance degradation, confirming that the VideoLM attends to and utilizes the information of these tokens. outperforms LLaVA-Video by +5.2%, indicating that the codec-aware training procedure itself improves temporal reasoning beyond what Î”-tokens alone contribute. Furthermore, replacing half of the I-frames with P-frames yields comparable accuracy at nearly half the token cost, which confirms Î”-tokens serve as an efficient substitute for full RGB tokens without sacrificing understanding. Method Frames / GOP I-frames P-frames LlaVA-Video [4] Ours-7B Ours-7B 8 4 8 0 4 0 # Tokens / GOP PerceptionTest 1680 876 1680 65.4 70.5 70. Table G.4 Codec-aware training vs. RGB-only baseline. We compare LLaVA-Video and our model under identical input (8 I-frames) and under mixed configuration (4 I-frames + 4 P-frames). Our model achieves higher accuracy at equal token cost, and retains this gain when substituting half the I-frames with lightweight Î”-tokens at roughly half the tokens. G.5 Scaling to Higher Frame Rates key advantage of our tokenization is that Pframes are represented with only ğ‘=8 Î”-tokens, compared to ğ‘€=210 tokens for each I-frame, enabling higher effective frame rates under constrained token budget. To study this trade-off, we increase the effective FPS from 1 to 3 by reducing the P-frame fusion window ğ‘  from 30 to 10 frames, thereby encoding finer-grained temporal changes at increasing token cost. We evaluate on TempCompass [91] and MVBench [94], as both benchmarks are sensitive to temporal coverage. Increasing the frame rate leads to consistent gains on TempCompass (Tab. G.5). This illustrates the benefit of denser temporal signals for fine-grained temporal reasoning. MVBench also improves with higher FPS, though the gains are more moderate. Importantly, these improvements show that our framework enables practical and flexible trade-off between temporal resolution and token efficiency that would be prohibitive with RGB representations. 16 FPS Fusion ğ‘  P-frames / GOP Num. Tokens / GOP TempCompass MVBench 1 2 30 15 10 4 8 12 876 1752 2628 67.21 68.35 69.11 58.50 61.23 61.57 Table G.5 Effect of higher frame rates. We increase the effective FPS by reducing the P-frame fusion window ğ‘ , encoding finer temporal changes at modest token cost. All configurations use 1 keyframe per GOP. Token counts per GOP include ğ‘€=210 I-frame tokens plus ğ‘=8 Î”-tokens per P-frame group. G.6 Next-Frame Retrieval using Î”-"
        },
        {
            "title": "Encoder",
            "content": "To assess the representational quality of our compressed-domain features, we evaluate nextframe retrieval at 1 FPS on PerceptionTest [87]. Formally, the task can be defined as: given query frame ğ¼ (ğ‘¡1) at time ğ‘¡ 1, the task is to identify its true successor frame ğ¼ (ğ‘¡) at time ğ‘¡ from database containing all frames from the same video (except itself ğ¼ (ğ‘¡1) ). As baseline, SigLIP [34] processes the raw RGB frame ğ¼ (ğ‘¡1) , whereas our model uses ğ¼ (ğ‘¡1) together with the current motion vectors ğœ(ğ‘¡) and residuals ğ›¿(ğ‘¡) to produce ğ‘=8 Î”-tokens per P-frame via the Î”-Encoder and lightweight transformer branches used during pre-training. For this experiment, we use the intermediate checkpoint after pre-training, as it is compatible with the reference and warped transformers that are used to transform the features of the previous frame. The results are shown in Tab. G.6. Our method outperforms the baseline by large margin across the board. When retrieving single frame, the relatively low absolute performance is expected, as subsequent frames are heavily similar even at 1 FPS, but our method successfully leverages the codec primitives to significantly improve the performance. Furthermore, our method achieves almost perfect performance at 5 frames, with its very high recall of 94.86%. These strong improvements over SigLIP confirm that the Î”-Encoder preserves semantically meaningful motion and appearance cues that are critical for retrieval. This experiment also highlights that codec primitives contain rich temporal information that is often lost when sampling sparse keyframes. @1 FPS"
        },
        {
            "title": "Recall",
            "content": "@1 @2 @5 SigLIP Ours - Î”-Encoder 11.12 30.09 47.11 77. 78.65 94.86 Table G.6 Retrieval performance at 1 FPS. We evaluate the task of next-frame retrieval. The baseline uses the SigLIP embedding of the current frame, while we use the current frame along with the motion vectors and residuals to next frame, as processed by our Î”-encoder and pre-training transformers. The significantly better performance of our method shows that the information of the codec primitives is correctly compressed in the Î”-tokens."
        },
        {
            "title": "References",
            "content": "[1] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. VideoChat: ChatCentric Video Understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 3 [2] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [3] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. [4] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713. 2, 3, 8, 9, 10, 12, 13, 14, 16 [5] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding. arXiv preprint arXiv:2501.13106, 2025. 2, 3 [6] Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, and Thomas Malone. taxonomy for human-llm interaction modes: An initial 17 exploration. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI 24, page 111. ACM, May 2024. doi: 10.1145/3613905.3650786. URL http: //dx.doi.org/10.1145/3613905.3650786. 2 [7] Ren Wang, Haoliang Sun, Yuxiu Lin, Xinxin Zhang, and Yilong Yin. Improving generalization in meta-learning via meta-gradient In Proceedings of the Thirtyaugmentation. ThirdInternational Joint Conference on Artificial Intelligence, IJCAI-2024, page 63886396. International Joint Conferences on Artificial Intelligence Organization, August 2024. doi: 10.24963/ijcai.2024/711. URL http://dx.doi. org/10.24963/ijcai.2024/711. 2 [8] Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, and Yuke Zhu. Visionlanguage-action models for robotics: review towards real-world applications. IEEE Access, 13:162467162504, 2025. doi: 10.1109/ ACCESS.2025.3609980. 2 [9] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on visionlanguage-action models for embodied ai, 2025. URL https://arxiv.org/abs/2405.14093. 2 [10] Zhenyang Liu, Yongchong Gu, Sixiao Zheng, Yanwei Fu, Xiangyang Xue, and Yu-Gang Jiang. Trivla: triple-system-based unified visionlanguage-action model with episodic world modeling for general robot control, 2025. URL https://arxiv.org/abs/2507.01424. [11] Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, and Xiaolong Wang. Egovla: Learning vision-language-action models from egocentric human videos, 2025. URL https://arxiv.org/ abs/2507.12440. 2 [12] OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. 2, 3 [13] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 9, 10 Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. 2, 3 [15] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision. NeurIPS, 2024. [16] GeminiTeam. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 3 [17] Shyamal Buch, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. Flexible Frame Selection for Efficient Video Reasoning. In CVPR, 2025. 2 [18] Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In CVPR, 2025. 2 [19] Didier Le Gall. Mpeg: video compression standard for multimedia applications. Commun. ACM, 34(4):4658, April 1991. ISSN 00010782. doi: 10.1145/103085.103090. URL https://doi.org/10.1145/103085.103090. 2 [20] Thomas Wiegand, Gary J. Sullivan, Gisle BjÃ¸ntegaard, and Ajay Luthra. Overview of the h.264/avc video coding standard. IEEE Trans. Circuits Syst. Video Technol., 13:560576, 2003. URL https://api.semanticscholar.org/ CorpusID:3540699. 2 [21] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, and Chenliang Xu. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2025. doi: 10.1109/TCSVT.2025.3566695. [14] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng [22] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, 18 Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: comprehensive survey. Machine Intelligence Research, ISSN 2731-538X. 20(4):447482, 2023. doi: URL 10.1007/s11633-022-1410-8. https://www.mi-research.net/en/article/ doi/10.1007/s11633-022-1410-8. [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Visual Instruction Tuning. Yong Jae Lee. NeurIPS, 2023. [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Improved Baselines with arXiv prepreint Yong Jae Lee. Visual Instruction Tuning. arXiv:2310.03744, 2023. [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. URL https://llava-vl. github.io/blog/2024-01-30-llava-next/. [26] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVAOneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024. 9, 10 [27] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. URL https://arxiv.org/abs/2406.16852. 9, 10 [28] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: vision-language large model for advanced textimage comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [29] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlmxcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 9, 10 [30] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile videollm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. 10 [31] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 10, 13 [32] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, and Xide Xia. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. 3, 9, 10 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/ CorpusID:231591445. [34] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1194111952, 2023. URL https://api.semanticscholar.org/ CorpusID:257767223. 3, 8, 17 [35] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vsa: learning varied-size window attention in vision transformers. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXV, pages 466483. Springer, 2022. 3 [36] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision transformer with quadrangle attention. arXiv preprint arXiv:2303.15105, 2023. 3 [37] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Iain Barr, Yana HasLuc, Antoine Miech, son, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022. URL https://api.semanticscholar.org/ CorpusID:248476411. 3 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. 3 [39] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MTBench and Chatbot Arena. arXiv preprint arXiv:2306.05685, 2023. 3 [40] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966, 2023. 3 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 13 [42] Hang Zhang, Xin Li, and Lidong Bing. VideoLLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. arXiv preprint arXiv:2306.02858, 2023. [43] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 3, 4 [44] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, 2024. [45] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning, 2024. [46] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos, 2024. 3 [47] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. TimeChat: Time-sensitive Multimodal Large Language Model for Long Video Understanding. arXiv, abs/2312.02051, 2023. [48] OpenAI. GPT-5 system card, 2025. URL https: //openai.com/index/gpt-5-system-card/. 3, 9, 10 [49] Anthropic. Claude sonnet 4.5 system card, 2025. URL https://assets.anthropic. com/m/12f214efcc2f457a/original/ Claude-Sonnet-4-5-System-Card.pdf. 9, 10 3, [50] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: On Pre-training for Visual Language Models, 2023. 3, 9 [51] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning, 2024. 3, 4, 10, 12, [52] Yanwei Li, Chengyao Wang, and Jiaya Jia. LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models. In ECCV, 2024. 3 [53] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In ICLR, 2023. 3 [54] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified 20 visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. 3 [55] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochatflash: Hierarchical compression for longcontext video modeling. arXiv preprint arXiv:2501.00574, 2024. [56] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/ CorpusID:256390509. 3 [57] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token, 2025. URL https://arxiv.org/abs/ 2501.03895. 3 [58] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 3 [59] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 3 [60] Enxin Song, Wenhao Chai, Tian Ye, JenqNeng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. arXiv preprint arXiv:2404.17176, 2024. 3 [61] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, 2024. 3, 12, [62] Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Framefusion: Combining similarity and importance for video token reduction on large visual language models. arXiv preprint arXiv:2501.01986, 2024. 3 [63] Long Xing, Qidong Huang, Xiao wen Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. ArXiv, abs/2410.17247, 2024. URL https://api.semanticscholar.org/ CorpusID:273507889. 3 [64] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. In International Conference on Machine Learning, 2025. 3 [65] Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, and Huan Wang. When tokens talk too much: survey of multimodal long-context token compression across images, videos, and audios. arXiv preprint arXiv:2507.20198, 2025. 3 [66] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv, 2024. 4 [67] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In ECCV, 2024. [68] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18992 19001, 2025. 4, 12, 13 [69] Boyuan Sun, Jiaxing Zhao, Xihan Wei, and Qibin Hou. Llava-scissor: Token compression with semantic connected components for video llms. arXiv preprint arXiv:2506.21862, 2025. 4, 12, 13 [70] Xiao Wang, Qingyi Si, Shiyu Zhu, Jianlong Wu, Li Cao, and Liqiang Nie. AdaReTaKe: Adaptive redundancy reduction to perceive longer for video-language understanding. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 54175432, Vienna, Austria, July 2025. Association for Com21 putational Linguistics. ISBN 979-8-89176256-5. doi: 10.18653/v1/2025.findings-acl. 283. URL https://aclanthology.org/2025. findings-acl.283/. 4 [71] Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, and Linchao Zhu. Flexselect: Flexible token selection for efficient long video understanding, 2025. URL https://arxiv.org/ abs/2506.00993. [72] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, Manmatha, Alexander Smola, and Philipp KrÃ¤henbÃ¼hl. Compressed video action recognition. In CVPR, 2018. 4, 5 [73] Zhengwei Wang, Qi She, and Aljosa Smolic. Team-net: Multi-modal learning for video action recognition with partial decoding. ArXiv, URL https://api. abs/2110.08814, 2021. semanticscholar.org/CorpusID:239015960. 4 [74] Samuel Felipe dos Santos, Nicu Sebe, and Jurandy Almeida. Cv-c3d: Action recognition on compressed videos with convolutional 3d networks. In 2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), pages 2430, 2019. doi: 10.1109/SIBGRAPI. 2019.00012. 4 [75] Jin Yang, Daniel S. Marcus, and Aristeidis Sotiras. Dmc-net: Lightweight dynamic multi-scale and multi-resolution convolution network for pancreas segmentation in ct images. ArXiv, abs/2410.02129, 2024. URL https://api. semanticscholar.org/CorpusID:273098181. 4 [76] Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew M. Dai, and Dustin Tran. Training independent subnetworks for robust prediction. In International Conference on Learning Representations, 2021. 4 [77] Hayato Terao, Wataru Noguchi, Hiroyuki Iizuka, and Masahito Yamamoto. Efficient compressed video action recognition via late fusion with single network. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. doi: 10.1109/ICASSP49357.2023. 10096477. [78] Jiawei Chen and Chiu Man Ho. Mmvit: Multi-modal video transformer for compressed video action recognition. 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 786797, 2021. URL https://api.semanticscholar.org/ CorpusID:237266685. 4 [79] Shristi Das Biswas, Efstathia Soufleri, Arani Roy, and Kaushik Roy. Towards scalable modeling of compressed videos for efficient action recognition, 2025. URL https://arxiv.org/abs/2503. 13724. 4, 5 [80] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, and Yadong Mu. Video-lavit: Unified videolanguage pre-training with decoupled visualmotional tokenization. In International Conference on Machine Learning, pages 2218522209, 2024. 4, 9 [81] Zijia Zhao, Yuqi Huo, Tongtian Yue, Longteng Guo, Haoyu Lu, Bingning Wang, Weipeng Chen, and Jing Liu. Efficient motion-aware video mllm. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition URL (CVPR), pages 2415924168, 2025. https://api.semanticscholar.org/CorpusID: 277104804. 4, 9, [82] Yazhou Xing, Yang Fei, Yingqing He, Jingye Chen, Jiaxin Xie, Xiaowei Chi, and Qifeng Chen. Videovae+: Large motion video autoencoding with cross-modal video vae. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 4 [83] Dan Grois, Alex Giladi, Kiho Choi, Min Woo Park, Yinji Piao, Min Woo Park, and Kwang Pyo Choi. Performance comparison of emerging evc and vvc video coding standards with hevc and av1. SMPTE Motion Imaging Journal, 130:112, 2021. URL https://api.semanticscholar.org/ CorpusID:235569476. 4 [84] Thorsten Laude, Yeremia Gunawan Adhisantoso, Jan Voges, Marco Munderloh, and JÃ¶rn Ostermann. comprehensive video codec comparison. APSIPA Transactions on Signal and Information Processing, 8:e30, 2019. doi: 10.1017/ATSIP.2019.23. 4 [85] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2015. URL https://api.semanticscholar.org/ CorpusID:206594692. 6, 14 [86] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. 8 [87] Viorica PÄƒtrÄƒucean, Lucas Smaira, Ankush Gupta, AdriÃ  Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and JoÃ£o Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=HYEGXFnPoq. 8, 14, 17 [88] Junbin Xiao, Xindi Shang, Angela Yao, and TatSeng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 8, 9, 13, 14 [89] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134, 2019. 8, 13, 14 [90] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multimodal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 8, 12, 13 [91] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024. 8, 9, 16 [92] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models, 2024. URL https: //arxiv.org/abs/2410.23266. 8, 9 [93] Muhammad Uzair khattak, Muhammad Ferjad Naeem, Jameel Hassan, Naseer Muzzamal, Federcio Tombari, Fahad Shahbaz Khan, and Salman Khan. How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms. arXiv:2405.03690, 2024. 8, 9 [94] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 8, 10, 16 [95] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. URL https://arxiv.org/abs/ 2407.15754. 8, 10, 12 [96] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark, 2024. 8, [97] Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, and Ziwei Liu. Towards video thinking test: holistic benchmark for advanced video reasoning and understanding, 2025. URL https://arxiv.org/abs/2507. 15028. 8, 10 [98] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. 2025. URL https://arxiv.org/abs/ 2501.13826. 8, 10 [99] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In CVPR, pages 1912919139, 2022. 8, 12, 13 [100] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing 23 Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In ICLR, 2023. 8, 12, [101] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 8 [102] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 8, 9 [103] Google. Gemini 3 Pro model card, URL https://storage.googleapis. 2025. com/deepmind-media/Model-Cards/ Gemini-3-Pro-Model-Card.pdf. 9, [104] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. URL https://arxiv. org/abs/2406.07476. 10 [109] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In ICCV, pages 29112921, 2023. 13 [110] Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Dataefficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023. 13 [111] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 36: 2048220494, 2023. 13 [112] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024. 13 [113] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In CVPR, pages 26428 26438, 2024. [105] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos, 2024. URL https://arxiv.org/abs/ 2408.14023. 10 [114] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. 13 [106] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 10 [107] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multimodal large language models, 2024. URL https://arxiv.org/abs/2408.04840. 10 [108] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and Yinjie Lei. Context-aware alignment and mutual masking for 3d-language pretraining. In CVPR, pages 1098410994, 2023. 13 [115] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation for autonomous vehicles. In CVPR, pages 1545915469, 2024. [116] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024. 13 [117] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 13 [118] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video 24 representation for 3d scene understanding. arXiv preprint arXiv:2412.00493, 2024. 13 [119] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025. [120] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Microsoft Spatial AI Lab",
        "Stanford University"
    ]
}