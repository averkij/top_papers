{
    "paper_title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following",
    "authors": [
        "Tianyi Xiong",
        "Yi Ge",
        "Ming Li",
        "Zuolong Zhang",
        "Pranav Kulkarni",
        "Kaishen Wang",
        "Qi He",
        "Zeying Zhu",
        "Chenxi Liu",
        "Ruibo Chen",
        "Tong Zheng",
        "Yanshuo Chen",
        "Xiyao Wang",
        "Renrui Zhang",
        "Wenhu Chen",
        "Heng Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 6 6 1 2 . 1 1 5 2 : r Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following Tianyi Xiong1*, Yi Ge1*, Ming Li1, Zuolong Zhang1, Pranav Kulkarni1, Kaishen Wang1, Qi He1, Zeying Zhu1, Chenxi Liu1, Ruibo Chen1, Tong Zheng1, Yanshuo Chen1, Xiyao Wang1, Renrui Zhang, Wenhu Chen2, Heng Huang1 1University of Maryland, College Park, 2University of Waterloo multi-crit.github.io Figure 1. Overview of Multi-Crit. Left: Unlike prior works that assign single overall preference label, Multi-Crit provides pluralistic, multi-criterion human judgments, exposing conflicts between different evaluation criteria within the same sample (e.g., Logic vs. No Hallucination, Reflection vs. Efficiency). Right: We introduce three complementary metrics to systematically assess LMM judges on their ability to follow pluralistic evaluation criteria, recognize preference trade-offs, and capture criterion-level conflicts."
        },
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, finegrained evaluation criteria remains underexplored. We develop Multi-Crit, benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehen- *Core contributors. sive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteriaespecially in open-ended evaluation; 2) opensource models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation. 1. Introduction The recent advancements of large multimodal models (LMMs) [2, 4, 6, 8, 16, 21, 24] in visual understanding [13, 37, 50, 53] and reasoning [18, 27, 56] have underscored the growing need for scalable and reliable multimodal evaluation systems. However, evaluating free-form outputs in open-ended or reasoning-intensive tasks remains inherently challenging, as static or reference-based metrics often fail to generalize and provide limited insight into model behavior. growing trend has emerged to deploy LMMs themselves as generative judges (LMM-as-aJudge) [46, 57]. Given multimodal prompt consisting of an image and user query, model responses, and set of predefined evaluation criteria, the judge model produces either scalar scores for individual responses or pairwise preference judgments, accompanied by textual justifications for its evaluation. Owing to its scalability and flexibility in defining task-specific criteria, this paradigm has been widely adopted for open-ended evaluation across diverse multimodal benchmarks [13, 18, 21, 26, 34, 45, 49, 52, 56]. series of works fine-tune open-source models to develop dedicated judge and critic capacities, and have proven effective in providing AI feedback for aligning LMMs through reinforcement fine-tuning [22, 22, 25, 40, 43] or test-time scaling [41, 58]. The reliability of the LMM-as-a-Judge paradigm re- (1) consistency with hulies on two key components: man judgment, and (2) flexibility to follow diverse, taskspecific evaluation criteria that capture different aspects of model behavior (e.g., factuality, visual grounding, structural coherence). However, existing multimodal reward and judge benchmarks [5, 23, 55, 60] mainly address the first aspect by providing single overall preference label for each pair of model responses. This coarse formulation overlooks the evaluation of judge models in producing criterionspecific judgment. Specifically, two responses may exhibit trade-offs across different criteriafor instance, concise but factually correct answer versus visually grounded yet slightly hallucinated one. This motivates key question: Can LMM-based judges follow diverse evaluation criteria and recognize conflicts among criterion-level judgments? To address this limitation, we introduce Multi-Crit, challenging benchmark designed to comprehensively evaluate multimodal judges on their ability to follow diverse evaluation criteria and produce reliable, criterion-level judgments aligned with human evaluators. As shown in Figure 1, each evaluation sample is annotated with multiple criterion-level human preferences, capturing distinct aspects of response quality and the potential conflicts between criteria. Multi-Crit is built upon two key research questions: 1. How to construct evaluation data with multi-criterion human preferences that capture diverse aspects of response quality and reveal intra-sample conflicts? We design rigorous data curation and preferenceannotation pipeline that: (1) collects diverse multimodal prompts spanning both open-ended content generation and verifiable reasoning tasks; (2) selects challenging response pairs from various strong LMMs with finegrained criterion-level quality differences via multi-step filtering; (3) defines clear evaluation criteria covering complementary aspects of multimodal judgment; and (4) gathers high-quality human annotations ensuring reliability and inter-annotator consistency. 2. How to assess the performance of LMM-based judges in pluralistic criteria following? Apart from criterionlevel accuracy, we introduce three metrics to systematically assess judge models (1) adherence to pluralistic criteria (PAcc), (2) sensitivity to criterion-level tradeoffs within each prompt (TOS), and (3) ability to resolve criterion-level preference conflicts (CMR). Comprehensive evaluation across 25 LMMs exposes key challenges: 1) Strongest proprietary models (e.g., o4mini, Claude-3.7-Sonnet) struggle to maintain consistent pluralistic criteria following, achieving 32.78% in openended and 53.17% in reasoning judgments; 2) open-source LMMs lag notably behind in both criterion-level accuracy and their ability to recognize trade-offs and preference conflicts; and 3) models fine-tuned on holistic, singlepreference critic data primarily improve judgment consistency in visual grounding but generalize poorly to pluralistic, criterion-driven evaluation. Additional analyses further reveals the boundaries of current LMM judges: 1) reasoning fine-tuning fails to enhance reasoning judgment but weakens models ability to recognize trade-offs; 2) test-time scaling effects are most evident in the strongest model, o4-mini, but remain inconsistent across others in pluralistic criteria following; 3) the criterion-level upper bound of proprietary models aligns closely with human inter-annotator consistency, whereas open-source judges fail to exhibit this trend. Overall, Multi-Crit serves as the first benchmark for assessing multimodal judges adherence to pluralistic evaluation criteria, built upon curated dataset, novel metrics, and analyses that uncover key limitations and directions for more reliable, steerable evaluation systems. 2. Related Work Large Multimodal Model as Judges and Critics. Large multimodal models (LMMs) are widely adopted for evaluating model-generated responses, providing assessments ranging from scalar [13, 21, 53, 56] to pairwise preference [24, 26, 52]. GPT-4V [57] was first shown to align closely with human judges across diverse multimodal evaluation tasks, motivating efforts to develop open-source counterparts. Subsequent works fine-tuned open-source LMMs into either BT-style reward models with dedicated reward heads [34, 54, 59] or generative judges that produce textual justifications and scores [20, 41, 43, 46, 58, 60] in an autoregressive manner. While our benchmark centers on generative judge modelsdue to their flexible instructionfollowing abilities inherent in the autoregressive generation 2 Figure 2. Data Construction Pipeline. Multi-Crit is built from diverse prompts across open-ended and reasoning tasks, responses from various LMMs reflecting subtle quality distinctions, and multi-criterion human annotations highlighting preference conflicts across criteria. process of LMMsthe proposed methodology is broadly applicable across diverse reward modeling paradigms. Benchmarking Multimodal Judges. Benchmarking reward and judge models is crucial for evaluating the reliability of AI feedback, which underpins both automated evaluation and the enhancement of LMM performance through preference alignment. MLLM-as-a-Judge [5] is the first to evaluate LMM as judges on academic tasks; VLRewardbench [23], multimodal-rewardbench [48] and MMRLHF [60] Benchs extend prompts sources to diverse realworld scenarios, reasoning and safety tasks. Following reward and judge benchmarks in text-only LLMs [19, 35, 61], these multimodal benchmarks evaluate judges by their overall preference between paired responses, measured against human or verifier-provided golden labels. MM-Critic further extends evaluation to assess the correctness of generated textual critiques by leveraging GPT-generated scoring rubrics [55]. However, none of these benchmarks evaluate multimodal judges on their ability to follow diverse criteria for each response pair, and fine-grained, criterion-level evaluation remains underexplored. Criteria-Following Judgment. When deploying modelbased judges [24, 46, 52, 53, 56], existing benchmarks typically define task-specific criteria, such as helpfulness [26], visual grounding [49], or hallucination detection [34]. While recent work has begun evaluating criterion-following in the textual domaineither by embedding criterion-level differences into responses [32] or summarizing from human justifications [44]we extend this line of research to the multimodal setting, providing comprehensive analysis of how judges follow diverse criteria and capture criterionlevel preference conflicts. 3. Multi-Crit Benchmark Multi-Crit is rigorously curated benchmark designed to evaluate the pluralistic criteria-following ability of multimodal judge models. It includes diverse prompts from 8 data sources across open-ended and reasoning-dense domains, cross-model and intra-model response pairs from 11 LMMs of varying capabilities, and fine-grained multicriteria human annotations within each prompt. The data construction pipeline is illustrated in Figure 2. Additionally, three metrics are introduced in Multi-Crit to evaluate judge models on their ability to flexibly follow diverse evaluation criteria and identify criterion-level preference conflicts. 3.1. Task Formulation Existing multimodal judge benchmarks [5, 23, 48, 60] typically consist of preference pairs (q, la, lb, y), where is multimodal prompt containing an image and textual query, la and lb are candidate responses, and {a, b} denotes the golden preference by human annotation or rule-based verification. However, such one-dimensional annotations cannot capture the multifaceted nature of human evaluation different criteria may lead to conflicting judgments for the same response pair. Multi-Crit therefore extends this formulation to include criterion-specific preferences: (q, la, lb, {(ci, yi)}Kq i=1), where each ci denotes the i-th evaluation criterion and yi {a, b} represents the preferred response under that criterion. 3.2. Prompt and Response Pair Curation MMCrit primarily covers two major scopes of multimodal judgment: (1) open-ended content generation, where responses are free-form and traditional fixed metrics are lim3 Statistic Open-ended - Prompts - with conflicting preferences - Criterion-level judgment - Conflicting criterion pairs Verifiable Reasoning - Prompts - with conflicting preferences - Criterion-level judgment - Conflicting criterion pairs Number 299 206 (68.9%) 1,000 501 126 109 (86.5%) 425 281 Question length quantiles Response length quantiles (5, 10, 27) (104, 163.5, 247) Table 1. Multi-Crits key statistics. Figure 3. Distribution of prompt sources (left) and evaluation criteria (right). ited; and (2) reasoning tasks, where the judge model evaluates the quality of model-generated reasoning processes leading to objectively verifiable answers. unanimously agree are discarded, while the remaining ones are retained as challenging cases reflecting subtle criterionlevel distinctions. total of 707 response pairs are selected. Prompt Sources. For open-ended generation, we select images from ImageInWords [11] and DOCCI [28] evaluation set for captioning and incorporate imagequestion pairs from the WildVision-Bench and -Battle datasets [26], which contain diverse, in-the-wild visual conversation queries. For verifiable reasoning, we draw from reasoning-dense domains, including MathVerse [56] (math), MM-K12 [27] (STEM), EMMA-mini [14] (math, science, coding), and VisualPuzzles [33] (visual analogy and puzzle solving). Response Generation and Pair Curation. To minimize data contamination, we generate candidate responses using 11 high-performing models, covering both proprietary (e.g. GPT-4o [16], Gemini-2.5-Flash [9]) and open-source families (e.g., LLaMA3.2-V [12], Qwen2.5-VL [4], InternVL3 [8]); details in Appendix. We then form two types of response pairs: cross-model pairs, generated by randomly selecting two different LMMs, and intra-model pairs, produced by the same model via temperature sampling. For the latter, we sample = 5 responses at temperature 1.0 and select the pair with the largest cosine distance between their MiniLM-L6 [38] embeddings to ensure content diversity. This process yields 3,538 response pairs. Filtering Challenging Pairs. We apply three-stage filtering mechanism to retain challenging yet balanced exam- (1) Length norples suitable for fine-grained evaluation. malization: We exclude pairs with excessive length disparity, mostly retaining those within [0.7, 1.4] to mitigate su- (2) Verifiable reasoning filtering: For reaperficial bias. soning tasks, we use GPT-4o-mini to verify both responses and keep only pairs where both are correct or both incorrect, as correctness alone is trivial and our evaluation targets different criteria of reasoning quality. (3) Ensemblebased difficulty filtering: To emphasize nuanced criterionrelated differences, we use three strong judge models (GPT4o, Gemini-2.5-Flash, and Claude-3.7-Sonnet) for initial assessments of overall quality; pairs on which all judges 3.3. Criteria-Specific Preference Annotation Criteria Design. The criteria in Multi-Crit follow clear set of principles to ensure balanced coverage of multimodal judging capabilities: (1) Practicality: reflecting common use cases of multimodal judges; (2) Specificity: capturing diverse quality aspects of generated responses while avoiding overlap; and (3) Generality: evaluating fundamental dimensions of model behavior rather than prompt-specific content. Following these principles, we survey existing MLLM-as-a-Judge benchmarks, summarize their adopted criteria, and perform multi-round refinement to derive fivecriteria for open-ended tasks and another five for reasoning tasks, as briefed in Table 2 and detailed in Appendix. Criteria for Judging Open-ended Generation Completeness and Coverage: Address the full scope of the task in the users query, covering all major elements specified in the prompt as well as relevant visual aspects and contextual cues. Visual Grounding and Details: Reference observable elements in the image such as objects, spatial relationships, colors, or text, and bases its description or analysis on these details. Factuality / No Hallucination: Avoid visual or factual errors, ensuring all details and claims are presented in the image or reasonably supported by the prompt. Creativity and Expressiveness: Demonstrates imagination and originality when appropriate, or precise and knowledgeable articulation for analytical tasks, while remaining contextually appropriate. Clarity and Coherence: Communicates ideas clearly and logically, with fluent language, well-organized structure, and smooth flow of information. Criteria for Judging Verifiable Reasoning Visual Grounding: Reference important visual elementssuch as objects, layout, or textand integrates them meaningfully into the reasoning. Logic Coherence and Consistency: Follow clear, step-by-step logic without contradictions or unjustified leaps, and ensures the answer aligns with reasoning. Factuality / No Hallucination: Ensure accuracy of all claims and support them with the input, avoiding hallucinated visual details or factual errors. Reflection and Exploration: Demonstrate depth of reasoning through reflection and exploration of alternative interpretations, particularly in complex tasks. Conciseness and Efficiency: Remains concise and focused, matching the task complexity while avoiding redundancy or unnecessary over-analysis. Table 2. Summary of the evaluation criteria employed in the MultiCrit open-ended and reasoning splits, each capturing distinct dimensions of multimodal judgment. Rigorous Human Annotation. We recruit nine Ph.D. students majoring in Computer Science as annotators, all with strong backgrounds in multimodal AI and STEMrelated domains. Each receives detailed guidelines with clear illustrations of all evaluation criteria. During annotation, annotators are presented with multimodal instance containing prompt and two candidate responses. They exclusively evaluate one criterion at time, determine which response is better (or declare tie, limited to under 10%), and provide brief textual justification with supporting evidence. Annotators focus on response elements most relevant to the prompt and indicative of the given criterion. To ensure consistency, we first establish seed set of 10 openended and 10 reasoning examples. Annotators label this set individually, then participate in group discussions led by the project lead to align interpretations and reach consensus. After calibration, each evaluation samplecomprising the prompt, response pair, and set of criteriais randomly assigned to three annotators for cross-validation in the main phase. The annotation process involved 289 hours of human effort in total. Inter-annotator agreement is quantified using the average pairwise Cohens κ [3], achieving 0.718 on open-ended generation tasks and 0.805 on verifiable reasoning tasks, indicating substantial and reliable consistency among annotators. Preference Aggregation and Final Verification. First, preferences for each criterion are aggregated, retaining only cases where all annotators agree on one model or where two annotators agree and the third declares tie. Project leads manually review the textual justifications and discard samples with notable inconsistencies or verbose explanations. Subsequently, criterion-level judgments are aggregated at the promptresponse level. Finally, prompt-level filtering primarily retains pairs exhibiting criterion-level conflicts and balances the number of samples across all criteria. 3.4. Data Statistics Multi-Crit comprises 425 multimodal prompts and 1,425 criterion-level human judgments across open-ended and verifiable reasoning tasks, including 315 prompts with criterion-level conflicts that together constitute 782 conflicting criterion pairs. Key statistics and data distributions are shown in Table 1 and Figure 3, with details in the Appendix. 3.5. Measuring Pluralistic Criteria Following We introduce three complementary metrics to evaluate how well multimodal judge models adhere to pluralistic criteria, identify criterion-level trade-offs, and capture preference conflicts within response pairs. Pluralistic Accuracy (PAcc). Measures whether the judge produces correct judgments across all criteria for each evaluation instance. Let denote the set of evaluation instances, where each = (q, la, lb) represents multimodal prompt with its paired candidate responses, and let Cx denote the set of evaluation criteria associated with x: PAcc = 1 (cid:88) (cid:34) (cid:94) xX cCx (cid:35) ˆyx,c = yx,c , where ˆyx,c denotes the judges predicted preference and yx,c the ground-truth label for criterion c. Trade-off Sensitivity (TOS). Measures whether the judge model recognizes criterion-level trade-offs within each evaluation instance. Let Px = { (ci, cj) Cx Cx yx,ci = yx,cj } denotes the set of criterion pairs in instance that exhibit ground-truth conflicts. TOS = 1 (cid:88) xX I(cid:2) (ci, cj) Px : ˆyx,ci = ˆyx,cj (cid:3) , where = { ci, cj Cx such that yx,ci = yx,cj } denotes the subset of evaluation instances exhibiting at least one ground-truth conflict among criteria. TOS reflects whether the judge can perceive criterion-level tradeoffs within an instance, indicating its flexibility rather than absolute accuracy. higher TOS suggests the model is less criterion-agnostic or overconfident in assigning identical preferences across all criteria. Conflict Matching Rate (CMR). Measures the judges ability to correctly resolve ground-truth conflicts between criterion-based judgments. conflict is considered matched only if the model predicts both criteria in agreement with the ground truth preference: (cid:80) xX CMR = (cid:80) (ci,cj )Px I(cid:2)(ˆyx,ci , ˆyx,cj ) = (yx,ci , yx,cj )(cid:3) (cid:80) xX Px . This provides stricter and more fine-grained metric than TOS, evaluating whether the judge not only detects but also resolves conflicts consistently with humans. 4. Experiments To comprehensively assess multimodal judge boundaries, we evaluate 25 LMMs across three model groups: (1) proprietary models, including the GPT series [16, 17, 30], Gemini family [9], and Claude-3.7-Sonnet [31]; (2) leading open-source LMMs, including Qwen2.5/3-VL [4, 47], InternVL3/3.5 [39, 62], GLM-4.1V-Thinking [15], LLaMA3.2-V [12], MiniCPM-V-4.5 [51], Eagle2.5 [6], Molmo [10] and LLaVA-OneVision [21]; and (3) finetuned judge models, including LLaVA-Critic [46]/-R1 [41], R1Reward [58] and UnifiedReward [43]. 5 Model Prompt-Level Criterion-Level Pluralisic Conflict Tradeoff Completeness Grounding Hallucination Expressiveness Clarity Avg Open-Source LMMs Qwen2.5-VL-7B-Instruct LLaVA-OneVision-7B Llama-3.2-11B-Vision-Instruct Molmo-7B Qwen3-VL-8B-Instruct MiniCPM-V-4.5-8B Eagle2.5-8B GLM-4.1V-9B-Thinking InternVL3.5-8B InternVL3-8B-Instruct Qwen2.5-VL-72B-Instruct MiMo-VL-7B InternVL3-78B-Instruct InternVL3.5-38B LLaVA-Critic-7B (LlaVA-OV) R1-Reward-7B (Qwen2.5-VL) UnifiedReward-7B(Qwen2.5-VL) LLaVA-Critic-R1-7B(Qwen2.5-VL) Gemini-2.5-Flash Gemini-2.5-Pro GPT-5 o3 GPT-4o Claude-3.7-Sonnet o4-mini 9.41 11.46 12.71 17.06 18.39 18.73 20.40 24.08 25.08 26.09 28.43 29.10 29.10 30.43 11.70 17.73 18.06 18.39 25.42 28.76 29.77 31.10 31.44 31.77 32. 17.28 12.77 15.37 15.97 18.36 14.77 23.75 30.54 32.34 30.34 35.53 39.52 32.53 33.73 14.17 20.36 8.38 17.96 31.34 37.92 38.52 42.71 44.91 42.32 43.11 56.12 53.20 55.75 48.23 56.19 53.10 64.16 59.73 61.50 64.60 69.47 65.93 73.01 71.68 36.14 37.86 41.26 34.47 34.47 27.67 50.97 55.34 62.14 61.17 60.68 65.53 56.31 64.08 Finetuned Judge Models 43.2 45.63 16.50 39.32 56.93 59.29 57.96 55. Proprietary LMMs 62.14 66.50 62.62 62.62 66.02 64.08 64.56 64.6 65.93 69.91 66.37 68.14 71.68 76.11 51.70 48.80 44.64 44.20 47.77 46.43 54.91 55.80 59.38 62.95 63.39 62.95 65.62 65.18 47.32 60.71 52.23 57.59 60.71 59.82 69.64 74.55 71.88 74.55 74. 48.20 43.10 48.07 55.80 59.12 53.59 52.49 61.33 56.35 56.35 58.56 64.09 56.35 62.98 29.82 49.72 52.49 46.96 66.30 70.17 75.69 72.93 65.75 63.54 65.75 64.12 55.68 61.14 58.03 55.44 65.80 59.59 67.36 69.43 67.88 70.98 70.47 68.91 63.73 50.28 55.44 57.51 63.73 63.73 62.18 58.55 63.21 76.17 66.84 69. 51.82 54.39 52.26 50.61 46.59 51.24 50.57 51.37 52.27 54.16 50.00 53.78 54.55 57.14 51.14 59.07 58.52 61.04 56.25 61.61 56.82 63.84 53.41 63.37 59.66 64.71 61.93 65.10 44.08 45.69 53.98 55.83 55.68 55.17 55.11 55.74 57.39 62.55 60.23 63.67 68.75 68.51 68.75 69.16 65.91 69.57 60.23 67.37 62.50 69.67 Table 3. Results on open-ended split. Criterion-level judgment performance is summarized by mean accuracy. The best and second best results are shown in bold and underlined respectively. Notably, the top performing o4-mini only achieves 32.78% in pluralistic accuracy. Implementation Details. During inference, each model was prompted to evaluate single multimodal sample containing one prompt and pair of responses, focusing exclusively on one criterion at time. The full evaluation prompts are provided in the Appendix, with minor adjustments for model-specific formatting. Sampling temperatures follow the original repository settings, or default to 0.6 with topp of 0.95 when unspecified. 4.1. Main Results Results are presented in Table 3 for open-ended responses and Table 4 for reasoning responses, revealing the following key observations and analyses: Pluralistic judging is highly challenging, especially for open-ended tasks. The Pluralistic Accuracy (PAcc), which requires correct preferences across all criteria for single query, is notably low. o4-mini and Claude-3.7Sonnet achieve the highest open-ended PAcc at 32.78% and 31.77%, while o4-mini and GPT-5 lead on verifiable reasoning with 53.17% and 45.24%. This highlights the difficulty of pluralistic judging even for state-of-the-art LMMs. As in Figure 4, for top-performing models, results on open-ended Figure 4. Average performance across each criterion. While the top model differs across criteria, all models show stronger pluralistic alignment in verifiable reasoning than in open-ended tasks. tasks are lower, reflecting the greater subjectivity and reliance on fine-grained visual perception for evaluating freeform responses (e.g., real-world images or screenshots). In contrast, existing multimodal reasoning taskstypically in6 Model Prompt-Level Criterion-Level Pluralisic Conflict Tradeoff Grounding Logic Hallucination Exploration Efficiency Avg LLaVA-OneVision-7B Llama-3.2-11B-Vision-Instruct Molmo-7B Qwen3-VL-8B-Instruct Qwen2.5-VL-7B-Instruct Eagle2.5-8B GLM-4.1V-9B-Thinking MiniCPM-V-4.5-8B InternVL3-8B-Instruct InternVL3-78B-Instruct Qwen2.5-VL-72B-Instruct InternVL3.5-8B MiMo-VL-7B InternVL3.5-38B UnifiedReward-7B (Qwen2.5-VL) LLaVA-Critic-7B (LLaVA-OV) LLaVA-Critic-R1-7B(Qwen2.5-VL) R1-Reward (Qwen2.5-VL) Claude-3.7-Sonnet Gemini-2.5-Flash Gemini-2.5-Pro GPT-4o o3 GPT-5 o4-mini 9.52 10.32 11.11 16.67 16.67 19.05 23.02 25.40 26.98 29.37 32.54 32.54 37.30 37.30 11.90 13.49 16.67 19.05 36.51 37.30 41.27 41.27 44.44 45.24 53.17 Open-Source LMMs 51.38 44.95 34.86 35.78 66.06 48.62 55.05 75.23 66.06 67.89 77.06 69.72 71.56 75.23 40.74 49.38 45.68 50.62 46.91 49.38 54.32 56.79 64.20 62.96 60.49 65.43 59.26 71.60 Finetuned Judges 64.2.0 43.21 53.09 58. 22.02 50.46 49.54 62.39 44.19 45.35 40.70 56.98 52.33 47.67 60.47 66.28 65.12 69.77 61.63 63.95 59.30 70.93 70.93 44.19 50.00 54.65 Proprietary Judge Models 83.49 73.39 75.93 84.40 82.57 78.90 83.49 74.07 67.50 79.01 61.73 67.90 74.07 77. 62.79 63.95 67.44 67.44 70.93 70.93 75.58 17.08 19.93 13.17 19.22 25.62 27.05 27.40 38.08 39.50 39.50 45.91 39.15 41.99 47.69 13.17 22.42 16.37 24.56 51.96 53.05 52.33 55.16 62.28 56.58 65.84 29.31 43.10 43.10 65.52 55.17 44.83 65.52 56.90 58.62 53.45 56.90 56.90 70.69 55.17 50.00 43.10 51.72 46. 65.52 64.91 68.97 55.17 84.21 82.76 79.31 50.55 53.85 43.96 47.25 57.14 58.24 50.55 65.93 67.03 63.74 68.13 71.43 67.03 78.02 58.24 45.05 56.04 52.75 68.13 78.02 75.56 80.22 84.62 81.32 83.52 58.72 62.39 61.47 64.22 65.14 69.72 66.97 66.97 76.15 74.31 75.23 71.56 75.23 73.39 54.13 60.55 55.05 60. 76.15 77.98 74.31 84.40 81.65 77.98 88.07 44.70 50.81 46.98 56.92 55.34 53.97 59.57 62.57 66.22 64.85 64.48 65.85 66.30 69.82 59.50 47.22 53.18 54.50 69.33 70.47 73.06 69.79 77.86 77.41 80.85 Table 4. Results on verifiable reasoning. Best and second-best results are in bold and underlined. volving multimodal math, STEM, puzzle, and coding problemsare more objective, with clearer verification and explicit visual grounding. These findings highlight the inherent difficulty of pluralistic judgment and underscore the necessity of the proposed Multi-Crit benchmark. Model performance varies across criteria and metrics. For judgment in verifiable reasoning, o4-mini performs best on Logic (75.58%) and Efficiency (88.07%), but is surpassed by o3 in Hallucination (84.21%) and Exploration (84.62%), and by Gemini-2.5-Pro in Grounding (79.01%). At the prompt level, GPT-4o shows the highest sensitivity to trade-offs in prompts with criterion conflicts, achieving the best Trade-off Sensitivity of 84.40%. However, its ability to resolve conflicting criterion-level preference pairs is notably weaker than that of the top-performing o4mini (55.16% vs. 65.84% in CMR). similar trend is observed in open-ended tasks: GPT-4oexcels in Expressiveness (76.17%) but lags in Completeness (68.14%) and Hallucination (65.75%). In contrast, o3 performs strongly on Hallucination (72.93%) but ranks low in Expressiveness (63.21%). These results highlight the multifaceted nature of pluralistic judgmentno model excels across all aspects, each showing distinct strengths and blind spots. Open-source LMMs fall short in pluralistic criterionfollowing, especially in capturing criterion-level preference conflicts. As shown in Tables 3 and 4, proprietary LMMs (e.g., o4-mini, GPT-5, o3) maintain clear lead over open-source counterparts (e.g., InternVL3.5-38B, Qwen2.5VL-72B-Instruct) across all prompt-level metrics (PAcc, CMR, and TOS) and criterion-level accuracies. Specifically, the conflict matching rate drops by about 9.4 points in open-ended tasks (43.1 33.7) and 18.1 points in reasoning tasks (65.8 47.7), far exceeding the 4and 11-point declines observed in criterion-level accuracy. This indicates that open-source models struggle to maintain consistent pluralistic evaluation across conflicting criteria, highlighting their limited ability to integrate multi-faceted criterionlevel signals into reliable judgments. Finetuned judge models improve evaluation in visual grounding but are limited in handling pluralistic criteria. striking observation is that models specifically finetuned as judges or reward models (e.g., LLaVA-Critic [46], R1-Reward [58], LLaVA-Critic-R1 [41], UnifiedRewardThink [43]) do not exhibit consistent performance gains over their base models, unlike prior benchmarks [23, 35]. For Qwen-based fine-tuned judges, R1-Reward is the only model that outperforms the base Qwen2.5-VL-7B-Instruct 7 in PAcc across both the open-ended split (9.4117.73) and the reasoning split (16.6719.05)yet its improvement at recognizing trade-off (36.1445.63, 66.0662.39) and conflict capturing (17.2820.36, 25.6224.56) remains inconsistent. Other fine-tuned judges perform similarly or worse than similar-sized models without critic tuning, remaining far behind proprietary counterparts. Another notable trend is that all Qwen-based fine-tuned judges show consistent improvements in evaluating visual grounding across both splits R1-Reward (51.760.7, 46.958.0), LLaVA-Critic-R1 (51.757.6, 46.953.1), and UnifiedReward (51.752.2, 46.964.2). For the LLaVA-based judge, LLaVA-Critic-7B (LLaVA-OV) improves on reasoning (40.743.2) but marginal drop on open-ended evaluation (48.847.3). Overall, current critic fine-tuning pipelinesdriven by holistic preference signals and limited critic prompt templatesmainly enhance judgment capacity in visualtextual grounding and alignment, but fail to generalize to diverse, fine-grained, and often conflicting evaluation criteria, thereby underscoring the need for pluralistic, criterion-aware training as introduced in Multi-Crit. 4.2. Additional Discussions To further investigate the capabilities and limitations of LMM judges in pluralistic criteria-following, we conduct three additional analyses as follows: 1. Does RLVR on multimodal reasoning improve judgment in reasoning traces? We evaluate three GRPOfinetuned models [7, 27, 42] on general reasoning tasks (e.g., math and charts), all showing clear reasoning gains. However, these models, trained on holistic accuracy rewards, show reduced sensitivity to trade-offs and conflict indicating matching in reasoning judgments (Figure 5), weaker recognition of criterion-level preference conflicts. ing that test-time scaling offers only limited and modeldependent benefits in multi-criteria judgment. Figure 6. Test-time scaling behavior by pluralistic accuracy. 3. What are the current boundaries of criteria following for open-source and proprietary models? To probe the upper limits of criterion-level consistency, we correlate each model groups strongest criterion accuracies with human inter-annotator agreement (Cohens κ), excluding reasoning-efficiency, which is largely influenced by response length. As in Figure 7, proprietary models exhibit strong and significant correlation (r=0.73, p=0.024), indicating that their upper-bound judgment patterns align closely with human agreement. In contrast, open-source models show weaker, more inconsistent relationship (r=0.36, p=0.344), reflecting limited capacity to internalize human-consistent evaluation criteria. These findings suggest that while open-source models would benefit from scaling high-quality human annotations to improve pluralistic criteria-following, proprietary systems might face the next challenge of surpassing human-level evaluation alignment. Figure 5. Results of RL-tuned reasoning models on the Multi-Crit reasoning split, all based on Qwen2.5-VL-7B. 2. Does test-time scaling hold? We run each judge model times, take majority vote for each criterion sample as the final judgment, and then compute the pluralistic accuracy. As shown in Figure 6, the strongest model, o4-mini, exhibits the most robust test-time scaling behavior for both open-ended (32.7837.12) and reasoning (53.1757.94) judgments, with GPT-4o showing similar gains. Other models show inconsistent trends and large variance, indicatFigure 7. Correlation between criterion-level top judge accuracy and inter-annotator agreement. Each point denotes one criterion. 5. Conclusion We introduced Multi-Crit, comprehensive benchmark for evaluating multimodal judges under pluralistic, criterionlevel settings. By providing fine-grained multi-criterion human annotations and complementary pluralistic metrics, Multi-Crit enables systematic analysis of models adherence to diverse criteria, trade-off recognition, and conflict resolution. Comprehensive experiments reveal key limitations of existing LMM-as-a-Judge systems: proprietary models struggle with consistent pluralistic adherence, particularly in open-ended evaluation; open-source models lag further behind in their criteria-following capacities; and critic fine-tuning, while improving visual grounding and reference alignment, remains narrowly effective and fails to generalize to fine-grained or conflicting evaluation dimensions. Additional analyses highlight diminished trade-off recognition in reasoning fine-tuning, inconsistent test-time scaling behaviors, and the human-aligned upper bounds of proprietary models in criterion-level consistency. In summary, Multi-Crit serves as challenging suite to explore the boundaries of current AI-based multimodal judge systems, paving the way toward more steerable, reliable, and even superhuman AI feedback."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 12 [2] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 1 [3] Ron Artstein and Massimo Poesio. Inter-coder agreement for computational linguistics. Computational linguistics, 34(4): 555596, 2008. 5 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 4, 5, 12 [5] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal In Fortyllm-as-a-judge with vision-language benchmark. first International Conference on Machine Learning, 2024. 2, 3, 12 [6] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting longcontext post-training for frontier vision-language models. arXiv preprint arXiv:2504.15271, 2025. 1, [7] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models, 2025. 8 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 1, 4 [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 4, 5, 12 [10] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art In Proceedings of the Computer vision-language models. Vision and Pattern Recognition Conference, pages 91104, 2025. 5 [11] Roopal Garg, Andrea Burns, Burcu Karagol-Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Michael Baldridge, and Radu Soricut. Imageinwords: Unlocking hyper-detailed image descriptions. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 93 127, 2024. 4 [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. 4, 5, The llama 3 herd of models. [13] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 1, 2 [14] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal arXiv preprint arXiv:2501.05444, reasoning benchmark. 2025. 4 [15] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 5, 12 [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 4, 5 [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 1, 2 9 [19] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 3 [20] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model In Findings of the as judge for fine-grained evaluation. association for computational linguistics ACL 2024, pages 1128611315, 2024. 2 [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 5 [22] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. Vlfeedback: large-scale ai feedback dataset for large vision-language models alignment. arXiv preprint arXiv:2410.09421, 2024. [23] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vl-rewardbench: challenging benchmark for vision-language generative reward models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2465724668, 2025. 2, 3, 7, 12 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 2, 3 [25] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Menghan Xia, Xintao Wang, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 2 [26] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. Advances in Neural Information Processing Systems, 37:4822448255, 2024. 2, 3, 4, 12 [27] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 1, 4, 8 [28] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. In European Conference on Computer Vision, pages 291309. Springer, 2024. 4 [29] OpenAI. https : / / openai.com/index/gpt41/, 2024. Accessed: 2025-11-03. ntroducing gpt-4.1 in the api. [30] OpenAI. Openai o3 and o4-mini system card. Webpage, 2025. 5 [31] Anthropic PBC. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-37-sonnet, 2025. Model release; hybrid reasoning model supporting extended thinking mode. 5 [32] Silviu Pitis, Ziang Xiao, Nicolas Le Roux, and Alessandro Sordoni. Improving context-aware preference modeling for language models. Advances in Neural Information Processing Systems, 37:7079370827, 2024. 3 [33] Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. 4 [34] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 2, 3 [35] Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. 3, 7 [36] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 12 [37] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, IYER, Sai Charitha Akula, Adithya Jairam Vedagiri Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [38] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:5776 5788, 2020. 4 [39] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 5, 24 [40] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Taha Kass-Hout, et al. Enhancing visuallanguage modality alignment in large vision language models via self-improvement. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 268282, 2025. 2 [41] Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, and Furong Huang. Llava-critic-r1: Your critic model is secretly strong policy model. arXiv preprint arXiv:2509.00676, 2025. 2, 5, 7, 12 [42] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. 8 [43] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. 2, 5, 7, [56] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. 1, 2, 3, 4 [57] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as generalarXiv preprint ist evaluator for vision-language tasks. arXiv:2311.01361, 2023. 2 [58] Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, et al. R1-reward: Training multimodal reward model through stable reinforcement learning. arXiv preprint arXiv:2505.02835, 2025. 2, 5, 7, 12 [59] Yi-Fan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen, Haochen Tian, Chaoyou Fu, Haotian Wang, Kai Wu, Bo Cui, et al. Basereward: strong baseline for multimodal reward model. arXiv preprint arXiv:2509.16127, 2025. 2 [60] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025. 2, 3, 12 [61] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. [62] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 5, 12 [44] Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, and Oleksii Kuchaiev. Rlbff: Binary flexible feedback to bridge between human feedback & verifiable rewards. arXiv preprint arXiv:2509.21319, 2025. 3 [45] Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, and Liang Pan. Are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives. arXiv preprint arXiv:2501.04003, 2025. 2 [46] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llavacritic: Learning to evaluate multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1361813628, 2025. 2, 3, 5, 7, 12 [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 5, 24 [48] Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. arXiv preprint arXiv:2502.14191, 2025. 3, [49] Qinghao Ye, Xianhan Zeng, Fu Li, Chunyuan Li, and Haoqi Fan. Painting with words: Elevating detailed image captioning with benchmark and alignment learning. arXiv preprint arXiv:2503.07906, 2025. 2, 3 [50] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn European conference on computer vision, pages sions. 6985. Springer, 2016. 1 [51] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025. 5 [52] Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, et al. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19985 19995, 2025. 2, 3 [53] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 2, 3 [54] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. 2 [55] Gailun Zeng, Ziyang Luo, Hongzhan Lin, Yuchen Tian, Kaixin Li, Ziyang Gong, Jianxiong Guo, and Jing Ma. MMCRITIC: holistic evaluation of large multimodal models as multimodal critique. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1360313630, Suzhou, China, 2025. Association for Computational Linguistics. 2, 11 A. Overview of the Appendix Benchmark Resp. Quantile Multi-Criterion Anno. MLLM-as-a-Judge [5] VL-RewardBench [23] MM-RLHF [60] Multimodal-RewardBench [48] Multi-Crit (ours) (54, 89, 153) (48, 99, 136) (16, 49, 144) (1, 72, 138) (104, 164, 247) Table 5. Comparison with existing benchmarks. Split #Prompts Cross-model pairs Intra-model pairs Open-ended Reasoning Total 299 126 425 168 (56.2%) 76 (60.3%) 131 (43.8%) 50 (39.7%) 244 (57.4%) 181 (42.6%) Table 6. Composition of response pairs. finetuned judges [41, 43, 46, 58], we adapt this template to match the prompt format used in their original repositories, avoiding significant out-of-distribution judgment behaviors. C.2. Additional Data Statistics Response Pair Composition. As shown in Table 6, for both splits, roughly 40% of response pairs originate from the same model via random sampling, while the remaining pairs are generated by different models. This balanced mix ensures that Multi-Crit evaluates whether LMM judges can capture response-quality differences both across models (reflecting model and training diversity) and within the same model (reflecting sampling diversity). Cross-Criterion Correlation and Conflict Frequency. We report additional statistics on criterion-level human preferences to illustrate how Multi-Crit captures crosscriterion correlations and conflicts. Figure 8 shows how human preferences correlate across criteria on prompts that exhibit criterion-level conflicts. Figure 9 reports how often two criteria yield opposite human preferences on the same response pair. Multi-Crit contains preference conflicts across all criterion pairs in both splits, reflecting both the nuanced differences within the response pairs and the inherent diversity and tension across evaluation dimensions. Certain criterion pairs display intuitive trade-offssuch as completeness vs. no-hallucination and no-hallucination vs. expressiveness in the open-ended split, and exploration vs. efficiency in the reasoning splitand these pairs also exhibit the highest frequencies of human-annotated conflicts. Section illustrates the differences between Multi-Crit and prior multimodal judge benchmarks. Section provides additional benchmark details, including the response models, evaluation criteria, judge prompt template, and supplementary data statistics. Section presents qualitative examples of multimodal prompts and complete evaluation instances. Section reports additional results and analysis. B. Comparison Against Prior Benchmarks As shown in Table 5, earlier benchmarks assign only single overall preference label for each pair of model responses, whereas Multi-Crit provides multiple criterionlevel human annotations. This makes Multi-Crit the first benchmark capable of assessing whether LMM judges can follow pluralistic evaluation criteria and deliver reliable criterion-level judgments. In addition, the average length of candidate model responses in Multi-Crit is significantly longer (164 vs. 99 words), further highlighting the increased difficulty and overall challenge of the benchmark. C. Benchmark Construction and Statistics C.1. Additional Benchmark Details Models for Generating Candidate Responses: Gemini2.5-Pro [9], Gemini-2.5-Flash [9], GPT-4.1 [29], GPT- [4], Qwen2.5-VL4o [1], Qwen2.5-VL-7B-Instruct 72B-Instruct [12], InternVL3-8B-Instruct [62], InternVL3-38B-Instruct [62], MiMo-VL-7B-RL [36], GLM-4.1V-9B-Thinking [15]. [4], LLaMA-3.2-11B-Vision-Instruct Specifically, for prompts from WildVision-Battle [26], if the two original responses receive tie in their human preference, we keep the original pair; otherwise, we generate new responses using the models listed above. Detailed Evaluation Criteria. Table 7 presents the detailed criteria for judging open-ended generation, and Table 8 lists those for judging verifiable reasoning. To better guide both human annotators and judge LMMs, we organize the description of each criterion into three components: 1. Definition: general descriptions of the criterion and the key aspects response should demonstrate. 2. Positive Indicators: Behaviors and qualities that are encouraged and should be preferred in responses. 3. Negative Indicators: Errors, undesirable behaviors, or deficiencies that should be penalized under this criterion. Evaluation Prompt Template. We provide the evaluation prompt used for judge-model inference in Table 9. When evaluating LMM judges on the Multi-Crit benchmark, each criterion instanceconsisting of the question, the pair of responses, and the target criterionis filled into this template and sent to the model during inference. For 12 Evaluation Criterion Descrption Completeness and Coverage The response should provide thorough and well-developed answer that fully addresses the intent of the prompt. It must address the complete scope of the task, incorporating all major elements specified in the prompt, as well as relevant visual aspects and broader contextual cues. When appropriate, drawing on relevant external knowledge is encouraged to enrich the explanation. - Reward: The response addresses all key parts of the prompt and image, showing depth and effort in the description or explanation. - Penalize: The response is underdeveloped, fails to meet one or more specific requirements in the prompt, or omits important visual elements or interpretive points. Visual Grounding and Details The response should demonstrate clear and meaningful connection to the visual input. It should refer to observable elements in the imagesuch as objects, spatial relationships, colors, or textand build its description or interpretation based on those elements. - Reward: The response explicitly references relevant visual details that are clearly visible in the image. - Penalize: The response fails to connect meaningfully to the image, or uses vague, generic language that lacks specific visual grounding. Factuality / No Hallucination The response should avoid introducing any visual details, objects, relationships, or factual claims that are not present in the image or reasonably suggested by the prompt. This includes both visual hallucinations (e.g., describing elements not visible in the image) and factual inaccuracies in general knowledge. - Reward: The response stays grounded in the image and prompt, without inventing visual elements or making unsupported factual claims. - Penalize: The response introduces hallucinated visual content or inaccurate factual statements that are unsupported or misleading. Creativity and Expressiveness The response should show originality or stylistic flair for open-ended tasks, and knowledge-informed articulation with precision and depth for analytical tasks. All responses must remain contextually appropriate and grounded in the visual input, while enhancing richness, nuance, and overall engagement. - Reward: The response uses vivid language, unique phrasing, or inventive associations that enrich the interpretation, or it demonstrates professional articulation through deep and knowledge-grounded analysis. - Penalize: The response is overly literal, flat, or dull, lacking originality, variation in expression, or in analytical contexts, fails to demonstrate professional depth or expertise. Clarity and Coherence The response should communicate ideas clearly and logically, with coherent structure and fluent language. This involves not only grammatical correctness, but also effective organization of information, smooth transitions, and consistent flow of ideas. - Reward: The response is clearly written, logically structured, and easy to follow. brief summary at the beginning may further improve clarity. - Penalize: The response is difficult to follow due to unclear structure, disorganized reasoning, poor transitions, or awkward and repetitive phrasing. Table 7. Five evaluation criteria for judging open-ended content generation tasks in Multi-Crit Benchmark. 13 Evaluation Criterion Descrption Visual Grounding Logic Coherence and Consistency Factuality / No Hallucination The response should be explicitly grounded in the visual input. It must refer to salient visual elementssuch as specific objects, spatial arrangements, colors, or visible textand incorporate them meaningfully into the reasoning process. Visual references should be accurate and relevant to the task. - Reward: The response clearly references important visual features and integrates them into the reasoning in precise and relevant manner. - Penalize: The response fails to reference relevant visual elements, or uses generic or weakly connected visual details that do not meaningfully support the reasoning. The reasoning should follow logically sound and step-by-step progression, with each step building upon the previous one. The reasoning should be internally consistent, with no contradictions, missing steps, or unjustified leaps. The final answer should naturally and justifiably emerge from the reasoning process. - Reward: The response presents well-structured, internally consistent chain of reasoning that leads clearly and justifiably to the final answer. - Penalize: The response contains contradictions, missing steps, or disconnects between reasoning and answer. Short-cut behaviorssuch as giving the final answer first with unsupported or inconsistent reasoningshould also be penalized. All claims and reasoning steps must be factually accurate and supported by the image or the prompt. The response should avoid hallucinated visual content, misidentifications, or factual inaccuracies in the reasoning process. - Reward: The response is free from factual errors or hallucinations and relies only on valid observations and logical inferences. - Penalize: The response introduces hallucinated details, misidentifications, or incorrect factual claims that compromise the reasoning. Reflection and Exploration The reasoning should demonstrate thoughtful reflection and willingness to explore multiple possibilities, particularly when the task is ambiguous or complex. This includes acknowledging uncertainty, considering alternative interpretations, or revising initial assumptions. - Reward: The response demonstrates depth through reflection, critical evaluation, or exploration of different solutions before reaching conclusion. - Penalize: The response is overly rigid, superficial, or rushed, showing little to no depth of thought, reflection, or exploration of alternative possibilities. Conciseness and Efficiency The reasoning should be clear, focused, and efficiently communicate the steps. It should avoid redundancy, digressions, or unnecessary elaboration that dilute the argument. For straightforward tasks, over-explaining or over-analyzing should also be avoided. - Reward: The response is concise and well-structured, conveying reasoning steps precisely and proportionally to the task complexity. - Penalize: The response is verbose, repetitive, or includes irrelevant content that distracts from the reasoning. It may also overthink or over-explain simple prompts. Table 8. Five evaluation criteria for judging verifiable reasoning tasks in Multi-Crit benchmark. 14 You are an expert in evaluating the quality of AI-generated responses according to specific evaluation criteria. Your task is to assess two responses generated by different AI assistants in reply to users question about an image. The image is provided as part of the input. You must evaluate the responses **strictly and exclusively** based on the following evaluation criterion: {Criterion} Do not consider any other dimensions or criteria beyond what is specified above. Here are the inputs for your evaluation: [Question]: {Question} [Response 1]: {Response1} [Response 2]: {Response2} First, provide detailed justification for your evaluation. Refer to specific elements in the responses, how they align with the evaluation criterion, and relevant visual details from the image. On the final line, provide your final judgment on which response is better. Your judgment must be based solely on the specified criterion. Strictly follow this format: Response is better. Table 9. Evaluation prompt template used for LMM judge inference in Multi-Crit. Judge models are explicitly instructed to focus strictly and exclusively on the target evaluation criterion during each inference. Figure 8. Correlation of criterion-level human preferences for prompts that exhibit preference conflicts in the open-ended (left) and reasoning (right) splits of Multi-Crit. Figure 9. Counts of criterion pairs exhibiting human preference conflicts in the open-ended (left) and reasoning splits (right) of Multi-Crit. key intermediate inference, while the other offers more visually detailed description of the food-web structure. D. Qualitative Examples D.1. Multimodal Prompts Figure 10 visualizes selected multimodal prompts from Multi-Crit, spanning diverse open-ended generation scenariossuch as captioning, storytelling, knowledge-based tasks, and text-rich understandingas well as verifiable reasoning domains including math, science, coding, and visual analogy. These prompts cover wide range of LMMas-a-Judge use cases. D.2. Full Evaluation Instances We present six full evaluation instanceseach consisting of prompt and paired model responsesalong with their multi-criterion human preference annotations. These examples illustrate how our curated data reflects fine-grained response differences and demonstrates criterion-level preference conflict patterns within each response pair. The first three examples illustrate judgment in openended tasks: Table 10 shows an evaluation instance for judging two models in blind storytelling. This example reveals preference conflicts such as completeness vs. grounding and grounding vs. no-hallucination. Table 11 presents an evaluation instance for judging two models on captioning creative images. It exhibits representative preference conflicts including completeness vs. no-hallucination, completeness vs. expressiveness, and expressiveness vs. clarity. Table 12 provides an example of judging two responses from the same model on an instruction-rich image analysis task. Observed conflicts include grounding vs. clarity and the less common no-hallucination vs. clarity. The following three examples illustrate judgment in verifiable reasoning tasks: Table 13 shows an evaluation instance for judging two models thinking traces on math reasoning task. Although both traces lead to incorrect answers, one exhibits better visual grounding and explores alternative answers, whereas the other maintains stronger logical consistency with fewer perceptual hallucinations. Table 14 presents an evaluation instance for judging two thinking traces produced by the same model on object counting. While both responses lead to the correct final answer, one conducts concise reasoning, whereas the other identifies more visual details and actively reflects on each object, but introduces minor hallucinations that do not affect the final result. Table 15 provides an example of judging two models responses on biological reasoning task involving food In this case, one response is more concise and web. demonstrates deeper reflection by explicitly identifying 16 Figure 10. Example prompts in the open-ended (top) and verifiable reasoning (bottom) splits of Multi-Crit. Our benchmark spans diverse multimodal prompts, reflecting the broad evaluation domains in which LMM judges are used. 17 Open-ended Example 1 Question Response (llava-v1.6-vicuna-7b) Response (yi-vl-plus) You are storyteller for the blind. Please tell this comic book panel by naming character according to the image and the following script extract. Corresponding characters names have been added in white rectangles over the original image (do not mention rectangle or label in the storytelling). DIALOGUE Marco: AN INVENTOR? DONT BELIEVE YOU! Babi: WHY NOT? In the comic book panel, character is named Marco. He is speaking to another character named Babi. Marco is questioning Babis claim that he is an inventor. Babi responds with an open-ended question, encouraging Marco to reconsider his skepticism. Marcos skepticism towards Babis claim is evident from his words, DONT BELIEVE YOU! It seems like Marco is questioning Babis credibility or the authenticity of his claim. Babis open-ended question, WHY NOT?, indicates that he is not backing down and is pushing for dialogue or conversation where Marco can clarify his doubts. This comic panel seems to be humorous take on skepticism and the importance of asking questions to explore different perspectives and possibilities. In vividly colored comic book panel, we see two characters engaging in lively conversation. One character, with bright yellow head and red shirt, exclaims, An inventor? dont believe you! His name, Marco, is displayed in white rectangle. Opposite him, another character, sporting black hairstyle and blue shirt, confidently asks, Why not? His name, Babi, is also shown in white rectangle. The contrasting colors of their clothes and hair add to the dynamic energy of their exchange, as they stand against plain blue backdrop. The speech bubbles capture their dialogue, highlighting the playful and inquisitive spirit of their interaction. Ground Truth Completeness and Coverage Visual Grounding and Details Factuality / No Hallucination > > > Table 10. Example of an open-ended evaluation instance for blind storytelling. Response conveys the moral of the story and fully follows the instruction to avoid mentioning rectangles. Response includes more visual details but hallucinates the shirt colors. 18 Open-ended Example 2 Question What does this image show? Response (Llama-3.2-11B-Vision-Instruct) Response (Qwen2.5-VL-7B-Instruct) Ground Truth The image depicts surreal scene where bottle of beer is pouring its contents onto the ground, creating puddle that morphs into miniature pool. Within this pool, two women are shown sitting and posing in swimsuits, as if they were enjoying day at the beach or poolside. The juxtaposition of the beers spillage with the serene atmosphere of the women creates humorous and unexpected visual contrast. This image is creative and surreal depiction that combines elements of everyday objects with imaginative scenarios. It shows large, tilted bottle pouring liquid onto the ground. The liquid spreads out in pool around two women who are sitting on the edge of the puddle. One woman is partially submerged in the liquid, while the other sits on the edge looking at her. The scene is contrasted by the dark, plain background, which makes the bottle and the women stand out prominently. This type of imagery is often used in advertising or artistic representations to convey message or evoke emotions. Completeness and Coverage Factuality / No Hallucination Creativity and Expressiveness Clarity and Coherence > > > > Table 11. Example of an open-ended evaluation instance for creative-image captioning. Response creatively highlights the humorous and unexpected effect and introduces less hallucinated content. Response includes brief TL;DR and an inference about image usage, with more coherent flow across elements. 19 Open-ended Example 3 Question Response (GPT-4.1) Response (GPT-4.1) Create detailed one sentence image caption, capture the essence and include the following structured elements without assumptions or ambiguous language: Background and Foreground: Separate background objects and foreground objects to enhance focus; Location: The setting can be either indoors or outdoors; Shot Size: The image sizes: Extreme close up, Close up, Medium close up, Medium shot, Medium full shot, Full length. Use this parameter if person is included in the photo; Angle: low angle shot, high angle shot, top view shot, flat lay. If the angle is straight, omit this specification; Temperature: Color temperature should be considered, with options including cold, warm, neutral, or vibrant. If the temperature is neutral, you may choose to not specify this detail; Lens: Use fisheye lens or wide-angle as appropriate. If neither is used, this detail can be omitted; Lighting: Very important options include High-key Lighting, Low-key Lighting, Dramatic light, Backlight, silhouette, cinematic light; Composition: Employ techniques such as the rule of thirds, double exposure, over-the-shoulder shots, or symmetrical composition to structure the image; Day Time: Specify the time of day (e.g., blue hour, sunrise, morning, noon, afternoon) if this parameter is explicitly relevant; Person Caption: If person is included, describe their race of skin color (Asian, Caucasian, Black, Arab, Indian, etc), hair (black, brown, blond, white/gray, and rarely red) view direction, emotions and mood, and clothing details. Foreground: Two children, one with light skin and straight brown hair, wearing charcoal grey shirt and black shorts, and the other with dark skin and short curly black hair, dressed in beige shirt and black shorts, are seated close to each other with the child on the right resting an arm around the others shoulders; both appear calm and friendly, looking directly at the camera. Background: Metal fencing with vertical green posts is visible, along with an overcast sky and faint outlines of streetlight or utility pole. Location: Outdoors. Shot Size: Medium shot. Angle: Low angle shot. Temperature: Neutral. Lighting: Natural diffused daylight. Composition: Centered composition, subjects positioned side by side with upper bodies and faces as focal point. Day Time: Daytime. Foreground: Two boys are sitting close together on what appears to be outdoor seating; Background: wire mesh fence and holding poles are visible against blank, overcast sky; Location: Outdoors; Shot Size: Medium shot; Angle: Low angle shot; Temperature: Neutral; Lighting: Diffused natural light due to an overcast day; Composition: Symmetrical composition with both boys centered in the frame; Day Time: Daytime, likely noon or afternoon given the soft daylight; Person Caption: On the left, Caucasian boy with straight, shoulder-length brown hair faces the camera with neutral, slightly guarded expression, wearing charcoal grey henley shirt and black shorts; on the right, Black boy with short, curly black hair also faces the camera with calm, faintly smiling demeanor, dressed in loose beige t-shirt and black shorts, his left arm draped supportively behind the other boy. Ground Truth Completeness and Coverage Visual Grounding and Details Factuality / No Hallucination Clarity and Coherence > > > > Table 12. Example of an evaluation instance in instruction-rich image analysis. Both responses are randomly sampled from GPT-4.1. Response provides more background details and is less hallucinated, while Response better follows all required elements in the user prompt and offers more clear and readable description of the foreground and composition. 20 Verifiable Reasoning Example 1 Question Answer Response (Qwen2.5-VL -72B-Instruct) Response (Gemini-2.5 -Flash) The entire pie seen in the figure is divided among several children. Each child receives piece of pie, and each piece of pie looks identical. The maximum possible number of children there is: Choices: (A) 10 (B) 12 (C) 9 (D) 11. To determine the maximum possible number of children who can receive an identical piece of the pie, we need to analyze the symmetry of the figure. 1) Identify the Symmetry: The figure appears to be circular design with central red area and green circles arranged around it. The key is to find the rotational symmetry of the design. 2) Count the green circles: There are 12 green circles arranged around the central red area. This suggests that the design might have 12-fold rotational symmetry. 3) If we rotate the figure by 360 12 = 30, the figure should look the same. This means that the design is symmetric under 12-fold rotation. 4) Conclusion: Since the design has 12-fold rotational symmetry, the pie can be divided into 12 identical pieces. Therefore, the maximum possible number of children who can each receive an identical piece of the pie is 12. Thus, the correct answer is . To find the maximum possible number of children, we need to determine the number of identical pieces the pie can be divided into. This means identifying the smallest repeating unit in the circular pattern. Lets observe the elements arranged in circle: 1) Large green circles: Count the number of large green circles arranged in the outer ring. There are 12 of them. 2) Small green/grey circles: Count the number of smaller green/grey circles in the inner ring. There are also 12 of these. 3) Orange shapes: Count the number of orange shapes in the very center. There are 12 of these as well. Since all three types of repeating elements (large green circles, small green/grey circles, and orange shapes) appear 12 times and are arranged symmetrically around the center, the pie exhibits 12-fold rotational symmetry. If each child receives an identical piece of pie, then the pie must be divided into sectors that are identical in shape and contain identical patterns. The smallest such sector would be one that contains one of each of the repeating elements. Imagine drawing radial lines from the center of the pie that equally divide the circle. If we divide the pie into 12 equal sectors (each 360/12 = 30 degrees), each sector will contain: One large green circle. One small green/grey circle. One orange shape. Since these 12 pieces would be identical and cover the entire pie, the maximum possible number of children is 12. If there were fewer children (e.g., 6), each piece would be larger and contain multiple sets of these elements (e.g., two large green circles, two small green circles, two orange shapes), but the question asks for the *maximum* number, which corresponds to the smallest possible identical pieces. Therefore, the maximum number of children is 12. The final answer is 12 . Ground Truth Visual Grounding Logical Coherence and Consistency Factual Correctness / No Hallucination Reflection and Exploration Conciseness and Efficiency > > > > > Table 13. Example of an evaluation instance for math reasoning. Both responses appeal to rotational symmetry but arrive at incorrect final answers. Response is more concise and avoid hallucinated colors. Response demonstrates slightly better grounding for the inner orange shapes and actively explores alternative solutions, yet contains some logical shifts that reduce coherence. 21 Verifiable Reasoning Example Question Answer Is the number of green bus greater than the number of blue school bus? Options: (A) Yes (B) No. Response (Gemini-2.5-Flash) Response (Gemini-2.5-Flash) 1.Identify green buses: In the image, there is one long green vehicle that is bus. So, the number of green buses is 1. 2.Identify blue school buses: There are no blue vehicles visible in the image, and certainly no blue school buses. So, the number of blue school buses is 0. 3. Compare the numbers: 1 (green buses) is greater than 0 (blue school buses). Therefore, the number of green buses is greater than the number of blue school buses. The final answer is Heres breakdown of the vehicles in the image: - There is one green bus in the lower right portion of the image. - There is gray dirt bike in the middle-left. - There is golden motorcycle (possibly chopper or custom bike) with red and green accents in the upper-middle. - There is gray airplane with blue and green accents in the upper-right. Based on this observation: - Number of green buses = 1 - Number of blue school buses = 0 (There are no blue buses, nor any vehicles that distinctly look like school bus of any color, let alone blue.) Since 1 is greater than 0, the number of green buses is greater than the number of blue school buses. The final answer is Ground Truth Visual Grounding Factual Correctness / No Hallucination Reflection and Exploration Conciseness and Efficiency > > > > Table 14. Example of an evaluation instance for object counting. Both Gemini-2.5-Flash responses correctly identify more green buses than blue ones. Response is more compact. Response provides finer object details and spatial relationships and explicitly checks each visible object, but introduces minor hallucination by stating that no school bus is present, even though one appears in the image. Verifiable Reasoning Example 3 Question Answer Response (MiMo-VL -7B-RL) Response (Gemini-2.5 -Flash) In the diagram of the food web shown, pointing arrow from to indicates that is the source of energy that helps survive. If the number of lizard and grasshopper decrease, the population of grass will likely? (A) decreases (B) increases (C) remains the same (D) cannot be determined from the image. <think> need to analyze this food web diagram to understand the relationships between organisms and determine what would happen to the grass population if lizards and grasshoppers decrease. Looking at the diagram: - Grass is eaten by: grasshopper, rabbit, and mouse - Grasshopper is eaten by: lizard and hawk - Lizard is eaten by: hawk - Rabbit is eaten by: hawk - Mouse is eaten by: hawk and snake - Snake is eaten by: hawk If the number of lizards and grasshoppers decreases: 1. Grasshoppers eat grass, so fewer grasshoppers would mean less consumption of grass. 2. Lizards dont directly eat grass, but they eat grasshoppers. With fewer lizards, there would be fewer predators controlling the grasshopper population, which could potentially lead to more grasshoppers in the long run. However, the question states that both lizard and grasshopper populations are decreasing. Since both grasshoppers (which eat grass) and lizards (which control grasshoppers) are decreasing, we have two opposing effects: - Fewer grasshoppers means less grass consumed - Fewer lizards means less predation on grasshoppers, which could lead to more grasshoppers in the future. However, since the question specifically asks about the immediate effect of these decreases, the most direct impact would be that with fewer grasshoppers consuming grass, the grass population would likely increase. </think> In this ecosystem: - Grass is consumed by three primary herbivores: grasshoppers, rabbits, and mice - Grasshoppers are preyed upon by both lizards and hawks - Lizards are preyed upon by hawks. When the number of lizards and grasshoppers decreases: 1. Grasshoppers directly consume grass. With fewer grasshoppers, there would be less consumption of grass. 2. Lizards dont directly consume grass, but they do control the grasshopper population through predation. With fewer lizards, there would be less predation pressure on grasshoppers, which could potentially lead to more grasshoppers in the future. However, the question specifies that both lizard and grasshopper populations are decreasing. The most immediate effect would be reduced grass consumption due to fewer grasshoppers. Since grass is also consumed by rabbits and mice (whose populations arent mentioned as changing), the reduction in grasshoppers would likely result in increased grass availability. Therefore, the population of grass will likely (B)increases . To determine the likely change in the grass population, we need to look at what organisms consume grass in this food web. 1. Identify consumers of Grass: The diagram shows arrows from Grass to Grasshopper, Rabbit, and Mouse. This means Grasshoppers, Rabbits, and Mice all eat Grass. 2. Analyze the given scenario: the number of lizard and grasshopper decrease. 3. Focus on the direct impact on Grass: The Grasshopper population decreases. Since grasshoppers eat grass, decrease in their numbers means there will be fewer organisms consuming grass. The Lizard population decreases. Lizards eat grasshoppers. While decrease in lizards might lead to an increase in grasshoppers (their prey) in some scenarios, the problem explicitly states that grasshoppers also decrease. Therefore, the decrease in lizards doesnt counteract the effect of fewer grasshoppers eating grass. In fact, fewer lizards might be consequence of fewer grasshoppers. 4. Conclusion: With fewer grasshoppers eating grass, the predation pressure on the grass population is reduced. Assuming other factors (like Rabbit and Mouse populations, or environmental conditions) remain constant, the grass will be consumed less. This reduced consumption will likely lead to an increase in the grass population. Therefore, if the number of lizard and grasshopper decrease, the population of grass will likely increase. The final answer is . Ground Truth Visual Grounding Reflection and Exploration Conciseness and Efficiency > > > Table 15. Example of an evaluation instance in judging biological reasoning. Both responses reach the correct conclusion (grass increases). Response provides more complete and detailed description of the food web. Response is more efficient and together shows deeper reflection by explicitly recognizing that the decrease in lizards does not counteract the effect of fewer grasshoppers eating grass. 23 E. Additional Experimental Results E.2. Joint Multi-Criterion Judgment Task Formulation. In our standard setting, each evaluation instance (prompt + response pair) with applicable criteria requires the LMM judge to perform separate inferences, each instantiated with single-criterion evaluation prompt as shown in Table 9. Here, we investigate an alternative setting in which the judge performs joint multicriterion judgment. For each instance, all criteria are presented to the model simultaneously, and single inference is used to produce criterion-level judgments for all criteria. Table 17 shows the corresponding joint evaluation prompt: the model is instructed to treat each criterion separately while outputting all criterion-level judgments in one pass. All inference hyper-parameters remain identical to the standard single-criterion setup, allowing us to isolate the effect of joint prompting on judge model behavior. Results and Analysis. We evaluate four top-performing proprietary LMMsGPT-4o, GPT-5, o4-mini, and Gemini2.5-Prounder the joint multi-criterion judgment setting. The results in Table 18 reveal two major observations: 1. Joint multi-criterion judgment affects models unevenly. GPT-4o exhibits clear performance drop across all metrics, suggesting that combining multiple criteria in single pass amplifies its internal biases and weakens its ability to follow diverse criteria. GPT-5, in contrast, benefits from the joint settingshowing improved pluralistic accuracy and better alignment with human preferences on conflict cases. For o4-mini and Gemini-2.5-Pro, the effects of joint criterion judgment are mixed on openended tasks but consistently drop on reasoning domains. 2. Joint judgment generally reduces the models sensitivity to criterion-level trade-offs. Across all modelsexcept GPT-5 in the reasoning splitthe trade-off sensitivity decreases. This trend is expected: given the autoregressive nature of LLMs/LMMs, generating multiple criterion-level judgments within single inference pass inevitably induces inter-criterion dependencies, making the judge model more likely to assign the same preference direction across criteria for the same pair of responses. Table 19 shows case where o4-mini fails to capture criterion-level conflicts under joint criterion judgment. E.1. To Think or Not? To study how thinking influences judge model behaviors in following diverse criteria, we examine two strong open-source LMM families that are known for producing high-quality reasoning traces across diverse domains. InternVL3.5 [39] supports both thinking (default) and non-thinking modes controlled via system prompts, enabling clean mode-switch comparison within the same model weights. Qwen3-VL [47] provides two model variantsInstruct and Thinkingthe latter specifically developed to enhance reasoning capabilities. Table 16 presents the results, showing two trends: 1. Smaller models benefit more from thinking, while larger models show limited gains. For the smaller 8B models, both the mode-switch (InternVL3.5) and model-variant (Qwen3-VL) show clear and consistent improvements from enabling thinking, with gains across all metrics on both open-ended and In contrast, the larger 30B models reasoning splits. show more modest effects: thinking brings no significant improvement for InternVL3.5, and for Qwen3-VL it enhances reasoning judgments but slightly reduces performance on open-ended splits. These results suggest that explicit thinking during judgment helps smaller models better adhere to fine-grained evaluation criteria and produce criterion-specific critic reasoning. For larger models that already internalize such criteria-following capacities and reasoning patterns, additional thinking offers little benefit and may even amplify judge model biases. 2. Thinking benefits reasoning judgments more. For both Qwen3-VL and InternVL families, the thinking judge yields larger gains in evaluating verifiable reasoning than in open-ended generation tasks. Specifically, Qwen3-VL-Thinking outperforms its Instruct variant at both 8B and 32B scales, demonstrating improved ability to recognize criterion trade-offs and capture preference conflicts. This finding differs from the observation in Sec. 4.2, where RL-finetuned reasoning models on domain-specific data based on Qwen2.5-VL showed no improvement in reasoning judgments and even weakened trade-off recognition. This indicates that the capacity for reasoning judgment arises from general thinking abilities developed through broad reasoning training across diverse domains, rather than from narrow domain-specific fine-tuning, which often leads to overfitting and reduced generalization. 24 Model Open-ended Verifiable Reasoning Pluralistic Conflict Tradeoff Crit.-Avg. Pluralistic Conflict Tradeoff Crit.-Avg. InternVL3.5-8B (no-think) InternVL3.5-8B (think) Qwen3-VL-8B-Instruct Qwen3-VL-8B-Thinking InternVL3.5-38B (no-think) InternVL3.5-38B (think) Qwen3-VL-32B-Instruct Qwen3-VL-32B-Thinking 23.41 25.08 +1.67 18.39 24.75 +6.36 29.43 30.43 +1.00 30.43 29.10 -1.33 24.95 32.34 +7.39 18.36 37.33 +18.97 35.93 33.73 -2.20 40.32 40.12 -0. 44.66 62.14 +17.48 34.47 66.50 +32.03 61.65 64.08 +2.43 68.93 67.96 -0.97 59.09 61.04 +1.95 54.16 60.61 +6.45 65.36 65.10 -0.26 65.49 64.88 -0.61 27.78 32.54 +4.76 16.67 38.89 +22.22 40.48 37.30 -3.18 39.68 43.65 +3. 28.83 39.15 +10.32 19.22 51.25 +32.03 47.69 47.69 0.00 48.75 53.38 +4.63 54.13 69.72 +15.59 35.78 83.49 +47.71 75.23 75.23 0.00 70.64 80.73 +10.09 61.96 65.85 +3.89 56.92 70.70 +13.78 71.37 69.82 -1.55 71.42 73.88 +2. Table 16. Comparison of thinking vs. non-thinking LMM judges. Relative improvements are shown in green and decreases in red. 25 You are an expert in evaluating the quality of AI-generated responses according to multiple evaluation criteria. Your task is to assess two responses generated by different AI assistants in reply to users question about an image. The image is provided as part of the input. You must evaluate the responses based on the following {K} evaluation criteria. Analyze each criterion independently and exclusivelyyour judgment on one criterion should not influence your judgment on another. Do not consider any other dimensions or criteria beyond what is specified below. {Criteria} Here are the inputs for your evaluation: [Question]: {Question} [Response 1]: {Response1} [Response 2]: {Response2} Instructions: For each criterion listed above, you must: (1) provide detailed justification for your evaluation, referring to specific elements in the responses, how they align with that criterion, and relevant visual details from the image; and (2) on the final line of each criterions evaluation, provide your judgment based solely on that criterion. You must choose one response as better; do not indicate tie. Format your response as follows for each criterion: Criterion: [Criterion Name 1] [Your detailed justification here] Judgment: Response is better. Criterion: [Criterion Name 2] [Your detailed justification here] Judgment: Response is better. . . . (continue for all criteria) Strictly follow this format: Response is better. on the last line of each criterion block. Table 17. Joint multi-criterion evaluation prompt for LMM judges. All applicable criteria for an evaluation instance are assessed in single pass, and the judge is explicitly instructed to treat each criterion separately. Model GPT-4o + Joint multi-criterion GPT-5 + Joint multi-criterion o4-mini + Joint multi-criterion Gemini-2.5-Pro + Joint multi-criterion Pluralistic Conflict Tradeoff Criterion Avg. Pluralistic Conflict Tradeoff Criterion Avg. Open-ended Verifiable Reasoning 31.44 30.10 -1.34 29.77 34.78 +5. 32.78 36.79 +4.01 28.76 32.11 +3.35 44.91 26.35 -18.56 38.52 43.51 +4.99 43.11 40.12 -2.99 37.92 41.12 +3. 66.02 38.83 -27.19 62.62 60.68 -1.94 64.56 58.74 -5.82 66.50 62.62 -3.88 69.57 64.91 -4.66 68.51 70.43 +1. 69.67 68.82 -0.85 63.67 67.06 +3.39 41.27 34.13 -7.14 45.24 47.62 +2.38 53.17 45.24 -7.93 41.27 35.71 -5. 55.16 34.52 -20.64 56.58 61.57 +4.99 65.84 55.87 -9.97 52.33 40.21 -12.12 84.40 58.72 -25.68 78.90 82.57 +3. 83.49 77.06 -6.43 75.93 56.88 -19.05 69.79 63.15 -6.64 77.41 79.58 +2.17 80.85 75.61 -5.24 73.06 70.96 -2. Table 18. Comparison of joint multi-criterion judgment against standard single-criterion judgment. Relative improvements are shown in green, decreases in red. 26 Case Study: Joint Multi-Criterion Failure of the o4-mini Judge in Capturing Preference Conflicts Question Response 1 Response Ground Truth Single-Criterion Judgment (o4-mini) Joint Multi-Criterion Judgment (o4-mini) Provide single 10-year age bracket (e.g., 30s, 40s, etc.) and describe the mans hair color and hairstyle based only on the image. Based on the image provided, the man appears to be in his 30s. His hair is dark brown and is styled in classic fashion, swept to the side, which complements his period attire. The style is reminiscent of early 20th-century fashion, which aligns with the vintage feel of the image. The man in the photo appears to be in his late 30s to early 40s. His hair is dark brown with slight wave pattern. It is neatly styled, suggesting that he takes care of his appearance. The hair is cut short, and the parting on the side complements his overall look. Completeness and Coverage Factuality / No Hallucination Resp. 1 > Resp. 2 Resp. 2 > Resp. 1 Completeness and Coverage The prompt asks for single 10-year age bracket (e.g., 30s, 40s, etc.). Response 1 clearly states in his 30s, satisfying that requirement exactly. Response 2 instead gives late 30s to early 40s, which spans two decades and thus does not fulfill the prompts decade-long range specification. - Both responses describe the hair color (dark brown) and hairstyle. However, Response 1 stays focused on the core question by succinctly noting the side-swept, classic style evident in the image. Response 2 adds extra inferences (suggesting that he takes care of his appearance) and splits the age range improperly, making it less precise with respect to the users request. Because Response 1 fully and directly addresses both parts of the questionproviding the correct decade-long age bracket plus hair color and stylewhile Response 2 does not, Response 1 is the better answer under the completeness and coverage criterion. Response 1 is better. Factuality / No Hallucination Response 1 introduces unsupported details about period attire and early 20th-century fashion, neither of which are evident in the imagethis is visual hallucination. Response 2, by contrast, sticks to what is visible (age estimate spanning late 30s to early 40s, dark brown hair, slight wave, side part, neatly styled) without inventing extraneous context. Response 2 is better. Criterion: Completeness and Coverage Response 1 directly answers both parts of the prompt within the requested format: it gives single 10-year range (30s) and describes hair color (dark brown) and style (classic fashion, swept to the side), even tying in contextual cues (period attire, vintage feel). Response 2, by contrast, straddles two decades (late 30s to early 40s), which fails to provide single 10-year band as requested, and although it describes hair color and neat styling, it does not anchor its observations as neatly to the prompts format. Judgment: Response 1 is better. Criterion: Factuality / No Hallucination Both responses correctly observe dark brown hair and side part, but Response 2 introduces imprecision by spanning late 30s to early 40s, which contradicts the users instruction for single decade. Response 1 sticks to plausible age (30s) based on the mans appearance. Neither response hallucinates details about color, but Response 2s age estimate violates the prompts constraints. Judgment: Response 1 is better. Table 19. In this case, o4-mini correctly identifies the completeness vs. no-hallucination conflict under single-criterion judgments but collapses to uniform preference under joint multi-criterion judgment. With single-criterion evaluation prompts, the model successfully adheres to each criterion independently and captures the preference disagreement, whereas under joint prompting, completeness dominates and the model fails to follow the no-hallucination criterion."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park",
        "University of Waterloo"
    ]
}