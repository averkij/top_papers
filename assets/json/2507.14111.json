{
    "paper_title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
    "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm. CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100. The capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 4 1 1 1 4 1 . 7 0 5 2 : r CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li and Chris Shum DeepReinforce Team research@deep-reinforce.com"
        },
        {
            "title": "Abstract",
            "content": "The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current state-of-the-art models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning (RL) framework for CUDA optimization that employs novel contrastive RL algorithm. CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of 3.12 with median speedup of 1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching 120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of 3.12 (median 1.31) on L40, 2.50 (median 1.18) on RTX 3090, 2.39 (median 1.32) on H100, and 2.37 (median 1.34) on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: (1) It discovers variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; (2) It uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations; (3) It identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance. The capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. The trained RL model can successfully identify CUDA optimization patterns, discovers new techniques, synthesizes them to achieve speedups, and more importantly, extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking. diag(A) * Reference Code 1 3 4 5 6 7 class Model(nn.Module): def forward(self, A, B): # A: (N,) - 1D tensor of shape # B: (N, M) - 2D tensor of shape # torch.diag(A): (N, N) - creates diagonal matrix from # Result: (N, N) @ (N, M) = (N, M) return torch.diag(A) @ diag(A) * CUDA-L1 Implementation (64x faster) 2 3 class Model(nn.Module): def forward(self, A, B): return A.unsqueeze(1) * Figure 1(a) Average speedup across different architectures on KernelBench. Figure 1(b) case from KernelBench (Level 1, Task 12), computing diag(A) * B. We present reference code and CUDA-L1 implementation. The CUDA-L1 implementation reduces complexity from O(N 2M ) to O(NM ), achieving 64 speedup by replacing full matrix multiplication with elementwise operations. Figure 2: Overview of the CUDA-L1 training pipeline. The approach consists of three progressive stages: (1) Stage 1: Supervised Fine-tuning with Data Augmentation We augment the training dataset with CUDA code variants generated by LLMs and fine-tune the base model on executable and correct implementations to establish foundational CUDA knowledge. (2) Stage 2: Self-supervised Learning The model iteratively generates CUDA kernels, validates their correctness and executability, and trains on successfully validated examples, enabling autonomous improvement without human supervision. (3) Stage 3: Contrastive Reinforcement Learning We employ contrastive learning with execution-time rewards, training the model to distinguish between faster and slower CUDA implementations, ultimately optimizing for superior performance."
        },
        {
            "title": "Introduction",
            "content": "The exponential growth in demand for GPU computing resources, driven primarily by the rapid advancement and deployment of Large Language Models (LLMs), has created an urgent need for highly efficient CUDA optimization strategies. Traditionally, CUDA optimization has been highly manual and time-intensive process, where skilled engineers must meticulously analyze memory access patterns, experiment with different thread block configurations, and iteratively profile their code through extensive trial-and-error cycles. Recent advances in LLMs [25, 24, 26, 7, 32, 9, 11, 15, 19], especially those powered with RL [10, 8, 27, 17], have demonstrated remarkable capabilities in code generation and algorithm design. RL-powered LLMs hold significant potential to revolutionize the CUDA optimization process: CUDA optimization provides uniquely clear reward signalexecution speedwhich could be directly leveraged to automatically train reinforcement learning models. By treating performance improvements as rewards, RL-powered LLMs could iteratively generate, test, and refine CUDA optimizations without human intervention. This approach would not only automate the labor-intensive optimization process, potentially saving countless engineering hours, but also opens possibilities for discovering novel speedup algorithms that may surpass human-designed solutions. Unlike human engineers who are constrained by existing knowledge and conventional approaches, these systems could explore unconventional optimization combinations and potentially discover counterintuitive strategies that deliver significant performance improvements, offering new possibilities for advancing GPU computing efficiency. Despite the promise, current performance remains limited. State-of-the-art LLM models such as DeepSeek-R1 [8] and OpenAI-o1 [10] achieve low success rates in generating optimized CUDA code (only approximately 15% on KernelBench [20]), which is primarily due to the scarcity of CUDA code in training datasets. To address these limitations and unlock the potential of LLMs for automated CUDA optimization, in this work, we propose CUDA-L1, an LLM framework powered by contrastive reinforcement learning for CUDA optimization. CUDA-L1 is pipelined framework, the core of which is newly-designed contrastive RL framework. Different from previous RL models [31, 23, 22] , contrastive RL performs comparative analysis of previously generated CUDA variants alongside their execution performance, enabling the model to improving through distinguishing between effective and ineffective optimization strategies. Contrastive-RL simultaneously optimizes the foundation model through gradientbased parameter updates while fulfilling the maximum potential from the current model through contrastive analysis from high-performance CUDA variants, creating co-evolutionary dynamic that drives superior CUDA optimization performance. CUDA-L1 delivers significant improvements on the CUDA optimization task: trained on NVIDIA A100, it achieves an average speedup of 3.12 (median 1.42) across all 250 KernelBench CUDA kernels, with maximum speedups reaching 120. Furthermore, the CUDA codes optimized specifically for A100 demonstrate portability across GPU architectures, achieving average speedups of 3.12 (median 1.31) on L40, 2.50 (median 1.18) on RTX 3090, 2.39 (median 1.32) on H100, and 2.37 (median 1.34) on H20. In addition to benchmark results, CUDA-L1 demonstrates several remarkable properties: Automatic Discovery of Optimization Techniques: It automatically discovers comprehensive range of optimization techniques, including both CUDA-specific optimizationssuch as memory layout optimization, operation fusion, loop unrolling, and memory coalescingand mathematical optimizations like algebraic simplification, constant folding, and numerical approximation. While some of these techniques are already widely adopted in the optimization community, others remain underutilized. Optimal Combination Selection: Upon the discovery of these techniques, CUDA-L1 can identify the optimal combination of them to achieve maximum speedup for different CUDA tasks. Uncovering Fundamental Principles: CUDA-L1 is able to uncover fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain gatekeeper techniques must be applied first to unlock the effectiveness of others. Identifying Hidden Bottlenecks: It identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance. Beyond, CUDA-L1 reveals remarkable capability of RL in autonomous learning for CUDA optimization: 1. Even starting with foundation model with poor CUDA optimization ability, by using code speedups as RL rewards and proper contrastive RL training techniques, we can still train an RL system capable of generating CUDA optimization codes with significant speedups. 2. Without human prior knowledge, RL systems can independently discover CUDA optimization techniques, learn to combine them strategically, and more importantly, extend the acquired CUDA reasoning abilities to unseen kernels. This capability unlocks the potential for variety of automatic CUDA optimization tasks, e.g., kernel parameter tuning, memory access pattern optimization, and different hardware adaptations, offering substantial promises to enhance GPU utilization during this period of unprecedented computational demand. Despite its capabilities, it is important to acknowledge the potential challenges and pitfalls in training RL systems for tasks including but not limited to CUDA kernel development. We discovered that RL can be remarkably adept at exploiting loopholes in reward systems rather than solving the intended problem. For example, in our experiments, RL discovered vulnerability in the KernelBench evaluation: by creating additional CUDA streams in the generated code, it could manipulate the timing measurements. This exploitation resulted in reported 18x speedup that was entirely artificial with the actual computation performance unchanged. Such reward hacking behaviors are particularly concerning because they often require careful human inspection to detect. This paper identifies these failure modes, and proposes practical methods for more robust reward mechanisms and training procedures."
        },
        {
            "title": "2.1 Overview",
            "content": "Existing large language models [8, 32, 7] demonstrate significant limitations in generating executable and correct CUDA code with speedup, as reported in prior research [20]. This deficiency likely stems from the insufficient representation of CUDA code in the training datasets of these models. To address this fundamental gap, we introduce three-stage pipelined training strategy for CUDA-L1, aiming to progressively enhances the models CUDA programming capabilities: 1. Supervised fine-tuning via data augmentation, which aims to expand the models exposure to CUDA patterns and programming constructs, with the primary goal of producing correct and executable CUDA code. 2. Self-supervised learningm which focuses on enabling models to develop deeper understanding of CUDA semantics and programming principles, primarily aiming to achieve significant improvements in executability and correctness, while providing moderate speedup gains. 3. Contrastive reinforcement learning, which is designed to significantly optimize code execution speed, with the goal of generating high-performance CUDA implementations that deliver substantial speedup. Before we delve into the details of each stage, we provide key definitions adopted throughout the rest of this paper: 1. Executability: CUDA code is executable if it successfully compiles, launches, and executes to completion within 1000 the runtime of the reference implementation. Code exceeding this runtime threshold is considered unexecutable.1 2. Correctness: CUDA code is correct if it produces equivalent outputs to the reference implementation across 1000 random test inputs.2 3. Success: CUDA code is successful if it is both executable and correct."
        },
        {
            "title": "2.2 SFT via Data Augmentation",
            "content": "In the SFT stage, we collect dataset by using existing LLMs to generate CUDA code snippets and selecting successful one. This dataset is directly used to fine-tune the model. Throughout this paper, we use deepseek-v3-671B [15] as the model backbone. Data Collection To expand the models exposure to CUDA patterns, we begin with data augmentation based on reference code from 250 tasks in KernelBench, which provides the official implementations used in PyTorch. To generate executable and correct CUDA code efficiently, we leverage six existing LLM models: GPT-4o, OpenAI-o1, DeepSeek-R1, DeepSeek V3, Llama 3.1-405B Instruct, and Claude 3.7. For each model, we construct prompts using the one-shot strategy, where the prompt contains the reference code (denoted by qi, [1, 250]) and asks the LLM to generate an alternative speedup implementation. We employ multiple models to maximize the diversity of successful CUDA code generation. The detailed prompt structure is provided in Table 2. For each of the six models, we iterate through all 250 tasks. Each task allows up to 20 trials and terminates early if we successfully collect 2 trials that are both executable and correct. Notably, some tasks may fail to produce any successful code across all trials. The successful code is denoted by di,j, where {1, 2, . . . , ni}, and ni denotes the number of successful code snippets for the reference code qi. Through this process, we collected 2,105 successful CUDA code snippets. Now we have collected the dataset = {(qi, {di,j}ni j=1)}i. The collected dataset is used to finetune the fundation model. The instruction to the model is the same as the prompt for dataset generation, where the reference code qi is included in the instruction and the model is asked to generate an improved version. The model is trained to predict each token in di,j given the instruction."
        },
        {
            "title": "2.3 Self-supervised Learning",
            "content": "Now we are presented with the finetuned model after the SFT stage, where the model can potentially generate better CUDA code with higher success rates than the original model without finetuning. We wish to further improve the models ability to generate successful CUDA code by exposing it to more code snippets generated by itself. We achieve this iteratively by sampling CUDA code from the model, evaluating it for executability and correctness, removing the unsuccessful trials and keeping the successful ones. Successful ones are batched and used to update the model parameters. Using the updated model, we repeat the process: generating code, evaluating it, and retraining the model. The psudo code for the algorithm is shown in Table 2. The self-supervised learning strategy can be viewed as special case of the REINFORCE algorithm [31], typical policy gradient reinforcement learning method, where the reward is set to 1 for successful trials and 0 for unsuccessful trials, without applying any baseline. Interestingly, we find this adopted training strategy to be more stable than the REINFORCE variant with baseline applied. We conjecture that this stability arises because during the self-supervised learning stage, significant 1This threshold is reasonable since code with 1000 slower performance contradicts our speedup optimization goals. 2Prior work uses only 5 random inputs, which we found insufficient for robust validation. proportion of generated instances remain unsuccessful. This approach avoids the potential instability caused by applying negative updates to unsuccessful samples when using baseline. It is worth noting that during the self-supervised learning stage, we focus exclusively on the executability and correctness of the generated code, without considering speed as metric. This design choice reflects our primary objective of establishing reliable code generation before optimizing for performance. Data Augmentation Prompt Used in Supervised fine-tuning"
        },
        {
            "title": "Task for CUDA Optimization",
            "content": "You are an expert in CUDA programming and GPU kernel optimization. Now youre tasked with developing high-performance cuda implementation of Softmax. The implementation must: Produce identical results to the reference PyTorch implementation. Demonstrate speed improvements on GPU. Maintain stability for large input values. Reference Implementation (exact copy) 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 import torch import torch.nn as nn class Model(nn.Module): \"\"\" Simple model that performs Softmax activation. \"\"\" def __init__(self): super(Model, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Applies Softmax activation to the input tensor. Args: (torch.Tensor): Input tensor of shape (batch_size, num_features). Returns: torch.Tensor: Output tensor with Softmax applied, same shape as input. \"\"\" return torch.softmax(x, dim=1) batch_size = 16 dim = def get_inputs(): = torch.randn(batch_size, dim) return [x] def get_init_inputs(): return [] # No special initialization inputs needed Table 1: Prompt illustration for data augmentation in Section 2.2. For each KernelBench task (softmax shown here for illustration), the prompt is fed to each of six LLM modelsGPT-4o, OpenAI-o1, DeepSeek-R1, DeepSeek V3, Llama 3.1-405B Instruct, and Claude 3.7 Sonnetto generate alternative CUDA implementations."
        },
        {
            "title": "2.4 Contrastive Reinforcement Learning",
            "content": "Now we have model capable of generating successful CUDA code at reasonable success rate. Next, we aim to optimize for execution speed. One straightforward approach is to apply existing reinforcement learning algorithms such as REINFORCE [31], GRPO [23], or PPO [22]. In this approach, we would ask the model to first perform chain-of-thought reasoning [29], then generate code, Self-supervised Learning Algorithm 1: Initialize finetuned model M0 after SFT stage with parameters θsft 2: for = 1 to Niterations do 3: 4: 5: 6: Generate batch of CUDA codes Ci = {c1, ..., ck} using model Mi1 Evaluate each Ci for: 1. Executability (compiles and runs) 2. Correctness (produces expected output) 7: 8: 9: 10: 11: 12: Filter successful codes: success if success = then Compute gradient update θ using success Update model: θi θi1 + ηθ = {c Ciexecutable correct} else θi θi1 (no update) end if 13: 14: end for 15: return Final improved model MN Table 2: Self-supervised learning for cuda optimization in Stage 2. evaluate it, and use the evaluation score to update the model parameters. However, our experiments reveal that these methods perform poorly in this task. The issue is as follows: standard RL algorithms compute scalar reward for each generated CUDA code sample. During training, this reward undergoes algorithm-specific processing (e.g., baseline subtraction in REINFORCE, advantage normalization in GRPO, importance sampling in PPO). The processed reward then serves as loss weighting term for gradient updates, increasing the likelihood of high-reward sequences while decreasing the likelihood of low-reward sequences. Critically, in this paradigm, the reward signal is used exclusively for parameter updates and is never provided as input to the LLM. Consequently, the LLM cannot directly reason about performance trade-offs during code generation. To address this limitation, we propose incorporating reward information directly into the reasoning process by embedding performance feedback within the input prompt. Specifically, we present the model with multiple code variants alongside their corresponding speedup scores. Rather than simply generating code, the LLM is trained to first conduct comparative analysis of why certain implementations achieve superior performance, then synthesize improved solutions based on these insights. Each generated code sample undergoes evaluation to obtain performance score, which serves dual purposes in our training framework: 1. Immediate Parameter Updates: The score functions as reward signal for gradient-based parameter optimization, directly updating the model weights. 2. Future Prompt Construction: The scored code sample becomes part of the exemplar set for subsequent training iterations, enriching the contrastive learning dataset. This dual-utilization strategy enables iterative optimization across two complementary dimensions: 1. Foundation Model Enhancement: Parameter updates progressively improve the models fundamental understanding and capabilities for CUDA optimization tasks, expanding its representational capacity. 2. Fixed-Parameter Solution Optimization: The contrastive approach seeks to extract the maximum potential from the current models parameters by leveraging comparative analysis of high-quality exemplars. These two optimization processes operate synergistically: enhanced foundation models enable more accurate contrastive reasoning, while improved reasoning strategies provide higher-quality training signals for parameter updates of foundation models. This co-evolutionary dynamic drives convergence toward optimal performance. We term this approach contrastive reinforcement learning (contrastive-RL for short). It is worth noting that this co-evolving optimization paradigm can be found in many machine learning frameworks, including the EM algorithm [1], where the E-step optimizes latent variable assignments given fixed parameters while the M-step updates parameters given fixed assignments; Variational inference [3], which alternately optimizes the variational parameters to approximate the posterior distribution and updating model parameters to maximize the evidence lower bound; Actor-critic methods [12] in reinforcement learning similarly alternate between policy evaluation (critic update) and policy improvement (actor update)."
        },
        {
            "title": "2.4.1 Contrastive-RL’s Advantages over Evolutionary LLM Approaches",
            "content": "Contrastive-RL draws inspiration from broad range of literature, including evolutionary algorithms [2] and their applications to LLMs [16, 21, 18, 28], where multiple solution instances with associated fitness scores are presented to LLMs to analyze performance patterns and generate improved solutions. However, Contrastive-RL improves evolutionary LLM approaches in several critical aspects: Model Adaptation vs. Fixed-Model Reasoning: Contrastive-RL employs gradient-based parameter updates to continuously enhance model capabilities, whereas evolutionary LLM approaches rely exclusively on in-context learning with static parameters. This fundamental architectural difference endows Contrastive-RL with substantially greater representational capacity and task adaptability. Evolutionary LLM methods are fundamentally limited by the frozen foundation models initial knowledge and reasoning abilities, while Contrastive-RL progressively refines the models domain-specific expertise through iterative parameter optimization. From this perspective, evolutionary LLM approaches can be viewed as degenerate case of Contrastive-RL that implements only the Fixed-Parameter Solution Optimization component while omitting the Foundation Model Enhancement mechanism. This theoretical relationship explains why Contrastive-RL consistently outperforms evolutionary approaches: it leverages both optimization dimensions simultaneously rather than constraining itself to single fixed-capacity search space. Scalability and Generalization: Contrastive-RL demonstrates superior scalability by training single specialized model capable of handling diverse CUDA programming tasks and generating various types of optimized code. In contrast, evolutionary LLM approaches typically require separate optimization processes for each distinct task or domain, limiting their practical applicability and computational efficiency."
        },
        {
            "title": "2.4.2 Prompt Construction",
            "content": "Here we describe the construction of prompts provided to the LLM. The prompt provided to the LLM during Contrastive-RL training comprises the following structured components: Task Descrpition: detailed description of the computational problem, including input/output specifications, performance requirements, and optimization objectives. Previous Cuda Codes with Scores: Previously generated CUDA implementations paired with their corresponding performance scores (e.g., execution time, throughput, memory efficiency), providing concrete examples of varying solution quality. Generation Protocol: Explicit instructions defining the required output format and components. Requirements and Restrictions: Requirements and restrictions to prevent reward hacking in RL. The models response must contain the following three structured components: (3.1) Performance Analysis: comparative analysis identifying which previous kernel implementations achieved superior performance scores and the underlying algorithmic or implementation factors responsible for success. (3.2) Algorithm Design: high-level description of the proposed optimization strategy, outlining the key techniques to be applied, presented as numbered points in natural language. (3.3) Code Implementation: The complete CUDA kernel implementation incorporating optimizations. detailed demonstration for the prompt in shown in Table 3."
        },
        {
            "title": "2.4.3 Contrastive Exemplar Selection",
            "content": "The selection of code exemplars for prompt construction is critical, as core of Contrastive-RL is to perform meaningful comparative analysis. The selection strategy needs to addresses the following two key requirements: 1. Competitive Performance: The exemplar set should include higher-performing implementations to guide the model toward competitive CUDA codes, avoiding local minima that result from analyzing and comparing inferior codes. 2. Performance Diversity: The selected codes must exhibit substantial performance differences to enable effective contrastive analysis. We employ sampling strategy akin to that adopted by evolutionary LLM models: Let denote the number of code exemplars included in each prompt (set to = 2 in our experiments). During RL training, we maintain performance-indexed database of all successful code samples generated during RL training. Codes are organized into performance buckets Bk based on discretized score intervals, where bucket Bi contains codes with scores in range [sk, sk + s). We first sample distinct buckets according to temperature-scaled softmax distribution: (Bi) = exp (( si µs)/τ ) exp (( sj µs)/τ ) (cid:80) (1) where si denotes the aggregate score of bucket Bi, computed as the mean of its constituent code scores, µs = mean({ sj}M j=1) represents the global mean of all bucket scores, and τ is the temperature parameter governing the exploration-exploitation tradeoff. The sampling strategy in Equation 1 differs from conventional temperature sampling in evolutionary LLM approaches through modification: the deduction of µs stabilizes the distribution by centering scores around zero, which prevents absolute score magnitudes from dominating the selection. From each selected bucket Bi, we uniformly sample one representative code to construct the final prompt set. This approach satisfies both design criteria: Regarding competitive Performance, score-weighted bucket sampling biases selection toward higher-performing implementations, ensuring the exemplar set contains competitive solutions; Regarding performance Diversity, enforcing selection from distinct buckets ensures sufficient performance variance for effective contrastive analysis. more sophisticated alternative is to use an island-based approach for exemplar selection, as proposed in [reference]. However, we find no significant difference in performance between our bucket-based method and the island-based approach. Given this, we opt for the simpler bucket-based strategy."
        },
        {
            "title": "2.4.4 Reward",
            "content": "In this subsection, we detail the computation of the execution time-based reward function, which serves dual purposes: (1) guiding parameter updates in reinforcement learning and (2) constructing effective prompts. Given reference CUDA implementation qi from PyTorch with successful execution time tqi, and generated code candidate with execution time td, we define the single-run speedup score as: rsingle-run(d) = tqi td (2) Higher values indicate greater speedup relative to the reference implementation. Evaluations are performed on NVIDIA A100 PCIe. We observe significant variance in td measurements for identical implementations d, which introduces noise in reward estimation. This noise is particularly detrimental to RL training stability. To address these challenges, we implement the following robust measurement strategies: 1. Dedicated GPU Allocation: Each evaluation runs on an exclusively allocated GPU. Shared GPU usage leads to significantly higher variance in timing measurements, even when memory and compute utilization appear low. 2. Paired Execution with Order Randomization: For fair comparison, each evaluation round executes both the reference qi and candidate implementations. Crucially, we randomize the execution order within each round to account for GPU warm-up effects, where subsequent runs typically benefit from cache warming. 3. Extended Measurement Window: We conduct multiple evaluation rounds with predefined running time of 30 minutes per candidate. This adaptive approach yields between several tens of thousands to 1M rounds depending on individual kernel execution times. 4. Bucketized Variance Control: We partition all Scoresingle-run(d) measurements into 7 buckets and compute bucket-wise averages. Evaluations with inter-bucket variance exceeding 0.005 are discarded. 5. Robust Central Tendency: The final reward uses the median of bucket averages, which proves more stable than the mean against outlier effects: r(d) = median({Bucketk}7 k=1) (3) 6. Conservative Rounding: We apply conservative rounding to speedup ratios (i.e., Score(d) ), truncating to two decimal places while biasing toward unity (e.g., 1.118 1.11, 0.992 1.00). 7. Strict Verification Protocol: Despite these precautions, we still occasionally observe spurious large speedups due to GPU turbulence. For any candidate showing either: Absolute value of speedup > 3, or Speedup exceeding twice the previous maximum we perform verification on different GPU of the same type. The result is accepted only if the verification measurement differs by < 10% from the original. CUDA Optimization Task Prompt Used in Contrastive-RL"
        },
        {
            "title": "Task for CUDA Optimization",
            "content": "You are CUDA programming expert specializing in GPU kernel optimization. Given reference CUDA implementation, your objective is to create an accelerated version that maintains identical functionality. You will receive previous CUDA implementations accompanied by their performance metrics. Conduct comparative analysis of these implementations and use the insights to develop optimized and correct CUDA code that delivers superior performance."
        },
        {
            "title": "Reference Code",
            "content": "1 2 3 4 5 __global__ void kernel_v1(float* input, float* output, int N) { // Baseline implementation ... } }"
        },
        {
            "title": "Previous Cuda Implementations with Scores",
            "content": "1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 // code1 (score1) __global__ void kernel_v1(float* input, float* output, int N) { ... } // code2 (score2) __global__ void kernel_v2(float* input, float* output, int N) { ... } // code3 (score3) __global__ void kernel_v3(float* input, float* output, int N) { ... } // code4 (score4) __global__ void kernel_v3(float* input, float* output, int N) { ... }"
        },
        {
            "title": "Generation Protocol",
            "content": "You MUST use exactly two hash symbols (##) at the beginning of each section. ## Performance Analysis: Compare code snippets above and articulate on : 1. Which implementations demonstrate superior performance and why? 2. What particular optimization strategies exhibit the greatest potential for improvement? 3. What are the primary performance limitations in the implementation? 4. What CUDA-specific optimization techniques remain unexploited? 5. Where do the most significant acceleration opportunities exist? ## Algorithm Design: Describe your optimization approach ## Code Implementation: Provide your improved CUDA kernel"
        },
        {
            "title": "Requirements and Restrictions",
            "content": "## Critical Requirements: 1. Functionality must match the reference implementation exactly. Failure to do so will result in score of 0. 2. Code must compile and run properly on modern NVIDIA GPUs ## Key Restrictions: 1. Do not cache or reuse previous results the code must execute fully on each run. 2. Keep hyperparameters unchanged (e.g., batch size, dimensions, etc.) as specified in the reference. Table 3: Prompt structure for CUDA optimization task showing reference implementations and their performance scores used in Contrastive-RL."
        },
        {
            "title": "2.4.5 RL Training",
            "content": "For RL training, we adopt the Group Relative Policy Optimization (GRPO) strategy [23]. Specifically, for each reference prompt containing selected exemplars as shown in Table 3, we sample code outputs from the current policy πold, denoted as {d1, d2, . . . , dG}. Let = (r1, r2, . . . , rG) represent the reward scores associated with the generated code samples. Different from standard GRPO training, rewards are smoothed to mitigate the reward hacking issue; the details of this approach will be elaborated in Section 3. Further, as in GRPO, rewards are normalized within each group using: The complete GRPO objective optimizes the policy model by maximizing: ˆri = ri mean(r) std(r) LGRPO(θ) = qP (q),{di}G i=1πθold (dq)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 di di (cid:88) (cid:18) t= min (cid:18) πθ(di,tq, di,<t) πθold(di,tq, di,<t) ˆri, clip (cid:18) πθ(di,tq, di,<t) πθold (di,tq, di,<t) (cid:19) (cid:19) (cid:19)(cid:21) , 1 ε, 1 + ε ˆri βDKL[πθπref ] where: πθ is the policy model being optimized πθold is the old policy model from the previous iteration ε is the parameter for clipping β is the KL penalty coefficient that controls deviation from the reference policy DKL denotes the KL divergence between the current and reference policies (4) (5) We refer readers to [23] for details of GRPO. Model parameters are optimized using the GRPO objective, with contrastive prompts that incorporate comparative examples. This concludes our description of contrastive RL."
        },
        {
            "title": "3 Mitigating Reward Hacking in RL Training",
            "content": "Reinforcement learning is notorious for exhibiting reward hacking behaviors, where models exploit system vulnerabilities to achieve higher rewards while generating outputs that deviate from the intended objectives. particularly challenging aspect of these pitfalls is that they cannot be anticipated prior to training and are only discovered during the training process."
        },
        {
            "title": "3.1 Reward Hacking Cases",
            "content": "During our initial training procedure, we identified the following categories of reward hacking behaviours: Improper Timing Measurement. KernelBench measures execution time by recording timing events on the main CUDA stream: 1 2 3 4 start_event.record(original_model_stream) model(*inputs) end_event.record(original_model_stream) torch.cuda.synchronize(device=device) However, RL-generated code exploits this by creating additional CUDA streams that execute asynchronously. Since KernelBench only monitors the main stream, it fails to capture the actual execution time of operations running on parallel streams. This vulnerability is significant: in our initial implementation, we find that 82 out of 250 (32.8%) RL-generated implementations exploit this timing loophole to appear faster than they actually are, leading to an overall speedup of 18. To address this issue, prompt engineering alone is insufficient. The evaluation methodology should be modified to synchronize all CUDA streams before recording the end time, ensuring accurate performance measurement across all concurrent operations as follows: 1 2 3 start_event.record(custom_model_stream) custom_model(*inputs) # Wait for all model streams to complete before recording end event if custom_contain_new_streams: 5 6 7 8 for stream in custom_model_streams: custom_model_stream.wait_stream(stream) end_event.record(custom_model_stream) torch.cuda.synchronize(device=device) Hyperparameter Manipulation: In KernelBench, each computational task is associated with specific hyperparameters, including batch_size, dim, in_features dimension, out_features dimension, scaling_factor, and others. The RL agent learned to exploit these parameters by generating code that artificially reduces their values, thereby achieving superficial speedup improvements that do not reflect genuine optimization performance. Result Caching: The RL agent developed strategies to cache computational results across evaluation batches based on input addresses. When another inputs address matches cached one, it returns the cached output. In theory, this should not pass correctness validation because the cached output differs from the expected one. However, given that correctness validation checks whether the difference at each position between the reference output and custom code output is below certain threshold, there are few cases where it is able to squeeze past the correctness bar. The following code snippet gives an illustration: 1 2 3 4 cache_key = x.data_ptr() # Check if result is in cache if cache_key in self.cache: return self.cache[cache_key]"
        },
        {
            "title": "3.2 Towards Robust Reward Design and Training Procedures",
            "content": "To mitigate reward hacking, we implement the following strategies during training: reward checking model When there is significant leap in reward, an adversarial model intervenes to determine whether the code exploits the reward system. We use DeepSeek-R1 for this purpose and find that it successfully identifies reward hacking above over 60% of the time. Hacking-case database We maintain dynamic hacking-case database that is updated whenever new reward hacking behavior is detected. The reward checking model leverages this database for detection: given newly generated code snippet to examine, we retrieve the three most similar cases from the database and include them as context for the reward checking models input. Reward smoothing Sharp reward increases are smoothed to reduce their magnitude, preventing the RL agent from overprioritizing any single high-reward solution, whether legitimate or not: rnormalized = µ σ rsmooth = clip(rnormalized, k, k) (6) where µ and σ are the mean and the mean and standard deviation of the reward distribution, respectively. is hyperparameter that controls the clipping threshold set to 1.5, as we think as achieving 1.5 speedup over the official PyTorch implementation already represents significant optimization performance."
        },
        {
            "title": "4.1 KernelBench",
            "content": "Our evaluation is conducted on the KernelBench dataset [20] . The KernelBench Dataset contains collection of 250 PyTorch workloads designed to evaluate language models ability to generate efficient GPU kernels. The dataset is structured across three hierarchical levels based on computational complexity: Level 1 contains 100 tasks with single primitive operations (such as convolutions, matrix multiplications, activations, and normalizations), Level 2 includes 100 tasks with operator sequences that can benefit from fusion optimizations (combining multiple operations like convolution + ReLU + bias), and Level 3 comprises 50 full ML architectures sourced from popular repositories including PyTorch, Hugging Face Transformers, and PyTorch Image Models (featuring models like AlexNet and MiniGPT). Each task in the dataset provides reference PyTorch implementation with standardized input/output specifications, enabling automated evaluation of both functional correctness and performance through wall-clock timing comparisons. The dataset represents real-world engineering challenges where successful kernel"
        },
        {
            "title": "Method Mean Max",
            "content": "75% 50% 25% Success # out of total Speedup # out of total"
        },
        {
            "title": "All",
            "content": "Level 1 Level 2 Level 3 3.12 2.78 3.55 2.96 120 65.8 120 24.9 2. 1.75 2.05 2.60 1.42 1.28 1.39 1.94 1.17 1.12 1.20 1.42 249/ 99/100 100/100 50/50 240/250 94/100 98/100 48/50 Table 4: Performance of RL-CUDA1 on KernelBench. Success and Speedup show counts out of total benchmarks per level."
        },
        {
            "title": "Mean Max",
            "content": "75% 50% 25% Success # out of 250 Speedup # out of"
        },
        {
            "title": "Evolve",
            "content": "RL-Cuda1 Llama 3.1-405B DeepSeek-V3 DeepSeek-R1 OpenAI-O1 Llama 3.1-405B DeepSeek-V3 DeepSeek-R1 OpenAI-O1 Stage 1 Stage 1+2 Stage 1+2+GRPO 3 stages - random - island - bucket 0.23 0.34 0.88 0.73 1.18 1.32 1.41 1. 1.14 1.36 2.41 2.14 3.21 3.12 3.14 2.96 14.4 12.4 18.4 52.4 44.2 63.9 32.7 48.3 84.6 64.5 126 120 0.63 0.76 1.00 1.00 1.03 1.32 1.45 1. 1.00 1.41 1.83 1.62 2.21 2.25 0 0 0.75 0.55 1.00 1.03 1.17 1.16 1.00 1.09 1.33 1.21 1.40 1.42 0 0 0 0 1.00 1.00 1.00 1. 0.96 1.00 1.11 1.09 1.16 1.17 68 99 179 141 247 247 248 247 240 247 247 241 249 249 Table 5: Model performances on KernelBench All Level. 6 10 22 94 132 181 178 56 165 221 206 238 240 optimization directly translates to practical performance improvements. Throughout this paper, we use KernelBench as the evaluation benchmark. KernelBench is recognized as challenging benchmark in the community [20], with even the best current LLMs improving fewer than 20% of tasks."
        },
        {
            "title": "4.2 Evaluation Metric",
            "content": "For each task with reference implementation q, we evaluate the performance of generated CUDA code using similar protocol to training: We execute both and in randomized order within fixed time budget of 20 minutes per task. The number of execution rounds varies across tasks due to differences in individual runtimes. The final evaluation score for is computed as the average speedup ratio across all execution rounds within the allocated time window. Unsuccessful implementations receive score of zero. The metrics we report include speedup statistics (mean, maximum, and 75th, 50th, and 25th percentiles), success rate, and percentage of improvements."
        },
        {
            "title": "4.3 Main Results on KernelBench",
            "content": "The experimental results in Table 4 demonstrate RL-CUDA1s effectiveness across KernelBench tasks on A100, achieving an average 3.12 speedup with maximum gains reaching 120. Level 1 (single operations) achieves 2.78 mean speedup with 1.28 median gains; Level 2 (operator sequences) shows 3.55 mean speedup with similar 1.39 median gains. Level 3 (complex ML tasks) reveals comparable optimization abilities, with 2.96 mean and 1.94 median speedup. This demonstrates RL-CUDA1s consistent effectiveness across different complexity levels, suggesting strong potential for real-world LLM acceleration. The 99.6% overall success rate and 100% success rate on Levels 23 further validates the approachs robustness."
        },
        {
            "title": "4.4 Baseline Comparison",
            "content": "We compare the results with the following three groups of baselines: Vanilla Foundation Models: To establish baseline performance benchmarks, we evaluate OpenAI-o1, DeepSeek-R1, DeepSeekV3, and Llama 3.1-405B Instruct (denoted by OpenAI-o1-vanilla, DeepSeek-R1-vanilla, DeepSeek-V3-vanilla and Llama 3.1-405B-vanilla) by prompting each model to optimize the reference CUDA code. The generated CUDA code is directly used for evaluation without further modification. For each task, we repeat this process 5 times and report the best score. Evolutionary LLM: We implement evolutionary LLM strategies where, given set of previous codes, we sample up to 4 high-performing kernels based on evaluation scores. The key difference is that the model only performs contrastive analysis without updating model parameters. We adopt the island strategy for code database construction and sampling, as suggested in [18]. We conduct experiments on DeepSeek-R1, OpenAI-o1 and and Llama 3.1-405B, denoted as DeepSeek-R1-evolve, OpenAI-o1-evolve, DeepSeek-V3-evolve and Llama 3.1-405B-evolve. Different combinations of CUDA-L1 components and variants: stage1: Uses only the outcome from the first stage with supervised fine-tuning applied stage1+2: Applies only the first two stages without reinforcement learning stage1+2 + GRPO: Replaces the contrastive RL with vanilla GRPO strategy, without comparative analysis random sampling: Replaces the bucket sampling strategy with simple random sampling of exemplars island sampling: Adopts an island-based sampling strategy [18], where examples are distributed across different islands, prompts are constructed using exemplars from the same island, and newly generated examples are added to that island. After fixed number of iterations, examples in half of the inferior islands are eliminated and examples from superior islands are copied to replace them. Results are shown in Table 5. As observed, all vanilla foundation models perform poorly on this task. Even the top-performing modelsDeepSeek-R1 and OpenAI-o1achieve speedups over the reference kernels in fewer than 10% of tasks, while Llama 3.1-405B optimizes only 2.4% of tasks. This confirms that vanilla foundation models cannot be readily applied to CUDA optimization due to their insufficient grasp of CUDA programming principles and optimization techniques. We observe significant performance improvements introduced by the Evolutionary LLM models compared to vanilla foundation model setups, despite sharing the same parameter sets. All Evolve models achieve speedups in over 70% of tasks, with DeepSeek-R1 reaching 72.4% success rate. This demonstrates that leveraging contrastive analysis, which exploits the models general reasoning abilities, is more effective than direct output generation. The superiority of evolutionary LLM over vanilla LLM also provides evidence that contrastive RL should outperform non-contrastive RL approaches like vanilla GRPO, as the relationship between evolutionary and vanilla LLMs parallels that between contrastive and non-contrastive RL methods. When comparing the different combinations of CUDA-L1 components, we observe progressive increase in speedup rates from stage1 (SFT only) at 22.4% to stage1+2 (SFT + self-supervised) at 66%, and further to stage1+2+GRPO at 88.4%. This demonstrates the cumulative benefits of each training stage in improving model performance. When comparing different database construction and exemplar sampling strategies, both the bucket-sampling strategy (96% speedup rate) and the island-based strategy (95.2% speedup rate) achieve near-optimal performance, with both significantly outperforming the random sampling strategy (82.4% speedup rate). This aligns with our expectations, as competitive exemplars must be included in the prompt to guide the model toward generating more competitive solutions. All RL-based approaches significantly outperform evolutionary LLM baselines with fixed model parameters, with the best RL methods achieving over 95% speedup rates compared to 72.4% for the best evolutionary approach. This demonstrates the necessity of model parameter updating for achieving optimal performance in CUDA optimization tasks. While evolutionary approaches can leverage reasoning capabilities through contrastive analysis, fine-tuning the model parameters allows for deeper adaptation to the specific domain knowledge and optimization patterns required for effective CUDA kernel generation. The performance gap suggests that domain-specific parameter adaptation is crucial for bridging the gap between general reasoning abilities and specialized code optimization expertise."
        },
        {
            "title": "4.5 Generalization of A100-Optimized Kernels to Other GPU Architectures",
            "content": "Even without being specifically tailored to other GPU architectures, we observe performance improvements across all tested GPU types, with mean speedups ranging from 2.37 to 3.12. While A100 PCIe and L40 achieve the highest mean speedups (both 3.12), L40 demonstrates the highest maximum speedup (182) among all GPUs. The consumer RTX 3090 shows mean speedup of 2.50, while H100 and H20 achieve mean speedups of 2.39 and 2.37 respectively. Notably, A100 maintains the highest 75th percentile (2.25), 50th percentile (1.42), and 25th percentile (1.17) values, indicating more consistent optimization performance on the target architecture. These results demonstrate that while A100-optimized kernels transfer to other GPUs with varying degrees of effectiveness, the optimizations are most consistently effective on the target"
        },
        {
            "title": "GPU Device Mean Max",
            "content": "75% 50% 25% Success Speedup # out of # out of 250 A100 PCIe H100 L40 RTX 3090 3.12 2.39 3.12 2.50 2.37 81.9 182 114 63.7 2.25 1. 1.89 1.57 1.81 1.42 1.32 1. 1.18 1.34 1.17 1.09 1.08 1. 1.11 249 250 248 242 240 227 228 213 233 Table 6: CUDA-L1 overall performance on KernelBench across different GPU devices. A100 architecture, suggesting that architecture-specific optimizations would be beneficial for achieving optimal performance on each GPU type. We plan to release kernels specifically trained for different GPU types in an updated version of CUDA-L1."
        },
        {
            "title": "4.6 Discovered Cuda Optimization Techniques",
            "content": "An analysis of optimization strategies commonly employed in enhanced CUDA implementations reveals interesting patterns. Through GPT-4o-based technical term extraction and frequency analysis, we identified the ten most prevalent optimization techniques: 1. Memory Layout Optimization, which ensures data is stored in contiguous memory blocks; 2. Memory Access Optimization, which arranges data access patterns to maximize memory bandwidth and minimize latency through techniques like shared memory usage, coalesced global memory access, and memory padding; 3. Operation Fusion, which combines multiple sequential operations into single optimized kernel execution; 4. Memory Format Optimization, which aligns data layout with hardware memory access patterns; 5. Memory Coalescing, which optimizes CUDA kernel performance by ensuring threads in the same warp access contiguous memory locations; 6. Warp-Level Optimization, which leverages the parallel execution of threads within warp (typically 32 threads) to efficiently perform collective operations; 7. Optimized Thread Block Configuration, which carefully selects grid and block dimensions for CUDA kernels to maximize parallel execution efficiency and memory access patterns; 8. Shared Memory Usage, enables fast data access by storing frequently used data in cache accessible by all threads within thread block; 9. Register Optimization, which stores frequently accessed data in fast register memory to reduce latency and improve computational throughput; 10. Stream Management, which enables parallel execution of operations for improved GPU utilization. Tables 11, 13 and 14 present detailed CUDA optimization techniques with accompanying code examples."
        },
        {
            "title": "5 Case Studies",
            "content": "Table 7 presents the KernelBench tasks that achieved the highest speedups. We examine these some of them in detail and perform an ablation study of the applied CUDA optimization techniques, showing how much each technique contributes to the final speedup. 5.1 diag(A) * B: 64 faster We first examine the code for level 1, task 12, which performs matrix multiplication between diagonal matrix (represented by its diagonal elements) and dense matrix, both with dimension N=4096. The reference code is as follows where __init__ function of the class is omitted: 1 2 3 4 5 class Model(nn.Module): def forward(self, A, B): # A: (N,) - 1D tensor of shape # B: (N, M) - 2D tensor of shape # torch.diag(A): (N, N) - creates diagonal matrix from A"
        },
        {
            "title": "Task Name",
            "content": "2 1 2 1 3 2 1 3 3 83 80 9 31 96 66 43 44 83_Conv3d_GroupNorm_Min_Clamp_Dropout 12_Matmul_with_diagonal_matrices 80_Gemm_Max_Subtract_GELU 9_Tall_skinny_matrix_multiplication 31_VisionAttention 96_ConvTranspose3d_Multiply_Max_GlobalAvgPool_Clamp 66_Matmul_Dropout_Mean_Softmax 13_Matmul_for_symmetric_matrices 43_MinGPTCausalAttention 44_MiniGPTBlock Table 7: KernelBench Tasks Ranked by RL-CUDA1 Acceleration (Top 10)"
        },
        {
            "title": "Speedup",
            "content": "120.3 64.4 31.3 24.9 24.8 16. 14.5 14.4 13.1 10.5 6 7 # Result: (N, N) @ (N, M) = (N, M) return torch.diag(A) @ Lets see the optimized code by CUDA-l1: 1 2 3 class Model(nn.Module): def forward(self, A, B): return A.unsqueeze(1) * The optimized implementation leverages PyTorchs broadcasting mechanism to perform diagonal matrix multiplication efficiently. It first reshapes the diagonal vector from shape (N, ) to (N, 1) using unsqueeze(1), transforming it into column vector. Next, it utilizes PyTorchs automatic broadcasting to multiply each row of matrix by the corresponding element of A, where the (N, 1) shaped tensor is implicitly expanded to match the (N, ) dimensions of B. This approach completely avoids creating the full diagonal matrix, which would be sparse and memory-intensive. The key benefits of this technique are substantial: it requires only O(1) extra memory instead of O(N 2) for storing the diagonal matrix, reduces computational complexity from O(N 2M ) operations for full matrix multiplication to just O(N ) element-wise operations, leading to 64 speedup. What makes this particularly valuable is that RL can systematically explore the vast space of equivalent implementations. By exploring semantically equivalent implementations, RL learns to identify patterns where computationally expensive operations can be replaced with more efficient alternatives. The power of RL extends beyond simple algebraic simplifications and it can uncover sophisticated optimizations such as: replacing nested loops with vectorized operations identifying hidden parallelization opportunities discovering memory-efficient mathematical reformulations finding non-obvious algorithmic transformations that preserve correctness while improving performance What makes this particularly valuable is that RL can systematically explore the vast space of equivalent implementationssomething that would be impractical for human engineers to do manually."
        },
        {
            "title": "5.2 LSMT: 3.4× faster",
            "content": "Now lets look at classical neural network algorithm LSTM (level 3, task 35), on which CUDA-l1 achieves speedup of 3.4. By comparing the reference PyTorch implementation with the optimized output, we identified the following optimization techniques: 1. CUDA Graphs, which captures the entire LSTM computation sequence (including all layer operations) into replayable graph structure, eliminating kernel launch overhead by recording operations once and replaying them with minimal CPU involvement for subsequent executions. 2. Memory Contiguity, which ensures all tensors maintain contiguous memory layouts through explicit .contiguous() calls before operations, optimizing memory access patterns and improving cache utilization for CUDA kernels processing sequential data. Table 8: Speedup achieved by different CUDA optimization techniques on LSTMs. 3. Static Tensor Reuse, which pre-allocates input and output tensors during graph initialization and reuses them across forward passes with non-blocking copy operations, eliminating memory allocation overhead and enabling asynchronous data transfers. Table 8 represents the results for 8 different optimization combinations across the three optimization techniques above. As can be seen, CUDA Graphs is essential for achieving any meaningful speedup in this LSTM model. All configurations with CUDA Graphs achieve 2.77x-3.42x speedup, while all configurations without it achieve only 1.0x (no speedup). The combination of all three techniques provides the best performance at 3.42x, demonstrating that while CUDA Graphs provides the majority of the benefit ( 81% of total speedup), the additional optimizations contribute meaningful improvements when combined together. 5.3 3D transposed convolution: 120 faster We examined the code for Level 2, Task 38, which implements sequence of 3D operations: transposed convolution, average pooling, clamping, softmax, and element-wise multiplication. By comparing the reference PyTorch implementation with the CUDA-L1 optimized output, we identified the following optimization techniques applied by CUDA-L1: 1. Mathematical Short-Circuit, which detects when min_value equals 0.0 and skips the entire computation pipeline (convolution, normalization, min/clamp operations), directly returning zero tensors since the mathematical result is predetermined. 2. Pre-allocated Tensors, which creates zero tensors of standard shapes during initialization and registers them as buffers, eliminating memory allocation overhead during forward passes for common input dimensions. 3. Direct Shape Matching, which provides fast path for standard input shapes by immediately returning pre-allocated tensors without any shape calculations, bypassing the computational overhead entirely. 4. Pre-computed Parameters, which extracts and stores convolution parameters (kernel size, stride, padding, dilation) during initialization, avoiding repeated attribute lookups and tuple conversions during runtime. Table ?? represents the results for 16 different optimization combinations across the four optimization techniques above. As can be seen, mathematical short-circuit is essential for this task, where all configurations with mathematical short-circuit achieve 28.6x+ speedup, while all configurations without it achieve only 1.0x (no). The fact that CUDA-L1 identified this precise optimization strategy demonstrates the power of reinforcement learning in navigating complex optimization spaces. While human developer might intuitively focus on computational optimizations (like parallel algorithms) or memory layout improvements (like tensor pre-allocation), RL discovered that the mathematical properties of the operation completely dominate performance. This discovery is particularly impressive because: RL is able to find this non-obvious solution: The 120x speedup from exploiting the mathematical short-circuit is counterintuitive as most developers would expect to optimize the convolution kernel or memory access patterns for such compute-heavy operation, This shows how RL can discover optimal solutions that challenge conventional wisdom in deep learning optimization. Where human intuition might suggest \"optimize the convolution algorithm first,\" CUDA-L1 learned through empirical evidence that \"recognize when computation can be entirely skipped\" yields dramatically better results. The agents ability to identify that min(x, 0) followed by clamp(0, 1) always produces zeros demonstrates how RL can uncover mathematical invariants that humans might overlook in complex computational pipelines. Table 9: Speedup achieved by different CUDA optimization techniques on the Conv3d task."
        },
        {
            "title": "6.1 RL-augmented LLMs for Code Optimization",
            "content": "Starting this year, there has been growing interest in using LLM or RL-augmented LLM models for code optimization, including recent work on compiler optimization [5] and assembly code optimization [28], which use speed and correctness as RL training rewards. Other more distant related is software optimization that scale RL-based LLM reasoning for software engineering [30]. Regarding CUDA optimization, the only work that comprehensively delves into KernelBench is from [13], which uses meta-generation procedure that successfully optimizes 186 tasks out of 250 tasks in KernelBench with medium speedup of 34%. Other works remain in preliminary stages, including [4], which has optimized 20 GPU kernels selected from three different sources: the official NVIDIA CUDA Samples, LeetGPU, and KernelBench using proposed feature search and reinforcement strategy; and an ongoing tech report [22] that optimizes 4 kernels."
        },
        {
            "title": "6.2 Evolutionary LLMs",
            "content": "Evolutionary large language models [33, 16, 21, 18, 28, 6, 14] represent paradigm shift in automated algorithm discovery, exemplified by systems such as Google DeepMinds AlphaEvolve [18] and FunSearch [21]. These systems harness LLMs and operate through an iterative evolutionary process: 1. program sampler samples the high-score programs from previous generations to construct the prompt. Programs are usually sampled based on scores to promote diversity. 2. An LLM that generates new algorithmic variants based on the generated prompt. 3. An automated evaluator tests and scores the generated programs for correctness and performance. 4. An evolutionary database stores successful candidates and selects the most promising ones for future iterations. In more sophisticated setup called island-based evolution, candidates from low-performing islands are wiped out from the database after finite number of iterations, while candidates from high-performing islands are migrated to repopulate the wiped islands. Evolutionary LLMs operates iteratively and progressively improves algorithm performance This methodology has achieved breakthroughs including new matrix multiplication algorithms surpassing Strassens 1969 approach and practical optimizations for Googles data centers, demonstrating the systems ability to evolve."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose CUDA-L1, pipelined system for CUDA optimization powered by contrastive RL. CUDA-L1 achieves significant performance improvements on the CUDA optimization task, delivering an average speedup of 3.12 (median 1.42) across all 250 CUDA kernels of KernelBench, with peak speedups reaching 120 on A100. CUDA-L1 can independently discover CUDA optimization techniques, learn to combine them strategically, and more importantly, extend the acquired CUDA reasoning abilities to unseen kernels with meaningful speedups. We hope that CUDA-L1 would open new doors for automated optimization of CUDA, and substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.RetryClaude can make mistakes. Please double-check responses. Case Study: Code Snippets Before and After Optimizations Tech + Desc Before optimization After optimization Memory Layout Optimization Memory Layout Optimization ensures data is stored in contiguous memory blocks to maximize cache efficiency and reduce memory access latency during GPU computations. Memory Coalescing coalescing optiMemory mizes GPU memory access by ensuring threads in warp contiguous memory access reducing memory locations, increasing transactions bandwidth utilization. and Warp-Level Optimizations Optimizations Warp-Level leverage the CUDA execution model where threads execute in groups of 32 (warps) to improve parallel through efficiency collaborative operations and memory access patterns. - Non-contiguous memory access 1 Python 2 def matrix_multiply(A, B): - Ensuring contiguous memory layout 1 Python 2 def matrix_multiply_optimized(A, B): # and might not be contiguous in memory = torch.mm(A, B) return 3 4 5 6 3 4 6 # Ensure contiguous memory layout for efficient access patterns = A.contiguous() if not A.is_contiguous() else = B.contiguous() if not B.is_contiguous() else = torch.mm(A, B) return 7 8 - Uncoalesced memory access - Coalesced memory access with loop unrolling 1 cuda 2 __global__ void uncoalesced_kernel(float* input, 1 cuda 2 __global__ void coalesced_kernel(float* input, 3 4 5 6 8 float* output) { int tid = threadIdx.x; int stride = blockDim.x; // Each thread accesses non-contiguous memory locations for (int = 0; < 1024; i++) { output[tid + * stride] = input[tid + * stride] * 2.0f; } 9 10 } 11 float* output) { int tid = threadIdx.x; int batch_idx = blockIdx.x; // Base pointers for this batch item const float* batch_input = input + batch_idx * 1024; float* batch_output = output + batch_idx * 1024; // Each thread processes contiguous memory in chunks #pragma unroll 4 for (int = 0; < 1024; += 16) { batch_output[i] = batch_input[i] * 2.0f; batch_output[i+1] = batch_input[i+1] * 2.0f; batch_output[i+2] = batch_input[i+2] * 2.0f; // ... more contiguous accesses batch_output[i+15] = batch_input[i+15] * 2.0f; 3 4 5 6 8 9 10 11 12 14 15 16 17 } 18 19 } 20 - Each thread independently calculates min value - Using warp-level operations for parallel reduction 1 cuda 2 __global__ void min_kernel_before(const float* input, float* output, int size) { 3 5 6 7 8 9 int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx < size) { float min_val = 1e10f; for (int = 0; < DEPTH; i++) { min_val = min(min_val, input[idx + * size]); } output[idx] = min_val; } 10 11 } 12 1 cuda 2 3 __global__ void min_kernel_after(const float* input, float* output, int size) { 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 int idx = blockIdx.x * blockDim.x + threadIdx.x; int lane_id = threadIdx.x % 32; // Threads position within warp int warp_id = threadIdx.x / 32; // Warp number within the block float min_val = 1e10f; if (idx < size) { // Each thread finds its local minimum for (int = 0; < DEPTH; i++) { min_val = min(min_val, input[idx + * size]); } // Warp-level parallel reduction using shuffle for (int offset = 16; offset > 0; offset /= 2) { float other = __shfl_down_sync(0xffffffff, min_val, offset); min_val = min(min_val, other); } // First thread in warp writes the result if (lane_id == 0) { output[blockIdx.x * (blockDim.x/32) + warp_id] = min_val; } } 25 26 } 27 Table 10: (Part 1) Code snippets before and after optimizations. Tech + Desc Before optimization After optimization Memory Hierarchy Optimization Memory Hierarchy Optimization involves strategically utilizing different levels of GPU memory (registers, shared memory, constant memory) to minimize global memory access latency and maximize data reuse. Asynchronous Execution ExecuAsynchronous tion in CUDA allows operations to be queued and executed concurrently on separate streams, enabling overlapping computation with memory transfers for improved GPU utilization. - Using global memory directly - Using memory hierarchy (shared, constant, registers) 1 cuda 2 __global__ void 1 cuda 2 __constant__ float c_depthwise_weight[3*3*3]; // depthwise_separable_conv_kernel_unoptimized( Constant memory for weights 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 const float* input, const float* depthwise_weight, const float* pointwise_weight, float* output, /* other parameters */) { int out_y = blockIdx.y * blockDim.y + threadIdx.y; int out_x = blockIdx.x * blockDim.x + threadIdx.x; // Each thread directly accesses global memory for each computation for (int oc = 0; oc < out_channels; oc++) { float result = 0.0f; for (int ic = 0; ic < in_channels; ic++) { float depthwise_result = 0.0f; // Direct global memory access for each kernel element for (int ky = 0; ky < 3; ky++) { for (int kx = 0; kx < 3; kx++) { int in_y = out_y * stride + ky - padding; int in_x = out_x * stride + kx - padding; if (in_y >= 0 && in_y < in_height && in_x >= 0 && in_x < in_width) { depthwise_result += input[((batch_idx * in_channels + ic) * in_height + in_y) * in_width + in_x] * depthwise_weight[ic * 9 + ky * 3 + kx]; } } } result += depthwise_result * pointwise_weight[oc * in_channels + ic]; } output[((batch_idx * out_channels + oc) * out_height + out_y) * out_width + out_x] = result; } 28 29 } 30 - Sequential execution 1 Python 2 def forward(self, x): # Operations execute in the default stream, blocking sequentially result = self.conv_transpose3d(x) return result 3 4 5 6 3 __constant__ float c_pointwise_weight[3*64]; 4 5 __global__ void depthwise_separable_conv_kernel_optimized( const float* input, float* output, /* other parameters */) { // Shared memory for input tile with padding __shared__ float shared_input[3][SHARED_MEM_HEIGHT][SHARED_MEM_STRIDE]; // Collaborative loading of input data to shared memory // [shared memory loading code...] __syncthreads(); // Register caching for intermediate results float depthwise_results[3]; // Store in registers // Compute using shared memory and constant memory for (int = 0; < in_channels; ++c) { float sum = 0.0f; // Fully unrolled convolution using shared memory sum += shared_input[c][sm_y_base][sm_x_base] * c_depthwise_weight[c*9 + 0]; // [more unrolled operations...] depthwise_results[c] = sum; // Store in register } // Cache output values in registers float output_cache[32]; // Compute pointwise convolution using registers and constant memory for (int = 0; < oc_limit; ++i) { output_cache[i] = depthwise_results[0] * c_pointwise_weight[i * 3 + 0] + depthwise_results[1] * c_pointwise_weight[i * 3 + 1] + depthwise_results[2] * c_pointwise_weight[i * 3 + 2]; } // Coalesced write to global memory for (int = 0; < oc_limit; ++i) { output[output_idx] = output_cache[i]; 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 } 40 41 } 42 - Asynchronous execution with custom stream 1 Python 2 def forward(self, x): # Create dedicated compute stream self.compute_stream = torch.cuda.Stream(priority=-1) # High priority stream # Execute operations asynchronously in the custom stream with torch.cuda.stream(self.compute_stream): result = self._optimized_cuda_forward(x, x.dtype) 3 4 5 6 8 9 10 # Control returns immediately while computation continues in background return result 11 12 Table 11: (Part 2) Code snippets before and after optimizations. Tech + Desc Before optimization After optimization Memory Access Optimization Memory Access Optimization in CUDA improves performance by organizing data access patterns to maximize cache utilization and minimize memory latency through techniques like tiling, coalescing, and shared memory usage. Operation Fusion Operation Fusion combines multiple consecutive operations into single optimized kernel to reduce memory transfers and improve computational efficiency on CUDA devices. - Naive matrix multiplication with poor memory access - Using shared memory tiling and register blocking 1 cuda 2 // Before optimization - Naive matrix multiplication with poor memory access 3 __global__ void matmul_naive(float* A, float* B, float* C, int M, int N, int K) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row < && col < N) { float sum = 0.0f; for (int = 0; < K; ++k) { sum += A[row * + k] * B[col * + k]; } C[row * + col] = sum; 4 5 6 7 9 10 11 12 } 13 14 } 15 - Separate operations 1 Python 2 def forward(self, x): = F.max_pool3d(x, kernel_size=self.pool_kernel_size, stride=self.pool_stride) = torch.softmax(x, dim=1) = - self.subtract.view(1, -1, 1, 1, 1) = * torch.sigmoid(x) return torch.max(x, dim=1)[0] 4 5 6 7 8 1 cuda 2 __global__ void matmul_optimized(float* A, float* B, float* C, int M, int N, int K) { 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 // Block index and thread index const int bx = blockIdx.x; const int by = blockIdx.y; const int tx = threadIdx.x; const int ty = threadIdx.y; // Output positions const int row = by * 8 + ty; const int col = bx * 32 + tx; // Register accumulation float sum00 = 0.0f, sum01 = 0.0f; float sum10 = 0.0f, sum11 = 0.0f; // Shared memory tiles with padding to avoid bank conflicts __shared__ float As[8][33]; __shared__ float Bs[32][33]; // Loop over tiles for (int tile = 0; tile < (K + 31) / 32; ++tile) { // Collaborative loading of tiles into shared memory if (row < && tile * 32 + tx < K) As[ty][tx] = A[row * + tile * 32 + tx]; else As[ty][tx] = 0.0f; if (col < && tile * 32 + ty < K) Bs[ty][tx] = B[col * + tile * 32 + ty]; else Bs[ty][tx] = 0.0f; __syncthreads(); // Compute partial dot products using shared memory #pragma unroll 8 for (int = 0; < 32; ++k) { float a0 = As[ty][k]; float a1 = As[ty + 4][k]; float b0 = Bs[k][tx]; float b1 = Bs[k][tx + 16]; sum00 += a0 * b0; sum01 += a0 * b1; sum10 += a1 * b0; sum11 += a1 * b1; } __syncthreads(); } // Write results to global memory if (row < && col < N) C[row * + col] = sum00; if (row < && col + 16 < N) C[row * + col + 16] = sum01; if (row + 4 < && col < N) C[(row + 4) * + col] = sum10; if (row + 4 < && col + 16 < N) C[(row + 4) * + col + 16] = sum11; 58 } 59 - Fused operations with JIT 1 Python 2 @torch.jit.script 3 def fused_post_process(x, subtract_view): 4 5 6 7 = torch.softmax(x, dim=1) = - subtract_view = * torch.sigmoid(x) return torch.max(x, dim=1)[0] 8 9 def forward(self, x): 10 11 = F.max_pool3d(x, kernel_size=self.pool_kernel_size, stride=self.pool_stride) return self.fused_post_process(x, self.subtract.view(1, -1, 1, 1, 1)) 12 Table 12: (Part 3) Code snippets before and after optimizations. Tech + Desc Before optimization After optimization Optimized Thread Block Configuration Thread Optimized Block Configuration involves carefully selecting grid and block dimensions for CUDA kernels to maximize parallelism, memory access efficiency, and computational throughput based on the hardware architecture and algorithm characteristics. Branchless Implementation - Basic thread block configuration - Carefully tuned thread block configuration 1 Python 2 block_dim = (16, 16) # Simple square thread block 3 grid_dim = (math.ceil(N / 16), math.ceil(M / 16)) 4 5 kernel(grid=grid_dim, block=block_dim, args=[A.data_ptr(), B.data_ptr(), C.data_ptr(), M, N, K]) 6 1 Python 2 block_dim = (32, 8) # Rectangular block optimized for matrix multiplication 3 grid_dim = (math.ceil(N / 32), math.ceil(M / 8)) 4 5 kernel(grid=grid_dim, block=block_dim, args=[A.data_ptr(), B.data_ptr(), C.data_ptr(), M, N, K]) 6 - With branches - Branchless Branchless implementation replaces conditional statements with mathematical operations to avoid branch divergence and improve GPU performance. 1 cuda 2 if (val > 1.0f) { output = 1.0f; 3 4 } else if (val < -1.0f) { output = -1.0f; 5 6 } else { output = val; 7 8 } 9 1 cuda 2 output = fmaxf(-1.0f, fminf(1.0f, val)); 3 Shared Memory Usage threads within Shared memory in CUDA allows the same block to efficiently share data, reducing global memory accesses and improving performance for algorithms with data reuse patterns. Minimal Synchronization Minimal Synchronization reduces overhead by minimizing the number of synchronization points between CPU and GPU operations, asynchronous execution through dedicated CUDA streams. allowing - Each thread reads diagonal element from global memory - Using shared memory to cache diagonal elements 1 cuda 2 __global__ void diag_matmul_kernel_unoptimized(const float* A, const float* B, float* C, int N, int M) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row < && col < M) { // Each thread loads the same diagonal element multiple times from global memory C[row * + col] = A[row] * B[row * + col]; 3 4 6 7 8 } 9 10 } 11 - Default synchronization behavior 1 Python 2 def forward(self, x): # Each CUDA operation implicitly synchronizes = x.contiguous() result = self.conv_transpose3d(x) return result 3 4 5 6 7 1 cuda 2 __global__ void diag_matmul_kernel_optimized(const float* A, const float* B, float* C, int N, int M) { 3 4 5 6 8 9 10 11 12 14 15 16 17 18 const int BLOCK_SIZE_Y = 8; __shared__ float A_shared[BLOCK_SIZE_Y]; // Shared memory for diagonal elements int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; // Load diagonal elements into shared memory (once per row in block) if (threadIdx.x == 0 && row < N) { A_shared[threadIdx.y] = A[row]; } __syncthreads(); // Ensure all threads see the loaded values if (row < && col < M) { // Use cached diagonal element from shared memory C[row * + col] = A_shared[threadIdx.y] * B[row * + col]; } 19 20 } 21 - Minimal Synchronization 1 Python 2 def forward(self, x): 3 5 6 7 8 # Create dedicated stream for computation with torch.cuda.stream(self.compute_stream): # Operations run asynchronously in this stream x_optimized = x.contiguous(memory_format=torch.channels_last_3d) result = self.conv_transpose3d(x_optimized) # Implicit synchronization only happens when result is used return result 9 10 Table 13: (Part 4) Code snippets before and after optimizations. Tech + Desc Before optimization After optimization Thread Coarsening - Each thread processes one feature element - Each thread processes two feature elements at once is Thread Coarsening an optimization technique where each thread processes multiple data elements instead of just one, arithmetic intensity and reducing thread overhead. increasing Asynchronous Execution Asynchronous Execution in CUDA allows operations to be queued and executed concurrently on separate streams, enabling overlapping computation with memory transfers for improved GPU utilization. 1 cuda 2 for (int = tx; < feature_size; += threads_x) { 3 4 scalar_t x_val = x[b * max_sample * feature_size + * feature_size + d]; atomicAdd(&vlad[k * feature_size_padded + d], assign_val * x_val); 5 } 6 - Sequential execution 1 Python 2 def forward(self, x): # Operations execute in the default stream, blocking sequentially result = self.conv_transpose3d(x) return result 3 4 5 6 1 cuda 2 #pragma unroll 4 3 for (int = tx; < feature_size - 1; += threads_x * 2) { scalar_t x_val1 = x[b * max_sample * feature_size + * feature_size + d]; scalar_t x_val2 = x[b * max_sample * feature_size + * feature_size + + threads_x]; atomicAdd(&vlad[k * feature_size_padded + d], assign_val * x_val1); atomicAdd(&vlad[k * feature_size_padded + + threads_x], assign_val * x_val2); 4 5 6 8 9 } 10 11 // Handle remaining elements 12 for (int = tx + (feature_size / threads_x) * threads_x * 2; < feature_size; += threads_x) { 13 14 scalar_t x_val = x[b * max_sample * feature_size + * feature_size + d]; atomicAdd(&vlad[k * feature_size_padded + d], assign_val * x_val); 15 } 16 - Asynchronous execution with custom stream 1 Python 2 def forward(self, x): # Create dedicated compute stream self.compute_stream = torch.cuda.Stream(priority=-1) # High priority stream # Execute operations asynchronously in the custom stream with torch.cuda.stream(self.compute_stream): result = self._optimized_cuda_forward(x, x.dtype) 3 4 5 6 8 9 10 # Control returns immediately while computation continues in background return result 11 12 Table 14: (Part 5) Code snippets before and after optimizations. Case Study: Comparing Reference Code and CUDA-L1 Optimized Neural"
        },
        {
            "title": "Network Implementations",
            "content": "B.1 LSTMs LSTM Reference Code - Simple baseline implementation Table 15: Reference code and CUDA-L1 generation for LSTM class 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 import torch import torch.nn as nn class Model(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0): \"\"\" Initialize the LSTM model. \"\"\" super(Model, self).__init__() # Initialize hidden state with random values self.h0 = torch.randn((num_layers, batch_size, hidden_size)) self.c0 = torch.randn((num_layers, batch_size, hidden_size)) self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): \"\"\" Forward pass through the LSTM model. \"\"\" self.h0 = self.h0.to(x.device) self.c0 = self.h0.to(x.device) # BUG: This should be self.c0.to(x.device) # Forward propagate LSTM out, state = self.lstm(x, (self.h0, self.c0)) # shape of out: (batch_size, seq_length, hidden_size) # Decode the hidden state of the last time step out = self.fc(out[:, -1, :]) # shape of out: (batch_size, output_size) return state[0] # Test code batch_size = 10 sequence_length = 512 input_size = 128 hidden_size = 256 num_layers = 6 output_size = 10 dropout = 0.0 def get_inputs(): return [torch.randn(batch_size, sequence_length, input_size)] def get_init_inputs(): return [input_size, hidden_size, num_layers, output_size, dropout] LSTM Fully Optimized Code - All optimizations enabled (3.4x faster) 1 2 3 4 6 7 8 9 10 import torch import torch.nn as nn import torch.cuda as cuda class ModelNew(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0): \"\"\" Initialize the LSTM model with three core optimization techniques. Color coding: 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 - - BLUE: CUDA Graphs optimization GREEN: Memory Contiguity optimization ORANGE: Static Tensor Reuse optimization - \"\"\" super(ModelNew, self).__init__() # Initialize hidden states as buffers self.register_buffer(h0, torch.randn((num_layers, batch_size, hidden_size))) self.register_buffer(c0, torch.randn((num_layers, batch_size, hidden_size))) # Use PyTorchs optimized LSTM implementation self.lstm = nn.LSTM( input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False ) self.fc = nn.Linear(hidden_size, output_size) CUDA GRAPHS: Variables for graph capture and replay # self.graph = None self.graph_ready = False self.input_shape = None STATIC TENSOR REUSE: Pre-allocated tensors for graph execution # self.static_input = None self.static_output = None CUDA GRAPHS: Streams for graph operations # self.graph_stream = None # Track if were running on CUDA self.is_cuda_available = torch.cuda.is_available() def _initialize_cuda_resources(self): \"\"\" if self.graph_stream is None: CUDA GRAPHS: Initialize CUDA stream for graph operations\"\"\" self.graph_stream = cuda.Stream() def _capture_graph(self, x, result): \"\"\" \"\"\" CUDA GRAPHS: Capture the computation graph for replay STATIC TENSOR REUSE: Create static tensors for graph capture STATIC TENSOR REUSE: Clone tensors for static allocation # self.static_input = x.clone() self.static_output = result.clone() CUDA GRAPHS: Capture the computation graph # with torch.cuda.stream(self.graph_stream): self.graph = cuda.CUDAGraph() with cuda.graph(self.graph): # Operations to capture in the graph static_out, _ = self.lstm(self.static_input, (self.h0, self.c0)) MEMORY CONTIGUITY: Ensure contiguous memory layout # static_last = static_out[:, -1, :].contiguous() self.static_output.copy_(self.fc(static_last)) # Wait for graph capture to complete torch.cuda.synchronize() 76 78 79 80 81 82 84 85 86 87 88 90 91 92 93 94 96 97 98 99 100 102 103 104 105 106 108 109 110 111 112 114 115 116 117 118 120 121 122 123 124 126 127 128 129 130 132 133 134 135 136 138 139 140 # Mark graph as ready for use self.graph_ready = True def _standard_forward(self, x): \"\"\"Standard forward pass with memory contiguity optimization\"\"\" MEMORY CONTIGUITY: Ensure input is contiguous # if not x.is_contiguous(): = x.contiguous() # Forward pass through LSTM out, _ = self.lstm(x, (self.h0, self.c0)) MEMORY CONTIGUITY: Make last output contiguous for optimal memory access # last_out = out[:, -1, :].contiguous() return self.fc(last_out) def forward(self, x): \"\"\" Forward pass through the LSTM model with three optimization techniques. Optimization flow: 1. 2. 3. \"\"\" CUDA GRAPHS: Check if we can use the captured graph (fast path) STATIC TENSOR REUSE: Use pre-allocated tensors for graph replay MEMORY CONTIGUITY: Ensure optimal memory layout throughout CUDA GRAPHS: Fast path - use captured graph if available # if (x.is_cuda and self.graph_ready and x.shape == self.input_shape): STATIC TENSOR REUSE: Copy to pre-allocated tensor with non-blocking transfer # self.static_input.copy_(x, non_blocking=True) CUDA GRAPHS: Replay the captured graph # self.graph.replay() # Return the output from static buffer return self.static_output.clone() # Standard execution path with torch.no_grad(): result = self._standard_forward(x) CUDA GRAPHS: Initialize graph on first CUDA input # if x.is_cuda and self.is_cuda_available and not self.graph_ready: try: # Store the current input shape self.input_shape = x.shape CUDA GRAPHS: Initialize CUDA resources # self._initialize_cuda_resources() CUDA GRAPHS + # self._capture_graph(x, result) STATIC TENSOR REUSE: Capture the graph except Exception as e: # If graph capture fails, continue without it self.graph_ready = False return result # Hyperparameters from the reference implementation batch_size = 10 sequence_length = 512 input_size = 128 hidden_size = 256 num_layers = 6 output_size = 10 dropout = 0.0 def get_inputs(): return [torch.randn(batch_size, sequence_length, input_size)] def get_init_inputs(): return [input_size, hidden_size, num_layers, output_size, dropout] # Example usage demonstrating the three techniques if __name__ == \"__main__\": import time print(\" BLUE: CUDA Graphs optimization\") print(\" GREEN: Memory Contiguity optimization\") print(\" print(\"=\" * 60) ORANGE: Static Tensor Reuse optimization\") # Create model model = ModelNew(*get_init_inputs()) model.eval() # Test input = get_inputs()[0] # Move to GPU if available if torch.cuda.is_available(): model = model.cuda() = x.cuda() print(\"Running on CUDA - all three optimizations active\") # First run - captures graph print(\"n with torch.no_grad(): output = model(x) First forward pass: Capturing CUDA graph...\") print(f\" print(f\" Output shape: {output.shape}\") Graph ready: {model.graph_ready}\") # Subsequent runs - uses captured graph print(\"n Subsequent passes: Using captured graph with\") print(\" static tensor reuse and memory contiguity\") # Warmup for _ in range(10): with torch.no_grad(): _ = model(x) # Measure performance torch.cuda.synchronize() start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) n_runs = 100 start_event.record() with torch.no_grad(): for _ in range(n_runs): output = model(x) end_event.record() torch.cuda.synchronize() avg_time = start_event.elapsed_time(end_event) / n_runs 141 142 143 144 146 147 148 149 150 152 153 154 155 156 158 159 160 161 162 164 165 166 167 168 170 171 172 173 174 176 177 178 179 180 182 183 184 185 186 188 189 190 191 192 194 195 196 197 198 200 201 202 203 204 206 207 208 209 210 212 213 214 215 216 print(f\"nPerformance: {avg_time:.3f} ms per forward pass\") print(f\" Expected speedup: 3.42x with all optimizations\") else: print(\"n print(\" Running on CPU - only memory contiguity active\") (CUDA graphs and static tensor reuse require GPU)\") with torch.no_grad(): output = model(x) print(f\"n Output shape: {output.shape}\") B.2 3DConv Conv3D Reference Code - Simple baseline implementation Table 16: Reference code and CUDA-L1 generation for Conv3D class 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 import torch import torch.nn as nn class Model(nn.Module): \"\"\" Model that performs 3D convolution, applies Group Normalization, minimum, clamp, and dropout. \"\"\" def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p): super(Model, self).__init__() self.conv = nn.Conv3d(in_channels, out_channels, kernel_size) self.norm = nn.GroupNorm(groups, out_channels) self.dropout = nn.Dropout(dropout_p) self.min_value = min_value self.max_value = max_value def forward(self, x): = self.conv(x) = self.norm(x) = torch.min(x, torch.tensor(self.min_value)) = torch.clamp(x, min=self.min_value, max=self.max_value) = self.dropout(x) return # Hyperparameters batch_size = 128 in_channels = 3 out_channels = 16 depth, height, width = 16, 32, 32 kernel_size = 3 groups = 8 min_value = 0.0 max_value = 1.0 dropout_p = 0. def get_inputs(): return [torch.randn(batch_size, in_channels, depth, height, width)] def get_init_inputs(): return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p] Conv3D Fully Optimized Code - All optimizations enabled (120x faster) 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 import torch import torch.nn as nn # Hyperparameters batch_size = 128 in_channels = 3 out_channels = 16 depth, height, width = 16, 32, 32 kernel_size = 3 groups = 8 min_value = 0.0 max_value = 1.0 dropout_p = 0.2 class ModelNew(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p): super(ModelNew, self).__init__() # Store the original layers for parameter compatibility self.conv = nn.Conv3d(in_channels, out_channels, kernel_size) self.norm = nn.GroupNorm(groups, out_channels) 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 65 66 67 69 70 71 72 73 75 76 77 78 79 81 self.dropout = nn.Dropout(dropout_p) self.min_value = min_value self.max_value = max_value self.dropout_p = dropout_p TECH 1: Mathematical Short-Circuit Optimization # # Detects when min_value=0.0 to skip entire computation self.use_optimized_path = (min_value == 0.0) TECH 4: Pre-computed Convolution Parameters # # Extract and store conv parameters once during initialization if isinstance(kernel_size, int): self.kernel_size = (kernel_size, kernel_size, kernel_size) else: self.kernel_size = kernel_size self.stride = self.conv.stride self.padding = self.conv.padding self.dilation = self.conv.dilation TECH 4: Pre-compute output dimensions for standard input # self.out_depth = ((depth + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0]) + 1 self.out_height = ((height + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[1]) + 1 self.out_width = ((width + 2 * self.padding[2] - self.dilation[2] * (self.kernel_size[2] - 1) - 1) // self.stride[2]) + 1 # Standard output shape for the default batch size self.standard_shape = (batch_size, out_channels, self.out_depth, self.out_height, self.out_width) TECH 2: Pre-allocated Zero Tensors # # Create zero tensors once to avoid allocation overhead if self.use_optimized_path: self.register_buffer(zero_output_float32, torch.zeros(self.standard_shape, dtype=torch.float32), persistent=False) self.register_buffer(zero_output_float16, self.register_buffer(zero_output_bfloat16, torch.zeros(self.standard_shape, dtype=torch.float16), persistent=False) torch.zeros(self.standard_shape, dtype=torch.bfloat16), persistent=False) def calculate_output_shape(self, input_shape): \"\"\"Calculate the output shape of the convolution operation.\"\"\" batch_size, _, d, h, = input_shape TECH 4: Use precomputed parameters # # Avoid repeated attribute lookups out_d = ((d + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0]) + 1 out_h = ((h + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[1]) + 1 out_w = ((w + 2 * self.padding[2] - self.dilation[2] * (self.kernel_size[2] - 1) - 1) // self.stride[2]) + 1 return (batch_size, self.conv.out_channels, out_d, out_h, out_w) def forward(self, x): TECH 1: Mathematical Short-Circuit - Main optimization # # Skip all computation when we know result will be zeros if not self.use_optimized_path: # Standard path for non-optimized cases = self.conv(x) = self.norm(x) = torch.minimum(x, torch.tensor(self.min_value, device=x.device)) = torch.clamp(x, min=self.min_value, max=self.max_value) 82 84 85 86 87 88 90 91 92 93 94 96 97 98 99 100 102 103 104 105 106 108 109 110 111 112 114 115 116 117 = self.dropout(x) return # Optimized path when min_value == 0.0 # Since min(x, 0) followed by clamp(0, 1) always produces zeros TECH 3: Direct Shape Matching # # Fast path for standard input dimensions if x.shape == (batch_size, in_channels, depth, height, width): TECH 2: Use pre-allocated tensors # # Return pre-allocated zeros matching input dtype if x.dtype == torch.float32: return self.zero_output_float32 elif x.dtype == torch.float16: return self.zero_output_float16 elif x.dtype == torch.bfloat16: return self.zero_output_bfloat16 else: # Fallback for other dtypes return torch.zeros(self.standard_shape, device=x.device, dtype=x.dtype) else: # For non-standard input shapes, calculate output shape output_shape = self.calculate_output_shape(x.shape) return torch.zeros(output_shape, device=x.device, dtype=x.dtype) def get_inputs(): return [torch.randn(batch_size, in_channels, depth, height, width)] def get_init_inputs(): return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p] # Color Legend: # # # # TECH 1: Mathematical Short-Circuit (Blue) - Skips computation when min_value=0 TECH 2: Pre-allocated Tensors (Purple) - Pre-allocates zero tensors TECH 3: Direct Shape Matching (Green) - Fast path for standard shapes TECH 4: Pre-computed Parameters (Orange) - Pre-computes conv parameters"
        },
        {
            "title": "References",
            "content": "[1] The expectation-maximization algorithm. IEEE Signal processing magazine 13, 6 (1996), 4760. [2] BÄCK, T., AND SCHWEFEL, H.-P. An overview of evolutionary algorithms for parameter optimization. Evolutionary computation 1, 1 (1993), 123. [3] BLEI, D. M., KUCUKELBIR, A., AND MCAULIFFE, J. D. Variational inference: review for statisticians. Journal of the American statistical Association 112, 518 (2017), 859877. [4] CHEN, W., ZHU, J., FAN, Q., MA, Y., AND ZOU, A. Cuda-llm: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092 (2025). [5] CUMMINS, C., SEEKER, V., GRUBISIC, D., ROZIERE, B., GEHRING, J., SYNNAEVE, G., AND LEATHER, H. Llm compiler: Foundation language models for compiler optimization. In Proceedings of the 34th ACM SIGPLAN International Conference on Compiler Construction (2025), pp. 141153. [6] DAT, P. V. T., DOAN, L., AND BINH, H. T. T. Hsevo: Elevating automatic heuristic design with diversity-driven harmony search and genetic algorithm using llms. In Proceedings of the AAAI Conference on Artificial Intelligence (2025), vol. 39, pp. 2693126938. [7] GRATTAFIORI, A., DUBEY, A., JAUHRI, A., PANDEY, A., KADIAN, A., AL-DAHLE, A., LETMAN, A., MATHUR, A., SCHELTEN, A., VAUGHAN, A., ET AL. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [8] GUO, D., YANG, D., ZHANG, H., SONG, J., ZHANG, R., XU, R., ZHU, Q., MA, S., WANG, P., BI, X., ET AL. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [9] HURST, A., LERER, A., GOUCHER, A. P., PERELMAN, A., RAMESH, A., CLARK, A., OSTROW, A., WELIHINDA, A., HAYES, A., RADFORD, A., ET AL. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [10] JAECH, A., KALAI, A., LERER, A., RICHARDSON, A., EL-KISHKY, A., LOW, A., HELYAR, A., MADRY, A., BEUTEL, A., CARNEY, A., ET AL. Openai o1 system card. arXiv preprint arXiv:2412.16720 (2024). [11] JIANG, A. Q., SABLAYROLLES, A., ROUX, A., MENSCH, A., SAVARY, B., BAMFORD, C., CHAPLOT, D. S., CASAS, D. D. L., HANNA, E. B., BRESSAND, F., ET AL. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024). [12] KONDA, V., AND TSITSIKLIS, J. Actor-critic algorithms. Advances in neural information processing systems 12 (1999). [13] LANGE, R. T., PRASAD, A., SUN, Q., FALDOR, M., TANG, Y., AND HA, D. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. Tech. rep., 2025. [14] LEE, K.-H., FISCHER, I., WU, Y.-H., MARWOOD, D., BALUJA, S., SCHUURMANS, D., AND CHEN, X. Evolving deeper llm thinking. arXiv preprint arXiv:2501.09891 (2025). [15] LIU, A., FENG, B., XUE, B., WANG, B., WU, B., LU, C., ZHAO, C., DENG, C., ZHANG, C., RUAN, C., ET AL. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [16] LIU, F., TONG, X., YUAN, M., LIN, X., LUO, F., WANG, Z., LU, Z., AND ZHANG, Q. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051 (2024). [17] MUENNIGHOFF, N., YANG, Z., SHI, W., LI, X. L., LI, F.-F., HAJISHIRZI, H., ZETTLEMOYER, L. S., LIANG, P., CANDES, E. J., AND HASHIMOTO, T. s1: Simple test-time scaling. ArXiv abs/2501.19393 (2025). [18] NOVIKOV, A., U, N., EISENBERGER, M., DUPONT, E., HUANG, P.-S., WAGNER, A. Z., SHIROBOKOV, S., KOZLOVSKII, B., RUIZ, F. J., MEHRABIAN, A., ET AL. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131 (2025). [19] OLMO, T., WALSH, P., SOLDAINI, L., GROENEVELD, D., LO, K., ARORA, S., BHAGIA, A., GU, Y., HUANG, S., JORDAN, M., ET AL. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656 (2024). [20] OUYANG, A., GUO, S., ARORA, S., ZHANG, A. L., HU, W., RÉ, C., AND MIRHOSEINI, A. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517 (2025). [21] ROMERA-PAREDES, B., BAREKATAIN, M., NOVIKOV, A., BALOG, M., KUMAR, M. P., DUPONT, E., RUIZ, F. J., ELLENBERG, J. S., WANG, P., FAWZI, O., ET AL. Mathematical discoveries from program search with large language models. Nature 625, 7995 (2024), 468475. [22] SCHULMAN, J., WOLSKI, F., DHARIWAL, P., RADFORD, A., AND KLIMOV, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [23] SHAO, Z., WANG, P., ZHU, Q., XU, R., SONG, J., BI, X., ZHANG, H., ZHANG, M., LI, Y., WU, Y., ET AL. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [24] SHENGYU, Z., LINFENG, D., XIAOYA, L., SEN, Z., XIAOFEI, S., SHUHE, W., JIWEI, L., HU, R., TIANWEI, Z., WU, F., ET AL. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792 (2023). [25] TEAM, G., ANIL, R., BORGEAUD, S., ALAYRAC, J.-B., YU, J., SORICUT, R., SCHALKWYK, J., DAI, A. M., HAUTH, A., MILLICAN, K., ET AL. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [26] TEAM, G., MESNARD, T., HARDIN, C., DADASHI, R., BHUPATIRAJU, S., PATHAK, S., SIFRE, L., RIVIÈRE, M., KALE, M. S., LOVE, J., ET AL. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024). [27] WANG, S., ZHANG, S., ZHANG, J., HU, R., LI, X., ZHANG, T., LI, J., WU, F., WANG, G., AND HOVY, E. Reinforcement learning enhanced llms: survey. arXiv preprint arXiv:2412.10400 (2024). [28] WEI, A., SURESH, T., TAN, H., XU, Y., SINGH, G., WANG, K., AND AIKEN, A. Improving assembly code performance with large language models via reinforcement learning. arXiv preprint arXiv:2505.11480 (2025). [29] WEI, J., WANG, X., SCHUURMANS, D., BOSMA, M., XIA, F., CHI, E., LE, Q. V., ZHOU, D., ET AL. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [30] WEI, Y., DUCHENNE, O., COPET, J., CARBONNEAUX, Q., ZHANG, L., FRIED, D., SYNNAEVE, G., SINGH, R., AND WANG, S. I. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449 (2025). [31] WILLIAMS, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8 (1992), 229256. [32] YANG, A., LI, A., YANG, B., ZHANG, B., HUI, B., ZHENG, B., YU, B., GAO, C., HUANG, C., LV, C., ET AL. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [33] ZHANG, R., LIU, F., LIN, X., WANG, Z., LU, Z., AND ZHANG, Q. Understanding the importance of evolutionary search in automated heuristic design with large language models. In International Conference on Parallel Problem Solving from Nature (2024), Springer, pp. 185202."
        }
    ],
    "affiliations": [
        "DeepReinforce Team"
    ]
}