{
    "paper_title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models",
    "authors": [
        "Matteo Merler",
        "Nicola Dainese",
        "Minttu Alakuijala",
        "Giovanni Bonetta",
        "Pietro Ferrazzi",
        "Yu Tian",
        "Bernardo Magnini",
        "Pekka Marttinen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning."
        },
        {
            "title": "Start",
            "content": "ViPlan: Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models Matteo Merler1,2 , Nicola Dainese1 , Minttu Alakuijala1, Giovanni Bonetta2, 5 2 0 2 9 ] . [ 1 0 8 1 3 1 . 5 0 5 2 : r Pietro Ferrazzi2,3, Yu Tian1, Bernardo Magnini2 1Department of Computer Science, Aalto University , Pekka Marttinen1 2Fondazione Bruno Kessler 3Department of Mathematics, Universit√† degli Studi di Padova {mmerler, gbonetta, pferrazzi, magnini}@fbk.eu {nicola.dainese, minttu.alakuijala, yu.tian, pekka.marttinen}@aalto.fi"
        },
        {
            "title": "Abstract",
            "content": "Integrating Large Language Models with symbolic planners is promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLMgrounded symbolic approaches and methods that plan directly with VLM has been hindered by lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features series of increasingly challenging tasks in two domains: visual variant of the classic Blocksworld planning problem and simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning."
        },
        {
            "title": "Introduction",
            "content": "Automated planning is fundamental capability for general-purpose agents, consisting of the ability to generate action plans based on their knowledge of the environment they act in. This enables them to make decisions, adapt to dynamic environments and achieve complex goals [Ghallab et al., 2004]. The rise in popularity of foundation models [Bommasani et al., 2021], including large language models (LLMs), has prompted many to test their ability for planning [Ahn et al., 2022, Huang et al., 2022, 2023, Hao et al., 2023]. Others have been more critical, arguing that LLMs remain unreliable and lack the capacity for formal planning [Valmeekam et al., 2023b, Kambhampati et al., 2024] and recommending to integrate them with symbolic planners. Such planners rely on formal logic, such as those using the Planning Domain Definition Language (PDDL) [McDermott et al., 1998], to specify the relevant properties of the world state and how actions affect them in order to solve tasks. Extending this line of thought, Vision-Language Models (VLMs) [Liu et al., 2023b, Li et al., 2023a] have gathered interest for their ability to ground symbolic representations of the world, such as Equal contribution. Equal advising. Preprint. Figure 1: Planning with VLMs. Two settings of VLMs used by agents for planning set of actions to reach goal. VLM-as-planner uses the VLM directly to produce new plan after every action. VLM-as-grounder uses the VLM to ground symbolic agents plans to the observations from the environment. Grounding takes the form of yes-no question-answering about whether the conditions that make an action executable are met and whether the expected outcomes of the action are realized. the boolean variables defined in PDDL, named predicates, in perceptual data. This is particularly important in open-world domains, such as embodied AI tasks, where the state of the world can change in ways that symbolic planners alone cannot handle due to their lack of built-in perceptual grounding. Thanks to their generality, VLMs have the potential to be leveraged without additional training to unlock planning agents grounded in the real world. However, recent evidence has shown that current state-of-the-art VLMs can still struggle with detecting objects [Augustin et al., 2025] and identifying relationships between entities [Tong et al., 2024], both crucial abilities for grounding planners to observations. Furthermore, planning is an inherently sequential task, where each choice depends on previous decisions, which is not captured in typical benchmarks that evaluate VLMs on stand-alone datapoints. Even model with low hallucination rate can fail to produce successful plans, since single mistake might compound over time and invalidate the plan. Thus, task-driven dynamic evaluation setup is required to benchmark the abilities of VLMs both for planning and for grounding symbolic planners. We present ViPlan, the first benchmark to evaluate VLMs in visual planning tasks1. We include two distinct evaluation settings, as shown in Figure 1: i) VLM-as-planner, which asks the VLM to directly predict actions to take, and ii) VLM-as-grounder, which queries the VLM for the truth values of predicates used by symbolic planner. ViPlan is inherently dynamic: instead of asking static set of questions, each step differs based on the environment state that was reached following the models own decisions. This enables us to observe how the model performance scales with the length of the task, as depicted in Figure 2, unlike pre-existing VLM benchmarks. Ultimately, what we aim at measuring is the success rate that an agent employing the VLM as planner or as grounder achieves in solving the problems in the benchmark. We develop two domains for our benchmark: ViPlan-Blocksworld (ViPLan-BW), visual version of the popular Blocksworld planning domain, and ViPlan-Household (ViPlan-HH), household robotics simulator featuring navigation and manipulation tasks. We evaluate on these domains various open and closed-source VLMs on problems of increasing difficulty using the two evaluation settings outlined above. Our results show that current VLMs work better in the VLM-as-grounder approach than in the VLM-as-planner one in ViPlan-BW, while the opposite is true for ViPlan-HH. We hypothesize this is because the VLM can leverage emergent world-modeling capabilities [Li et al., 2023b] to generate 1We open-source the benchmark at https://github.com/merlerm/ViPlan. 2 Figure 2: Compounding Errors in Planning. Analysis of the fractions of tasks solved in ViPlan-BW (all difficulties), based on the number of predictions model would need to answer correctly to succeed in the VLM-as-grounder evaluation setting. Benchmarks that ask independent questions to VLMs and measure their accuracy do not capture the effect that compounding errors have on solving problem, and would correspond to measuring only one prediction. Most models score well on the single prediction, but deteriorate quickly as the errors compound. plans for the household domain, even if the visual observation is unclear. Instead, VLMs struggle to do this on ViPlan-BW as goals are more abstract (e.g., arranging the blocks in specific way), so the reliance on perceptual input is much greater, with the language component providing little benefit. This contrast enables the benchmark to assess different capabilities of VLMs in both abstract and real-world-like scenarios and shows where VLMs are capable of modeling the world. Finally, we test if the addition of Chain-of-Thought (CoT) [Wei et al., 2022] reasoning techniques benefit either method, and find that for most models the performance is largely unchanged. This supports recent evidence [Chen et al., 2024] that VLMs struggle to generate consistent intermediate steps, hindering the effectiveness of reasoning-based approaches."
        },
        {
            "title": "2 Background",
            "content": "In the ViPlan benchmark, we investigate goal-directed planning problems, also referred to as classical planning problems [Ghallab et al., 2004]. These involve finding sequence of actions that transitions an agent from an initial state to goal state, given model of the environment. classical planning problem is defined as tuple = D, I, G, where is the problem domain, is the initial state and is the desired goal state. The domain is further defined as = P, A, where is the set of all boolean predicates used to describe the environments state, and is the set of all actions the agent can perform. See Figure 3 for visual example. Predicates represent properties of the world and can be either lifted (parameterized) or grounded (instantiated). For example, the lifted predicate (holding ?m - movable2) becomes grounded as (holding bowl) when the argument is instantiated. Only grounded predicates can be evaluated as true or false. The initial state and the goal state are specified as sets of truth values over such grounded predicates. Each action in is defined by set of preconditions that must hold in the current state for the action to be applicable, and set of effects that describe how the state changes after the action is executed. Actions, like predicates, can be lifted and then grounded by instantiating their parameters. Actions are typically executed by low-level controllers assumed to perfectly achieve their effects, following the downward refinement property [Bacchus and Yang, 1991] and this allows planners to ignore intermediate states. However, real-world controllers may fail or face changing conditions, and as such grounding is required to identify these failures. 2movable is an example of object type, used to restrict certain actions or predicates to given object category. Figure 3: Example of the basic components for formal planning with PDDL. PDDL domain includes the list of possible lifted actions, which are then grounded by PDDL problem, that provides the initial and goal state. symbolic planner takes as input the PDDL domain and problem to generate symbolic plan to reach the desired goal state through sequence of action. Classical planning problems are typically described using the PDDL formal language. PDDL domain file defines predicates and actions (along with object types), while PDDL problem file defines the initial and goal states along with available objects. Example PDDL domain and problem files are provided in Appendix H. We use the term problem to refer to specific PDDL problem file (i.e., I, plus object list), and task to refer to concrete instance of that problem (e.g., specific environment layout using the same domain). planner takes domain and problem file and returns plan, which is list of grounded actions that leads from to G."
        },
        {
            "title": "3 Related Work",
            "content": "Plan Synthesis with Vision-Language Models. prominent line of research employs LLMs and VLMs as planners, prompted to directly select actions from predefined list [Ahn et al., 2022, Huang et al., 2023]. LLM-Planner [Song et al., 2023] was among the first to integrate visual feedback through an object detector informing the LLM. KnowNo [Ren et al., 2023] performs multiple-choice QA over the next possible actions, and asks for clarifications when the confidence in the answers is low; however, it uses vision oracle for perception. LLM-State [Chen et al., 2023a] builds custom state representation using LLMs that receive text-based observation coming from an object detector, and then plans based on it, with similar method presented by Wu et al. [2023]. DoReMi [Guo et al., 2024] uses an LLM to generate plan and set of constraints that hold true during the execution of specific skill, and then employs VLM to continuously check if this constraint is broken, and replan if thats the case. ReplanVLM [Mei et al., 2024] uses VLM to generate plan and reflects to replan in case of failures. Similarly, ViLa [Hu et al., 2024], which we follow in our implementation of the VLM-as-planner in the benchmark, uses VLM in closed-loop fashion by generating new plan at each step. Crucially, all of these methods build systems upon foundation models without closely investigating the choice of the VLM itself or formalizing benchmark. Visual Symbolic Planning. The integration of LLMs and symbolic planning frameworks, such as PDDL, was first proposed by works like LLM+P [Liu et al., 2023a], with this line of work typically relying on the language model to generate symbolic domain to be used with classical planners [Huang et al., 2024]. Recently, VLMs have emerged as powerful tools for grounding classical planners with visual perception, which was previously only possible through domain-specific models [Migimatsu and Bohg, 2022], which were trained end-to-end. VisualPredicator [Liang et al., 2024] uses VLMs to generate PDDL predicates together with Python implementation, which can use VLM to query their truth value in the world. Image2PDDL [Dang et al., 2025] generates PDDL domain and problem starting from images representing the initial and goal states. AHA [Duan et al., 2024] fine-tunes VLM to reason about failures and then refines VLM-generated symbolic plans based on this feedback. Athalye et al. [2024] start from an initial set of predicates and learn more 4 complex predicates and actions through interaction directly from images. VLM-TAMP [Yang et al., 2024] asks the VLM to provide high-level plan and then reaches these sub-goals using classical motion planning. S3E [Azran et al., 2025] enumerates all possible predicate-object combinations and converts them into natural language questions for VLM at every step of the plan, but the method is not scalable and does not investigate the success rate of planning with the estimated states, which as we show in Figure 2 is hard to predict from prediction accuracy alone. Most relevant to our work, TP-VQA [Zhang et al., 2023] and the later version DKPROMPT [Zhang et al., 2025] use VLMs to monitor the truth values of predicates by asking specific questions based on an actions preconditions and effects. Our implementation of the VLM-as-grounder baseline, while independently developed, resembles the DKPROMPT method, as asking yes-no questions about visual predicates is one of the most straightforward ways of interfacing VLM and PDDL planner. However, DKPROMPT is tested using solely closed-source models on private OmniGibson [Li et al., 2024] set of only five tasks, and no code is publicly available at the time of writing. Most importantly, the lack of diversity in environments, of metrics beyond task success and especially of comparison with strong VLM-as-planner make it hard to assess the full promise of the method. The results on our benchmark, which addresses all the points above, paint very different picture on how competitive VLM-as-grounder approaches are in home robotics tasks such as ViPlan-HH, showing that VLM-as-planners are clearly superior in our experiments. We refer the reader to Appendix for further discussion on works related to benchmarking VLMs for hallucination detection and Visual Question Answering (VQA)."
        },
        {
            "title": "4 The ViPlan Benchmark",
            "content": "This section describes the two ViPlan evaluation settings, as well as the experimental domains implemented in the benchmark. We also introduce the metrics and the VLMs used for our experiments."
        },
        {
            "title": "4.1 Evaluation settings",
            "content": "For each domain, models are evaluated in two settings: VLM-as-planner and VLM-as-grounder. VLM-as-planner. We implement VLM-as-planner using the Robotic Vision-Language Planning (ViLa) [Hu et al., 2024] approach. The VLM is provided with an image of the current state, textual description of the goal and the set of all available actions and is tasked with generating full plan in parsable format (JSON) that satisfies the goal. Then, the first action of the plan is executed and the VLM is prompted to generate full plan again at the next step. This kind of closed-loop planning is well-suited for our tasks, as it can naturally recover from action failures. We include in the prompt additional natural language information about non-visible objects in the case of partially observable environments (e.g., where to find objects that are relevant to the solution of the task). VLM-as-grounder. The VLM-as-grounder approach uses the model to ground symbolic planner (specifically, the Fast Downward planner [Helmert, 2006], as implemented by the Unified Planning library [Micheli et al., 2025]). We assume, as is typically the case with classical planning, that the domain and the goal are given, but that the initial state is unknown (or partially so for ViPlan-HH, as explained later). First, the VLM is tasked with filling in the starting truth values for all possible grounded predicates to generate an initial plan (which we call predicate enumeration). After, for every action to be executed, it is first prompted to verify that the preconditions hold and then that all effects are observed as expected, as shown in Figure 1. We verify each predicate by first converting it to natural language yes-no question through pre-defined template (e.g., (holding bowl) becomes \"Is the agent holding bowl?\") and then prompting the VLM to answer the question based on the current state image. In ViPlan-HH, partially observable domain, enumerating all grounded predicates or verifying the preconditions or effects of an action may not always be possible, as specific predicate might not be visible and the VLM would have no way of answering. In this case, we only ask questions about visible predicates, and initialize the rest of the state with privileged information, which could be provided to the agent in other ways (e.g., communicated in natural language). We leave further integration of symbolic planners in open-world partially observable environments to future work. If, at any point, there is mismatch between the VLM answers and the values expected by the effects of the PDDL action, the state of the world is deemed inconsistent. In this case, the action might have had unintended consequences, and changed other predicates besides the ones specified by its effects. For example, bowl could be knocked over while moving another one due to failed low-level execution. Thus, we re-establish the state must through predicate enumeration, checking all combinations of visible grounded predicates. Afterwards, new plan is generated, and the agent can continue the task while recovering from action failures."
        },
        {
            "title": "4.2 Domains",
            "content": "ViPlan is structured around two domains: ViPlan-Blocksworld (ViPlan-BW) and ViPlan-Household (ViPlan-HH). ViPlan-Blocksworld. Blocksworld is popular classical planning domain [McDermott, 2000] in which the agent must stack blocks on table in specific configuration. Our ViPlan-BW domain is written for the photorealistic Blocksworld renderer3 introduced by Asai [2018]. Blocks can only be moved to the top of column and can only be moved if they dont have other blocks on top. We consider blocks with identical sizes and shapes, but varying in color. ViPlan-BW is composed of 25 distinct procedurally generated problems for three difficulty splits: simple, medium and hard (see Appendix H.1). Figure 6 shows an example image from ViPlan-BW. For the main benchmark, we assume perfect actions as this reduces stochasticity in the benchmark results and ensures that failed tasks are solely due to the VLMs mistakes. We also experiment with non-zero failure probability in Appendix K, and find comparable results in this setting. ViPlan-Household. While ViPlan-BW provides controlled environment, we are also interested in evaluating planning in domain closer to real-world embodied AI tasks. For this, we implement ViPlan-HH on top of iGibson 2.04 [Li et al., 2022], specifically using the Fetch robot5. iGibson is an open-source household simulator, where the agent can interact with various objects to complete tasks (e.g., setting the table or cleaning the dishes). As PDDL deals with high-level actions, we implement each one by bypassing low-level control and instead directly transitioning the environment to one of the actions valid terminal states, saving time and simulation cost. In order to introduce realistic variability, object positions are sampled within allowable regions, which also introduces occasional action failures that need to be detected. Due to this randomness, we verify that each task is solvable using an oracle version of VLM-as-grounder which receives the full symbolic state from the simulator. As in ViPlan-BW, we divide ViPlan-HH in three splits with 25 tasks each. All our original contributions, implementation details, domain and task information can be found at Appendix H.2."
        },
        {
            "title": "4.3 Metrics",
            "content": "For both VLM-as-grounder and VLM-as-planner, we report the success rate, i.e., the fraction of completed tasks, for each split as directly comparable measure of performance. For VLM-asgrounder, the accuracy of single prediction is also relevant metric, which more closely resembles traditional VQA tasks. We report those results in Appendix due to space constraints."
        },
        {
            "title": "4.4 Models",
            "content": "We evaluate nine open-source VLM families at multiple model scales: Llava-OneVision (7B, 72B) [Li et al., 2025], Qwen2.5-VL (7B, 72B) [Bai et al., 2025], AyaVision (8B, 32B) [CohereLabs, 2025], Gemma-3 (12B, 27B) [Gemma Team et al., 2025], DeepSeek-VL2 (MoE with 27B, 4.5B activated) [Wu et al., 2024b], Phi-4 Multimodal (5.6B) [Abouelenin et al., 2025], Molmo (7B) [Deitke et al., 2024], Mistral-Small-3.1 (24B) [Mistral AI, 2025] and InternVL3 (8B, 78B) [Zhu et al., 2025]. We also evaluate closed-source models GPT-4.1 and GPT-4.1 Nano [OpenAI, 2025]. We report the 3Released under Apache 2.0 license at https://github.com/ibm/photorealistic-blocksworld 4Released under MIT license at https://github.com/StanfordVL/iGibson 5https://fetchrobotics.com 6 full model names as found in HuggingFace or in the APIs in Appendix D, while referring to their shorthands in all plots and tables for readability. For each model and task, we test direct I/O prompt as well as zero-shot Chain-of-Thought (CoT) prompt variant [Kojima et al., 2022]. The zero-shot CoT prompt includes one unrelated in-context example to demonstrate the output format expected for parsing (see Appendix I). For all models, we use temperature value of 0 to ensure deterministic outcome."
        },
        {
            "title": "5 Results",
            "content": "In this Section, we first compare the VLM-as-grounder and VLM-as-planner performances for the selected models, then discuss the impact of CoT on VLMs performance and finally look into the relationship between accuracy in predictions and success rate when VLMs are employed as grounders."
        },
        {
            "title": "5.1 VLMs for Planning and Grounding",
            "content": "Figure 4: Planning versus Grounding. The VLM-as-grounder approach excels in ViPlan-BW (top), where GPT-4.1 and InternVL3 78B complete significant fraction of tasks. The VLM-as-planner approach is instead better on ViPlan-HH (bottom), where medium, large and closed models perform generally better than with VLM-as-grounder. We present an overview of the results in Figure 4 comparing the two evaluation settings in the two domains of ViPlan. The exact numbers with errors are reported in Appendix J. ViPlan-BW. In this domain, nearly all models (with the exception of Gemma-3) perform better in the VLM-as-grounder evaluation setting than in the VLM-as-planner one, with the best large open and closed-source models (InternVL3 78B and GPT-4.1) clearly outperforming all other models across all splits. GPT-4.1 achieves perfect accuracy on simple tasks and maintains robust results on medium and hard ones. Among open-source models, InternVL3 78B exhibits the strongest performance, slightly above GPT-4.1 for the medium and hard splits. In contrast, LLaVA-Onevision 72B underperforms on these tasks, indicating that scale does not always correlate with performance. Small (78B) and medium-sized models (1232B) show moderate success on simple tasks but fail on harder splits. 7 In contrast, the VLM-as-planner setting struggles to complete tasks from any split, with even the best-performing models barely reaching success rate of 25% on the simplest split. With this approach, the VLM has to think many steps ahead to make decisions about the next action to take, as opposed to simply answering yes-no questions about the current image. This requires explicit world-modeling capabilities (e.g., state tracking), which VLMs seem to lack in this case, as tasks in ViPlan-BW are highly domain and instance specific (e.g., \"the red block needs to be on top of the blue block\") and models benefit less from general real-world knowledge. ViPlan-HH. The opposite trend is observed in ViPlan-HH, with the VLM-as-planner approach outperforming VLM-as-grounder. Since the ViPlan-HH PDDL domain is more complex and there are more possible interactions to account for than in ViPlan-BW, verifying the actions preconditions and effects typically involves higher number of challenging questions, leading to reduced accuracy. On the other hand, even under state uncertainty, the VLM-as-planner approach can make reasonable predictions for the next action. The results suggest that VLMs already possess emergent worldmodeling capabilities for tasks representative of their training data, making direct VLM planning better option than integration with symbolic planner in these contexts. However, as seen in ViPlan-BW, this does not generalize outside of narrow scopes that closely reflect the pretraining bias of the models, in which having access to an external model (i.e., PDDL) benefits the VLMs. Best model and approach. As part of the benchmark, we also propose leaderboard ranking models, evaluation settings, and CoT setting triplets on the two ViPlan domains. The top 10 entries, selected accordingly to the combined average over ViPlan-BW and ViPlan-HH, are reported in Table 1. While the two best-performing evaluation settings use CoT, none are strictly outperforming the variant without on both domains. In summary, there is no model nor evaluation setting dominating over all others. Table 1: Leaderboard for VLMs on ViPlan-BW and ViPlan-HH. The table reports the average success rate for the top 10 best-performing (model, evaluation setting, CoT setting) triplets, where the evaluation setting is either VLM-as-planner or VLM-as-grounder, and CoT refers to the presence or absence of Chain-of-Thought prompting. Each unique triplet is treated independently and ranked based on its combined success rate, computed as the average of the ViPlan-BW and ViPlan-HH scores; the best triplet appears at the top. The highest value in each column is bolded. Success rates are averaged across all tasks and difficulty splits. We report the standard error of the mean in parenthesis, computed as described in Appendix E. Model Evaluation Setting CoT VLM-as-planner GPT-4.1 VLM-as-grounder InternVL3 78B VLM-as-grounder GPT-4.1 VLM-as-grounder InternVL3 78B VLM-as-grounder GPT-4.1 VLM-as-planner GPT-4.1 Gemma-3 27B VLM-as-planner Mistral-Small-3.1 24B VLM-as-planner VLM-as-planner Gemma-3 27B VLM-as-planner InternVL3 78B Average Success Rate ViPlan-BW ViPlan-HH Combined 0.48 (0.05) 0.77 (0.04) 0.75 (0.04) 0.76 (0.05) 0.71 (0.05) 0.11 (0.03) 0.09 (0.03) 0.05 (0.02) 0.12 (0.04) 0.03 (0.02) 0.45 (0.06) 0.11 (0.03) 0.05 (0.02) 0.04 (0.02) 0.08 (0.03) 0.59 (0.05) 0.51 (0.05) 0.44 (0.05) 0.32 (0.04) 0.36 (0.05) 0.47 (0.04) 0.44 (0.03) 0.40 (0.03) 0.40 (0.03) 0.39 (0.03) 0.35 (0.03) 0.30 (0.03) 0.25 (0.03) 0.22 (0.03) 0.19 (0.03) 5.2 Impact of Chain-of-Thought In Figure 5, we report the difference in success rate for each VLM when employing zero-shot CoT prompting compared to regular prompting (no CoT), for all evaluation settings and domains (see the full metrics in Appendix J). The results of VLM-as-grounder generally show little to no improvement when adding CoT prompting across the two domains, with the exception of smaller models (Molmo and Qwen2.5-VL 7B), where performance degrades significantly (up to an absolute difference of 68%). In VLM-as-planner, the performance instead degrades much more notably, with only few exceptions (in particular, GPT-4.1 improves significantly in ViPlan-BW, but worsens for ViPlan-HH). We conclude that there is no evidence that CoT helps in our benchmark on average across models, as 8 Figure 5: Impact of Chain-of-Thought. The plots show the difference in success rate when using (top half) and not using (bottom half) zero-shot CoT prompting for all domains and evaluation settings. The rightmost column of each plot reports the average difference across models for each split. further explored in Appendix F. Furthermore, some models (like Phi-4 Multimodal and Molmo 7B) are unable to properly follow the required output format (see Appendix I), invalidating their answers."
        },
        {
            "title": "5.3 Accuracy-Success Gap",
            "content": "The prediction accuracy of the model, available for the VLM-as-grounder setting, more closely reflects the accuracy on traditional VQA benchmarks, so it is valuable to compare it with the success rate. In fact, most models successfully predict more than 90% of the predicates in ViPlan-BW, with the average being lower in ViPlan-HH due to the more cluttered images (see Appendix L). However, high accuracy does not imply high success rate. This gap can be attributed to error compounding: each action depends on multiple preand post-condition predicates, and an error on grounding one predicate can easily propagate. For instance, even with 97% predicate accuracy, the probability of an action being fully correct (involving 7 predicates to ground per action on average for ViPlan-BW) is 0.977 0.80, which explains the drop in performance despite high predicate-level accuracy. Figure 2 exemplifies this, by showing how the success rate consistently decreasing for all models as the number of required predictions increases. This clearly differentiates ViPlan from prior benchmarks: measuring the success rate is better indicator for the planning abilities of VLM-as-grounder model, even if traditional benchmarks suggest high single-prediction accuracy."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we introduced ViPlan, the first open-source benchmark for visual planning with symbolic predicates and VLMs, consisting of two complementary evaluation settings (VLM-as-grounder and VLM-as-planner) and two interactive domains (ViPlan-BW and ViPlan-HH). Our benchmark enables the community to compare approaches using VLMs directly for planning with symbolic planning frameworks visually grounded by VLMs. By evaluating wide range of VLM families, we identify significant limitations in state-of-the-art models, suggesting need for further work in predicate estimation and task planning. Across models, we find that VLM-as-planner approach is more promising when commonsense knowledge is informative for action selection, whereas VLM-asgrounder approaches are better for tasks that are less related to pre-training knowledge. Furthermore, we show that Chain-of-Thought reasoning techniques provide little to no improvement across the majority of models and approaches, highlighting the ongoing difficulties current VLMs have with visual reasoning. Finally, our work is not free of limitations, which we discuss in depth in Appendix B."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI, and grants 352986, 358246), EU (H2020 grant 101016775 and NextGenerationEU) and by the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by NextGenerationEU. We thank Aalto-IT (IT Services of Aalto University, Finland) for the provided support with computational resources."
        },
        {
            "title": "References",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. Masataro Asai. Photo-realistic blocksworld dataset. arXiv preprint arXiv:1812.01818, 2018. Ashay Athalye, Nishanth Kumar, Tom Silver, Yichao Liang, Tom√°s Lozano-P√©rez, and Leslie Pack Kaelbling. Predicate invention from pixels via pretrained vision-language models. arXiv preprint arXiv:2501.00296, 2024. Maximilian Augustin, Yannic Neuhaus, and Matthias Hein. Dash: Detection and assessment of systematic hallucinations of vlms. arXiv preprint arXiv:2503.23573, 2025. Guy Azran, Yuval Goshen, Kai Yuan, and Sarah Keren. S3e: Semantic symbolic state estimation with vision-language foundation models. In AAAI 2025 Workshop LM4Plan, 2025. Fahiem Bacchus and Qiang Yang. The downward refinement property. In IJCAI, pages 286293, 1991. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Siwei Chen, Anxing Xiao, and David Hsu. Llm-state: Open world state representation for longhorizon task planning with large language model. arXiv preprint arXiv:2311.17406, 2023a. Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and improving chain-of-thought reasoning in vision-language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 192210, Mexico City, Mexico, June 2024. Association for Computational Linguistics. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023b. CohereLabs. Aya Vision: Expanding the worlds AI can see, March 2025. URL https://cohere. com/blog/aya-vision. 10 Xuzhe Dang, Lada Kudl√°Àáckov√°, and Stefan Edelkamp. Planning with vision-language models and use case in robot-assisted teaching. In AAAI 2025 Workshop LM4Plan, 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors. In Sarath Chandar, Razvan Pascanu, Hanie Sedghi, and Doina Precup, editors, Proceedings of The 2nd Conference on Lifelong Learning Agents, volume 232 of Proceedings of Machine Learning Research, pages 120136. PMLR, 2225 Aug 2023. Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: vision-language-model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram√©, Morgane Rivi√®re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Malik Ghallab, Dana S. Nau, and Paolo Traverso. Automated Planning: Theory and Practice. Elsevier, 2004. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:7908179094, 2023. Yanjiang Guo, Yen-Jen Wang, Lihan Zha, and Jianyu Chen. Doremi: Grounding language model by detecting and recovering from plan-execution misalignment. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1212412131. IEEE, 2024. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore, December 2023. Association for Computational Linguistics. Malte Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191246, 2006. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 91189147. PMLR, 2022. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, pages 17691782. PMLR, 2023. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716, 2024. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Position: Llms cant plan, but can help planning in llmmodulo frameworks. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. 11 Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. Chengshu Li, Fei Xia, Roberto Mart√≠n-Mart√≠n, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Elliott Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pages 455465. PMLR, 0811 Nov 2022. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart√≠nMart√≠n, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, et al. Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023a. Kenneth Li, Aspen Hopkins, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. ICLR, 2023b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023c. Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua Tenenbaum, Tom Silver, Jo√£o Henriques, and Kevin Ellis. Visualpredicator: Learning abstract world models with neuro-symbolic predicates for robot planning. arXiv preprint arXiv:2410.23156, 2024. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023b. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1648816498, 2024. Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl-the planning domain definition language. 1998. Drew McDermott. The 1998 ai planning systems competition. AI magazine, 21(2):3535, 2000. Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, and Zhongxue Gan. Replanvlm: Replanning robotic tasks with visual language models. IEEE Robotics and Automation Letters, 2024. Andrea Micheli, Arthur Bit-Monnot, Gabriele R√∂ger, Enrico Scala, Alessandro Valentini, Luca Framba, Alberto Rovetta, Alessandro Trapasso, Luigi Bonassi, Alfonso Emilio Gerevini, Luca Iocchi, Felix Ingrand, Uwe K√∂ckemann, Fabio Patrizi, Alessandro Saetti, Ivan Serina, and Sebastian Stock. Unified planning: Modeling, manipulating and solving ai planning problems in python. SoftwareX, 29:102012, 2025. ISSN 2352-7110. 12 Toki Migimatsu and Jeannette Bohg. Grounding predicates through actions. In 2022 International Conference on Robotics and Automation (ICRA), pages 34983504. IEEE, 2022. Mistral AI. Mistral mistral-small-3-1. small 3.1, March 2025. URL https://mistral.ai/news/ Mitja Nikolaus, Emmanuelle Salin, Stephane Ayache, Abdellah Fourtassi, and Benoit Favre. Do vision-and-language transformers learn grounded predicate-noun dependencies? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 15381555, 2022. OpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/ gpt-4-1/. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Allen Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. In Conference on Robot Learning, pages 661682. PMLR, 2023. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart√≠n-Mart√≠n, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on robot learning, pages 477490. PMLR, 2022. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36:3897538987, 2023a. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:7599376005, 2023b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 47(8):10871102, 2023. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024a. 13 Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024b. Zhutian Yang, Caelan Reed Garrett, Dieter Fox, Tom√°s Lozano-P√©rez, and Leslie Pack Kaelbling. In 2nd CoRL Guiding long-horizon task and motion planning with vision language models. Workshop on Learning Effective Abstractions for Planning, 2024. Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, and Liang Wang. Investigating compositional challenges in vision-language models for visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1414114151, 2024. Xiaohan Zhang, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, and Shiqi Zhang. Grounding classical task planners via vision-language models. arXiv preprint arXiv:2304.08587, 2023. Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, and Shiqi Zhang. DKPROMPT: Domain knowledge prompting visionlanguage models for open-world planning. In AAAI 2025 Workshop LM4Plan, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Broader Impact",
            "content": "The ViPlan benchmark enables researchers to gauge the abilities of Vision-Language Models for planning, by either direct planning or by grounding symbolic planners. If used responsibly, the benchmark can accelerate progress towards better decision-making from embodied AI agents. However, good performance on the benchmark alone should not be assumed to be enough for real-world deployment of such systems and care should always be exercised, especially in safety-critical applications."
        },
        {
            "title": "B Limitations",
            "content": "In terms of evaluation settings, we assume that in ViPlan-HH some privileged information is given to the VLM both when serving as planner and as grounder. This is because scenario where the model has no information about where certain objects can be found would require big amount of exploration. It is out of the scope of this work how to avoid using privileged information, but future directions should investigate how to lift this assumption, especially in the case of VLM-as-grounder, where PDDL is not easily extendable to unobserved predicates otherwise. In both settings, we also assume that the detection of when high-level action has terminated is handled by the low-level controllers themselves, and the VLM is instead tasked with detecting its effects (or choosing the next action to take) within our benchmark. In principle, VLM can also be used to detect the moment when an action terminates, as shown by Du et al. [2023]. Furthermore, in VLM-as-planner we ask the VLM to produce an entire plan, but then execute only the first action of it, and replan from scratch at the following step. Asking for the full plan is likely still beneficial in the CoT case, but its usefulness remains to be measured, especially in the case of non-CoT prompting. More in general, ViPlan-HH presents non negligible stochasticity which could lead to fluctuations in the results, although these are averaged out by the 25 problems per split that we consider. Due to our computational budget and the large amount of models tested (see Appendix for breakdown of all the computational resources needed for the experiments), we couldnt run multiple seeds for each experiment, but this would be advisable in future experiments with the benchmark. We also assume, as is typically the case with classical planning literature, that the domain is given. However recent works, such as Guan et al. [2023], explore how the domain could be generated using LLMs. We also relied on templates to translate grounded predicates into questions. Recent work [Azran et al., 2025] has shown how this can be automated with LLMs, but as the scope of our benchmark is evaluating VLMs, we avoided this for simplicity."
        },
        {
            "title": "C Additional Related Work",
            "content": "The following is an expansion on Related Work (Section 3) with literature referring to Benchmarks for Vision-Language Models that evaluate their planning abilities and robustness to hallucinations. Benchmarks for Vision-Language Models. Our work closely aligns with other benchmarks, specifically in the area of planning, object hallucinations and relationship hallucinations with VLMs. For planning, previous research typically framed the problem as Visual Question Answering (VQA) [Antol et al., 2015], asking questions about which actions to take [Chen et al., 2023b, Majumdar et al., 2024]. Visual Spatial Planning (VSP) [Wu et al., 2024a] specifically focuses on spatial relationships, but without any notion of symbolic predicates. Our work is closer to PlanBench [Valmeekam et al., 2023a], which investigates the combination of LLMs with symbolic planning, but without considering the visual component. Hallucinations in foundation models are widely studied phenomenon [Liu et al., 2024]. POPE [Li et al., 2023c] is an established benchmark for object hallucination in VLMs, with adversarial examples constructed out of frequently co-occurring objects. This kind of evaluation was scaled up by DASH [Augustin et al., 2025] through smart retrieval from bigger dataset. MMVP [Tong et al., 2024] investigates both object and relationship hallucinations by looking at CLIP-blind pairs of images, placing large emphasis on the role of the vision encoder [Radford et al., 2021]. Benchmarks for relationship hallucination typically work in similar way, by presenting pairs of images with 15 adversarial captions [Thrush et al., 2022, Nikolaus et al., 2022], with later advancements also focusing on the particular role of the vision encoder [Zeng et al., 2024]."
        },
        {
            "title": "D Model Information",
            "content": "For clarity and readability, we report simplified model names throughout the main text and figures. In Table 2 are the correspondance between the full names of the models and the simplified versions. For our experiments GPT-4.1 and GPT-4.1 Nano were accessed via the OpenAI API6, while all other models were downloaded from the Hugging Face Hub7. Table 2: Simplified Model Names. Mapping between specific model identifiers and simplified model names used in the paper. Identifier"
        },
        {
            "title": "Simplified Name",
            "content": "gpt-4.1-nano-2025-04-14 gpt-4.1-2025-04-14 llava-onevision-qwen2-7b-ov-hf llava-onevision-qwen2-72b-ov-hf Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct InternVL3-8B InternVL3-78B gemma-3-12b-it gemma-3-27b-it aya-vision-32b aya-vision-8b Molmo-7B-D-0924 Phi-4-multimodal-instruct Mistral-Small-3.1-24B-Instruct-2503 Mistral-Small-3.1 24B deepseek-vl2 GPT-4.1 Nano GPT-4.1 LLaVA-Onevision 7B LLaVA-Onevision 72B Qwen2.5-VL 7B Qwen2.5-VL 72B InternVL3 8B InternVL3 78B Gemma-3 12B Gemma-3 27B AyaVision 32B AyaVision 8B Molmo 7B Phi-4 Multimodal DeepSeek-VL"
        },
        {
            "title": "E Error Computations",
            "content": "In this Section we outline how the errors are computed throughout the paper. To estimate the error of the mean on the success rate on split, we assume each problem being Bernoulli trial (i.e., drawn from binomial distribution) with true success probability p. Our empirical estimate of p, denoted as ÀÜp, is used to estimate the standard error of the mean (SEM) as SEM(ÀÜp) (cid:112)(ÀÜp(1 ÀÜp)/n , where is the number of problems in the split. This normal approximation to the binomial distribution is reasonable for = 25, which holds for all splits in our experiments. (1) In the leaderboard tables we further report the error of the average success rates across difficulty splits and occasionally even combined across the two domains of ViPlan. These are computed using standard error propagation for the average of independent estimates: SEM(x) = SEM (cid:33) xi/m = (cid:32) (cid:88) i=1 1 (cid:115)(cid:88) SEM(xi)2 . (2) Statistical Significance of Chain-of-Thought We report in Table 3 the numerical differences between the average success rate with and without CoT prompting, together with their errors. Except for VLM-as-planner in the medium split of both 6https://openai.com/index/openai-api/ 7https://huggingface.co/docs/hub/index 16 domains, there is no significant difference (measured in 3 standard deviations of the mean from 0) in employing CoT or not, and even in this case, the difference is positive in ViPlan-BW, but negative in ViPlan-HH. We conclude that there is no evidence that CoT helps in our benchmark when averaging across models. Table 3: Statistical significance of CoT difference. We report the average difference between experiments with and without CoT prompting across models for each split and evaluation setting. We further report the standard error of the mean, and the ratio between the absolute value of the average and its error in parenthesis. We bold ratios that are below 3, indicating no statistical difference between the two approaches. Evaluation Setting Domain Simple Medium Hard Average Error Average Error Average Error VLM-as-grounder VLM-as-planner ViPLan-BW -0.060 0.010 ViPLan-HH ViPLan-BW 0.052 -0.068 ViPLan-HH 0.022 (2.8) 0.016 (0.6) 0.018 (3.0) 0.024 (2.8) -0.018 0.000 0.033 -0.065 0.015 (1.2) 0.005 (0.0) 0.009 (3.6) 0.018 (3.5) -0.005 -0.003 0.007 -0.040 0.013 (0.4) 0.005 (0.5) 0.005 (1.4) 0.017 (2.4)"
        },
        {
            "title": "G Computational Resources",
            "content": "We ran all our experiments on cluster equipped with three different GPU models from NVIDIA: A100 with 80GB VRAM, H100 with 80GB VRAM and H200 with 141GB VRAM. The computational resources needed to run the full benchmark for 9 models fitting in single 80GB GPU and 5 models fitting in two GPUs, are reported in Table 4, totaling 895 GPU hours. While this is considerable amount, for practitioners willing to benchmark VLM of around 7-8B parameters, this would require roughly 39 hours for the full benchmark and less than 15 if running only the experiments without Chain-of-Thought. More in general, we estimate that all the experiments of the project required us 2900 GPU hours, several of which employing smaller V100 GPUs to run the iGibson simulator. Finally, the experiments with GPT-4.1 and GPT-4.1 Nano cost around 150$. Table 4: Summary of Computational Resources. GPU hours required to run all 14 open-source models for each evaluation scenario of the paper."
        },
        {
            "title": "Setting",
            "content": "ViPLan-BW ViPLan-HH VLM-as-grounder (no-CoT) VLM-as-grounder (CoT) VLM-as-planner (no-CoT) VLM-as-planner (CoT) 55h 132h 68h 86h 103h 154h 147h 150h"
        },
        {
            "title": "H Domain details",
            "content": "H.1 ViPlan-BW We divided the tasks into three splits, simple, medium and hard, each composed of 25 different problems which were automatically generated and validated. The details for each split can be found in Table 5. An example of an image taken from ViPlan-BW problem (hard split) can be seen in Figure 6. While typical PDDL domains for Blocksworld also include predicate for the agent holding block in its hand, since our image observation space does not show the agent, we instead define simplified version of the domain with only single action that moves block to the top of specified column. The full PDDL domain for ViPlan-BW can be seen in Figure 7. Note that the rightOf and leftOf predicates are never actually used in the moveBlock action; however they are still filled in by 17 Table 5: ViPlan-BW Task Splits. Task splits based on difficulty, number of blocks, columns, and plan length."
        },
        {
            "title": "Split",
            "content": "# Blocks # Columns"
        },
        {
            "title": "Simple\nMedium\nHard",
            "content": "3 5 6 4 5 4 35 510 815 the VLM when enumerating all predicates, and can thus serve as way of measuring the VLMs awareness of spatial directions such as left and right. An example of problem that works with the domain can be seen in Figure 8. The domain and problem files can be given to classical planner to compute plan. Figure 6: ViPlan-BW rendering. Example of rendered state from the ViPlan-BW domain. Columns are labeled explicitly in order to avoid posing ambiguous questions to the VLM. ViPlan-BW Domain (define (domain Blocksworld) (:requirements :strips :typing :negative-preconditions :conditional-effects :equality) (:types block column) (:predicates (on ?b1 - block ?b2 - block) ;; block b1 is on block b2 (inColumn ?b - block ?c - column) ;; block is in column (clear ?b - block) ;; block is clear (i.e., nothing is on top of it) (rightOf ?c1 - column ?c2 - column) ;; column c1 is to the right of column c2 (leftOf ?c1 - column ?c2 - column) ;; column c1 is to the left of column c2 ) (:action moveBlock :parameters (?b1 - block ?c1 - column) ;; move block b1 to column c1 :precondition (and (clear ?b1) (not (inColumn ?b1 ?c1))) ;; block b1 must be clear and not already in column c1 :effect (and (forall (?b2 - block) ;; for all blocks b2 (and (when (on ?b1 ?b2) (and (not (on ?b1 ?b2)) (clear ?b2)) ;; if block b1 was on block b2, then b1 is no longer on b2 and b2 is clear ) (when (and (inColumn ?b2 ?c1) (clear ?b2) (not (= ?b2 ?b1))) (and (on ?b1 ?b2) (not (clear ?b2))) ;; if another block b2 was in the column c1 where b1 is moving and b2 was clear, then b1 is now on b2 and b2 is no longer clear ) ) ) (forall (?c2 - column) ;; for all columns c2 (when (inColumn ?b1 ?c2) (not (inColumn ?b1 ?c2))) ;; if block b1 was in column c2, then b1 is no longer in c2) (inColumn ?b1 ?c1) ;; block b1 is now in column c1 (clear ?b1) ;; block b1 is now clear (as it must be if it was moved) ) ) ) Figure 7: ViPlan-BW Domain. PDDL domain for ViPlan-BW. 19 Example Problem for ViPlan-BW (define (problem simple_problem_0) (:domain Blocksworld) (:objects - block C1 C2 C3 C4 - column ) (:init (clear Y) (clear P) (clear R) (inColumn C2) (inColumn C1) (inColumn C4) (rightOf C2 C1) (rightOf C3 C2) (rightOf C4 C3) (leftOf C1 C2) (leftOf C2 C3) (leftOf C3 C4) ) (:goal (and (clear Y) (clear P) (clear R) (inColumn C3) (inColumn C4) (inColumn C1) ) ) ) Figure 8: ViPlan-BW Problem. Example of problem for the ViPlan-BW domain. 20 Table 6: Task splits used in the ViPlan-HH domain of our benchmark. Tasks appearing in multiple splits have been adjusted in terms of goal to achieve, to make them easier or harder, depending on the split. The two additional columns report the number of problem instances per task and the minimum number of actions required to complete it."
        },
        {
            "title": "Difficulty Task",
            "content": "# Instances # Actions"
        },
        {
            "title": "Hard",
            "content": "cleaning out drawers locking every door locking every window packing food for work sorting books cleaning out drawers collect misplaced items packing food for work putting away toys sorting books sorting groceries cleaning out drawers organizing boxes in garage organizing file cabinet putting away toys sorting groceries 5 5 5 5 5 2 4 4 5 4 6 5 5 4 4 5 4 6 5 4 10 8 10 8 8 10 15 11 14 12 13 H.2 ViPlan-HH ViPlan-HH is composed of selection of Behavior Domain Definition Language (BDDL)8 problems from the BEHAVIOR-100 task suite [Srivastava et al., 2022]. As BDDL problems are not enough for planning, but are used as constraints to generate the problem layout in the iGibson simulator, we translated them in PDDL and wrote an adequate domain, reported in Figure 9. The iGibson simulator includes multiple scenes that are compatible with each problem, and each scene has multiple instances (e.g., different dispositions of objects or different looking 3D models for the same object). Thus, in order to reach 25 problems, we select 5 tasks for the simple split, 6 tasks for the medium split and 5 tasks for the hard split. For each tasks, we write PDDL problem file (see e.g., Figure 10) and then select multiple scenes and instances, to reach total of 25 unique (PDDL problem, scene, scene instance) triplets, as reported in Table 6. For each split, the problems are edited (e.g., by considering more or less objects) to ensure that length of the plan required to solve them matches the one used for ViPlan-BW  (Table 5)  . Sample images from iGibson can be found in Figures 1 and 3. Note that successor to iGibson, named OmniGibson [Li et al., 2024], was recently proposed. While it provides more realistic graphics, which would be an advantage for VLM evaluation, it requires NVIDIA RTX GPU (NVIDIA RTX 2070+), causing significant accessibility issues. We thus opted for iGibson instead for ease of reuse. H.2.1 ViPlan-HH Original Contributions While we build on top of iGibson and leverage BDDL problems from BEHAVIOR-100, adapting the environment to our benchmark required months of work. Our main contributions in this regard can be summarized as follows: 1. Implement PDDL domain and interface it with symbolic planner 2. Implement in iGibson each predicate and high-level action listed in the domain by leveraging previleged low-level access to the internal states of the simulator 3. Add 3D bounding boxes with object labels when multiple objects of the same type are in sight 4. Building server-client framework to enable containerization of the iGibson environment with GPU acceleration on SLURM server 8This is simplified version of PDDL, where no domain is provided, only the problem. 21 ViPlan-HH Domain (define (domain igibson) (:requirements :strips :typing :negative-preconditions :conditional-effects :equality) (:types container movable - object ) (:predicates ;; Agent predicates (reachable ?o - object) (holding ?m - movable) ;; Object attributes (open ?c - container) ;; Object relations (ontop ?o1 - object ?o2 - object) (nextto ?o1 - object ?o2 - object) ;; Only containers can contain objects (inside ?o - object ?c - container) ) (:action grasp :parameters (?m - movable) :precondition (and (reachable ?m) ;; Agent must not be holding anything (forall (?x - movable) (not (holding ?x)) ) ) :effect (and (holding ?m) (forall (?y - object) (and ;; If grasped object is on top of something, ;; it is no longer on top of it (not (ontop ?m ?y)) ;; Same for nextto (not (nextto ?m ?y))) ) ;; If was in container, its not anymore (forall (?c - container) (when (inside ?m ?c) (not (inside ?m ?c))) ) ) ) (:action place-on :parameters (?m - movable ?o2 - object) :precondition (and (holding ?m) (reachable ?o2) ) :effect (and (ontop ?m ?o2) (not (holding ?m)) ) ) (:action place-next-to :parameters (?m - movable ?o2 - object) :precondition (and (holding ?m) (reachable ?o2) ) :effect (and (nextto ?m ?o2) (not (holding ?m)) ) ) (:action place-inside :parameters (?m - movable ?c - container) :precondition (and (holding ?m) (reachable ?c) (open ?c) ) :effect (and (inside ?m ?c) (not (holding ?m)) ) ) (:action open-container :parameters (?c - container) :precondition (and (reachable ?c) (not (open ?c)) ;; Agent must not be holding anything (forall (?x - movable) (not (holding ?x)) ) ) :effect (and (open ?c) ;; All objects inside the container are reachable (forall (?o - object) (when (inside ?o ?c) (reachable ?o)) ) ) ) (:action close-container :parameters (?c - container) :precondition (and (reachable ?c) (open ?c) ) :effect (and (not (open ?c)) ;; All objects inside the container are unreachable (forall (?o - object) (when (inside ?o ?c) (not (reachable ?o))) ) ) ) (:action navigate-to :parameters (?o - object) 23 :precondition (and (not (reachable ?o)) ;; Do not navigate-to things hidden in closed container (forall (?c - container) (or (not(inside ?o ?c)) (open ?c)) ) ) :effect (and (reachable ?o) ;; make target object reachable ;; Make every other object unreachable (forall (?x - object) (when (not (= ?x ?o)) (not (reachable ?x)))) ;; Also, if there exists container which is ?o and that its open, ;; set the objects inside as reachable (forall (?c - container ?x - object) (when (and (= ?c ?o) (open ?c) (inside ?x ?c)) (reachable ?x))) ) ) ) Figure 9: PDDL domain for the ViPlan-HH environment. 24 Example Problem for ViPlan-HH (define (problem cleaning_out_drawers_0) (:domain igibson) (:objects bowl_1 - movable cabinet_1 - container sink_1 - object ) (:init (inside bowl_1 cabinet_1) (not (open cabinet_1)) ) (:goal (and (ontop bowl_1 sink_1) ) ) ) Figure 10: ViPlan-HH Problem. Example of problem for the ViPlan-HH domain."
        },
        {
            "title": "I Prompts",
            "content": "In the following, we report all the prompts used in our experiments. VLM-as-grounder prompt for ViPlan-BW without CoT <system> You are tasked with replying to question about the given image. You will be given single question, defined after the keyword \"Question:\" and will need to answer it ONLY with Yes or No. Do not write anything else besides your answer. The image will be about colored blocks and how they relate to each other. In the environment, the blocks will be arranged in columns, spanning from left to right. Keep in mind that some of these columns can be empty with no blocks currently placed in them. Within column one or multiple blocks of different colors can be stacked on top of each other. Your task is to correctly evaluate the question based on the image provided. </system> <user> {image} </user> VLM-as-grounder prompt for ViPlan-BW with CoT <system> You are tasked with replying to question about the given image. You will be given single question, defined after the keyword \"Question:\" and will need to first reason about it and then give Yes or No answer. The reasoning for your answer should be written within the XMLstyle <explanation> </explanation>tags. To write the final answer, you should write only \"Yes\" or \"No\" surrounded by <answer></answer>tags. Do not write anything else besides your stepbystep reasoning and your answer. Example output for question about the an image: Question: Is there dog on top of table? <explanation> 25 First, will look for dog in the image. Then, will check if the dog is on top of table. In the image, there is dog and there is table, but the dog is not on top of the table. Therefore, the answer is \"No\". </explanation> <answer> No </answer> </system> <user> The image will be about colored blocks and how they relate to each other. In the environment, the blocks will be arranged in columns, spanning from left to right. Keep in mind that some of these columns can be empty with no blocks currently placed in them. Within column one or multiple blocks of different colors can be stacked on top of each other. {image} </user> VLM-as-grounder prompt for ViPlan-HH without CoT <system> You are tasked with replying to question about the given image. You will be given single question, defined after the keyword \"Question:\" and will need to answer it ONLY with Yes or No. Do not write anything else besides your answer. </system> <user> The environment is virtual household simulator, with objects and furniture which can be interacted with. There is robotic arm, which is the agent, that can hold objects. {image} </user> VLM-as-grounder prompt for ViPlan-HH with CoT <system> You are tasked with replying to question about the given image. You will be given single question, defined after the keyword \"Question:\" and will need to first reason about it and then give Yes or No answer. The reasoning for your answer should be written within the XMLstyle <explanation> </explanation>tags. To write the final answer, you should write only \"Yes\" or \"No\" surrounded by <answer></answer>tags. Do not write anything else besides your stepbystep reasoning and your answer. Example output for question about the an image: Question: Is there dog on top of table? <explanation> First, will look for dog in the image. Then, will check if the dog is on top of table. In the image, there is dog and there is table, but the dog is not on top of the table. Therefore, the answer is \"No\". </explanation> <answer> No </answer> </system> <user> The environment is virtual household simulator, with objects and furniture which can be interacted with. There is robotic arm, which is the agent, that can hold objects. {image} </user> VLM-as-planner prompt for ViPlan-BW without CoT <system> You are an expert planning assistant. You will be given an image which represents the current state of the environment you are in, natural language description of the goal that needs to be achieved and set of actions that can be performed in the environment. Your task is to generate plan that achieves the goal, in the form of sequence of actions that need to be executed to reach the goal. The format of your output should be JSON object with the following structure: json { \"plan\": [ { \"action\": action_name, \"parameters\": { parameter_name: parameter_value } }, ... other actions ... ] } You will also receive feedback of the previously taken actions, with note showing if they failed or not. If an action failed, think about why that could be and then output new plan accordingly. </system> <user> ## Description of the environment The environment is about colored blocks and how they relate to each other. In the environment, the blocks will be arranged in columns, spanning from left to right. Keep in mind that some of these columns can be empty with no blocks currently placed in them. Within column one or multiple blocks of different colors can be stacked on top of each other. Your task is to correctly evaluate the question based on the image provided. ## Available actions You have only one action available, called moveblock(block, column). This action allows you to move block from its current column to the specified column. In order to perform this action, the block you want to move must be the topmost block of its column and must not already be in the target column. If the action is valid, the block will be moved to the specified column and will be placed on top of any blocks that are already in that column, if any. To refer to the blocks, use lowercase letters for the colors: for red, for green, for blue, for yellow, for purple, for orange. To refer to the columns, use the labels provided in the image, c1, c2, c3, c4 and c5. ## Goal {goal_string} ## Previously taken actions {previous_actions} ## Current environment state {image} </user> VLM-as-planner prompt for ViPlan-BW with CoT <system> You are an expert planning assistant. You will be given an image which represents the current state of the environment you are in, natural language description of the goal that needs to be achieved and set of actions that can be performed in the environment. Your task is to generate plan that achieves the goal, in the form of sequence of actions that need to be executed to reach the goal. 27 Before answering with the plan, think carefully step by step about the actions you need to take and what the expected outcome of each action is. Write the reasoning behind the plan and justify each action you are going to take. Make sure that each action is possible, and if previous actions failed, reason about why this could be the case. The format of your output should be JSON object with the following structure. Make sure that the explanation is also written inside the json. json { \"explanation\": <a detailed explanation of the plan>, \"plan\": [ { \"action\": action_name, \"parameters\": { parameter_name: parameter_value } }, ... other actions ... ] } You will also receive feedback of the previously taken actions, with note showing if they failed or not. If an action failed, think about why that could be and then output new plan accordingly. </system> <user> ## Description of the environment The environment is about colored blocks and how they relate to each other. In the environment, the blocks will be arranged in columns, spanning from left to right. Keep in mind that some of these columns can be empty with no blocks currently placed in them. Within column one or multiple blocks of different colors can be stacked on top of each other. Your task is to correctly evaluate the question based on the image provided. ## Available actions You have only one action available, called moveblock(block, column). This action allows you to move block from its current column to the specified column. In order to perform this action, the block you want to move must be the topmost block of its column and must not already be in the target column. If the action is valid, the block will be moved to the specified column and will be placed on top of any blocks that are already in that column, if any. To refer to the blocks, use lowercase letters for the colors: for red, for green, for blue, for yellow, for purple, for orange. To refer to the columns, use the labels provided in the image, c1, c2, c3, c4 and c5. ## Goal {goal_string} ## Previously taken actions {previous_actions} ## Current environment state {image} </user> VLM-as-planner prompt for ViPlan-HH without CoT <system> You are an expert planning assistant. You will be given an image which represents the current state of the environment you are in, natural language description of the goal that needs to be achieved and set of actions that can be performed in the environment. Your task is to generate plan that achieves the goal, in the form of sequence of actions that need to be executed to reach the goal. The format of your output should be JSON object with the following structure: 28 json { \"plan\": [ { \"action\": action_name, \"parameters\": [parameter1, parameter2, ...] }, ... other actions ... ] } You will also receive feedback of the previously taken actions, with note showing if they failed or not. If an action failed, think about why that could be and then output new plan accordingly. </system> <user> ## Description of the environment The environment is virtual household simulator, with objects and furniture which can be interacted with. Keep in mind that some objects might not be visible or immediately reachable, in which case you need to navigate to them first. If after navigating to an object it is still not reachable, you might need to open container. ## Additional information {priviledged_info} ## Available actions Action: grasp Parameters: 1. movable object Preconditions: The object is within reach. The agent is not holding anything. Effects: The agent picks up that object. It is no longer on top of or next to any other object. If it was inside container, it leaves the container. Action: placeon Parameters: 1. the movable object being held 2. another object to serve as support Preconditions: The agent is holding the first object. The support object is within reach. Effects: The held object is placed on top of the support object. The agents hands become free. Action: placenextto Parameters: 1. the movable object being held 2. another object to stand beside Preconditions: The agent is holding the first object. The other object is within reach. Effects: The held object is positioned next to the other object. The agents hands become free. Action: placeinside Parameters: 1. the movable object being held 2. an open container Preconditions: The agent is holding the object. The container is open and within reach. Effects: The object is placed inside the container. The agents hands become free. Action: opencontainer Parameters: 1. closed container Preconditions: The container is within reach. The agent is not holding anything. Effects: The container becomes open. All objects inside it become reachable. Action: closecontainer Parameters: 1. an open container Preconditions: The container is within reach. Effects: The container becomes closed. All objects inside it become unreachable. Action: navigateto Parameters: 1. any target object Preconditions: The target object is currently out of reach and not hidden in closed container. Effects: The target object becomes reachable. All other objects become out of reach. If the target is an open container, everything inside it also becomes reachable. ## Goal {goal_string} ## Previously taken actions {previous_actions} ## Current environment state {image} </user> VLM-as-planner prompt for ViPlan-HH with CoT <system> You are an expert planning assistant. You will be given an image which represents the current state of the environment you are in, natural language description of the goal that needs to be achieved and set of actions that can be performed in the environment. Your task is to generate plan that achieves the goal, in the form of sequence of actions that need to be executed to reach the goal. Before answering with the plan, think carefully step by step about the actions you need to take and what the expected outcome of each action is. Write the reasoning behind the plan and justify each action you are going to take. Make sure that each action is possible, and if previous actions failed, reason about why this could be the case. 30 The format of your output should be JSON object with the following structure. Make sure that the explanation is also written inside the json. json { \"explanation\": <a detailed explanation of the plan>, \"plan\": [ { \"action\": action_name, \"parameters\": [parameter1, parameter2, ...] }, ... other actions ... ] } You will also receive feedback of the previously taken actions, with note showing if they failed or not. If an action failed, think about why that could be and then output new plan accordingly. </system> <user> ## Description of the environment The environment is virtual household simulator, with objects and furniture which can be interacted with. Keep in mind that some objects might not be visible or immediately reachable, in which case you need to navigate to them first. If after navigating to an object it is still not reachable, you might need to open container. ## Additional information {priviledged_info} ## Available actions Action: grasp Parameters: 1. movable object Preconditions: The object is within reach. The agent is not holding anything. Effects: The agent picks up that object. It is no longer on top of or next to any other object. If it was inside container, it leaves the container. Action: placeon Parameters: 1. the movable object being held 2. another object to serve as support Preconditions: The agent is holding the first object. The support object is within reach. Effects: The held object is placed on top of the support object. The agents hands become free. Action: placenextto Parameters: 1. the movable object being held 2. another object to stand beside Preconditions: The agent is holding the first object. The other object is within reach. Effects: The held object is positioned next to the other object. The agents hands become free. 31 Action: placeinside Parameters: 1. the movable object being held 2. an open container Preconditions: The agent is holding the object. The container is open and within reach. Effects: The object is placed inside the container. The agents hands become free. Action: opencontainer Parameters: 1. closed container Preconditions: The container is within reach. The agent is not holding anything. Effects: The container becomes open. All objects inside it become reachable. Action: closecontainer Parameters: 1. an open container Preconditions: The container is within reach. Effects: The container becomes closed. All objects inside it become unreachable. Action: navigateto Parameters: 1. any target object Preconditions: The target object is currently out of reach and not hidden in closed container. Effects: The target object becomes reachable. All other objects become out of reach. If the target is an open container, everything inside it also becomes reachable. ## Goal {goal_string} ## Previously taken actions {previous_actions} ## Current environment state {image} </user>"
        },
        {
            "title": "J Complete benchmark results",
            "content": "We report the individual success rate and accuracy for each model, evaluation setting, domain and split in Tables 7, 8, 9 and 10, showing the VLM-as-grounder and VLM-as-planner results for ViPlan-BW and ViPlan-HH respectively. Table 7: Full Results for VLM-as-grounder on ViPlan-BW. Success rate indicates the fraction of problems solved, accuracy is the fraction of correct predictions for single predicate. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded. Model AyaVision 32B AyaVision 32B AyaVision 8B AyaVision 8B DeepSeek-VL2 DeepSeek-VL2 GPT-4.1 GPT-4.1 GPT-4.1 Nano GPT-4.1 Nano Gemma-3 12B Gemma-3 12B Gemma-3 27B Gemma-3 27B InternVL3 78B InternVL3 78B InternVL3 8B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 72B LLaVA-Onevision 7B LLaVA-Onevision 7B Mistral-Small-3.1 24B Mistral-Small-3.1 24B Molmo 7B Molmo 7B Phi-4 Multimodal Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 72B Qwen2.5-VL 7B Qwen2.5-VL 7B CoT Simple Success Accuracy Medium Success Accuracy Hard Success Accuracy 0.04 (0.04) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 1.00 (0.00) 0.92 (0.05) 0.12 (0.06) 0.36 (0.10) 0.04 (0.04) 0.16 (0.07) 0.20 (0.08) 0.28 (0.09) 0.92 (0.05) 0.96 (0.04) 0.32 (0.09) 0.08 (0.05) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.40 (0.10) 0.56 (0.10) 0.52 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.56 (0.10) 0.48 (0.10) 0.76 (0.09) 0.08 (0.05) 0.88 (0.01) 0.89 (0.01) 0.68 (0.01) 0.51 (0.01) 0.91 (0.01) 0.91 (0.00) 1.00 (0.00) 0.99 (0.00) 0.89 (0.01) 0.96 (0.00) 0.96 (0.01) 0.93 (0.00) 0.96 (0.00) 0.96 (0.00) 1.00 (0.00) 1.00 (0.00) 0.97 (0.00) 0.95 (0.00) 0.95 (0.01) 0.96 (0.00) 0.92 (0.01) 0.93 (0.01) 0.99 (0.00) 0.98 (0.00) 0.94 (0.00) 0.77 (0.01) 0.92 (0.01) 0.08 (0.01) 0.98 (0.00) 0.97 (0.00) 0.99 (0.00) 0.98 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.76 (0.09) 0.76 (0.09) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.80 (0.08) 0.80 (0.08) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.08 (0.05) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.40 (0.10) 0.20 (0.08) 0.12 (0.06) 0.00 (0.00) 0.82 (0.00) 0.85 (0.00) 0.69 (0.01) 0.53 (0.01) 0.88 (0.01) 0.85 (0.00) 0.99 (0.00) 0.98 (0.00) 0.85 (0.00) 0.96 (0.00) 0.95 (0.00) 0.91 (0.01) 0.93 (0.00) 0.94 (0.00) 0.99 (0.00) 0.99 (0.00) 0.95 (0.00) 0.92 (0.00) 0.94 (0.00) 0.94 (0.00) 0.88 (0.01) 0.88 (0.01) 0.96 (0.00) 0.95 (0.00) 0.93 (0.00) 0.75 (0.01) 0.89 (0.01) 0.15 (0.01) 0.99 (0.00) 0.99 (0.00) 0.97 (0.00) 0.92 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.48 (0.10) 0.44 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.56 (0.10) 0.56 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.76 (0.01) 0.79 (0.00) 0.76 (0.01) 0.60 (0.01) 0.88 (0.01) 0.86 (0.00) 0.98 (0.00) 0.98 (0.00) 0.84 (0.00) 0.92 (0.00) 0.91 (0.00) 0.88 (0.01) 0.93 (0.01) 0.91 (0.00) 0.98 (0.00) 0.98 (0.00) 0.92 (0.00) 0.86 (0.00) 0.93 (0.01) 0.94 (0.00) 0.86 (0.01) 0.89 (0.01) 0.96 (0.00) 0.94 (0.00) 0.91 (0.00) 0.74 (0.01) 0.83 (0.01) 0.19 (0.01) 0.97 (0.00) 0.96 (0.00) 0.94 (0.00) 0.89 (0.00) 33 Table 8: Full Results for VLM-as-planner on ViPlan-BW. Success rate indicates the fraction of problems solved. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded."
        },
        {
            "title": "Hard",
            "content": "AyaVision 32B AyaVision 32B AyaVision 8B AyaVision 8B DeepSeek-VL2 DeepSeek-VL2 GPT-4.1 GPT-4.1 GPT-4.1 Nano GPT-4.1 Nano Gemma-3 12B Gemma-3 12B Gemma-3 27B Gemma-3 27B InternVL3 78B InternVL3 78B InternVL3 8B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 72B LLaVA-Onevision 7B LLaVA-Onevision 7B Mistral-Small-3.1 24B Mistral-Small-3.1 24B Molmo 7B Molmo 7B Phi-4 Multimodal Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 72B Qwen2.5-VL 7B Qwen2.5-VL 7B 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.24 (0.09) 0.84 (0.07) 0.04 (0.04) 0.24 (0.09) 0.16 (0.07) 0.28 (0.09) 0.24 (0.09) 0.20 (0.08) 0.08 (0.05) 0.12 (0.06) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.16 (0.07) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.08 (0.05) 0.16 (0.07) 0.08 (0.05) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.48 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.12 (0.06) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.12 (0.06) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) Table 9: Full Results for VLM-as-grounder on ViPlan-HH. Success rate indicates the fraction of problems solved, accuracy is the fraction of correct predictions for single predicate. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded. Model AyaVision 32B AyaVision 32B AyaVision 8B AyaVision 8B DeepSeek-VL2 DeepSeek-VL2 GPT-4.1 GPT-4.1 GPT-4.1 Nano GPT-4.1 Nano Gemma-3 12B Gemma-3 12B Gemma-3 27B Gemma-3 27B InternVL3 78B InternVL3 78B InternVL3 8B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 72B LLaVA-Onevision 7B LLaVA-Onevision 7B Mistral-Small-3.1 24B Mistral-Small-3.1 24B Molmo 7B Molmo 7B Phi-4 Multimodal Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 72B Qwen2.5-VL 7B Qwen2.5-VL 7B CoT Simple Success Accuracy Medium Success Accuracy Hard Success Accuracy 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.12 (0.06) 0.00 (0.00) 0.16 (0.07) 0.20 (0.08) 0.04 (0.04) 0.04 (0.04) 0.16 (0.07) 0.12 (0.06) 0.08 (0.05) 0.12 (0.06) 0.12 (0.06) 0.28 (0.09) 0.00 (0.00) 0.08 (0.05) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.61 (0.02) 0.72 (0.01) 0.46 (0.02) 0.67 (0.01) 0.71 (0.01) 0.69 (0.01) 0.68 (0.01) 0.69 (0.01) 0.54 (0.01) 0.67 (0.01) 0.52 (0.02) 0.65 (0.01) 0.67 (0.01) 0.70 (0.01) 0.71 (0.01) 0.66 (0.01) 0.73 (0.01) 0.62 (0.01) 0.64 (0.02) 0.69 (0.02) 0.59 (0.01) 0.58 (0.01) 0.69 (0.01) 0.60 (0.02) 0.60 (0.02) 0.68 (0.01) 0.31 (0.02) 0.05 (0.03) 0.72 (0.01) 0.70 (0.01) 0.73 (0.01) 0.68 (0.01) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.65 (0.01) 0.75 (0.00) 0.44 (0.01) 0.70 (0.01) 0.75 (0.00) 0.70 (0.00) 0.74 (0.01) 0.72 (0.01) 0.60 (0.01) 0.72 (0.01) 0.68 (0.01) 0.58 (0.01) 0.77 (0.00) 0.68 (0.01) 0.68 (0.01) 0.69 (0.01) 0.78 (0.00) 0.59 (0.01) 0.80 (0.01) 0.79 (0.01) 0.68 (0.01) 0.65 (0.01) 0.83 (0.00) 0.64 (0.01) 0.65 (0.01) 0.71 (0.01) 0.22 (0.01) 0.02 (0.00) 0.80 (0.00) 0.75 (0.01) 0.81 (0.00) 0.73 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.08 (0.05) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.65 (0.01) 0.74 (0.01) 0.44 (0.01) 0.69 (0.00) 0.75 (0.01) 0.68 (0.01) 0.70 (0.01) 0.71 (0.00) 0.53 (0.01) 0.66 (0.01) 0.59 (0.01) 0.60 (0.01) 0.75 (0.00) 0.66 (0.00) 0.65 (0.01) 0.70 (0.01) 0.78 (0.01) 0.56 (0.01) 0.75 (0.01) 0.78 (0.02) 0.69 (0.01) 0.70 (0.01) 0.83 (0.00) 0.67 (0.01) 0.67 (0.00) 0.78 (0.00) 0.33 (0.01) 0.17 (0.02) 0.78 (0.00) 0.76 (0.00) 0.82 (0.00) 0.62 (0.00) 35 Table 10: Full Results for VLM-as-planner on ViPlan-HH. Success rate indicates the fraction of problems solved. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded."
        },
        {
            "title": "Hard",
            "content": "AyaVision 32B AyaVision 32B AyaVision 8B AyaVision 8B DeepSeek-VL2 DeepSeek-VL2 GPT-4.1 GPT-4.1 GPT-4.1 Nano GPT-4.1 Nano Gemma-3 12B Gemma-3 12B Gemma-3 27B Gemma-3 27B InternVL3 78B InternVL3 78B InternVL3 8B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 72B LLaVA-Onevision 7B LLaVA-Onevision 7B Mistral-Small-3.1 24B Mistral-Small-3.1 24B Molmo 7B Molmo 7B Phi-4 Multimodal Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 72B Qwen2.5-VL 7B Qwen2.5-VL 7B 0.36 (0.10) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.88 (0.06) 0.60 (0.10) 0.08 (0.05) 0.16 (0.07) 0.64 (0.10) 0.48 (0.10) 0.92 (0.05) 0.76 (0.09) 0.44 (0.10) 0.36 (0.10) 0.16 (0.07) 0.44 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.68 (0.09) 0.44 (0.10) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.68 (0.09) 0.44 (0.10) 0.16 (0.07) 0.24 (0.09) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.52 (0.10) 0.44 (0.10) 0.04 (0.04) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.32 (0.09) 0.16 (0.07) 0.44 (0.10) 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.32 (0.09) 0.28 (0.09) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.32 (0.09) 0.16 (0.07) 0.12 (0.06) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.36 (0.10) 0.32 (0.09) 0.00 (0.00) 0.04 (0.04) 0.20 (0.08) 0.00 (0.00) 0.28 (0.09) 0.04 (0.04) 0.20 (0.08) 0.08 (0.05) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.32 (0.09) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.32 (0.09) 0.08 (0.05) 0.00 (0.00) Action failures on ViPlan-BW While the main ViPlan-BW experiments were performed without action failures, the possibility of actions going wrong in the real world is motivating example particularly for the VLM-as-grounder evaluation setting, as verifying preconditions and effects is natural way of counteracting action failures. We opted to exclude this from the main results as, with action failures, task can fail either due to the VLMs poor performance or simply due to poor luck, which in turn makes comparing different models difficult. However, to test the robustness to action failures of the approach, we report the results for the VLM-as-grounder setting with probability of action failure pf = 0.1, which we implement into our simulator, in Table 11 for VLM-as-grounder and in Table 12 for VLM-as-planner. Overall, we observe similar trends as the version with no failures  (Table 7)  , showing the resilience of the VLM-as-grounder setting, and validating the no-failure choice for the main benchmark. 36 Table 11: Full Results for VLM-as-grounder on ViPlan-BW with action failures. Actions have 10% chance of failing. Success rate indicates the fraction of problems solved, accuracy is the fraction of correct predictions for single predicate. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded. Model AyaVision 32B AyaVision 8B DeepSeek-VL2 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B CoT Simple Success Accuracy Medium Success Accuracy Hard Success Accuracy 0.04 (0.04) 0.04 (0.04) 0.00 (0.00) 0.16 (0.07) 0.04 (0.04) 0.20 (0.08) 0.92 (0.05) 0.32 (0.09) 0.00 (0.00) 0.00 (0.00) 0.36 (0.10) 0.44 (0.10) 0.00 (0.00) 0.52 (0.10) 0.76 (0.09) 0.88 (0.01) 0.68 (0.01) 0.91 (0.01) 0.89 (0.01) 0.96 (0.00) 0.96 (0.00) 0.99 (0.00) 0.97 (0.00) 0.95 (0.01) 0.92 (0.01) 0.99 (0.00) 0.94 (0.00) 0.92 (0.01) 0.98 (0.00) 0.98 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.84 (0.07) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.04 (0.04) 0.00 (0.00) 0.44 (0.10) 0.12 (0.06) 0.84 (0.00) 0.70 (0.01) 0.88 (0.01) 0.84 (0.00) 0.95 (0.00) 0.94 (0.00) 0.99 (0.00) 0.95 (0.00) 0.94 (0.00) 0.88 (0.01) 0.96 (0.00) 0.93 (0.00) 0.90 (0.00) 0.99 (0.00) 0.97 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.76 (0.09) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.08 (0.05) 0.00 (0.00) 0.08 (0.05) 0.00 (0.00) 0.76 (0.00) 0.76 (0.01) 0.88 (0.01) 0.84 (0.00) 0.91 (0.00) 0.92 (0.01) 0.98 (0.00) 0.92 (0.00) 0.93 (0.00) 0.86 (0.01) 0.95 (0.00) 0.92 (0.00) 0.83 (0.01) 0.97 (0.00) 0.95 (0.00) 37 Table 12: Full Results for VLM-as-planner on ViPlan-BW with action failures. Actions have 10% chance of failing. Success rate indicates the fraction of problems solved. The result is an average over the 25 problems in each split. Standard error of the mean is reported in parenthesis. The highest value in each column is bolded. Model"
        },
        {
            "title": "CoT",
            "content": "AyaVision 32B AyaVision 8B DeepSeek-VL2 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.08 (0.05) 0.20 (0.08) 0.24 (0.09) 0.08 (0.05) 0.04 (0.04) 0.00 (0.00) 0.00 (0.00) 0.20 (0.08) 0.00 (0.00) 0.00 (0.00) 0.08 (0.05) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00)"
        },
        {
            "title": "L Predicate accuracy results",
            "content": "We report the accuracy for individual predicates on ViPlan-BW in Tables 13 (No CoT) and 14 (CoT), and on ViPlan-HH in Tables 15 and 16 (CoT). Some predicates are clearly harder than others to predict: in ViPlan-BW, the clear predicate tends to show much worse performance compared to the others, with some models even approaching random chance (0.50). clear indicates block that can be moved, and is translated to \"Is the block the topmost of its column?\", which is harder question compared to the other predicates, which are more straight-forward. This is an example of what is sometimes known as derived predicate, as it could also be obtained with combination of forall and on, which confirms that predicates encoding higher-level relationships can be more challenging also for VLMs. The accuracy also tends to worsen as the splits increase in difficulty, which suggests that having more objects in the image (as harder splits have more blocks) results in harder task for the VLM. The performance is overall worse for ViPlan-HH, where many predicates, such as nextto, open and reachable fail to reach 90% accuracy even for the best-performing models. This can be attributed to the ambiguity of the domain: if in ViPlan-BW all predicates are distinctly and correctly identifiable, asking if an object is reachable by the agent or next to another object can require degree of interpretation, which is challenging for VLMs. More well-defined predicates, such as holding and inside show higher accuracy, confirming this hypothesis. Furthermore, our experiments also measure the individual performance by ground truth answer (yesno), which we omit from this manuscript for brevity. This reveals that some models, like Molmo and the AyaVision family, have strong bias towards answering \"yes\", while others like DeepSeek-VL2 tend to always answer \"no\", leading to poor performance. 39 Table 13: Individual Predicate Accuracy for VLM-as-grounder on ViPlan-BW. The table shows the accuracy for each predicate in each split. Bolded values show the best accuracy for each predicate and split. Standard error of the mean is reported in parenthesis. Model Split clear incolumn leftof on AyaVision 32B AyaVision 8B DeepSeek-VL GPT-4.1 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B 0.70 (0.03) Simple Medium 0.74 (0.02) 0.59 (0.02) Hard Simple 0.78 (0.02) Medium 0.62 (0.04) 0.57 (0.03) Hard Simple 0.23 (0.05) Medium 0.33 (0.04) 0.54 (0.04) Hard Simple 0.99 (0.01) Medium 0.93 (0.02) 0.95 (0.01) Hard 0.67 (0.05) Simple Medium 0.70 (0.02) 0.54 (0.02) Hard Simple 0.57 (0.06) Medium 0.64 (0.04) 0.69 (0.03) Hard Simple 0.85 (0.03) Medium 0.62 (0.04) 0.69 (0.04) Hard 1.00 (0.00) Simple Medium 0.99 (0.01) 0.96 (0.01) Hard Simple 0.73 (0.03) Medium 0.66 (0.03) 0.58 (0.03) Hard Simple 0.49 (0.06) Medium 0.47 (0.05) 0.68 (0.04) Hard Simple 0.40 (0.06) Medium 0.36 (0.04) 0.52 (0.03) Hard Simple 0.89 (0.03) Medium 0.72 (0.04) 0.75 (0.03) Hard Simple 0.77 (0.02) Medium 0.81 (0.01) 0.77 (0.01) Hard Simple 0.44 (0.05) Medium 0.36 (0.04) 0.54 (0.03) Hard Simple 0.95 (0.02) Medium 0.97 (0.01) 0.94 (0.02) Hard Simple 0.95 (0.02) Medium 0.87 (0.03) 0.83 (0.03) Hard 0.77 (0.02) 0.76 (0.01) 0.79 (0.01) 0.72 (0.01) 0.75 (0.01) 0.81 (0.01) 0.95 (0.01) 0.95 (0.01) 0.98 (0.01) 0.99 (0.01) 0.98 (0.00) 0.97 (0.00) 0.97 (0.01) 0.91 (0.01) 0.94 (0.01) 0.94 (0.01) 0.94 (0.01) 0.91 (0.01) 0.96 (0.01) 0.96 (0.01) 0.94 (0.01) 0.99 (0.01) 1.00 (0.00) 0.99 (0.00) 0.97 (0.01) 0.95 (0.01) 0.97 (0.00) 0.94 (0.01) 0.91 (0.01) 0.94 (0.01) 0.90 (0.02) 0.90 (0.01) 0.94 (0.01) 0.98 (0.01) 0.96 (0.01) 0.98 (0.01) 0.98 (0.00) 0.97 (0.00) 0.95 (0.00) 0.96 (0.01) 0.97 (0.01) 0.95 (0.01) 0.95 (0.01) 0.97 (0.01) 0.96 (0.01) 1.00 (0.00) 0.98 (0.00) 0.97 (0.01) 40 1.00 (0.00) 0.98 (0.00) 1.00 (0.00) 0.70 (0.01) 0.62 (0.02) 0.66 (0.02) 0.92 (0.01) 0.88 (0.01) 0.88 (0.02) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.90 (0.01) 0.79 (0.01) 0.89 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.95 (0.01) 0.93 (0.01) 0.97 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.97 (0.01) 0.96 (0.01) 0.96 (0.01) 1.00 (0.00) 0.98 (0.01) 1.00 (0.00) 0.95 (0.01) 0.96 (0.00) 0.96 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.98 (0.00) 0.90 (0.01) 0.69 (0.02) 0.57 (0.01) 0.55 (0.01) 0.91 (0.01) 0.89 (0.01) 0.91 (0.01) 0.93 (0.02) 0.81 (0.02) 0.82 (0.01) 0.99 (0.01) 0.98 (0.00) 0.98 (0.00) 0.91 (0.02) 0.87 (0.01) 0.79 (0.01) 0.96 (0.01) 0.93 (0.01) 0.88 (0.01) 0.96 (0.01) 0.92 (0.01) 0.90 (0.01) 1.00 (0.00) 0.98 (0.01) 0.96 (0.00) 0.93 (0.01) 0.92 (0.01) 0.88 (0.01) 0.95 (0.01) 0.93 (0.01) 0.91 (0.01) 0.96 (0.01) 0.83 (0.01) 0.79 (0.01) 0.99 (0.01) 0.95 (0.01) 0.94 (0.01) 0.88 (0.01) 0.86 (0.01) 0.88 (0.00) 0.77 (0.02) 0.71 (0.02) 0.64 (0.01) 0.98 (0.01) 0.98 (0.01) 0.96 (0.01) 1.00 (0.00) 0.98 (0.00) 0.96 (0.01) rightof 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.48 (0.01) 0.53 (0.02) 0.53 (0.02) 1.00 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.86 (0.02) 0.86 (0.01) 0.86 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.95 (0.01) 0.95 (0.01) 0.93 (0.01) 1.00 (0.00) 0.99 (0.00) 0.99 (0.00) 0.97 (0.00) 0.97 (0.00) 0.92 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 0.97 (0.01) 0.94 (0.01) Table 14: Individual Predicate Accuracy for VLM-as-grounder on ViPlan-BW with CoT. The table shows the accuracy for each predicate in each split. Bolded values show the best accuracy for each predicate and split. Standard error of the mean is reported in parenthesis. Model Split clear incolumn leftof on AyaVision 32B AyaVision 8B DeepSeek-VL2 GPT-4.1 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B 0.73 (0.03) Simple Medium 0.69 (0.02) 0.53 (0.02) Hard Simple 0.52 (0.06) Medium 0.45 (0.05) 0.53 (0.04) Hard Simple 0.71 (0.03) Medium 0.64 (0.03) 0.60 (0.03) Hard 1.00 (0.00) Simple Medium 0.95 (0.01) 0.96 (0.01) Hard 0.88 (0.03) Simple Medium 0.84 (0.02) 0.84 (0.02) Hard Simple 0.83 (0.03) Medium 0.68 (0.04) 0.72 (0.03) Hard Simple 0.86 (0.03) Medium 0.80 (0.02) 0.84 (0.02) Hard Simple 0.98 (0.02) Medium 1.00 (0.00) 0.99 (0.01) Hard Simple 0.77 (0.04) Medium 0.73 (0.03) 0.61 (0.03) Hard Simple 0.76 (0.04) Medium 0.54 (0.04) 0.73 (0.04) Hard Simple 0.42 (0.06) Medium 0.41 (0.04) 0.63 (0.04) Hard Simple 0.96 (0.02) Medium 0.90 (0.02) 0.87 (0.02) Hard Simple 0.28 (0.05) Medium 0.37 (0.04) 0.49 (0.04) Hard Simple 0.09 (0.03) Medium 0.12 (0.03) 0.11 (0.03) Hard Simple 0.92 (0.03) Medium 0.95 (0.02) 0.93 (0.02) Hard Simple 0.78 (0.04) Medium 0.72 (0.03) 0.76 (0.02) Hard 0.78 (0.02) 0.80 (0.01) 0.84 (0.01) 0.58 (0.03) 0.58 (0.02) 0.65 (0.02) 0.96 (0.01) 0.87 (0.01) 0.95 (0.01) 0.97 (0.01) 0.97 (0.00) 0.96 (0.00) 0.91 (0.01) 0.93 (0.01) 0.92 (0.01) 0.88 (0.01) 0.92 (0.01) 0.91 (0.01) 0.94 (0.01) 0.95 (0.01) 0.94 (0.01) 0.99 (0.01) 0.98 (0.00) 0.98 (0.00) 0.98 (0.01) 0.95 (0.01) 0.96 (0.01) 0.92 (0.01) 0.92 (0.01) 0.93 (0.01) 0.89 (0.02) 0.82 (0.02) 0.92 (0.01) 0.96 (0.01) 0.95 (0.01) 0.97 (0.01) 0.83 (0.02) 0.87 (0.01) 0.80 (0.02) 0.12 (0.02) 0.29 (0.02) 0.35 (0.02) 0.92 (0.01) 0.98 (0.00) 0.94 (0.01) 0.98 (0.01) 0.95 (0.01) 0.97 (0.00) 41 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.46 (0.02) 0.44 (0.02) 0.43 (0.02) 0.92 (0.01) 0.91 (0.01) 0.94 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.97 (0.01) 0.95 (0.01) 0.94 (0.01) 1.00 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 0.96 (0.01) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.78 (0.02) 0.76 (0.02) 0.69 (0.02) 0.04 (0.01) 0.03 (0.01) 0.06 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.74 (0.02) 0.66 (0.01) 0.65 (0.01) 0.85 (0.02) 0.80 (0.02) 0.76 (0.01) 0.77 (0.02) 0.74 (0.01) 0.76 (0.01) 0.99 (0.01) 0.97 (0.00) 0.97 (0.00) 0.91 (0.01) 0.94 (0.01) 0.88 (0.01) 0.84 (0.02) 0.81 (0.02) 0.81 (0.01) 0.89 (0.02) 0.83 (0.01) 0.84 (0.01) 1.00 (0.00) 0.98 (0.00) 0.95 (0.00) 0.82 (0.02) 0.80 (0.01) 0.71 (0.01) 0.93 (0.01) 0.95 (0.01) 0.93 (0.01) 0.93 (0.02) 0.84 (0.01) 0.83 (0.01) 0.95 (0.01) 0.88 (0.01) 0.87 (0.01) 0.93 (0.02) 0.80 (0.02) 0.81 (0.01) 0.19 (0.03) 0.26 (0.02) 0.22 (0.01) 0.97 (0.01) 0.98 (0.00) 0.94 (0.01) 0.95 (0.01) 0.77 (0.01) 0.77 (0.01) rightof 0.98 (0.00) 0.98 (0.00) 0.96 (0.01) 0.30 (0.02) 0.33 (0.02) 0.34 (0.02) 0.96 (0.01) 0.94 (0.01) 0.96 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 0.98 (0.01) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.99 (0.00) 0.99 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.98 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.71 (0.02) 0.64 (0.02) 0.61 (0.02) 0.04 (0.01) 0.05 (0.01) 0.05 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) Table 15: Individual Predicate Accuracy for VLM-as-grounder on ViPlan-HH. The table shows the accuracy for each predicate in each split. Bolded values show the best accuracy for each predicate and split. Standard error of the mean is reported in parenthesis. nextto reachable holding inside Model ontop open Split AyaVision 32B AyaVision 8B DeepSeek-VL2 GPT-4.1 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B Simple 0.79 (0.04) Medium 0.86 (0.03) 0.92 (0.02) Hard Simple 0.59 (0.05) Medium 0.64 (0.03) 0.57 (0.04) Hard 0.91 (0.02) Simple Medium 0.98 (0.00) 0.91 (0.01) Hard Simple 0.79 (0.03) Medium 0.81 (0.02) 0.86 (0.02) Hard Simple 0.85 (0.04) Medium 0.89 (0.02) 0.86 (0.04) Hard 0.63 (0.05) Simple Medium 0.71 (0.03) 0.63 (0.04) Hard Simple 0.80 (0.03) Medium 0.86 (0.01) 0.87 (0.01) Hard Simple 0.89 (0.03) Medium 0.86 (0.02) 0.83 (0.03) Hard 0.91 (0.02) Simple Medium 0.91 (0.01) 0.90 (0.02) Hard Simple 0.87 (0.04) Medium 0.94 (0.01) 0.96 (0.01) Hard Simple 0.87 (0.03) Medium 0.89 (0.02) 0.94 (0.01) Hard 0.98 (0.01) Simple Medium 1.00 (0.00) 0.99 (0.00) Hard Simple 0.88 (0.03) Medium 0.86 (0.02) 0.90 (0.01) Hard Simple 0.51 (0.06) Medium 0.69 (0.03) 0.56 (0.03) Hard 0.95 (0.01) Simple Medium 0.95 (0.01) 0.95 (0.01) Hard Simple 0.96 (0.01) Medium 0.91 (0.01) 0.96 (0.01) Hard 0.79 (0.04) 0.76 (0.03) 0.57 (0.03) 0.61 (0.05) 0.52 (0.03) 0.42 (0.03) 0.81 (0.03) 0.90 (0.02) 0.93 (0.01) 0.80 (0.03) 0.94 (0.02) 0.79 (0.03) 0.66 (0.05) 0.83 (0.03) 0.74 (0.03) 0.71 (0.07) 0.79 (0.03) 0.76 (0.03) 0.80 (0.04) 0.95 (0.01) 0.85 (0.01) 0.82 (0.03) 0.87 (0.02) 0.83 (0.03) 0.83 (0.02) 0.89 (0.01) 0.93 (0.02) 0.48 (0.09) 0.91 (0.01) 0.81 (0.03) 0.82 (0.03) 0.96 (0.01) 0.88 (0.03) 0.84 (0.04) 1.00 (0.00) 0.96 (0.00) 0.75 (0.04) 0.60 (0.03) 0.69 (0.01) 0.39 (0.05) 0.38 (0.03) 0.43 (0.03) 0.75 (0.05) 0.88 (0.01) 0.83 (0.01) 0.89 (0.04) 0.92 (0.01) 0.90 (0.01) 42 0.34 (0.03) 0.40 (0.01) 0.55 (0.02) 0.12 (0.02) 0.14 (0.01) 0.19 (0.02) 0.54 (0.02) 0.59 (0.00) 0.45 (0.01) 0.52 (0.02) 0.55 (0.01) 0.56 (0.02) 0.18 (0.02) 0.30 (0.01) 0.21 (0.02) 0.23 (0.03) 0.61 (0.01) 0.39 (0.02) 0.52 (0.03) 0.63 (0.01) 0.63 (0.01) 0.59 (0.02) 0.48 (0.01) 0.43 (0.02) 0.61 (0.02) 0.65 (0.01) 0.71 (0.01) 0.50 (0.04) 0.68 (0.01) 0.59 (0.01) 0.28 (0.02) 0.41 (0.01) 0.45 (0.01) 0.72 (0.02) 0.79 (0.01) 0.78 (0.00) 0.51 (0.03) 0.55 (0.01) 0.62 (0.00) 0.20 (0.02) 0.11 (0.01) 0.24 (0.01) 0.79 (0.02) 0.81 (0.01) 0.72 (0.00) 0.73 (0.02) 0.77 (0.01) 0.80 (0.00) 0.78 (0.03) 0.87 (0.01) 0.78 (0.01) 0.57 (0.03) 0.67 (0.01) 0.63 (0.02) 0.85 (0.02) 0.92 (0.00) 0.89 (0.01) 0.83 (0.02) 0.89 (0.01) 0.86 (0.01) 0.73 (0.02) 0.84 (0.01) 0.69 (0.02) 0.65 (0.03) 0.76 (0.01) 0.76 (0.02) 0.92 (0.01) 0.94 (0.00) 0.93 (0.00) 0.77 (0.02) 0.83 (0.01) 0.83 (0.02) 0.84 (0.01) 0.91 (0.01) 0.90 (0.01) 0.84 (0.03) 0.94 (0.01) 0.88 (0.01) 0.81 (0.02) 0.88 (0.01) 0.94 (0.01) 0.87 (0.01) 0.94 (0.00) 0.93 (0.00) 0.58 (0.03) 0.68 (0.01) 0.70 (0.00) 0.14 (0.02) 0.18 (0.01) 0.18 (0.01) 0.83 (0.02) 0.93 (0.01) 0.91 (0.00) 0.86 (0.02) 0.96 (0.00) 0.93 (0.00) 0.30 (0.05) 0.47 (0.07) 0.49 (0.04) 0.39 (0.06) 0.33 (0.05) 0.79 (0.04) 0.46 (0.05) 0.77 (0.04) 0.88 (0.02) 0.50 (0.04) 0.74 (0.04) 0.77 (0.04) 0.49 (0.06) 0.62 (0.06) 0.64 (0.06) 0.27 (0.05) 0.44 (0.06) 0.68 (0.05) 0.46 (0.05) 0.54 (0.03) 0.38 (0.03) 0.38 (0.04) 0.37 (0.04) 0.79 (0.05) 0.36 (0.04) 0.55 (0.03) 0.62 (0.05) 0.20 (0.06) 0.60 (0.03) 0.60 (0.06) 0.64 (0.04) 0.73 (0.03) 0.84 (0.04) 0.67 (0.03) 0.73 (0.02) 0.72 (0.01) 0.29 (0.05) 0.54 (0.04) 0.61 (0.03) 0.33 (0.06) 0.26 (0.04) 0.59 (0.04) 0.60 (0.04) 0.51 (0.03) 0.64 (0.02) 0.72 (0.03) 0.44 (0.03) 0.60 (0.02) 0.71 (0.03) 0.64 (0.03) 0.58 (0.02) 0.68 (0.04) 0.63 (0.03) 0.38 (0.03) 0.69 (0.03) 0.50 (0.01) 0.83 (0.01) 0.66 (0.03) 0.72 (0.02) 0.49 (0.03) 0.67 (0.04) 0.55 (0.03) 0.53 (0.04) 0.70 (0.04) 0.64 (0.02) 0.43 (0.03) 0.50 (0.03) 0.55 (0.02) 0.50 (0.01) 0.76 (0.03) 0.74 (0.02) 0.42 (0.03) 0.76 (0.02) 0.71 (0.02) 0.44 (0.02) 0.64 (0.04) 0.63 (0.02) 0.68 (0.03) 0.44 (0.03) 0.52 (0.02) 0.45 (0.03) 0.28 (0.02) 0.40 (0.01) 0.40 (0.01) 0.65 (0.04) 0.76 (0.02) 0.71 (0.01) 0.63 (0.04) 0.57 (0.02) 0.63 (0.03) 0.35 (0.02) 0.36 (0.01) 0.39 (0.01) 0.47 (0.02) 0.43 (0.01) 0.37 (0.01) Table 16: Individual Predicate Accuracy for VLM-as-grounder on ViPlan-HH with CoT. The table shows the accuracy for each predicate in each split. Bolded values show the best accuracy for each predicate and split. Standard error of the mean is reported in parenthesis. holding reachable inside nextto Model ontop open Split AyaVision 32B AyaVision 8B DeepSeek-VL2 GPT-4.1 GPT-4.1 Nano Gemma-3 12B Gemma-3 27B InternVL3 78B InternVL3 8B LLaVA-Onevision 72B LLaVA-Onevision 7B Mistral-Small-3.1 24B Molmo 7B Phi-4 Multimodal Qwen2.5-VL 72B Qwen2.5-VL 7B Simple 0.88 (0.03) Medium 0.86 (0.02) 0.87 (0.02) Hard Simple 0.62 (0.04) Medium 0.52 (0.03) 0.57 (0.02) Hard 0.72 (0.05) Simple Medium 0.67 (0.02) 0.42 (0.03) Hard Simple 0.87 (0.03) Medium 0.85 (0.02) 0.92 (0.01) Hard Simple 0.79 (0.04) Medium 0.82 (0.02) 0.71 (0.03) Hard 0.64 (0.05) Simple Medium 0.55 (0.03) 0.48 (0.03) Hard Simple 0.83 (0.03) Medium 0.64 (0.02) 0.79 (0.02) Hard Simple 0.87 (0.03) Medium 0.76 (0.02) 0.95 (0.01) Hard 0.73 (0.04) Simple Medium 0.67 (0.02) 0.82 (0.02) Hard Simple 0.86 (0.04) Medium 0.95 (0.01) 0.89 (0.07) Hard Simple 0.91 (0.03) Medium 0.89 (0.02) 0.95 (0.01) Hard 0.77 (0.04) Simple Medium 0.59 (0.03) 0.70 (0.04) Hard 0.99 (0.01) Simple Medium 0.72 (0.02) 0.96 (0.01) Hard Simple 0.00 (0.00) Medium 0.00 (0.00) 0.22 (0.08) Hard 0.90 (0.02) Simple Medium 0.87 (0.02) 0.92 (0.01) Hard Simple 0.84 (0.03) Medium 0.72 (0.02) 0.61 (0.01) Hard 0.85 (0.03) 0.93 (0.01) 0.80 (0.02) 0.70 (0.03) 0.72 (0.03) 0.85 (0.01) 0.79 (0.06) 0.97 (0.01) 0.85 (0.02) 0.68 (0.04) 0.91 (0.01) 0.78 (0.02) 0.88 (0.03) 0.92 (0.01) 0.78 (0.02) 0.83 (0.04) 0.97 (0.01) 0.89 (0.01) 0.73 (0.03) 0.97 (0.01) 0.96 (0.01) 0.73 (0.05) 0.95 (0.01) 0.94 (0.01) 0.76 (0.04) 0.85 (0.02) 0.52 (0.02) 0.50 (0.13) 0.94 (0.01) 0.61 (0.09) 0.74 (0.05) 0.94 (0.02) 0.92 (0.02) 0.67 (0.05) 0.89 (0.02) 0.84 (0.02) 0.95 (0.04) 0.92 (0.02) 0.90 (0.01) 0.00 (0.00) 0.17 (0.11) 0.26 (0.08) 0.82 (0.03) 0.95 (0.01) 0.97 (0.01) 0.84 (0.03) 0.98 (0.00) 0.96 (0.00) 0.66 (0.02) 0.58 (0.01) 0.59 (0.01) 0.47 (0.02) 0.57 (0.01) 0.49 (0.01) 0.61 (0.03) 0.51 (0.01) 0.61 (0.01) 0.60 (0.02) 0.51 (0.01) 0.55 (0.01) 0.60 (0.03) 0.57 (0.01) 0.58 (0.01) 0.37 (0.03) 0.38 (0.01) 0.38 (0.01) 0.61 (0.02) 0.39 (0.01) 0.33 (0.01) 0.50 (0.03) 0.45 (0.01) 0.40 (0.01) 0.53 (0.02) 0.45 (0.01) 0.40 (0.01) 0.66 (0.04) 0.63 (0.01) 0.76 (0.04) 0.32 (0.03) 0.38 (0.01) 0.45 (0.01) 0.44 (0.03) 0.46 (0.01) 0.59 (0.02) 0.49 (0.02) 0.56 (0.01) 0.75 (0.01) 0.00 (0.00) 0.01 (0.00) 0.02 (0.01) 0.65 (0.02) 0.66 (0.01) 0.66 (0.01) 0.55 (0.03) 0.52 (0.01) 0.46 (0.00) 0.82 (0.02) 0.88 (0.01) 0.92 (0.01) 0.92 (0.01) 0.90 (0.01) 0.89 (0.00) 0.76 (0.03) 0.83 (0.01) 0.86 (0.01) 0.81 (0.02) 0.87 (0.01) 0.85 (0.00) 0.78 (0.02) 0.85 (0.01) 0.73 (0.01) 0.83 (0.02) 0.69 (0.01) 0.84 (0.01) 0.91 (0.01) 0.89 (0.01) 0.89 (0.00) 0.75 (0.02) 0.86 (0.01) 0.86 (0.01) 0.67 (0.02) 0.65 (0.01) 0.61 (0.01) 0.87 (0.03) 0.94 (0.01) 0.92 (0.03) 0.77 (0.02) 0.87 (0.01) 0.93 (0.01) 0.69 (0.03) 0.79 (0.01) 0.79 (0.02) 0.80 (0.02) 0.87 (0.01) 0.87 (0.01) 0.00 (0.00) 0.02 (0.01) 0.27 (0.05) 0.80 (0.02) 0.86 (0.01) 0.82 (0.01) 0.83 (0.02) 0.90 (0.00) 0.75 (0.00) 0.51 (0.04) 0.82 (0.03) 0.80 (0.03) 0.47 (0.04) 0.45 (0.05) 0.70 (0.03) 0.53 (0.06) 0.61 (0.04) 0.88 (0.02) 0.52 (0.05) 0.70 (0.03) 0.63 (0.04) 0.34 (0.05) 0.55 (0.04) 0.53 (0.03) 0.58 (0.05) 0.70 (0.04) 0.38 (0.03) 0.43 (0.04) 0.69 (0.03) 0.80 (0.03) 0.41 (0.05) 0.75 (0.04) 0.70 (0.04) 0.36 (0.05) 0.36 (0.04) 0.67 (0.04) 0.27 (0.06) 0.61 (0.04) 0.53 (0.13) 0.55 (0.05) 0.69 (0.04) 0.79 (0.04) 0.43 (0.05) 0.40 (0.05) 0.81 (0.03) 0.45 (0.06) 0.77 (0.04) 0.36 (0.02) 0.67 (0.19) 0.67 (0.19) 0.64 (0.14) 0.53 (0.04) 0.59 (0.04) 0.76 (0.02) 0.49 (0.04) 0.67 (0.03) 0.45 (0.02) 0.62 (0.03) 0.74 (0.02) 0.52 (0.02) 0.67 (0.03) 0.59 (0.02) 0.68 (0.02) 0.74 (0.03) 0.80 (0.01) 0.34 (0.02) 0.66 (0.03) 0.69 (0.02) 0.64 (0.02) 0.60 (0.03) 0.65 (0.02) 0.62 (0.02) 0.71 (0.03) 0.59 (0.02) 0.41 (0.02) 0.50 (0.02) 0.81 (0.01) 0.82 (0.01) 0.72 (0.03) 0.73 (0.02) 0.92 (0.01) 0.68 (0.03) 0.75 (0.02) 0.75 (0.02) 0.59 (0.04) 0.67 (0.02) 0.69 (0.07) 0.45 (0.03) 0.53 (0.02) 0.49 (0.02) 0.65 (0.03) 0.73 (0.02) 0.40 (0.03) 0.63 (0.03) 0.59 (0.02) 0.56 (0.01) 0.00 (0.00) 0.10 (0.04) 0.05 (0.03) 0.54 (0.02) 0.58 (0.02) 0.68 (0.01) 0.61 (0.03) 0.65 (0.01) 0.58 (0.01)"
        }
    ],
    "affiliations": [
        "Department of Computer Science, Aalto University",
        "Department of Mathematics, Universit√† degli Studi di Padova",
        "Fondazione Bruno Kessler"
    ]
}