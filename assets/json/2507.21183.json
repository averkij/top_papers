{
    "paper_title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
    "authors": [
        "Guangchen Lan",
        "Sipeng Zhang",
        "Tianle Wang",
        "Yuwei Zhang",
        "Daoan Zhang",
        "Xinpeng Wei",
        "Xiaoman Pan",
        "Hongming Zhang",
        "Dong-Jun Han",
        "Christopher G. Brinton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 8 1 1 2 . 7 0 5 2 : r MaPPO: Maximum Posteriori Preference Optimization with Prior Knowledge Guangchen Lan1 , Sipeng Zhang2 , Tianle Wang2, Yuwei Zhang2, Daoan Zhang3, Xinpeng Wei4, Xiaoman Pan5, Hongming Zhang5, Dong-Jun Han6, Christopher G. Brinton1 1 Purdue University 2 University of California, San Diego 4 Georgia Institute of Technology 5 Tencent AI Lab 6 Yonsei University 3 University of Rochester 1 lan44@purdue.edu"
        },
        {
            "title": "Abstract",
            "content": "As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become central approach to aligning LLMs with human preferences and improving performance. We propose Maximum Posteriori Preference Optimization (MaPPO), framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into principled Maximum Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency."
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as general paradigm for aligning large language models (LLMs) with human preferences. Pioneering work framed the problem as reinforcement learning (RL) on reward model trained from group-wise comparisons, yielding notable improvements in summarization and dialogue (Christiano et al., 2017; Stiennon et al., 2020). Subsequent systems such as InstructGPT (Ouyang et al., 2022) demonstrated that RLHF could scale to billion-parameter models and substantially boost helpfulness and safety (Lan et al., 2025). Despite its practical successes, RLHF still suffers from noisy feedback, reward-model misalignment, optimization instability, and computation inefficiency with high memory cost, which together hinder its scalability and reliability (Casper et al., 2023; Dai et al., 2023; Pan et al., 2022). Direct Preference Optimization (DPO) (Rafailov et al., 2023) reframes the preference learning with KullbackLeibler (KL) regularized objective as log-odds objective, effectively turning the task into Maximum Likelihood Estimation (MLE) over pairwise comparisons: The model is trained to assign higher likelihood to the preferred response than to the rejected, while staying close to reference policy. The MLE perspective accounts for the efficiency of DPO, as it eliminates the need for rollouts or value functions. However, this Authors contributed equally. 1 formulation also introduces fundamental limitation: It considers only the relative likelihoods within each pair, overlooking the absolute reward magnitude and any external prior knowledge (Amini et al., 2024; DOosterlinck et al., 2025). As result, the training signal in DPO is inherently local, bounded by pairwise comparisons, and lacks global calibration across examples. Challenge. fundamental limitation of MLE-based preference optimization lies in its purely relative nature: It focuses on maximizing the gap between chosen and rejected responses, yet lacks mechanism to anchor their absolute probabilities. As training progresses, the MLE objective tends to suppress the likelihood of the rejected response rather than elevate that of the preferred one. Empirical investigations (Pal et al., 2024; Rafailov et al., 2024; Tajwar et al., 2024; Zhang et al., 2024) consistently show simultaneous reduction in the absolute probabilities assigned to both preferred and rejected answers, resulting in abnormal output distributions. This undesirable dynamic, empirically known as the squeezing effect (Ren & Sutherland, 2024), undermines policy calibration and injects instability into generation. The issue is especially severe in near-tie cases (in Figure 1), especially when models approach human-level performance (Liu et al., 2024a; Guo et al., 2024), where both responses are reasonable yet MLE still enforces an artificial separation, draining probability mass from the high-quality region of the output space. Motivated by this, the key question that this paper aims to address is: How can we improve language model alignment through more principled training signal, instead of an oversimplified MLE pipeline? In this paper, we answer the above question by introducing Maximum-a-Posteriori Preference Optimization (MaPPO), simple yet principled extension of DPO that injects data-driven prior knowledge into preference training. MaPPO augments the standard maximum-likelihood objective with lightweight MaP regularizer, an additional log-prior scaled by calibrated reward gap, which proportionally adjusts each update to the confidence difference between the preferred and rejected answers. Instead of the oversimplified binary classification in MLE, this mechanism curbs the excessive penalization of near-tie pairs while preserving DPOs one-step closed form and computational efficiency. Extensive experiments demonstrate that MaPPO delivers consistently stronger performance across three public alignment benchmarks: AlpacaEval 2.0, Arena-Hard, and MT-Bench. We evaluate MaPPO on multiple model families, including Llama-3, Qwen2.5, and Mistral, under multiple model sizes. Compared to DPO, MaPPO achieves absolute win-rate gains of 94.3% on AlpacaEval and 37.1% on Arena-Hard when fine-tuned on the Mistral-7B-Instruct model. Moreover, the proposed MaPPO is suitable for both offline and online settings. These results validate that lightweight prior is sufficient to produce stronger and better-calibrated policies. Furthermore, MaPPO is designed as drop-in regularization module and seamlessly integrates with broad spectrum of recent DPO variants, including Iterative-DPO (Dong et al., 2024), SimPO (Meng et al., 2024), IPO (Gheshlaghi Azar et al., 2024), and CPO (Xu et al., 2024). In all cases, we observe consistent gains up to 31.3% on Arena-Hard in alignment scores without requiring additional computation or changes to the optimization pipeline. This suggests that MaPPO serves as robust and general enhancement strategy for advanced preference training pipelines. Contributions. In summary, the main contributions of this work are as follows: 1. We propose MaPPO, principled extension of Direct Preference Optimization, which incorporates data-driven prior reward estimates into Maximum-a-Posteriori (MaP) objective. 2. We demonstrate that MaPPO naturally supports both offline (e.g., DPO) and online (e.g., I-DPO) preference optimization. 3. We show that MaPPO is compatible with and enhances existing DPO variants, including SimPO, IPO, and CPO. For all variants, no additional hyperparameter is needed. 4. Empirical results across multiple model series and model sizes confirm consistent improvements in alignment performance on standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard. 2 1+ex . 1[condition] is an indicator Notations. We use σ() to denote the logistic (sigmoid) function σ(x) = 1 function, and the value is 1 if the condition is true, and 0 otherwise. For preference pairs, yw denotes the chosen (winning) response, while yl denotes the rejected (losing) response."
        },
        {
            "title": "2 Related Work",
            "content": "Direct Preference Optimization and its Variants. Driven by the complexity of online RLHF algorithms (Santacroce et al., 2023; Zheng et al., 2023b), recent research has pivoted toward efficient offline preference optimization. Direct Preference Optimization (DPO) (Rafailov et al., 2023) frames preference alignment as maximum-likelihood estimation (MLE) under the Bradley-Terry (BT) model (Bradley & Terry, 1952), while IPO (Gheshlaghi Azar et al., 2024) generalizes this framework without the pointwise-reward assumption. Further, CPO (Xu et al., 2024) jointly optimizes the sequence likelihood and contrastive reward to perform supervised fine-tuning (SFT) and alignment in one pass. KTO (Ethayarajh et al., 2024) extends the paradigm to single-response feedback via prospect theoretic utility. Recent DPO variants, ORPO (Hong et al., 2024), R-DPO (Park et al., 2024), and SimPO (Meng et al., 2024), further push performance by discarding the reference model or regularizing response length. Yang et al. (2025) further introduces weight hyperparameter to balance the influence of preference pairs from different policies. Zhao et al. (2025) then aims to combine the previous PO methods into one cohesive objective. However, all DPO-style variants rely on MLE in the training process, which oversimplifies the tuning of preferred and unpreferred responses as binary classification problem. Confidence Degeneration in DPO. Pal et al. (2024) and Tajwar et al. (2024) show analytically and empirically that the expected DPO gradient often decreases the log-likelihood of the preferred response yw instead of increasing it, leading to simultaneous shrinkage of both responses. Rafailov et al. (2024) observe the same trend, attributing the drop to the expected log ratio between the optimized and reference models. By showing that this is equivalent to the non-negative KL divergence, they conclude that DPO training inevitably lowers the likelihood of the chosen response. More recent analyses of the learning dynamics in Ren & Sutherland (2024) have identified phenomenon termed the squeezing effect, whereby DPO training aggressively drains probability mass from all responses except the most confident one, = arg maxi[V ]yl πθ(y = i), consequently funneling this mass towards y. Our method utilizes prior knowledge to soften the downward pressure on the rejected response yl, it markedly mitigates the squeezing effect."
        },
        {
            "title": "3 Preliminary & Problem Setup",
            "content": "3.1 RL Tuning First, we introduce the general framework of Reinforcement Learning (RL). Consider the Markov decision process (MDP) as tuple (S, A, P, R), where is the state space, is finite action space, : AS is Markov kernel that determines transition probabilities, and : is reward function. At each time step t, the agent executes an action yt from the current state st S, following stochastic policy π, i.e., yt π(st). The corresponding reward is defined as rt. Following the conventional setting in LLMs, the policy πθ represents the LLM with model parameters θ. The action space is set as the vocabulary. At step t, st = (x, y<t) is cascade of the query and the tokens y<t = (y1, , yt1) that have been predicted, and yt is the next token to be predicted. The transition kernel is deterministic as st+1 = (st, yt). The complete answer = (y1, , yT ) with length = . The step reward rt = r(x, yt) can be obtained from trained reward model. After formalizing the LLM tuning as an RL problem, the goal of RL tuning (Ouyang et al., 2022) is to maximize the expectation of the cumulative reward := r(x, y) with KullbackLeibler (KL) constraint as follows max πθ xD,yπθ(x) [r(x, y)] βDKL [πθ(x)πref (x)] , (1) 3 where DKL() denotes the KL divergence, and β is constant weight. πref is reference model, which is usually the initial policy model before tuning. This optimization problem can be solved by any RL algorithms, e.g., PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024)."
        },
        {
            "title": "3.2 Direct Preference Optimization",
            "content": "In Direct Preference Optimization (DPO) (Rafailov et al., 2023), closed-form expression of equation 1 is used, and connection between policy π and reward function is built as π(yx) ="
        },
        {
            "title": "1\nZ(x)",
            "content": "πref (yx) exp (cid:0) 1 β r(y, x)(cid:1), (2) where Z(x) is partition function to normalize the probability. With prompt x, we sample two responses from the current policy model y1, y2 πθ(x). human expert then demonstrates the preference and ranks the responses as yw (win) and yl (lose). After plugging in equation 2 into the reward model training (MLE) loss function, the target of RL tuning becomes to minimize the loss function shown below L(θ) = (yw,yl,x)D log σ (cid:16) β log πθ(ywx) πref (ywx) β log πθ(ylx) πref (ylx) (cid:17)i , (3) where σ() is the logistic function. The training process can be done using mini-batch gradient descent and the variants, e.g., AdamW (Loshchilov & Hutter, 2019). Notably, the derivation on reward model training is based on MLE, which oversimplifies the process as binary classification problem. As result, minimizing equation 3 is nothing but increasing the gap between the output probability of yw and yl. 3.3 Current issues with MLE Figure 1: Under the standard MLE-based DPO (left), empirical studies (Pal et al., 2024; Rafailov et al., 2024; Tajwar et al., 2024; Ren & Sutherland, 2024) demonstrated that training tends to simultaneously downscale (with different magnitudes) both the chosen and rejected responses to increase their gap. Our MaP-based method (right) mitigates this harmful tendency by re-weighting the rejected response based on prior knowledge. Here, the x-axis denotes the initial model θ1 and potentially harmful model θ2 that may arise during training, while the y-axis shows the log-likelihood of fixed preference pair under different policies. The learning process is to estimate the parameters θ via maximum likelihood, which is to maximize the gap of the output probability between the winning response πθ(ywx) and the losing response πθ(ylx). Despite its computational efficiency and simplicity, the MLE formulation introduces critical limitation: It focuses solely on relative probabilities within each preference pair, while ignoring the absolute magnitude 4 of confidence in the responses. Shown as Figure 1, training encourages simultaneous downscaling of both πθ(ywx) and πθ(ylx), to enlarge the preference gap. This undesired tendency can lead to over-penalization of both responses, especially in near-tie cases, thereby reducing output confidence and harming policy calibration (Ren & Sutherland, 2024). Such issues become particularly problematic as models approach human-level performance, where both the winning and losing responses may be of high quality. For example, consider preference pair where both yw and yl are grammatically correct, contextually relevant, and factually accurate, with the only preference driven by stylistic nuances. In this case, the MLE objective may still enforce large probability gap by downscaling both log-likelihoods. This inadvertently shifts the probability mass away from the high-quality response space, introducing unnecessary uncertainty into the policy, and can lead to erratic or unstable generation in downstream usage."
        },
        {
            "title": "Prompt x",
            "content": "A printer prints 17 pages per minute. How many minutes will it take to print 200 pages? Express your answer to the nearest whole number. Response yw, = 0.95 To find out how many minutes it will take to print 200 pages, we need to divide the total number of pages by the number of pages the printer can print per minute. So we divide 200 by 17: 200 17 11.76. But since we want the nearest whole number, we need to round it up. That means it will take 12 minutes to print 200 pages. The answer is: 12 Response yl, = 0.91 To find out how many minutes it will take to print 200 pages at rate of 17 pages per minute, divide: 200 17 11.76. Rounding 11.76 to the nearest whole number gives: 12. So it will take 12 minutes to print 200 pages. The answer is: 12 Figure 2: An example of (x, yw, yl) pair. Both responses yw and yl have good quality as they achieve high rewards, where r(x, yw) = 0.95, r(x, yl) = 0.91, and [0, 1]. Table 1: Comparison of log-probabilities for responses yw and yl in PO methods. logp(yl) logp(yw) - logp(yl) logp(yw) Model IT DPO MaPPO 14.3 121.5 10. 43.4 443.2 321.1 +29.1 +321.7 +310.7 In Figure 2, we show an example of the prompt and response pair (x, yw, yl). Given the prompt x, both yw and yl have correct answers, while in slightly different expressions. Both yw and yl have high rewards, which reflect the high qualities. However, in MLE and its derived DPO, the learning objective is nothing but to increase the gap between yw and yl, regardless of the fact that both of them have high qualities with correct answers, and their qualities match each other. As shown in Table 1, after DPO training, the log output probability of yw decreases from 14.3 to 121.5, and yl decreases from 43.4 to 443.2. Although the gap indeed increases from 29.1 to 321.7, it violates the principal goal: Increase the output probability of the high-quality responses. 5 These issues highlight the need for more principled formulation that preserves relative preferences while incorporating global calibration and prior reward knowledge. In the next section, we introduce our Maximuma-Posteriori (MaP) framework that addresses these shortcomings in unified and efficient manner."
        },
        {
            "title": "4.1 MaPPO Loss",
            "content": "In this subsection, we start the derivation step by step from the first principle. With prompt and responses (y1, y2), an oracle gives its preference on the responses as (yw, yl). The Bradley-Terry (BT) model (Bradley & Terry, 1952) builds the connection between the rewards and the preference probability as follows: p(yw ylx) = exp(r(yw, x)) exp(r(yw, x)) + exp(r(yl, x)) = 1 1 + exp(r(yl, x) r(yw, x)) . (4) The preference dataset have samples denoted as = {yi i=1. We can parametrize reward model with model parameters ϕ as rϕ(y, x). Given x, assume we have prior knowledge of rewards as rw and rl. This can be obtained from an oracle, e.g., pre-trained reward model. To incorporate the prior knowledge of rewards, we need to use the gap = rw rl as suggested in equation 4. To keep the softmax form in the BT model, we can construct the prior probability as follows l, xi}N w, yi p(rϕ) = exp(rϕ(yw, x)) + exp(rrϕ(yl, x)) exp(rϕ(yw, x)) + exp(rϕ(yl, x)) . (5) We use the reward gap on the softmax probability to make the probability always greater than 0 and smaller than 1. Notably, this form is not unique, and other forms are also acceptable if they satisfy the properties of the probability function. We further discuss the prior function in Appendix C.1. The MaP loss is the combination of the MLE loss and the prior knowledge loss as follows LMaP(rϕ) = LMLE(rϕ) + Lp(rϕ) = = (yw,yl,x)D (yw,yl,x)D (cid:16) (cid:16) log σ log σ rϕ(yw, x) rϕ(yl, x) (cid:17) log p(rϕ) rϕ(yw, x) rrϕ(yl, x) (cid:17)i . (6) As proved in previous works (Rafailov et al., 2023; Go et al., 2023; Korbak et al., 2022), given reward function r(y, x), we have closed-form expression of the policy π as π(yx) = 1 Z(x) πref (yx) exp (cid:0) 1 β r(y, x)(cid:1), (7) where Z(x) is partition function. With parametrized policy πθ, we can plug this result into the loss function equation 6, and get the MaPPO loss LMaP(θ) = (yw,yl,x)D log σ (cid:16) β log πθ(ywx) πref (ywx) rβ log πθ(ylx) πref (ylx) (cid:17)i . (8) With the MaP estimation, we achieve clean result compared to the MLE estimation in DPO with calibration term [0, 1] from the prior knowledge. Remark. In our MaPPO method, no additional hyperparameter is introduced compared to the original DPO method. Thus, MaPPO offers clean and easily pluggable solution, and no extra hyperparameter tuning is needed."
        },
        {
            "title": "4.2 Analysis of MaPPO",
            "content": "In this subsection, we analyze the connection with MaPPO, DPO, and SFT. Connection with SFT. First, in equation 8, when rw = rl (i.e., = 0), the loss function becomes L(θ) = (yw,yl,x)D log σ (cid:16) β log πθ(ywx) πref (ywx) (cid:17)i , (9) which is equivalent to the SFT loss function, and yw is the supervised target. The reason is that the function log σ() is monotonic. Thus, the optimal solution is equal to that from the loss function below L(θ) = (yw,yl,x)D log πθ(ywx) πref (ywx) . (10)"
        },
        {
            "title": "The gradient is",
            "content": "E (yw,yl,x)D log πθ(ywx) πref (ywx) = (yw,yl,x)D log πθ(ywx) = LSFT(θ). (11) Thus, it degenerates to the SFT loss function, and only differs in learning rates. Notably, with online response data collection (yw is generated from the current policy model πθ), this is also known as the Reject Sampling (RS) method (Dong et al., 2023). Connection with DPO. becomes In equation 8, when rw = 1 and rl = 0, we have = 1. The loss function L(θ) = (yw,yl,x)D log σ (cid:16) β log πθ(ywx) πref (ywx) β log πθ(ylx) πref (ylx) (cid:17)i , (12) which degenerates to the DPO loss function in equation 3. Overall, both DPO and SFT loss functions (I-DPO and RS in the online setting) can be taken as special cases of MaPPO. In this sense, MaPPO can be taken as dynamic weighted mechanism, where the weight depends on the relative quality (rewards) of the winning response yw and the losing response yl. Gradient Dynamics Analysis. To analyze the update of MaPPO, the gradient of equation 8 is LMaP(θ) = E (yw,yl,x)D β(1 σ(u)) (cid:16) log πθ(ywx) log πθ(ylx) (cid:17)i , (13) πref (ywx) log πθ(ylx) (cid:1) serves as confidence measure of preference separation. 1 σ(u) where = β(cid:0) log πθ(ywx) down-weights the gradient when the model is already confident in distinguishing yw and yl. directly scales the contribution of the losing sample yl, modulating the penalization. The gradient norm of MaPPO is upper-bounded compared to DPO, leading to less aggressive updates and more stable policy calibration. We provide more detailed theoretical analysis, including the stationary convergence analysis in Appendix A.1 and Lipschitz stability analysis in Appendix A.2. πref (ylx) 4.3 Online MaPPO Beyond the offline setting, our MaPPO method can be directly used in the online or iterative settings. As shown in Algorithm 1, we describe the online version of MaPPO. In online MaPPO, one key difference is that the responses {y} are generated online from the current policy πθ instead of the initial policy πθ0 in the offline setting. In practice, considering training efficiency, online PO can be implemented in an iterative way, known as I-DPO (Dong et al., 2024). In Figure 3, we illustrate the iterative MaPPO pipeline. With prompt set D, we can equally divide into subsets as D1 DK. In the k-th iteration, we first freeze the current policy model πθ, and then get responses (y1, y2) from the policy according to the prompt set Dk. We then 7 Algorithm 1 Online MaPPO Require: Prompt data set D; Number of iterations K; Initial policy model θ0. 1: for = 0, , 1 do 2: 3: 4: Sample prompt D. Sample responses from the current policy yw, yl πθk (x). Get corresponding rewards rw r(yw, x) and rl r(yl, x). r(yw, x) r(yl, x) Compute L(θk) according to equation 8. θk+1 θk ηL(θk) # Or other optimizer, e.g., AdamW. 5: 6: 7: 8: end for Ensure: θK Figure 3: Illustration of the iterative MaPPO pipeline in each iteration k. use reward model to get the responses corresponding rewards and collect (yw, yl) pairs, which reflect the preference. After response collection on Dk, we conduct the MaPPO training process using equation 8 on the subset Dk. After training on the prompt subset, we repeat the process in the next iteration + 1 until we finish all training iterations. Remark. reward model (or rule-based verifier) is necessary for all online methods, including online DPO (iterative DPO) (Dong et al., 2024) and reject sampling (Dong et al., 2023). 4.4 Adaptation to Other PO Methods We have shown MaPPO in the offline and online DPO settings. As this Maximum Posteriori (MaP) method is generally suitable for all DPO variants, we show how MaPPO modifies other DPO variants in this subsection. Simply replace the MLE part in preference optimization with MaP, and follow the same derivation in Section 4.1. Most DPO variants, as long as MLE is used in the original methods, can be modified with MaPPO as plugin. We show some widely adopted methods as examples here, including SimPO, IPO, and CPO. First, in SimPO (Meng et al., 2024), with the length control penalty, the loss function is given as LSimPO(θ) = (yw,yl,x)D log σ (cid:16) β yw log πθ(ywx) log πθ(ylx) γ (cid:17)i , β yl (14) where γ is constant hyperparameter, yw and yl denote the length of yw and yl, respectively. 8 With the MaPPO plugin, the loss function is modified as LSimPO+(θ) = (yw,yl,x)D log σ (cid:16) β yw log πθ(ywx) log πθ(ylx) γ (cid:17)i . β yl In IPO (Azar et al., 2024), the original loss function is LIPO(θ) = (yw,yl,x)D h(cid:16) log πθ(ywx) πref (ywx) log πθ(ylx) πref (ylx) (cid:17)2i . 1 2β With the MaPPO plugin, the loss function is modified as LIPO+(θ) = (yw,yl,x)D h(cid:16) log πθ(ywx) πref (ywx) log πθ(ylx) πref (ylx) (cid:17)2i . 1 2β In CPO (Xu et al., 2024), the original loss is LCPO(θ) = (yw,yl,x)D (cid:16) log σ β log πθ(ywx) β log πθ(ylx) (cid:17) λ log πθ(ywx) , where λ is constant hyperparameter. With the MaPPO plugin, the CPO loss is modified as (15) (16) (17) (18) LCPO+(θ) = (yw,yl,x)D (cid:16) log σ β log πθ(ywx) βr log πθ(ylx) (cid:17) λ log πθ(ywx) . (19) To verify the effectiveness, we show the experimental results with the improvement of these DPO variants in Section 5.3. Remark. With our MaPPO plugin, no additional hyperparameter is introduced in all DPO variants."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we empirically verify the effectiveness of our MaPPO methods. 5.1 Setup Instead of costly human Pipeline Settings. We follow the RLHF framework in Dong et al. (2024). annotations, we employ off-the-shelf reward models to generate the preferences. We use the public pre-trained BT reward model1 as the prior knowledge. For the response selection, we follow the rejection sampling strategy suggested by Liu et al. (2024b); Gulcehre et al. (2023). For each prompt, we generate = 8 responses and use the best-of-8 as yw and the worst-of-8 as yl. We provide hyperparameter details and computing resources in Appendix B.1. Dataset. We use the prompt set in Dong et al. (2024). In the offline setting, we generate responses from the initial model with the whole prompt set. In the online (iterative) setting, we separate the prompt set into three subsets of the same size. The learning process lasts for = 3 iterations. In each iteration, we sample responses from our current policy with one prompt subset, and use preference signals on these responses to improve our policy. Models. To show the scalability of our methods, we choose models in two dimensions: (1) Model sizes: Qwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, and Qwen2.5-7B-Instruct. (2) Model series: Qwen2.5-7BInstruct, Mistral-7B-Instruct-v0.3, and Llama-3-8B-Instruct in our experiments. 1https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0. 9 Evaluation. We evaluate the model performance on three widely used benchmarks: MT-Bench (Zheng et al., 2023a), AlpacaEval 2.0 (Li et al., 2023), and Arena-Hard v0.1 (Li et al., 2024). MT-Bench contains 80 questions from 8 categories, with answers rated by GPT-4 on scale of 1 10. Arena-Hard v0.1 contains 500 technical problem-solving questions, and the answers are compared to reference responses from the baseline model GPT-4-0314. We report the win rate (WR) in percentage as judged by GPT-4 Turbo (Preview-1106). AlpacaEval 2.0 includes 805 questions from five datasets, with the judge model GPT-4 Turbo (Preview-1106) comparing the answers to reference responses from itself. We report the length-controlled (LC) WR as suggested in Dubois et al. (2024)."
        },
        {
            "title": "5.2 Main Results",
            "content": "Our main results on three standard benchmarks, introduced in Section 5.1, are shown in Table 2. For the alignment methods, we show the evaluation results of Instruction Tuning (IT), the original offline setting (Rafailov et al., 2023) (DPO), and the online setting (Dong et al., 2024) (I-DPO) as described in Section 4.3. For DPO and I-DPO, we show their improvements that incorporate the MaPPO design (+MaPPO). For Llama-3-8B-Instruct, Mistral-7B-Instruct, and Qwen2.5-7B-Instruct models, the performances are significantly improved with MaPPO on AlpacaEval 2.0 and Arena-Hard in both the offline setting (DPO) and the online setting (I-DPO). It reflects the effectiveness of MaPPO on different model series in both online and offline settings. On the MT-Bench, the performances are slightly improved on Qwen2.5-7B-Instruct and Llama-3-8B-Instruct, because the base models have already achieved very good results on this benchmark, which has limitations to reflect the effective improvement. The improvement on MT-Bench becomes much more significant on models with mediocre base or DPO performances, e.g., Mistral-7B-Instruct and Qwen2.5-1.5B-Instruct. In one model series, the larger models achieve higher overall scores in both base performances and after online & offline alignment tuning, suggesting that scaling up model size enhances alignment capability as expected. With MaPPO, the improvement is consistent in scale with different model sizes in both online and offline settings, and the alignment can make smaller models outperform larger base models. 5.3 Adaptation to DPO Variants In Table 3, we show the improvement of the vanilla DPO (Rafailov et al., 2023) and its variants with MaPPO, including widely used I-DPO (Dong et al., 2024), SimPO (Meng et al., 2024), IPO (Gheshlaghi Azar et al., 2024), and CPO (Xu et al., 2024). We list the hyperparameter settings in the DPO variants in Appendix B. Their loss functions with MaPPO adaptation are shown in Section 4.3 and Section 4.4. For all DPO variants, no additional hyperparameter is needed from the MaPPO plugin. For the model in evaluation, we keep Qwen2.5-7B-Instruct as the default model. In general, MaPPO consistently improves all DPO variants with the MLE design on all three benchmarks. Although it drops little on MT-Bench in some methods, the original approach has essentially saturated at the achievable score on MT-Bench, which barely reflects the improvement with variance in evaluation. The overall consistent improvements observed across DPO variants after applying the MaPPO plugin underscore its flexibility and generality in enhancing preference optimization methods. Notably, MaPPO effectively complements both simple and complex variants without requiring architectural modifications or hyperparameter tuning. For instance, SimPO benefits from the MaPPO adjustment by further balancing the length-controlled optimization with better calibration of confidence scores, while IPO and CPO experience gains due to MaPPOs capacity to regularize reward signals with prior knowledge, mitigating overfitting to pairwise preferences. The improvements span diverse evaluation metrics and benchmarks, demonstrating that MaPPOs reward-aware calibration systematically addresses the shortcomings of MLE-based objectives inherent in existing variants. This indicates that MaPPO is not merely tweak, but general principle that can be seamlessly integrated into the PO pipelines to achieve more reliable alignment results. Table 2: Main evaluation results on three standard benchmarks. indicates the higher the better."
        },
        {
            "title": "Method",
            "content": "AlpacaEval 2.0 Arena-Hard MT-Bench Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Mistral-7B-Instruct Llama-3-8B-Instruct 11.10 18.71 IT DPO +MaPPO 19.35 +0.64 I-DPO +MaPPO 19.84 +1.95 17.89 18.91 20.16 IT DPO +MaPPO 26.68 +6.52 I-DPO +MaPPO 25.99 +6.30 19. 27.03 32.01 IT DPO +MaPPO 38.24 +6.23 I-DPO +MaPPO 39.10 +5.30 33.80 15.35 18.24 IT DPO +MaPPO 30.56 +12.32 I-DPO +MaPPO 33.28 +16.14 17. 10.85 22.48 IT DPO +MaPPO 28.37 +5.89 I-DPO +MaPPO 32.68 +3.21 29.47 5.0 11.6 15.3 +3.7 12.1 15.7 +3.6 24.0 29.2 35.1 +4.9 36.6 35.8 0.8 42.9 45.5 59.2 +13.7 46.9 61.6 +14. 13.1 14.2 18.4 +4.2 14.3 19.6 +5.3 10.2 22.4 29.5 +7.1 25.6 31.0 +5.4 7.06 7.29 7.57 +0.28 7.39 7.63 +0.24 7.92 8.02 8.13 +0.11 8.10 8.01 0.09 8.61 8.56 8.79 +0.23 8.55 8.54 0.01 5.40 6.86 7.51 +0.65 6.92 7.59 +0. 7.52 8.07 8.18 +0.11 8.01 8.04 +0.03 Table 3: Evaluation results of DPO variants with MaPPO plugin. indicates the higher the better. Method AlpacaEval 2.0 Arena-Hard MT-Bench DPO (Rafailov et al., 2023) +MaPPO I-DPO (Dong et al., 2024) +MaPPO SimPO (Meng et al., 2024) +MaPPO IPO (Gheshlaghi Azar et al., 2024) +MaPPO CPO (Xu et al., 2024) +MaPPO 32.01 38.24 +6.23 33.80 39.10 +5.30 25.15 32.75 +7.60 27.76 28.84 +1.08 32.94 33.71 +0. 45.5 59.2 +13.7 46.9 61.6 +14.7 64.2 69.5 +5.3 53.0 64.4 +11.4 47.6 54.1 +6.5 8.56 8.79 +0.23 8.55 8.54 0.01 9.02 8.94 0.08 8.83 8.84 +0.01 8.62 8.68 +0.06 5.4 Other Results on Academic Benchmarks It is widely observed that alignment impairs models performance on calibration, reasoning, and accuracy (Ouyang et al., 2022; Lin et al., 2024; Zhang et al., 2025), which is also known as the alignment tax. As result, it is also needed to assess the models performance using more academic benchmarks. In this subsection, we investigate whether the serval methods for alignment with human preference could sacrifice the general model performance. 11 We test the performance on six widely used academic benchmarks, evaluating various model abilities, including explicit instruction following (IFEval) (Zhou et al., 2023), general knowledge (GPQA) (Rein et al., 2024), multitask language understanding (MMLU) (Hendrycks et al., 2021), commonsense reasoning (HellaSwag) (Zellers et al., 2019), human falsehoods mimicking (TruthfulQA) (Lin et al., 2022), and math word problem-solving (GSM8K) (Cobbe et al., 2021). We show the results on the six academic benchmarks from Llama-3-8B-Instruct model in Table 4, and Qwen2.5-7B-Instruct model in Table 5. In general, for offline DPO with MaPPO outperforms the original DPO in all benchmarks for both models. The improvement is significant on GSM8K for Qwen2.5-7B-Instruct, and on TruthfulQA for Llama-3-8B-Instruct. For the iterative DPO with MaPPO, the performances are better than the original I-DPO on most benchmarks, and maintains the performances on IFEval and GPQA. Overall, the performances of online methods are better than offline methods, and MaPPO generally improves or maintains the performances on academic benchmarks in both settings. Table 4: Evaluation results on six academic benchmarks with Llama-3-8B-Instruct model. IFEval GPQA MMLU HellaSwag TruthfulQA GSM8K Method IT DPO +MaPPO I-DPO +MaPPO 70.4 77.0 82.0 74.6 76.4 30.2 27.5 29.5 29.8 28.8 62.4 62.7 63.2 63.1 63. 78.6 79.5 80.1 80.5 80.7 53.7 51.5 58.2 60.7 63.7 73.4 75.5 79.5 81.3 82.4 Table 5: Evaluation results on six academic benchmarks with Qwen2.5-7B-Instruct model. Method IFEval GPQA MMLU HellaSwag TruthfulQA GSM8K IT DPO +MaPPO I-DPO +MaPPO 73.5 73.2 73.8 72.9 72.6 31.5 32.0 33.1 33.0 33.3 71.8 71.9 72.0 71.9 72.9 62.1 62.0 62.1 62.2 62.2 56.4 57.1 59.2 55.9 56. 81.7 71.3 80.1 73.2 82."
        },
        {
            "title": "6 Discussions",
            "content": "6.1 Limitations and Future Work 1. Our results indicate that larger models consistently perform better with the MaPPO method. Future work with more computing resources could explore applying the proposed training pipeline to models larger than 8B parameters. 2. The prior knowledge function design relies on experts domain knowledge. We give reasonable and general design. Future works could explore different designs in specific domains. 3. If resources permit, human evaluation can be included for better judgment of alignment. 6.2 Conclusion In this work, we propose MaPPO, general and principled framework for preference optimization that incorporates prior knowledge into the optimization objective. By extending the Maximum Likelihood Estimation (MLE)-based Preference Optimization (PO) approach to Maximum Posteriori (MaP) formulation, MaPPO effectively mitigates confidence degeneration and provides more calibrated training signal. Our method requires no additional hyperparameters, supports both offline and online settings, and can be seamlessly integrated into existing Direct Preference Optimization (DPO) variants, including widely used SimPO, IPO, 12 and CPO. Without sacrificing efficiency, extensive empirical results demonstrate that MaPPO consistently improves alignment performance on different model series (e.g., Qwen, Mistral, and Llama), and on scaling to different model sizes (e.g., 1.5B, 3B, 7B, and 8B) across three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard. We also evaluate the performance on six widely used academic benchmarks after alignment, which shows that MaPPO, compared to the other methods, maintains the performance in various dimensions."
        },
        {
            "title": "Broader Impact Statement",
            "content": "MaPPO aligns language models by incorporating prior reward knowledge into preference optimization, leading to better-calibrated and more robust outputs. While beneficial, it relies on reward models that may encode biases or misrepresent human values, potentially reinforcing harmful patterns. Its use in persuasive or deceptive applications also poses potential impacts. To mitigate these impacts, we encourage careful curation and auditing of reward models, broader involvement in defining reward signals, and transparency in how preference optimization frameworks, such as MaPPO, are applied in real-world AI systems."
        },
        {
            "title": "References",
            "content": "Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint arXiv:2402.10571, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 44474455, 2024. David Blei, Lawrence Carin, and David Dunson. Probabilistic topic models. IEEE Signal Processing Magazine, 27(6):5565, 2010. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan HadfieldMenell. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information processing Systems (NeurIPS), 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun SHUM, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research (TMLR), 2023. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. RLHF workflow: From reward modeling to online RLHF. Transactions on Machine Learning Research (TMLR), 2024. Karel DOosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, and Shikib Mehri. Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment. Transactions of the Association for Computational Linguistics (ACL), 13: 442460, 2025. Yann Dubois, Percy Liang, and Tatsunori Hashimoto. Length-controlled AlpacaEval: simple debiasing of automatic evaluators. In Conference on Language Modeling (COLM), 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In International Conference on Machine Learning (ICML), 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 238, pp. 44474455, 2024. Dongyoung Go, Tomasz Korbak, Germàn Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through -divergence minimization. In International Conference on Machine Learning (ICML), pp. 1154611583, 2023. 14 Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Yuxiang Guo, Lu Yin, Bo Jiang, and Jiaqi Zhang. Todo: Enhancing LLM alignment with ternary preferences. arXiv preprint arXiv:2411.02442, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021. Jiwoo Hong, Noah Lee, and James Thorne. ORPO: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1117011189, 2024. Edwin Jaynes. Prior probabilities. IEEE Transactions on Systems Science and Cybernetics, 4(3):227241, 2007. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc Dymetman. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Guangchen Lan, Huseyin Inan, Sahar Abdelnabi, Janardhan Kulkarni, Lukas Wutschitz, Reza Shokri, Christopher Brinton, and Robert Sim. Contextual integrity in LLMs via reasoning and reinforcement learning. arXiv preprint arXiv:2506.04245, 2025. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The Arena-Hard pipeline. arXiv preprint arXiv:2406.11939, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models. https: //github.com/tatsu-lab/alpaca_eval, 2023. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 32143252, 2022. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. Mitigating the alignment tax of RLHF. In Empirical Methods in Natural Language Processing (EMNLP), 2024. Jinsong Liu, Dongdong Ge, and Ruihao Zhu. Reward learning from preference with ties. arXiv preprint arXiv:2410.05328, 2024a. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations (ICLR), 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with reference-free reward. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. 15 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:2773027744, 2022. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with DPO-positive. arXiv preprint arXiv:2402.13228, 2024. Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022. Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pp. 5372853741, 2023. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to Q: Your language model is secretly Q-function. arXiv preprint arXiv:2404.12358, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In Conference on Language Modeling (COLM), 2024. Yi Ren and Danica Sutherland. Learning dynamics of LLM finetuning. arXiv preprint arXiv:2407.10490, 2024. Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient RLHF: Reducing the memory usage of ppo. arXiv preprint arXiv:2309.00754, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 33:30083021, 2020. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. arXiv preprint arXiv:2401.08417, 2024. Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, and Xiaojun Quan. Weighted-reward preference optimization for implicit model fusion. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 47914800, 2019. 16 Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. In First Conference on Language Modeling (COLM), 2024. Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, and Dong Yu. Iterative Nash policy optimization: Aligning LLMs with general preferences via no-regret learning. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. Hanyang Zhao, Genta Indra Winata, Anirban Das, Shi-Xiong Zhang, David Yao, Wenpin Tang, and Sambit Sahu. RainbowPO: unified framework for combining improvements in preference optimization. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLMas-a-judge with MT-bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023a. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of RLHF in large language models part i: PPO. arXiv preprint arXiv:2307.04964, 2023b. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "A Theoretical Results",
            "content": "A.1 Stationary Convergence of MaPPO"
        },
        {
            "title": "Recall that",
            "content": "LMaP(θ) = (yw,yl,x)D log σ (cid:16) β log πθ(ywx) πref (ywx) rβ log (cid:17)i πθ(ylx) πref (ylx) = (yw,yl,x)D log σ(u) . (20) At the first-order stationary point (FOSP), the gradient of the loss with respect to θ becomes 0. Thus, we have LMaP(θ) = (yw,yl,x)D (cid:16) β(1 σ(u)) log πθ(ywx) log πθ(ylx) (cid:17)i = 0. (21) (22) (23) Let the optimal policy be π. The above equation holds when log π(ywx) log π(ylx) = 0. Thus, the optimal policy achieves log π(ywx) = log π(ylx) + c, where is scaling constant determined by the initialization. Thus, the model learns stable log-linear relationship between preferred and less-preferred responses, scaled by the prior reward gap. In DPO, the optimal policy at the FOSP is log π(ywx) = log π(ylx) + c. (24) DPO converges to maximizing the log-odds between yw and yl, but no inherent bound on the preference gap, which can lead to confidence degeneration. As training progresses, DPO may tend to decrease the likelihood of both yw and yl (the squeezing effect), because there is no constraint on absolute probabilities only the relative gap matters. MaPPO prevents overconfidence and instability by grounding optimization in the reward-based prior. The FOSP of MaPPO guarantees bounded, calibrated log-probability ratio between yw and yl. Naturally limits the squeezing effect by scaling the impact of yl via r. Therefore, MaPPO is theoretically more stable, especially for near-tie preference pairs and in large models where DPO can exacerbate miscalibration. A.2 Lipschitz Stability First, we list the standard assumptions for the analysis. Assumption A.1. 1. The score function is Lipschitz continuous as log πθ(yx) log πθ(yx) Mg. Let the gradient operator be defined as τθ := LMaP(θ). Then, the gradient operator τ is Lipschitz continuous with where LMaP = β(1 σ(u))(1 + r)Mg < β(1 + r)Mg. τθ τθ LMaPθ θ, (25) (26) Proof. We have τθ = β(1 σ(u)) (cid:16) log πθ(ywx) log πθ(ylx) (cid:17) ."
        },
        {
            "title": "The norm of the gradient difference is",
            "content": "τθ τθ = β(1 σ(u)) log πθ(ywx) log πθ(ywx) + β(1 σ(u))r log πθ(ylx) log πθ(ylx). (27) (28) We have that σ(u) is Lipschitz with constant 1 we have 4 . Combining the Mg Lipschitz of the score function log πθ(yx), τθ τθ β(1 σ(u))(1 + r)Mgθ θ := LMaPθ θ. In contrast, in DPO, the gradient is Lipschitz continuous as τθ τθ LDPOθ θ, (29) (30) where LDPO = 2β(1 σ(u))Mg < 2βMg. The Lipschitz constant of the gradients in DPO is larger than that in MaPPO, which shows the gradients have less stability in DPO. With formal upper bound on MaPPOs gradient variation, we show clear theoretical justification for why MaPPO is less prone to instability and exploding confidence gaps, compared to DPO."
        },
        {
            "title": "B Supplementary Experiments",
            "content": "B.1 Supplementary Experimental Settings Hyperparameter settings. We follow the standard settings and list the hyperparameter details in the training process of MaPPO in Table 6. We keep the hyperparameter settings for different model series and model sizes, including Qwen2.5-{1.5B, 3B, 7B}-Instruct, Mistral-7B-Instruct, and Llama-3-8B-Instruct models. Table 6: Hyperparameter settings in MaPPO. Hyperparameter Value global batch size learning rate η warmup steps weight decay optimizer KL weight β number of responses temperature precision 128 5 107 100 0.01 AdamW 0.1 8 1.0 bfloat16 Table 7: Hyperparameter settings in DPO Variants. Hyperparameter Value SimPO: γ IPO: β CPO: λ 1 0.1 0.2 In Table 7, we list the extra hyperparameters in the reproduce of the DPO variants. The other hyperparameter settings keep the same in Table 6. Notably, we choose nearly the best hyperparameters for the other methods, and our reproduction achieves higher performances than the original or other reproduction reports on some benchmarks, e.g., SimPO on Arena-Hard. Computing Resources. All tasks are trained and evaluated on platform with 8 NVIDIA A100 GPUs on each node, and 80 GB of memory for each GPU. Each training task requires between 4 and 20 hours to execute, depending on the size of the model."
        },
        {
            "title": "C Further Discussions",
            "content": "C.1 Prior Function In Bayes estimation, the prior distribution is usually constructed by experts with domain knowledge without an exception to avoid it (Jaynes, 2007). We choose simple form in equation 5, which has the same structure as the widely used prior function (Blei et al., 2010) and aligns with the softmax probability. This brings very clean result in equation 8. In this paper, we offer an effective function with good performances and concise formulation. We bring the MaP design into the DPO pipeline, and the prior function construction is not the aim of this paper. Other function choices that are designed by domain experts are also acceptable and open to be used."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Purdue University",
        "Tencent AI Lab",
        "University of California, San Diego",
        "University of Rochester",
        "Yonsei University"
    ]
}