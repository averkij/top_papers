{
    "paper_title": "\"I May Not Have Articulated Myself Clearly\": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time",
    "authors": [
        "Jinkun Chen",
        "Fengxiang Cheng",
        "Sijia Han",
        "Vlado Keselj"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model \"loses the thread\" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (\\emph{corrective instability}), whereas late instability is more often followed by failure (\\emph{destructive instability}), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism."
        },
        {
            "title": "Start",
            "content": "I May Not Have Articulated Myself Clearly: Diagnosing Dynamic Instability in LLM Reasoning at Inference Time Jinkun Chen 1 Fengxiang Cheng 2 3 Sijia Han 4 Vlado Keselj"
        },
        {
            "title": "Abstract",
            "content": "Reasoning failures in large language models (LLMs) are typically measured only at the end of generation, yet many failures manifest as process-level breakdown: the model loses the thread mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or finetuning. We define simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and correct final answer (corrective instability), whereas late instability is more often followed by failure (destructive instability), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as diagnostic lens rather than corrective or control mechanism. 6 2 0 2 2 ] . [ 1 3 6 8 2 0 . 2 0 6 2 : r 1. Introduction LLM reasoning failures are typically evaluated as end-point errors, yet the generation process is temporal dynamical system whose internal evolution can become unstable. We study whether inference-time signals can distinguish class 1Dalhousie University 2University of Amsterdam 3Tsinghua Jinkun Chen Correspondence to: University 4Meta. <jinkun.chen@dal.ca>. Preprint. February 4, 2026. 1 Figure 1. Inference-time dynamic instability as diagnostic signal. Conceptual overview of an inference-time diagnostic for dynamic instability in large language model reasoning. A: At each decoding step t, we observe only the renormalized top-k next-token distribution pt, which is accessible as black-box inference-time signal; no hidden states, gradients, or training access are required. B: An instability event is characterized by an abrupt reshuffling of probability mass among high-probability candidates, accompanied by increased uncertainty (higher entropy) between consecutive steps. C: We define diagnostic instability signal It = Dt + λHt, combining distributional shift Dt = JSD(pt, pt1) and uncertainty Ht. All quantities are computed solely from pt at inference time, with overall instability strength summarized as = maxt It. D: Instability strength is associated with increased failure risk, while the peak location (relative position ρ) provides complementary information, separating earlier (more recoverable/corrective) from later (less recoverable/destructive) instability episodes. of failures driven by dynamic instability, rather than by static knowledge limitations. Our goal is diagnostic rather than corrective: identify process-level indicators that reliably predict failure, without modifying the model. Motivation. In many applications, the relevant question is not only whether an answer is correct, but when the generation begins to break down and whether it is likely to recover. Final-answer accuracy is retrospective, and static confidence proxies can miss intra-trajectory transitions; conversely, multi-sample consistency methods require multiple runs. We therefore ask minimal inference-time question: can single decoding trace reveal an observable signature of unstable reasoning using only token probabilities? Figure 1 provides conceptual overview of our inference-time diagDiagnosing Dynamic Instability in LLM Reasoning nostic, illustrating what is observable at decoding time, how instability manifests in token distributions, and why instability strength and timing are predictive of reasoning failure. Unlike accuracy-oriented interventions, our analysis does not aim to improve correctness, but to characterize when and how reasoning processes become unstable. We focus on detecting and characterizing instability, not on correcting or controlling it. To avoid confusion with related notions, our instability signal is not confidence score, calibration method, hallucination detector, or verifier. It targets trajectory-level dynamical behavior rather than outcome-level uncertainty: two traces with similar average entropy can exhibit very different instability profiles, and only the latter reflects temporal disruption during reasoning. Why can the next-token distribution represent reasoning stability? Autoregressive decoding induces closed-loop state evolution in which the internal state updates based on previously generated tokens, yet the models token distribution is the only black-box observable of this evolving state. Small perturbations may therefore be amplified over time through this feedback loop. Formally, letting ht denote the internal state and xt the emitted token, ht+1 = (ht, xt), pt = g(ht), (1) where pt is the full next-token distribution; in black-box settings we observe pt, its top-k truncation and renormalization. We therefore study intra-trajectory temporal instability: abrupt changes in pt (or pt when truncated) over time within single trace. This differs from self-consistency and other sampling-based methods, which analyze intertrajectory variability across multiple samples. It also differs from static uncertainty: we do not treat entropy as standalone confidence score, but combine entropy with consecutive-step distributional shifts to quantify localized regime shifts during single generation. Prior work shows that LLMs often fail to identify their own reasoning errors without explicit localization, motivating process-level diagnostics of reasoning traces (Tyen et al., 2023). Recent methods propose temporal signals for reasoning process error identification and visualization of reasoning paths, while uncertainty analyses highlight the limits of explanation faithfulness (Guo et al., 2025; Li et al., 2025b; Da et al., 2025). We complement these directions with an inference-time instability signal defined directly on token distributions. Figure 2 previews our central observation: instability strength alone is insufficient; its timing determines whether reasoning recovers or collapses. Our contributions are: 1. Inference-time instability signal. We define training-free instability signal It = Dt + λHt that Figure 2. Timing-dependent regimes of inference-time instability. Representative decoding traces illustrating how the timing of instability, rather than its magnitude alone, is associated with different reasoning outcomes. A: representative trace with an early instability peak (recoverable / corrective). Although the instability signal It reaches high value, it occurs sufficiently early in decoding for the model to recover, resulting in correct final answer. B: representative trace with late instability peak (unrecoverable / destructive). Despite comparable instability strength, the peak occurs near the end of decoding, leaving limited opportunity for recovery and resulting in an incorrect final answer. C: controlled comparison with instability signals normalized to comparable peak strength, demonstrating that magnitude alone is insufficient: similar instability strength can correspond to different outcomes depending on peak timing. All curves show representative (not averaged) decoding trajectories, with instability defined as It = Dt + λHt. combines consecutive-step distributional shift and uncertainty, computed solely from token log probabilities. 2. Single-trace failure predictability. Instability strength predicts reasoning failure from single decoding trace, achieving AUC 0.660.74 on GSM8K and remaining predictive at scale on the full GSM8K test set and the HotpotQA distractor validation split. 3. Timing-dependent diagnostic refinement. We distinguish destructive from corrective instability via peak timing, showing that instability magnitude alone is insufficient and that recoverability depends on when instability occurs. Scope. We test single claim: instability strength increases with failure rate under both deterministic and stochastic decoding. We prioritize cross-family reproducibility (Llama/Qwen) across matched size scales (0.5B8B) for diagnosis rather than exhaustive benchmarking. 2. Related Work Prior work related to our diagnostic can be broadly grouped into four threads: (i) monitoring and error localization of reasoning trajectories, (ii) prompting, sampling, and lightweight adaptation for improving reasoning, (iii) uncertainty and confidence estimation for LLM outputs, and Diagnosing Dynamic Instability in LLM Reasoning (iv) adversarial and disruptive perturbations that probe sensitivity of decoding dynamics. In contrast, we study intratrajectory temporal instability from black-box next-token distributions, without retraining, additional models, or multisample aggregation. Process monitoring and error localization. Reasoning error detection and monitoring have been explored through explicit error localization and temporal consistency signals (Tyen et al., 2023; Guo et al., 2025). Visualization and structural analysis of reasoning paths provide complementary tools for interpreting failures and diagnosing process breakdowns (Li et al., 2025b; Da et al., 2025). Process supervision and verifier-style training emphasize stepwise correctness but require additional supervision or fine-tuning (Lightman et al., 2023); recent work on process reward models (PRMs) explores data-efficient stepwise verification via chain-of-thought verifiers (Khalifa et al., 2025) and studies best practices for PRM development (Zhang et al., 2025). Relatedly, Mao et al. (2026) propose viewing confidence as temporal signal and use Signal Temporal Logic (STL) to mine temporal patterns that distinguish correct from incorrect LLM reasoning traces. While their method requires multi-sample generation and STL mining, our approach operates on single trace and defines instability directly from consecutive-step distributional changes. Our approach is fully inference-time and training-free, and uses only tokendistribution observables available during decoding. Prompting, sampling, and lightweight adaptation. Chain-of-thought prompting can elicit multi-step reasoning (Wei et al., 2022), and sampling-based aggregation such as self-consistency improves accuracy by marginalizing over multiple traces (Wang et al., 2022). ReAct-style prompting interleaves reasoning and acting, but can be brittle in sequential settings when intermediate steps drift (Yao et al., 2022). Beyond prompting, parameter-efficient adaptation methods such as prefix-tuning (Li & Liang, 2021), prompt tuning (Lester et al., 2021), and LoRA (Hu et al., 2021) modulate model behavior with minimal parameter updates; these methods highlight that small changes in prompt/context can induce large changes in token-level dynamics. Our instability signal is complementary: rather than changing the model, we quantify distributional drift and uncertainty along the decoding trajectory. Related work also studies prompt sensitivity directly by measuring and manipulating prompt influence on model outputs (Feng et al., 2024), which supports the broader view that token-level dynamics can be highly responsive to small input perturbations. In short, selfconsistency and related approaches are, by design, intertrajectory techniques, whereas we diagnose intra-trajectory temporal stability for single run. 3 Uncertainty and confidence. Black-box self-checking approaches target hallucination detection via consistency signals (Manakul et al., 2023). Confidence calibration remains an active area, with multicalibration methods proposed to improve reliability of confidence scores (Detommaso et al., 2024). Prior work also studies the extent to which model probabilities reflect epistemic uncertainty and correlate with correctness (Kadavath et al., 2022). From an informationtheoretic perspective, the next-token entropy corresponds to the conditional entropy of the next symbol given the prefix in an autoregressive process (Shannon, 1948; Cover & Thomas, 2006). Sequential inference can be viewed as repeated belief updates that ideally stabilize as evidence accumulates (MacKay, 2003). Prior work leverages predictive uncertainty (e.g., entropy) as mostly static correctness proxy (Guo et al., 2017; Malinin & Gales, 2018); we extend this view by analyzing the temporal evolution of uncertainty and distributional change along single reasoning trajectory, without claiming that information theory alone proves causal failure. Semantic-level uncertainty measures such as semantic entropy (uncertainty computed at the level of meaning rather than token sequences) have been proposed as principled hallucination detectors (Farquhar et al., 2024). Extensions include semantic entropy probes (SEPs) that extract semantic uncertainty from hidden states (Kossen et al., 2024), kernel-based fine-grained uncertainty quantification (Nikitin et al., 2024), efficient Bayesian estimation methods (Ciosek et al., 2025), and semantic energy operating on logits to capture uncertainty in scenarios where semantic entropy fails (Ma et al., 2025). Token-level hallucination detection via variance and probability signals has also been explored (Quevedo et al., 2024; Kumar, 2025; Niu et al., 2025). Recent evaluations and frameworks compare uncertainty estimation methods and emphasize remaining challenges (Bakman et al., 2024; Heo et al., 2024; Li et al., 2025a). These methods predominantly target static uncertainty at the output or semantic level, often requiring multiple samples or hidden-state access. In contrast, our focus is specifically on temporal instability within single decoding trace: we track how the next-token distribution changes over time, which captures distinct failure mode (dynamic regime shifts during reasoning) that static uncertainty measures may miss. Finally, work on self-correction reaches mixed conclusions: some studies argue that self-correction is not an inherent capability and depends on scaffolding (Huang et al., 2023; Liu et al., 2024b), while others find evidence of intrinsic self-correction under certain settings (Liu et al., 2024a). Recent work trains models to self-correct via reinforcement learning (Kumar et al., 2024), and studies of internal representations show that LLM hidden states encode information Diagnosing Dynamic Instability in LLM Reasoning predictive of chain-of-thought success (Afzal et al., 2025). Our corrective vs. destructive distinction is compatible with this picture: high-instability episodes may correspond either to beneficial revision or to divergence toward failure. Our method differs in focus: we do not attempt to produce calibrated confidence score; instead we characterize dynamical instability via temporal changes in token distributions. Adversarial and disruptive perturbations. Recent work shows that stepwise interventions can deliberately disrupt intermediate reasoning and induce downstream errors (Peng et al., 2024). This supports our framing that process-level dynamics, not only final answers, constitute meaningful object of study, and motivates robust, inference-time diagnostics of instability. Unlike intervention work (editing/steering), this paper is diagnosis-only: we define and validate an instability signal without applying stabilization operators. 3. Method 3.1. Inference-Time Observables As illustrated in figure 1 (A, B, and C), our instability signal is computed entirely from logged top-k next-token distributions at inference time, without access to hidden states or full logits. At each generation step t, the model produces predictive distribution pt over tokens. In our implementation, we only observe top-k log probabilities. Let Vt be the top-k token set at step and define the truncated, renormalized disVt by setting pt(x) = 0 for / tribution pt supported on Vt Vt. When computing divergences and renormalizing over between consecutive steps, we treat pt and pt1 as distributions over the union support Vt Vt1 by zero-padding outside each steps logged set (equivalently, we compute JSD over the union key set). Unless otherwise noted, we compute all observables from pt; when full vocabulary logits are available, we compute the same quantities from pt without truncation. Our primary objective is black-box diagnostic based only on log probabilities; therefore we treat pt as the operative observable and use quantities computed from full vocabulary logits only as validation control when available. We compute (using natural logarithms): (cid:88) pt(x) log pt(x), Ht = Dt = JSD(pt, pt1), (2) (3) )+ where JSD(P, Q) = 1 ) and = 2 KL(P 1 [0, log 2]. 2 (P +Q). With natural logarithms, JSD(P, Q) In all experiments we compute logarithms only on positive probabilities. We compute KL terms only where the numerator probability is positive (as in the standard definition 2 KL(Q of KL), noting that in JSD the mixture = 1 2 (P + Q) is strictly positive wherever or is positive. For numerical stability, we optionally add small ϵ (e.g., 1012) inside logarithms; we verified this does not change results. 3.2. Instability Signal and Strength We define instability per-step: It = Dt + λHt, (4) and note that both correct and incorrect traces can exhibit high instability (figure 2, A, B, and C), motivating focus on temporal structure beyond magnitude. We set λ = 1 as fixed default (equal-weighting in nats) across all tasks, models, and decoding settings, and do not tune λ to avoid optimistic bias from post-hoc hyperparameter selection. We report sensitivity via minimal ablation comparing λ = 0 (JSD only) and λ = 1 (figure 8), and additional baselines in Appendix G. For each trace, the instability strength is = maxt It to capture brief spikes. To control for trace-length confounding (longer traces have more opportunities for spikes), we also report early-window maxima Sw = maxtw It for pre-specified windows , highlighting S50 as fixed- { window control (section F). 10, 20, 50, 100 } Implementation and complexity. Given per-step top-k log probabilities, we compute Dt by evaluating JSD on the VtVt1 via zero-padding outside each steps union support logged set, and compute Ht on the renormalized pt. This yields an O(T k) per-trace computation (streamable with O(k) working memory), making the diagnostic practical at scale. For determinism, when taking arg max over time we break ties by the smallest (as specified for and 50). 3.3. Theoretical Intuition Autoregressive decoding is closed-loop dynamical system: as the model generates tokens, it updates an unobserved internal state that determines the next-token distribution. Abrupt step-to-step shifts in the distribution are therefore natural observable signatures of internal regime changes. Our instability signal combines realized distributional change (Dt via consecutive-step JSD) with uncertainty (Ht via entropy) to capture both route switching and decision fragility. For space, we defer the full theoretical statements (including the recoverability/timing formalization) to Appendix B. Recoverability under finite decoding horizon (intuition). We view decoding as finite-horizon process in which entering stable continuation regime requires minimum amount of time. When instability occurs early, there Diagnosing Dynamic Instability in LLM Reasoning remains sufficient decoding budget for the model to restabilize and converge to coherent trajectory. In contrast, when instability peaks late, the remaining budget becomes insufficient for recovery, even if the magnitude of the instability is comparable. This perspective highlights that recoverability depends not only on how strongly the models distribution changes, but also on when such changes occur relative to the remaining decoding horizon; we emphasize that this argument is explanatory rather than formal proof (Appendix B). 4. Experiments 4.1. Task and Models Datasets: GSM8K (test split) (Cobbe et al., 2021) and HotpotQA (distractor validation split) (Yang et al., 2018). For controlled GSM8K-300 experiments we use the first 300 GSM8K test examples (starting from index 0 for determinism) with Qwen2.5-1.5B-Instruct (Yang et al., 2024) and Llama-3.2-1B-Instruct (Grattafiori et al., 2024). We additionally report dense per-step baseline runs on the full GSM8K test set (1319 examples) and HotpotQA distractor validation split (7405 examples), including Llama-3.23B-Instruct and Llama-3.1-8B-Instruct (Grattafiori et al., 2024) (section L). Models are instruction-tuned to reduce 0.5B8B to test scale formatting confounds; sizes span robustness under fixed diagnostic. 4.2. Prompting and Decoding For GSM8K, prompting and scoring follow the standard format: we prompt with the question text, and prediction is scored correct if the final extracted numeric answer matches the reference answer. We extract the last number in the model output as the predicted answer. For HotpotQA, we prompt with the question and concatenated context passages and ask for short, direct answer; we score correctness by comparing the first line of the model output to the reference using normalized matching with containment check. We deliberately use minimal, zero-shot prompting (no fewshot exemplars) to keep decoding dynamics comparable and avoid confounds from prompt engineering. As result, absolute accuracy can be low for these small models; our focus is on ranking and separability (AUC, monotonic bucket trends), which is less sensitive to the overall error rate than raw accuracy and is therefore suitable for diagnostic separability. We report greedy decoding and stochastic decoding. For GSM8K-300, we evaluate Qwen2.5-1.5B-Instruct with greedy decoding and evaluate Llama-3.2-1B-Instruct under small decoding grid with top-p 0.9 and fixed random seed. For Llama-3.2-1B-Instruct, we additionally evaluate τ = 0.3 (top-p 0.9), yielding small decoding grid { 0.0, 0.3, 0.7 . In analyses that pool multiple Llamaτ 3.2-1B-Instruct decoding settings (e.g., peak-step characteristics), we aggregate across this grid, yielding 3 300 traces in total. } 4.3. Logging and Trace Length Controls In our main experiments, we log top-k token log probabilities per-step and cap generation to 128 new tokens to reduce length confounding. In addition to top-k traces, we validate key timing findings with held-out run that logs entropy/JSD from full vocabulary logits (no truncation; section 6). We also compute early-window versions of our strength statistic to control for trace length (see Metrics and section F). Unless otherwise noted, we log top-k distributions with = 50 and report top-k sensitivity in Appendix E. For timing analyses, we additionally use fixed-window peak position (e.g., ρ50) to decouple peak-timing effects from the total trace length (section M). 4.4. Metrics We evaluate instability as diagnostic signal rather than an intervention. Let be the error label (y = 1 if the final answer is wrong). We report: { 0, 1 } Bucketed accuracy: partition examples into five equalsized quantile buckets by instability strength and report accuracy per bucket. Rank association: Spearman correlation between and correctness (encoded as 1 for correct and 0 for wrong). Separability: ROCAUC for predicting from (denoted AUCwrong). We additionally compute early-window versions by replacing with Sw to verify that the signal is not purely driven by longer generations (a key confound for any max-over-time statistic). 4.5. Ablations and Controls We include minimal ablation on the instability signal by comparing λ = 0 (JSD only) and λ = 1 (JSD+entropy). We also report small decoding grid (temperature τ with top-p 0.9) to verify robustness under 0.0, 0.3, 0.7 { stochasticity. } 5. Results 5.1. Instability Strength Correlates Monotonically with Failure Rate Instability strength shows clear monotonic relationship with failure rate. Across models and decoding settings, Diagnosing Dynamic Instability in LLM Reasoning bucketed accuracy decreases as instability strength increases (figure 3). The monotonic decrease is observed under equalsized buckets, rather than an artifact of sample imbalance. This is consistent with the primary claim that dynamically unstable trajectories are disproportionately associated with wrong final answers. 5.2. Instability Strength Separates Correct and Incorrect Reasoning Instability strength separates correct from incorrect outputs: Spearman correlations are consistently negative and AUCwrong values are consistently above chance  (table 1)  . Early-window signals preserve predictive power, indicating the effect is not simply driven by longer generations. Bootstrap confidence intervals over examples support the stability of these trends (section H). We reuse the same traces and vary only the analysis window to control for length. Even when restricted to the first 50 steps, instability remains predictive (e.g., AUC 0.66 on Llama-3.2-1B-Instruct), indicating that the signal is not an artifact of longer sequences. We report explicit early-window AUC values (using S50) in section F. This fixed-window evaluation directly targets the trace-length confound: longer generations provide more opportunities for large maxt It even under identical per-step dynamics. For example, using S50 yields AUCwrong of 0.605 on Qwen2.5-1.5B-Instruct greedy and 0.665 on Llama-3.2-1BInstruct (τ = 0.0)  (table 6)  . This suggests secondary claim: instability is detectable early, before full reasoning completion, which is valuable for diagnosis and triage. We include full early-warning curve (varying the prefix window length) in Appendix F. Table 1 summarizes cross-model and decoding robustness, reporting overall accuracy alongside the separability of instability strength. Across models and decoding settings  (table 1)  , AUC values remain stable. On Llama-3.2-1B-Instruct, the monotonic trend appears with AUC values around 0.66 (greedy) to 0.74 (stochastic). The small decoding grid (τ ) } preserves the trend: increased stochasticity reduces accuracy but does not remove the instability-failure relationship. 0.0, 0.3, 0.7 { We additionally validate on full GSM8K and HotpotQA runs, and include ReClor (Yu et al., 2020) check as representative setting where the signal can weaken. ReClor is multiple-choice logical reasoning benchmark; under our short-answer decoding and first-line matching setup, correct predictions are extremely sparse. As result, errors are dominated by stable-but-wrong trajectories, which can induce weak or even reversed correlations between instability and correctness in this setting. Detailed tables and figures are reported in Appendix L. Figure 3. Monotonic bucket trends for Llama-3.2-1B-Instruct across temperature settings (τ {0.0, 0.3, 0.7}). Higher instability buckets consistently show lower accuracy, confirming the predictive value of the instability signal. 5.3. Not All Instability Is Harmful Instability magnitude alone is insufficient: some highinstability episodes reflect beneficial self-correction rather than failure. We refine diagnosis by distinguishing corrective versus destructive instability via the timing of the peak (Section 6). 5.4. Signal Ablation Using JSD alone (λ = 0) collapses the strength distribution and yields near-random predictive power (AUC 0.52) on Llama-3.2-1B-Instruct, while the combined signal (λ = 1) restores clear separation and monotonicity. This supports the need to combine distributional change with uncertainty. We observe that JSD-only signal (λ = 0) becomes nearly constant on Llama-3.2-1B-Instruct, yielding near-random separability. plausible explanation is saturation under top-k approximations: when successive steps have weak support overlap, renormalized top-k distributions can reduce the dynamic range of divergence estimates, reducing sensitivity to more graded changes in instability. Adding entropy restores robust signal by capturing uncertainty and competition among alternatives even when support overlap is limited. This ablation indicates that uncertainty is necessary component of our instability proxy. We also explored change-point detectors, but found continuous strength more stable as diagnostic signal; see section D. We additionally report time-structure negative controls and entropy-family baselines on the same traces; see section G. We include additional baseline tables and ablation figures in Appendix G. 6. Corrective vs. Destructive Instability critical observation emerges from our analysis: not all high-instability episodes are harmful. We identify two qualitatively distinct regimes of dynamic instability with opposite implications for reasoning outcomes. These regimes are deDiagnosing Dynamic Instability in LLM Reasoning Table 1. Cross-model and decoding robustness on GSM8K (first 300 test examples; top-p 0.9 for stochastic decoding). AUC values remain stable across models and decoding settings, supporting model-agnostic signal."
        },
        {
            "title": "Spearman",
            "content": "Qwen2.5-1.5B-Inst Llama-3.2-1B-Inst Llama-3.2-1B-Inst Llama-3.2-1B-Inst greedy τ = 0.0 τ = 0.3 τ = 0.7 0.430 0.123 0.123 0.094 0.681 0.657 0.667 0.741 0.311 0.178 0.190 0.244 fined jointly by instability events and recoverability, rather than by instability magnitude alone. 6.1. Two Regimes of Instability Destructive instability. Destructive instability refers to high-instability episodes that are followed by continued instability and incorrect final answers. In these traces, uncertainty tends to rise and the trajectory does not stabilize onto coherent solution path. Table 2. Accuracy by peak instability position on the held-out baseline run (Llama-3.2-1B-Instruct, GSM8K, 100 traces). Earlypeak traces achieve more than 3 the accuracy of late-peak traces, despite both exhibiting high instability at some point."
        },
        {
            "title": "Peak position",
            "content": "% of traces Accuracy Early (ρ < 0.25) Middle (0.25 ρ Late (ρ > 0.5) 0.5) 57% 29% 14% 0.46 0.35 0. Corrective instability. Corrective instability refers to high-instability episodes that are followed by stabilization and correct final answers. In these traces, uncertainty often decreases after the peak and the trajectory converges. causes success or failure. As control against the trivial late means no time left critique, we additionally measure peak position within fixed early window and observe the same qualitative ordering (section M). 6.2. Operationalization 6.3. Empirical Evidence We operationalize recoverability using the peak instability position, which captures when the strongest instability occurs within reasoning trace. { pt} t=1, let = arg maxt It (breaking ties Given trace by the smallest t) denote the step of peak instability. We define the relative peak position as ρ = t/T , where is the trace length, and classify traces as: Early peak (ρ < 0.25): Instability occurs early, leaving time for subsequent recovery; this regime is characteristic of corrective instability. Late peak (ρ > 0.5): Instability occurs late, with insufficient time for recovery; this regime is characteristic of destructive instability. We adopt coarse thresholds for interpretability (clear early vs. late separation); the qualitative ordering is robust to alternative threshold pairs and to binned analysis of ρ (section I), and also holds under fixed-window peak definition (section M). This operationalization captures simple intuition: when difficulty arises early, the model may still recover through subsequent stable reasoning, whereas late instability leaves limited opportunity for correction. Importantly, ρ is descriptive rather than causal; we do not claim that peak position 7 We partition traces by peak instability position and compare their final-answer accuracy on held-out baseline run of Llama-3.2-1B-Instruct (100 traces, logged without top-k truncation for this validation). Table 2 shows clear monotonic relationship between peak position and accuracy. Traces where instability peaks early have substantially higher accuracy (46%) than those where it peaks late (14%), suggesting that instability magnitude alone is an incomplete diagnostic: the timing and recoverability of instability matter. To characterize peak steps beyond timing, we also report two low-cost probes (margin drops and support-set turnover) in Appendix K. At the peak step, corrective traces tend to exhibit sharper probability margin drops, while destructive traces exhibit higher support-set turnover, consistent with decisive revision versus chaotic route switching (Appendix K). We provide visualization in Appendix (Figure 4). 6.4. Implications This distinction has two important implications for how inference-time instability should be interpreted. First, raw instability strength (S = maxt It) remains useful aggregate predictor of failure, but peak position provides complementary summary that separates traces which recover from those that do not. This highlights that temporal Diagnosing Dynamic Instability in LLM Reasoning structure carries information beyond magnitude alone. Second, our analysis clarifies the scope of this work. We do not propose or evaluate interventions; instead, our goal is diagnosis and mechanistic characterization based solely on black-box inference-time observables. Taken together, these findings refine our central claim: while dynamic instability is predictive of failure, the underlying mechanism is nuanced. Destructive instability drives most of the predictive signal, whereas corrective instability acts as confound that, once accounted for, sharpens the diagnostic. 7. Discussion Takeaways. What we showed: inference-time instability can be detected from black-box signals (top-k token distributions) and reliably predicts reasoning failure, with consistent monotonic bucket trends and above-chance AUC across models, decoding settings, and tasks (including fullset validation on Llama-3.2-3B-Instruct and Llama-3.1-8BInstruct). What we clarified: not all instability is harmful; its diagnostic meaning depends on timing, with early peaks more often followed by recovery and late peaks more often followed by failure. What we did not claim: we do not claim unique internal mechanism, causality, or controllability; this paper is diagnosis-only and does not propose stabilization operators. Taken together, our results suggest that inference-time instability provides reliable diagnostic signal for reasoning behavior, while its consequences depend critically on when it arises. 7.1. Failure Taxonomy Our framing separates at least three conceptual failure (1) Stable-but-wrong failures produce incorrect modes. answers despite low instability strength S, often consistent with knowledge gaps or spurious heuristics. (2) Dynamically unstable failures exhibit high instability strength and cascading divergence, which is the focus of this paper. (3) Early-collapse failures exhibit high early-window instability (e.g., large S20 = maxt20 It) and are often tied to formatting errors or decoding pathologies. This differs from early-peak-but-recoverable cases in section 6, where early instability is followed by stabilization and correct completion. This taxonomy clarifies that instability is specific diagnostic dimension, not universal explanation of failure. We provide breakdown figure in Appendix (Figure 5). 7.2. Mechanistic Interpretation The instability signal is purely inference-time and black-box: it uses only per-step token probabilities. Mechanistically, spikes can reflect route switching (abrupt changes in which continuation is locally preferred) and decision fragility (near ties among candidates). We provide longer interpretation, together with the supporting low-cost probes, in Appendix J. 8. Limitations First, not all failures are dynamic; stable-but-wrong cases exist and are not addressed by instability signals. Second, change-point detection is sensitive to detector choice and thresholding, making binary change-point labels less reliable than continuous strength. Third, our approach assumes that relative changes in model probabilities carry meaningful information about underlying state transitions. While we do not require perfect probability calibration, future work can study how calibration quality affects this diagnostic signal. Finally, this study focuses on single primary task (GSM8K) for controlled grid experiments on two small models, with additional full-set runs on GSM8K and HotpotQA that include Llama-3.2-3B-Instruct and Llama3.1-8B-Instruct. Broader evaluation across tasks and larger models is left to future work but is not necessary to validate the central mechanism. These limitations are inherent to diagnostic perspective and do not detract from the goal of identifying dynamically unstable reasoning failures. When the signal weakens. The signal targets dynamically unstable failure modes and can be weaker when stablebut-wrong errors dominate. Concretely, on ReClor in our setting, higher instability tends to correlate with correctness (AUCwrong < 0.5). The signal also weakens when the logged top-k list is too small to estimate entropy/JSD reliably (Appendix E). 9. Conclusion We provide simple and reproducible inference-time diagnostic for dynamic instability in LLM reasoning. Instability strength yields robust, monotonic relationship with failure rate and predicts errors with AUC in the range 0.57 to 0.78 across GSM8K and HotpotQA, using controlled GSM8K300 runs and dense baseline runs at full-set scale. Crucially, we show that not all instability is harmful: corrective instability reflects beneficial self-correction and is associated with higher accuracy, while destructive instability drives the failure-predictive signal. Peak-step probes further indicate that corrective traces exhibit sharper margin drops, while destructive traces exhibit higher support-set turnover. We report these low-cost probes and longer mechanistic interpretation in Appendix and Appendix K. Taken together, these results establish instability strength and peak position as robust, black-box diagnostics of reasoning breakdowns. 8 Diagnosing Dynamic Instability in LLM Reasoning"
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance the understanding of inferencetime behavior in large language models by introducing diagnostic signal for identifying dynamic instability during reasoning. The proposed method is purely observational, operates entirely at inference time, requires no model modification or retraining, and is intended as an analysis tool rather than corrective or control mechanism. Potential positive impacts include improved transparency and diagnostic assessment of LLM reasoning processes, which may support evaluation, debugging, and risk-aware deployment, particularly in settings where reasoning failures are costly. By distinguishing between corrective and destructive instability, the work also cautions against interpreting all forms of instability as uniformly harmful. We emphasize that the method is not designed for influencing model behavior, automating decisions, or surveilling users, and it does not provide mechanisms for steering or modifying model outputs. Accordingly, we do not identify new ethical risks beyond those already associated with large language models. Any downstream use of inferencetime diagnostics should be considered within appropriate task-specific and human-in-the-loop evaluation frameworks."
        },
        {
            "title": "References",
            "content": "Afzal, A., Matthes, F., Chechik, G., and Ziser, Y. Knowing before saying: LLM representations encode information about chain-of-thought success beIn Findings of the Association for fore completion. Computational Linguistics: ACL 2025, pp. 12791 12806, 2025. 10.18653/v1/2025.findings-acl. 662. URL https://aclanthology.org/2025. findings-acl.662/. doi: Bakman, Y. F., Yaldiz, D. N., Buyukates, B., Tao, C., Dimitriadis, D., and Avestimehr, S. MARS: Meaningaware response scoring for uncertainty estimation in generative LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 77527767, 2024. doi: 10.18653/v1/2024.acl-long.419. URL https: //aclanthology.org/2024.acl-long.419/. Ciosek, K., Felicioni, N., and Ghiassian, S. Hallucination detection on budget: Efficient Bayesian estimation of semantic entropy. arXiv preprint arXiv:2504.03579, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168v2, 2021. Cover, T. M. and Thomas, J. A. Elements of Information Theory. Wiley-Interscience, 2 edition, 2006. ISBN 9780471241959. Da, L., Liu, X., Dai, J., Cheng, L., Wang, Y., and Wei, H. Understanding the uncertainty of LLM explanations: perspective based on reasoning topology. arXiv preprint arXiv:2502.17026v2, 2025. Detommaso, G., Bertran, M., Fogliato, R., and Roth, A. Multicalibration for confidence scoring in LLMs. arXiv preprint arXiv:2404.04689v1, 2024. Farquhar, S., Kossen, J., Kuhn, L., and Gal, Y. Detecting hallucinations in large language models using semantic entropy. Nature, 630:625630, 2024. doi: 10.1038/ s41586-024-07421-0. Feng, Z., Zhou, H., Zhu, Z., Qian, J., and Mao, K. Unveiling and manipulating prompt influence in large language models. arXiv preprint arXiv:2405.11891v1, 2024. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783v3, 2024. Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599v2, 2017. Guo, J., Wu, Y., Qiu, J., Huang, K., Juan, X., Yang, L., and Wang, M. Temporal consistency for LLM reasoning process error identification. arXiv preprint arXiv:2503.14495v1, 2025. Heo, J., Xiong, M., Heinze-Deml, C., and Narain, J. Do LLMs estimate uncertainty well in instruction-following? In International Conference on Learning Representations, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685v2, 2021. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798v2, 2023. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Dodds, Z. H., DasSarma, N., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, S., Fort, S., Ganguli, D., Hernandez, D., Jacobson, J., Kernion, J., Kravec, S., Lovitt, L., Ndousse, K., Olsson, C., Ringer, S., Amodei, D., Brown, T., Clark, J., Joseph, Diagnosing Dynamic Instability in LLM Reasoning N., Mann, B., McCandlish, S., Olah, C., and Kaplan, J. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221v2, 2022. Ma, H., Liu, J., Fan, L., Wu, T.-H., Liang, F., and Wang, X. Semantic energy: Detecting LLM hallucination beyond entropy. arXiv preprint arXiv:2508.14496, 2025. Khalifa, M., Agarwal, R., Logeswaran, L., Kim, J., Peng, H., Lee, M., Lee, H., and Wang, L. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. MacKay, D. J. C. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003. ISBN 9780521642989. Kossen, J., Han, J., Razzak, M., Schut, L., Malik, S., and Gal, Y. Semantic entropy probes: Robust and cheap hallucination detection in LLMs. arXiv preprint arXiv:2406.15927v1, 2024. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., Zhang, L. M., McKinney, K., Shrivastava, D., Paduraru, C., Tucker, G., Precup, D., Behbahani, F., and Faust, A. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Kumar, K. Detecting token-level hallucinations using variance signals: reference-free approach. arXiv preprint arXiv:2507.04137v3, 2025. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691v2, 2021. Li, R., Long, J., Qi, M., Xia, H., Sha, L., Wang, P., and Sui, Z. Towards harmonized uncertainty estimation for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025a. doi: 10.18653/v1/2025.acl-long. 1118. URL https://aclanthology.org/2025. acl-long.1118/. Li, X. L. and Liang, P. continuous prompts for generation. arXiv:2101.00190v1, 2021. Prefix-tuning: Optimizing arXiv preprint Li, Z., Shareghi, E., and Collier, N. Visualisation of reasoning paths. arXiv:2503.03979v1, 2025b. Reasongraph: arXiv preprint Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050v1, 2023. Liu, D., Nassereldine, A., Yang, Z., Xu, C., Hu, Y., Li, J., Kumar, U., Lee, C., Chu, Z., Song, L., Xiao, C., Choi, Y., Chang, K.-W., Tsvetkov, Y., and Smith, N. A. Large language models have intrinsic self-correction ability. arXiv preprint arXiv:2406.15673v2, 2024a. Malinin, A. and Gales, M. J. F. Predictive uncertainty estimation via prior networks. arXiv preprint arXiv:1802.10501v2, 2018. Manakul, P., Liusie, A., and Gales, M. J. F. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896v3, 2023. Mao, Z., Venkat, A., Bisliouk, A., Kothiyal, A., Subramanian, S. K., Singhu, S., and Ruchkin, I. Confidence over time: Confidence calibration with temporal logic for large language model reasoning, 2026. Nikitin, A., Kossen, J., Gal, Y., and Marttinen, P. Kernel language entropy: Fine-grained uncertainty quantification for LLMs from semantic similarities. arXiv preprint arXiv:2405.20003, 2024. Niu, M., Haddadi, H., and Pang, G. Robust hallucination detection in LLMs via adaptive token selection. arXiv preprint arXiv:2504.07863, 2025. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. Incontext learning and induction heads. arXiv preprint arXiv:2209.11895v1, 2022. Peng, J., Wang, M., Zhao, X., Zhang, K., Wang, W., Jia, P., Liu, Q., Guo, R., and Liu, Q. Stepwise reasoning error disruption attack of LLMs. arXiv preprint arXiv:2412.11934v5, 2024. Quevedo, E., Yero, J., Quesada, A., and Mart inez, Y. Detecting hallucinations in large language model generation: token probability approach. arXiv preprint arXiv:2405.19648, 2024. Shannon, C. E. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x. Liu, G., Qi, Z., Zhang, X., Cheng, L., and Johnson, K. M. Self-correction is not an innate capability in large language models. arXiv preprint arXiv:2410.20513v7, 2024b. Tyen, G., Mansoor, H., Carbune, V., Chen, P., and Mak, T. LLMs cannot find reasoning errors, but can corarXiv preprint rect them given the error location. arXiv:2311.08516v3, 2023. 10 Diagnosing Dynamic Instability in LLM Reasoning Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171v4, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903v6, 2022. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. Qwen2 technical report. arXiv preprint arXiv:2407.10671v4, 2024. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600v1, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Wang, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629v3, 2022. Yu, W., Jiang, Z., Dong, Y., and Feng, J. Reclor: reading comprehension dataset requiring logical reasoning, 2020. URL https://arxiv.org/abs/2002.04326. Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Liu, B. Y. D., Zhou, J., and Lin, J. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. 11 Diagnosing Dynamic Instability in LLM Reasoning distributions. The results below are intentionally limited in scope: they do not assert that instability causes failure, only that large observable distributional changes are consistent with substantial internal state changes, and that timing effects admit simple recoverability interpretation. Proofs are provided in section C. These statements are not used to estimate hidden-state distances or to claim causal mechanisms. The goal is not to model Transformers in full detail, but to formalize two core ideas: observable shifts in nexttoken distributions are mathematically linked to changes in internal state under mild local conditions, and the timing of such shifts matters for recoverability under finite decoding budget. Where possible we state explicit constants, but our use of these results is qualitative: none of the bounds are used to set thresholds or guarantee prediction performance, and trajectory-dependent constants (e.g., curvature terms) may vary widely or degenerate in certain decoding regimes. Autoregressive reasoning as dynamical system. We model autoregressive inference as discrete-time closedRd denote an unobserved loop system. Let ht internal state at generation step and let xt be the emitted token. The state evolves as ht+1 = (ht, xt). (5) The next-token distribution is obtained from logits zt via an output map and softmax: zt = ht + b, pt = softmax(zt), (6) RV and is the vocabulary size. In blackwhere box settings we only observe pt, truncated and renormalized top-k approximation of pt, and we therefore compute observables from pt throughout. Assumptions. We adopt the following local technical assumptions (standard in local stability analyses) only to make the bounds below well-defined. (A1) Local smoothness. The state transition (h, x) is locally differentiable with respect to along the trajectories considered. 1 (A2) Observable gain of the output map. Let Π := 11 be the projection onto 1 (removing the softmax shift invariance). There exists local constant σW > 0 along the trajectories considered such that 2 for all relevant state differences ΠW u. σW 2 These assumptions do not guarantee global stability property for the model. They only support local connection between changes in internal state and changes in observable 12 Figure 4. Accuracy by peak instability position on the 100-trace held-out baseline run (same setting as Table 2). Early peaks allow recovery and yield higher accuracy; late peaks leave insufficient time to recover. Figure 5. Failure-mode distribution on GSM8K (300 examples) for Llama-3.2-1B-Instruct greedy. We partition wrong traces into three disjoint categories for interpretability: stable wrong (wrong traces in the lowest quintile of instability strength S), early collapse (wrong traces in the highest quintile of early-window strength S20 = maxt20 It), and unstable wrong (the remaining wrong traces). This breakdown highlights that non-trivial fraction of errors are stable wrong, consistent with the limitation that instability is not universal explanation of failure. A. Appendix: Additional Figures Peak-timing visualization. Figure 4 visualizes the peaktiming effect reported in section 6: earlier peak positions are more recoverable and are associated with higher finalanswer accuracy, while later peaks are associated with substantially lower accuracy, consistent with the corrective vs. destructive interpretation. Failure-mode breakdown. Figure 5 provides complementary view of error types in the same GSM8K-300 setting. It summarizes the stable-but-wrong, early-collapse, and dynamically unstable categories used in section and discussed in section J. B. Appendix: Theoretical Details B.1. Theoretical Analysis: Dynamic Instability in Autoregressive Reasoning We give compact theoretical framing that connects internal trajectory instability to observable changes in next-token Diagnosing Dynamic Instability in LLM Reasoning token distributions. We emphasize that (A2) is strong, local identifiability-type condition: it may fail globally and need not hold uniformly over all directions in practice, but is sufficient to formalize stepwise link between hiddenstate changes and projected logit changes. Definitions: finite-horizon stability and local expansion. Definition 1 (Finite-horizon stability). Given trajectory (ht)T t=0, we say it is stable over horizon if for any sufficiently small perturbation δh0 there exists > 0 such that sup tT δht2 δh02. (7) Otherwise the trajectory is dynamically unstable over horizon . Definition 2 (Local expansion rate). Let Jf (ht, xt) := h (ht, xt) and define αt := log Jf (ht, xt) op. (8) When (cid:80)t1 t=t0 αt is large and positive, small perturbations expand rapidly over the interval, indicating an internally unstable segment. These quantities are not directly observable in our black-box setting. We use them only to formalize what it means for an internal trajectory to be unstable. From internal instability to observable distributional change. Lemma 1 (Hidden-state change induces projected logit change). Under (A2), for consecutive steps, := zt1) 2 zt ht σW zt1 Π(zt ht12. (9) Lemma 2 (Observable distributional change reflects logit change). Let pt = softmax(zt) and pt1 = softmax(zt1) be consecutive next-token distributions with RV . Let Π denote the projection onto 1. logits zt, zt1 Define the trajectory-dependent curvature constant κt,traj := inf s[0,1] (cid:16) λmin J(p(s)) (cid:17) , (cid:12) (cid:12) (cid:12)1 (10) zt1), p(s) = softmax(z(s)), where z(s) = zt1 + s(zt pp is the softmax Jacobian. This and J(p) = diag(p) quantity captures the minimum local curvature of the softmax map along the segment between zt1 and zt. Then JSD(pt, pt1) κ2 t,traj zt zt1 2 . (11) Remark. The constant κt,traj is local and trajectorydependent. When the next-token distribution is nearly deterministic (for example near termination), the softmax Jacobian becomes nearly singular on 1, so κt,traj can approach zero and the bound becomes loose. This is consistent with our goal of diagnosing large transitions during uncertain and decision-critical segments; we do not claim uniform lower bound that holds across all decoding regimes. Proposition 1 (Top-k analogue (restricted simplex)). Let pt and pt1 be the renormalized top-k distributions supported Vt1. Let on Vt Vt1 and view pt, pt1 as Vt and distributions on by zero-padding outside each steps supV port. This embedding leaves each steps logged probabilities unchanged and provides shared support for divergence computation. Since any distribution on the restricted simplex with strictly positive coordinates can be written as softmax of finite logits (unique up to an additive constant), the statement of Lemma 2 applies verbatim on the restricted logit space V, yielding = JSD(pt, pt1) κ2 t,traj (cid:13) (cid:13) Π (zt zt1)(cid:13) 2 2, (cid:13) (12) where Π is the projection onto 1 in V, zt denotes logits restricted to up to an additive constant, and κt,traj is defined from the softmax Jacobian on the restricted simplex. We emphasize that the existence of such restricted logits is used only to justify the geometry of the mapping. The analysis does not require access to zt in practice. Theorem 1 (Observable instability lower-bounds internal step change). Under (A2), for consecutive steps, JSD(pt, pt1) t,trajσ2 κ2 8 ht 2 2. ht1 (13) This result formalizes why large consecutive-step distributional shifts are consistent with substantial internal state changes, supporting our use of Dt = JSD(pt, pt1) as an observable proxy for regime shifts. Entropy as proxy for decision fragility. Definition 3 (Decision margin). Let := zt,(1) zt,(2) be the gap between the largest and second-largest logits at step t. Lemma 3 (Margin robustness). If an additive logit per- < 1 2 t, then the argmax token turbation η satisfies under greedy decoding does not change. η Lemma 4 (Entropy upper-bounds peak confidence). For any distribution , H(P ) log Pmax, where Pmax := maxi (i). High entropy therefore rules out highly peaked distribution and is consistent with locally fragile decisions where several candidates are competitive. This motivates combining distributional change with an uncertainty term in It = Dt + λHt. Empirically, this complements Dt under top-k logging, consistent with our λ ablation. 13 Diagnosing Dynamic Instability in LLM Reasoning Corrective vs. destructive instability and timing effects. We interpret instability events as transitions between more stable basins of continuation. Let AG denote an empirical basin of internal states that tend to converge to correct answers and let AB denote an empirical basin of internal states that tend to converge to incorrect answers. transiAB AG corresponds to corrective instability, while tion transition AG AB corresponds to destructive instability. Stabilization assumption. We formalize the recoverability intuition with stylized stabilization-time assumption: entering (or re-entering) the good basin may require minimum number of remaining steps before the trajectory reliably terminates correctly. Let Correct denote the event that the final answer at step is correct. We assume there exist τmix (0, 1) such that: and δ (S1) Sufficient budget implies high success. For any step t, if ht AG and Pr(Correct τmix, then δ. (14) τmix) ht AG, 1 (S2) Fresh entry with insufficient budget is unreliG AG such < τmix, then able. There exists an entry subset entry that if ht Pr(Correct < τmix) entry and entry , δ. (15) ht Intuitively, τmix captures finite-horizon stabilization or mixing time needed after corrective transition before the trajectory reliably terminates correctly. Theorem 2 (Late correction penalty). Assume (S1) and (S2). entry If corrective transition occurs at step and ht with correct final answer is at most δ. Conversely, if ht and δ. < τmix, then the probability of producing AG τmix, then the success probability is at least This formalizes the recoverability intuition underlying our empirical timing effect: late instability peaks are less recoverable given limited remaining decoding budget. Summary. B.2. Implementation Note: Top-k Truncation We use top-k logging because it is available in many blackbox inference settings and keeps storage and compute tractable at per-step resolution. Truncation and renormalization can affect both entropy and JSD: when consecutive steps have low support overlap (high support-set turnover), computing divergences on the union of two renormalized top-k supports can amplify spikes relative to full vocabulary Figure 6. Theory and experiment alignment on representative Llama-3.2-1B-Instruct GSM8K greedy traces. We plot step-tostep JSD Dt, entropy Ht, the local softmax Jacobian curvature proxy κ(k) computed as the smallest non-zero eigenvalue of diag(pt) pt (equivalently, the minimum eigenvalue on 1) on the renormalized top-k distribution, and the combined instability It = Dt + Ht (with λ = 1). The example traces are selected deterministically to illustrate an early-peak correct trace and late-peak wrong trace. Large JSD spikes occur during uncertain segments where κ(k) is non-negligible, consistent with Lemma 2, and late peak position aligns with the recoverability interpretation in Theorem 2. distributions. Intuitively, when the top candidates change substantially from one step to the next, the truncated distributions share little support, producing large JSD spike. To ensure our conclusions are not artifacts of particular truncation choice, we run two validation checks. First, on held-out traces we validate the corrective vs. destructive timing effect using entropy/JSD computed from full vocabulary logits (no truncation; section 6). Second, we recompute the main signal from the same logged top-50 traces using smaller effective values of (by truncating the logged list) and find qualitatively similar separability trends once is moderately large (section E). C. Appendix: Proofs This appendix provides proofs for the theoretical statements in Appendix B. C.1. Preliminaries: Softmax shift invariance Softmax is invariant to adding constants: softmax(z) = softmax(z + c1) for any scalar c. We therefore measure logit differences on the identifiable subspace 1 := RV : using the projection v, 1 = 0 { } Π := 1 11, := Πu 2. (16) C.2. Proof of projected logit lower bound on JensenShannon divergence Proof of Lemma 2. Fix consecutive logits and with = softmax(z) and = softmax(z). Define the line 14 Diagnosing Dynamic Instability in LLM Reasoning Table 3. How the theoretical statements relate to the main empirical findings."
        },
        {
            "title": "Theoretical intuition",
            "content": "JSD spikes predict failure trends High entropy indicates fragile decisions Not all instability is harmful Late peaks are associated with lower accuracy Theorem 2 Theorem 1 Lemmas above on margin and entropy Basin interpretation and timing segment z(s) = + s(z p(s) = softmax(z(s)). Define z) for [0, 1] and let κt,traj := inf s[0,1] (cid:16) λmin J(p(s)) (cid:17) , (cid:12) (cid:12) (cid:12)1 (17) where J(p) = diag(p) definition, for any vJ(p(s))v κt,traj tion, pp is the softmax Jacobian. By 1 and any [0, 1] we have 2. Let = 1 2 2 (p + q). By definim) m). (18) m) + JSD(p, q) = 1 2 KL(q 2 TV(p, m)2 and KL(q 2 KL(p Pinskers inequality (constants differ by convention) gives 2 TV(q, m)2, KL(p where TV(a, b) = 1 1 (and we use natural logab rithms). Any equivalent Pinsker constant convention would lead to the same qualitative conclusion up to universal factors. Since p), we have q) and = = 1 m) 2 (p 2 (q 2 JSD(p, q) 2 1 1 2 TV(p, q)2 = 1 8 2 by z) for q It remains to lower-bound the path z(s) = + s(z p(s) = softmax(z(s)). Then 2 2. (19) 1 8 . Consider [0, 1] and define Since J(p(s))1 = 0, we interpret λmin(J(p(s)) 1 ) as the smallest eigenvalue over unit vectors orthogonal to 1. By Cauchy-Schwarz, , hence q, p κt,traj 2 2 2 = κt,traj z . (23)"
        },
        {
            "title": "Combining the two bounds gives",
            "content": "JSD(p, q) 1 8 2 2 κ2 t,traj 8 2 , (24) as claimed. C.3. Proof of the late correction penalty Let Correct denote the event that the final answer is correct at step . Proof of Theorem 2. Let be the step at which corrective < transition occurs, so that ht τmix. By (S2), for any realization of ht , let denote the event . Assume entry < τmix. Then entry = p(1) (cid:90) p(0) p(s) ds ds = = 0 (cid:90) 0 J(p(s))(z z) ds, Pr(Correct ht , E) δ. (25) Taking expectation over the randomness of ht and applying the law of total probability, Pr(Correct E) = Eht [Pr(Correct δ. ht , E)] (26) (20) pp is the where J(p) = zsoftmax(z) = diag(p) softmax Jacobian. Because J(p)1 = 0, only the projected z). component matters: J(p(s))(z 1. Then Let := Π(z (cid:90) 1 z) = J(p(s))Π(z z) = 0 J(p(s))v ds. (21) This proves the late correction penalty. Conversely, if ht AG and τmix, then (S1) yields Pr(Correct ht AG, τmix) δ. 1 (27) Taking the inner product with and using symmetry of J(p(s)) yields D. Appendix: Detector Comparison q, = (cid:90) 1 0 (cid:90) 1 vJ(p(s))v ds = κt,traj κt,traj 2 2, 2 2 ds (22) 15 The derivative-based change-point detector produces frequent detections, while CUSUM is much more conservative under the current thresholds. In our experiments, the binary change-point label is less predictive than continuous strength, reinforcing the decision to focus on strength-based analyses in the main text. Diagnosing Dynamic Instability in LLM Reasoning Figure 7. Early-warning separability as function of prefix window length on GSM8K-300. We report AUCwrong(Sw) as function of the window size w. Separability emerges with short prefixes and improves with longer windows, supporting that instability is detectable before completion of the full reasoning trace. Figure 8. Signal ablation on Llama-3.2-1B-Instruct: comparing JSD-only (λ = 0) vs. JSD+Entropy (λ = 1). The entropy component improves discrimination, particularly in lower-instability buckets where pure JSD may saturate. E. Appendix: Top-k Sensitivity To validate truncation effects, we recompute the instability strength = maxt It from the same logged top-50 traces using smaller effective values of by truncating the stored list. Table 4 shows that the diagnostic signal weakens for very small but is qualitatively stable once is moderately large. Top-k sensitivity across models. As an additional check, we recompute the diagnostic from full-set GSM8K baseline runs while varying the diagnostic truncation level k. Table 5 shows that AUCwrong and rank association change only slightly between across all evaluated models, supporting that our conclusions are not artifacts of particular top-k choice. 20, 50, 100 { } F. Appendix: Early-Window Separability To further control for length confounds, we compute S50 = maxt50 It, the max instability within the first 50 steps, and evaluate separability using the same AUCwrong metric as in table 1. The signal remains predictive under this earlywindow restriction  (table 6)  . G. Appendix: Negative Controls and"
        },
        {
            "title": "Alternative Baselines",
            "content": "This appendix reports two reviewer-oriented controls on GSM8K (300 examples) using the same logged traces as the main results for Llama-3.2-1B-Instruct greedy. First, we include time-structure negative controls by randomizing the temporal order and recomputing strength statistics. Second, we compare against simple entropy-family baselines computed from the same per-step observables without rerunning the models. G.1. Time-Structure Negative Controls Because the peak-based strength statistic = maxt It is invariant to permutations of the sequence values, time16 { pt} structure controls are most informative for timing-aware or windowed statistics (e.g., Sw and peak position ρ), rather than for itself. We consider two temporal randomizations. (1) Shuffle : for each trace we randomly permute the per-step logged top-k distributions before recomputing Ht, Dt, and It. (2) Shuffle : for each trace we compute the original It sequence and then randomly permute it. Since the main strength statistic = maxt It is invariant to permutations of It, this control primarily targets windowed statistics such as S50. All shuffles are deterministic per example identifier. It} { G.2. Entropy-Family Baselines We compare our main strength statistic SI = maxt It against three simple alternatives computed from the same traces: SH = maxt Ht (max entropy), SH = maxt Ht (max entropy change), and SD = maxt Dt (max Ht1 JSD). We also report the early-window variants obtained by restricting the max to the first 50 steps. We report both full-trace and fixed early-window variants below. G.3. Signal Ablation (Additional) H. Appendix: Bootstrap Confidence Intervals To quantify uncertainty under the GSM8K 300-example setting, we compute percentile bootstrap confidence intervals over examples (1000 resamples) for AUCwrong and bucketed accuracy. Table 11 reports AUC confidence intervals for and S50, and Figure 9 visualizes bucket accuracy with 95% confidence intervals. I. Appendix: Peak Threshold Sensitivity We evaluate the robustness of the peak-timing interpretation by sweeping early and late thresholds for the peak position ρ = t/T on the 100-trace full-vocabulary baseline run used in Table 2. Table 12 shows that early-peak traces consistently achieve higher accuracy than late-peak traces across small threshold grid. We also plot accuracy as function of peak position in 10 equal-width bins (figure 10). Diagnosing Dynamic Instability in LLM Reasoning Table 4. Top-k sensitivity on Llama-3.2-1B-Instruct greedy (GSM8K, 300 traces). Larger yields more stable estimate of entropy/JSD and slightly stronger separability; the overall trend remains."
        },
        {
            "title": "Spearman",
            "content": "10 20 50 0.626 0.635 0.657 0.144 0.154 0.178 Table 5. Diagnostic top-k sensitivity across models on GSM8K (full test set, =1319). We recompute = maxt It using different diagnostic truncation levels and report separability. All quantities are computed from the logged top-k distributions at inference time. Within each model block, shaded cells indicate the best value across diagnostic for that metric (higher AUC; more negative Spearman). Model diag Acc AUCwrong(S) Spearman(S) Llama-3.2-1B-Inst Llama-3.2-3B-Inst Llama-3.1-8B-Inst Qwen2.5-0.5B-Inst Qwen2.5-1.5B-Inst Qwen2.5-7B-Inst 20 50 100 20 50 100 20 50 100 20 50 100 20 50 20 50 100 0.419 0.419 0.419 0.707 0.707 0.707 0.749 0.749 0.749 0.301 0.301 0.301 0.416 0.416 0. 0.500 0.500 0.500 0.653 0.662 0.665 0.691 0.698 0.700 0.684 0.689 0.691 0.717 0.716 0.715 0.639 0.641 0. 0.642 0.644 0.644 -0.262 -0.277 -0.283 -0.300 -0.312 -0.315 -0.276 -0.284 -0.286 -0.344 -0.343 -0.341 -0.237 -0.241 -0. -0.245 -0.249 -0.249 Figure 9. Bootstrap confidence intervals for bucketed accuracy on GSM8K (300 examples). Each curve plots accuracy across five equal-sized instability buckets (B1 to B5), with 95% bootstrap confidence intervals over examples (1000 resamples). Figure 10. Accuracy by peak position ρ = t/T in 10 equal-width bins on the 100-trace full-vocabulary baseline run. Bin counts are annotated above each point. J. Appendix: Mechanistic Interpretation The discussion in the main text provides brief, noncommittal mechanistic interpretation of the signal. Here we provide longer interpretation, emphasizing that these abstractions do not imply unique internal mechanism. text, and therefore the next-token distribution. In this view, regime shifts correspond to discrete reconfigurations of the effective computation performed by attention and MLP blocks, which can amplify local sensitivity in the state update map. Closed-loop amplification. Autoregressive decoding is feedback system: the emitted token becomes part of the next input, updating the residual stream and attention conDecision fragility and near ties. Our instability signal combines realized distributional change with uncertainty. When top candidates are close in score, small state changes 17 Diagnosing Dynamic Instability in LLM Reasoning Table 6. Early-window separability using S50 = maxt50 It (GSM8K, 300 traces). AUC remains above chance under the fixed early-window restriction."
        },
        {
            "title": "Model",
            "content": "Decoding AUCwrong(S) AUCwrong(S50) Qwen2.5-1.5B-Inst Llama-3.2-1B-Inst greedy τ = 0.0 0.681 0.657 0.605 0.665 Table 7. Time-structure negative controls on GSM8K (300 examples) for greedy decoding. Bucket slope is measured as the accuracy difference between bucket B5 and B1. Shuffling {It} does not affect = maxt It by definition. Model Setting AUCwrong (S) Spearman (S) B5B1 (S) Llama-3.2-1B-Instruct original Llama-3.2-1B-Instruct Llama-3.2-1B-Instruct shuffle {pt} shuffle {It} 0.657 0.652 0.657 0.178 0.183 0.173 0.150 0.178 0.183 can flip the argmax. In Transformer, such near ties can arise when multiple partial solutions remain compatible with the prefix, leading to competing attention patterns and feature activations. In this regime, entropy is proxy for proximity to decision boundary, and it complements JSD when divergence estimates are compressed under top-k logging. Route switching and local curvature. Large distributional shifts can reflect route switching, where attention reallocates from one subset of context tokens to another as the model commits to an intermediate value or revises an earlier step. Prior work suggests that Transformers can implement discrete, circuit-like behaviors in context (Olsson et al., 2022), which is compatible with abrupt changes in the effective computation and observed JSD spikes. Basins and stabilization time. The basin sets AG and AB and the stabilization time τmix provide compact way to state the recoverability intuition behind corrective versus destructive instability. In Transformer, these basins can be interpreted as regions of residual-stream activation space that reliably produce continuations consistent with correct or incorrect solution. The stabilization time can be interpreted as the number of subsequent steps required for the model to propagate corrected intermediate value through the remaining reasoning and final-answer formatting, which explains why late instability peaks are less recoverable. K. Appendix: Peak-Step Probes To probe differences at the peak step = arg maxt It, we analyze two additional metrics: margin drops and supportset turnover. (Log-)probability margin drop. Let = log p(1) log p(2) be the gap between the top-2 log probabilities at and p(2) step under the renormalized top-k distribution pt. Here p(1) denote the largest and second-largest probabilt ities under pt (ranked values), not powers. We measure the margin drop t1 as proxy for how abruptly the model transitions from locally dominant next-token preference to more competitive state. We use log probabilities under pt as black-box proxy; analyzing raw logit margins would require white-box access but is conceptually aligned with the same mechanistic interpretation. / Tt Tt1 , and define turnover as 1 Support-set turnover. Let Tt be the top-10 token set at step t. We compute the Jaccard overlap = Tt J. We Tt1 use top-10 here to focus on the most competitive candidates at the peak step, while pt in our main signal uses top-k (typically = 50) for more stable entropy/JSD estimate. High turnover implies low consecutive-step support overlap, which contributes directly to large JSD spikes under truncated distributions. Table 13 reveals key distinction: correct traces exhibit larger margin drops (1.82 vs. 1.05) at the peak step t, while wrong traces exhibit higher support-set turnover (84% vs. 82%). We treat these metrics as descriptive characterizations; some effect sizes (e.g., turnover) are modest, but the patterns are consistent under pooling across decoding settings. L. Appendix: Additional Full-Set Runs To complement the main analyses, we report dense per-step baseline runs for six models on the full GSM8K test set (1319 examples) and on the HotpotQA distractor validation split (7405 examples). For each example we compute = maxt It from the logged per-step instability and evaluate separability via AUCwrong. We also report S50 = maxt50 It as an early-window control, and bucket accuracy by five equal-sized quantile buckets of S. Hot18 Diagnosing Dynamic Instability in LLM Reasoning Table 8. Time-structure negative controls on GSM8K (300 examples) for greedy decoding, evaluated with the early-window statistic S50 = maxt50 It. Shuffling reduces early-window separability, consistent with the role of temporal organization in windowed diagnostics. Model Setting AUCwrong (S50) Spearman (S50) B5B1 (S50) Llama-3.2-1B-Instruct original Llama-3.2-1B-Instruct Llama-3.2-1B-Instruct shuffle {pt} shuffle {It} 0.664 0.641 0.617 0.187 0.167 0.161 0.133 0.134 0.117 Table 9. Entropy-family baselines on GSM8K (300 examples) for greedy decoding. JSD-only statistics are near chance, consistent with our λ ablation. Model Statistic AUCwrong(S) Spearman(S) Llama-3.2-1B-Instruct SI = maxt It Llama-3.2-1B-Instruct SH = maxt Ht Llama-3.2-1B-Instruct SH = maxt Ht Ht1 Llama-3.2-1B-Instruct SD = maxt Dt 0.657 0.650 0.563 0.511 0.178 0.171 0.072 0.015 arg maxt50 It (breaking ties by the smallest t) and ρ50 = 50/50 on the same held-out baseline run as Table 2. The qualitative ordering remains: later peak positions within the same window correspond to lower accuracy  (table 15)  . N. Appendix: Reproducibility and"
        },
        {
            "title": "Implementation Details",
            "content": "All results are generated via the project scripts in this repository with fixed configuration files for dataset, model, decoding settings, and logging parameters. The pipeline consists of: (i) generating traces by logging per-step top-k token log probabilities during decoding, (ii) computing Ht, Dt, and It from the renormalized top-k distributions as described in the Method section, and (iii) aggregating per-trace statistics (S and early-window variants), bucket summaries, and figures used in the paper. Unless otherwise noted, we use the GSM8K test split with 300 problems, log top-50 tokens per-step, and cap generation at 128 new tokens. The full-set runs use the GSM8K test split (1319 examples) and the HotpotQA distractor validation split (7405 examples), evaluated with the same per-trace metrics and bucket procedure. For stochastic decoding we use temperature τ = 0.7 with nucleus sampling (p = 0.9) and fixed random seed; for Llama-3.2-1B-Instruct we additionally run τ = 0.3 under the same and seed. We also include small full vocabulary validation run for key timing claims (section 6). Figure 11. Bucketed accuracy trends for the full-set runs in table 14. Each curve plots accuracy across five equal-sized instability buckets (B1 to B5), showing monotonic decline as instability increases on both tasks and for all models. Figure 12. Cross-task validation on fixed model (Llama-3.2-1B). We plot bucketed accuracy trends (B1 to B5) by instability strength on GSM8K, HotpotQA, and ReClor. On GSM8K and HotpotQA, accuracy declines with instability. On ReClor in this short-answer multiple-choice setting, the trend inverts: most errors are stablebut-wrong, and the rare correct traces exhibit higher instability. potQA correctness uses short-answer match on the first line of the generated output after normalization, with containment check against the reference. The results of these full-set runs are summarized below. M. Appendix: Peak Position in Fixed Early"
        },
        {
            "title": "Window",
            "content": "To control for time-budget confounds in the corrective vs. destructive analysis, we compute peak position within fixed early window (first 50 steps). Let 50 = 19 Table 10. Entropy-family baselines on GSM8K (300 examples) for greedy decoding, evaluated in the fixed early window (first 50 steps). Diagnosing Dynamic Instability in LLM Reasoning Model Statistic AUCwrong(S50) Spearman(S50) Llama-3.2-1B-Instruct S50,I = maxt50 It Llama-3.2-1B-Instruct S50,H = maxt50 Ht Llama-3.2-1B-Instruct S50,H = maxt50 Ht Ht1 Llama-3.2-1B-Instruct S50,D = maxt50 Dt 0.664 0.648 0.544 0.418 0.187 0.169 0.050 0.140 Table 11. Bootstrap confidence intervals for AUCwrong on GSM8K (300 examples). Intervals are percentile 95% confidence intervals over 1000 bootstrap resamples. Model Decoding AUCwrong (S) [95% CI] AUCwrong (S50) [95% CI] Llama-3.2-1B-Instruct τ = 0.0 Llama-3.2-1B-Instruct τ = 0.3 Llama-3.2-1B-Instruct τ = 0.7 0.657 [0.553, 0.756] 0.664 [0.545, 0.769] 0.667 [0.571, 0.757] 0.679 [0.584, 0.772] 0.741 [0.636, 0.834] 0.721 [0.614, 0.808] Table 12. Peak-timing threshold sweep on the 100-trace full-vocabulary baseline run. Gap denotes Accearly minus Acclate. Early ρ Late ρ Nearly Accearly Nlate Acclate Gap 0.20 0.20 0.20 0.25 0.25 0.25 0.30 0.30 0.30 0.45 0.50 0.60 0.45 0.50 0. 0.45 0.50 0.60 39 39 39 57 57 57 68 68 68 0.538 0.538 0.538 0.456 0.456 0. 0.441 0.441 0.441 19 0.211 0.328 14 0.143 0.396 8 0.125 0.413 19 0.211 0.246 14 0.143 0.313 8 0.125 0.331 19 0.211 0.231 14 0.143 0.298 8 0.125 0.316 Table 13. Peak-step characteristics for correct vs. wrong traces (Llama-3.2-1B-Instruct, GSM8K). We aggregate 300 problems across three decoding settings (greedy; and stochastic sampling with τ {0.3, 0.7}, top-p 0.9), yielding 900 traces. Correct traces show larger margin drops (decisive transition); wrong traces show higher turnover (chaotic support-set change). (For per-setting robustness across this grid, see Table 1; here we pool settings for mechanistic characterization.) Margin at Margin drop Jaccard overlap Turnover (1 J) Correct Wrong 0.30 0.45 1.82 1. 0.18 0.16 0.82 0.84 Table 14. Dense per-step runs on GSM8K and HotpotQA (full test/validation splits), plus ReClor validation subset (300 examples) for Llama-3.2-1B-Instruct under stochastic decoding. We report separability via AUCwrong for the full-trace peak strength = maxt It and an early-window control S50 = maxt50 It. Model Dataset Accuracy AUCwrong (S) Spearman AUCwrong (S50) 1319 Llama-3.2-1B-Inst GSM8K 1319 Llama-3.2-3B-Inst GSM8K 1319 Llama-3.1-8B-Inst GSM8K 1319 Qwen2.5-0.5B-Inst GSM8K 1319 Qwen2.5-1.5B-Inst GSM8K Qwen2.5-7B-Inst 1319 GSM8K Llama-3.2-1B-Inst HotpotQA 7405 Llama-3.2-3B-Inst HotpotQA 7405 Llama-3.1-8B-Inst HotpotQA 7405 Qwen2.5-0.5B-Inst HotpotQA 7405 Qwen2.5-1.5B-Inst HotpotQA 7405 HotpotQA 7405 Qwen2.5-7B-Inst 300 ReClor Llama-3.2-1B-Inst 0.434 0.707 0.749 0.301 0.416 0.500 0.483 0.632 0.704 0.410 0.537 0.675 0.027 0.673 0.687 0.692 0.714 0.643 0.644 0.594 0.662 0.675 0.568 0.603 0.654 0.054 0.298 0.295 0.288 0.341 0.244 0.250 0.163 0.271 0.277 0.116 0.179 0.250 0.250 0.615 0.618 0.598 0.654 0.565 0.581 0.594 0.662 0.675 0.575 0.603 0.654 0.046 Diagnosing Dynamic Instability in LLM Reasoning Table 15. Accuracy by peak position within fixed early window (same 100-trace full vocabulary baseline as Table 2). Even when peak position is defined within the first 50 steps, later peaks are associated with lower accuracy. Peak Position (first 50 steps) % of Traces Accuracy Early (ρ50 < 0.25) Middle (0.25 Late (ρ50 > 0.5) ρ50 0.5) 13% 34% 53% 0.54 0.47 0."
        }
    ],
    "affiliations": [
        "Dalhousie University",
        "Meta",
        "Tsinghua University",
        "University of Amsterdam"
    ]
}