{
    "paper_title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning",
    "authors": [
        "Yang Chen",
        "Zhuolin Yang",
        "Zihan Liu",
        "Chankyu Lee",
        "Peng Xu",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Wei Ping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable."
        },
        {
            "title": "Start",
            "content": "2025-5-23 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Yang Chen, Zhuolin Yang*, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping"
        },
        {
            "title": "Abstract",
            "content": "Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, smalland mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose simple yet effective approach: first training on mathonly prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve code benchmark performance with minimal or no degradation in math results. We develop robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the models reasoning ability, enabling it to solve problems that were previously unsolvable. We release the model at: https://huggingface.co/nvidia/AceReason-Nemotron-14B. 5 2 0 2 2 2 ] . [ 1 0 0 4 6 1 . 5 0 5 2 : r Figure 1: Benchmark accuracy of AceReason-Nemotron-7B/14B on AIME25 (avg@64) and LiveCodeBench v5 (2024.08 - 2025.02, avg@8) using 32,768 output length. Equal contribution. Leads the effort. Correspondence to: Yang Chen<yachen@nvidia.com>, Zhuolin Yang<zhuoliny@nvidia.com>, Wei Ping<wping@nvidia.com>. 2025 NVIDIA. All rights reserved. AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning"
        },
        {
            "title": "4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.1 Math Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.2 Code Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Main Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.1 Response Length Grows alongside Performance . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.2 Hard Prompts Drive the Largest Gains\n4.3.3 Starting from 8K Improves Later Performance . . . . . . . . . . . . . . . . . . . . . . .\n4.3.4 Interplay of Math-only RL and Code-only RL . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.5 Topic-wise Accuracy Analysis on LiveCodeBench v5 . . . . . . . . . . . . . . . . . . . .\n4.3.6 False Positive and False Negative Rewards in Code RL Training . . . . . . . . . . . . . .\n4.3.7 Does RL Improve pass@k or pass@1 over the SFT Model? . . . . . . . . . . . . . . . .\n4.3.8 Where does RL Improve over the SFT Model? . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5 Conclusion 6 Acknowledgement Appendix A.1 Instruction for evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Example of AceReason-Nemotron-14B response to simple query . . . . . . . . . . . . . . . . . A.3 Additional Math-RL Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Code-RL Dataset Curation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 5 6 7 7 7 7 7 7 8 9 9 9 9 10 10 11 12 13 14 14 14 18 18 18 19 21 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning 1. Introduction Reasoning capabilities are fundamental component of AI. Since the introduction of OpenAI o1 (OpenAI, 2024), building reasoning models using large-scale reinforcement learning (RL) has attracted significant attention. Remarkable progress has followed the open-sourcing of DeepSeek-R1 (Guo et al., 2025), empowering the open LLM and research communities to develop state-of-the-art reasoning models through RL or distillation. However, key technical details necessary for reproduction, such as data curation strategies and the specific RL training recipe, were omitted from the original DeepSeek-R1 report (Guo et al., 2025), leaving the community scrambling to replicate its success. Subsequent efforts by different teams explored diferent model sizes (e.g., 1.5B (Luo et al., 2025), 7B (Wen et al., 2025), 14B (Luo et al., 2025), and 32B-only (Yu et al., 2025)), different initial checkpoints (e.g., base models (Yu et al., 2025) and distilled reasoning models (He et al., 2025)), and different target domains (e.g., math (Luo et al., 2025), code (Luo et al., 2025), and physical AI (Azzolini et al., 2025)). Each study demonstrates potential path to success in specific settings but lacks conclusive or consistent training recipe. Moreover, both DeepSeek-R1 (Guo et al., 2025) and Llama-Nemotron (Bercovich et al., 2025) report that distillation outperforms RL for small and mid-sized models, recommending RL only for the largest models, such as the DeepSeek-V3-671B (Liu et al., 2024) or Llama-3.1-Nemotron-Ultra-253B. The most recent release of Qwen3 adopts similar strategy (Qwen, 2025). In this work, we demonstrate that large-scale reinforcement learning (RL) can significantly enhance the reasoning capabilities of strong smalland mid-sized SFT models (DeepSeek-R1-Qwen-Distilled-7B/14B)achieving performance competitive with state-of-the-art distillation-based results at 7B, and surpassing them at 14B (Ahmad et al., 2025; Moshkov et al., 2025). Specifically, we make the following contributions: 1. We propose conducting math-only and code-only RL separately: the distilled SFT model is first trained on math-only prompts, followed by training on code-only prompts. This approach was initially motivated by training efficiency considerations, as the average verification time for code is significantly longer than that for math. Subsequently, we found two exciting observations: i) Math-only RL significantly boosts the performance of strong distilled models not only on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also on code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench v5 for the 7B / 14B models); see Table 1 for details. ii) Extended iterations of code-only RL lead to minimal or no degradation on math reasoning tasks (e.g., +1.0% / -0.8% on AIME 2024 / 2025 for the 7B model); see Table 4 for details. These observations contrast with domain-specific supervised fine-tuning (SFT), which can lead to catastrophic forgetting and degraded performance on other domains. 2. We develop and share systematic data curation recipe to collect high quality math problems with verifiable answers, as well as coding descriptions with test cases, ensuring that all data is reliable and testable. We will open-source the dataset for the benefit of the community. 3. To ensure consistent conclusions, we examine the RL training recipe through detailed ablation studies and analysis under state-of-the-art settings. Our findings include: i) Curriculum learning with progressively increasing maximum response length improves both training efficiency and final accuracy on reasoning benchmarks. ii) On-policy parameter updates stabilize the RL process. iii) RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), as evidenced by significant improvements in pass@1, but also expands the models capabilities to solve previously unsolvable problems, as demonstrated by substantial gains in pass@64. 3 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning 2. Related Work Training LLMs to reason has been long-standing research focus (Wei et al., 2022), especially in the domains of code (Chen et al., 2021) and math (Cobbe et al., 2021). In recent years, major development efforts have focused on acquiring reasoning capabilities by training on math and code data during both the pretraining and supervised fine-tuning (SFT) stages (Grattafiori et al., 2024; Guo et al., 2024; Liu et al., 2024; Shao et al., 2024; Yang et al., 2024). Reinforcement learning (RL) has previously been explored for math reasoning using reward models tailored to the math and code domains (Shao et al., 2024; Yang et al., 2024). However, the gains have been limited, largely due to the inherent challenges of reward modeling in mathematical and coding domains (Lightman et al., 2023; Liu et al., 2024,). The release of OpenAI o1 (OpenAI, 2024), and especially the open-sourcing of DeepSeek-R1 (Guo et al., 2025), highlights the effectiveness of large-scale RL through rule-based verification. In the case of math problems with deterministic answers, models are required to output the final result in specific format (e.g., boxed), enabling accurate rule-based verification (e.g., Liu et al., 2024; Yang et al., 2024). For code problems, feedback is provided through compilation and execution against predefined test cases (e.g., Luo et al., 2025; Zeng et al., 2025). Due to the absence of key implementation details in frontier models, such as RL training recipes and data curation strategies, subsequent works have explored and shared data curation methods (He et al., 2025; Luo et al., 2025,), and introduced various techniques to improve and stabilize the widely adopted GRPO training (Shao et al., 2024). These include progressively increasing the maximum response length (He et al., 2025; Luo et al., 2025,), clip-higher to mitigate entropy collapse (Yu et al., 2025), and overlong filtering to avoid penalties from truncated generations within the maximum response length (Yu et al., 2025). Many of these efforts focus exclusively on either the math domain (Chen et al., 2025; Luo et al., 2025; RL Lab, 2025; Yu et al., 2025) or the code domain (Luo et al., 2025; Zeng et al., 2025), highlighting the difficulty of handling heterogeneous prompts and inherent complexity of RL training. Furthermore, the range of reported benchmarks remains limited, typically to AIME 2024 / 2025, and LiveCodeBench (Jain et al., 2024), compared to broader evaluations in frontier reasoning models (Guo et al., 2025; Qwen, 2025). Another line of follow-up work focuses on distilling existing frontier reasoning models, which are originally trained via RL (Guo et al., 2025; Qwen, 2025), through strong-to-weak distillation using rejection sampling (Ahmad et al., 2025; Bercovich et al., 2025; Moshkov et al., 2025), as prior studies have found that RL yields suboptimal results for smaller models compared to distillation (Bercovich et al., 2025; Guo et al., 2025). In this work, we initiate RL from strong distilled models, and show that it can achieve results that are competitive with or surpass existing state-of-the-art distillation-based approaches on math (Moshkov et al., 2025) and code (Ahmad et al., 2025). 3. Method 3.1. Framework We adopt the GRPO algorithm (Shao et al., 2024), as used in DeepSeek-R1, instead of PPO (Schulman et al., 2017), due to its simplicity and the advantage of not requiring separate value function model. For each question-answer pair (𝑞, 𝑎), we sample the policy model 𝜋𝜃old to generate group of 𝐺 individual rollouts {𝑜𝑖}𝐺 𝑖=1. We assign reward score 𝑆𝑖 = 𝑆(𝑜𝑖, 𝑎) to each response 𝑜𝑖, given the oracle answer 𝑎, using rule-based reward function 𝑆, and compute the advantage as the normalized reward scores across the group {𝑆𝑖}𝐺 𝑖=1. 𝒥GRPO(𝜃) = (𝑞,𝑎)𝒟, {𝑜𝑖}𝐺 𝑖=1𝜋𝜃old (𝑞) [ 1 𝑖=1 𝑜𝑖 𝐺 𝐺 𝑜𝑖 ( 𝑖= 𝑡=1 ( ( 𝑟𝑖,𝑡(𝜃) ˆ𝐴𝑖,𝑡, clip min 𝑟𝑖,𝑡(𝜃), 1 𝜀, 1 + 𝜀 ) ) ˆ𝐴𝑖,𝑡 )] (1) 𝛽𝐷KL(𝜋𝜃𝜋ref) , AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning where 𝑟𝑖,𝑡(𝜃) = 𝜋𝜃(𝑜𝑖,𝑡 𝑞, 𝑜𝑖,<𝑡) 𝜋𝜃old(𝑜𝑖,𝑡 𝑞, 𝑜𝑖,<𝑡) , ˆ𝐴𝑖,𝑡 = 𝑆𝑖 mean({𝑆𝑖}𝐺 std({𝑆𝑖}𝐺 𝑖=1) 𝑖=1) . (2) Our experiments strictly follow the on-policy training method, performing only one gradient update after the model generates group of 𝐺 rollouts, to ensure stable RL training and prevent entropy collapse. We also remove the KL divergence term by setting 𝛽 = 0. We started RL experiments from distilled reasoning models such as DeepSeek-R1-Qwen-Distilled-7B/14B (Guo et al., 2025), ensuring that the experiments were conducted under controlled conditions without introducing variations in distillation data or fine-tuning compute. We use RL framework veRL (Sheng et al., 2024) with implementation of token-level loss (Yu et al., 2025) of GRPO and inference engine vLLM (v0.7.3) (Kwon et al., 2023). Our custom modification includes adding math and code reward functions (verifiers) to the implementation. All experiments are conducted using 128 NVIDIA H100 GPUs. Reward functions: For verification of math problems, we employ rule-based Python verification function built on top of sympy, following the approach of AceMath (Liu et al., 2024). Specifically, it relies on antlr4-python3runtime (v4.11.1) and sympy (v1.12). This specific configuration is crucial for ensuring accurate symbolic equivalence. We extract the answer from boxed{} appearing after the <think> token and assign rewards strictly based on the correctness of this answer (1 for correct, 0 for incorrect), without applying any format-based rewards or length penalties. Using process pool with 64 workers, the average verification time is approximately 3.9 seconds per 1,024 instances. For coding problem verification, we utilize local sandbox verifier, following the code execution tools implemented in the LiveCodeBench repository (Jain et al., 2024). Given the models output, we extract the code generated within python[code] code block that follows <think> token. Binary rewards are then assigned based on code execution outcome on full set of test cases. positive reward will be granted if and only if the extracted code successfully passes all test cases within the specific time limit. Using process pool with 64 workers, the average verification time for code is approximately 552.4 seconds per 1,024 instances. Given the significant difference in verification time between math and code, we propose conducting math-only and code-only RL separately. 3.2. Math-only RL 3.2.1. Data Curation We developed data collection and verification pipeline to generate high-quality mathematical data for RL training. Our dataset combines DeepScaler (Gao et al., 2024; Luo et al., 2025; Min et al., 2024) and NuminaMath (Li et al., 2024), covering algebra, combinatory, number theory, and geometry. We apply 9-gram filtering to avoid contamination with common math benchmarks and implement filtering rules to exclude unsuitable data, such as questions involving multiple sub-questions, multiple-choice or true/false questions, overly long or complex answers, proof-based questions, non-English content, references to figures, or excessively brief prompts. Since NuminaMath data often originates from online sources processed through OCR and parsing tools, it contains considerable noise due to incorrect questions or answers. To address this, we use the DeepSeek-R1 model with up to eight attempts per question, retaining only those that achieve correct majority-voted solutions via rule-based verifier. Questions that are consistently unsolvable by DeepSeek-R1 often exhibit ambiguity or OCR-related errors upon human inspection and are therefore discarded. We further filter out questions requiring fewer than 2,000 R1 response tokens to answer, as we consider these questions to be solvable without extensive reasoning, and downsample problems with responses of 2,0004,000 tokens to balance the dataset 5 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Table 1: Math-only RL improves code reasoning, demonstrating cross-domain generalization through reinforcement learning. In contrast, math-only SFT can yield poor performance on code benchmark. Models AIME24 avg@64 AIME25 avg@ LCB v5 avg@8 DeepSeek-R1-Distill-Qwen-7B AceReason-Nemotron-7B (math-only RL) OpenMath-Nemotron-14B (math-only SFT) DeepSeek-R1-Distill-Qwen-14B AceReason-Nemotron-14B (math-only RL) 55.5 69.0 76. 69.7 78.6 39.0 53.6 63.0 50.2 67.4 37.6 44.4 (6.8) 19. 53.1 58.9 (5.8) based on response length. Our final, rigorously verified dataset contains approximately 49,000 high-quality math problems suitable for RL training. 3.2.2. Training process RL training can be computationally intensive when involving long CoT reasoning, with around 80% training time spent on generating model outputs. To address this challenge, our RL pipeline focuses on enhancing reliability and efficiency through three primary strategies: 1) strict on-policy training to maintain stable training and prevent entropy collapse, 2) stage-wise length extension from 8K to 32K tokens, and 3) curriculum training using increasingly difficult prompts at later stages. On-policy training to stabilize entropy loss. The entropy of the policy model serves as key metric for assessing its ability to explore during RL training. In early experiments, we found applying multiple (2 or 4) gradient updates after model generation with group of 𝐺 rollouts per prompt led to rapid entropy collapse around 100 steps (see Figure 3c). In contrast, using exactly one gradient update after model generation, as in original DeepSeek-Maths GRPO implementation (Shao et al., 2024), consistently prevented collapse. We therefore adopted this strict on-policy approach throughout RL training. Length extension to accelerate training. Length extension has been shown to be effective for smaller models (e.g., the 1.5B DeepScaler (Luo et al., 2025)), but Wen et al. (2025) reported challenges in scaling to larger models, as training at an 8K response length initially led to degraded performance. In contrast, we were surprised to observe substantial performance improvements when extending training from 8K to 16K maximum response length. Based on this, we adopted stage-wise length extension strategy (8K 16K 24K 32K) to enable more efficient training, as directly starting from 16K or 24K resulted in suboptimal results (see Figure 3b). Harder problems to push the model. We used curriculum learning by introducing more difficult prompts during the 24K and 32K stages. As the model mastered easier examples, their advantage reach 0 in the GRPO objective. We filtered prompts by model pass rate, filtering out those with pass rate > 6/16, which significantly improves model performance  (Table 3)  . Math RL improves code reasoning. In Table 1, we show performing math RL not only improves math reasoning on AIME24/25 but also boosts LiveCodeBench v5 score to 44.4% (6.8% ) for 7B and 58.9% (5.8% ) for 14B, which already outperforms the very recent code RL model DeepCoder-14B (57.9%) (Luo et al., 2025). Furthermore, we show that math-only RL improves coding performance across all problem topicsnot just math-related coding tasks (see Figure 4 in section 4.3.5). This cross-domain generalization is compelling advantage of reinforcement learning. In contrast, domain-specific supervised fine-tuning (SFT) often results in poor performance on other domains. We used batch size of 128, sampling 𝐺 = 8 responses per prompt for 8K length training and 16 responses otherwise. We adopted learning rate of 1 106 with AdamW (Kingma, 2014), and set both the entropy loss coefficient and KL loss coefficient 𝛽 to 0. 6 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning 3.3. Code-only RL 3.3.1. Data Curation We curated our code-only RL training dataset from modern competitive programming platforms using strict selection criteria to ensure high-quality coding problems. The dataset includes both function-calling and standard input/output (stdin/stdout) formats and covers wide range of algorithmic topics, including graph theory, data structures, number theory, greedy algorithms, and more. To ensure stability for RL training, we filtered out problems incompatible with standard output comparison (e.g., multi-solution or interactive problems requiring special judges) or those needing platform-specific templates, thereby minimizing potential false negative reward. Furthermore, we curated strong testcases covering tricky edge cases or extreme cases under input limitations, ensuring that incorrect solutions would fail and thereby eliminating potential false positive reward. As discussed in Sec. 4.3.6, both false positive reward and false negative reward can obfuscate RL training by introducing noisy reward signals. To gauge difficulty, we evaluated each problem using DeepSeek-R1-671B with 8 rollouts, assigning difficulty score from 0 to 8. Problems where the model failed all 8 attempts (level 8) were excluded. Finally, we performed careful benchmark decontamination and problem deduplication across platforms using n-gram context analysis and original URL matching (see App. A.4 for details). After such aggressive filtering process, 8,520 problems remained, forming our final training set. 3.3.2. Training Process We apply the two-stage code-only RL pipeline designed to accommodate models of varying scales. The pipeline leverages training sets composed of coding problems within specific difficulty ranges, along with customized settings for maximum response length and sampling temperature. Stage 1 initiates the code RL process, launching after prior math-only RL to ensure training stability. In Stage 1, training data is constructed by difficulty: problems with difficulty up to level 5 are used for 7B model, while problems up to level 7 are used for 14B model. We set maximum response length as 24,000, temperature as 0.6 and number of rollouts as 8 for Stage 1 training. Stage 2 employs the full set of training problems with 32,768 maximum response length. In this stage, we implement an epoch-wise filtering strategy by filtering out relatively easy problems w.r.t. prior epoch checkpoints and gradually increasing the sampling temperature from 0.6 to 1.0, number of rollouts from 8 to 16 across epochs. This aims to encourage policy convergence while encouraging exploration. We set batch size to 128 and learning rate to 5 106 with AdamW, continuing training in both stages until policy converges. Regarding the reward function, we adopt the strict rule-based reward: positive reward 1 is granted if and only if the generated code successfully passes all test cases for the given problem. As for efficient evaluation, we deploy parallelized local verifier to check testcase correctness. 4. Evaluation 4.1. Experimental Setup Our experiments start from strong SFT models, DeepSeek-R1-Distill-Qwen-7B and 14B, which are based on the Qwen2.5 model family (Yang et al., 2024) and distilled from DeepSeek-R1 (Guo et al., 2025). To ensure consistency and reproducibility, we follow the DeepSeek-R1 evaluation protocol, using temperature of 0.6, top-p of 0.95, and maximum output length of 32,768 tokens. 4.1.1. Math Evaluation We use diverse math competition benchmarks, including AIME2024, AIME2025, MATH500 (Hendrycks et al., 2021), in addition with HMMT2025 Feb and BRUMO2025 from MathArena (Balunović et al., 2025). Due to the high variance in outputs from reasoning models when using sampling, we report pass@1 performance averaged over 𝑘 generations (avg@𝑘). For small-scale benchmarks such as AIME, we use 𝑘 = 64, following DeepSeek-R1. 7 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Table 2: Math and Code reasoning evaluation. We report pass@1 averaged over 𝑘 generations (avg@𝑘) following the DeepSeek-R1 evaluation framework (template, temperature=0.6, top_p=0.95, max response length=32,768). By default, we report self-reported numbers from model developers if they are available. Otherwise, we evaluate the model using the same evaluation setting, or we collected from MathArena or LiveCodeBench leaderboard. Models QwQ-32B DeepSeek-R1-671B Llama-Nemotron-Ultra-253B o3-mini (low) o3-mini (medium) AReal-boba-RL-7B Skywork-OR1-Math-7B OlympicCoder-7B Light-R1-7B Light-R1-14B DeepCoder-14B (32K-Inference) OpenMath-Nemotron-7B OpenMath-Nemotron-14B OpenMath-Nemotron-32B OpenCodeReasoning-Nemotron-7B OpenCodeReasoning-Nemotron-14B Llama-Nemotron-Nano-8B-v1 Llama-Nemotron-Super-49B-v DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-70B AceReason-Nemotron-7B AceReason-Nemotron-14B AIME 2024 avg@64 2025 avg@64 MATH 500 avg@4 HMMT 2025 avg@ BRUMO 2025 avg@64 LiveCodeBench Codeforces v5 avg@8 v6 avg@8 ELO pass@ Percentile pass@1 EvalPlus avg@4 79.5 79.8 80.8 60.0 79.6 61.9 69.8 59.1 74.0 71.0 74.8 76.3 76.5 61.3 67.5 55.5 69.7 72.6 70.0 69.0 78. 65.8 70.0 72.5 48.3 76.7 48.3 52.3 44.3 60.2 56.1 61.2 63.0 62.5 47.1 60.0 39.0 50.2 54.9 55.0 53.6 67.4 96.0 97.3 97.0 95.8 97.3 93.8 94.4 92.4 94.6 95.4 96.6 92.8 93.9 94.3 94.5 94.1 95. 47.5 41.7 28.3 53.3 29.4 31.4 27.6 37.8 26.3 31.7 33.3 33.3 33.9 46.4 80.8 66.7 80.0 58.9 60.6 52.8 67.1 51.2 61.1 68.3 66.7 62.2 72. 63.4 65.9 66.3 60.9 67.4 34.3 43.6 40.7 40.6 57.9 57.9 51.3 59.4 46.6 45.5 37.6 53.1 57.2 57.5 51.8 61.1 37.1 36.4 51.5 50.4 46.1 54.1 46.2 34.1 47.9 44.1 54. 1982 2029 1918 2036 1922 1189 1481 1691 1633 1475 2024 97.7 98.1 97.1 98.1 97.2 57.4 85.6 93.2 91.4 84.8 98. 79.8 85.3 83.4 84.1 81.2 80.4 83.9 84.6 85.7 This choice of 𝑘 is critical for obtaining reliable evaluation, as lower values of 𝑘 lead to significantly higher standard error of the mean (e.g., on AIME2024 @16/32/64 : 1.8/1.2/0.7). To isolate the effects of pre-training, we primarily compare with reasoning models based on either Qwen2.5 or Llama-3.1 at similar parameter scales. These include SFT models (distilled from much larger frontier models) such as Light-R1-7B (Wen et al., 2025), OpenMathReasoning-7/14/32B (Moshkov et al., 2025), and LLaMA-Nemotron-Nano/Super-8/49B (Bercovich et al., 2025), as well as RL models like AReal-boba-RL7B (RL Lab, 2025), Skywork-OR1-Math-7B (He et al., 2025), and Light-R1-14B (Wen et al., 2025). For context, we also include frontier reasoning models such as DeepSeek-R1 (Guo et al., 2025), QwQ-32B (Qwen, 2025), LLaMA-Nemotron-Ultra-253B (Bercovich et al., 2025), and o3-mini (OpenAI, 2024). 4.1.2. Code Evaluation For coding tasks, we evaluate our AceReason-Nemotron models on LiveCodeBench (Jain et al., 2024) v5 (20240801 20250201) and v6 (20250201 20250501) subsets, containing recently released AtCoder, LeetCode problems. We also report Codeforces ELO and percentile number of our models based on LiveCodeBench Pro dataset (Zheng et al., 2025), which contains Codeforces problems ranging from 202407 to 202412. We also include evaluations on EvalPlus (Liu et al., 2023, 2024) benchmark. We compare our model with state-of-the-art open-sourced code-gen LLMs of similar parameter scales, including OlympicCoder-7B (HuggingFace, 2025), Llama-3.1-Nemotron-Nano-8B-v1 (Bercovich et al., 2025), OpenCodeReasoning-7B/14B (Ahmad et al., 2025), DeepCoder-14B (Luo et al., 2025). For further context, we also include strong frontier reasoning models as titled above. 8 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning 4.2. Main Results From the evaluation results in Table 2, we summarize the key conclusions as follows: RL significantly improves reasoning capabilities. Our AceReason-Nemotron-7B/14B models show that using RL significantly improves over the initial SFT models (DeepSeek-R1-Distill-Qwen-7B/14B) on both math and coding tasks. Specifically, for math tasks, our AceReason-Nemotron-7B achieves remarkable improvements over SFT model by increasing 14.5% accuracy on AIME 2024, and 14.6% accuracy on AIME 2025. For coding tasks, it achieves 14.2% and 8% accuracy improvements over DeepSeek-R1-Distill-Qwen-7B on LiveCodeBench v5 and v6, respectively. Meanwhile, AceReason-Nemotron-14B improves from 69.7%/50.2% to 78.6%/67.4% on AIME24/25, and 53.1%/47.9% to 61.1%/54.9% on LiveCodeBench v5/v6 from initial SFT model DeepSeekR1-Distill-Qwen-14B, even surpassing significantly larger SFT models such as DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B. AceReason-Nemotron vs. SOTA RL-based reasoning models. While comparing with state-of-the-art open RL-based reasoning models under the same parameter scale, AceReason-Nemotron model still remains its superiority. In math reasoning domain, AceReason-Nemotron-7B model provides competitive results while comparing with strong RL-based models (Skywork-OR1-Math, Light-R1, etc.), while AceReason-Nemotron14B provides the best-in-class results. In code generation domain, AceReason-Nemotron models outperform all open-sourced reasoning models with similar parameter scale. Its Math-Code ability remains competitive even comparing with frontier reasoning models, such as QWQ-32B, o3-mini, etc. AceReason-Nemotron vs. SOTA models through distillation. AceReason-Nemotron-14B shows better math and code performance than the latest SOTA specialized distilled model such as OpenMath-14B/32B by +2.1%/+4.4% on AIME24/25, OpenCodeReasoning-14B by +1.7%/+0.8% on LiveCodeBench v5/v6, demonstrating RL leads to higher upper-bound of model performance than distillation. In contrast, at 7B model scale, AceReason-Nemotron-7B performs competitively with OpenCodeReasoning-7B on coding tasks, while OpenMath-7B appears to have bigger advantage than RL-trained AceReason-Nemotron-7B on math reasoning. In all cases, AceReason-Nemotron models significantly outperform the general-purpose reasoning model Llama-Nemotron-8B/49B, which is trained via distillation. This indicates that the effectiveness of distillation versus RL still depends on model size and task domain, though RL offers the potential for significantly higher accuracy at the 14B scale and beyond. 4.3. Analysis 4.3.1. Response Length Grows alongside Performance Fig. 2 (left subfigure) and Fig. 3a illustrate the evolution of response length on the training set and corresponding AIME24 accuracy (AIME25 in appendix Figure 8) throughout the RL training of AceReason-Nemotron-7B model. We analyze two distinct stages in the 8K 16K length-extension training strategy: 1) Stage 1 (8K) Transition from imitation to RL: During this stage, the model learns to compress its reasoning process to adapt to an 8K token limit, causing an initial drop in problem-solving capabilities compared to baseline. However, after approximately 1K2K RL steps, the model gradually recovers its performance; 2) Stage 2 (16K) Accelerated learning: Upon extending the token limit to 16K, the model immediately exploits the extra reasoning tokens capacity: within 500 RL steps, its average response length increases from 4K to around 6.5K tokens. At the same time, AIME24 accuracy improves sharply from 55% to 62%, after which both response length and accuracy plateau. Extending the maximum response length further to 24K and 32K demonstrates similar trend. 4.3.2. Hard Prompts Drive the Largest Gains At the 24K response length RL stage, we build {Easy, Medium, Hard}-prompt sets based on difficulty estimated by 7B models performance over 16 attempts. Easy prompt set includes prompts except those solved more than 14 times, Medium prompt set excludes those solved more than 10, and Hard excludes those solved more than 6. As shown in Table 3, our ablation confirms that training with Hard prompts yields 2.6% improvement on 9 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Table 3: Prompt difficulty and its impact on Stage 3 (24K) training. Data AIME24 avg@ AIME25 avg@64 Starting Checkpoint - 7B Full (no filtering, #49K) Easy (#10K) Medium (#4.6K) Hard (#2.2K) 62.2 63.3 64.4 65.3 65.9 50. 51.1 50.8 51.9 52.5 Figure 2: Model accuracy on AIME2024 and LiveCodeBench v5 during math-only RL (left subfigure) and continued code-only RL (right subfigure). We observe significant accuracy boost during Stage 2 (with 16K response length) of math-only RL. For code-only RL, initializing from math-RL checkpoints provides substantially better starting point and leads to significantly higher final accuracy on LiveCodeBench. the AIME24 benchmark compared to fullset data and outperforms using Easy and Medium prompts, although it only contains 2.2K prompts. 4.3.3. Starting from 8K Improves Later Performance Although training initially at 8K max response length offers faster training per step than starting at 16K or 24K, the early decline in AIME24 accuracy raises questions about its necessity for 7B-sized models. Figure 3b presents an ablation study comparing three strategies: 8K16K24K, 16K24K, and directly at 24K. Each strategy utilizes identical full data, with accuracy measured on AIME24 alongside GPU hours consumed. The results indicate that while the 16K24K strategy initially outperforms the baseline, the 8K16K strategy quickly catches up during the 16K stage and consistently outperforms other strategies thereafter. Conversely, starting directly at 24K results in minimal performance gains and significantly longer training times, suggesting limited effectiveness during RL training for 7B model. 4.3.4. Interplay of Math-only RL and Code-only RL In Fig. 2 (right subfigure), we observe that initializing code RL training from math-RL checkpoints offers substantially better starting point and results in significantly higher final accuracy on LiveCodeBench. We investigate the effects of sequential Math RL and Code RL training stages on developing strong reasoning model in both math and code reasoning domains. Starting from DeepSeek-R1-Distill-Qwen-7B/14B as our 10 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning (a) (b) (c) Figure 3: Analysis of RL training: (a) response length during math-RL training, (b) GPU hours for different length extension configurations, and (c) entropy of output logits using on-policy RL training versus off-policy training with Chip-Higher trick. Table 4: Interplay of math-only RL and code-only RL. Math-only RL significantly improves performance on LiveCodeBench v5 and v6. Extended iterations of code-only RL result in minimal or no degradation on AIME 2024 and 2025. Models AIME 2024 avg@ 2025 avg@64 LiveCodeBench v6 v5 avg@8 avg@8 DeepSeek-R1-Distill-Qwen-7B + Math-only RL (8K 24K) + + Code-only RL (24K 32K) 55.5 65.9 (10.4) 66.9 (1.0) DeepSeek-R1-Distill-Qwen-14B + Math-only RL (8K 24K) + + Code-only RL (24K 32K) 69.7 76.6 (6.9) 75.7 (0.9) 39.0 52.5 (13.5) 51.7 (0.8) 50.2 63.4 (13.2) 63.9 (0.5) 37.6 44.4 (6.8) 51.8 (7.4) 53.1 58.6 (5.5) 61.1 (2.5) 34.1 37.6 (3.5) 44.1 (6.5) 47.9 50.9 (3.0) 54.9 (4.0) initial models, we first apply Math-RL and then Code-RL, evaluating performance on standard Math (AIME 24/25) and Code (LiveCodebench v5/v6) benchmarks. Surprisingly, as shown in Tab. 4, initial Math-RL training not only significantly improves accuracy on math benchmarks, but also improves model performance on coding benchmarks. Subsequent Code-RL training further boosts coding benchmark scores, with negligible degradation on math benchmark performance. This indicates that our proposed sequential training strategy is effective in developing models with strong reasoning abilities across both math and coding domains. 4.3.5. Topic-wise Accuracy Analysis on LiveCodeBench v5 Given the observation that both Math-RL and Code-RL enhance code generation ability on coding benchmarks, we are interested to see in detail how these two RL stages improve accuracy on topic-specific coding problems. Specifically, this ablation aims to identify which problem topics benefit the most from Math-RL and the subsequent Code-RL. Motivated by this, we conducted ablation studies on LiveCodeBench v5 dataset, which consists of coding problems from AtCoder and LeetCode platforms. While LeetCode problems come with human-annotated topic tags (e.g., Greedy, Math, DFS), there is no tag on Atcoder problems. To address this, we first extract set of problem tags from LeetCode. Then, for each AtCoder problem, we query the o4-mini-high model to infer candidate tags given the problem statement and the set of topics. Furthermore, we group all LiveCodeBench v5 problems by their assigned topics and evaluate model performance (avg@8 accuracy) for each topic group. We compare the performance of our initial SFT models, DeepSeek-R1-Distill-Qwen-7B/14B, against corresponding AceReason-Nemotron-7B/14B after applying Math-RL stage only and final models that incorporate 11 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Figure 4: Both math-only and code-only RL enhance the performance of AceReason-Nemotron-7B and 14B across all coding problem topics. As expected, math-only RL provides greater gains on math-related coding tasks. both Math-RL and Code-RL. As shown in Fig. 4, we plot the accuracy for each topic before and after Math-RL and Code-RL. The figure shows that applying math-only RL enhances model performance across all coding problem topics, with especially strong gains in algorithmic and math-related areas such as Math, Counting, and Combinatoricsdomains that rely heavily on mathematical concepts and general reasoning abilities. Furthermore, for topics like Simulation, String, and Graph, which rely more heavily on coding implementation and data structure skills, Code-RL leads to significant further improvement. 4.3.6. False Positive and False Negative Rewards in Code RL Training To highlight the importance of eliminating false positive reward (incorrect code passing all tests within time constraints) and false negative reward (incorrect test cases that fail correct code) in RL Training, we conduct two ablation experiments, showing that both types of errors can be harmful to RL training, resulting in early convergence on sub-optimal policies, or even complete training collapse. Figure 5: The impact of false positive and false negative rewards in Code RL Training To simulate the impact of false negative rewards, we introduce subset of problems into the training data where either the correctness of test cases could not be verified, or the official \"golden\" solution failed to pass all provided tests. From Fig. 5 we can see that, the RL model tends to converge to sub-optimal point, with no 12 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Figure 6: The Pass@K of RL (AceReason-Nemotron) and SFT models (DeepSeek-R1-Distilled) on AIME 2024/2025 and LiveCodeBench v5/v6. Figure 7: Problem-level solving rates comparison between distilled model and after RL training. Accuracy for each problem is calculated on average of 64 attempts. further improvement on the evaluation set. We hypothesize that such false negative reward causes the model to discard correct algorithms for certain training problems. Consequently, when encountering testing problems that share similar underlying concepts and require these discarded algorithms, the model remains unable to identify the correct solution, thereby degrading its performance. To simulate RL training with false positive rewards, we blend problems with weak test cases that allow incorrect solutions to pass into our training set. As notable example, we examined DeepCoders RL training set. Despite their claims of providing strong test case selection criteria, we find that their tests still cannot cover all tricky edge cases - there exists incorrect solutions that can pass all their selected test cases but will subsequently fail on official test cases. After incorporating unfiltered Code-RL training data from these sources into our problem set, we observed sudden drop on testing set accuracy midway through the training process, as shown in Fig. 5. We suspect that this performance degradation occurs because the model learns to solve some training problems without considering all corner cases or by generating brute-force solutions with incorrect time complexity. This wrongly learned policy is then penalized while evaluating against strong test cases in the testing set. 4.3.7. Does RL Improve pass@k or pass@1 over the SFT Model? Recent studies (Shao et al., 2024; Yue et al., 2025) suggest that RL primarily improves pass@1 accuray over SFT model ((DeepSeek-R1-Distilled) without significantly impacting pass@𝑘. However, Figure 6 demonstrates that 13 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning RL consistently improves pass@𝑘 score (from 𝑘 = 8 to 𝑘 = 64) for both 7B and 14B models on LiveCodeBench v5 and v6, with an improved pass@k scores maintaining 10% margin from pass@8 to pass@64. On AIME24/25, we found the 14B model also shows better pass@k, although the AIME25 gap on 7B is closed by the SFT model at pass@64. 4.3.8. Where does RL Improve over the SFT Model? Figure 7 compares the problem-level accuracies of the initial 7B SFT model with AceReason-Nemotron-7B after RL on LiveCodeBench v5/v6 and AIME 2024/2025. Results for the 14B model are shown in Appendix Figure 10. On LiveCodeBench, we observe RL unlocks long tail of hard coding problems that the distilled model fails to solve in 64 attempts, adding 30 and 23 additional solvable problems to LiveCodeBench v5 and v6. It also significantly improves on challenging problems where the SFT model has lower than 20% accuracy. On the AIME, for the most challenging problems with zero solve rate, RL enables the model to solve 3 more problems on AIME24. In conclusion, we find RL not only improves the accuracy on problems with high solve-rate but also extends the boundary to solve hard problems that the SFT model was initially unable to solve. 5. Conclusion We demonstrate that large-scale reinforcement learning (RL) can substantially enhance the reasoning capabilities of strong, smalland mid-sized SFT models. We propose performing RL on math-only prompts first, followed by code-only prompts. Notably, math-only RL significantly boosts performance not only on math benchmarks but also on code reasoning tasks. Crucially, subsequent code-only RL further improves code benchmark performance with minimal to no degradation in math results. To support this process, we develop robust data curation pipeline that collects challenging prompts with high-quality, verifiable answers and test cases, enabling verification-based RL across both domains. We derive several key empirical insights from RL training. In particular, we find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning, but also pushes the limits of the models reasoning ability, enabling it to solve previously unsolvable problems. 6. Acknowledgement We would like to extend our gratitude to the NVIDIA Nemo team for the valuable discussion and collaboration on building reasoning models. We especially wish to thank Boris Ginsburg, Oleksii Kuchaiev, Igor Gitman, Wei Du, Somshubra Majumdar, Siddhartha Jain, Jiaqi Zeng, Yi Dong, Alexander Bukharin, Olivier Delalleau, Tugrul Konuk, Vahid Noroozi, and Jonathan Cohen. AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning"
        },
        {
            "title": "References",
            "content": "[1] Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. 3, 4, 8 [2] Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. 3 [3] Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharena.ai/. 7 [4] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-Nemotron: Efficient Reasoning Models. arXiv preprint arXiv:2505.00949, 2025. 3, 4, 8 [5] Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, and Haoxiang Wang. Bridging supervised learning and reinforcement learning in math reasoning. arXiv preprint, 2025. 4 [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 4 [8] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. 5 [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 4 [10] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. 4 [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 4, 5, 7, 8 [12] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series, 2025. Notion Blog. 3, 4, [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. 7 [14] HuggingFace. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. 8 15 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning [15] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 4, 5, 8 [16] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 5 [18] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https: //github.com/project-numina/aimo-progress-prize/blob/main/report/numina_ dataset.pdf), 2024. 5 [19] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-V3 technical report. arXiv preprint arXiv:2412.19437, 2024. 3 [21] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. 4 [22] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=1qvx610Cu7. 8 [23] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IBCBMeAhmC. 8 [24] Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. AceMath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084, 2024. 4, 5 [25] Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. 3, 4, 6, [26] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL, 2025. Notion Blog. 3, 4, 5, 6 [27] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. 5 16 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning [28] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. 3, 4, 8 [29] OpenAI. Learning to reason with LLMs, 2024. 3, 4, [30] Team Qwen. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. 3, 4 [31] Team Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. 4, 8 [32] Ant Research RL Lab. Areal: Ant reasoning rl. https://github.com/inclusionAI/AReaL, 2025. 4, 8 [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4, 6, 13 [35] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 5 [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 4 [37] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond. arXiv preprint arXiv:2503.10460, 2025. 3, 6, 8 [38] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [39] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. 4 [40] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 3, 4, 5 [41] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 13 [42] Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025. 4 [43] Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Stanley Wei, Hansen He, Dongruixuan Li, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Peter Henderson, Aleksandra Korolova, Pramod Viswanath, Saining Xie, and Jingbo Shang. Livecodebench pro: How olympiad medalists view llms in competitive programming?, 2025. URL https://livecodebenchpro.com/. Available at LiveCodeBench Pro website. 17 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning A. Appendix A.1. Instruction for evaluation"
        },
        {
            "title": "Math",
            "content": "Please reason step by step, and put your final answer within boxed{{}}. No Starter Code (Python) Write Python code to solve the problem. Please place the solution code in the following format: python # Your solution code here"
        },
        {
            "title": "Has Starter Code",
            "content": "Solve the problem starting with the provided function header. Function header: <starter_code> Please place the solution code in the following format: python # Your solution code here A.2. Example of AceReason-Nemotron-14B response to simple query hi User: Model: <think> Alright, the user wrote \"hi\" in the middle of the message. Probably just saying hello. should respond in friendly and welcoming way. Maybe ask how can assist them today. Keep it open-ended to encourage them to share what they need help with. </think> Hello! How can assist you today? 18 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning User: How many strawberries do need to collect 9 rs? Model: <think> First, need to understand what the user is asking. They want to know how many strawberries are needed to collect 9 rs. interpret rs as the letter r. Each strawberry is associated with certain number of rs. Looking at the word strawberry, count the number of rs it contains. There are three rs in strawberry. If one strawberry provides three rs, then to get total of nine rs, divide the total needed by the number each strawberry provides. So, nine divided by three equals three. Therefore, the user needs to collect three strawberries to have nine rs. </think> To determine how many strawberries you need to collect **9 \"rs**, lets break it down step by step. ### Step 1: Understand the Context - It seems youre referring to the letter **\"r\"** within the word **strawberry.** ### Step 2: Count the rs in strawberry - Lets look at the word **strawberry**: - **s w r y** - **r**, **r**, **r** (there are **3 rs**) ### Step 3: Calculate the Number of Strawberries Needed - Each **strawberry** contains **3 rs.** - To collect **9 rs**, you would need: [ text{Number of strawberries} = frac{text{Total rs needed}}{text{rs per strawberry}} = frac{9}{3} = ] ### Final Answer [ boxed{3} ] A.3. Additional Math-RL Analysis In Figure 8, we show the accuracy on AIME2025 during math RL training. We found the stage 2 (16K) training leads to large performance improvement within 300 steps. In Figure 9, we plot boxplots of AIME2024 and AIME2025 accuracy for the 7B and 14B AceReason model comparing to DeepSeek-R1-Distill-Qwen. We can observe the accuracy of AceReason over 64 generations at 25% and 75% percentile and median, significantly improve over the distilled SFT model. Finally, in Figure 10, we inspect the problem-level solve rate for 19 AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning Figure 8: Model accuracy on AIME2025 during math-only RL training. (a) 7B (b) 14B Figure 9: Boxplot of AceReason vs Deepseek-R1-Distill on AIME24/25 over 64 generations. Figure 10: Comparison of problem-solving rates after RL training. AceReason-Nemotron-14B on AIME2024 and AIME2025. On AIME24 and AIME25, the AceReason model solves one additional problem. We also observe large gains on problems with higher initial accuracy, showing the benefits of RL training. AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning A.4. Code-RL Dataset Curation Details We collect our Code-RL training data from various modern competitive programming platforms, such as AtCoder, LeetCode, Aizu, etc., with public strong test cases, while most open-sourced coding datasets (e.g., TACO, APPs) suffer from noisy problem statements, self-contamination, and weak LLM synthetic test cases that are unreliable for RL training. To ensure data quality, we performed very strict filtering rules, by filtering out 1) Multi-solution or interactive problems that requires special judge or other external tools; 2) Problems where images within the statement obscure clear understanding; 3) Problems containing incorrect test cases or those lacking golden solutions; 4) Problems with weak test cases that allow incorrect solutions to pass all tests. Furthermore, to prevent self-contamination within our collected problem set, we conduct strict problem statement and source URL matching. To avoid any potential contamination of our test set, we exclude all problems released after 20240801, and apply 𝑛-gram matching (with 𝑛 = 14) to our testing set problem statement. To prepare for subsequent curriculum training for Code-RL, we estimate the relative difficulty of the collected problems. We deploy the local DeepSeek-R1-671B model, allow it to generate 8 attempts on each problem, and assign corresponding difficulty score ranging from 0 to 8. Problems that the DeepSeek-R1-671B model failed to solve in all 8 attempts are excluded from the training set. Following this aggressive filtering process, 8,520 problems remain, forming our final Code-RL training set."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}