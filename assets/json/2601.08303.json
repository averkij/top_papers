{
    "paper_title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "authors": [
        "Dongting Hu",
        "Aarush Gupta",
        "Magzhan Gabidolla",
        "Arpit Sahni",
        "Huseyin Coskun",
        "Yanyu Li",
        "Yerlan Idelbayev",
        "Ahsan Mahmood",
        "Aleksei Lebedev",
        "Dishani Lahiri",
        "Anujraaj Goyal",
        "Ju Hu",
        "Mingming Gong",
        "Sergey Tulyakov",
        "Anil Kag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware."
        },
        {
            "title": "Start",
            "content": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices Dongting Hu1,2 Aarush Gupta1 Magzhan Gabidolla1 Arpit Sahni1 Huseyin Coskun1 Yanyu Li1 Yerlan Idelbayev1 Ahsan Mahmood1 Aleksei Lebedev1 Dishani Lahiri1 Anujraaj Goyal1 Ju Hu1 Mingming Gong2, 3 Sergey Tulyakov1 Anil Kag1 1 Snap Inc. 2 The University of Melbourne 3 MBZUAI 6 2 0 J 3 1 ] . [ 1 3 0 3 8 0 . 1 0 6 2 : r Figure 1. Top: Our text-to-image Diffusion Transformer (0.4B parameters) generates diverse, high-fidelity 1K images in just 1.8 on mobile device. All examples are produced by this on-device model at resolution of approximately 10242. Bottom: Comparison across various text-to-image models. Both our on-device (small) and server-side (full) versions achieve competitive visual quality."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose compact DiT architecture with an adaptive globallocal sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within unified supernetwork, allowing single model to dynamically adjust for efficient inference across different hardware. Finally, we develop K-DMD (Knowledge-Guided Distribution Matching Distillation), step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and lowlatency generation (e.g., 4-step) suitable for real-time ondevice use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware. 1. Introduction Image generation models [18, 35, 53, 55, 69] have made remarkable progress, enabling wide range of creative applications. Recent advances [11, 52] show clear shift toward diffusion transformer (DiT) architectures, with large-scale models such as Flux [35] and Qwen-Image [69] achieving state-of-the-art image quality, editing flexibility, and personalization. However, these transformer-based models are extremely largeoften containing tens of billions of parametersrequiring server-grade GPUs and custom CUDA kernels [39] for inference, which introduces high computational cost and dependence on cloud infrastructure. To improve accessibility, recent works [28, 40, 84] have explored deploying compact diffusion models directly on mobile devices. Systems such as SnapFusion [40], Mobile Diffusion [84], and SnapGen [28] demonstrate efficient on-device text-to-image (T2I) generation using lightweight U-Net backbones that achieve favorable qualityefficiency trade-offs. While these on-device models alleviate latency and cloud dependence, their U-Net-based architectures lag far behind recent DiT models in scalability and generative performance. To bridge this architectural gap, we propose an Efficient Diffusion Transformer tailored for mobile and edge deployment, achieving server-level generation quality under strict resource constraints. To address the quadratic complexity of attention especially at high resolutions (e.g., 1K), we introduce three-stage DiT with an adaptive globallocal sparse attention mechanism that effectively combines coarse-grained KeyValue (KV) Compression for global context modeling with fine-grained Blockwise Neighborhood Attention for spatial relation modeling. By dynamically allocating attention based on content, the model achieves flexible computation and high representational fidelity, outperforming U-Netbased systems such as SnapGen [28] in generation quality while maintaining comparable inference speed. Deploying such models on real-world devices presents another key challenge: the heterogeneity of deployment hardware. On-device generation must meet stringent compute, memory, and power constraints, while devices vary widelyfrom entry-level smartphones to high-end flagships and lightweight edge servers. single static model cannot perform efficiently across this spectrum, leading to fragmented development and suboptimal deployment. To address this, we introduce an Elastic Training Framework that jointly optimizes sub-DiTs of varying capacities within unified DiT supernetwork. This elastic framework enables single model to encompass multiple sub-networks, each tailored to different hardware. At inference, the appropriate sub-network is selected dynamically, enabling seamless adaptation across heterogeneous devices without retraining. This design ensures scalability, efficiency, and consistent generation quality across diverse deployment scenarios. To close the performance gap between large-scale and compact diffusion models, we employ knowledge distillation to transfer the generative capability of the full-step teacher to the student. We further propose KnowledgeGuided Distribution Matching Distillation (K-DMD), step-distillation framework that integrates the DMD objective [77, 78] with knowledge transfer from few-step (i.e., 4-step) teacher, enabling efficient distillation while preserving high fidelity and supporting on-device generation. Our main contributions are as follows: 1. Efficient DiT-based architecture. We design compact yet expressive diffusion transformer optimized for on-device generation, achieving strong performance under strict computational and memory constraints. 2. Elastic training framework. We propose an elastic training paradigm to jointly optimize sub-DiTs of varying capacities within unified supernetwork, enabling adaptive inference across heterogeneous hardware with stable convergence and robust generalization. 3. Knowledge-guided distillation pipeline. We introduce K-DMD, step-distillation framework that integrates the DMD objective with knowledge transfer from fewstep teacher models, achieving high-fidelity image synthesis with substantially reduced inference latency and supporting efficient on-device generation. 2. Related Work T2I Diffusion Models. Diffusion models [24, 35, 59, 69] have become the state of the art in text-to-image (T2I) generation, surpassing earlier GAN-based approaches [6, 20] in fidelity and diversity. Early latent diffusion models [10, 33, 3638, 40, 53, 55, 56] employed U-Net backbones for iterative denoising in latent space, balancing image quality and memory efficiency. Recent advances replace U-Nets with Diffusion Transformers (DiTs) [35, 52, 66, 69], achieving improved scalability, quality, and generalization across generation and editing tasks [45, 49, 66, 69]. However, their billion-scale parameters and high computational cost make them impractical for on-device deployment. Efficient Diffusion Transformers. Recent efforts [10, 13, 39, 46, 51, 75] aim to improve DiT efficiency. PixArtΣ [10] introduces keyvalue compression for 4K image generation, while SANA [75] employs linear self-attention to enable efficient synthesis on consumer GPUs. LinFusion [46] replaces quadratic attention in Stable Diffusion [55] with Mamba-based [14, 21] attention for ultrahigh-resolution (16K) generation. Hybrid designs such as Simple Diffusion [25, 26], HourGlass-DiT [13], and UDiTs [64] combine convolutional and transformer blocks in U-Netstyle hierarchies. U-ViT [4] introduces long-skip connections for faster convergence, and Playgroundv3 [44] reduces key/value dimensions to mimic single-level U-Nets. Despite these advances, DiTs still depend on quadratic attention and large memory footprints, limiting efficient highresolution (e.g., 1024 1024) generation on mobile devices. On-Device Generative Models. To enable on-device deployment, prior works have explored quantization [39, 61], pruning [28, 40, 71, 72], and knowledge distillation [28, 34] to reduce model size and latency. Early on-device systems [9, 40, 84] pruned and distilled U-Net architectures to generate 512-pixel images within seconds. SnapGen [28] demonstrated 1024-pixel image generation with compact U-Net, though with trade-offs in quality and editing flexibility. To our knowledge, no prior work has deployed an efficient DiT for high-fidelity on-device generation. Model Scalability and Elastic Networks. Once-for-All [7] and Slimmable Networks [80] pioneered supernetworks adaptable to varying computational budgets for recognition and detection tasks. Follow-up studies [17, 27, 65, 68] extended this idea to transformers and large language models. However, elastic architectures remain underexplored in generative models. We build our model in this direction by introducing an Elastic DiT framework that enables flexible diffusion transformer deployment across heterogeneous devices without retraining separate models. Sparse Attention. Yuan et al. [82] and Hassani et al. [22] propose hardware-efficient sparse attention designs using blockand neighborhood-based formulations optimized for GPUs. For video generation, Zhang et al. [83] and Xi et al. [73] exploit local and spatiotemporal sparsity for efficient attention, while Xia et al. [74] introduce adaptive sparse attention with online sparsity discovery without retraining. Step Distillation. Step distillation accelerates diffusion inference by compressing multi-step sampling into few denoising iterations [3, 41, 57, 60, 76, 77]. Progressive Distillation [48, 57] first showed that student models can learn from intermediate teacher trajectories, while Consistency and Phase Consistency Models [60, 67] enhance stability by enforcing cross-step prediction consistency. Adversarial Diffusion Distillation (ADD) [41] introduces GANstyle objectives for few-step, high-fidelity synthesis, and Distribution Matching Distillation (DMD) [77, 78] aligns teacherstudent noise distributions for improved perceptual quality. Recent works such as UFOGen [76], SANASprint [12], and SD3.5-Flash [3] combine distillation with architectural optimizations for near real-time generation. 3. Method We introduce an efficient Diffusion Transformer (DiT) architecture, an elastic training framework, and multi-stage distillation pipeline. Together, these components enable efficient high-fidelity image generation on edge devices. 3.1. Efficient Three-Stage DiT Architecture We develop the efficient DiT through series of key architectural design ablations. All variants are trained on the ImageNet-1K dataset [16] for conditional image generation 256 resolution and evaluated using the validation at 256 loss (Val Loss) following the protocol of [66]. This metric shows stronger correlation with perceptual quality and human preference than conventional image metrics such as FID [23], aligned with the findings in [18]. Model efficiency is measured by parameter count and inference latency on iPhone 16 Pro Max. For consistency, all models employ the Flux VAE [35] and the CLIP-L [54] text encoder, and are trained for 200K iterations using the flowmatching [43, 47]. As reference, we implement the SnapGen [28] (Fig. 3, rightmost column) as our baseline, which achieves latency of 274 ms and Val Loss of 0.5131. (A) Baseline Architecture. Our design builds on the PixArt-α [11] DiT backbone, chosen for its strong balance between parameter efficiency and computational cost. To adapt it for edge deployment, we incorporate multi-query attention (MQA) [58] and reduce the feed-forward expansion ratio to 3, yielding compact 424M-parameter DiT. This baseline attains validation loss of 0.506 with an inference latency of 2000 ms (Fig. 3, first column). Computation Analysis. The main computational bottleneck arises from self-attention (SA) at high resolutions. For 10242 image, the VAE encoder yields 1282 latent map. After patchification, this corresponds to 642 tokens (4096 in total), substantially increasing SA cost and often causing 3 Figure 2. Efficient DiT Overview. Left: Our model consists of three stages: Down, Middle and Up. Down and Up blocks operate on high-resolution latent while using our novel Adaptive Sparse Self-Attention (ASSA) layers. Middle blocks operate at latents downsampled by 2 2 window and use standard Self-Attention (SA) layers. Other layers in the blocks are Cross-Attention (CA) for modulating with input text conditioning and Feed-Forward (FFN) layer. Right: We delve deeper into our ASSA layer. It consists of two parallel attention processing branches: (i) coarse-grained key-value compression for overall structure, and (ii) fine-grained blockwise neighborhood attention features. Finally, the layers to weight these two features are adaptively per head through an input-dependent weighting function. Figure 3. Efficient DiT Ablations. We plot the performance (validation loss) and model footprint (parameters & latency on iPhone 16 Pro Max) for various stages in our ablations. Using baseline DiT yields extremely high latency. Our multi-stage design with ASSA layers and additional enhancements results in an Efficient DiT with comparable latency and better performance than the state-of-the-art on-device model SnapGen [28]. (C) Adaptive Sparse Self-Attention (ASSA) at HighResolution Stages. Although token downsampling in the middle stage reduces the overall computational cost, the bottleneck remains in the SA operations of the down and up stages. To alleviate this, we introduce an adaptive sparse self-attention (ASSA) (Fig. 2 (b)) that replaces full SA over 4096 tokens with two complementary components: RH (i) Global Attention. We apply Key-Value (KV) compression by performing 2 2 convolution with stride 2 on the and feature maps. Given the key and value tensors k, d, we compute the compressed tensors vc = Conv22, s=2(v), (1) d. This reduces the key/value resulting in kc, vc 2 token length by factor of four, enabling each query to attend to compressed global context with substantially lower memory and computational overhead. kc = Conv22, s=2(k), 2 out-of-memory (OOM) errors on edge hardware. To address this, we introduce several architectural modifications that improve efficiency while preserving generation fidelity. (B) Three-Stage Diffusion Transformer. Inspired by recent efficient architectures such as Hourglass-DiT [13] and U-DiT [64], we extend the baseline into three-stage design (Fig. 2 (a), left). The three stages are denoted as Down, Middle, and Up. single downsample layer is applied after the down stage and an upsample layer before the up stage, producing compact latent representation of 1024 tokens (32 32) in the middle stage. Half of the transformer layers are assigned to the middle, while the remaining layers are divided between the down and upwith slightly more layers in the up blocks, following SiD2 [26]. This design cuts latency from 2000 ms to 550 ms, while increasing the validation loss to 0.513 (Fig. 3, second column). (ii) Local Attention. To preserve fine-grained spatial details, we introduce Blockwise Neighborhood Attention (BNA), which restricts attention computation to local region around each token. As shown in Fig. 4(a), naive local attention restricts each token to attending only to its spatial neighbors within fixed window (e.g., 3 3), analogous to convolutional receptive field. When visualized in the attention matrix, this local interaction pattern forms banddiagonal structure, as shown in Fig. 4(b). While such localized attention is more efficient than full self-attention, it is not natively supported on mobile hardware and still incurs nontrivial overhead when applied per token. To further optimize for edge deployment, we adopt blockwise formulation (Fig. 4(c)), where the token grid is divided into (a hyperparameter) non-overlapping spatial blocks, and attention is computed independently within each block. Formally, we partition the query, key, and value matrices along the sequence dimension into q, k, non-overlapping blocks: R(HW ) = [q1; . . . ; qB], = [k1; . . . ; kB], = [v1; . . . ; vB], (2) RNb and block size Nb = where each block qb, kb, vb HW/B. For each query block qb, attention is computed only within limited neighborhood of keyvalue blocks , where denotes the Nr(b) = block neighborhood radius (bandwidth). The blockwise neighborhood attention is defined as r, . . . , b, . . . , + } { (cid:32) Ab = Softmax (cid:33) qb[k Nr(b)] [v Nr(b)], = 1, . . . , B, Nr(b)] and [v (3) Nr(b)] represent the concatenation where [k of key and value blocks within the neighborhood Nr(b). This formulation enforces spatial locality, produces block- (N 2/B), sparse attention pattern that scales efficiently as and preserves strong local contextual modeling for highresolution features. It is worth noting that different hyperparameter combinations of the block number and neighborhood radius can be used, effectively controlling the token-level spatial neighborhood size (see the supplementary material for detailed illustration). The final attention score is linear interpolation between glocal attention and local attention, conditional on the input hidden states. This nove sparse attention design substantially reduces the overall attention overhead while preserving generation quality. As shown in Fig. 3 (third column), our sparse attention model achieves latency of 293 ms without loss of generation quality (val loss of 0.513). (D) Additional Enhancements. To further improve performance, we introduce several enhancements: Dense long-range skip connections: Following [5], we add dense skip connections in the middle stage to increase the capacity of the bottleneck representation. 5 Figure 4. Illustration of Blockwise Neighborhood Attention. (a) Naive Neighborhood Attention, where each query attends to its local window of 3 neighboring tokens. (b) Corresponding selfattention mask showing the limited receptive field for each query. (c) Blockwise Neighborhood Attention extends this concept by grouping tokens into 8 local blocks, enabling efficient attention computation while preserving locality. Grouped Query Attention (GQA): We employ GQA [2] by increasing the number of key/value heads to eight, improving multi-head diversity and reducing querykey bottlenecks with minimal additional parameter overhead. Expanded FFN capacity: The FFN expansion ratio increases to four in down and up stages, yielding higher representation power without excessive computational cost. Layer redistribution: Four transformer layers are reassigned from the middle stagetwo each to the down and upto achieve more balanced depth and better information hierarchy. Thanks to the efficiency of the proposed sparse self-attention, we can afford slight increase in computational load to gain capacity and performance. As shown in Fig. 3 (fourth column), this configuration achieves latency of 360 ms and validation loss of 0.509, offering strong trade-off between efficiency and accuracy. With all components combined, our efficient DiT architecture attains conv-level latency while surpassing it in both visual quality and scalability in image generation, outperforming SnapGen by large margin in validation loss. Some qualitative results are in the supplementary material. 3.2. Elastic DiT Framework Recent works such as Matformer [17] and Gemma-3n [62] demonstrate the importance of building unified yet adaptable LLM architectures that can be deployed efficiently across heterogeneous platforms (e.g., high-end smartphones, low-power devices, and server-side environments). Motivated by this, we design an Elastic DiT framework that enables single diffusion transformer to flexibly scale its capacity according to available computational resources. Framework Design. To enable this flexibility, we identify structural decomposition that allows parameter sharing across subnetworks of different widths [80], slicing the projection matrices in the attention and FFN layers along the hidden dimension to sample subnetworks of varying sizes from single supernetwork. In cross-attention layers, the seamlessly across heterogeneous platforms while maintaining strong performance and visual fidelity. As shown in Tab. 1, the elastic training recipe achieves comparable validation loss and DINO-FID to standalone training while reducing the overall model-state footprint through parameter sharing. Note that these results are obtained from relatively small-scale experiments on ImageNet, where the overhead from data loading and embedding computation is limited. When scaling to large-scale text-to-image (T2I) training and distillation, this overhead becomes significantly more pronounced, as the data pipeline and larger teacher components dominate the total training cost. Figure 5. Elastic Training Framework. Given supernetwork, we define sub-networks as different granularities of the hidden dimension. During training, we sample sub-networks uniformly and supervise them using the output from the supernetwork. In addition, we use standard diffusion loss on all granularities. This leads to more stable training and imparts knowledge to sub-networks. Training Recipe Standalone Elastic Model Val Loss 0.4B 1.6B 0.4B 1.6B 0.5090 0.5073 0.5093 0.5071 DINO FID Training Footprint 128 109 125 110 6.6 GB 18.8 GB 18.8 GB key and value projections are not sliced, as they are independent of the model width (hidden dimension). Parameters strictly tied to the hidden-state lengthsuch as those in layer normalization and modulation layersare isolated, since they are lightweight and dimension-specific. This design produces three model variants: tiny 0.3B model width) for low-end Android devices, small 0.4B (0.375 width) for high-end smartphones, and full model (0.5 1.6B supernetwork (1 width) that can be quantized for ondevice deployment or server-side inference. Training Recipe. Naively optimizing multiple subnetworks with shared weights often leads to unstable gradient updates, even under low learning rates. To mitigate this issue, we propose unified elastic training strategy that stabilizes joint optimization across subnetworks of different widths  (Fig. 5)  . During training, subnetworks parameterΘ are sampled jointly with the full superized by Θs network Θ in each iteration and optimized under unified flow-matching objective: (cid:104) (cid:105) ϵ , (ϵ (4) x0) { (0,I), 2 2 vθ(xt, t) Θ, Θs} Ldiff (θ) = where θ . Their gradients are then aggregated using adaptive scaling to ensure balanced updates across subnetworks. Additionally, lightweight distillation loss is applied between each subnetwork and the full-capacity (supernetwork) model to further improve training stability and ensure consistent convergence behavior: Ldist(Θs) = (cid:13) (cid:13)vΘs (xt, t) vΘ(xt, t)(cid:13) 2 2, (cid:13) (cid:0)(cid:0) (5) where (cid:0)(cid:0) denotes the stop-gradient operator. This elastic training framework enables DiT models to be deployed Table 1. Comparison between Standalone and Elastic training for 0.4B and 2B models. Elastic training reuses parameters between model scales, reducing memory allocation while maintaining similar validation loss and DINO-FID. 3.3. Distillation Pipelines We apply both the flow matching loss (Eq. (4)) and the distillation loss (Eq. (5)) during the pretraining stage. Following the SnapGen [28] pipeline, we then perform large-scale knowledge distillation to substantially enhance the performance of small student models, followed by step distillation enabling efficient inference and real-time generation on edge devices. Knowledge Distillation. large cloud-scale teacher [69] (denoted as ξ) supervises the training of the elastic DiT models through both outputand feature-level distillation. The student θ is first encouraged to match the teachers velocity predictions: Θ, Θs} { ξ out(θ) = (cid:13) (cid:13)vξ(xt, t) vθ(xt, t) (cid:13) 2 2, (cid:13) (6) and further aligns its internal representations via feature distillation on the final transformer layer: feat(θ, ϕ) = (cid:13) ξ (cid:13)fξ(xt, t) ϕ(cid:0)fθ(xt, t)(cid:1)(cid:13) 2 2, (cid:13) (7) where ϕ is the projector. The overall distillation objective combines both levels of supervision with timestep-aware scaling [28]: (cid:0) Ldiff , LKD(θ, ϕ) = ) the timestep-aware scaling operator. L ξ feat, ξ out (cid:1) + where ( 6 (8) 4. Experiments 4.1. Experimental Setup T2I Configuration. We use the 1.6B parameter efficient DiT (Sec. 3.1) as the supernetwork for our elastic training (Sec. 3.2) which embeds two sub-networks of 0.3B and 0.4B parameters. We employ TinyCLIP [70] and Gemma34b-it [63] as text encoders with token-wise concatenation for rich semantic embeddings. Following [18, 28], we drop these independently to enable inference even in the absence of other encoder. Since we use Qwen-Image [69] as our teacher, we use their VAE to align the latent space. We also train tiny decoder similar to [28] for on-device generation. On-Device Runtime. The VAE decoder takes 120 ms, and the per-step latency of the DiT (0.4B) is 360 ms, yielding nominal runtime of about 1.6 for 4-step generation. Including additional system overhead, the total on-device runtime is around 1.7 s. Further implementation details are provided in the supplementary material. Training Recipe. Inspired by recent works [69], we use multi-aspect ratio data to pre-train the elastic model using flow-matching loss [18, 47] at 256 resolution, followed by 1024 base resolution. In the next stage, we use knowledge distillation from Qwen-Image [69] and K-DMD stepdistillation training with Qwen-Image-Lightening [50]. We provide additional details in supplementary. 4.2. Evaluations Quantitative Results. We evaluate our T2I model against standard baselines on DPG-Bench [30], GenEval [19], and T2I-CompBench [31] to assess key T2I generation attributes. Following [28], we also report CLIP-Score [54] on subset of MS-COCO [42]. Results for the tiny (0.3B), small (0.4B), and full (1.6B) variants of our elastic model are shown in Tab. 2, with main findings summarized below. Our models achieve competitive or superior performance across all major benchmarksincluding DPG, GenEval, T2I-CompBench, and CLIPcompared to much larger models such as Flux.1-dev [35] and SD3.5-Large [1]. The small variant (0.4B) surpasses models up to 20 larger while retaining on-device efficiency comparable to SnapGen, and the tiny variant (0.3B) achieves the highest throughput among all evaluated models. The elastic design enables smooth trade-off between visual quality and computational cost, achieving strong balance of fidelity, scalability, and on-device efficiency. Qualitative Results. To visually assess imagetext alignment and overall aesthetics, we compare images generated by different T2I models in Fig. 1. We observe that many existing models tend to produce overly stylized or less realistic images, and often fail to capture the full prompt and omit important visual elements. Figure 6. Knowledge-guided Distribution Matching Distillation (K-DMD). Our step distillation method combines distribution matching with knowledge transfer from few-step teacher. Step Distillation. Following recent one-step distillation methods [77, 78], we adopt Distribution Matching Distillation (DMD) for step distillation. However, DMD requires careful tuning of hyperparameters such as teacher guidance scale and auxiliary loss weight. We observe that optimal settings vary across model capacities, and applying DMD to smaller models with only millions of parameters often causes unstable convergence. To address these issues, we propose Knowledge-guided DMD (K-DMD), which extends DMD-based step distillation by incorporating knowledge distillation from fewstep teacher [50]  (Fig. 6)  . Following [78], we compute the KL divergence between the real score from the teacher ξ and the student output distribution estimated by critic model (initialized with the same weights as the student θ): (cid:104) θL (cid:0) ξ DMD(θ) = with (ˆx0, τ ), τ (cid:1) fc σt vθ(xt, t), ˆx0 = xt (cid:0) fξ (ˆx0, τ ), τ (cid:1)(cid:105) dˆx0 dθ , (9) ) the input where τ is randomly sampled to diffuse (via ˆx0 before passing it to the teacher ξ and critic c. To further leverage the power of the large-scale fewstep teacher [50] (denoted as ξ), we feed the same input ξ xt as the student and incorporate feat (Eq. (7)) into the training objective. The final step distillation objective is defined as: ξ out (Eq. (6)) and LK-DMD(θ, ϕ) = ξ DMD + ξ out + ξ feat. (10) This objective enables stable convergence across models of varying capacities without requiring additional hyperparameter tuning. Furthermore, the few-step teacher can be activated by enabling the few-step LoRA [29], introducing no extra memory overhead, as illustrated in Fig. 6. The critic model is updated alternatively with flow-matching (Eq. (4)) on students distribution ˆx0 aligned with previous works [3, 32, 79]. 7 Figure 7. Human Evaluation. We conduct user study comparing our small (0.4B) and full (1.6B) variants with three baselinesSANA (1.6B), SD3-Medium (2B), and Flux.1-dev (12B)across three key attributes: realism, visual fidelity, and textimage alignment. Table 2. Quantitative Evaluation. Scores are reported on DPGBench, GenEval, T2I-CompBench, and CLIP (COCO). Throughput/FPS (samples/s) is measured on single 80GB A100 GPU using the largest batch size that fits for 10242 images. Latency (ms) is measured on iPhone 16 Pro Max with one forward pass. quality remains nearly lossless, demonstrating the effectiveness of our step-distillation approach. Model Arch. Param. SnapGen [28] PixArt-α [11] PixArt-Σ [10] SANA [75] LUMINA-Next [85] SD3-Medium [18] SDXL [53] Playgroundv2.5 [38] IF-XL [15] SD3.5-Large [1] Flux.1-dev [35] U-Net DiT DiT Hybrid DiT DiT U-Net DiT U-Net DiT DiT 0.4B 0.6B 0.6B 1.6B 2.0B 2.0B 2.6B 2.6B 5.5B 8.1B 12B FPS 0.51 0.42 0.46 0.91 0.06 0.28 0.18 0.18 0.06 0.08 0. DPG 81.1 71.1 80.5 84.8 74.6 84.1 74.7 75.5 75.6 85.6 83.8 GenEval 0.66 0.48 0.53 0.66 0.46 0.68 0.55 0.56 0.61 0.71 0.66 T2I-C.B. 0.351 0.427 0.476 0.353 0.522 0.402 0.237 0.421 0.507 0.471 CLIP 0.332 0.316 0.317 0.327 0.309 0.323 0.301 0.319 0.311 0.326 0.316 Ours-tiny 0.330 Ours-small 0.332 0.338 Ours-full Note. indicates out-of-memory (OOM) at 1024 1024 resolution. 0.502 0.506 0. 0.3B 0.4B 1.6B 0.81 0.62 0.28 0.69 0.70 0.76 84.6 85.2 87.2 DiT DiT DiT Latency 274 280 360 1580 Human Preference Study. For thorough comparison between baselines, we conduct user study following the widely used Parti prompts [81]. We include SANA (1.6B), SD3-M (2B), and Flux.1-dev (12B) as the baselines and ask participants to select images with better attributes between the baselines and our models. The evaluation considers three key aspects: realism, fidelity, and text alignment. As shown in Fig. 7, our full variant surpasses all baselines in both fidelity and realism, while remaining highly competitive in imagetext alignment, particularly against SD3-M. The small variant also demonstrates robust performance, outperforming larger baselines such as Flux.1-dev and SANA on most attributes. Few-Step Generation. After applying Knowledge-guided Distribution Matching Distillation (K-DMD), our models are capable of generating high-quality images in only four steps. As shown in Fig. 8, we compare the performance of the 28-step base models with the 4-step distilled models using DPG and GenEval scores. The results indicate that the distilled 4-step models achieve performance comparable to the 28-step baselines, despite the significant reduction in sampling steps. While there is slight drop in scores, the Figure 8. Few-step Generation. Comparison of images produced by the tiny (0.3B), small (0.4B), and full (1.6B) models under 28step (w/o K-DMD) and 4-step (w/ K-DMD) settings. Numbers in the corners denote DPG / GenEval scores. 5. Conclusion In this work, we presented an Efficient Diffusion Transformer that brings transformer-based image generation to mobile and edge devices. Through adaptive globallocal sparse attention, our model achieves strong qualityefficiency trade-offs under strict resource limits. An Elastic Training Framework enables dynamic scalability across heterogeneous hardware, while K-DMD distills high-fidelity knowledge from few-step teachers for fast, high-quality generation. Extensive experiments demonstrate that our models achieve near server-level generation quality while operating efficiently on mobile devices. Together, these advances make diffusion transformers practical for real-world on-device deployment, paving the way for scalable generative intelligence on edge devices."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable https://github.com/Stability-AI/sd3.5, 2024. 16 diffusion 3.5. 7, 8, 15, [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, Singapore, 2023. Association for Computational Linguistics. 5 [3] Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, and Varun Jampani. Sd3.5flash: Distribution-guided distillation of generative flows, 2025. 3, 7 [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 3 [5] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 5 [6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image syntheIn International Conference on Learning Representasis. tions (ICLR), 2019. 3 [7] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations (ICLR), 2020. [8] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, Yimeng Wang, Kai Yu, Wenxuan Chen, Ziwei Feng, Zijian Gong, Jianzhuang Pan, Yi Peng, Rui Tian, Siyu Wang, Bo Zhao, Ting Yao, and Tao Mei. Hidream-i1: high-efficient image generative foundation arXiv preprint model with sparse diffusion transformer. arXiv:2505.22705, 2025. 15, 16 [9] Thibault Castells, Hyoung-Kyu Song, Tairen Piao, Shinkook Choi, Bo-Kyeong Kim, Hanyoung Yim, Changgwun EdgeFusion: Lee, Jae Gon Kim, and Tae-Ho Kim. arXiv preprint On-Device Text-to-Image Generation. arXiv:2404.11925, 2024. 3 [10] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 3, 8, 15, 16 [11] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 2, 3, 8, 15, 16 [12] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation, 2025. 3 [13] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers. In Proceedings of the 41st International Conference on Machine Learning, pages 95509575. PMLR, 2024. 3, 4 [14] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. 3 [15] DeepFloyd. Deepfloyd. https://github.com/deep-floyd/IF, 2023. 8, 15, 16 [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 3 [17] Khatri Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham M. Kakade, Ali Farhadi, and Prateek Jain. Matformer: Nested transformer for elastic inference. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3, [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2, 3, 7, 8, 15, 16 [19] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 7 [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS), 2014. 3 [21] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 3 [22] Ali Hassani, Steven Walton, Humphrey Shi, et al. Generalized neighborhood attention: Multi-dimensional arXiv preprint sparse attention at the speed of light. arXiv:2504.16922, 2025. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 2020. 3 [25] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In Proceedings of the 40th International Conference on Machine Learning, pages 1321313232. PMLR, 2023. 3 [26] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion: 1.5 fid on imagenet512 with pixel-space diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1806218071, 2025. 3, 4 [27] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert with adaptive width In Advances in Neural Information Processing and depth. Systems, pages 97829793. Curran Associates, Inc., 2020. 3 [28] Dongting Hu, Jierun Chen, Xijie Huang, Huseyin Coskun, Arpit Sahni, Aarush Gupta, Anujraaj Goyal, Dishani Lahiri, Rajesh Singh, Yerlan Idelbayev, Junli Cao, Yanyu Li, Kwang-Ting Cheng, S.-H. Chan, Mingming Gong, Sergey Tulyakov, Anil Kag, Yanwu Xu, and Jian Ren. Snapgen: Taming high-resolution text-to-image models for mobile devices with efficient architectures and training. arXiv:2412.09619 [cs.CV], 2024. 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [29] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [30] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 7 Ella: [31] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-toImage Generation . IEEE Transactions on Pattern Analysis Machine Intelligence, (01):117, 5555. 7 [32] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 7 [33] Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, and Jian Ren. Ascan: Asymmetric convolution-attention networks arXiv preprint for efficient recognition and generation. arXiv:2411.04967, 2024. [34] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally Compressed Stable Diffusion for Efficient Text-to-Image Generation. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. 3 [35] Black Forest Labs. Flux: generative model by black forest labs. https://github.com/blackforestlabs/flux, 2024. Accessed: 2025-05-14. 2, 3, 7, 8, 15, 16 [36] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v1, . 3 [37] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v2, . [38] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground V2. 5: Three Insights towards Enhancing Aesthetic Quality in Text-to10 Image Generation. arXiv preprint arXiv:2402.17245, 2024. 3, 8, 15, 16 [39] Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3 [40] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 13 [41] Shanchuan Lin, Anran Wang, and Xiao Yang. SDXLLightning: Progressive Adversarial Diffusion Distillation, 2024. 3 [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 7 [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [44] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 3 [45] Renjing Liu, Jiatao Li, William Peebles, and Saining Xie. Magicedit: High-fidelity and temporally coherent video editing. arXiv preprint arXiv:2303.08354, 2023. 3 [46] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. [47] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 7 [48] Chenlin Meng, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of In NeurIPS 2022 Workshop on guided diffusion models. Score-Based Methods, 2022. 3 [49] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 3 [50] ModelTC. Qwen-image-lightning: Distilled qwen-image fast, high-fidelity text-to-image generation. models for https://github.com/ModelTC/QwenImageLightning, 2025. Version V1.x/ V2.x available; Apache2.0 license. 7, 15 [51] Dogyun Park, Moayed Haji-Ali, Yanyu Li, Willi Menapace, Sergey Tulyakov, Hyunwoo J. Kim, Aliaksandr Siarohin, and Anil Kag. Sprint: Sparse-dense residual fusion for efficient diffusion transformers, 2025. 3 [52] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2023. 2, [53] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3, 8, 15, 16 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 7 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [56] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep lanIn Advances in Neural Information guage understanding. Processing Systems, pages 3647936494. Curran Associates, Inc., 2022. 3 [57] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 3 [58] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. 3 [59] Yang Song, Jascha Sohl-Dickstein, Durk Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations (ICLR), 2021. 3 [60] Yang Song, Chenlin Meng, and Stefano Ermon. Consistency models. International Conference on Machine Learning (ICML), 2023. 3 [61] Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey Tulyakov, and Jian Ren. Bitsfusion: 1.99 bits weight quantization of diffusion model. In Advances in Neural Information Processing Systems, pages 7677576818. Curran Associates, Inc., 2024. 3 [62] Gemma Team. Gemma 3n. 2025. 5 [63] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gael Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Iain Barr, Benjamin Coleman, Yi Gao, Basil Mustafa, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andras Gyorgy, Andre Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Poder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Leonard Hussenot. Gemma 3 technical report, 2025. [64] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers, 2024. 3, 4 [65] Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Parsa Kavehzadeh, Marzieh Tahaei, Boxing Chen, and Ali Ghodsi. Sortednet: scalable and generalized framework for training modular deep neural networks, 2024. 3 [66] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, 11 Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3 [67] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. Phased consistency model. [68] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural language processing. In Annual Conference of the Association for Computational Linguistics, 2020. 3 [69] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 2, 3, 6, 7, 15, 16 [70] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi (Stephen) Chen, Xinggang Wang, Hongyang Chao, and Han Hu. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2197021980, 2023. 7 [71] Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, Yanzhi Wang, and Sergey Tulyakov. Taming diffusion transformer for efficient mobile video generation in seconds, 2025. 3 [72] Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris N. Metaxas, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapgen-v: Generating five-second video within five seconds on mobile device. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 24792490, 2025. [73] Ruijie Xi, Qingxiong Zhang, Hongyu Gao, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 3 12 [74] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. 3 [75] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. 3, 8, 15, 16 [76] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generIn Proceedings of the IEEE/CVF ation via diffusion gans. Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. 3 [77] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 2, 3, 7 [78] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2, 3, [79] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 7 [80] Jiahui Yu, Linjie Huang, Shixiang Wang, Aviv Efrat, Jaehoon Cho, Jonathan Brandt, Tong Gao, Wei Chen, and Thomas Han. Slimmable neural networks. In International Conference on Learning Representations (ICLR), 2019. 3, 5 [81] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. Featured Certification. 8 [82] Yifan Yuan, Jiayi Zhang, Peng Sun, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. 3 [83] Zeyu Zhang, Weihao Xu, Yujie Wang, et al. Fast video arXiv preprint generation with sliding tile attention. arXiv:2502.04507, 2025. 3 [84] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to-image generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. 2, [85] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Lirui Zhao, Si Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-next : Making In Advances lumina-t2x stronger and faster with next-dit. in Neural Information Processing Systems, pages 131278 131315. Curran Associates, Inc., 2024. 8, 15, 16 SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Discussion of On-Device Latency We report the per-step latency and total generation time in Tab. 1. Note that the VAE decoder requires approximately 120ms [28], and additional components such as latent scaling, scheduler stepping, and CLIP embedding introduce negligible latency, similar to observations in [28, 40]. Thanks to our proposed Adaptive Sparse Self-Attention, the quantized full model can still run on mobile devices without encountering out-of-memory issues. Table 1. Latency and Generation Time of Our Models Model Parameters Ours-tiny Ours-small Ours-full* 0.3B 0.4B 1.6B * Model is 4-bit quantized. Per-step Latency 280ms 360ms 1580ms 4-step Generation 1.2s 1.8s 6.7s B. Demo on Mobile Device We include an on-device demonstration on the project page, showcasing our small model (0.4B). It achieves generation time of 1.8s per image and produces high-quality outputs at 10241024 resolution on an iPhone 16 Pro Max. The application is implemented using the open-source Swift Core ML Diffusers framework. Upon launching the app, users can input textual prompts and generate corresponding images by simply tapping the Generate button. Two screenshots of on-device generation on an iPhone 16 Pro Max are shown in Fig. 1, featuring results from both our small and full variant with 4-bit quantization. C. On-device Deployment Details To enable mobile-friendly deployment, we optimize the model to minimize computational overhead by reducing operations such as transpose and reshape. We structure the model in convolutional fashion, where the channel dimension is placed as the third-to-last dimension (i.e., (B, C, H, )), rather than following the conventional transformer layout of (B, L, D). We reimplement the attention mechanism using split einsum operations to improve on-device efficiency. For Blockwise Neighborhood Attention (BNA), computations for each block are executed in parallel through for-loop, enabling efficient execution Figure 1. On-device Image Generation Demo. Screenshots from our on-device application running on an iPhone 16 Pro Max. The left panel shows results from the small (0.4B) model, and the right panel shows results from the full variant with 4-bit quantization. on mobile hardware. Finally, the model is exported via CoreML to generate computation graph for deployment. To deploy the full model (1.6B) on deivce, we quantize all linear and convolutional layer weights using k-means clustering over their values. Most layers are quantized to 4 bits (16 clusters), while more sensitive layers are assigned 8 bits. Sensitivity is determined with simple heuristic: for each layer, we measure the mean-squared error (MSE) between the layers quantized output and the corresponding output from the unquantized model, when quantizing that layer in isolation. Layers with the largest degradation in MSE are designated as sensitive and quantized at 8 bits, resulting in an overall average quantization of 4.3 bits. After quantization, we freeze the weights and fine-tune the remaining parameters, such as biases and normalization layers, using self-distillation for several thousand iterations. 13 Figure 2. Illustration of Blockwise Neighborhood Attention (BNA). Visualization of BNA under different hyperparameter settings of block number (b) and neighborhood radius (r), showing the corresponding spatial neighbor coverage and attention sparsity. D. Additional Illustration of Blockwise NeighF. Qualitative Comparison on ImageNet borhood Attention In Fig. 2, we illustrate BNA under different configurations. Specifically, configurations (b) and (c) in BNA produce spatial neighbor coverage similar to the standard self-attention mask with three spatial neighbors in (a), while configurations (e) and (f) correspond closely to the five-neighbor case in (d). By adjusting the block number and neighborhood radius r, one can flexibly control the sparsity of BNA to balance computational efficiency and representational fidelity. In our experiments setting we set to 16 and to 1, essentially yields 9 spatial neighbor tokens at 10242 resolution. E. Detailed Results on T2I Benchmarks We present detailed results for DPG-Bench in Tab. 2, GenEval in Tab. 3 and T2I-CompBench in Tab. 4. We present some visual results of ImageNet-1K between our 0.4B small model (Validation Loss = 0.5090) and SnapGen U-Net [28] (0.4B, Validation Loss = 0.5131) in Fig. 3. G. Additional Qualitative Comparison on T2I To further demonstrate the visual fidelity and prompt adherence of our model, we provide additional qualitative comparisons on text-to-image (T2I) generation tasks. Our models are evaluated across diverse prompts spanning objects, scenes, and artistic compositions, highlighting their ability to produce high-quality, semantically accurate, and visually consistent outputs. As shown in Fig. 4 and Fig. 5, our approach delivers competitive visual quality and superior alignment with textual descriptions, outperforming baseline methods with significantly larger parameter counts. 14 Table 2. Detailed Results of DPG-Bench Comparisons. Model Param Global Entity Attribute Relation Other Overall SnapGen [28] PixArt-α [11] PixArt-Σ [10] SANA [75] LUMINA-Next [85] SD3-Medium [18] SDXL [53] Playgroundv2.5[38] IF-XL [15] SD3.5-Large [1] Flux.1-dev [35] HiDream-I1-Full [8] Qwen-Image [69] Ours-tiny Ours-small Ours-full 0.4B 0.6B 0.6B 1.6B 2.0B 2.0B 2.6B 2.6B 5.5B 8.1B 12B 17B 20B 0.3B 0.4B 1.6B 88.3 75.0 86.9 86.0 82.8 83.5 83.3 83.1 77.7 87.4 74.4 76.4 91.3 88.5 84.2 85. 85.1 79.3 82.9 91.5 88.7 89.6 82.4 82.6 81.2 92.1 90.0 90.2 91.6 90.2 90.9 91.5 87.0 78.6 88.9 88.9 86.4 86.7 80.9 81.2 83.3 90.0 89.9 89.5 92.0 88.8 89.0 89.6 87.3 82.6 86.6 91.9 80.5 93.2 86.8 84.1 81.8 88.2 90.9 93.7 94.3 92.6 93.1 94. 87.6 77.0 87.7 90.7 81.8 92.5 80.4 83.5 82.9 88.1 88.3 91.8 92.7 78.8 79.6 80.4 81.1 71.1 80.5 84.8 74.6 85.1 74.7 75.5 75.6 85.6 83.8 85.9 88.3 84.6 85.2 87.2 H. Training Implementation Details 10 We adopt FSDP2 for distributed training across 32 nodes, each equipped with 8 A100 GPUs (80 GB). The model is initially trained at resolution of 2562 with global batch size of 8192 using the Adam optimizer and learning rate 4 for 400K iterations under elastic training. Subof 1 sequently, the resolution is increased to 10242 with global batch size of 2048 and gradient checkpointing enabled. This stage incorporates knowledge distillation (KD) and continues under elastic training for an additional 100K iterations. For the step-distillation stage (K-DMD), we set the time shift to 3, following the few-step teacher configuration in [50]. The teacher in the DMD objective employs cfg = 4, consistent with the default setting of Qwen-Image [69]. We apply LoRA to both the student network and the critic, using rank of 64 and α = 128. The student is updated every 5 iterations. Training is conducted for 10K iterations across 4 nodes (global batch size 512) using the Adam optimizer with learning rate of 1 10 4 and β = (0, 0.99). 15 Table 3. Detailed Results of GenEval Bench Comparisons. Param. Single Object Two Objects Counting Colors Position Color Attribution Overall 0.4B 0.6B 0.6B 1.6B 2.0B 2.0B 2.6B 2.6B 5.5B 8.1B 12B 17B 20B 0.3B 0.4B 1.6B 1.00 0.98 0.99 0.99 0.92 0.98 0.98 0.98 0.97 0.98 0.98 1.00 0.99 1.00 1.00 1.00 0.84 0.50 0.65 0.77 0.46 0.74 0.74 0.77 0.74 0.89 0.81 0.98 0. 0.91 0.91 0.97 0.60 0.44 0.46 0.62 0.48 0.63 0.39 0.52 0.66 0.73 0.74 0.79 0.89 0.62 0.64 0.66 0.88 0.80 0.82 0.88 0.70 0.67 0.85 0.84 0.81 0.83 0.79 0.91 0.88 0.85 0.89 0.90 0.18 0.08 0.12 0.21 0.09 0.34 0.15 0.11 0.13 0.34 0.22 0.60 0. 0.26 0.22 0.32 0.45 0.07 0.12 0.47 0.13 0.36 0.23 0.17 0.35 0.47 0.45 0.72 0.77 0.56 0.55 0.70 0.66 0.48 0.53 0.66 0.46 0.62 0.55 0.56 0.61 0.71 0.66 0.83 0.87 0.69 0.70 0.76 Model SnapGen [28] PixArt-α [11] PixArt-Σ [10] SANA [75] LUMINA-Next [85] SD3-Medium [18] SDXL [53] Playgroundv2.5 [38] IF-XL [15] SD3.5-Large [1] FLUX.1-dev [35] HiDream-I1-Full [8] Qwen-Image [69] Ours-tiny Ours-small Ours-full Table 4. Detailed Results of T2I CompBench Comparisons. Model Param. Color Complex Nonspatial Shape Spatial Texture Overall PixArt-α [11] PixArt-Σ [10] SANA [75] LUMINA-Next [85] SD3-Medium [18] SDXL [53] Playgroundv2.5 [38] IF-XL [15] SD3.5-Large [1] FLUX.1-dev [35] HiDream-I1-Full [8] Qwen-Image [69] Ours-tiny Ours-small Ours-full 0.6B 0.6B 1.6B 2.0B 2.0B 2.6B 2.6B 5.5B 8.1B 12B 17B 20B 0.3B 0.4B 1.6B 0.416 0.585 0.660 0.511 0.794 0.570 0.644 0.591 0.768 0.764 0.749 0.836 0.765 0.770 0.794 0.334 0.380 0.377 0.350 0.384 0.331 0.364 0.354 0.382 0.374 0.401 0.399 0.372 0.370 0.375 0.308 0.309 0.312 0.303 0.315 0.311 0.308 0.311 0.316 0.307 0.314 0. 0.316 0.316 0.316 0.389 0.479 0.529 0.333 0.582 0.481 0.486 0.512 0.591 0.501 0.592 0.605 0.545 0.551 0.600 0.197 0.244 0.322 0.185 0.324 0.199 0.217 0.182 0.275 0.253 0.399 0.443 0.331 0.350 0.419 0.461 0.566 0.652 0.438 0.731 0.520 0.607 0.577 0.712 0.627 0.696 0. 0.680 0.679 0.712 0.351 0.427 0.476 0.353 0.522 0.402 0.437 0.421 0.507 0.471 0.525 0.557 0.502 0.506 0.536 16 Figure 3. Qualitative comparison on ImageNet-1K. Visual comparison between on-device models SnapGen [28] (0.4B, left in each pair, validation loss = 0.5131) and our small model (0.4B, right in each pair, validation loss = 0.5090). Our model produces sharper textures, more consistent colors, and improved structural fidelity across diverse categories. Figure 4. Additional Qualitative Comparison. Our models demonstrate competitive visual quality and superior prompt-following ability. Input text prompts are shown above each image grid; all images are generated at 10242 resolution. Zoom in for details. 18 Figure 5. Additional Qualitative Comparison. Our models demonstrate competitive visual quality and superior prompt-following ability. Input text prompts are shown above each image grid; all images are generated at 10242 resolution. Zoom in for details."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "Snap Inc.",
        "The University of Melbourne"
    ]
}