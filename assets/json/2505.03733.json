{
    "paper_title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch",
    "authors": [
        "Zimu Lu",
        "Yunqiao Yang",
        "Houxing Ren",
        "Haotian Hou",
        "Han Xiao",
        "Ke Wang",
        "Weikang Shi",
        "Aojun Zhou",
        "Mingjie Zhan",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 3 3 7 3 0 . 5 0 5 2 : r WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi Aojun Zhou, Mingjie Zhan, Hongsheng Li Multimedia Laboratory (MMLab), The Chinese University of Hong Kong luzimu@mail.ustc.edu.cn hsli@ee.cuhk.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, novel benchmark designed to measure an LLM-based agents ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworksBolt.diy, OpenHands, and Aiderusing multiple proprietary and open-source LLMs as engines. The bestperforming combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from subset of this training set achieves an accuracy of 38.2%, surpassing the performance of the best proprietary model. We release our data-generation, training, and testing code, along with both the datasets and model weights at https://github.com/mnluzimu/WebGen-Bench."
        },
        {
            "title": "Introduction",
            "content": "Recent developments in large language models (LLMs) have demonstrated increasingly strong performance. When paired with agent frameworks, they have become much more competent at solving challenging tasks such as fixing bugs in complex codebases and competing in coding competitions. Prior works have sought to quantify the software engineering abilities of these LLM-powered agents by testing them on curated GitHub issues [16, 39] and feature-patching requests [23]. These tasks involve advanced modifications to existing codebases and primarily target expert engineers. On the other hand, there is growing need for code agents to assist non-experts with little or no programming background in building applications tailored to their needs and expectations. For example, Bolt.new1 and Lovable.dev2 are two projects that generate complete websites based on user 1https://bolt.new 2https://lovable.dev Preprint. Under review. Table 1: Comparison of WebGen-Bench with other repository-level software engineering benchmarks. indicates that the statistics for SWE-Bench Multimodal are median values, whereas the others are average values. The values for our benchmarks are gathered from the test results of Bolt.diy, OpenHands, and Aider using DeepSeek-V3. The values for the other benchmarks are taken from [16], [23], and [39], respectively. Benchmark WebGen-Bench (ours) SWE-Bench SWE-Bench Multimodal SWE-Lancer From Scratch Training Set Number of Files Lines of Code 8.1 1.7 2 315.3 32.8 27 55 requests and have become very popular among customers. This task poses significant challenges for LLM-based agents, as building fully functional and customized web application from scratch tests wide range of capabilitiesincluding high-level planning, organizing complex multi-file codebases, and implementing nuanced user requirements. However, there is currently lack of systematic and reliable evaluation methods for this task. The high demand for such applications, coupled with the value of assessing agent capabilities, highlights the need for novel benchmark to evaluate the ability to generate websites from scratch based on natural language instructions. To this end, we introduce WebGen-Bench, the first benchmark to systematically evaluate LLM-based agents ability to construct websites that satisfy the functional and appearance requirements specified in user instructions. As shown in Table 1, unlike prior software-engineering benchmarks [16, 23, 39], which focus on fixing bugs or supplying patches to existing codebases, our benchmark requires models to build complex codebase from scratch, assessing agents ability to plan, develop, and manage projects with multi-file structures. There are two critical challenges to address when creating the benchmark: (1) how to curate diverse instructions covering major web-application categories and (2) how to accurately evaluate the websites generated from scratch. To tackle these problems, we introduce systematic data curation and evaluation pipeline for assessing website-generation agents. Starting from 20 common categories identified across popular development platforms, we use GPT-4o to generate diverse instructions and test cases that cover both functionality and appearance, followed by manual verification. For evaluation, we leverage WebVoyager for automated functional testing and prompt GPT-4o to rate design aesthetics on scale of 1 to 5. Using this framework, we benchmark Bolt.diy, OpenHands, and Aider, and find that Bolt.diy performs best. Further evaluation across models shows that DeepSeek-R1 achieves the highest functional success rate (27.8%), whereas Claude-3.5-Sonnet leads in appearance with an average score of 3.0, indicating substantial room for improvement. We also construct training dataset named WebGen-Instruct, which contains 6,667 website-generation instructions. To avoid data contamination, we removed instructions that are semantically similar to those in WebGen-Bench by applying Jaccard-similarity filtering and Sentence-Transformersbased deduplication [28], as detailed in Appendix D. Fine-tuning Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectoriesgenerated from subset of WebGen-Instruct by DeepSeek-V3 with rejection sampling raises its accuracy to 38.2%, substantial improvement over its original 9.5% and even higher than the performance of DeepSeek-R1. We also fine-tune Qwen2.5-Coder-7B-Instruct and Qwen2.5-Coder-14B-Instruct on the same training data, and name the resulting family of website-generation models WebGen-LM. Our contributions are as follows: We introduce WebGen-Bench, the first benchmark designed to test the ability of an LLM-based agent to generate websites from scratch. It includes diverse instructions for website generation and corresponding test cases to evaluate website functionalities. We conduct comprehensive evaluations of three high-performance code-agent frameworks Bolt.diy, OpenHands, and Aider using different proprietary LLMs as engines, demonstrating the challenging nature of our benchmark. We construct WebGen-Instruct, training set consisting of 6,667 website-generation instructions. We use this training set to fine-tune Qwen2.5-Coder-Instruct models of sizes 7B, 14B, and 32B, 2 Figure 1: The data-curation and automatic-testing pipeline. (a) depicts the process for curating website-generation instructions together with their corresponding test cases. (b) presents the testing pipeline for verifying whether the generated websites meet functionality and design requirements with the WebVoyager UI agent, and for analyzing their aesthetic quality using GPT-4o. resulting in family of LLMs specialized in website generation, named WebGen-LM. WebGenLM-32B achieves an accuracy of 38.2% on WebGen-Bench, surpassing DeepSeek-R1."
        },
        {
            "title": "2 Related Work",
            "content": "Software Engineering Benchmarks. Code generation has long been used as means to evaluate the abilities of LLMs [12, 5, 4]. Previous works have collected coding problems from various sources, such as user queries [42], coding contests [15], model synthesis [44], and expert design [24, 5], to evaluate LLMs performance on single-file, function-level coding tasks. Recently, as stronger models have reached plateau on these simpler benchmarks, new benchmarks such as SWE-bench [16, 39] and SWE-Lancer [23] have been constructed by collecting real-world code repositories and corresponding issue requests to test models ability to solve bugs and implement new functionalities. These benchmarks require models to identify and fix issues [16, 39, 2], perform code completions [18, 41], or provide functionality patches [23] within an existing multi-file codebase. Different from previous works, our benchmark focuses on creating web applications from scratch based on natural language instructions, requiring models to generate complex, multi-file codebase, implement multiple functionality and appearance requirements, and make independent technical design decisions. LLM-based Code Agents and Pipelines. Various agent-based [33, 38, 1, 7, 6, 34] and pipelinebased [35, 29, 43] methods have been proposed to address software engineering problems such as code completion and GitHub issue resolution. While pipeline-based methods sometimes demonstrate strong performance on specific tasks with fixed pipelines [16], agent-based methods are generally more flexible. Code agent frameworks such as OpenHands [33] and SWE-agent [38] interact with executable environments to obtain feedback from the execution of generated code. To evaluate our benchmark, we selected three open-source code agents. Among them, OpenHands [33] and Aider [1] are general-purpose code agent frameworks that we adapted for our benchmark, while Bolt.diy [30] is specialized framework for generating web applications. Prior works [26, 22, 20, 36, 21] have employed various post-training methods to improve the performance of open-source models. In this work, we also fine-tune open-source models with generated trajectories. Automatic Software User-testing. User-testing is common method in software engineering to assess the functionality of software with high user-interaction requirements. However, human testing can be costly and introduce significant management complexities. Various works have employed 3 agents to test websites [19], graphical user interfaces (GUIs) [8], and games [31, 9]. Among them, UXAgent [19] uses UI agents with pre-defined personas to simulate user experiences on websites. Our work also utilizes web navigation UI agent to evaluate generated websites. Different from prior works, we define atomic test cases targeting functionality and appearance requirements, enabling the agent to perform operations and observe whether the website behaves as intended."
        },
        {
            "title": "3 WebGen-Bench",
            "content": "In this section, we introduce WebGen-Bench, the first benchmark designed to test the ability of LLM-based agents to generate websites from scratch based on natural language instructions. The benchmark consists of diverse website-generation instructions and comprehensive test cases that have been carefully constructed and repeatedly validated. reliable and cost-effective testing pipeline, built around strong web navigation agent, has been developed to ensure efficient evaluation of the generated websites. The data curation process and testing pipeline are shown in Fig. 1 (a) and (b) respectively. 3.1 Instruction Curation Web Development Project Descriptions Collection. To ensure the diversity and practicality of the instructions, we first carefully browsed several platforms containing website development project listings, including Upwork3, Freelancer4, and Proginn5. We identified twenty prevalent web application categories, as outlined in Table 8. To simulate numerous customized web applications, we employ panel of forty computer science Ph.D students to conduct brainstorming sessions to determine various specific web applications belonging to these categories, as well as brief and clear list of corresponding functionality and appearance requirements for each application. customized application and its corresponding requirements are combined into project description. We manually created 10152 project descriptions in total. Website-Generation Instruction Curation. From the collected project descriptions, we use one-shot prompting with GPT-4o to generate the corresponding instructions. The prompt template is shown in Fig. 5 of Appendix C. Because the total number of generated instructions exceeds the practical limits of benchmarking code agentswhich require substantial computational resources and long inference trajectorieswe sample 2 to 8 representative examples from each category to preserve both coverage and diversity. This procedure produces curated test set containing 101 instructions. Next, we decontaminate the remaining instructions by first filtering those with 5-gram Jaccard similarity score exceeding 0.6 relative to any testing instruction. We then perform semantic deduplication by computing cosine similarity between sentence embeddings [28] of the remaining instructions and the testing set. This process produces training set of 6,667 website-generation instructions, which we name WebGen-Instruct. Details of the decontamination process are provided in Appendix D. Test Set Adjustment and Validation. We refine and validate the selected test instructions to ensure they exclude unreasonable designs and specific technical details. We intentionally omit technical design specifications because our dataset aims to evaluate code agents in scenarios where they receive instructions from non-expert users. The agents should autonomously determine the optimal technical approach. Including tool-specific hints in the instructions would compromise this objective. Technical Classification of the Testing Set. Given the limited number of testing instructions per application category, analyzing categorical statistics based on the original 20 application categories would be confusing. To enable higher-level analysis, we reorganize the 101 testing instructions into three broader technical categories (see Tab. 2): 1. Content Presentation: Static page generation (e.g., corporate/portfolio sites), dynamic rendering (e.g., blogs/news feeds), data visualization (e.g., dashboards), and immersive media displays (e.g., 360 product views). 2. User Interaction: Form systems, authentication flows, real-time collaboration tools, e-commerce transactions, and AI-enhanced features (e.g., chatbots). 3. Data Management: CRUD operations for content administration, third-party 3https://www.upwork.com 4https://www.freelancer.com 5https://www.proginn.com 4 Table 2: The number of website-generation instructions in each technical category in WebGen-Bench is shown. Each main category contains multiple subcategories. sample may belong to one main category and multiple subcategories. Main Categories Sample Number Sub Category Sample Number Content Presentation User Interaction Data Management Total Static Page Generation Dynamic Content Rendering Data Visualization Media Display Form Systems Authentication Real-time Features E-commerce AI Integration CRUD Operations API Integration Big Data File Handling 20 18 36 6 40 18 20 22 29 20 12 5 28 49 24 101 API integrations (e.g., payment/social platforms), analytical processing of user behavior data, and file operations (e.g., cloud synchronization, bulk exports). 3.2 Test Case Construction and Evaluation Since the websites are generated from scratch based on the instructions, the tested agents have significant freedom in their implementation choices. To accurately evaluate how well the agents satisfy the instruction requirements while accommodating diverse implementation approaches, we construct test cases targeting each and every requirement in the instructions. Test Case Construction. Each test case consists of an operation verifying specific functionality or appearance requirement, paired with its expected outcome. We first generate draft test cases using GPT-4o with the prompt shown in Fig. 10. Two computer science Ph.D. students then independently review and refine these test cases. After comparing their adjustments, we resolved discrepancies through discussion, yielding final set of 647 test cases (411 per instruction). This manual validation process guarantees strict alignment between test cases and instructions, ensuring: (1) all instruction requirements are covered by test cases, and (2) each test case corresponds to an instruction requirement. This approach ensures comprehensive evaluation while preserving implementation flexibility for the tested agents. UI Agent-based Evaluation. With instructions and test cases prepared, we must determine how to effectively evaluate the generated websites. Manual testing by human evaluators is costly and time-consuming, as completing test case takes at least 60 seconds, and finishing all 647 test cases would require approximately 10.8 hours at an estimated cost of $377.8 [32]. This slow, labor-intensive process would hinder rapid iteration during framework development, preventing researchers from obtaining timely feedback when refining website-generation systems. To improve testing efficiency, we automate test case evaluation. Inspired by [19], which employs persona-based agents for web usability testing, we utilize WebVoyager [11], robust web navigation UI agent, to execute test operations and verify outcomes. We structure each test cases operation and expected outcome into standardized prompt  (Fig. 12)  , which directs the agent to simulate user interactions, analyze action trajectories and screenshots, and return YES, NO, or PARTIAL assessments based on requirement fulfillment. The process is shown on the right side of Fig. 1 (b). When the agent reaches its interaction limit, we trigger decision prompt, inducing the agent to make final decision  (Fig. 11)  . Considering the cost induced by multiple interactions with the website in evaluating each test case, we employ Qwen2.5-VL-32B-Instruct, an efficient open-source vision-language model that balances performance and cost-effectiveness, as the agents engine. (a) Instruction-string lengths (b) Test case numbers Figure 2: Distributions of instruction lengths (left) and test case numbers per instruction (right). Table 3: Statistics for the instruction string lengths and the test case numbers in our dataset. Statistic Minimum Maximum Median Average Instruction Length (chars) Test Case Number 324 4 876 483 6 496.8 6.4 3.3 Evaluation of Website Appearance Apart from the fulfillment of the functionality and appearance constraints in the instructions, another important aspect of website generation is the level of relevance, harmony, and aesthetics of the webpage. To conduct quantitative analysis of this aspect, we designed set of detailed metrics, ranging from the success of rendering and the relevance of the content to the harmony of the layout and the modernness of the design. We then place the metrics in prompt, asking GPT-4o to grade the appearance of the website with score ranging from 1 to 5 (the higher the better), as demonstrated in the middle part of Fig. 1 (b). The prompt is shown in Fig. 13. Examples are presented in Appendix. M. 3.4 Analysis of Dataset Attributes In this section, we analyze the distribution of instruction lengths and the number of test cases per instruction. The corresponding plots are shown in Fig.2, and the minimum, maximum, median, and average values are summarized in Tab.3. As depicted in Fig. 2,(a), most website-generation instructions contain between 400 and 600 characters, with median length of 483 and an average length of 496.8. These relatively long prompts add considerable complexity, posing meaningful challenge to the agents under evaluation. Fig. 2,(b) indicates that most instructions are associated with five to seven test cases. The median and average numbers of test cases are 6 and 6.4, respectively. Because each test case corresponds to distinct requirement in the instruction, these statistics confirm that every instruction encompasses sufficient set of functional and appearance requirements."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Frameworks. We evaluate three popular code-agent frameworks: Bolt.diy [30], OpenHands [33], and Aider [1]. Bolt.diy is the open-source version of Bolt.new6, browser-based framework for generating and previewing web applications. It provides user interface and Linux-like WebContainer environment that can execute code. It first prompts the model to decide which frontend and backend frameworks to use (such as Vite, React, Remix, etc.), then imports the basic template and builds upon it. OpenHands is platform for AI-powered software development agents. For OpenHands, we pair it with CodeActAgent to evaluate it on our benchmark. The adapted instruction is presented 6https://bolt.new 6 Table 4: Evaluation of three powerful code-agent frameworks using different proprietary and opensource models. Accuracy is computed using weighted score, where YES samples are weighted by 1 and PARTIAL samples are weighted by 0.5; the total score is then divided by the number of test cases. The highest accuracy and appearance scores are marked in bold. Start Failed Accuracy Appearance Partial Rate Test Name Yes Rate No Rate Score Bolt.diy Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 GPT-4o o3-mini Qwen2.5-Coder-32B Qwen2.5-72B-Instruct WebGen-LM-7B WebGen-LM-14B WebGen-LM-32B OpenHands Deepseek-V3 Aider Deepseek-V3 22.6 24.7 18.5 10.4 17.9 8.2 12.1 24.9 25.0 34. 7.4 12.5 7.6 6.2 4.5 4.8 3.4 2.6 3.6 7.1 8.7 8.0 3.2 3.1 64.1 64.3 73.9 64.5 40.0 81.8 80.7 68.0 66.3 57. 5.7 4.8 3.1 20.4 38.6 7.4 3.7 0.0 0.0 0.0 26.4 27.8 20.8 12.8 19.6 9.5 13.8 28.4 29.4 38.2 73.9 15.5 9.0 54. 30.1 14.1 3.0 2.5 2.0 1.5 1.6 1.1 1.4 2.5 2.5 2.8 1.5 1.2 in Appendix I. Aider is terminal-based AI programming framework that natively supports many popular programming languages, including Python, JavaScript, PHP, HTML, CSS, and more. Aider constructs map of the entire codebase, which helps it function well in larger projects. We use the adapted instruction in Appendix to generate websites with Aider. Models. We first evaluate the three frameworks on DeepSeek-V3 [17], model that is both performant and cost-effective. We then evaluate several strong general-purpose proprietary and open-source LLMsincluding Claude-3.5-Sonnet [3], DeepSeek-R1 [10], GPT-4o [14], o3-mini [25], Qwen2.5Coder-32B [13], and Qwen2.5-72B-Instruct [37]on the best-performing framework, Bolt.diy. We do not test general-purpose models smaller than Qwen2.5-Coder-32B, as we observe that such models often fail to follow the specified output format and therefore cannot generate valid websites. Training Details. To validate the effectiveness of our training set, we selectively generated Bolt.diy trajectories for subset of 2K instructions from WebGen-Instruct using DeepSeek-V3. Using rejection sampling [40], we retained only the trajectories whose corresponding websites achieved an appearance score greater than or equal to 3, resulting in 600 trajectories. This filtering ensures that the remaining generated websites are relevant to the instructions and do not exhibit major rendering issues. We then fine-tuned Qwen2.5-Coder-Instruct models of sizes 7B, 14B, and 32B for 2 epochs, with learning rate of 4e-5 and batch size of 32. The 7B, 14B, and 32B models were trained on 8, 16, and 32 A800 GPUs, respectively. This fine-tuning process yields family of models specialized in website generation, which we name WebGen-LM. 4.2 Experimental Results We present the results on the entire WebGen-Bench dataset in Tab.4, and the accuracy for each category of instructions and test cases in Tab. 5. Accuracy is computed using the formula Accuracy = NYes+0.5NPartial 100%, where NYes and NPartial denote the number of test cases assessed as YES and NTotal PARTIAL, respectively, and NTotal is the total number of test cases. Main Results. Based on the experimental results, we make the following observations: (1) As shown in Tab. 5, WebGen-LM-32B achieves the highest accuracy of 38.2%, surpassing the best proprietary model, DeepSeek-R1, by 10.4%, demonstrating the effectiveness of our training set and the rejection-sampling process. (2) Bolt.diy with DeepSeek-R1 as the engine achieves the highest accuracy among general LLMs at 27.8%, closely followed by Claude-3.5-Sonnet with an accuracy of 26.4%. This indicates that the best-performing models are still far from saturating WebGen-Bench, Table 5: Category-wise evaluation results. The first three columns represent categories of websitegeneration instructions, while the last three represent categories of test cases. The highest score in each category is marked in bold. Instruction Categories Test Case Categories Content Presentation User Interaction Data Management Functional Testing Data Display Testing Design Validation Testing Test Name Bolt.diy Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 GPT-4o o3-mini Qwen2.5-Coder-32B Qwen2.5-72B-Instruct WebGen-LM-7B WebGen-LM-14B WebGen-LM-32B OpenHands Deepseek-V3 Aider 35.6 43.7 37.1 26.4 28.7 17.5 28.2 27.9 30.2 46.6 21.2 20.6 16.6 5.9 17.7 6.9 10.1 23.8 27.8 33.2 26.2 24.7 11.2 11.2 13.4 5.9 5.6 38.1 31.6 38. 12.6 7.3 8.4 17.1 21.1 10.5 4.7 11.4 1.9 5.8 22.0 23.6 29.1 3.8 9. 26.3 29.3 28.2 19.6 25.5 14.5 21.0 27.7 26.9 43.0 8.1 19.1 52.0 44.3 38.1 24.6 33.6 23.0 25.4 47.5 49.2 56.1 25.0 18. Deepseek-V3 17.8 12.8 12.5 Table 6: Alignment between the UI agent testing results and human testing results. The alignment rate denotes the proportion of test cases in which the UI agents results match those of human testers. Model Claude-3.5-Sonnet Deepseek-R1 Deepseek-V3 Testing Method UI Agent Manual UI Agent Manual UI Agent Manual Yes Rate Partial Rate No Rate Accuracy Alignment Rate 22.6 22.4 24.7 28.0 18.5 19.0 7.6 7.1 6.2 4.3 4.5 4. 64.1 59.0 64.3 58.1 73.9 70.3 26.4 26.0 27.8 30.1 20.8 21. 90.3 86.1 94.4 highlighting that our benchmark remains challenging for current LLMs and agent frameworks. (3) Smaller general open-source models, such as Qwen2.5-Coder-32B and Qwen2.5-72B-Instruct, show significant performance gap compared to proprietary models. (4) In terms of appearance scores, Bolt.diy with Claude-3.5-Sonnet achieves the best performance of 3.0. The appearance score exhibits loose correlation with accuracy, as functional webpages typically do not suffer from major rendering issues. To better understand the statistical characteristics of the generated websites, we analyzed the file count and line count in the generated codebases, as detailed in Appendix K. Categorical Results. Apart from the three main instruction categories (shown in Tab. 2), we also classify the test cases into three primary categories based on what they are intended to evaluate: Functional Testing, Data Display Testing, and Design Validation Testing. Detailed definitions and statistics for these categories are provided in Fig. 18 and Tab. 9 in Appendix L. As shown in Tab. 5, among the different categories of test cases, Design Validation Testing achieves the highest accuracy in most cases, while Functional Testing generally yields lower accuracy. Among instruction categories, Content Presentation consistently demonstrates the highest accuracies. This indicates that superficial aspects, such as color themes, are easier to implement than deeper internal functionalities. Table 7: Comparison of yes rate and accuracy at different sample sizes. The base model is Qwen2.5-Coder32B-Instruct. Sample Number Yes Rate Accuracy 150 300 600 21.8 28.6 34.2 25.1 31.9 38.2 Figure 3: Accuracy vs. sample number. 4.3 Ablation Studies Analysis of the Accuracy of UI Agent Testing Results. To analyze the accuracy of the UI agent testing process, we manually examined three sets of testing results on Bolt.diy. We select the results of Claude-3.5-Sonnet, DeepSeek-R1, and DeepSeek-V3 as the accuracies of these three models are high and are close to each other. The manual testing results serve as the ground truth and require precision; therefore, three human testers independently annotated the results and we assessed the consistency of their annotations. If the annotations of test case are inconsistent, fourth human tester is tasked with re-examining the test case and the inconsistent annotations to decide on final annotation. We present the results of manual testing in Tab. 6. The Alignment Rate is computed with Alignment Rate = NManual=Agent 100%, where NManual=Agent denotes the number of test cases where the agent-generated result aligns with the manually-annotated result. Ntotal Analysis of the Number of Training Samples. We analyze the effect of the number of training samples on the accuracy of the fine-tuned models. Specifically, we fine-tune Qwen2.5-Coder32B-Instruct using 150, 300, and 600 samples, respectively. As shown in Fig. 3 and Tab. 7, accuracy consistently increases with the number of training samples, highlighting the potential of our training set. We did not sample additional trajectories due to API budget constraints. Nevertheless, the current sample size already demonstrates the effectiveness of WebGen-Instruct for training website generation LLMs. Further accuracy improvements through additional data or techniques such as data augmentation are left for future work. Figure 4: The distribution of the task errors. Analysis of Errors in WebGen-Bench Tasks. We analyze the errors and flaws that occur in web generation pipelines using mainstream LLMs such as DeepSeek-V3, as illustrated in Fig. 20. detailed explanation of the error types can be found in Appendix N. In addition, we present statistics on the distribution of error types for each task in the test set, as shown in Fig. 4. Currently, more than half of the task errors are due to failures in launching web page or in modifying template. These results highlight significant potential for future research focused on improving the success rate of web page initialization and template adaptation in generated websites."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce WebGen-Bench, novel benchmark for evaluating the ability of LLMbased agents to generate websites from scratch. The benchmark requires agents to construct and organize multi-file codebases while satisfying various functional and visual constraints. We evaluate three code-agent frameworks using both proprietary and open-source LLMs. The best-performing combination, Bolt.diy with DeepSeek-R1, achieves an accuracy of only 27.8%, highlighting the challenging nature of our benchmark. Additionally, we construct training set of 6,667 websitegeneration instructions and fine-tune Qwen2.5-Coder-32B on 600 Bolt.diy trajectories generated by DeepSeek-V3, resulting in an accuracy of 38.2%surpassing even the best proprietary model."
        },
        {
            "title": "References",
            "content": "[1] Aider-AI. Ai pair programming in your terminal, 2024. Accessed: 2025-04-22. [2] Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024. [3] Anthropic. Introducing claude 3.5 sonnet, 2024. Accessed: 2025-04-22. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [6] GitHub Copilot. Github copilot, 2024. Accessed: 2025-04-22. [7] Cursor. Cursor: The ai code editor, 2024. Accessed: 2025-04-22. [8] Juha Eskonen, Julen Kahles, and Joel Reijonen. Automating gui testing with image-based deep reinforcement learning. In 2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS), pages 160167, 2020. [9] Pedro M. Fernandes, Manuel Lopes, and Rui Prada. Agents for automated user experience testing. In 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), pages 247253, 2021. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [12] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [13] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [15] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [16] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [17] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [18] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023. 10 [19] Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Laurence Li, Jiri Gesi, Qi He, Toby Jia-Jun Li, and Dakuo Wang. Uxagent: An llm agent-based usability testing framework for web design. arXiv preprint arXiv:2502.12561, 2025. [20] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. [21] Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, and Bing Xie. Repository structure-aware training makes slms better issue resolver. arXiv preprint arXiv:2412.19031, 2024. [22] Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, and Bing Xie. Sorft: Issue resolving with subtask-oriented reinforced fine-tuning. arXiv preprint arXiv:2502.20127, 2025. [23] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. [24] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [25] OpenAI. Openai o3-mini, 2025. Accessed: 2025-04-22. [26] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. [29] Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. Specrover: Code intent extraction via llms. arXiv preprint arXiv:2408.02232, 2024. [30] stackblitz labs. bolt.diy, 2024. Accessed: 2025-04-22. [31] Samantha . Stahlke, Atiya Nova, and Pejman Mirza-Babaei. Artificial playfulness: tool for automated agent-based playtesting. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, CHI EA 19, page 16, New York, NY, USA, 2019. Association for Computing Machinery. [32] US Bureau of Labor Statistics. Table b-3. average hourly and weekly earnings of all employees on private nonfarm payrolls by industry sector, seasonally adjusted., 2024. [33] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software In The Thirteenth International Conference on Learning developers as generalist agents. Representations, 2024. [34] Scott Wu. Introducing devin, the first ai software engineer, 2024. Accessed: 2025-04-22. [35] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [36] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. 11 [37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [38] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [39] John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024. [40] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [41] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. [42] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts. arXiv preprint arXiv:2405.04520, 2024. [43] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604, 2024. [44] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A Ethics Statement",
            "content": "The WebGen-Bench dataset is entirely composed of synthetically generated instructions and test cases, curated manually and synthesized using artificial intelligence. While this resource is non-commercial, we emphasize that its construction process maintains clear distance from potential ethical or legal concerns, particularly regarding intellectual property. Legal compliance. We take great care in our methodology to uphold copyright integrity, utilizing three protective approaches to safeguard against infringement: (1) all base project descriptions originate from the creative efforts of the authors and student volunteers; (2) the 20 fundamental categories are sufficiently abstracted through systematic analysis; and (3) our framework does not copy content from existing websites or platforms, thereby avoiding copyright infringement risks associated with specific commercial implementations. Dataset Intended Usage and License. We document the WebGen-Bench dataset in this paper and note that both the dataset and the code used for reproducing results are publicly available. We intend for researchers to use this dataset to better evaluate the ability of LLM-based agents to generate websites from scratch. We take full responsibility in the event of any rights violations. The WebGen-Bench dataset and our open-source code are released under the MIT license."
        },
        {
            "title": "B Limitations and Future Work",
            "content": "Website generation in this work is primarily conducted using TypeScript, JavaScript, CSS, and HTML. Other languages such as Python, Java, and Go are not used, due to the complexity of integrating them into the agent framework. Expanding the range of supported languages and tools for automatic website generation with code agents is promising direction for future research. Additionally, we only employed supervised fine-tuning to enhance the performance of open-source LLMs on website generation, without utilizing other post-training strategies such as reinforcement learning or direct preference optimization [27]. These methods present valuable opportunities for future exploration."
        },
        {
            "title": "Descriptions",
            "content": "Fig. 5 presents the prompt used to derive website-generation instructions from web development project descriptions created by human annotators. Notably, the model is instructed to exclude any requirements related to technical implementation details, as the goal is to evaluate the code agents ability to make such decisions independently."
        },
        {
            "title": "D Details of the Decontamination Process",
            "content": "In this section, we introduce the methods we used to decontaminate the training set from the testing set. We first employ 5-gram Jaccard similarity, removing the instructions in the training set with similarity score higher than 0.6 with one of the instructions in the testing set. Then, to remove the instructions that are semantically similar to those in the testing set, we compute the sentence embeddings of the instructions using the all-MiniLM-L6-v2 model of Sentence-Transformer [28], and compute the cosine similarity of the embeddings. We experimented with various threshold settings, and finally settled on removing the training instructions with cosine similarity of larger than 0.55. We then inspect whether the remaining training samples contain instructions that are semantic duplicates of the instructions in the testing. For each testing instruction, we retrieve the top-3 training instructions with the highest cosine similarity, and manually inspect them for semantic duplication. We found that the retrieved training samples are all completely different from the testing samples, proving that the final training set is not contaminated. The first three samples in WebGen-Bench and their top matches in the training set are shown in Fig. 7, Fig. 8, and Fig. 9, respectively. The matches are completely different from the test samples. Fig. 6 shows the distribution of the cosine similarity between the test set and the training set. The cosine similarity is gathered around 0.2 to 0.3, which is relatively low. 13 Prompt: <task> You will be given piece of text containing the basic information of web development project. The information involves main objective and list of functional and appearance requirements. You are requested to convert the information into instructions to build web application. You should output detailed multi-sentence instruction in English explaining in detail the different functions the applications should have. </task> <important> 1. Your output should align with the main objective of the website and expand upon the requirements. 2. You should not specify any technical details in the instructions. 3. You should not refer to any outside applications in your instructions. 4. You should not output any additional comments. </important> The following is an example: <example> Objective: hotel and travel ticket distribution website. Other requirements: 1. User login 2. Order tickets and hotels 3. Cancel orders 4. Verify orders 5. Browse tickets and hotels 6. Light blue background and dark olive green component Converted Instruction: Please implement distribution website for travel and ticketing that sells products such as tickets and hotels. The website should have functionalities for placing, canceling, and verifying orders. Users should be able to log in, browse products like tickets and hotels, place orders for selected products, cancel selected orders, and verify consumption records. Use light blue in the background layer and dark olive green for the component layer. </example> Objective: {Objective} Other requirements: {Other requirements} Converted Instruction: Figure 5: The prompt for deriving instructions from human annotated descriptions. 14 Figure 6: The distribution of the cosine similarity between the test set and the training set. Test Instruction 1: Please implement website for generating stock reports to provide stock information and analysis. The website should have the functionality to search and summarize stock information, and generate customized stock reports based on user requirements. Users should be able to input stock codes or names, select report formats and content, and the website will automatically generate the corresponding reports. The reports should include basic stock information, market trends, financial data, and more. Set the background color to white and the component color to navy. Match 1: Please implement website for generating PDF reports that creates PDF files containing directories, word clouds, logos, and chart displays. The website should have functionalities for uploading data, selecting templates, customizing content, previewing, and downloading PDFs. Users should be able to upload relevant data, choose from different templates, customize the report content, preview the generated PDF file, and download the final PDF report. Specify bisque as the base color and dark salmon for all components. Similarity: 0.549 Match 2: Please implement an accounting factory website for enterprise financial management and statistics. The website should have functionalities for creating service enterprises, setting declaration types, and extracting statistics by quarter and year. Users should be able to log in, create and manage service enterprises, set declaration types, view and analyze financial data, and perform WeChat payment and other operations. Set page background to light beige; color all components with sienna. Similarity: 0. Match 3: Please implement report frontend website to display vehicle inspection report data. The website should have functionalities for displaying report templates, inspection report information, and audit status. Users should be able to log in, browse, and view inspection reports, including report details, inspection results, and audit status. Use powder blue for container backgrounds and royal blue for component visuals. Similarity: 0.538 Figure 7: Top semantic matches for the first test instruction in WebGen-Bench with similarity scores. Application Categories of WebGen-Instruct and WebGen-Bench. Tab. 8 lists the 20 application categories manually summarized by the authors through browsing web development projects on popular platforms that connect programmers with clients seeking custom website solutions, such as Upwork7, Freelancer8, and Proginn9. These application categories serve as seed ideas for our human annotators during the brainstorming of new application scenarios. Detailed definition of each category is as follows: 7https://www.upwork.com 8https://www.freelancer.com 9https://www.proginn.com 15 Test Instruction 2: Please implement web-based neighborhood mapping application for comparing data across different areas. The application should allow users to compare demographic, economic, and crime data across different areas. The application should also include data dashboards with interactive charts and customizable layouts. Use ivory for the background and forest green for components. Match 1: Please implement geographic spatial data processing website for handling and analyzing geographic spatial data. The website should have functionalities for data conversion, file interpolation, data operation, and data extraction. Users should be able to upload geographic spatial data files, choose different data formats for conversion, perform data interpolation and operation, and extract the required data. The website should also provide data visualization functionality, allowing users to view and analyze geographic spatial data. Assign mint frost as the background color and apply seagreen to all elements. Similarity: 0.546 Match 2: Please implement geographic information system website for displaying maps and managing the backend. The website should have map visualization capabilities to display different types of geographic information. The backend management platform should be able to manage users, permissions, roles, menus, and support specific business management, such as setting up construction orders, inspecting and evaluating drainage facilities, and managing facilities. Users should be able to log in, browse maps, manage backend data, and perform related operations. Set all pages to have cream background and dark orange components. Similarity: 0.542 Match 3: Please develop Boundary Hunter app to provide nearby data research services. The app should have functionalities for data research, report generation, and user management. Users should be able to log in, browse nearby data research projects, submit research requests, and view reports. The app should also have automated testing and stress testing capabilities to ensure its stability and performance. Use ghost white for the outer layout and cadet blue for UI blocks. Similarity: 0.542 Figure 8: Top semantic matches for the second test instruction in WebGen-Bench with similarity scores. Personal Portfolio Sites: Showcase individual professional projects, achievements, and skills. Company Brochure Sites: Static or minimally interactive websites providing company information, products, services, and contact details. Personal Blog Sites: Regularly updated content sites focusing on personal writing, opinions, experiences, and sharing of knowledge. Social Media Platforms: Applications enabling users to interact socially, share content, and build networks. Discussion Forums: Platforms facilitating conversations, topic-based discussions, threads, and community interactions. E-commerce Web Applications: Online platforms designed for buying and selling goods and services, handling transactions, inventory, and payments. Email Clients: Applications for managing emails, sending, receiving, organizing, and scheduling email communication. Project Management Tools: Platforms aiding in task organization, scheduling, collaboration, and resource management for projects. Streaming and Interactive Platforms: Media-centric platforms for video, audio streaming, or interactive media consumption. CRM Systems: Customer Relationship Management tools designed to manage interactions, sales, customer data, and marketing. ERP Platforms: Enterprise Resource Planning systems integrating core business processes such as finance, HR, supply chain, and operations. Internal Tools: Applications focused on internal company operations, communication, and collaboration. News and Information Sites: Platforms primarily dedicated to delivering news content, articles, and timely updates. 16 Test Instruction 3: Please implement multi-company dashboard for managing and displaying financial data from multiple companies. The dashboard should be able to collect and display financial information from each company, provide consolidated reports, and support cross-company comparisons and reporting. Users should be able to browse financial data from each company, view consolidated reports, and perform financial management and reporting. Apply mint cream as the background; style all components with teal. Match 1: Please implement multi-lingual accounting website for managing financial accounts. The website should have functionalities for logging in, registering, recording, querying, and statistical analysis. Users should be able to log in, create, edit, and delete financial accounts, query historical accounts, and analyze financial status. The website should support multiple languages to facilitate use by users of different languages. Configure the background color to spring green, with components using lime green. Similarity: 0.549 Match 2: Please implement an enterprise resource planning backend management system for managing internal company data. The system should have user management, permission management, module lists, add, edit, delete, and display functions. Users should be able to log in to the system, browse and manage data in different modules, including adding new data, editing existing data, deleting unnecessary data, and displaying all data. The system should also support Excel import and export functions for convenient batch data operations. Use alabaster as the screen background and dark cyan for component highlights. Similarity: 0. Match 3: Please implement data visualization website for telecommunications company to display company data. The website should have multiple pages, each with different dynamic effects. The website should include various charts and maps, with charts having dynamic refresh effects and maps implementing three-level drill-down functionality. Users should be able to browse different pages and view the companys data and statistical information. Use almond as the screen background and sienna for component highlights. Similarity: 0.540 Figure 9: Top semantic matches for the third test instruction in WebGen-Bench with similarity scores. Table 8: 20 application categories manually summarized from popular web-development websites. Application Category Application Category Productivity Applications Internal Tools E-commerce Web Applications Analytics Platforms/Dashboards Publishing/Blogging Platforms Travel Booking Portals CRM Systems Discussion Forums Email Clients Job Search Platforms Project Management Tools Company Brochure Sites Streaming and Interactive Platforms News and Information Sites ERP Platforms Learning Platforms Social Media Platforms Personal Blog Sites Browser-Based Games Personal Portfolio Sites Publishing/Blogging Platforms: Platforms enabling users to publish, edit, and manage content on large scale. Analytics Platforms/Dashboards: Applications providing insights through data visualization, including Business Intelligence and Financial Dashboards. Browser-Based Games: Interactive, entertainment-focused applications running directly in web browsers. Learning Platforms: Educational platforms providing courses, training materials, quizzes, and learning management systems. Travel Booking Portals: Platforms allowing users to search, compare, and book travel services like flights, hotels, and car rentals. Job Search Platforms: Websites connecting job seekers with employers, allowing job postings, applications, and resume management. Productivity Applications: Tools for productivity tasks like document editing, spreadsheets, presentations, and collaborative work. 17 Prompt: Act as testing specialist. Based on the provided prompt below, which was used to generate website, create list of 5-10 actionable instructions to test the websites functionality, content accuracy, and user experience. Each instruction must: 1. Direct UI agent to perform single, atomic task. 2. Include validation criteria. 3. Align with the goals and features described in the original prompt. 4. Ensure each task is atomic (tests one function at time) and avoids combining multiple sub-tasks. Structure each instruction as: Task: Clear, singular task for the UI agent. Expected Result: Specific outcome to confirm success. Original prompt: {orig prompt} Focus on testing: - Core functionalities (e.g., forms, navigation). - Content relevance to the prompts intent. - Accessibility and responsiveness. - Appearance requirements. IMPORTANT: The tasks must directly reflect ALL of the prompts requirements and ensure each instruction is independent and minimal. You must not include tasks that test functions that are not explicitly required by the original prompt! Figure 10: The prompt for deriving test cases that covers all the functional and appearance requirements in the instruction. The {orig prompt} is replaced with the corresponding website-generation instruction."
        },
        {
            "title": "F Prompt for Creating Website Test Cases",
            "content": "Fig. 10 presents the prompt used to construct test cases that evaluate whether the generated website fulfills the requirements specified in the corresponding instruction. The prompt emphasizes the importance of ensuring that all functionality and appearance requirements are covered by the generated test cases. Conversely, every test case should directly reflect an aspect of the instruction. This ensures that the website is thoroughly evaluated and that all test cases are valid."
        },
        {
            "title": "G Prompt for Automatic Evaluation of Test Cases Using an UI Agent",
            "content": "Fig. 12 presents the prompt used to instruct the UI agent to perform the operation described in the test case and respond with YES, NO, or PARTIAL, depending on whether the expected outcome is achieved. Fig. 11 shows the prompt used to induce the agent to make final decision when the maximum number of allowed website interactions has been reached."
        },
        {
            "title": "H Prompt for Grading Website Appearance",
            "content": "Fig. 12 shows the prompt used to grade the aesthetics of webpage appearances. The grading visionlanguage model (GPT-4o in this case) is instructed to consider metrics such as successful rendering, content relevance, layout harmony, and the modernity and visual appeal of the design, and then output grade ranging from 1 to 5 (the higher, the better)."
        },
        {
            "title": "I Prompt for Adapting OpenHands Paired with CodeActAgent for",
            "content": "WebGen-Bench Evaluation Figure 14 presents the prompt used to evaluate OpenHands in combination with CodeActAgent on the WebGen-Bench benchmark. 18 Start-Testing Prompt: Task: {task} Expected Result: {expected result} Instructions: - Attempt the task as user would, using the UI elements available. - Make multiple attempts if needed to try and achieve the expected result. - Observe whether the expected result is fully, partially, or not at all achieved. - IMPORTANT: You can at most interact with the website 15 times. If the limit is reached, directly output your answer. At the end of your testing, answer only with one of the following: - YES: if the expected result was fully achieved. - NO: if the expected result could not be achieved at all. - PARTIAL: if only some aspects of the expected result were achieved. Figure 11: The prompt for starting the operation of test case, where {task} is replaced with the operation to be performed, {expected result} is replaced with the expected state of the website after the operation is performed. Limit-reached Prompt: You have reached the maximum number of allowed interactions with the website. Please evaluate the outcome of your attempts based on the expected result: Expected Result: {expected result} Now, answer with one of the following: - YES: if the expected result was fully achieved during your interactions. - NO: if the expected result was not achieved at all. - PARTIAL: if the expected result was only partially achieved. Provide your final answer based on your testing experience. Figure 12: The prompt for inducing an answer when the limit of the number of website interactions is reached, where {task} is replaced with the operation to be performed, {expected result} is replaced with the expected state of the website after the operation is performed. Prompt for Aider to Generate Websites for WebGen-Bench Evaluation Fig. 15 shows the prompt used by Aider to generate websites for the WebGen-Bench evaluation."
        },
        {
            "title": "K Analysis of Average File Count and Average Line Count",
            "content": "Fig. 16 reports the average file and line counts produced by each model, while Fig. 17a and Fig. 17b show the distributions of file and line counts generated specifically by WebGen-LM-32B. As shown in Fig. 16, GPT-4o, o3-mini, Qwen2.5-72B-Instruct, and Qwen2.5-Coder-32B-Instruct exhibit high average file counts relative to their average line counts, yet their overall performance remains relatively low. One plausible explanation is that, although these models create many files, the files are poorly organized and each contains too little code to support complete website. By contrast, the WebGen-LM models generate more lines of code without disproportionately increasing the number of files. Their average line counts all exceed those of DeepSeek-V3, the teacher model used during distillationan effect that can partly be attributed to the use of rejection sampling. For every WebGen-LM variant, both the file count and the line count rise consistently with model size, indicating that the generated websites become increasingly comprehensive and complex as model scale grows. 19 Appearance-Grading Prompt: Instruction: You are tasked with evaluating the functional design of webpage that had been constructed based on the following instruction: {instruction} Grade the webpages appearance on scale of 1 to 5 (5 being highest), considering the following criteria: - Successful Rendering: Does the webpage render correctly without visual errors? Are colors, fonts, and components displayed as specified? - Content Relevance: Does the design align with the websites purpose and user requirements? Are elements (e.g., search bars, report formats) logically placed and functional? - Layout Harmony: Is the arrangement of components (text, images, buttons) balanced, intuitive, and clutter-free? - Modernness & Beauty: Does the design follow contemporary trends (e.g., minimalism, responsive layouts)? Are colors, typography, and visual hierarchy aesthetically pleasing? Grading Scale: - 1 (Poor): Major rendering issues (e.g., broken layouts, incorrect colors). Content is irrelevant or missing. Layout is chaotic. Design is outdated or visually unappealing. - 2 (Below Average): Partial rendering with noticeable errors. Content is partially relevant but poorly organized. Layout lacks consistency. Design is basic or uninspired. - 3 (Average): Mostly rendered correctly with minor flaws. Content is relevant but lacks polish. Layout is functional but unremarkable. Design is clean but lacks modern flair. - 4 (Good): Rendered well with no major errors. Content is relevant and logically organized. Layout is harmonious and user-friendly. Design is modern and visually appealing. - 5 (Excellent): Flawless rendering. Content is highly relevant, intuitive, and tailored to user needs. Layout is polished, responsive, and innovative. Design is cutting-edge, beautiful, and memorable. Task: Review the provided screenshot(s) of the webpage. Provide detailed analysis and then assign grade (15) based on your analysis. Highlight strengths, weaknesses, and how well the design adheres to the specifications. Your Response Format: Analysis: [24 paragraphs addressing all criteria, referencing the instruction] Grade: [15] Your Response: Figure 13: The prompt for grading the appearance of the webpage. OpenHands Prompt: Create website app using typescript, html, and css. Your codebase should be able to be setup using npm install, and the service should be able to be started using npm run dev. {instruction} Figure 14: The prompt for testing OpenHands paired with CodeActAgent on WebGen-Bench. 20 Aider Prompt: You are Aider, an expert AI assistant and exceptional senior software developer with vast knowledge across multiple programming languages, frameworks, and best practices. <system_constraints> - You MUST generate the code and files Directly without telling me the implementation plan, just generate the codes and files. - No C/C++ compiler, native binaries, or Git - Prefer Node.js scripts over shell scripts - Use Vite for web servers and Node.js for backend - Databases: prefer libsql, sqlite, or non-native solutions - When for react dont forget to write vite config and index.html to the project - You MUST generate complete package.json file with valid package release version. </system_constraints> {instruction} Make sure all the files imported are correctly generated, and complete package.json file with valid package release version exists. Generate the remaining files if needed. Figure 15: The prompt for aider websites generation. Figure 16: The average file and line counts of each model using Bolt.diy as the framework. As shown in Fig. 17, most samples contain between 4 and 10 files, while their line counts are largely concentrated between 400 and 500. Only two samples include more than 15 files."
        },
        {
            "title": "L Test Case Categories",
            "content": "Fig. 18 shows the main category distribution of the task cases. Nearly half of the test cases fall under Functional Testing, around 30% under Data Display Testing, and approximately 20% under Design Validation Testing. This is reasonable distribution, as functional testing typically constitutes the majority of web page evaluations. Additionally, Tab. 9 presents the detailed subcategories along with their respective frequencies. Functional testing ensures that all features of an application work as intended. This includes testing form operations such as submission and validation workflows; verifying authentication flows like user registration, login, and permission checks; and validating payment functionalities in e-commerce checkouts or donation processes. It also encompasses search capabilities across various domains such as stock codes, products, or employees, and filtering data based on specific requirements. Additionally, functional testing covers generation tasks such as creating reports or files; file operations including 21 (a) File-count distribution. (b) Line-count distribution. Figure 17: Distributions of the number of files and lines produced by WebGen-LM-32B. Figure 18: The distribution of the task case categories. downloading, uploading, and printing; e-commerce activities such as purchasing or booking items; and communication features like sending messages or emails. Data display testing focuses on how data is presented and updated within an application. This involves ensuring that dynamic content rendering works correctly, including real-time data updates, website navigation, and page refresh mechanisms. It also includes verifying the accuracy of data visualization elements such as charts, graphs, and maps. Furthermore, this type of testing checks the functionality of displaying detailed information when users request more specific data. Design validation testing focuses on the aesthetic and responsive aspects of an applications user interface. It involves verifying UI consistency across the application and ensuring that color schemes, typography, and spacing are correctly implemented. Responsive behavior is also tested to confirm that the application adapts properly to different devices and screen sizes. Finally, component styling is checked to ensure that elements such as buttons, icons, and cards adhere to the intended design standards. 22 Table 9: The number of task cases in each category. There are multiple subcategories under each main category. task case can belong to one main category and multiple subcategories. Main Categories Task Number Sub Category Task Number Functional Testing Data Display Testing Design Validation Testing Total 186 122 Form Operations Authentication Flows Payment Searching Filtering Generation File Operation E-commerce Communication Dynamic Content Rendering Data Visualization Details Information UI Consistency Responsive Behavior Component Styling 134 48 7 49 27 63 23 58 71 155 30 91 122"
        },
        {
            "title": "M Examples of Websites with Different Appearance Scores",
            "content": "Fig. 19 presents examples of websites with varying appearance scores. As shown in the figure, the visual quality of the websites improves as the appearance score increases. At score of one, the websites exhibit major rendering errors or contain irrelevant content, whereas at score of five, the design appears highly harmonious."
        },
        {
            "title": "N Examples of Websites with Different errors or flaws",
            "content": "Fig. 20 presents the errors or flaws that generated website may contain. For example, instances (a), (b), and (c) illustrate three types of errors related to website loading failures. Instances (d), (e), and (f) show incomplete websites: instance (d) displays only the background, instance (e) lacks UI components such as buttons, and instance (f) fails to display an image correctly. Additionally, instance (g) is website that only uses template without customization; instance (h) shows incorrect placement of webpage content, such as misaligned text; and instance (i) uses an inappropriate background color."
        },
        {
            "title": "O Examples of UI Agent Testing Processes",
            "content": "In this section, we present examples of UI agent testing trajectories. Fig. 21, Fig. 22, Fig. 23, Fig. 24, and Fig. 25 show examples of test cases that output YES, as the outcome of the operation matches the expected result. Fig. 26, Fig. 27, and Fig. 28 show examples of test cases that output PARTIAL, as the expected result is only partially achieved. Fig. 29, Fig. 30, and Fig. 31 show examples of test cases that output NO, as the websites behavior does not match the expected outcome. 23 Figure 19: Examples of the screenshots of websites of different appearance scores. 24 Figure 20: The examples of errors or flaws that generated webs may include. Figure 21: The examples of UI agent testing processes resulting in YES. 26 Figure 22: The examples of UI agent testing processes resulting in YES. 27 Figure 23: The examples of UI agent testing processes resulting in YES. Figure 24: The examples of UI agent testing processes resulting in YES. 29 Figure 25: The examples of UI agent testing processes resulting in YES. 30 Figure 26: The examples of UI agent testing processes resulting in PARTIAL. Figure 27: The examples of UI agent testing processes resulting in PARTIAL. 32 Figure 28: The examples of UI agent testing processes resulting in PARTIAL. 33 Figure 29: The examples of UI agent testing processes resulting in NO. Figure 30: The examples of UI agent testing processes resulting in NO. 35 Figure 31: The examples of UI agent testing processes resulting in NO."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: You should answer [Yes] , [No] , or [NA] . [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. Please provide short (12 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: Delete this instruction block, but keep the section heading NeurIPS Paper Checklist\", Keep the checklist subsection headings, questions/answers and guidelines below. Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: As shown in Abstract and Introduction. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: As shown in Limitations and Future Work. 37 Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: As detailed in Experiments and the open-source data and code. Guidelines: The answer NA means that the paper does not include experiments. 38 If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We open-source all our code and data. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). 39 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: As shown in Experiments. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As detailed in Experiments. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. 40 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: As explained in Ethics Statement in Appendix. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As explained in Ethics Statement in Appendix. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: As explained in Ethics Statement in Appendix. Guidelines: The answer NA means that the paper poses no such risks. 41 Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: As explained in Ethics Statement in Appendix. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: As shown in the released code and data. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We did not use crowdsourcing in our experiments. We used authors and student volunteers instead. 42 Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our study does not pose any potential risks to the participants. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: As detailed in Method. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
    ]
}