{
    "paper_title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation",
    "authors": [
        "Yakun Song",
        "Jiawei Chen",
        "Xiaobin Zhuang",
        "Chenpeng Du",
        "Ziyang Ma",
        "Jian Wu",
        "Jian Cong",
        "Dongya Jia",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 5 8 3 0 0 . 6 0 5 2 : r MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation Yakun Song1,2 Jiawei Chen2 Xiaobin Zhuang2 Chenpeng Du2 Ziyang Ma1,2 Jian Wu Jian Cong2 Dongya Jia2 Zhuo Chen2 Yuping Wang2 Yuxuan Wang2 Xie Chen 1Shanghai Jiao Tong University 2Bytedance Inc."
        },
        {
            "title": "Abstract",
            "content": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec."
        },
        {
            "title": "Introduction",
            "content": "Recently, large language models (LLMs) have made transformative progress in natural language processing[Achiam et al., 2023, Liu et al., 2024a, Yang et al., 2024] and audio generation[Anastassiou et al., 2024, Borsos et al., 2023], exhibiting strong capability to model long sequences of discrete tokens. Recent studies have widely adopted audio codec models, such as SoundStream[Zeghidour et al., 2021] and EnCodec[Défossez et al., 2022], as audio tokenizers within audio language modeling frameworks. However, these methods continue to focus on fidelity and computational efficiency in reconstruction as their primary objectives, and often overlook the semantic modelability of discrete representations. As research in representation learning progresses, many researchers have increasingly recognized the optimization dilemma between generative capacity and reconstruction quality. Specifically, improving reconstruction quality can compromise generation performance and require larger models and more Equal contribution. Preprint. Under review. training resources. Conversely, limiting reconstruction capabilities can reduce the upper limit of generation quality[Yao et al., 2025]. Therefore, overemphasis on reconstruction objectives tends to substantially complicate the training of generative models. To enhance generative performance, several studies have introduced explicit semantic supervision to strengthen the encoding of low-frequency semantic content. For example, SemantiCodec[Liu et al., 2024b] employs dual-encoder architecture, combining high-level semantic features extracted by self-supervised, pretrained AudioMAE[Huang et al., 2022] with an acoustic encoder that captures fine details, and utilizes diffusion-based decoder to achieve high-quality reconstruction at ultra-low bitrates. X-Codec[Ye et al., 2025] integrates self-supervised semantic representations directly into the quantization process, thereby enhancing the generative capability of audio language models. Although these strategies yield improved semantic retention, they also result in the loss of high-frequency texture details and the introduction of minor artifacts. In addition, they depend on external models, such as diffusion models[Anastassiou et al., 2024], and thus fail to provide an efficient or fundamental solution. The question of how to achieve high-fidelity reconstruction and improved modelability of discrete codes through intrinsic frequency-domain constraints or regularization mechanisms, without resorting to additional annotation or complex pretraining, remains central issue in contemporary audio codec research. Another major challenge for neural audio codecs arises from their underlying architecture. typical neural codec can be divided into three components: an encoder that projects the raw waveform into continuous latent vector space; vector quantization (VQ) module that discretizes the latent vectors into tokens drawn from finite codebook[Van Den Oord et al., 2017]; and decoder that reconstructs the audio signal from these discrete tokens[Wu et al., 2024]. The VQ module is the central element in achieving discretization. During training, straight-through gradient estimation is typically employed; although this simplifies backpropagation, it also amplifies the error introduced by quantization. Whether using vanilla vector quantization or residual vector quantization (RVQ)[Zeghidour et al., 2021, Défossez et al., 2022], training commonly suffers from codebook collapse, in which many codebook entries go underutilized or only small subset of vectors is frequently activated. This phenomenon leads to poor coverage of the encoding space and reduced token diversity, thereby undermining the expressive capacity and efficiency of downstream generative models. In response to these challenges, we propose MagiCodec, simple Ma sked aussian njected Codec for high-fidelity reconstruction and generation. Largely inspired by TS3Codec[Wu et al., 2024], MagiCodec also adopts an efficient, single-layer streaming codec built on Transformer backbone. Through multi-stage training procedure with clearly articulated motivations, it implicitly suppresses high-frequency noise and strengthens low-frequency structure modeling, thereby achieving joint optimization of reconstruction quality and downstream generative performance. Our core idea is to dispense with external labels and rely solely on intrinsic Gaussian noise injection, so that the codec learns to allocate modeling capacity appropriately across different frequency bands, achieving both high-fidelity reconstruction and efficient, modelable discrete codes at constrained bitrates. In addition, we decompose traditional codec training into three phases: autoencoder, vector quantization, and vocoder. Training in the first two stages involves only the generator, thereby preventing audio phase information from being incorporated into the intermediate representations. This staged training approach effectively avoids the issue of codebook collapse and maximizes codebook utilization. Our main contributions can be summarized as follows: Theoretical contributions. We analytically derive the frequency-domain effect of Gaussian noise injection, showing that it is equivalent to mixing the original signal with lowpassfiltered version, thereby implicitly regularizing high-frequency components. Methodological contributions. We design multi-stage training framework that effectively mitigates codebook collapse and enhances overall token performance, incorporating Gaussian noise injection and latent regularization. These techniques require no external models or labels, yet encourage the codec to learn low-frequency semantic representations and avoid overfitting to high-frequency noise. Experimental contributions. We conduct comprehensive evaluations, demonstrating that MagiCodec achieves state-of-the-art reconstruction quality across multiple bitrates and metrics. In downstream tasks such as text-to-speech, automatic speech recognition, and semantic information extraction, MagiCodec also significantly outperforms baseline methods, confirming its strong modelability. Analysis of the code distribution further reveals Figure 1: The pipeline of the proposed MAGICODEC. its close adherence to the Zipf distribution observed in natural language, which facilitates downstream model training."
        },
        {
            "title": "2 Related Work",
            "content": "Neural audio codecs aim to encode continuous audio signals into discrete latent representations, which is form of discrete audio tokenization. In recent years, neural audio codecs have become research focus to achieve high-quality audio reconstruction at low bitrates[Kumar et al., 2023, Ai et al., 2024]. These methods leverage vector quantization to learn compact codebooks that effectively compress audio information while preserving essential details. To further improve reconstruction quality, various techniques have been proposed. PromptCodec[Pan et al., 2024] introduces additional input prompts to enrich the latent space representation, improving the models adaptability to complex audio content. Moreover, DAC[Kumar et al., 2023] combines quantizer dropout with multi-scale STFT discriminators, effectively enhancing spectral recovery accuracy and improving the naturalness and clarity of reconstructed audio. FreeCodec[Zheng et al., 2024] enables efficient compression and high-quality reconstruction by decoupling inherent properties of speech (such as tone, rhythm and content). APCodec[Ai et al., 2024] and Apcodec+[Du et al., 2024] integrate staged training strategy to significantly enhance the representational power of both encoder and decoder, leading to superior reconstruction performance. However, an excessive focus on reconstruction quality often increases the complexity of the latent space, which in turn imposes heavier training burden on the generation model and reduces generation efficiency[Yao et al., 2025]. This inherent tension between reconstruction and generation remains core challenge in the design of discrete audio tokenization. To address this limitation, recent works like SemanticCodec[Liu et al., 2024b], X-Codec[Ye et al., 2025] and VQGAN-LC[Zhu et al., 2024a] have introduced explicit supervision from pretrained models to enhance semantic retention. While these approaches improve semantic representation, they may lead to the loss of high-frequency details and increase reliance on external resources. In contrast, MagiCodec leverages frequency-domain regularization and Gaussian noise injection to significantly enhance token semantic expressiveness without requiring external supervision, while maintaining high-fidelity reconstruction quality."
        },
        {
            "title": "3 MagiCodec",
            "content": "In this section, we introduce MagiCodec, simple yet efficient single-layer streaming codec that delivers both high-fidelity reconstruction and strong downstream task performance. 3 3.1 Model Architecture In Figure 1, we present high-level overview of MagiCodecs end-to-end workflow. As in traditional audio codec frameworks, the raw waveform is first downsampled and mapped into low-dimensional latent space by the encoder, which is then discretized into tokens by the quantizer. Finally, the decoder reconstructs the waveform from these tokens. The encoder consists of linear downsampling module, windowed Transformer, and linear reduction layer. For the 16kHz model, all input audio is sampled at 16kHz. The encoder first applies two-layer linear network (the linear downsampling module) to downsample the input waveform RT by factor {160, 320, 640}, corresponding to token rates Tr {100, 50, 25} Hz, and projects it into hidden space of dimension = 4096, yielding the Transformer input RTrH . This frame sequence is then fed into Transformer [Vaswani et al., 2017] with sliding window of size 32, in which each token attends only to itself and its left context to enforce strict streaming inference. single linear projection (the linear reduction layer) maps the dimension from down to the VQ codebook embedding size = 16, producing Ze RTrD. After the encoder module, the single quantizer uses codebook of size = 131072 to quantize Ze into discrete tokens. During training, gradients are passed through straight-through estimator (STE) [Bengio et al., 2013]. At inference time, each feature is quantized to its nearest neighbor in the codebook, yielding discrete tokens. In the decoding stage, discrete tokens are converted back to embeddings Zq via codebook lookup, then passed through the decoder, which mirrors the encoders architecture. single linear lifting layer restores the dimension to H, followed by Transformer with left-context window of 32 and right-context window of 2 to enhance reconstruction quality while preserving streaming properties. Finally, linear upsampling layer reconstructs the waveform ˆx RT at the original sampling rate. To further improve perceptual fidelity, MagiCodec incorporates GAN-based optimization during certain training phases. We elaborate on this in Section 3.3. 3.2 Gaussian Noise Injection 3.2.1 Motivation Conventional audio codec research has predominantly emphasized reconstruction quality, often neglecting the dimension of generative performance. However, in downstream generative tasks, reconstruction fidelity, compression efficiency, and modelability are all indispensable: inaccurate reconstruction directly constrains the fidelity of generated signals; suboptimal compression efficiency not only slows down generation but also substantially increases computational and storage costs; and insufficient modelability typically forces the use of larger, more expensive, and more complex language or other generative model backbones, further exacerbating system compute demands and limiting overall performance[Skorokhodov et al., 2025]. Recent works have integrated semantic labels and other supervisory signals into VQ training to enhance the downstream modelability of tokens[Liu et al., 2024b, Zhang et al., 2023, Huang et al., 2023, Défossez et al., 2024]. Although this semantic supervision helps the model capture low-frequency structural information, it typically requires additional neural networks, and models often sacrifice high-frequency texture details to minimize semantic loss, resulting in reconstruction artifacts[Ye et al., 2025]. Neural networks inherently exhibit spectral bias[Rahaman et al., 2019], preferentially learning low-frequency structures first; high-frequency local oscillations are harder to fit and more prone to overfitting noise. Excessively preserving high-frequency content both wastes bits and increases the difficulty of model fitting. We hypothesize that preserving excessive random high-frequency components degrades both the perceptual quality of the latent representation and its modelability in downstream generative tasks. To address this, we introduce Gaussian noise injection, general-purpose method that requires no additional supervision. Our approach is exceptionally simple to implement and, from Fourieranalytic perspective, can be shown to impose an exponentially decaying regularization on highfrequency components. As result, it maintains high reconstruction fidelity while markedly enhancing the codecs performance on downstream tasks. 4 3.2.2 Method For the input frame X, we independently sample per-frame masks mt Bernoulli(p). If mt = 1, the original frame is fully replaced by i.i.d. Gaussian noise ϵt (0, σ2I); otherwise, it remains unchanged, yielding the noise-injected sequence X. Training with random additive noise has been shown to be equivalent to incorporating Tikhonov regularization term into the loss function [Bishop, 1995], and subsequent studies have further interpreted this as an explicit high-frequency regularizer, thereby encouraging the model to focus on semantically relevant low-frequency structures [Camuto et al., 2020]. By using replacement noise instead of additive noise, we entirely remove local time-domain information in masked frames, forcing the model to rely on longer-range context for reconstruction and overall semantics. Leveraging longer contextual dependencies enables language models to learn smoother, low-frequencydominated latent dynamics, thereby reducing the receptive field required by downstream language models to achieve comparable semantic coverage . We state concise proposition that, in the Fourier domain, elucidates the high-frequency attenuation induced by Gaussian noise injection. Proposition 1. For any Fourier-transformable network mapping , applying Gaussian noise injection to the input x, we have E(cid:2)f (x)(cid:3) = (cid:104) (1 p) + 1 2 σ2ω2(cid:105) (cid:98)f (ω) . Here, (cid:98)f (ω) denotes the Fourier coefficient of at frequency ω. See Appendix A.1 for the full formulations and derivations. Consequently, high-frequency components are explicitly attenuated, whereas low-frequency structures remain virtually unaffected. The Experiments 4 further validates the effectiveness of our proposed method. 3.3 Training Stages In single-stage end-to-end VQ training, an unpretrained encoder coupled with randomly initialized codebook often maps large fraction of inputs to nearly identical embeddings, causing VQ collapse and consequently ineffective quantization, distorted generation, or even complete training stagnation. Prior work[Zhao et al., 2024] categorizes this failure into token collapse and embedding collapse, attributing the root cause to the synchronous cold start of both the encoder and the codebook. To mitigate this, we employ three-stage training strategy combined with latent-variable regularization, which in our experiments yields significantly more stable codebook learning and improved reconstruction and generative metrics. Specifically, our training stages are as follows: Stage 1: Autoencoder We first train only the encoder-decoder with no quantization applied so that it learns stable representations. This warm-up provides strong initialization for the subsequent quantization stage and prevents the synchronous oscillations that can arise when the codebook and encoder receive simultaneous gradient updates in early training. Additionally, we incorporate latent-space regularization loss Lnorm = Ze2 2 into the training objective. This latent regularization can prevent unstable, unconstrained latent vectors from causing training collapse in the early stages. It can be viewed as simplified form of KL regularization, encouraging the latent space to be more compact and continuous, which is advantageous for vector quantization. Stage 2: Quantizer During this phase, we freeze the encoder and exclusively optimize the vector quantizer and the decoder. Fed by the high-quality continuous latent representations, the quantizer can learn more robustly, mitigating codebook collapse caused by early-stage oscillations. The codebook is optimized using an L1 loss, computed between the features prior to and following quantization, with stop-gradient operation applied as described in[Van Den Oord et al., 2017]. Consistent with the methodology adopted in SimVQ[Zhu et al., 2024b], we reparameterize the code vectors through linear transformation layer based on learnable latent basis. Additionally, to prevent the encoder outputs from attaining excessively large magnitudes, commitment loss with weighting factor of 0.25 is introduced. Stage 3: Vocoder At this stage, the parameters of both the encoder and the vector quantizer are frozen, and only the decoder is updated during training. multi-scale Mel-spectrogram reconstruc5 tion loss is employed, defined as the L1 distance between the predicted and reference spectrograms across multiple frequency resolutions. The mel-spectrogram is widely acknowledged as reliable proxy for perceptual audio quality. Furthermore, to enhance the perceptual realism of the reconstructed audio, we adopt the adversarial training strategy introduced in BigCodec[Xin et al., 2024], incorporating two distinct types of discriminators. The first is the Multi-Period Discriminator (MPD) from HiFi-GAN[Kong et al., 2020], which is designed to capture diverse periodic structures in speech waveforms. The second is the Multi-Scale Short-Time Fourier Transform (MS-STFT) Discriminator, as implemented in EnCodec[Défossez et al., 2022], which captures spectral features at multiple time-frequency resolutions. Training objectives The training losses for the generator of MagiCodec include mel-spectrogram reconstruction loss Lmel, quantizer loss Lq, latent regularization loss Le, adversarial loss Ladv, and feature matching loss Lf eat. Specifically, our training loss is formulated as follows: For melspectrogram reconstruction loss, we compute the L1 distance between the predicted and reference mel-spectrograms at multiple frequency resolutions. For GAN loss, we calculate the L2-norm as the adversarial loss over the logits of the discriminators and use the L1-norm to calculate the feature matching loss. For VQ loss, we follow the classic VQ training scheme, using an L1 codebook loss and an L1 commitment loss. Thus, the total loss for the i-th training stage of MagiCodec, Li, is given by: L1 = λ1 melL1 mel +λ1 eL1 e, L2 = λ melL2 mel +λ2 qL2 q, and L3 = λ3 melL3 mel +λ advL3 adv +λ3 eatL3 eat."
        },
        {
            "title": "4 Experiments",
            "content": "To comprehensively assess MagiCodecs reconstruction quality and downstream generative modelability, we carried out extensive experiments. 4.1 Experimental Setup 4.1.1 Datasets We train our codec models on the Libri-light corpus[Kahn et al., 2020], which contains approximately 60,000 hours of unlabelled English speech sampled at 16 kHz. We evaluate reconstruction fidelity on the LibriSpeech test-clean subset, comprising 2,620 utterances from 40 speakers. 4.1.2 Evaluation Metrics We employ range of evaluation metrics spanning several dimensions. For computational efficiency, we consider model parameter count (nParams), bitrate, frame rate, token rate, streaming capability, and the number of codebook layers. Speech intelligibility is assessed using Short-Time Objective Intelligibility (STOI), Word Error Rate (WER), and Phone Error Rate (PER). Distortion and perceptual audio quality are evaluated with metrics such as Perceptual Evaluation of Speech Quality (PESQ), Virtual Speech Quality Objective Listener (ViSQOL), and UTokyo-SaruLab MOS (UTMOS). To assess speaker similarity, we report SPK-SIM. Detailed definitions and calculation procedures for each metric are provided in Appendix B. 4.1.3 Baselines We selected series of state-of-the-art codecs as baselines. To ensure fair comparison, we employed the official pretrained weights for EnCodec2[Défossez et al., 2022], Mimi3[Défossez et al., 2024], DAC4[Kumar et al., 2023], Vocos5[Siuzdak, 2023], SNAC6[Siuzdak et al., 2024], WavTokenizer7[Ji 2https://huggingface.co/facebook/encodec_24khz 3https://huggingface.co/kyutai/mimi 4https://huggingface.co/descript/dac_16khz 5https://huggingface.co/charactr/vocos-mel-24khz 6https://github.com/hubertsiuzdak/snac 7https://github.com/jishengpeng/WavTokenizer 6 Table 1: Computation efficiency comparison of various codec models.. indicates the streaming BigCodec reproduced by the TS3-Codec authors. Model Bitrate nParams Frame Rate Token Rate Codebook Layer Streaming"
        },
        {
            "title": "DAC\nWavTokenizer\nSpeechTokenizer\nSemantiCodec",
            "content": "Encodec Mimi Vocos SNAC BigCodec TS3Codec MagiCodec (Ours) - 1000 900 1000 700 1500 550 1500 984 1040 850 850 - 74.65M 80.9M 103.7M 699.4M 14.85M 79.3M 7.9M 19.8M 159.9M 203.6M 209.7M - 50 75 50 25 75 12 75 46.88 80 50 50 - 100 75 100 150 50 150 82 80 50 50 - 2 1 2 2 2 4 2 3 1 1 1 - et al., 2024], SpeechTokenizer8[Zhang et al., 2023], and SemantiCodec9[Liu et al., 2024b]. Given that TS3Codec serves as our primary baseline and no official pretrained model is available, we directly extracted the experimental results for both TS3Codec and BigCodec-S from the TS3Codec paper[Wu et al., 2024]. Here, BigCodec-S refers to the streaming variant of BigCodec[Xin et al., 2024] implemented by the TS3Codec authors using the official BigCodec code. We adopt the identical training and evaluation datasets used by TS3Codec and BigCodec-S to ensure fair comparison (see Table 1 for more details for various codec models). 4.1.4 Tasks In addition to the reconstruction task, we conducted extensive downstream experiments to evaluate MagiCodecs modelability, focusing on two main categories of tasks: generation and understanding. We validate generative capability via zero-shot TTS, while the comprehension tasks encompass phone-level speech recognition, emotion recognition, and non-verbal detection. Zero-shot TTS For TTS applications, using the codecs outputs as intermediate speech representations allows us to assess whether the quantized features can drive decoder to generate natural, coherent speech. Zero-shot TTS demands precise reproduction of speaker prosody, thereby revealing whether codec preserves the information necessary for modeling rhythm and intonation. We evaluate multiple codecs on zero-shot TTS task to determine whether MagiCodec can enhance downstream TTS systems or the performance of audio-based LLMs. For the zero-shot TTS task, we extract discrete tokens from the LibriSpeech[Panayotov et al., 2015] training set and evaluate the TTS models synthesis on utterances of 4 to 10 seconds from the LibriSpeech test-clean set. Since multi-layer VQ codecs typically introduce additional modeling complexity and computational overhead in downstream language-modeling tasks, we restrict our baselines to single-layer codecs. We therefore include WavTokenizer and BigCodec as baselines, using their official pretrained weights. We employ traditional TTS evaluation metrics to assess TTS performance, namely the aforementioned WER, PER, UTMOS, and SPK-SIM. Phone-level Speech Recognition To more precisely assess the codecs ability to preserve finegrained speech details such as consonant and vowel transitions, we define the automatic speech recognition (ASR) tasks output units as phonemes rather than the more common word-level tokens (or Byte-Pair Encoding units). Phoneme-level recognition not only mitigates performance degradation due to out-of-vocabulary items but also reflects differences in intelligibility more sensitively after quantization through the phoneme error rate (PER). 8https://github.com/ZhangXInFD/SpeechTokenizer 9https://github.com/haoheliu/SemantiCodec-inference 7 For the phoneme-level speech recognition evaluation, discrete tokens are extracted from the LibriSpeech training set and performance is measured on utterances of 4 to 15 seconds drawn from the LibriSpeech test-clean set. Only single-layer codecs are used as baselines, specifically WavTokenizer and BigCodec with their official pretrained weights. PER is adopted as the evaluation metric to quantify recognition accuracy. Emotion and Nonverbal Detection In addition to lexical sequences and phoneme-level information, we also aim to evaluate the capacity of codec tokens to capture variety of acoustic cues beyond semantics. To this end, we conduct classification tasks for both emotion recognition and nonverbal detection. These paralinguistic cues represent performance factors that cutting-edge large language models are increasingly prioritizing . For the emotion classification task, we adopt the official training and testing splits of the English subset of the ESD dataset [Zhou et al., 2022]. The training set comprises 3,000 utterances from ten native speakers, while the test set contains 300 utterances drawn from speakers disjoint from those in the training set. The ESD corpus provides balanced coverage of five emotion categories (neutral, happiness, anger, sadness, and surprise), and each audio sample has an average duration of 2.7 seconds. In the non-verbal detection evaluation, we use the VocalSound 16 kHz dataset [Gong et al., 2022], which contains approximately 20k audio samples spanning six categories of non speech vocalizations, including laughter, sighs, coughs, throat clearing, sneezes and sniffs. Following the official data split, the training subset comprises 15,570 recordings while the test subset comprises 3,594 recordings. Alongside BigCodec and WavTokenizer we also employed DAC as baseline, using each models official pretrained weights to extract discrete tokens and training downstream models with single-layer VQ configuration. Emotion and Nonverbal Detection In addition to lexical sequences and phoneme-level information, we also aim to evaluate the capacity of codec tokens to capture variety of acoustic cues beyond semantics. To this end, we conduct classification tasks for both emotion recognition and nonverbal detection. These paralinguistic cues represent performance factors that cutting-edge large language models are increasingly prioritizing. For the emotion classification task, we adopt the official training and testing splits of the English subset of the ESD dataset[Zhou et al., 2022]. The training set comprises 3,000 utterances from ten native speakers, while the test set contains 300 utterances drawn from speakers disjoint from those in the training set. The ESD corpus provides balanced coverage of five emotion categories (neutral, happiness, anger, sadness, and surprise), and each audio sample has an average duration of 2.7 seconds. In the nonverbal detection evaluation, we use the VocalSound 16 kHz dataset[Gong et al., 2022], which contains approximately 20,000 audio samples spanning six categories of non-speech vocalizations, including laughter, sighs, coughs, throat clearing, sneezes, and sniffs. Following the official data split, the training subset comprises 15,570 recordings, while the test subset comprises 3,594 recordings. Alongside BigCodec and WavTokenizer, we also employ DAC as baseline, using each models official pretrained weights to extract discrete tokens and training downstream models with singlelayer VQ configuration. 4.1.5 Implementation Details All models were trained using 16 NVIDIA A100 80GB GPUs. Audio was uniformly resampled to 16 kHz, and each training sample was randomly cropped, fixed-length 10-second segment to enhance dataset diversity. The batch size was adjusted according to model size and GPU memory constraints to achieve optimal hardware utilization. Optimization was performed using the AdamW[Loshchilov and Hutter, 2017] algorithm with β1 = 0.8, β2 = 0.99, and ϵ = 1 109 for robust convergence. The learning rates for the generator and discriminator were initially set to 1 104, annealed to 1 105 via cosine schedule with 1,000-step warmup phase. Training was conducted for total of 100,000 steps. For zero-shot TTS and ASR tasks, we trained the models from scratch using the official open-source GPT-210[Radford et al., 2019]. The model was trained on 8 NVIDIA A100 80GB GPUs. We used 10https://huggingface.co/openai-community/gpt2 8 Table 2: Comparison of reconstruction ability between different codec models around 1000 bps and 50 token per second. indicates results taken from the TS3-Codec paper. Note that models marked with are trained on the same corpus as MagiCodec. WER PER Model STOI PESQ ViSQOL UTMOS SPK-SIM Streaming Ground Truth DAC WavTokenizer SpeechTokenizer SemantiCodec Encodec Mimi Vocos SNAC BigCodec TS3Codec MagiCodec (Ours) 1.85 10.68 3.75 3.61 4. 4.22 4.58 4.32 3.43 3.80 3.60 3.16 0.79 6.47 1.95 1.84 2.55 2.15 2.46 2.39 1.73 - - 1.63 1.00 0.73 0.90 0.77 0. 0.85 0.85 0.89 0.89 0.91 0.91 0.93 4.64 1.13 2.13 1.21 1.79 1.56 1.65 1.96 2.09 2.17 2.23 2.56 5.00 2.85 3.95 3.06 3. 3.59 3.48 3.79 3.85 - - 4.15 4.09 1.29 3.79 2.32 2.93 1.58 3.07 3.04 3.49 3.73 3.84 4.18 1.00 0.32 0.66 0.33 0. 0.60 0.50 0.63 0.66 0.65 0.68 0.76 - 12-layer, 12-head GPT-2 backbone with 768-dimensional hidden size. All dropout probabilities were fixed at 0.1. Optimization was performed using AdamW (β1 = 0.9, β2 = 0.999). The learning rate was set to 5 104. After 5,000 warm-up steps, the rate followed cosine annealing schedule that decayed smoothly to zero over the remaining updates. We trained the model for 20 epochs, with batch size of 32. For emotion and nonverbal detection tasks, we trained BERT[Devlin et al., 2019] model with the same training hyperparameters as above. 4.2 Experimental Results 4.2.1 Reconstruction Quality Table 2 presents comprehensive comparison of MagiCodec against several state-of-the-art neural audio codecs at similar bitrate (8501000 bps) and token rate (50 tokens per second). All metrics reported are related to reconstruction quality. From the results, MagiCodec demonstrates clear advantages in multiple aspects: 1) Speech Content Fidelity. MagiCodec achieves the lowest Word Error Rate (WER) of 3.155 and Phoneme Error Rate (PER) of 1.634 among all neural codecs at comparable bitrate, significantly outperforming strong baselines such as BigCodec (WER 3.800) and TS3Codec (WER 3.600). This indicates that MagiCodecs discrete representations preserve the linguistic content of speech more accurately, attributed to its noise-injected codebook optimization (Sec.3.2). 2) Perceptual Quality and Intelligibility. In terms of perceptual metrics, MagiCodec attains PESQ score of 2.562 and STOI of 0.925, surpassing all listed neural codecs by noticeable margin. These improvements reflect higher perceived speech quality and intelligibility, approaching the natural speech baseline (PESQ 4.640, STOI 1.000). The VISQOL score of 4.147 further confirms MagiCodecs capability in preserving fine-grained acoustic details, contributing to more natural listening experience. 3) Speaker Similarity and Naturalness. MagiCodec achieves the highest speaker similarity score (SPK-SIM = 0.762) and leading naturalness measure (UTMOS = 4.183) among all compared models. This demonstrates that our codec effectively maintains speaker identity and prosodic characteristics during reconstruction. 4) Streaming Capability. Despite its strong performance, MagiCodec maintains moderate model size (209.7M parameters) and supports streaming inference with single-layer codebook architecture. Compared to larger or multi-layer models such as SemanticCodec (699.4M parameters) and BigCodec (159.9M parameters), MagiCodec achieves better trade-off between reconstruction quality and computational efficiency. Its streaming capability further enables real-time deployment scenarios. 9 Table 3: Zero-Shot TTS evaluation of various single-layer codecs to demonstrate their generative modelability on the LibriSpeech test-clean set."
        },
        {
            "title": "Ground Truth",
            "content": "BigCodec WavTokenizer MagiCodec (Ours) Bitrate WER PER UTMOS SPK SIM"
        },
        {
            "title": "Streaming",
            "content": "- 1040 900 850 1.44 6.49 3.83 3.30 0.62 4.07 1.91 1. 4.07 4.18 3.95 4.27 1.00 0.67 0.54 0.61 - Table 4: Results of phone-level speech recognition for various codecs."
        },
        {
            "title": "Model Name",
            "content": "BigCodec WavTokenizer MagiCodec (Ours) PER 8.0 13.1 7.7 4.2.2 Generative modelability Table 3 shows the results of zero-shot TTS task. MagiCodec achieves the lowest word error rate (WER = 3.30 %) and phoneme error rate (PER = 1.71 %) at just 850 bps, while also registering the highest naturalness score (UTMOS = 4.27). This represents substantial improvement over the single-layer quantizer WavTokenizer and the higher-bitrate, non-streaming BigCodec. Although BigCodec edges out MagiCodec slightly in speaker similarity, that advantage comes with significantly greater bitrate overhead and non-streaming latency. MagiCodec tokens make the TTS model more predictable, thereby enabling it to lead in both content accuracy and naturalness. 4.2.3 Comprehension capability We assess the phone-level modeling capacity of each codec by training an ASR model to predict phoneme sequences from codec tokens. As shown in Table 4, MagiCodec achieves the lowest phone error rate (PER) of 7.7%, outperforming BigCodec (8.0%) and substantially improving over WavTokenizer (13.1%). This reduction in PER indicates that MagiCodecs discrete representations preserve finer-grained phonetic information. Next, we evaluate the comprehension capability on sentiment classification and non-verbal detection. we report mean accuracy and F1 (with standard deviations over 10 runs) in Table 5. MagiCodec again leads, achieving 70% accuracy and F1 on sentiment classification and 63% accuracy and F1 on non-verbal detection. By comparison, WavTokenizer attains 62% on both metrics for sentiment and 59% for non-verbal detection, while BigCodec lags further behind. Taken together, these results demonstrate that MagiCodecs single-layer quantization not only excels at retaining phonetic detail (lower PER) but also encodes richer semantic and paralinguistic cues, thereby enhancing modelability on diverse downstream tasks. 4.3 Ablation Study We conducted the ablation experiments using the same experimental settings as in the main experiments. The reconstruction results are shown in Table 6 and the downstream scores in Table 7. Mask ratio. Increasing the proportion of masked frames yields consistent gains across almost all metrics. On the core reconstruction benchmarks, WER drops from 3.34 to 3.16 as the mask ratio rises to 20%, and then plateaus (3.17 at 30%). Similar monotonic improvements appear for PER and the perceptual metrics (PESQ, ViSQOL, UTMOS), suggesting that moderate corruption encourages the encoder to form more robust, context-aware representations. We can see that zero-shot TTS reaches its lowest WER at 30% masking (3.30) and emotion recognition peaks at the same ratio with ACC = 0.70 and F1 = 0.70. We hypothesize that hiding up to one 10 Table 5: Performance comparison of various codecs on downstream tasks of sentiment classification and non-verbal detection. Each model was trained 10 times, and the standard deviation of each evaluation metric is shown as superscript to the mean value."
        },
        {
            "title": "DAC\nBigCodec\nWavTokenizer",
            "content": "MagiCodec (Ours) Sentiment Classification Non-verbal Detection ACC F1 ACC F1 0.540.006 0.590.010 0.620.009 0.700.017 0.540.006 0.590.009 0.620.008 0.700.016 0.590.006 0.510.007 0.590.006 0.630.007 0.590.006 0.510.007 0.590.006 0.630.007 Table 6: Ablation study on reconstruction metrics with MagiCodec under different mask ratios, token rates, and encoder sizes. Reconstruction Model WER PER STOI PESQ ViSQOL UTMOS SPK SIM 50Hz mask 0% 50Hz mask 10% 50Hz mask 20% 50Hz mask 30% 25Hz 100Hz 3.34 3.22 3.16 3.17 6.59 2.23 1.77 1.68 1.63 1.62 3.83 1. 0.93 0.93 0.93 0.93 0.88 0.95 2.56 2.57 2.56 2.56 1.90 3.00 4.16 4.17 4.15 4.16 3.85 4. 4.17 4.18 4.18 4.17 3.59 4.19 0.77 0.77 0.76 0.76 0.61 0.87 third of the acoustic codes forces the quantizer to infer longer-range semantic structurean effect reminiscent of the gestalt reasoning observed in MAE for images. Token rate. Changing the token size trades temporal resolution against sequence length. Halving the rate to 25, Hz degrades reconstruction severely (WER 6.59) and harms every downstream task, confirming that information is simply discarded when tokens are too sparse. Conversely, doubling the rate to 100, Hz pushes reconstruction to its best numbers (WER 2.23, STOI 0.95, PESQ 3.00), but the longer sequences complicate autoregressive generation and thus does harm to downstream tasks. Emotion and non-verbal detection improve only marginally. Taken together, 50Hz offers the best compromise between fidelity and modelability, aligning with prior work that caps useful temporal granularity near the frame rate of human speech perception. We observe that moderate levels of masking consistently improve both the reconstruction performance and the generative usability of the codec, trend that aligns with observations in masked image modeling. On the other hand, setting the token rate too low leads to the loss of essential phonetic information, while excessively high token rates offer little additional benefit for downstream tasks and may even introduce redundancy. Latent Visualization To provide more intuitive comparison of the encoding results from different models, we visualize the latent representations extracted by MagicCodec, BigCodec, and wavtokenizer. To this end, we employ t-SNE to project the high-dimensional latent spaces of these models onto two-dimensional plane using the ESC-50 dataset [Piczak]. As shown in Figure 2, the latent representations produced by MagicCodec exhibit more distinct clustering in the two-dimensional space, with samples from the same audio class being grouped more closely together compared to the other models. In contrast, the latent spaces of BigCodec and wavtokenizer show less clear separation between different audio categories, with more overlap observed among classes. Furthermore, we investigate the effect of varying the mask ratio in MagicCodec. Our experiments reveal that increasing the mask ratio leads to more concentrated semantic distribution in the latent space, as evidenced by tighter and more compact clusters in the t-SNE visualization. This suggests that higher mask ratio encourages the model to learn more abstract and semantically meaningful representations. Overall, these results demonstrate that MagicCodec not only achieves superior 11 Table 7: Ablation study on TTS, emotion and non-verbal tasks across model variants. TTS Emotion Non-verbal Model WER PER UTMOS SPK SIM ACC F1 ACC F1 50Hz mask 0% 50Hz mask 10% 50Hz mask 20% 50Hz mask 30% 25Hz 100Hz 5.51 3.57 3.37 3.30 5.47 3.26 1.88 1.70 1.71 2.77 4.24 4.27 4.28 4.28 3.74 0.62 0.62 0.62 0.62 0.49 0.680.02 0.690.02 0.670.03 0.700.02 0.540.02 0.580.03 0.680.02 0.690.02 0.670.03 0.700.02 0.540.02 0.580.03 0.610.01 0.630.01 0.620.01 0.630.01 0.570.01 0.570.01 0.620.01 0.630.01 0.630.01 0.630.01 0.570.01 0.580. (a) MagiCodec (mask 0%) (b) MagiCodec (mask 10%) (c) BigCodec (d) MagiCodec (mask 20%) (e) MagiCodec (mask 30%) (f) WavTokenizer Figure 2: Visualization of the latent space using tSNE and 10 random classes in ESC-50 dataset. clustering performance in the latent space but also benefits from enhanced semantic structure as the mask ratio increases. Token Distribution It is well-known that text tokens in natural language follow Zipfs law [Stanisz et al., 2024, Chan et al., 2024], where few high-frequency tokens dominate while many lowfrequency tokens are sparse, reflecting rich semantic hierarchy. If audio tokens exhibit similar Zipf distribution, it indicates strong semantic representation capability. We conducted visualization analysis in 3, which shows normalized frequency versus rank for various token sets and n-grams (n = 1 to 6), including: 1) text word tokens (semantic gold standard), 2) phoneme-level tokens (less semantic content), 3) existing audio tokenization methods, and 4) the proposed MagiCodec. And we have several observations: 1) Word tokens display clear power-law decay across all n-grams, consistent with natural language. 2) Phone tokens have flatter distribution, especially for 1and 2-grams, indicating weaker semantic hierarchy. 3) Existing audio tokens fall between phone and word tokens; as increases, their distributions approach that of words but remain less semantically rich. 4) MagiCodecs distribution closely matches word tokens across all n-grams, particularly for 3, suggesting strong semantic structure and contextual dependence in its representations."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents MagiCodec, simple yet high-performance single-layer streaming audio codec. Compared to state-of-the-art models, MagiCodec improves both reconstruction quality and down12 (a) 1-grams (b) 2-grams (c) 3-grams (d) 4-grams (e) 5-grams (f) 6-grams Figure 3: Plots of normalized token log-frequency versus normalized log-rank for various codec models and natural languages across different n-gram levels. Hapax tokens (tokens appearing only once) have been excluded for all visualizations. stream modelability by generating discrete tokens that better capture semantic and paralinguistic information. multistage training pipeline with noise injection and latent regularization enables MagiCodec to achieve superior results in both objective and downstream tasks, demonstrating its effectiveness and broad potential in neural audio processing."
        },
        {
            "title": "6 Discussion",
            "content": "Limitation Although MagiCodec achieves strong speech reconstruction and downstream task performance, the single-layer quantization may still limit the preservation of fine details in broadband audio such as music. Moreover, since training is conducted only on 16kHz English speech, the robustness of the codec in noisy conditions or at higher sampling rates remains untested. Broader impacts The model is able to maintain high quality even at low bitrates, thereby reducing energy consumption during both training and inference. However, improved reconstruction capabilities may also facilitate unauthorized voice cloning or deepfakes. We encourage researchers to incorporate watermarking, detection tools, and clear usage policies when releasing downstream model weights and interfaces, and we urge the community to remain vigilant and monitor potential misuse."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yang Ai, Xiao-Hang Jiang, Ye-Xin Lu, Hui-Peng Du, and Zhen-Hua Ling. APCodec: neural audio codec with parallel amplitude and phase spectrum encoding and decoding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-TTS: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Chris M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation, 7(1):108116, 1995. doi: 10.1162/neco.1995.7.1.108. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. AudioLM: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31:25232533, 2023. Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen Roberts, and Chris Holmes. Explicit regularisation in Gaussian noise injections. Advances in Neural Information Processing Systems, 33:1660316614, 2020. David Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, and Trevor Darrell. Analyzing the language of visual tokens. arXiv preprint arXiv:2411.05001, 2024. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6): 15051518, 2022. Michael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev, Feargus OGorman, and Andrew Hines. ViSQOL v3: An open source production ready objective speech and audio metric. In 2020 twelfth international conference on quality of multimedia experience (QoMEX), pages 16. IEEE, 2020. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. Technical report, 2024. URL https://arxiv.org/abs/2410.00037. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL, pages 41714186, 2019. Hui-Peng Du, Yang Ai, Rui-Chen Zheng, and Zhen-Hua Ling. APCodec+: spectrum-coding-based high-fidelity and high-compression-rate neural audio codec with staged training paradigm. In 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), pages 676680. IEEE, 2024. Yuan Gong, Jin Yu, and James Glass. VocalSound: dataset for improving human vocal sounds recognition. In Proc. ICASSP, pages 151155. IEEE, 2022. Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. Advances in Neural Information Processing Systems, 35:2870828720, 2022. 14 Zhichao Huang, Chutong Meng, and Tom Ko. Repcodec: speech representation codec for speech tokenization. arXiv preprint arXiv:2309.00169, 2023. Shengpeng Ji, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, et al. WavTokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: benchmark for ASR with limited or no supervision. In Proc. ICASSP, pages 76697673. IEEE, 2020. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. Highfidelity audio compression with improved RVQGAN. Advances in Neural Information Processing Systems, 36:2798027993, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Haohe Liu, Xuenan Xu, Yi Yuan, Mengyue Wu, Wenwu Wang, and Mark Plumbley. SemantiCodec: An ultra low bitrate semantic audio codec for general sound. IEEE Journal of Selected Topics in Signal Processing, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Yu Pan, Lei Ma, and Jianjun Zhao. PromptCodec: High-fidelity neural speech codec using disentangled representation learning based adaptive feature-aware prompt encoders. arXiv e-prints, pages arXiv2404, 2024. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR corpus based on public domain audio books. In Proc. ICASSP, pages 52065210. IEEE, 2015. Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pages 10151018. ACM Press. ISBN 978-1-4503-3459-4. doi: 10.1145/2733373.2806390. URL http://dl.acm.org/citation.cfm?doid=2733373. 2806390. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/ abs/2212.04356. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International conference on machine learning, pages 53015310. PMLR, 2019. A.W. Rix, J.G. Beerends, M.P. Hollier, and A.P. Hekstra. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In Proc. ICASSP, volume 2, pages 749752 vol.2, 2001. doi: 10.1109/ICASSP.2001.941023. Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. UTMOS: Utokyo-sarulab system for voiceMOS challenge 2022. arXiv preprint arXiv:2204.02152, 2022. Hubert Siuzdak. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. arXiv preprint arXiv:2306.00814, 2023. 15 Hubert Siuzdak, Florian Grötschla, and Luca Lanzendörfer. SNAC: Multi-scale neural audio codec. arXiv preprint arXiv:2410.14411, 2024. Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. arXiv preprint arXiv:2502.14831, 2025. Tomasz Stanisz, Stanisław Drozdz, and Jarosław Kwapien. Complex systems approach to natural language. Physics Reports, 1053:184, 2024. Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen. short-time objective intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 42144217, 2010. doi: 10.1109/ ICASSP.2010.5495701. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Haibin Wu, Naoyuki Kanda, Sefik Emre Eskimez, and Jinyu Li. Ts3-codec: Transformer-based simple streaming single codec. arXiv preprint arXiv:2411.18803, 2024. Detai Xin, Xu Tan, Shinnosuke Takamichi, and Hiroshi Saruwatari. BigCodec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Zhen Ye, Peiwen Sun, Jiahe Lei, et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. Wenhao Zhao, Qiran Zou, Rushi Shah, and Dianbo Liu. Representation collapsing problems in vector quantization. arXiv preprint arXiv:2411.16550, 2024. Youqiang Zheng, Weiping Tu, Yueteng Kang, Jie Chen, Yike Zhang, Li Xiao, Yuhong Yang, and Long Ma. FreeCodec: disentangled neural speech codec with fewer tokens. arXiv preprint arXiv:2412.01053, 2024. Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Emotional voice conversion: Theory, databases and esd. Speech Communication, 137:118, 2022. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024a. Yongxin Zhu, Bocheng Li, Yifei Xin, and Linli Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024b."
        },
        {
            "title": "A Technical Proofs",
            "content": "A.1 Proof of Proposition 1 Let ϵ be an isotropic random Gaussian vector in Rd with ϵ (0, σ2I), its probability density function pϵ : Rd is pϵ(u) = 1 (2πσ2)d/2 u2 2σ Denote by Pϵ the corresponding Gaussian probability measure on Rd, then for any Borel set Rd, we have Pϵ(A) = (cid:82) pϵ(u) du , where denotes the Lebesgue measure on Rd. Then for any measurable function , we have E(cid:2)f (x + ϵ)(cid:3) = (cid:90) Rd (x + u) dPϵ(u) = (cid:0)pϵ (cid:1)(x) Let Bernoulli(p)d, {0, 1}d, and Gaussian noise ϵ (0, σ2I). Let the neural network be with input Rd. And m, ϵ, and are pairwise independent. Each frame of is replaced with the independent Gaussian noise with probability p. Then we have xi = (cid:26)xi, mi = 0, ϵi, mi = 1, = 1, . . . , d. Equivalently, the input satisfies = (1 m) + ϵ. where means the Hadamard product. We can write in additive form = + ϵ ϵ p,σ = (ϵ x). p,σ, where Eϵ p,σ (cid:2)f (x + ϵ p,σ)(cid:3) = (cid:90) Rd (x + u) dPϵ p,σ (u) = (1 p) (x) + (kσ )(x) = (gp,σ )(x) 2 exp(cid:0)x2/(2σ2)(cid:1) is the Gaussian kernel and gp,σ is defined by gp,σ(x) = where kσ(x) = (2πσ2) (1 p)δ(x) + pkσ(x). Taking the Fourier transform (denoted by (cid:98) ), and using (cid:98)δ(ω) = 1 and (cid:98)kσ(ω) = eσ2ω2/2, we obtain (cid:98)gp,σ(ω) = (1 p) + eσ2ω2/2 (2) Thus, for any Fourier-transformable function , we have E(cid:2)f (x)(cid:3) = (cid:2)(1 p) + eσ2ω2/2(cid:3) (cid:98)f (ω) ."
        },
        {
            "title": "B Evaluation Metrics",
            "content": "Below we define the evaluation metrics used in this paper. Metrics annotated with indicate that higher values are better, whereas those annotated with imply that lower values are preferable. Computation Efficiency Model Parameter Count (nParams): The total number of model parameters. Bitrate: The number of bits transmitted or stored per second. Frame Rate: The number of frames the encoder processes or outputs per second. Token Rate: The rate at which discrete tokens are emitted by the quantizer. 17 Streaming Capability: boolean flag indicating whether encoding/decoding can be performed on per-frame, online basis. True enables low-latency, real-time processing. False requires batch inputs and incurs higher end-to-end delay. Number of Codebook Layers: The number of layers in multi-layer or hierarchical quantization scheme. Increasing the number of layers enhances quantization expressiveness but also raises storage and lookup overhead, and imposes additional overhead in downstream task modeling. Speech Intelligibility Short-Time Objective Intelligibility (STOI ): An objective measure of speech intelligibility based on short-time signal alignment and correlation, ranging from 0 to 1 [Taal et al., 2010]. Higher STOI values indicate speech that is more easily understood by listeners. Word Error Rate (WER ): We utilize the official weights of the Whisper-large-v3 [Radford et al., 2022] model to transcribe the synthesized audio and compute the Word Error Rate (WER), metric indicative of the audios clarity. Phone Error Rate (PER ): PER offers finer-grained assessment of recognition performance compared to WER. Distortion and Perceptual Perceptual Evaluation of Speech Quality (PESQ ): standard that simulates subjective listening quality, with scores typically ranging from 1 to 5 [Rix et al., 2001]. Virtual Speech Quality Objective Listener (ViSQOL ): An objective, full-reference metric for perceived audio quality. The scores range from 1 (the worst) to 5 (the best) [Chinen et al., 2020]. UTokyo-SaruLab MOS (UTMOS ): machine-learningbased predictor of human MOS (Mean Opinion Score), generally in the range 15 [Saeki et al., 2022]. Speaker Similarity Speaker Similarity (SPK-SIM ): We use this metric to quantify the consistency between decoded audio and the characteristics of the original speaker. We use the WavLM-based11 model [Chen et al., 2022] for speaker verification. 11https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_ verification"
        }
    ],
    "affiliations": [
        "Bytedance Inc.",
        "Shanghai Jiao Tong University"
    ]
}