{
    "paper_title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects",
    "authors": [
        "Hanyang Guo",
        "Xunjin Zheng",
        "Zihan Liao",
        "Hang Yu",
        "Peng DI",
        "Ziyin Zhang",
        "Hong-Ning Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 2 6 5 8 4 1 . 9 0 5 2 : r CodeFuse-CR-Bench CodeFuse-CR-Bench: Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects Hanyang Guo,1 Xunjin Zheng,1 Zihan Liao1 Hang Yu1, Peng DI1,2, Ziyin Zhang1 Hong-Ning Dai3 1Ant Group 2UNSW Sydney 3Hong Kong Baptist University"
        },
        {
            "title": "Abstract",
            "content": "Automated code review (CR) is key application for Large Language Models (LLMs), but progress is hampered by reality gap: existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CRBench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants."
        },
        {
            "title": "Introduction",
            "content": "Code Review (CR) is core practice in modern software development that aims to improve code quality and identify defects through collaborative inspection (Bacchelli & Bird, 2013). As Large Language Models (LLMs) increasingly automate complex software engineering tasks, their application to CR holds immense promise for improving software quality and developer productivity. However, the development of such sophisticated tools is fundamentally constrained by how we measure their performance. Current benchmarks, while valuable, evaluate models in isolated, decontextualized settings, creating significant and growing reality gap between measured performance and real-world efficacy. The core of this problem lies in failure to capture the comprehensiveness of the CR process. Real-world CR is not simple text-matching exercise; instead it is holistic reasoning task that requires deep understanding of context. This disconnection manifests in three critical limitations of existing automated CR research, as demonstrated in Table 1: 1. Task Fragmentation: The cognitive process of human reviewerunderstanding the initial problem, locating potential issues in code change, and formulating coherent reviewis often broken down into isolated sub-tasks like comment generation or code refinement. This fragmentation prevents the evaluation of end-to-end reasoning, crucial capability for truly useful automated reviewer. Equal Contribution. Correspondence to: Hang Yu <hyu.hugo@antgroup.com> and Peng Di <dipeng.dp@antgroup.com>. 1 CodeFuse-CR-Bench Table 1: comparative analysis of CodeFuse-CR-Bench and other prominent code review (CR) and issue-solving benchmarks, illustrating the comprehensiveness gap in existing CR evaluation. The table demonstrates the task fragmentation and context poverty of prior benchmarks, which often omit crucial information necessary for holistic review. CodeFuse-CR-Bench addresses this gap by providing rich, repository-level context, encompassing PR and issue details, commit history, and the complete patch for review. Legend: = Partially Present. Refer to = Present, Section 3.3 for more details. = Absent, (cid:32) (cid:35) (cid:71)(cid:35) Benckmarks CodeFuse-CR-Bench Trans-Review (Tufano et al., 2021) AutoTransform (Thongtanunam et al., 2022) T5-Review (Tufano et al., 2022) CodeReviewer (Li et al., 2022b) AUGER (Li et al., 2022a) SWE-Bench (Jimenez et al., 2024) Multi-SWE-Bench (Zan et al., 2025) FAUN-Eval (Hu et al., 2024) Instance ID Owner/Repo Language Pull No. Title Created at Base Commit Body ISP1 Hint Text Resolved Issue No. CPR2 Head Commit HCM3 Problem Domain Difficulty RCT4 Diff Hunk Review Effort MCP5 Merge Commit 1 Issue Problem Statement (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) : MLP6 (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:32) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:35) 2 Commit Patch to Review (cid:71)(cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) :MLP (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) : MLP (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) : DHLP (cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) 3 Head Commit Message (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) 4 Review Comment Text (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) 5 Merge Commit Patch (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) : MLP (cid:35) (cid:32) (cid:32) (cid:35) (cid:32) (cid:35) (cid:32) (cid:32) (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:32) 6 Method-level Patch (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:32) (cid:32) (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:32) (cid:32) 7 Diff-Hunk-level Patch (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:32) (cid:35) 2. Context Poverty: Existing benchmarks typically provide only small, self-contained code snippets and strip away the rich context that is essential for meaningful review, such as the Pull Request (PR) description, the linked issue report, and the broader repository structure. Without this context, model cannot grasp the intent behind change, making its review superficial. 3. Evaluation Narrowness: Evaluation metrics are often inherited from natural language processing (NLP) tasks (e.g., BLEU). These metrics reward superficial textual similarity but fail to assess the substantive quality of review. They cannot distinguish technically insightful suggestion from syntactically similar but incorrect one, nor can they verify if review comment is even placed at the correct location in the code. In contrast, as shown in Figure 1, typical CR process is holistic reasoning task. The code reviewer receives CR request and the corresponding PR information, which includes PR-related information such as the PR description and associated issue data. Based on this rich context, the reviewer performs the review by writing comments and revision suggestions, which constitute the CR-related information. Figure 1: CR Process To bridge the gap between this reality and current evaluation methods, we introduce CodeFuse-CR-Bench, ComprehensivenessAware benchmark for Repository-level Evaluation of code review. CodeFuse-CR-Bench is designed from the ground up to model this full CR workflow. It comprises 601 high-quality instances curated from 70 real-world Python open-source projects. Each instance is rich, self-contained snapshot of real review task, encompassing basic information, PR-related information, CR-related information, and repository-level context. This multi-faceted context enables models to engage in the kind of holistic reasoning that developers perform daily. Furthermore, to move beyond narrow, syntax-focused metrics, we designed comprehensive evaluation framework for CodeFuse-CR-Bench. This framework integrates both fine-grained rulebased metrics (to assess location accuracy and semantic similarity) and holistic model-based evaluation (which uses the reward model and advanced LLMs-as-judges to score the overall quality, relevance, and correctness of review). Using this benchmark and framework, we conduct the first 2 CodeFuse-CR-Bench large-scale assessment of state-of-the-art LLMs on the comprehensive CR task. Our results establish crucial baselines and reveal the current capabilities and limitations of LLMs when faced with the complexities of real-world code review. In summary, this paper makes the following contributions: We identify and characterize the \"comprehensiveness gap\" in current CR research, highlighting how task fragmentation, context poverty, and narrow evaluation metrics hinder progress. We introduce CodeFuse-CR-Bench, the first comprehensiveness-aware CR benchmark that provides rich, repository-level context to enable the evaluation of end-to-end CR tasks across nine distinct problem domains. We propose novel, multi-faceted evaluation framework that combines rule-based precision with model-based quality assessment to provide more holistic measure of CR performance. We conduct an extensive empirical study on multiple state-of-the-art LLMs, providing the first robust baseline for comprehensive CR and offering insights into future research directions."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 CR-related Tasks Previously, the vast majority of benchmarks and approaches were constructed to improve CR performance (Jiang et al., 2025). Tufano et al. (Tufano et al., 2021) proposed Trans-Review, which adopted deep learning models to partially automate specific CR tasks. They trained two models to implement two CR sub-tasks: (i) code revision before review and (ii) code revision after review. Based on this prior work, they (Tufano et al., 2022) updated the models from deep learning models to the pre-training model T5 and named it T5-Review. The results demonstrated that T5-Review can outperform previous deep learning models for automating CR tasks. Thongtanunam et al. (Thongtanunam et al., 2022) proposed AutoTransform, which leverages Byte-Pair Encoding (BPE) approach to handle new tokens and Transformer-based Neural Machine Translation architecture to handle long sequences. It can be used in the task of code revision before review. Zhou et al. (Zhou et al., 2023) evaluated the above three automatic CR tools and pre-trained models for three processes in CR: code revision before review, review comment generation, and code revision after review. The results show that general-purpose pre-trained model CodeT5 can outperform other models in most cases. Li et al. (Li et al., 2022a) proposed review comments generator with pre-training models, which is called AUGER. They collected empirical review data from 11 notable Java projects and constructed dataset of 10,882 code changes to evaluate the performance of the proposed approach. Li et al. (Li et al., 2022b) proposed pre-trained model that utilized four pre-training tasks tailored specifically for the CR scenario, named CodeReviewer. They focused on three key tasks related to CR activities, including code change quality estimation, review comment generation, and code refinement to evaluate the model. They also constructed high-quality benchmark dataset based on our collected data for these three tasks and conducted comprehensive experiments on it. The experiments demonstrated the SOTA results. Additionally, some LLM-based CR approaches were proposed. Guo et al. (Guo et al., 2024) conducted the first empirical study to explore the capabilities of ChatGPT in CR tasks, specifically focusing on automated code revision after reviews. They constructed new CR dataset with high quality based on the existing benchmark CodeReview (Li et al., 2022b). SOTA CR tool (Li et al., 2022b) was selected as baseline. The research study provided insights into the potential of ChatGPT in automating the CR process. As an important upstream sub-task of CR, issue-solving has also been extensively studied. The content of issue-solving can provide more complete contexts for CR. Some issue-solving benchmarks had been proposed. Jimenez et al. (Jimenez et al., 2024) proposed benchmark, SWE-Bench, that evaluates LLMs in resolving an issue (typically bug report or feature request) submitted to popular Python GitHub repositories. Zan et al. (Zan et al., 2025) enlarged the SWE-Bench dataset by adding other programming languages issue-solving and named it Multi-SWE-bench. Hu et al. (Hu et al., 2024) proposed FAUN-Eval, benchmark specifically designed to evaluate the fine-grained 3 CodeFuse-CR-Bench issue-solving capabilities of LLMs. It can be used to systematically assess LLMs across three distinct tasks: Question-Answer (QA), fault localization, and code editing. However, either the above approaches or the benchmarks focus on the sub-tasks of CR. The design and construction are comprehensiveness-unaware. To fill this gap, we construct comprehensiveness-aware benchmark and evaluation metrics for CR. We conduct an empirical study on some SOTA LLMs based on this benchmark to evaluate their performance in comprehensive CR tasks. 2.2 LLM for Software Engineering Tasks Recently, LLMs have demonstrated revolutionary performance improvements in almost all softwareengineering-related tasks. Regarding general LLMs, the GPT series (Liang et al., 2024), Claude, Gemini, and others had demonstrated powerful code generation, summarization, and program repair through training on large corpora containing code (Feng et al., 2024; Cao et al., 2024; Wang et al., 2025b; Zhao et al., 2023; Fan et al., 2023). Specifically, systematic comparative analysis was conducted on three advanced LLMs, including ChatGPT (O1), DeepSeek (R1), and Gemini (2.0 Flash thinking), for Python code generation, evaluating their performance in correctness, code quality, and computational efficiency. Each of the three LLMs has its own strengths and limitations. Their findings underscored the inherent trade-offs between efficiency, accuracy, and quality in AI-generated code. Sobo et al. (Sobo et al., 2025) investigated the effectiveness of LLMs in generating code for Human-Robot Interaction applications. They compared the performance among ChatGPT 3.5, Gemini 1.5 Pro, and Claude 3.5 Sonnet. The study highlighted the rapid advancement in LLM capabilities for specialized programming tasks while also identifying persistent challenges in spatial reasoning and adherence to specific constraints. Zhang et al. (Zhang et al., 2024) evaluated the capability of advanced LLMs, including ChatGPT-4 and Claude, in fixing memory corruption vulnerabilities in real-world C/C++ code. They analyzed both the strengths and limitations of LLMs in automated program repair on genuine code. Sun et al. (Sun et al., 2025) conducted the examination of prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and found that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. They also discussed the limitations of LLMs in generating summarization in logic programming languages. All the software-engineering-related tasks mentioned above are highly relevant to CR in technical terms. Therefore, it is possible for LLMs to perform comprehensive CR tasks. In our paper, we select several representative LLMs and evaluate their comprehensive CR capabilities using our constructed benchmark and designed evaluation metrics."
        },
        {
            "title": "3 CodeFuse-CR-Bench Benchmark",
            "content": "Having established the \"comprehensiveness gap\" in current CR research, this section details the design and construction of CodeFuse-CR-Bench. We present the benchmark overview, benchmark construction pipeline, and benchmark characteristics in Section 3.1, Section 3.2, and Section 3.3, respectively. 3.1 Benchmark Overview CodeFuse-CR-Bench comprises 601 Python CR task instances, each carefully curated to reflect realworld development scenarios. Fig. 2 illustrates an overview of typical CR task instance. Each instance is associated with 22 structured fields, as summarized in Table 2. These instances can be systematically categorized into the following four types to support comprehensive CR: Basic information: Instance ID, Owner/Repo, and Language, providing fundamental identification of the task. PR-related information: Encompasses metadata critical to understanding the change intent and context, including Pull No., Title, Created at, Base Commit, Body, Issue Problem Statement, Hint 4 CodeFuse-CR-Bench Figure 2: The Overview of Typical CR Task Instance Table 2: Details of Fields in CR Task Instance Category Basic Information PR-related Information CR-related Information Field Instance ID Owner/Repo Language Pull No. Title Created at Base Commit Body Issue Problem Statement Hint Text Resolved issue No. Commit Patch to Review Head Commit Head Commit Message Problem Domain Difficulty Review Comment Text Diff Hunk Review Path Review Effort Description formatted instance identifier, which is named as repo_owner__repo_name-PR-number@commit_hash_prefix. Repository owner and repository name. Repository programming language. PR No. that this task instance is from. PR title. PR creation date. Commit hash in this repository before the PR is applied. PR description body. Issue(s) title and body. Comments made on the issue(s) before the creation of the commit to review of the solution PR. The number of the issue(s) solved by the PR. The target commit patch in the PR to review. The commit hash of the target commit patch in the PR. The commit message of the target commit patch. The nine problem domains where the issue problem statement belongs (Section 3.2.4). PR task implementation difficulty. The text of the review comment. The diff hunk where the review comment is located The file path where the review comment is located. The review effort for CR task. Repository-Level Context Information Merge Commit Patch Merge Commit The gold patch generated by the PR to resolve the issue. The merged commit hash of the PR. Text, Resolved issue No., Commit Patch to Review, Head Commit, Head Commit Message, Problem Domain, and Difficulty. CR-related information: Captures the review process itself, including Review Comment Text, Diff Hunk, Review Path, and Review Effort, enabling analysis of reviewer behavior and feedback quality. Repository-Level Context Information: Provides broader project-level context necessary for cross-file reasoning and impact analysis, including Merge Commit Patch and Merge Commit. 5 CodeFuse-CR-Bench 3.2 Benchmark Construction Pipeline As depicted in Fig. 3, the construction pipeline of CodeFuse-CR-Bench consists of five steps, namely, (1) Repository Selection; (2) PR Crawling and Attribute-based Filtering; (3) PR Classification; (4) Feature Labeling; and (5) Manual Selection & Annotation. We next briefly elaborate on them. Figure 3: CodeFuse-CR-Bench Construction Pipeline 3.2.1 Repository Selection To make CodeFuse-CR-Bench more representative, we adopt strict approach to selecting CR task instances from various open-source repositories. We focus on Python as it is one of the most popular languages on GitHub, possessing mature and diverse open-source ecosystem. This ensures rich source of projects with standardized and high-quality CR practices, which is essential for our benchmarks validity. Specifically, we first search for the top 1,000 starred Python repositories on GitHub1, Git-based code hosting and collaboration platform. higher star rating indicates that the repository has better popularity. To ensure the quality of the repository and acquire large pool of potential CR data, we further sort the projects by the number of PRs and filter out projects with less than 1,500 PRs, referring to Lis study (Li et al., 2022b). Additionally, we only keep the repositories that were maintained in the last year (from 2024-08-15 to 2025-08-15), i.e., the repositories that had commit or PR records over the past year. The process mentioned above aims to keep only active projects and remove repositories that are forked from other repositories, as the PR number is not inherited. This process yielded 230 Python projects that meet our criteria for activity and maturity. 3.2.2 PR Crawling & Attribute-based Filtering In this step, we aim to crawl high-quality PR data. To ensure the collected PR data contains highquality CR, we filter it based on the following attributes: (1) We only include PRs that have at least 1 closing issue reference; (2) We only collect the PR that is merged into the main branch. Merged status indicates that the code changes associated with the PR were accepted and incorporated into its parent repository. We conduct this process by using GitHub GraphQL API2. The PRs selected based on these attributes undergo rigorous CR, thereby ensuring that the CR data selected under these PRs is of high quality (Zheng et al., 2025). 3.2.3 PR Classification Since each PR instance contains multiple commits, our goal in this step is to identify high-quality commits within each PR and extract their corresponding high-quality CR-related information. Firstly, we design several heuristic rules to evaluate the commits and give an evaluation score. The details of heuristic rules are shown in Table 3. They are categorized into three types: highimpact rules, medium-impact rules, and low-impact rules. higher impact level signifies greater importance and carries more weight in the scoring (high weight: 3.0, medium weight: 2.0, low weight: 1.0). We assign the highest weight to rules indicating direct, actionable review feedback (e.g., has_resolved_review_comments), as these are the strongest indicators of meaningful CR process. Rules related to best practices and commit clarity receive medium weight, while secondary 1https://github.com/ 2https://docs.github.com/en/graphql 6 CodeFuse-CR-Bench Table 3: Heuristic Rules to Evaluate Commits Category Rules Description has_resolved_review_comments Evaluate if the commit has resolved review comments. True score: 1 (Receive meaningful feedback and improvements); False score: 0. High-impact Rules exclude_merge_commit has_referenced_line_changed_comments Evaluate if the commit has review comments where the referenced lines were changed in the merged commit. True score: 1 (Comments addressed real issues that got fixed); False score: 0. Evaluate if this is NOT merge commit (merge commits should have low scores). Commits with \"Merge branch\" or >2 parents score: 0. Otherwise score: 1. Evaluate if this is NOT base/merge commit (score 0 for base and merged commits). Base commits and final merge commits are typically not worthy of review. exclude_base_merge_commit clear_commit_message Medium-impact Rules conventional_commit reasonable_commit_size has_associated_review_comments issue_reference semantic_commit_message focused_file_changes descriptive_commit_content Low-impact rules Evaluate the clarity of the commit message based on message length, structure, and content. Give score from 0 to 1 based on proper message length (10-200 words), structure (multi lines), and meaningful content (include generic words such as fix\" and update\"). Evaluate if the commit follows conventional commit format. Score 1 to commits start with type(scope): description; else 0. Evaluate if the commit has reasonable size. Give score from 0 to 1 based on the number of changed files and lines (Ideal range: 10-200 lines changed and 1-10 files changed). Score 1 if the commit has any associated review comments; else 0. Commits with review comments are likely more significant. Score 1 if the commit message references an issue; else 0. Score of 1 if the commit message is semantic and descriptive; else 0. Checks for action words and descriptive content such as add\", remove\" and so on. Evaluate if the commit changes are focused on related files. Score from 0 to 1, and higher scores mean that commits that change files in the same directory or with similar purpose (i.e., fewer changed files). Evaluate if the commit has more descriptive content beyond just the message. Score from 0 to 1 based on line count, message length, and structured content (marked by lists or sections). indicators like issue references have low weight. Description presents the scoring details of each heuristic rule. By implementing weighted sum scoring, we obtain an overall score for each commit. Based on the overall score, we rank the commits in each PR and extract the target commit with the highest score. After getting the target commit, high-quality CR task instances can be extracted from the target commit. Specifically, we extract and review comments based on whether referenced lines were actually changed in the merged commit, or the review thread was resolved, outdated, or collapsed. Review comments containing the aforementioned attributes will have their corresponding CRs labeled as high-quality CR task instances. 3.2.4 Feature Labeling In this step, we classify the three attributes: problem domain, difficulty, and review effort. Problem domain refers to the category that the PRs associated issue problem statement belongs to. Referring to SWE-Bench (Jimenez et al., 2024), an issue-solving benchmark, there are nine categories of PR problem domains, as summarized in Table 4. Difficulty means how difficult it would be to implement PR. It can be categorized as low, medium, and high. Review effort means the effort required to review code change. It is on scale of 1 to 5. 5 means the most effort. If the repository contains annotations for the above three attributes, we directly utilize the labels from the repository. Otherwise, we employ the LLM-as-a-judge approach (Gu et al., 2025). We leverage the Qwen3-235BA22B model (Yang et al., 2025), constructing prompts from the Title, Body, Commit Patch to Review, and Head Commit Message fields in Table 2 as input to classify the three attributes. At last, we get 40,124 CR task instances. 3.2.5 Manual Selection & Annotation In this step, we manually select high-quality CR task instances that cover nine types of PR problem domains from the 40,124 CR task instances that have passed the filtering process above to serve as the benchmark. Specifically, we invite two of the authors who have more than 5 years of Python 7 CodeFuse-CR-Bench Table 4: Problem Domain Category Problem Domain Description Bug Fixes (BF) New Feature Additions (NFA) Code Refactoring / Architectural Improvement (CA) Documentation Update (DU) Test Suite / CI Enhancements (TC) Performance Optimizations (PO) Security Patches / Vulnerability Fixes (SV) Resolving functional errors, crashes, incorrect outputs Adding new functionality or features to the application Improving code structure, readability, maintainability without changing external behavior Changes related to code comments or external documentation Improving test coverage, test quality, or continuous integration processes Improving application speed, response time, or resource usage efficiency Fixing code defects that could lead to security issues Dependency Updates & Environment Compatibility (DE) Updating third-party library dependencies or ensuring compatibility across different environments Code Style, Linting, Formatting Fixes (CLF) Ensuring code complies with team coding standards and consistency development experience to conduct the selection. Firstly, we find that non-negligible percentage of review comments, while linked to source code lines in the commit patch, are unlikely to result in code revision in the next round. This kind of review comment is regarded as noise comment. For example, if review: (1) that is simple case (e.g., looks good to me, Thanks! or Nice); (2) only requests formatting changes with no impact on code logic (e.g., fix indentation, add spaces); (3) requests adding tests (these do not change the code under review); (4) asks for clarification or explanation without suggesting changes (e.g., please explain, what does this do?); (5) refer previous comments that cannot be identified (e.g., same as before, see above); (6) only requests adding comments or documentation (e.g., add Javadoc, document this method); (7) that is difficult to identify (e.g., At least here it is clear that the equals method of the implementers of TreeNode is important), it is considered as noise and irrelevant review comment. Instances that include these comments are not selected in the final benchmark. In addition, we remove instances whose issue problem statement has images, external hyperlinks, references to specific commit SHAs, and references to other pull requests or issues (Jimenez et al., 2024). The instance with fewer than 40 words in the problem statement is also removed. They are not considered in our scenario. Based on the above rules, we have filtered out 7,086 instances. After that, referring to SWE-Bench (Jimenez et al., 2024) and Multi-SWE-Bench (Zan et al., 2025), we design questionnaire3 about instance quality assessment and ask the two developers to complete the questionnaire for each instance. This questionnaire covers the following areas: (1) Problem statement and patch alignment; (2) Review scope and comment coverage; (3) Defects identified in the patch; (4) Difficulty and review effort; (5) Overall patch quality and risk; (6) Dataset suitability; and (7) Confidence. Thereby, we select the 601 high-quality instances from 70 projects by manual annotation. 3.3 Benchmark Characteristics 3.3.1 Comprehensiveness-aware Benchmark (cid:32) (cid:71)(cid:35) means containing relevant information, CodeFuse-CR-Bench is the pioneering CR benchmark to introduce the comprehensiveness-aware concept. Table 1 lists the difference of CodeFuse-CR-Bench and other representative benchmarks presented in Section 2.1, where means containing no relevant information, and means only containing partial relevant information. We highlight significant oversight in prior research: the comprehensiveness of CR task. Most of the other CR benchmarks lack basic information and repository-level context information. They only have method-level patches and review comments, which significantly deviate from real CR scenarios. Consequently, they make automatic CR approaches miss some context information that contributes to CR. Further, the performance of LLMs in real-world CR remains unknown. Regarding some issuesolving benchmarks, they also lack some PR and CR-related information. CodeFuse-CR-Bench fills this gap by including basic information, repository-level context information, and more completed PR-related and CR-related information. Thereby, they can achieve repository-level CR. It serves as benchmark that better simulates real-world CR, aiming to more accurately reflect the true performance of CR approaches. (cid:35) 3To be presented. 8 CodeFuse-CR-Bench Figure 4: Evaluation Metric Framework 3.3.2 Strict Filter Process We ensure the quality of the selected CR task instances from several aspects. First, we ensure that the selected projects are of high quality because high-quality projects are generally mature and active. They also have excellent maintenance and complete PR and CR process. We select Python projects from the top 1,000 projects based on the number of stars, with an average of 21k stars. Also, we keep projects with more than 1,500 PRs, making the collection focus on those with rich, mature, and standardized CR practices. Retaining only projects maintained within the past year ensures that the analysis focuses on active projects whose CR practices reflect contemporary standards, thereby guaranteeing the researchs cutting-edge relevance and practical applicability. In addition, we ensure that we select merged PRs and PRs with at least 1 closing issue reference. On the one hand, merged PRs indicate that the PR has passed the projects quality gates (including CR and automated testing). On the other hand, PRs associated with closing issues can make sure that every PR has clear, traceable development intent and rich context. Furthermore, we design complete heuristic rules to select representative commit for each PR. These rules can ensure the selected commits: (1) directly reflect collaboration and improvements during CR (e.g., has_resolved_review_comments); (2) possess clear, structured, and traceable contextual information (e.g., conventional_commit, issue_reference); (3) adhere to best practices for atomicity and focus (e.g., reasonable_commit_size, focused_file_changes); (4) exclude noise commits generated by version control operations that do not reflect development intent (e.g., exclude_merge_commit). All these rules can ensure the selected commits have high quality."
        },
        {
            "title": "4 Evaluation Metric Design",
            "content": "A comprehensive benchmark like CodeFuse-CR-Bench requires an equally comprehensive evaluation framework. Existing evaluation metrics on CR only focus on the method-level patch to review, and they only involve evaluation of semantics similarity and consistency, which can not adapt to comprehensive CR. We propose comprehensive evaluation metric to evaluate the quality of comprehensive CR. Fig. 4 shows an overview of the evaluation metric. It consists of two parts, model-based evaluation and rule-based evaluation. 4.1 Model-base Evaluation Model-based evaluation is black-box evaluation designed to leverage the powerful semantic understanding capabilities of LLMs to score and evaluate the quality of CR. It aims to simulate human judgment of CRs semantic quality and usefulness. In our paper, we use two kinds of evaluators in model-based evaluation, namely, the reward model and LLM-as-a-judge (Son et al., 2024). 4.1.1 Reward Model Reward model (Yang et al., 2024) is an AI model that learns human preferences. Its core task is: given an input and one or more model outputs (Output/Candidate), it outputs scalar score (Reward CodeFuse-CR-Bench Score) that predicts the degree of human preference for that output (i.e., the extent to which it is perceived as high quality). In order to train reward model to evaluate comprehensive CR, we need to collect the corresponding training set and design the training strategy. Data Collection. Firstly, we define the positive CR data and the negative CR data. We collect data from GitHub repositories with over 100 stars. To prevent data leakage, we filter out the repositories used to select CR task instances in Fig. 3. We use commits as the collection unit and adopt the has_resolved_review_comments and has_referenced_line_changed_comments rules from Table 3 as labeling criteria. If both rules are True, the commit is classified as positive CR data and labeled as 1. Otherwise, the commit is classified as negative CR data and labeled as 0. At last, we collect 174,661 positive data and 114,458 negative data. Training Strategy. Our reward model is built upon the Qwen3-8B (Yang et al., 2025) pretrained LLM, where we replace the final LM head layer with Multi-Layer Perceptron consisting of two linear layers and ReLU activation function. The first linear layer has dimensions of (hidden_size, hidden_size), while the second layer has dimensions of (hidden_size, 1). The final output represents relevance score indicating how well the CR correlates with the given query and patch. The training loss comprises two components. The first component follows similar approach to the reward model training in InstructGPT (Ouyang et al., 2022), employing contrastive learning-style Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2012). Specifically, for the same query and patch, we sample positive and negative reviews along with their corresponding code context (reviewed file content as default) and diff hunks to achieve this objective. The loss is formulated as: LBPR = log σ(rθ(x, y+) rθ(x, y)). (1) Additionally, considering our application scenario of classifying review relevance, we introduce an auxiliary classification objective using binary cross-entropy loss (Wang et al., 2025a), that is, LBCE = i=1 [li log pi + (1 li) log(1 pi)], (2) where pi = σ(rθ(xi, yi)). Specifically, rθ denotes the reward model parameterized by θ, represents the input consisting of query and patch information, y+ and denote positive and negative reviews respectively, σ is the sigmoid function, li {0, 1} is the binary relevance label for the i-th sample, and pi is the predicted probability of relevance. The total training loss is the weighted combination of these two components: Ltotal = LBPR + λLBCE, (3) where λ controls the relative importance of LBCE. We set it to 1 by default. Implementation Details. For efficient training, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2022a) as our parameter-efficient fine-tuning method with rank of 32 and an alpha value of 16, which significantly reduces the number of trainable parameters while maintaining model performance. The model is trained for 1 epoch with learning rate of 5e 5. To accommodate the comprehensive CR context, we set the maximum sequence length to 24,576 tokens, enabling the model to process lengthy code patches and associated reviews effectively. We utilize Flash Attention 2 (Dao, 2024) for memory-efficient attention computation, with mixed precision training with bfloat16. The training process incorporates 100 warm-up steps for learning rate scheduling to ensure stable convergence. The distributed training is conducted using DeepSpeed ZeRO (Rajbhandari et al., 2020) Stage 2. All experiments are conducted on 32 NVIDIA A100 GPUs with total effective batch size of 64. 4.1.2 LLM-as-a-Judge To evaluate the CR in general perspective, we also adopt LLM-as-a-judge to evaluate the CR quality. Specifically, we design prompt that consists of CR information and evaluation criteria to request 10 CodeFuse-CR-Bench Figure 5: LLM-as-a-Judge Prompt an LLM to provide score for the CR quality. Referring to some research about CR usefulness investigation (Yang et al., 2023; Turzo & Bosu, 2024), we request LLM to provide score of CR based on four perspectives, that is, Functionality, Quality, Style, and Documentation. For each perspective, we request LLM to analyze five dimensions of Correctness, Relevance, Clarity, Consistency and Language. Each dimension will be provided score by LLM and then an average overall score will be calculated at last. The detail prompt is shown in Fig. 5. We use OpenAI o3-2025-04-16 (OpenAI, 2025b) model to achieve LLM-as-a-judge. Thereby, we can obtain score provided by LLM. By combining scores based on the reward model and LLM-as-a-judge, respectively, we get an average score with model-based evaluation. 4.2 Rule-base Evaluation To achieve comprehensive evaluation metric, we also adopt rule-based evaluation. Rule-based evaluation is white-box evaluation, which aims to determine the formal correctness and superficial similarity of CR. We automatically extract structured defect information from the CR reports generated by the LLMs, including file paths, line numbers, and review comments. Then we conduct some heuristic rules which consists of location similarity, semantics similarity and defects match to implement rule-based evaluation. 4.2.1 Location Similarity Location similarity aims to evaluate the matching between the predicted CR location and the CR ground truth. It includes file path matching, line number proximity, and diff hunk match level. Specifically, if the file path where the predicted CR locates is exact match with the ground truth, we give score of 1. Otherwise, we give score of 0. Regarding the line number accuracy, if it is an exact match with the ground truth, we give score of 1. If the line difference diffline is more than 5 lines, we give score of 0.1. Otherwise, we conduct decay function to calculate the line number 11 CodeFuse-CR-Bench accuracy AccLN, which is shown as follows. AccLN = 1 1 0.1 diffline 10 , diffline = 0, , diffline 5, , diffline > 5. (4) In addition, to avoid unduly penalizing essentially correct predictions due to minor line number offsets, we introduce more robust metric: diff hunk similarity. This metric evaluates whether the LLMs predicted CR location and the ground truth location reside within the same code change block. Specifically, We analyze whether the predicted CRs line number resides within the same diff hunk as the ground truth. If it does, we assign score of 1. If the predicted CR is in different diff hunk, we assign score of 0. If the predicted CRs line number is not present in any diff hunk in the patch to review, we collect the five lines above and below the CR line and calculate the overlap ratio between this range and the ground truth diff hunks line number range as the diff hunk similarity score. Thereby, combining with the file path matching and line number accuracy, diff hunk similarity can contribute to more comprehensive and fairer location evaluation. We assign weights of 70%, 15%, and 15% to file path matching, line number accuracy, and diff hunk similarity, respectively, ultimately yielding composite location similarity score. 4.2.2 Semantics Similarity In order to evaluate the semantics performance of review comments, we use BLEU (Papineni et al., 2002), popular evaluation metric that is implemented in neural machine translation and conversation systems. BLEU is now widely used in code-related tasks, such as code comment generation (Guo et al., 2023), document generation (Hu et al., 2022b), review comment generation (Li et al., 2022b) and so on. Its core idea is to compare the n-gram overlap between the text generated by the model and the ground truth. The greater the overlap, the higher the BLEU score. The specific calculation process of BLEU is given as follows: BLEU = BP exp( n=1 ωn log pn), (5) where ωn is the weight of n-gram and pn is the precision of n-gram. Usually, the maximum value of is 4, which is represented by BLEU-4. BP is the brevity penalty factor for generated review comment length, which is shown as follows: BP = exp(1 , ), lr lc lc > lr, lc lr. (6) where lc is the length of generated review comment and lr represents the reference review comment. In this paper, we employ BLEU-4 as the semantics similarity. 4.2.3 Defects Match Since CR task may involve reviewing multiple review comments and locations within commit, we design defect match rule to evaluate performance under multi-location review scenarios. Specifically, for each predicted review comment and location (i.e., predicted defect), we calculate the average of the location similarity and semantics similarity scores between it and each ground-truth review comment and location, namely sub-defect-match scores. We select the highest sub-defectmatch score among them. If this score exceeds the set threshold, we consider the predicted review comment and location to be correctly matched to the defect information. Based on the number of correctly matched defects, we can calculate precision, recall, and F1 scores. Precision = Ncorrectly_matched_defects Npredicted_defects , (7) 12 CodeFuse-CR-Bench (a) Problem Domain Distribution (b) Review Effort & Difficulty Distribution Figure 6: Benchmark Distribution Recall = Ncorrectly_matched_defects Nground_truth_defects , F1 = 2 Precision Recall Precision + Recall . (8) (9) The total score of defect match is calculated by averaging the F1 score and the average of sub-defectmatch scores. After getting the total defect score, we combine and average it with location similarity and semantics similarity score to get rule-based score. At last we get an overall evaluation score based on the average of the model-based score and the rule-based score."
        },
        {
            "title": "5 Experiment Design",
            "content": "In this section, we first provide the detailed distribution of the benchmark, introduce the models, context acquisition strategy, and the prompt used in the experiments, and provide detailed description of the experimental setups. After that, we answer the following four research questions that the experiments aim to address: RQ1: What is the LLMs performance of comprehensive CR in CodeFuse-CR-Bench benchmark? RQ2: What is the LLMs performance of comprehensive CR in different PR types? RQ3: How does different contextual information contribute to CR performance? RQ4: What is the performance of the reward model? 5.1 Benchmark Distribution After implementing the pipeline in Section 3.2, we construct benchmark with 601 Python CR task instances, which were created from 2016-11-06 to 2025-07-07. The problem domain distribution is shown in Fig. 6a. It can be found that bug fixes constitute the problem domain with the highest volume of high-quality CRs. This indicates robust software maintenance practices and thriving CR ecosystem for this category of issues. The review effort and difficulty distribution are shown in Fig. 6b. Most instances are in medium difficulty of PR task implementation. The most common review effort is 3, which also indicates that most CRs are assigned to tasks of moderate difficulty. The 601 instances involve 70 Python projects. Fig. 7 illustrates the project distribution of the benchmark. We present the top 17 projects containing the highest number of instances. These projects account for 55.41% of the total instances. Most of these projects are well-known within the Python community (e.g., pandas, scikit-learn et al.). This indicates that these projects are well-maintained with high-quality CRs. 5.2 Studied LLMs 13 CodeFuse-CR-Bench We consider mainstream LLMsencompassing both opensource and proprietary modelsthat have been widely adopted in recent software-engineering-related studies (Fan et al., 2025; Batole et al., 2025; Wang et al., 2024; Yin et al., 2024). For open-source LLMs, we select DeepSeek-v3.1 (Guo et al., 2025), Kimi-K2-0905-preview (Team, 2025) Qwen3-235BA22B (Yang et al., 2025). For closed-source LLMs, we choose the commonly used commercial models: Claude-Sonnet-420250514 (Anthropic, 2025), Gemini 2.5 Pro (Comanici et al., 2025), GPT-4o (Hurst et al., 2024), GPT-5 (OpenAI, 2025a). They have excellent performance in code-related tasks in previous benchmarks. 5.3 Studied Context Acquisition Strategies Figure 7: Project Distribution As repository-level CR benchmark, CodeFuse-CR-Bench can provide repository-level contextual information as part of the LLM input to help the LLM achieve comprehensive CR. Due to the large size of modern software repositories, it is not feasible to feed the entire repository to the model. It is necessary to retrieve the most relevant information. Referring to SWE-Bench (Jimenez et al., 2024), we adopt two types of context acquisition strategies that might be beneficial for comprehensive CR: Retrieval-based Acquisition and Oracle-based context acquisition. In RQ3, we will validate the performance of these two strategies. Retrieval-based Context Acquisition. Since the problem description and the code implementing the functionality may share overlapping vocabulary, we employ BM25 Stephen2004 to retrieve code files similar to the problem statement within the project. BM25 is ranking function commonly used in information retrieval to measure the relevance between document and query. As kind of sparse retrieval method, BM25 offers fast computation speed and low resource consumption, making it highly suitable for file retrieval scenarios in large-scale software projects. In RQ3, we will separately analyze the impact of using the top-1, top-3 and top-5 relevant code files retrieved via BM25 as context on the effectiveness of LLM-based CR. Oracle-based Context Acquisition. As baseline, we also consider the oracle-based retrieval. Specifically, we retrieve the combination of changed files in diff(base commit, commit to review) diff(base commit, merged commit). The reason lies in the fact that there are some other commits related to the target commit in the PR when extracting the target commit in PR as the CR target. These related commits may serve as supplementary modifications to the target commit. Therefore, we choose the change files from commits in the same PR as the largest context space. 5.4 Experimental Setting We use the evaluation framework proposed in Section 4 to assess the CR generated by LLMs. We set the generation temperature of LLMs to 0.6 (except GPT-5 for 1), top-p to 0.95. The context window is set based on the maximum context window length specified in each LLMs system card. Note that, when employing different context retrieval strategies, once the token length exceeds the context window, we remove the retrieved files outside the context window. When the input remains longer than the context window length after removing all retrieved files, we consider the LLM unable to provide correct CR for that instance and assign an evaluation score of 0. To mitigate issues stemming from the randomness of model generation, the experimental results presented in this paper are obtained by conducting three repeated experiments and averaging the results. The prompt template is shown in Fig. 8, which consists of the issue tag including the problem statement, the code tag including the context, and the patch tag including the patch to review. The generated CR format is also described in the review tag, which involves the functional implementation, code quality, and defect information. 14 CodeFuse-CR-Bench Table 5: LLM Performance Score (%) under CodeFuse-CR-Bench Banchmark LLM Model-based Score Rule-based Score Comprehensive Score DeepSeek-v3.1 Kimi-K2-0905-preview Qwen3-235B-A22B Claude-Sonnet-4-20250514 Gemini 2.5 Pro GPT-4o GPT-5 58.69 62.11 58.10 60.67 63.65 54.57 64.80 28.34 20.81 24.30 33.31 29.47 8.10 18.30 42.51 46.77 40.45 47.46 52.37 35.47 41.96 5.5 RQ1: What is the LLMs performance of comprehensive CR in CodeFuse-CR-Bench benchmark? We conduct experiments under the oraclebased context as the baseline setting to explore the performance of mainstream LLMs in comprehensive CR. Table 5 shows the results, which reveal distinct strengths and weaknesses across models. This suggests that Gemini is currently the most well-rounded performer in generating contextually grounded and technically sound CR feedback. On the model-based evaluation dimension, GPT-5 attains the best score (64.80), followed closely by Gemini 2.5 Pro (63.65). This demonstrates that GPT-5 generates review comments with the highest perceived quality in terms of semantic quality and usefulness, even if its overall integration across dimensions is not optimal. In contrast, Claude-Sonnet-4-20250514 excels in the rule-based evaluation (33.31) and significantly outperforms models like GPT-4o (8.10) and GPT-5 (18.30), indicating its strong adherence to location similarity, semantics similarity, and defect match. This highlights Claudes strength in producing formally correct and precise feedback, albeit sometimes at the cost of broader contextual integration. Figure 8: Prompt Template Notably, several models exhibit significant imbalances. For instance, while GPT-5 performs best in model-based scoring, its rule-based performance lags, resulting in lower comprehensive score (41.96) than Gemini. Similarly, GPT4o shows moderate model-based performance but severely underperforms in rule-based checks, suggesting potential issues with factual grounding or hallucination in CR scenarios. These findings underscore that no single LLM dominates across all dimensions of comprehensive code review. Instead, performance varies significantly depending on the evaluation axis, emphasizing the necessity of our multi-faceted assessment framework. The results also highlight the importance of integrating both model-based and rule-based metrics to avoid overestimating the practical utility of generated CRs. CodeFuse-CR-Bench Table 6: LLMs Comprehensive Score (%) in Different Problem Domains LLM DeepSeek-v3.1 Kimi-K2-0905-preview Qwen3-235B-A22B Claude-Sonnet-4-20250514 Gemini 2.5 Pro GPT-4o GPT-5 BF 43.24 46.34 41.39 49.26 52.14 36.44 43. NFA 41.75 47.09 40.14 46.42 51.93 35.23 40.68 CA 38.73 46.68 30.47 47.04 49.83 32.09 36.73 DU 47.20 47.43 45.31 47.27 58.86 37.66 44. TC 47.98 51.20 46.91 46.66 55.83 36.00 44.25 PO 42.62 49.24 37.11 44.48 51.62 37.94 43.04 SV 38.45 43.36 35.23 39.21 52.11 31.93 36. DE 47.44 47.19 46.91 49.44 53.35 35.33 44.16 CLF 24.54 39.97 22.92 33.94 51.23 18.49 26.17 Answer to RQ1: Gemini 2.5 Pro achieves the highest comprehensive CR performance on CodeFuseCR-Bench, outperforming other LLMs by balancing strong model-based and rule-based scores, while GPT-5 and Claude-Sonnet excel in semantic quality and formal correctness, respectively, highlighting the necessity of multi-dimensional evaluation for realistic CR assessment. 5.6 RQ2: What is the LLMs performance of comprehensive CR in different PR types? In Section 3.2.4, we label the CR task instances into nine problem domains based on the PR types. In this RQ, we conduct fine-grained analysis of their comprehensive CR capabilities within each domain, with results presented in Table 6. It can be found that Gemini 2.5 Pro achieves the highest comprehensive score in all nine problem domains. It outperforms the second-best model in each domain by margins ranging from 5.93% to 28.17%, with particularly strong performance in DU (58.86), TC (55.83), and DE (53.35). This uniform dominance suggests that Gemini 2.5 Pro not only generalizes well across different types of code changes but also effectively leverages domain-specific contextsuch as test requirements, security constraints, or formatting rulesto generate more accurate and actionable CR feedback. The results highlight its robustness and adaptability in handling the heterogeneous nature of modern CR practices. Answer to RQ2: Gemini 2.5 Pro consistently outperforms all other LLMs in comprehensive CR across all nine problem domains, demonstrating its superior generalization and context utilization in diverse CR tasks. 5.7 RQ3: How does different contextual information contribute to CR performance? In Section 5.3, we propose two types of context acquisition strategies that may be helpful for LLMs to achieve comprehensive CR. In this RQ, we evaluate the effectiveness of these strategies under varying context availability. Particularly, regarding retrieval-based context acquisition, we adopt BM25 to retrieve top-1, top-3 and top-5 relevant files as the context, respectively. Results, as shown in Table 7, demonstrate that Gemini 2.5 Pro achieves the highest comprehensive score across all context configurations, including both oracle-based and BM25-based settings. Notably, its performance under the BM25 top-1 context (52.24) is nearly on par with that under the oracle-based setting (52.37), with only marginal gap of 0.13. This indicates that Gemini can achieve near-optimal review quality by leveraging just the single most relevant retrieved file, highlighting its strong capability in contextual relevance filtering and efficient information utilization. Furthermore, Gemini maintains stable performance across different retrieval depths (top-1: 52.24, top-3: 51.45, top-5: 51.63), suggesting robustness to context size and resilience to potential noise in larger retrieval sets. This stability contrasts with other modelssuch as GPT-5 and GPT-4owhose scores fluctuate more significantly with context size, indicating higher sensitivity to irrelevant or 16 CodeFuse-CR-Bench Table 7: LLMs Comprehensive Score (%) in Different Context LLM DeepSeek-v3.1 Kimi-K2-0905-preview Qwen3-235B-A22B Claude-Sonnet-4-20250514 Gemini 2.5 Pro GPT-4o GPTOracle-based Context BM25-based Context Top-1 BM25-based Context Top-3 BM25-based Context Top-5 42.51 46.77 40.45 47.46 52.37 35.47 41.96 48.13 48.15 47.29 51.02 52.24 39.34 44. 43.07 47.78 40.71 48.43 51.45 36.03 49.87 36.42 45.43 33.55 41.91 51.63 31.13 40.17 Table 8: Reward Model Performance Model Type Accuracy (%) Precision (%) Recall (%) F1 (%) Reward Model Reviewed File Reward Model w/o Context Kimi-K2-0711-preview Reviewed File Gemini 2.5 Pro Reviewed File 75.03 74.30 43.68 51.30 79.40 80.02 59.13 63.22 81.92 79.30 42.69 59.60 80.64 79.57 49.58 61. redundant information. These findings also Gemini 2.5 Pro is the most context-efficient and robust model for practical, scalable CR systems where oracle-level context is unavailable. Answer to RQ3: Gemini 2.5 Pro achieves near-oracle performance in comprehensive code review using only the top-1 retrieved context, demonstrating superior context efficiency and robustness across retrieval-based and oracle-based acquisition strategies. 5.8 RQ4: What is the performance of the reward model? In Section 4.1.1, we incorporate reward model as key component of our model-based evaluation framework. To assess its reliability and validity in distinguishing high-quality from low-quality code reviews, we conduct comparison study to evaluate the classification performance of the reward model. The input to the reward model consists of two elements: the query and the patch to review. The query itself is composed of the context (i.e., the full content of the reviewed file, used as default context), review context, and the review location information. We split the dataset described in Section 4.1.1 into 90% training set and 10% test set to evaluate the models classification effectiveness. CR is classified as positive if the reward model outputs score greater than 0.5; otherwise, it is classified as negative. As shown in Table 8. the reward model achieves an accuracy of 75.03% and an F1 score of 80.64%, demonstrating its strong discriminative capability. Notably, even when the context information is removed, the model maintains competitive performance, with accuracy and F1 scores of 74.30% and 79.57%, respectivelysuggesting that the review context, location and patch already provide substantial signal for review quality assessment. For comparison, we evaluate two SOTA LLMsKimi-K2-0711-preview and Gemini 2.5 Proon the same binary classification task. The prompt provided to each LLM mirrors the input format used by the reward model, and the LLMs output is directly mapped to binary label based on whether it expresses approval or disapproval of the review. The results show significantly lower performance: Kimi achieves 43.68% accuracy and 49.58% F1, while Gemini reaches 51.30% accuracy and 61.36% F1. These findings indicate that the fine-tuned reward model substantially outperforms general-purpose LLMs in this specialized classification task, validating its suitability as reliable evaluator within our comprehensive CR assessment framework. In our final evaluation pipeline, the reward model is applied with the reviewed file context to ensure maximal fidelity to real review scenarios. 17 CodeFuse-CR-Bench Answer to RQ4: The reward model we construct demonstrates objective performance in evaluating the quality of CRs, achieving an F1 score exceeding 80% and an accuracy exceeding 75%."
        },
        {
            "title": "6 Threats to Validity",
            "content": "Threats to Internal Validity. For threats to internal validity, the first concern is the selection of prompts. The performance of LLMs is sensitive to prompt design. Using an equivalent but differently phrased prompt may result in significant performance differences. To mitigate this threat, we ultimately chose the one that yielded the best results after trying several formats. However, since we did not cover all available prompt formats, the one we selected may not be the optimal one. Secondly, since the setting of LLM hyperparameters (e.g., temperature), the outputs of LLM may exhibit randomness. To mitigate this issue, we conduct three repeated experiments and average the results. Threats to External Validity. The first threat to external validity is the programming language generalization. CodeFuse-CR-Bench is benchmark based on Python. So it can not be used to evaluate comprehensive CR on other programming languages such as Java or C++. In the future, we plan to address this limitation by continuously expanding CodeFuse-CR-Bench to cover as many programming languages as possible. The second external validity is the type of generalization. CodeFuse-CR-Bench consists of instances including nine types of problem domains. However, there may exist some CRs involving some other types. We will try to expand the number of types in the future. Another threat is the model generalization. Due to limited resources, our experiments are not able to cover all the available LLMs, thus not fully reflecting the performance of all LLMs in actual development scenarios, which may slightly affect the representativeness of our experiments."
        },
        {
            "title": "7 Conclusion",
            "content": "CR is an important component for building stable, maintainable, and high-quality software products. Many CR benchmarks have been proposed to evaluate different kinds of automatic CR approaches. But there is lack of comprehensiveness in existing benchmarks and approaches, which are detached from real CR scenarios. In this paper, we introduce CodeFuse-CR-Bench, comprehensiveness-aware CR benchmark to fill this gap. The design of the benchmark can address the limitations of real-world CR. Correspondingly, we design an evaluation metric to achieve comprehensive CR evaluation. We conduct an empirical study to explore SOTA LLMs performance in the real-world CR scenario. This more realistic CR benchmark encourages creative automatic CR solutions that can have immediate applicability in open-source software development. We believe that CodeFuse-CR-Bench will offer valuable evaluation for the future development of LLM-based automatic CR approaches."
        },
        {
            "title": "References",
            "content": "Anthropic. System card: Claude opus 4 & claude sonnet 4. https://www-cdn.anthropic.com/426 3b940cabb546aa0e3283f35b686f4f3b2ff47.pdf, 2025. Accessed: 2025-05-24. Alberto Bacchelli and Christian Bird. Expectations, outcomes, and challenges of modern code review. In 2013 35th International Conference on Software Engineering (ICSE), pp. 712721, 2013. doi: 10.1109/ICSE.2013.6606617. Fraol Batole, David OBrien, Tien N. Nguyen, Robert Dyer, and Hridesh Rajan. An llm-based agent-oriented approach for automated code design issue localization. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pp. 13201332, 2025. doi: 10.1109/ICSE5534 7.2025.00100. Jialun Cao, Zhiyong Chen, Jiarong Wu, Shing-Chi Cheung, and Chang Xu. Javabench: benchmark of object-oriented code generation for evaluating large language models. In Proceedings of the CodeFuse-CR-Bench 39th IEEE/ACM International Conference on Automated Software Engineering, ASE 24, pp. 870882, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400712487. doi: 10.1145/3691620.3695470. URL https://doi.org/10.1145/3691620.3695470. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Gang Fan, Xiaoheng Xie, Xunjin Zheng, Yinan Liang, and Peng Di. Static code analysis in the ai era: An in-depth exploration of the concept, function, and potential of intelligent code analysis agents, 2023. URL https://arxiv.org/abs/2310.08837. Lishui Fan, Jiakun Liu, Zhongxin Liu, David Lo, Xin Xia, and Shanping Li. Exploring the capabilities of llms for code-change-related tasks. ACM Trans. Softw. Eng. Methodol., 34(6), July 2025. ISSN 1049-331X. doi: 10.1145/3709358. URL https://doi.org/10.1145/3709358. Jia Feng, Jiachen Liu, Cuiyun Gao, Chun Yong Chong, Chaozheng Wang, Shan Gao, and Xin Xia. Complexcodeeval: benchmark for evaluating large code models on more complex code. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE 24, pp. 18951906, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400712487. doi: 10.1145/3691620.3695552. URL https://doi.org/10.1145/3691620.3695 552. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. URL https://arxiv.org/abs/2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hanyang Guo, Xiangping Chen, Yuan Huang, Yanlin Wang, Xi Ding, Zibin Zheng, Xiaocong Zhou, and Hong-Ning Dai. Snippet comment generation based on code context expansion. ACM Trans. Softw. Eng. Methodol., 33(1), November 2023. ISSN 1049-331X. doi: 10.1145/3611664. URL https://doi.org/10.1145/3611664. Qi Guo, Junming Cao, Xiaofei Xie, Shangqing Liu, Xiaohong Li, Bihuan Chen, and Xin Peng. Exploring the potential of chatgpt in automated code refinement: An empirical study. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400702174. doi: 10.1145/3597503.3623306. URL https://doi.org/10.1145/3597503.3623306. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=nZeVKeeFYf9. Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. real-world benchmark for evaluating fine-grained issue solving capabilities of large language models, 2024. URL https://arxiv.org/abs/2411.18019. Xing Hu, Qiuyuan Chen, Haoye Wang, Xin Xia, David Lo, and Thomas Zimmermann. Correlating automated and human evaluation of code documentation generation quality. ACM Trans. Softw. Eng. Methodol., 31(4), July 2022b. ISSN 1049-331X. doi: 10.1145/3502853. URL https://doi.org/ 10.1145/3502853. 19 CodeFuse-CR-Bench Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yanjie Jiang, Hui Liu, Tianyi Chen, Fu Fan, Chunhao Dong, Kui Liu, and Lu Zhang. Deep assessment of code review generation approaches: Beyond lexical similarity, 2025. URL https://arxiv.org/ abs/2501.05176. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum ?id=VTF8yNQM66. Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang, and Chun Zuo. Auger: automatically generating review comments with pre-training models. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, pp. 10091021, New York, NY, USA, 2022a. Association for Computing Machinery. ISBN 9781450394130. doi: 10.1145/3540250.3549099. URL https: //doi.org/10.1145/3540250.3549099. Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundaresan. Automating code review activities by large-scale pre-training. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, pp. 10351047, New York, NY, USA, 2022b. Association for Computing Machinery. ISBN 9781450394130. doi: 10.1145/3540250.3549081. URL https://doi.org/10.1145/3540250.3549081. Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, and Thomas Zimmermann. Can gpt-4 replicate empirical software engineering research? Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145/3660767. URL https://doi.org/10.1145/3660767. OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025a. Accessed: 2025-08-13. OpenAI. Openai o3 and o4-mini system card. https://cdn.openai.com/pdf/2221c875-02dc-478 9-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf, 2025b. Accessed: 2025-04-16. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (ACL), pp. 311318. ACL, 2002. URL http://www.aclweb.org/antholo gy/P02-1040.pdf. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618, 2012. Andrei Sobo, Awes Mubarak, Almas Baimagambetov, and Nikolaos Polatidis. Evaluating llms for code generation in hri: comparative study of chatgpt, gemini, and claude. Applied Artificial Intelligence, 39(1):2439610, 2025. doi: 10.1080/08839514.2024.2439610. URL https://doi.org/10 .1080/08839514.2024.2439610. 20 CodeFuse-CR-Bench Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. Llm-as-a-judge & reward model: What they can and cannot do, 2024. URL https://arxiv.org/abs/2409.11239. Weisong Sun, Yun Miao, Yuekang Li, Hongyu Zhang, Chunrong Fang, Yi Liu, Gelei Deng, Yang Liu, and Zhenyu Chen. Source code summarization in the era of large language models. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pp. 18821894, 2025. doi: 10.1109/ICSE55347.2025.00034. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2501.12948. Patanamon Thongtanunam, Chanathip Pornprasit, and Chakkrit Tantithamthavorn. Autotransform: automated code transformation to support modern code review process. In Proceedings of the 44th International Conference on Software Engineering, ICSE 22, pp. 237248, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392211. doi: 10.1145/3510003.3510067. URL https://doi.org/10.1145/3510003.3510067. Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. Towards automating code review activities. In Proceedings of the 43rd International Conference on Software Engineering, ICSE 21, pp. 163174. IEEE Press, 2021. ISBN 9781450390859. doi: 10.1109/ICSE4390 2.2021.00027. URL https://doi.org/10.1109/ICSE43902.2021.00027. Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota. Using pre-trained models to boost code review automation. In Proceedings of the 44th International Conference on Software Engineering, ICSE 22, pp. 22912302, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392211. doi: 10.1145/3510003.3510621. URL https://doi.org/10.1145/3510003.3510621. Asif Kamal Turzo and Amiangshu Bosu. What makes code review useful to opendev developers? an empirical investigation. Empirical Software Engineering, 29(1):6, 2024. Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu, and Yi Wang. Rocks coding, not development: human-centric, experimental evaluation of llm-supported se tasks. Proc. ACM Softw. Eng., 1 (FSE), July 2024. doi: 10.1145/3643758. URL https://doi.org/10.1145/3643758. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation, 2025a. URL https://arxiv.org/abs/2503.05236. You Wang, Michael Pradel, and Zhongxin Liu. Are \"solved issues\" in swe-bench really solved correctly? an empirical study. ArXiv, abs/2503.15223, 2025b. URL https://api.semanticschola r.org/CorpusID:277113006. Adam X. Yang, Maxime Robeyns, Thomas Coste, Zhengyan Shi, Jun Wang, Haitham Bou Ammar, and Laurence Aitchison. Bayesian reward models for LLM alignment. In ICML 2024 Workshop on Structured Probabilistic Inference & Generative Modeling, 2024. URL https://openreview.net/for um?id=30jztCg6Yc. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Lanxin Yang, Jinwei Xu, Yifan Zhang, He Zhang, and Alberto Bacchelli. Evacrc: Evaluating code review comments. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, pp. 275287, New York, NY, 21 CodeFuse-CR-Bench USA, 2023. Association for Computing Machinery. ISBN 9798400703270. doi: 10.1145/3611643.36 16245. URL https://doi.org/10.1145/3611643.3616245. Xin Yin, Chao Ni, and Shaohua Wang. Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transactions on Software Engineering, 50(11):30713087, 2024. doi: 10.1109/TSE. 2024.3470333. Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. URL https://arxiv.org/abs/2504.02605. Lan Zhang, Qingtian Zou, Anoop Singhal, Xiaoyan Sun, and Peng Liu. Evaluating large language models for real-world vulnerability repair in c/c++ code. In Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics, IWSPA 24, pp. 4958, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400705564. doi: 10.1145/3643651.3659892. URL https://doi.org/10.1145/3643651.3659892. Zelin Zhao, Zhaogui Xu, Jialong Zhu, Peng Di, Yuan Yao, and Xiaoxing Ma. The right prompts for the job: Repair code-review defects with large language model, 2023. URL https://arxiv.org/ abs/2312.17485. Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu Zhang, and Zibin Zheng. Humanevo: An evolution-aware benchmark for more realistic evaluation of repository-level code generation. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pp. 13721384, 2025. doi: 10.1109/ICSE55347.2025.00228. Xin Zhou, Kisub Kim, Bowen Xu, DongGyun Han, Junda He, and David Lo. Generation-based code review automation: How far are we? In 2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC), pp. 215226, 2023. doi: 10.1109/ICPC58990.2023.00036."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Hong Kong Baptist University",
        "UNSW Sydney"
    ]
}