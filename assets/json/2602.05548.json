{
    "paper_title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "authors": [
        "Zhiqi Yu",
        "Zhangquan Chen",
        "Mengting Liu",
        "Heye Zhang",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 2 8 4 5 5 0 . 2 0 6 2 : r Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Zhiqi Yu1*, Zhangquan Chen2*, Mengting Liu3, Heye Zhang3 and Liangqiong Qu 1 1University of Hong Kong, 2Tsinghua University, 3Sun Yat-sen University Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration; (ii) learning efficiency is maximized by curriculum-like transitionprioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs. Keywords: Large Language Models, Reinforcement Learning, LLM Reasoning Date: February 5, 2026 Code Repository: https://github.com/HKU-HealthAI/A-GRAE Contact: zhiqiyu777@connect.hku.hk 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) Ouyang et al. (2022), Achiam et al. (2023), Shao et al. (2024), Wen et al. (2025), Guo et al. (2025) has become cornerstone for activating Foundation Models capacity to address complex reasoning tasks through Chain-of-Thought (CoT) generation. Among various algorithms, Group Relative Policy Optimization (GRPO) has emerged as the standard implementation in advanced systems such as DeepSeek-R1 Guo et al. (2025) and OpenAI-o1 Jaech et al. (2024). The core innovation of GRPO lies in Group Relative Advantage Estimation (GRAE), which computes relative advantage scores within sampled groups to obviate the need for value model. Despite its empirical success, recent studies have identified two fundamental limitations of GRPO. The first is capability boundary shrinkage: prominent critique posits that GRPO primarily leads to an exploration-exploitation trade-off within existing policy constraints rather than effectively expanding the decision boundary Yue et al. (2025), Bamba et al. (2025), He et al. (2025), Ma et al. (2025), Huang et al. (2025). This claim is supported by empirical evidence showing that GRPOs Pass@k can even fall below that of the base model at large k. The second limitation is inadequate focus on problem difficulty: GRPOs Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 1: The two-fold implicit advantage symmetry problem of GRAE in GRPO. At the group level, the advantage weights for correct trajectories equal those of incorrect trajectories. This symmetry leads to the logits of low-probability correct paths unchanged within the behavior space, thereby hindering the models exploration. At the sample level, samples of medium-difficulty exhibit the largest sum of absolute advantage values, which leads to insufficient training on harder data. reward mechanism is difficulty-agnostic, treating all tasks uniformly without accounting for their inherent complexity or the models current capacity Zhang and Zuo (2025), Zhou et al. (2025), Zhang et al. (2025b), Jeddi et al. (2025). This lack of granularity often leads to either catastrophic overfitting on simpler tasks or insufficient learning on more challenging ones. In this work, we begin by formalizing reinforcement learning as an advantage-led reweighting variant of SFT, and reveal that these two deficiencies stem from previously overlooked implicit advantage symmetry inherent in GRAE. As illustrated in fig. 1, this symmetry manifests at two levels: At the group level, the advantage weights for correct and incorrect trajectories are strictly equivalent. Through logits analysis in the behavior space, we formally demonstrate that this symmetry restricts the exploration of unsampled, potentially optimal paths. At the sample level, by quantifying the absolute sum of advantages across samples, we reveal that the algorithm implicitly prioritizes medium-difficulty instances. Consequently, the optimization remains agnostic to the non-stationary demands of training dynamics, failing to adapt its focus as the model evolves. Building on this analysis, we perform controlled interventions that deliberately break GRAEs intrinsic advantage symmetry at both the group and sample levels to examine its causal effect on learning dynamics. Our experiments reveal that the symmetric property is sub-optimal and yield two key design principles: (i) asymmetrically suppressing the weights of correct trajectories fosters essential exploration; (ii) learning efficiency is substantially boosted by curriculum-like progression, which prioritizes simpler samples initially before escalating to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which refines the original GRAE strategy by explicitly instantiating these two principles within the GRPO framework: it introduces asymmetric exploration to push the policy beyond its current solution set, and curriculum-like progression learning schedule to align the optimization focus with the models evolving capability. To fully validate A-GRAE, we conduct comprehensive experiments across seven diverse benchmarksspanning both natural language reasoning and vision-language reasoning tasksusing various commonly used LLMs and VLMs. Experimental results demonstrate that A-GRAE consistently enhances the reasoning performance 2 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation of GRPO and its representative variants DAPO Yu et al. (2025) and Dr.GRPO Liu et al. (2025b) across all settings, with significant improvements in key metrics (e.g., accuracy, pass@k). In summary, our contributions are as follows: We identify and define the implicit advantage symmetry property in GRAE. Through systematic investigation, we demonstrate that this symmetry is sub-optimal, prompting fundamental rethinking of advantage function design in RLVR. We reveal that asymmetrically suppressing the weight of correct trajectories at the group level can enhance reasoning performance. Furthermore, we provide theoretical analysis of the associated risk of learning collapse, attributing it to blindly intensified exploration. We uncover dynamic shift in optimal learning efficiency at the sample level: contrary to GRAEs static focus, prioritizing simple samples in early stages and harder ones later yields superior performance. We propose Asymmetric GRAE (A-GRAE), novel framework that dynamically encourages trajectory exploration and adapts sample difficulty bias. Extensive evaluations across seven benchmarks validate that it can consistently improve GRPO and its variants. 2. Preliminary i=1 and œÄŒ∏old . Each question-response pair (q, oi) is then assigned scalar reward ri Notation. In this work, we frame foundation model (either an LLM or MLLM), parameterized by Œ∏, as denote the current and prior policies, respectively. For batch processing, we policy model: œÄŒ∏ generates set of sample question from the dataset ùí¨. For each question q, the prior policy œÄŒ∏old candidate responses {oi} via rule-based verifier, where {1, 2, . . . , G} indexes responses per question. By default, we employ binary is correct for question q, and ri = 0 otherwise. For each question q, we accuracy reward: ri = 1 if response oi further compute group-relative advantage value Ai for each of its candidate responses oi Group Relative Policy Optimization (GRPO). GRPO simplifies the training pipeline of PPO by removing the critic modeltypically matching the size of the policy modeland instead estimates baselines via group-level reward scores. Specifically, for each query q, GRPO samples set of responses {o1, o2, . . . , oG} from the prior policy œÄŒ∏old by maximizing the following objective function: and optimizes the target policy œÄŒ∏ . JGRPO(œÄŒ∏) = qùí¨,{oi}G i=1 œÄŒ∏old (q) 1 i=1 1 oi oi t= [min(œÅi,t Ai,t, clip (œÅi,t, 1 œµ) Ai,t) Œ≤DKL], where œÅi,t = œÄŒ∏(oi,tq,oi,<t) œÄŒ∏old (oi,tq,oi,<t) , œµ and Œ≤ are hyperparameters, the KL term is defined as KL = œÄre (oi,tq, oi,<t) œÄŒ∏(oi,tq, oi,<t) log œÄre (oi,tq, oi,<t) œÄŒ∏(oi,tq, oi,<t) 1, and the advantage Ai is computed using group of rewards {r1, r2, ..., rG} with GRAE: Ai = ri mean({r1, r2, ..., rG}) std({r1, r2, ..., rG}) . (1) (2) (3) 3 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation 3. Implicit Advantage Symmetry in GRPO First, we demonstrate that GRPOs policy gradient optimization can be formulated as reweighting variant of Supervised Fine-Tuning (SFT). Specifically, the gradient update can be formulated as follows (proof in section B.1): Œ∏ùí•GRPO(Œ∏) = qùí¨,{oi}G i=1 œÄŒ∏old (q)"
        },
        {
            "title": "1\nG",
            "content": "G i=1 1 oi oi t=1 œÅi,t Ai,t (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) weight Œ∏ log œÄŒ∏(oi,t q, oi,<t) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) SFT . (4) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) Group Level (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) Sample Level is relatively stable with the clip operation, so the dominant part of reweighting is Ai,t In eq. (4), œÅi,t . In general, the advantage of GRPO is shared across the entire sequence, meaning that all tokens within single response correspond to the same advantage value, i.e.,Ai,1 = Ai,2 = = Ai,t = Ai . We then examine this reweighting mechanism from the group level and the sample level. Advantage Symmetry in Group Level: In RLVR, trajectory is considered correct provided that the final answer extracted from the response aligns with the ground truth. Given query, we partition the sampled within-group responses into correct (positive) and incorrect (negative) trajectories. Crucially, we observe that the weights of policy updates attributed to correct trajectories is strictly equivalent to that of incorrect ones (proof in section B.2). This property can be expressed as: iùí¢pos Ai = iùí¢neg Ai (5) where ùí¢pos ùí¢neg = and ùí¢pos and ùí¢neg ùí¢neg = ùí¢ (the sampled response set). represent the sets of positive and negative trajectories, satisfying the conditions ùí¢pos Theorem 1 (The Logits Update in Behavior Space) Assume that the set of all possible behaviors is ‚Ñ¨ = {bi} consisting of sampled set ùí¢ and unsampled set ùí∞. Given the intragroup advantage sum = negligible importance sampling bias, the produced probability updates of path bi can be expressed as: Œ∏ = Œ∑ [I(bi ùí¢)Ai i=1, oiùí¢ Ai and CœÄbi ], (6) where Œ∑ denotes the learning rate and œÄbi is the models current sampling probability in behavior space for path bi. The proof can be found in section B.3. In the standard GRPO setting, the advantage normalization enforces zero-sum property, i.e., = 0. The probability updates can be divided into the following cases. Case A. For the sampled positive responses bi ùí¢pos hbi : = Œ∑ Apos, (7) where Apos > 0. The logits of sampled correct trajectories receive positive update, strictly increasing their probability. This confirms that GRPO effectively exploits known correct solutions. Case B. For the sampled negative responses bi ùí¢neg : hbi = Œ∑ Aneg, (8) 4 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation where Aneg < 0. The logits are directly penalized by the negative advantage. This suppresses the generation of known errors. Case C. For the unsampled response bi ùí∞: hbi = Œ∑ (0 0 œÄbi ) = 0. (9) It can be observed that GRPO yields strictly zero gradient for any unsampled trajectory. Consequently, even if low probability correct trajectory exists in the behavior space, its logit remains static unless it is stochastically sampled. This mathematical property proves that GRPO lacks an intrinsic active exploration mechanism for unsampled correct trajectories, leading to local optima entrapment. Advantage Symmetry in Sample Level: At the sample level, the overall learning contribution can be captured by the sum of absolute advantages, which represents the total magnitude of one query. To evaluate the relative contributions across queries of varying complexities, we introduce the sample success rate, denoted as for group of size G, as proxy for task difficulty; specifically, higher indicates lower level of difficulty. By leveraging this metric, we can formally quantify the relationship between sample difficulty and the corresponding update magnitude. Theorem 2 (Update Magnitude with respect to Sample Difficulty). Consider group of trajectories for one query (sample) with binary rewards {ri} i=1, the sum of absolute advantages over the group under Group Relative Advantage Estimation (GRAE) can be derived as: Ai = 2G p(1 p). iG (10) where = and the proof can be found in section B.4. iG ri/G denotes the empirical success probability of the corresponding sample within group G, Theorem 2 reveals an intrinsic difficulty bias of GRAE, where samples of intermediate difficulty (p = 0.5) dominate policy updates, irrespective of the training stage. Furthermore, as shown in fig. 6, due to the symmetry of the term p(1 p), simple samples (e.g., = 0.75) and hard samples (e.g., = 0.25) exhibiting the same deviation from = 0.5 are assigned identical importance weights. However, considering the dynamics of model development, there is an inherent shift in the sample distribution: the ratio of relatively simple instances increases over time as model evolves, whereas the frequency of difficult samples steadily declines. This distributional shift predisposes the model to overfit on trivial data while leaving it insufficiently trained on challenging scenarios. Consequently, the intrinsic advantage symmetry at the sample level fails to satisfy the non-stationary demands of the evolving training process. 4. Deconstructing the Implicit Advantage Symmetry of GRAE 4.1. Experimental Setup To systematically investigate the impact of advantage symmetry on reasoning performance, we design two sets of ablation experiments as shown in table 1. The detailed setup is as follows: Control Experiment (Breaking Intra-Group Symmetry): This experiment examines whether the model benefits more from suppressing the contribution of correct trajectories. To achieve this, we introduce scaling coefficient Œ≤ = 10 to disrupt the zero-sum equilibrium ( Ai = 0). Denoting the original positive advantage as Apos, we design two variants: Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation 1. Positive-Dominant Group: We scale up positive advantages (A 2. Negative-Dominant Group: We scale down positive advantages (A pos = Œ≤ Apos). pos = Apos/Œ≤). The original GRPO serves as the control group, representing the symmetric equilibrium. Control Experiment II (Breaking Sample-Level Symmetry): This experiment investigates the appropriate difficulty focus during training process. We modify the advantage magnitude based on the sampling success rate p. Let Ai be the original advantage of GRPO. We define two curriculum variants: 1. Hard-Focused Group: We shift the learning focus toward harder queries by rescaling the advantage with 2. Easy-Focused Group: Conversely, we shift the focus to simple queries by scaling with the factor 1/ 1 the factor 1/ (A = Œ≥ Ai/ p). (A = Œ≥ Ai/ 1 p). Here, Œ≥ = 0.5 is normalization constant to ensure that the theoretical maximum value remains consistent with the control group (standard GRPO). Note that no extra rescaling is conducted when the success rate is 0 or 1, as the GRPO advantage is zero in these cases. Table 1: Summary of Controlled Experiments. We design two sets of ablation studies to investigate the impact of breaking symmetry at the group level and sample level. Œ± = 10 and Œ≥ = 0.5 are scaling constants. (a) Experiment I: Breaking Group-Level Symmetry. Variant Formulation (A ) A GRPO (Control Group) Positive-Dominant Negative-Dominant pos = Apos pos = Œ≤ Apos pos = Apos/Œ≤ = 0 > 0 < 0 (b) Experiment II: Breaking Sample-Level Symmetry. Variant Formulation (A ) GRPO (Control Group) Hard-Focused Easy-Focused = Ai = Œ≥ Ai/ = Œ≥ Ai/ 1 i 2G p(1 p) 1 Models and Training Setup: To ensure the model-agnostic generalizability of our investigations across different architectures, we conduct experiments using two distinct LLM backbones: the math-specialized Qwen2.5-Math-7B Yang et al. (2024) and the general-purpose Llama-3.2-3B-Instruct Dubey et al. (2024). We conduct training on the MATH dataset Hendrycks et al. (2021) under the verl framework Sheng et al. (2025). In terms of hyperparameters, we set the batch size to 1,024, with = 8 rollouts generated for 6. We evaluate each query. The policy is updated with mini-batch size of 256 and learning rate of 1 10 the models on three widely recognized mathematical reasoning benchmarks: the test sets of MATH, AMC23, and the most challenging AIME 2025. Following prior works Hochlehnert et al. (2025), Chen (2021), Chen et al. (2025), Zhu et al. (2025), we set our primary evaluation metric as Pass@k with an unbiased estimator. Experimental Details can be found in section C.2. 6 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 2: Experimental results on breaking group-level symmetry using Qwen2.5-Math-7B. We amplify (PositiveDominant) or suppress (Negative-Dominant) the advantages of correct trajectories to compare their performance with that of GRPO and the base model.The performance is evaluated using Pass@k (k = {1, 2, 4, 8, 16, 32, 64, 128, 256}). 4.2. Rethinking Symmetry in Group Level The results of Control Experiment are shown in fig. 2. The following is an analysis of each group. GRPO enhances efficiency without expanding reasoning boundaries. It is evident that GRPO significantly improves Pass@1 accuracy over the base model across all three datasets, demonstrating its effectiveness in enhancing sampling efficiency and reasoning capabilities. However, the performance gains are gradually eroded as increases. Notably, on the AMC23 and MATH datasets, GRPOs performance at Pass@256 falls below that of the base model. This suggests that while GRPO improves the sampling probability of correct paths, it fails to discover novel solutions that lie outside the base models original sampling support. In other words, GRPO does not fundamentally expand the intrinsic reasoning boundaries of the base model. This observation aligns with findings in recent literature Yue et al. (2025), Yao et al. (2025). Over-emphasizing correct paths triggers entropy collapse. In contrast to GRPO, amplifying the weight of positive trajectories fails to yield performance improvements. In particular, on the AIME2025 and MATH datasets, the Positive-Dominant group significantly underperforms other methods at larger sampling budgets (k > 16). To investigate the underlying cause, we found that the entropy of the Positive-Dominant group on the test set exhibits the most precipitous decline as illustrated in fig. 3. This phenomenon indicates that overemphasis on correct trajectories reduces sampling diversity by excessively sharpening the output distribution, leading to entropy collapse and thus impairing the models reasoning capability. Suppressing the correct path improves performance but risks instability. Remarkably, suppressing positive trajectories proves more effective than GRPO. On one hand, the Negative-Dominant group consistently outperforms GRPO in Pass@k metrics across all three datasets, with the advantage becoming increasingly pronounced as grows. On the other hand, even at high sampling budget (k = 256), this approach maintains parity with the base model and achieves significant gains on the most challenging AIME2025 dataset. This suggests that the strategy effectively mitigates the potential capability boundary shrinkage observed in GRPO. Correlating this with fig. 3, we attribute this gains to the continuous increase in entropy throughout training which indicates model explorationa trend diametrically opposed to the other groups. However, in practice, we observe that persistent entropy growth can lead to training instability in later stages, which is manifested by sudden increase in fully unsolved questions (demonstrated in section D.3). This instability stems from the fact that in the Negative-Dominant group, overconfident incorrect trajectories may displace the correct ones during training process. We discuss this situation in section B.5 based on Theorem 1. Consequently, dynamic adjustment mechanism may be required as training progresses to balance diversity and stability. 7 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 3: Entropy dynamics across the three groups in Experiment on the training set. Notably, the NegativeDominant group exhibits monotonic increase in entropy except at the very beginning, while the other groups show the opposite behavior. Takeaway 1: As discussed in section 4.2, while suppressing positive advantage incentivizes the model to explore unsampled correct trajectories to improve the reasoning ability, it simultaneously introduces the risk of training collapse in the later stages of reinforcement learning . Consequently, dynamic adjustment mechanism may be required as training progresses. 4.3. Rethinking Symmetry in Sample Level Figure 4: Experimental results on breaking sample-level symmetry using Qwen2.5-Math-7B. We rescaling the advantages to shift the learning focus toward harder queries (Hard-Focused) or easier queries (Easy-Focused) to compare their performance with that of GRPO and the base model.The performance is evaluated using Pass@k (k = {1, 2, 4, 8, 16, 32, 64, 128, 256}). The results for Experiment II are presented in fig. 4, from which the following observations can be derived. Difficulty-based reweighting offers no universal advantage and depends on the test difficulty. As shown in fig. 4, none of the methods yielded uniform advantage across all datasets. More precisely, their relative efficacy varies by benchmark: hard-focused methods achieve peak results on the challenging AIME2025, whereas easy-focused approaches marginally outperform others on simpler datasets such as AMC23 and MATH at Pass@1. These findings indicate that difficulty reweighting should be calibrated to test difficulty, challenging the prevailing trend of focusing solely on hard samples throughout training. Zhang and Zuo (2025), Pikus et al. (2025), Guan et al. (2025). 8 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 5: The within-batch count of correct sampling responses on the training set. Easy-Focused exhibits the most rapid initial convergence during the early stages of training, whereas Hard-Focused maintains sustained upward trajectory in the later phases, eventually achieving superior performance. Prioritizing simple samples in early stages and hard samples later yields better learning efficiency gains. As discussed above, direct evaluation of the trained models fails to identify universally optimal difficulty reweighting strategy. This necessitates deeper investigation into their respective training dynamics to potentially integrate their complementary strengths. During training, the within-batch count of correct samples serves as direct indicator of learning gains, the results of which are visualized in Figure 5. Notably, the Easy-Focused strategy exhibits the most rapid initial acceleration. This suggests that during the early phases of training, prioritizing simpler tasks promotes the learning of basic formatting rules and core reasoning patterns. In contrast, the Hard-Focused strategy emerges as the superior performer in the later stages. This shift suggests that once model capability reaches relative saturation, transition toward difficult samples becomes essential to further elevate the performance ceiling and mitigate the risk of overfitting on simpler data. Takeaway 2: Our findings in section 4.3 suggest that static difficulty reweighting is insufficient, as optimal sample utility is phase-dependent. While easy samples facilitate more effective learning in the early stage, strategic transition toward hard samples is useful as performance begins to plateau. 4.4. Connecting RL Methods via Advantage Symmetry While recent advancements such as Dr.GRPO Liu et al. (2025b) and DAPO Yu et al. (2025) have refined the GRAE paradigm, they overlook the intrinsic property of advantage symmetrywhich still holds under their methodologiesand consequently fail to address the persistent issues of boundary shrinkage and difficulty adaptation. Additionally, the concept of advantage symmetry provides an explanatory framework for the efficacy of existing methods. For instance, some approaches encourage exploration through negative learning Zhu et al. (2025), Yao et al. (2025), Nan et al. (2025), Li et al. (2024), Wang et al. (2024b), which implicitly disrupts group-level symmetry by suppressing or removing the advantages of correct trajectories. Others enhance model performance by emphasizing high-entropy tokens Cheng et al. (2025), Hao et al. (2025), Zhang et al. (2025c), Deng et al. (2025b) or hard samples Park et al. (2025), Zhang and Zuo (2025), Shen et al. (2025), Zhang et al. (2025a), Bae et al. (2025), essentially breaking sample-level symmetry. However, these methods only implicitly address symmetry at one of these levels, and fail to simultaneously Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation improve accuracy and diversity. In summary, advantage symmetry is critical yet underrecognized property. Integrating it into reinforcement learning frameworks will inspire rethinking of advantage design strategies1. 5. Asymmetric GRAE 5.1. Method Our prior analysis reveals that the advantage symmetry inherent in GRPO undermines model exploration and difficulty adaptation. To address these limitations, we propose the Asymmetric Group Relative Advantage Estimation (A-GRAE) framework to dynamically modulate exploration incentives and sample-difficulty focus. To implement this, metric is required to quantify training state; accordingly, we introduce the batch-wise mean reward as proxy indicator: where denotes the total number of trajectories in the batch, œâs denotes the mean reward of the current step, where higher value implies stronger model proficiency. Then we introduce dynamic attention shift at the sample level, transitioning from easy to hard samples as training progresses: œâs = i=1 ri , (11) Ai = œâs ri mean({r1, r2, ..., rG}) std({r1, r2, ..., rG}) + 1 œâs 2 ri std({r1, r2, ..., rG}) mean({r1, r2, ..., rG}) 1 , (12) where denotes the sampling success rate for given query. Note that rescaling is omitted when {0, 1}, as the standard advantage equals 0. As the model evolves with increasing sampling success rate, the weight assigned to the hard-focused component (first term) progressively increases, while the corresponding weight for the easy-focused component (second term) diminishes. This mechanism facilitates adaptive trade-offs, dynamically shifting training focus to the hard questions as the models proficiency improves. At the group level, we propose an attenuation suppression strategy for correct-response advantages, which encourages adequate exploration in the early training stage while preserving stability in the later phase: = { min(1, œâs Œ± ), Ai Ai, if > 0 if 0 (13) Here, Œ± 1 is scaling parameter. Once the refined advantages are computed, they can be seamlessly incorporated into the GRPO objective function, as defined in eq. (1), or other GRPO variants for policy optimization. 5.2. Experimental Setup We validate our method across seven benchmarks, including text-only mathematical tasks (AIME2025, AMC23, MATH Hendrycks et al. (2021)) with Qwen2.5-Math-7B and DeepSeek-R1-7B (section D.2) and multimodal tasks in mathematics (Geo3k Lu et al. (2021), MathVision Wang et al. (2024a), MathVerse Zhang et al. (2024)) and medicine (HuatuoGPT-Vision Chen et al. (2024)) with Qwen2.5-VL-3B-Instruct. Our 1Related work is discussed in detail in section A. 10 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Table 2: Pass@k results on MATH, AIME 2025 and AMC23 with Qwen2.5-Math-7B. Bold and underlined numbers denote the best and second-best results for each k. Method Base Model GRPO GRPO-LEAD W-REINFORCE GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE Base Model GRPO GRPO-LEAD W-REINFORCE GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE Base Model GRPO GRPO-LEAD W-REINFORCE GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE 2 4 8 Pass@k 16 32 128 256 63.4 76.5 77.8 76.6 78.3 75.0 76.9 77.2 78.6 6.1 10.3 11.0 10.6 11.3 12.0 13.3 11.0 11.8 40.6 60.2 62.3 62.0 62.6 62.0 63.3 60.7 62.8 74.8 82.3 83.0 82.8 85.0 79.8 82.6 82.5 86. 9.9 14.3 14.8 15.3 15.6 16.1 18.4 14.8 16.2 55.3 66.7 68.0 70.0 70.0 70.3 72.5 69.8 71.6 MATH 83.2 86.1 86.5 87.1 89.2 85.0 86.5 87.4 89.8 88.6 88.8 89.2 90.2 91.0 88.4 89.2 89.6 90.6 AIME 2025 19.3 23.1 23.4 24.7 24.7 25.2 26.3 24.3 25. 14.4 18.7 19.2 20.0 19.8 21.3 23.0 19.3 19.8 AMC23 68.6 72.1 73.3 77.0 77.5 77.2 80.5 75.6 78.2 78.6 76.4 77.8 83.1 83.7 83.1 86.7 82.6 84.0 91.2 90.3 90.5 92.4 92.5 89.8 90.6 91.0 92.8 24.4 27.5 27.8 29.7 28.6 29.4 30.0 28.8 29. 85.0 80.6 81.5 87.8 88.2 87.8 90.2 87.8 89.6 93.4 92.6 92.3 94.1 94.6 92.0 92.8 92.8 95.0 29.1 31.8 32.0 34.6 34.1 33.2 35.1 33.0 34.8 89.4 84.8 85.0 91.8 92.0 91.4 92.9 90.9 92.3 94.1 93.5 92.8 95.3 95.0 92.8 93.6 93.6 95.4 33.4 36.1 36.5 40.5 39.2 38.5 41.1 37.1 37. 93.4 88.3 88.2 95.2 95.1 94.0 95.0 93.2 95.2 95.0 93.9 93.6 96.1 95.5 93.4 94.2 94.3 96.0 39.2 40.8 41.4 47.8 47.8 45.4 48.7 41.2 48.0 97.3 90.8 90.3 97.1 96.8 96.1 97.0 94.6 95.9 96.3 95.0 95.0 96.7 96.5 94.3 95.3 95.0 96.9 46.7 46.7 47.3 56.7 56.7 53.3 60.0 46.7 56. 100.0 92.5 92.3 97.5 97.5 97.5 100.0 95.0 100.0 experiments cover GRPO variants (GRPO, DAPO, Dr.GRPO) and state-of-the-art methods W-REINFORCE Zhu et al. (2025) (addressing GRPOs insufficient exploration) and GRPO-LEAD Zhang and Zuo (2025) (tackling its difficulty adaptation). We report Pass@k on the text-only datasets and Pass@1 on the multi-modal datasets since most of them are multiple choice questions. For details please refer to section C.2. 5.3. Main Results Consistent improvements in Pass@1 and Pass@k. As illustrated in table 2, our proposed method yields substantial performance gains when integrated with various GRPO variants, which proves the effectiveness of our proposed method. Notably, in comparison with W-REINFORCE and GRPO-LEAD, our method realizes consistent gains in both accuracy (Pass@1) and diversity (Pass@k). Such results indicate that our method effectively mitigates the issue of capability boundary shrinkage inherent to traditional reinforcement learning paradigms, while enhancing reasoning accuracy at the same time. Universal applicability to multi-modal domains. To evaluate the versatility of A-GRAE, we extended our Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Table 3: Performance comparison on multi-modal benchmarks with Qwen2.5-VL-3B-Instruct. Bold and underlined numbers indicate the best and second-best results respectively. ID Domain OOD Domain Method Geo3K MathVision Mathverse Task A: General Mathematical Reasoning base model GRPO GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE 27.8 43.5 45.7 44.7 45.9 44.9 46.8 20.8 23.4 24.0 23.8 24.3 24.2 25.6 31.6 35.2 36.8 35.9 37.5 36.5 38.4 Task B: Medical Imaging Reasoning MRI300 CT300 Xray300 base model GRPO GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE 35.6 87.2 88.2 84.3 87.0 87.8 89.0 42.5 71.7 73.1 71.6 72.3 72.4 73. 42.0 63.2 71.3 63.1 71.6 69.5 72.0 empirical analysis to the multimodal domain, with the results summarized in table 3. The result reveals that A-GRAE not only delivers significant improvements in the in-distribution (ID) domain but also yields substantial gains in out-of-distribution (OOD) scenarios. These findings provide strong evidence that our approach effectively enhances sampling efficiency while simultaneously preserving the models generalization capabilities. In summary, the efficacy of A-GRAE across diverse domains and tasks underscores its universal applicability as robust framework. 5.4. Further Analysis Ablation studies. To validate the contribution of each component in A-GRAE, we perform series of ablation studies using and the results are demonstrated in table 5. Firstly, sample-level asymmetry primarily bolsters Pass@1 performance, consistently attaining second-best results across all three benchmarks. Conversely, the group-level asymmetric mechanism exerts more pronounced impact on enhancing Pass@k metrics, suggesting that this module effectively fosters generative diversity. Ultimately, the full framework yields the most significant gains across all evaluation metrics, demonstrating that the constituent modules of our methodology are mutually complementary. We also compare each module of our method with the control groups in section D.5 to verify their effectiveness. Training dynamics. To capture the evolution of training dynamics, we monitor the entropy and greedy decoding accuracy on the MATH dataset throughout the training process, as shown in fig. 10. On the training set, the entropy of GRPO exhibits continuous decline throughout the training process. In contrast, our proposed method shows rapid initial drop followed by sustained plateau, suggesting that it effectively mitigates the issue of entropy collapse. On the test set, the entropy of A-GRAE follows trajectory of initial increase followed by gradual decrease, reflecting learning paradigm that balances exploration and exploitation. Finally, the greedy decoding accuracy of our approach significantly surpasses that of GRPO in the latter stages of training, further validating the efficacy of our method in facilitating sustained learning. Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation 6. Conclusion In this paper, we rethink the mechanics of Group Relative Advantage Estimation (GRAE) within the GRPO framework. Our theoretical and empirical analyzes reveal previously overlooked advantage symmetry\" in standard GRPO, and we demonstrate that this symmetry restricts exploration and fails to adapt to the difficulty focus during the learning process. To overcome these limitations, we propose A-GRAE, novel mechanism that dynamically modulates exploration incentives and prioritizes samples based on their evolving utility. Extensive evaluations across seven benchmarks demonstrate our superiority."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. Udbhav Bamba, Minghao Fang, Yifan Yu, Haizhong Zheng, and Fan Lai. Xrpo: Pushing the limits of grpo with targeted exploration and exploitation. arXiv preprint arXiv:2510.06672, 2025. Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Xingyu Dang, Christina Baek, Zico Kolter, and Aditi Raghunathan. Assessing diversity collapse in reasoning. In Scaling Self-Improving Foundation Models without Human Supervision, 2025. Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065, 2025a. Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, and Ji-Rong Wen. Decomposing the entropy-performance exchange: The missing keys to unlocking effective reinforcement learning. arXiv preprint arXiv:2508.02260, 2025b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 13 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Wei Guan, Jun Lan, Jian Cao, Hao Tan, Huijia Zhu, and Weiqiang Wang. Emit: Enhancing mllms for industrial anomaly detection via difficulty-aware grpo. arXiv preprint arXiv:2507.21619, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, and Jiawei Chen. Rethinking entropy interventions in rlvr: An entropy change perspective. arXiv preprint arXiv:2510.10150, 2025. Andre Wang He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2555925571, 2025. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2217022183, 2024. Fanding Huang, Guanbo Huang, Xiao Fan, Yi He, Xiao Liang, Xiao Chen, Qinting Jiang, Faisal Nadeem Khan, Jingyan Jiang, and Zhi Wang. Beyond the exploration-exploitation trade-off: hidden state approach for llm reasoning in rlvr. arXiv preprint arXiv:2509.23808, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ahmadreza Jeddi, Hakki Can Karaimer, Hue Nguyen, Zhongling Wang, Ke Zhao, Javad Rajabi, Ran Zhang, Raghav Goyal, Babak Taati, and Radek Grzeszczuk. Puzzle curriculum grpo for vision-centric reasoning. arXiv preprint arXiv:2512.14944, 2025. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 14 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, and Jingzhao Zhang. Questa: Expanding reasoning capacity in llms via question augmentation. arXiv preprint arXiv:2507.13266, 2025a. Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, and Tianlu Wang. Jointly reinforcing diversity and quality in language model generations. arXiv preprint arXiv:2509.02534, 2025b. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, and Kan Li. Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1859118599, 2024. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Miao Lu, Han Zhong, Tong Zhang, and Jose Blanchet. Distributionally robust reinforcement learning with interactive data collection: Fundamental hardness and near-optimal algorithms. Advances in Neural Information Processing Systems, 37:1252812580, 2024. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Yanhao Li, et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, et al. Ngrpo: Negative-enhanced group relative policy optimization. arXiv preprint arXiv:2509.18851, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 337347. Springer, 2025. Jinyoung Park, Jeehye Na, Jinyoung Kim, and Hyunwoo Kim. Deepvideo-r1: Video reinforcement fine-tuning via difficulty-aware regressive grpo. arXiv preprint arXiv:2506.07464, 2025. 15 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Benjamin Pikus, Pratyush Ranjan Tiwari, and Burton Ye. Hard examples are all you need: Maximizing grpo post-training under annotation budgets. arXiv preprint arXiv:2508.14094, 2025. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. arXiv preprint arXiv:2402.11651, 2024b. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025. Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. Xinhao Yao, Lu Yu, Xiaolin Hu, Fengwei Teng, Qing Cui, Jun Zhou, and Yong Liu. The debate on rlvr reasoning capability boundary: Shrinkage, expansion, or both? two-stage dynamic view. arXiv preprint arXiv:2510.04028, 2025. Edward Yeo, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 16 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, and Lu Qianchun. Learning like humans: Advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided self-reformulation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 66306644, 2025a. Jixiao Zhang and Chunsheng Zuo. GRPO-LEAD: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 56425665. Association for Computational Linguistics, November 2025. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.287. URL https://aclanthology.org/ 2025.emnlp-main.287/. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, and Rujun Guo. Clpo: Curriculum learning meets policy optimization for llm reasoning. arXiv preprint arXiv:2509.25004, 2025b. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Edge-grpo: Entropy-driven grpo with guided error correction for advantage diversity. arXiv preprint arXiv:2507.21848, 2025c. Zhanke Zhou, Xiangyu Lu, Chentao Cao, Brando Miranda, Tongliang Liu, Bo Han, and Sanmi Koyejo. Codapo: Confidence and difficulty-adaptive policy optimization for post-training language models. In 2nd AI for Math Workshop@ ICML 2025, 2025. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in LLM reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 17 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation A. Related Works"
        },
        {
            "title": "Appendix",
            "content": "Advantage Estimations in Reinforcement Learning. In Proximal Policy Optimization (PPO), advantage estimation relies on the Generalized Advantage Estimation (GAE Schulman et al. (2015)) framework. Nevertheless, the critic model required by GAE introduces non-trivial computational overhead. To mitigate this cost, GRPO Shao et al. (2024) and REINFORCE++ Hu (2025) replace the critic with lightweight baselines (batch-averaged rewards) or group-relative rewards for advantage computation. Building on this, several methods have pointed out and addressed the limitations of GRPOs group relative advantage estimation. Dr.GRPO Liu et al. (2025b) enhances token efficiency by removing length and standard deviation normalization terms, while DAPO Yu et al. (2025) balances the policy gradient loss at the token level to mitigate the insufficient gradient contribution of long sequences in long CoT scenarios. However, the advantage symmetry phenomenon inherent in GRPOs advantage estimation remains under-explored, with the existing literature lacking an in-depth analysis of this property. Exploration-Exploitation Dilemma in Reinforcement learning with Verifiable Rewards (RLVR). Whether RLVR can genuinely expand the reasoning capabilities of LLMs has sparked extensive debate recently. Some studies Yue et al. (2025), Dang et al. (2025), He et al. (2025), Ma et al. (2025), Gandhi et al. (2025) argue that RLVR primarily enhances sampling efficiency at the cost of reduced diversity and exploration capacity, narrowing the models capability boundaryevidenced by its failure to improve Pass@k (e.g., pass@256). However, other approaches have effectively mitigated this limitation through prolonged training Liu et al. (2025a), negative learning Zhu et al. (2025), or curriculum learning Deng et al. (2025a), Li et al. (2025a), demonstrating that reinforcement learning can indeed yield novel reasoning strategies. In this work, we also observe that GRPO exhibits performance drop at pass@256 compared to the base model, but our improved advantage estimation can partially offset this loss or even enhance the base models pass@k. We hope this work can offer new insights into this debate from the perspective of advantage estimation in GRPO. B. Supplementary Proof B.1. Proof of Equation 4 Proof. We begin by reviewing the objective function of GRPO under the notation used in our formulation. ùí•GRPO(Œ∏) = qùí¨,{oi}G i=1 œÄŒ∏old (q)[ 1 i=1 1 oi oi t= { min (œÅi,t(Œ∏)Ai,t, clip(œÅi,t(Œ∏), 1 œµ, 1 + œµ)Ai,t)} Œ≤DKL(œÄŒ∏œÄref)], (14) where œÅi,t(Œ∏) = œÄŒ∏(oi,tq,oi,<t) œÄŒ∏old (oi,tq,oi,<t) , and Œ≤ is the coefficient for the KL divergence term. To better understand the models learning dynamics under the verifiable reward setting and to derive the specific reweighting formulation, we omit the regularization components (e.g., KL term & clipping operation) and focus on the core policy optimization objective: ùí•GRPO(Œ∏) = qùí¨,{oi}G i=1 œÄŒ∏old (q) [ 1 i=1 1 oi oi t= œÅi,t(Œ∏)Ai,t] , (15) 18 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Taking the gradient with respect to Œ∏: Œ∏ùí•GRPO(Œ∏) = qùí¨,{oi}G i=1 œÄŒ∏old (q) [ = qùí¨,{oi}G i=1 œÄŒ∏old (q) [ = qùí¨,{oi}G i=1 œÄŒ∏old (q) [ = qùí¨,{oi}G i=1 œÄŒ∏old (q)"
        },
        {
            "title": "1\nG",
            "content": "G i=1 i=1 i=1 i=1 1 oi 1 oi 1 oi 1 oi oi t=1 oi t=1 oi t=1 oi t= Ai,t Œ∏ œÅi,t(Œ∏)] Ai,t Œ∏œÄŒ∏(oi,t q, oi,<t) œÄŒ∏old(oi,t q, oi,<t) ] Ai,t œÄŒ∏(oi,t q, oi,<t) œÄŒ∏old(oi,t q, oi,<t) Œ∏ log œÄŒ∏(oi,t q, oi,<t)] œÅi,t(Œ∏)Ai,t (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) weight Œ∏ log œÄŒ∏(oi,t q, oi,<t) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) SFT . (16) We complete the proof of eq. (4). As the weight equal to 1, GRPO degrade to the standard SFT function, thus it can be formulated as reweighting variant of Supervised Fine-Tuning. B.2. Proof of Equation Given group of sampled responses indexed by {1, . . . , G}, partitioned into set of correct trajectories ùí¢pos and incorrect trajectories ùí¢neg . Let the advantage Ai be computed via GRAE as: Ai = ri ¬µ œÉ , where ¬µ = 1 j=1 rj, œÉ = 1 j=1 (rj ¬µ)2. Then, the sum of absolute advantages for correct trajectories equals that of incorrect trajectories: iùí¢pos Ai = iùí¢neg Ai. Proof. First, we examine the sum of the standardized advantages over the entire group. By definition: i=1 Ai = ri ¬µ œÉ i=1 = 1 œÉ ( = 1 œÉ ( = 1 œÉ ( i=1 i=1 i=1 ¬µ) i=1 1 ri ri j=1 rj) ri i=1 ri) = 0. (17) (18) (19) 19 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation This confirms the zero-sum property of the standardized advantages. Now, we decompose the summation into the two disjoint sets ùí¢pos (assuming the group contains mixed results, i.e., ùí¢pos ùí¢neg = ùí¢): and ùí¢neg Rearranging the terms, we obtain: i=1 Ai = iùí¢pos Ai + iùí¢neg Ai = 0. iùí¢pos Ai = iùí¢neg Ai. (20) (21) In the context of verifiable rewards (e.g., binary outcomes {0, 1}), correct trajectories receive higher . Conversely, incorrect trajectories receive rewards than the group mean (ri > ¬µ), implying Ai > 0 for ùí¢pos lower rewards (ri < ¬µ), implying Ai < 0 for ùí¢neg . We have: Thus, the magnitude of policy updates attributed to correct trajectories is strictly equivalent to that of incorrect ones. iùí¢pos Ai = iùí¢neg Ai. (22) B.3. Proof of Equation 6 To ensure rigorous derivation, we first establish the definitions and notations within the context of the behavior space. 1. Behavior Space and Set Definitions Let ‚Ñ¨ = {b1, b2, . . . , bN} denote the Behavior Space, representing the universal set of all possible trajectories (responses) the model can generate, with size N. Let ùí¢ ‚Ñ¨ be the Sampled Group for the current query, consisting of the trajectories actually generated and evaluated in the current step. Let its size be G. Let ùí∞ = ‚Ñ¨ ùí¢ be the Unsampled Set, representing the vast majority of potential trajectories not explored in this iteration. Note: In the context of LLM reasoning, the space of possible sequences is combinatorially large (N ) while the sample size is small (e.g., = 8), implying ùí∞ ùí¢. 2. Model Outputs in Behavior Space Let RN be the vector of Logits corresponding to the trajectories in ‚Ñ¨. Specifically, hbi represents the unnormalized log-probability (score) of trajectory bi . œÄ(h) denotes the probability distribution over the behavior space, modeled via the Softmax function: = œÄbi ehbi j=1 hbj . 3. Optimization Objective The objective function J(h) maximizes the expected return over the sampled group: J(h) = bkùí¢ ÀÜAbk ln œÄbk (h). (23) (24) 20 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Derivation of the Gradient Field. We seek to compute the gradient of with respect to the logit hbi arbitrary trajectory bi . Applying the chain rule: of an hbi = bkùí¢ ÀÜAbk ln œÄbk hbi . (25) Recall the standard derivative of the log-softmax function: ln œÄbk hbi (Œ¥ik = 1 if bi = bk , else 0). Substituting this into Eq. (25): = Œ¥ik œÄbi , where Œ¥ik is the Kronecker delta We decompose the summation into two distinct components: hbi = bkùí¢ ÀÜAbk (Œ¥ik œÄbi ). hbi = ( bkùí¢ ÀÜAbk Œ¥ik) (œÄbi ÀÜAbk ) . bkùí¢ (26) (27) is sampled The first term simplifies using the indicator function I(bi ùí¢), which isolates the advantage if bi trajectory. The summation in the second term corresponds exactly to our defined constant C. Thus, the gradient is expressed as: hbi = I(bi ùí¢) ÀÜAbi CœÄbi . (28) The Logits Dynamics. Finally, applying gradient ascent update rule with learning rate Œ∑ > 0, the update dynamics for the logit of any trajectory bi are governed by: hbi = Œ∑ [I(bi ùí¢) ÀÜAbi CœÄbi ] . (29) B.4. Proof of Theorem Theorem 2 (Update Magnitude with respect to Sample Difficulty). Consider group of trajectories for one query (sample) with binary rewards {ri} i=1, the sum of absolute advantages over the group under Group Relative Advantage Estimation (GRAE) can be derived as: Ai = 2G p(1 p). iG (30) iG ri/G denotes the empirical success probability of the corresponding sample within group G, where = and the proof can be found in section B.4. Proof. Given group of trajectories with binary rewards ri {0, 1}, let = success probability. The group mean ¬µ and standard deviation œÉ are: iG ri/G denote the (31) ¬µ = p, œÉ = p(1 p) The advantage in GRAE is defined as Ai = (ri ¬µ)/œÉ. Summing the absolute advantages over the group G, 21 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation we partition the sum into successful (ri = 1) and failed (ri = 0) subsets: iG Ai = ri=1 (cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187) = (G p) (cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187) 1 p(1 p) 1 p(1 p) + ri=0 (cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187) p(1 p) (cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187) + (G (1 p)) p(1 p) (32) = 2Gp(1 p) p(1 p) = 2G p(1 p) This completes the proof, and visualization of the relationship between update magnitude and sample difficulty is provided below, illustrating the derived curve: Figure 6: The Visualization of Update Magnitude with respect to Sample Difficulty. The update magnitude reaches its peak at = 0.5, simple samples (e.g., = 0.75) and hard samples (e.g., = 0.25) exhibiting the same deviation from = 0.5 are assigned identical importance weights. B.5. The Logits Update of Negative-Dominant Group in Behavior Space Theorem 1 (The Logits Update in Behavior Space) Assume that the set of all possible behaviors is ‚Ñ¨ = {bi} i=1, consisting of sampled set ùí¢ and unsampled set ùí∞.The produced probability updates of path bi can be expressed as: bi = Œ∑ [I(bi ùí¢)Ai CœÄbi ], (33) where Œ∑ denotes the learning rate and œÄbi is the models current sampling probability in behavior space for path bi. Suppressing the advantages of correct paths leads to < 0, and we subsequently examine the dynamics of probability updates across the following cases. Case A. For the sampled positive responses bi ùí¢pos : hbi = Œ∑(Apos CœÄbi ), (34) 22 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation where Apos > 0, (CœÄbi ) > 0 thus hbi increase. > 0. Hence, the sampling probability of correct response will strictly Case B. For the sampled negative responses bi ùí¢neg : hbi = Œ∑(Aneg CœÄbi ), (35) where Aneg < 0 but (CœÄbi ) > 0. Compared to GRPO(C = 0) which directly penalizes the incorrect responses, the logits of overconfident negative responses can even be increased. As reinforcement learning sharpens the output distribution He et al. (2025), Li et al. (2025b), Lu et al. (2024), Yue et al. (2025), overconfident erroneous responses occasionally displace correct ones in later stages, leading to sharp surge in fully unsolved samples. Case C. For the unsampled response bi ùí∞: It can be observed that the output probabilities of all unsampled responses exhibit an increase, whereas they remain constant under GRPO. This phenomenon demonstrates that the Negative-Dominant group provides an exploration incentive, potentially facilitating the discovery of correct yet previously unsampled trajectories. hbi = Œ∑(CœÄbi ) (36) C. Implementation Details C.1. Dataset Introduction Mathematical Reasoning We first conduct experiments on three widely recognized math benchmarks designed to evaluate the logical reasoning capabilities of LLM: MATH Hendrycks et al. (2021): comprehensive dataset containing challenging competition-level mathematics problems across various subjects. AMC23: Problems derived from the 2023 American Mathematics Competitions, representing highschool-level competitive reasoning. AIME 2025: The latest problems from the American Invitational Mathematics Examination, designed to test advanced problem-solving skills and long-chain reasoning. Multi-modal Mathematical Reasoning To validate the universality of our method in Vision-Language Models (VLMs), we extend our evaluation to three multi-modal mathematical reasoning datasets: Geo3K Lu et al. (2021): dataset focused on geometry problem-solving that requires synchronized understanding of both visual diagrams and textual descriptions. MathVerse Zhang et al. (2024): comprehensive benchmark designed to evaluate visual mathematical reasoning across wide array of subjects and visual patterns. MathVision Wang et al. (2024a): large-scale challenge consisting of diverse mathematical problems sourced from real-world competitions with complex visual contexts. Medical Imaging Reasoning Finally, to verify whether the proposed method enhances the models ability to perceive and reason over fine-grained image details, we utilize the HuatuoGPT-Vision Chen et al. (2024) 23 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation dataset. This benchmark is curated and combined dataset from several publicly available medical VQA benchmarks, including VQA-RAD Lau et al. (2018), SLAKE Liu et al. (2021), PathVQA He et al. (2020), OmniMedVQA Hu et al. (2024), and PMC-VQA Zhang et al. (2023). It is specifically designed to assess medical domain expertise and visual grounding capabilities in complex clinical scenarios. Specifically, the training set comprises 600 MRI image-question pairs, while the evaluation is conducted on diverse test suite consisting of 300 MRI, 300 CT, and 300 X-ray pairs following Pan et al. (2025). C.2. Hyperparameter Settings For all reinforcement learning experiments, responses were generated with temperature of 1.0 and maximum completion length of 2048 tokens. The learning rate is 1e-6. During evaluation, we used generation temperature of 0.6, top-p value of 0.95, and set the maximum new tokens to 4096. To ensure stable results, we perform 16 runs for AIME2025, and AMC23, and 4 runs for other benchmarks, reporting the average performance across the respective runs. Note that we did not use dynamic sampling strategy in DAPO to ensure fair overhead. Our experiments are conducted over single node with 8 NVIDIA H200 GPUs. For experiments trained on the MATH dataset, we used the following system prompt to guide the models reasoning process: Please reason step by step, and put your final answer within boxed{}. The training batch size is set to 1024 with total 20 epochs. For each prompt, we generated 8 responses within the group. The reward was based on binary accuracy, where correct final answer yielded reward of 1 and an incorrect one yielded 0. For experiments trained on the Geo3K dataset, we used the following system prompt to guide the models reasoning process: This is multiple-choice question. You FIRST think about the reasoning process as an internal monologue and then provide the final choice from the given options. The reasoning process MUST BE enclosed within tags.\" The training batch size is set to 512 with total 20 epochs. The final answer MUST BE put in boxed{}. For each prompt, we generated 5 responses within the group. The reward was based on 0.1*acc-reward + 0.9 * format-reward. As for the medical dataset, the training batch size is set to 32 with total 10 epochs, and the other settings are the same as those on Geo3K. Our proposed A-GRAE method is highly accessible, requiring only single hyperparameter: the scaling parameter Œ± in Eq. (13). In our experiments, Œ± is set to 1 for the Math dataset and 0.5 for the Geo3k and Medical datasets. This adjustment is motivated by our observation that multi-modal datasets exhibit higher propensity for training collapse, thus necessitating smaller scaling factor to maintain stability. Following prior works Hochlehnert et al. (2025), Chen (2021), Chen et al. (2025), Zhu et al. (2025), we set our primary evaluation metric as Pass@k. Pass@k estimates the probability of generating correct solution within attempts. Unlike greedy decoding, Pass@k provides more reliable assessment of models potential and capability boundaries Hochlehnert et al. (2025), Chen (2021), Chen et al. (2025), Zhu et al. (2025). We employ the standard unbiased estimator, which generates responses (n k) for each question q, counts the correct responses c, and computes the metric as follows: Pass@k = qùí¨ [1 (nc ) (n k) ] (37) Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation D. Supplementary Experimental Results D.1. Experimental Results of Control Experiment using Llama-3.2-3B-Instruct It is observed that the empirical patterns on Llama-3.2-3B-Instruct remain largely consistent with those detailed in the main text. Specifically, suppressing the positive advantages at the group level significantly enhances reasoning performance, particularly in terms of the Pass@k metric. Conversely, no universally optimal difficulty re-weighting strategy exists at the sample level. It is noteworthy, however, that when reaches 256, all evaluated methods underperform relative to the base model. This suggests that the backbone model itself often determines the efficacy of reinforcement learning, finding consistent with recent literature Yeo et al. (2025), Zhu et al. (2025), Wang et al. (2025). Nevertheless, the Negative-Dominant group remains the most effective approach for preserving the fundamental capabilities of the base model. Figure 7: Experimental results on breaking group-level symmetry using Llama-3.2-3B-Instruct. The performance is evaluated using Pass@k (k = {1, 2, 4, 8, 16, 32, 64, 128, 256}) to provide comprehensive analysis of the models capabilities. Figure 8: Experimental results on breaking sample-level symmetry using Llama-3.2-3B-Instruct. The performance is evaluated using Pass@k (k = {1, 2, 4, 8, 16, 32, 64, 128, 256}) to provide comprehensive analysis of the models capabilities. D.2. Supplementary Experimental Results using DeepSeek-R1-7B D.3. Training Collapse of Negative-Dominant Group To further investigate the reliability of the Negative-Dominant configuration, we conducted multiple training runs under identical hyperparameter settings. As illustrated in fig. 9, we observe striking stochastic instability within this specific group. While one trial remains relatively stable throughout the training 25 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Table 4: Pass@k results on MATH, AIME 2025 and AMC23 with DeepSeek-R1-7B. Bold and underlined numbers denote the best and second-best results for each k. Method Base Model GRPO GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE Base Model GRPO GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE Base Model GRPO GRPO + A-GRAE DAPO DAPO + A-GRAE Dr.GRPO Dr.GRPO + A-GRAE 2 4 8 16 32 128 256 Pass@k 84.3 92.4 92.9 92.6 93.0 92.8 93.3 22.7 28.5 29.6 29.2 30.3 29.0 29.8 64.2 82.8 84.2 83.3 84.6 83.0 84. 89.6 95.3 95.7 95.3 95.6 95.6 96.0 26.5 35.0 36.0 35.5 36.8 35.2 36.3 73.0 89.3 90.6 89.8 91.6 89.6 91.3 MATH 92.8 96.5 96.9 96.4 97.0 96.8 97.3 94.8 97.0 97.5 97.2 97.5 97.3 97. AIME 2025 33.2 29.7 47.7 41.4 48.9 42.6 48.5 42.2 49.3 43.2 48.2 41.8 49.8 43.3 AMC23 80.1 92.3 93.5 92.6 94.2 92.3 93.6 85.5 94.0 95.4 94.3 96.3 93.8 96.0 96.0 97.3 97.9 97.5 98.1 97.7 98.3 37.8 53.1 54.0 53.3 54.6 53.0 55. 89.8 95.2 96.6 95.8 97.0 95.6 97.2 96.8 97.7 98.3 97.8 98.2 98.0 98.6 43.2 57.6 58.3 57.7 58.6 57.5 58.8 92.8 96.1 97.7 96.5 97.8 96.3 98.0 97.4 98.0 98.6 98.0 98.4 98.3 98.8 48.0 62.1 63.0 62.5 63.3 62.3 63. 95.2 97.2 98.0 97.6 98.2 97.2 98.5 97.8 98.3 99.0 98.2 98.7 98.5 99.0 51.1 67.2 68.3 67.6 68.4 67.3 68.8 97.4 98.8 99.3 99.2 99.5 98.2 99.3 98.0 98.6 99.3 98.6 99.3 98.8 99.4 53.3 73.3 74.3 73.6 75.0 74.0 74. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 process, the other undergoes catastrophic training collapse after approximately step 78. This divergence highlights that the Negative-Dominant approach is highly sensitive to initial conditions or sampling noise. In the \"collapse\" instance, the number of fully unsolved questions exhibits high-amplitude oscillations and persistent upward trend, eventually peaking at over 110. This suggests that without proper regularization, the Negative-Dominant objective can easily lead the model into divergent optimization path, where erroneous trajectories reinforce themselves and eventually overwhelm the models reasoning capabilities. Such unpredictable behavior underscores the necessity of balancing negative trajectory weights to ensure consistent convergence. D.4. Experimental Results in Further Analysis D.5. Comparative Analysis with Control Groups Effectiveness of Attenuation Suppression Strategy (ASS). As shown in table 6, our proposed ASS method is comparable with the Negative-Dominant group in all Pass@k metrics. Furthermore, we conducted 10 independent training runs from scratch for each group to evaluate their empirical stability. Our results Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 9: Training collapse of Negative-Dominant group during training with Qwen2.5-Math-7B. Left Part: The unsolved question count suddenly increase in the later stage. Right Part: The collapse leads to the significant performance decrease. Table 5: Ablation studies on MATH, AIME 2025 and AMC23 with Qwen2.5-Math-7B. Method 2 4 8 16 32 128 256 Pass@k Base Model GRPO GRPO + A-GRAE (sample level) GRPO + A-GRAE (group level) GRPO + A-GRAE (full) Base Model GRPO GRPO + A-GRAE (sample level) GRPO + A-GRAE (group level) GRPO + A-GRAE (full) Base Model GRPO GRPO + A-GRAE (sample level) GRPO + A-GRAE (group level) GRPO + A-GRAE (full) 63.4 76.5 77.8 77.6 78.3 6.1 10.3 10.9 11.0 11.3 40.6 59.2 62.3 61.4 62.6 MATH 83.2 86.1 88.4 88.1 89.2 74.8 82.3 84.2 84.0 85.0 AIME 2025 14.4 18.7 19.6 20.3 19. 9.9 14.3 15.1 15.4 15.6 AMC23 68.6 72.1 76.8 77.0 77.5 55.3 66.7 70.4 69.8 70.0 88.6 88.8 90.5 91.2 91.0 19.3 23.1 24.1 24.5 24.7 78.6 76.4 83.0 82.3 83. 91.2 90.3 92.1 93.0 92.5 24.4 27.5 28.0 28.3 28.6 85.0 80.6 86.3 87.0 88.2 93.4 92.6 94.0 94.3 94.6 29.1 31.8 33.5 33.9 34.1 89.4 84.8 90.4 90.8 92. 94.1 93.5 94.3 95.3 95.0 33.4 36.1 40.0 41.0 39.2 93.4 88.3 93.2 94.4 95.1 95.0 93.9 94.8 95.6 95.5 39.2 40.8 47.0 49.6 47.8 97.3 90.8 94.3 97.3 96. 96.3 95.0 96.0 97.0 96.5 46.7 46.7 52.3 60.0 56.7 100.0 92.5 95.0 100.0 97.5 reveal that the Negative-Dominant group experienced the learning collapse phenomenonas characterized in section D.3in three out of the ten trials, whereas our proposed method exhibited no such failure cases. This demonstrates that our approach effectively maintains training stability while simultaneously facilitating model exploration. Effectiveness of Dynamic Difficulty Attention Shift (DDAS). To verify that our dynamic difficulty attention Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Figure 10: Training dynamics using Qwen2.5-Math-7B. Left Part: Models entropy on the Math training set. Center Part: Models entropy (actor entropy loss) on the Math test set. Right Part: The greedy decoding accuracy on the Math test set. shift truly improve the reasoning performance in comparison with the fixed difficulty preference in Control Experiment II, we demonstrate the results in table 6. It is observed that DDAS achieves substantial gains in the Pass@1 metric across all datasets. Furthermore, its performance remains competitive with Hard-Focused as increases. These results demonstrate that our method, by incorporating dynamic shift in difficulty-aware attention, effectively integrates the advantages of both Easy-Focused and Hard-Focused groups, thereby facilitating consistent performance improvements. 28 Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation Table 6: Performance comparison of A-GRAE against control groups on MATH, AIME 2025, and AMC23 datasets using Qwen2.5-Math-7B. Cells shaded in light red and light green denote Control Experiment and II, respectively. Method 1 2 4 16 32 64 128 256 Pass@k Base Model GRPO Positive-Dominant Negative-Dominant GRPO + ASS (group level) Easy-Focused Hard-Focused GRPO + DDAS (sample level) Base Model GRPO Positive-Dominant Negative-Dominant GRPO + ASS (group level) Easy-Focused Hard-Focused GRPO + DDAS (sample level) Base Model GRPO Positive-Dominant Negative-Dominant GRPO + ASS (group level) Easy-Focused Hard-Focused GRPO + DDAS (sample level) 63.4 76.5 75.5 77.2 77.6 77.0 75.8 77.8 6.1 10.3 9.7 10.8 11.0 9.7 11.8 10.9 40.6 59.2 59.2 60.9 61.4 61.5 60.8 62. MATH 83.2 86.1 83.2 87.6 88.1 86.4 86.0 88.4 88.6 88.8 85.4 90.4 91.2 89.2 89.8 90.5 AIME 2025 14.4 18.7 17.1 20.1 20.3 18.6 19.4 19.6 19.3 23.1 20.3 23.8 24.5 23.7 24.5 24.1 AMC23 68.6 72.1 72.1 76.6 77.0 72.3 75.6 76.8 78.6 76.4 76.4 82.1 82.3 76.4 82.5 83. 74.8 82.3 80.1 83.8 84.0 82.8 81.2 84.2 9.9 14.3 13.6 15.0 15.4 13.9 14.6 15.1 55.3 66.7 66.7 69.5 69.8 68.8 69.9 70.4 91.2 90.3 88.5 92.8 93.0 91.0 92.2 92.1 24.4 27.5 23.7 27.7 28.3 27.6 28.0 28.0 85.0 80.6 80.6 86.5 87.0 79.4 86.8 86. 93.4 92.6 90.1 94.1 94.3 92.5 93.6 94.0 29.1 31.8 27.1 34.2 33.9 31.1 33.0 33.5 89.4 84.8 84.8 90.6 90.8 82.3 91.3 90.4 94.1 93.5 91.2 95.2 95.3 93.8 94.8 94.3 33.4 36.1 30.9 40.9 41.0 36.8 36.9 40.0 93.4 88.3 88.3 94.1 94.4 88.4 93.5 93. 95.0 93.9 92.0 95.8 95.6 94.2 95.2 94.8 39.2 40.8 35.9 49.2 49.6 39.9 41.4 47.0 97.3 90.8 90.8 97.3 97.3 91.8 96.2 94.3 96.3 95.0 93.2 97.0 97.0 95.0 96.0 96.0 46.7 46.7 40.0 60.0 60.0 46.7 50.3 52.3 100.0 92.5 92.5 100.0 100.0 95.0 100.0 95."
        }
    ],
    "affiliations": [
        "Sun Yat-sen University",
        "Tsinghua University",
        "University of Hong Kong"
    ]
}