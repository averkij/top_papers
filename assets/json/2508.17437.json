{
    "paper_title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels",
    "authors": [
        "Long Le",
        "Ryan Lucas",
        "Chen Wang",
        "Chuhao Chen",
        "Dinesh Jayaraman",
        "Eric Eaton",
        "Lingjie Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 7 3 4 7 1 . 8 0 5 2 : r Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels Long Le1 Ryan Lucas2 Chen Wang1 Chuhao Chen1 Dinesh Jayaraman1 Eric Eaton1 Lingjie Liu1 1University of Pennsylvania 2Massachusetts Institute of Technology Figure 1: We introduce PIXIE, novel method for learning simulatable physics of 3D scenes from visual features. Trained on curated dataset of paired 3D objects and physical material annotations, PIXIE can predict both the discrete material types (e.g., rubber) and continuous values including Youngs modulus, Poissons ratio, and density for variety of materials, including elastic, plastic, and granular. The predicted material parameters can then be coupled with learned static 3D model such as Gaussian splats and physics solver such as the Material Point Method (MPM) to produce realistic 3D simulation under physical forces such as gravity and wind."
        },
        {
            "title": "Abstract",
            "content": "Inferring the physical properties of 3D scenes from visual information is critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, novel method that trains generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which Correspondence: vlongle@seas.upenn.edu Preprint. Under review. coupled with learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/"
        },
        {
            "title": "Introduction",
            "content": "Advances in learning-based scene reconstruction with Neural Radiance Fields [28] and Gaussian Splatting [18] have made it possible to recreate photorealistic 3D geometry and appearance from sparse camera views, with broad applications from immersive content creation to robotics and simulation. However, these approaches focus exclusively on visual appearancecapturing the geometry and colors of scene while remaining blind to its underlying physical properties. Yet the world is not merely static collection of shapes and textures. Objects bend, fold, bounce, and deform according to their material composition and the forces acting upon them. Consequently, there has been growing body of work that aims to integrate physics into 3D scene modeling [31, 27, 23, 11, 10, 41, 32, 12, 26, 42, 6]. Current approaches for acquiring the material properties of the scene generally fall into two categories, each with significant limitations. Some works such as [41, 12] require users to manually specify material parameters for the entire scene based on domain knowledge. This manual approach is limited in its application as it places heavy burden on the user and lacks fine-grained detail. Another line of work aims to automate the material discovery process via test-time optimization. Works including [15, 23, 45, 14, 26, 44] leverage differentiable physics solvers, iteratively optimizing material fields by comparing simulated outcomes against ground-truth observations or realism scores from video generative models. However, predicting physical parameters for hundreds of thousands of particles from sparse signals (i.e., single rendering or distillation scalar loss) is an extremely slow and difficult optimization process, often taking hours on single scene. Furthermore, this heavy per-scene memorization does not generalize: for each new scene, the incredibly slow optimization has to be run from scratch again. In this paper, we propose new framework, PIXIE, which unifies geometry, appearance, and physics learning via direct supervised learning. Our approach is inspired by how humans intuitively understand physics: when we see tree swaying in the wind, we do not memorize the stiffness values for each specific coordinate (x, y, z) instead, we learn that objects with tree-like visual features behave in certain ways when forces are applied. This physical understanding from visual cues allows us to anticipate the motion of different tree or even other vegetation like grass, in an entirely new context. Thus, our insight is to leverage rich 3D visual features such as those distilled from CLIP [33] to predict physical materials in direct supervised and feed-forward way. Once trained, our model can associate visual patterns (e.g., \"if it looks like vegetation\") with physical behaviors (e.g., \"it should have material properties similar to tree\"), enabling fast inference and generalization across scenes. To facilitate this research, we have curated and labeled PIXIEVERSE, dataset of 1624 paired 3D objects and annotated materials spanning 10 semantic classes. We developed sophisticated multi-step and semi-automatic data labeling process, distilling pretrained models including Gemini [36], CLIP [33], and human prior into the dataset. To our knowledge, this is the largest open-source dataset of paired 3D assets and physical material labels. Trained on PIXIEVERSE, our feed-forward network can predict material fields that are 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features, PIXIE can also zero-shot generalize to real-world scenes despite only ever being trained on synthetic data. Our contributions include: 1. Novel Framework for 3D Physics Prediction: We introduce PIXIE, unified framework that predicts discrete material types and continuous physical parameters (Youngs modulus, Poissons ratio, density) directly from visual features using supervised learning. 2. PIXIEVERSE Dataset: We curate and release PIXIEVERSE, the largest open-source dataset of 3D objects with physical material annotations (1624 objects, 10 semantic classes). 2 3. Fast and Generalizable Inference: By leveraging pretrained visual features from CLIP and feed-forward 3D U-Net, PIXIE performs inference orders of magnitude faster than prior test-time optimization approaches, achieving 1.46-4.39x improvement in realism scores as evaluated by state-of-the-art vision-language model. 4. Zero-Shot Generalization to Real Scenes: Despite being trained solely on synthetic data, PIXIE generalizes to real-world scenes, showing how visual feature distillation can effectively bridge the sim-to-real gap. 5. Seamless Integration with MPM Solvers: The predicted material fields can be directly coupled with Gaussian splatting models for realistic physics simulations under applied forces such as wind and gravity, enabling interactive and visually plausible 3D scene animations."
        },
        {
            "title": "2 Related Work",
            "content": "2D World Models Some early works [3, 2] learn to predict material labels on 2D images. Recently, learning forward dynamics from 2D video frames has also been explored extensively. For instance, Googles Genie [30] trains next-frame prediction model conditioned on latent actions derived from user inputs, capturing intuitive 2D physics in an unsupervised manner. While these methods achieve impressive 2D generation and control, they do not explicitly model 3D geometry or physically grounded world. Other works such as [7, 24] also explore generating or editing images based on learned real-world dynamics. While these methods achieve impressive results in 2D visual synthesis and can imply motion dynamics, they typically do not explicitly model 3D geometry, and only encode physics implicitly via next-frame prediction rather than through explicit material parameters, nor do they infer physically grounded material properties decoupled from appearances. These can lead to problems such as lack of object permanence or implausible interactions. In contrast, PIXIE directly operates in 3D, predicting explicit physical parameters (e.g., Youngs modulus, density) for 3D objects, enabling their integration into 3D physics simulators or neural networks [38, 29] for realistic interaction. Manual Assignment or Assignment of Physics using LLMs number of recent methods have explored combining learned 3D scene representations (e.g., Gaussian splatting) with physics solver where material parameters are assigned manually or through high-level heuristics. This often involves users specifying material types for the scene [41, 1] or using scripted object-to-material dictionaries [32] or large language and vision-language models [13, 5, 42, 22, 40, 25, 4] to guide the assignment. Test-time material optimization using videos Other works explore more automatic and principled ways to infer material properties using rendered videos. Some techniques [15, 23, 45, 16, 43] optimize material parameters by comparing simulated deformations against ground-truth observations, often requiring ground-truth multi-view videos of objects or ground-truth particle positions under known forces. More recent approaches [14, 26, 44] use video diffusion models as priors to optimize physics via motion distillation loss. Notably, these approaches suffer from extremely slow perscene optimization, often taking hours on single scene, and do not generalize to new scenes. In stark contrast, PIXIE employs feed-forward neural network that, once trained, predicts physical parameters in seconds, and can generalize to unseen scenes. recent work Vid2Sim [6] also aims to learn generalizable material prediction network across scenes. This was done by encoding front-view video of the object in motion with foundation video transformer [37] and learning to regress these motion priors into physical parameters. Unlike Vid2Sim, PIXIE does not require videos, relying instead on visual features from static images. Overall, PIXIE can also be used as an informed in conjunction with these test-time methods to further refine predictions."
        },
        {
            "title": "3 Method",
            "content": "Our central thesis is that 3D visual appearance provides sufficient information to recover an objects physical parameters. Texture, shading, and shape features captured from multiple calibrated images correlate with physical quantities such as Youngs modulus and Poissons ratio. By learning mapping from these visual features to material properties, we can augment volumetric reconstruction model (e.g., Gaussian splatting) with point-wise material estimate, without requiring force response observations. In Sec. 3.1, we detail our framework, leveraging rich visual priors from CLIP to predict material field, which can be used by physics solver to animate objects responding to external 3 Figure 2: Method Overview. From posed multi-view RGB images of static scene, PIXIE first reconstructs 3D model with NeRF and distilled CLIP features [35]. Then, we voxelize the features into regular grid where is the grid size and is the CLIP feature dimension. U-Net neural network [9] is trained to map the feature grid to the material field ˆMG which consists of discrete material model ID and continuous Youngs modulus, Poissons ratio, and density value for each voxel. Coupled with separately trained Gaussian splatting model, ˆMG can be used to simulate physics with physics solver such as MPM. forces. To train this model, we curated PIXIEVERSE, large dataset of paired 3D assets and material annotations, as detailed in Sec. 3.2. Figure 2 gives an overview of our method."
        },
        {
            "title": "Problem Formulation",
            "content": "Formally, the goal is to learn mapping: fθ : (I, Π) ˆM (1) that turns some calibrated RGB images of the static scene = {Ik}K k=1 and their joint camera specification Π into continuous three-dimensional material field. For every point R3 within the scene bounds, the field returns ˆM(p) = (cid:17) (cid:16)ˆℓ(p), ˆE(p), ˆν(p), ˆd(p) , where ˆℓ : R3 {1, . . . , L} is the discrete material class and ˆE, ˆν, : R3 are the continuous Youngs modulus, Poissons ratio, and density value respectively. Recall that the discrete material class, also known as the constitutive law, in Material Point Method is combination of the choices of an expert-defined hyperelastic energy function and return mapping (Sec. A). Learning point-mapping like this provides fine-grained material segmentation where for every spatial location we assign both semantic material label and the physical parameters that characterise that material. Learning the mapping in Eqn. (1) directly from 2D images to 3D materials is clearly not simple neither sample efficient. Instead, we leverage distilled feature field which has rich visual priors to represent the intermediate mapping between 2D images and 3D visual featutes, and then separate U-Net architecture to compute the mapping between 3D visual features and physical materials. We describe these components below. 3D Visual Feature Distillation Recent work on distilled feature fields has shown that dense 2D visual feature embeddings extracted from foundation models, such as CLIP, based on images can be lifted into 3D, yielding volumetric representation that is both geometrically accurate and rich in terms of visual and semantic priors [35]. Here, we also augment the classical NeRF representation [28] to predict view-independent feature vector in addition to color and density, i.e., Fθ : (x, d) (cid:55) (cid:0) (x), c(x, d), σ(x)(cid:1), where R3 and σ R0 are standard color and radiance NeRF outputs and Rd is highdimensional descriptor capturing visual semantics (e.g., object identity or other attributes), which we assume to be view-independent. We supervise color with image RGB and features with per-pixel CLIP embeddings extracted from the training images, using standard volume rendering (App. A.2). After training, we voxelize the feature field within known scene bounds to obtain regular grid FG of dimension grid, where = 64 is the grid size and = 768 is the CLIP feature dimension, serving as input to our material network. Material Grid Learning Our material learning network fM consists of feature projector fP and U-Net fU . As the CLIP features are very high-dimensional, we learn feature projector Figure 3: PIXIEVERSE Dataset Overview. We collect 1624 high-quality single-object assets, spanning 10 semantic classes (a), and 5 constitutive material types (b). The dataset is annotated with detailed physical properties including spatially varying discrete material types (b), Youngs modulus (c), Poissons ratio (d), and mass density (e). The left figure shows representative examples from the dataset: organic matter (tree, shrubs, grass, flowers), deformable toys (rubber ducks), sports equipment (sport balls), granular media (sand, snow & mud), and hollow containers (soda cans, metal crates). network fP , which consists of three layers of 3D convolution mapping CLIP features R768 to low-dimensional manifold R64. We then use the U-Net architecture fU to learn the mapping from the projected feature grid FG to material grid ˆMG(p), which is voxelized version of the material field ˆM(p). The feature projector fP and U-Net fU are jointly trained end-to-end via cross entropy and mean-squared error loss to predict the discrete material classification and the continuous values including Youngs modulus, Poissons ratio and density. More details is in Appendix E. We found that our voxel grids are very sparse with around 98% of the voxels being background. Naively trained, the material network fM would learn to always predict background. Thus, we also separately compute an occupancy mask grid RN RN RN , constructed by filtering out all voxels whose NeRF densities fall below threshold α = 0.01. The supervised lossescross entropy and mean squared errorsare only enforced on the occupied voxels. Concretely, the masked supervised loss consists of discrete cross entropy and continuous mean-squared error loss: Lsup ="
        },
        {
            "title": "1\nNocc",
            "content": "M(p) (cid:88) pG (cid:104) λ CE(ˆℓ(p), ℓGT (p)) + ( ˆE(p) EGT (p))2 + (ˆν(p) νGT (p))2 + ( ˆd(p) dGT (p))2(cid:105) (2) , where Nocc = (cid:80) M(p) is the total number of occupied voxels in the grid, ˆℓ(p) and ℓGT (p) are the predicted material class logits and the ground-truth, CE is the cross entropy loss, λ is loss balancing factor, and E, ν, are the Youngs modulus, Poissons ratio and density values, respectively. pG Physics Simulation We use the Material Point Method (MPM) to simulate physics. The MPM solver (Sec. A.3) takes point cloud of initial particle poses along with predicted material properties, and the external force specification, and simulates the particles transformations and deformations. Although it is possible to sample particles from NeRF model (e.g., via Poisson disk sampling [10]), we have found that it is easier to use Gaussian Splatting model (Sec. A) as each Gaussian can naturally be thought of as MPM particle [41]. Thus, we separately learn Gaussian splatting model from posed multi-view RGB images. We then transfer the material properties from our predicted material grid into the Gaussian splatting model via nearest neighbor interpolation."
        },
        {
            "title": "3.2 PIXIEVERSE Dataset",
            "content": "We collect one of the largest and highest quality known datasets of diverse objects with annotated physical materials. Our dataset  (Fig. 3)  covers 10 semantic classes, ranging from organic matter (trees, 5 Figure 4: Main VLM Results. (a) VLM score versus wall-clock time: PIXIE is three orders of magnitude faster than previous works while achieving 1.46-4.39x improvement in realism. Test-time optimization methods are run with varying numbers of epochs i.e., 1, 25, 50 for DreamPhysics and 1, 2, 5 for OmniPhysGS while inference methods are only run once. (b) Per-class VLM score: Our method leads on most object classes. Standard errors are also included. shrubs, grass, flowers) and granular media (sand, snow and mud) to hollow containers (soda-cans, metal crates), and toys (rubber ducks, sport balls). The dataset is sourced from Objaverse [8], the largest open-source dataset of 3D assets. Since Objaverse objects do not have physical parameter annotations, we develop an semi-automatic multi-stage labeling pipeline leveraging foundation vision-language models i.e., Gemini-2.5-Pro [36], distilled CLIP feature field [21] and manually tuned in-context physics examples. The full details is given in Appendix and C."
        },
        {
            "title": "4 Experiments",
            "content": "Dataset We train PIXIE on the PIXIEVERSE dataset and evaluate on 38 synthetic scenes from the test set and six real-world scene from the NeRF [28], LERF [19] datasets, and Spring-Gaus [45]. Simulation Details We use the material point method (MPM) implementation from PhysGaussian [41] as the physics solver. The solver takes gaussian splatting model augmented with physics where each Gaussian particle also has discrete material model ID, and continuous Youngs modulus, Poissons ratio, and density values. Each simulation is run for around 50 to 125 frames on single NVIDIA RTX A6000 GPU. External forces such as gravity and wind are applied to the static scenes as boundary conditions to create physics animations. Baselines We evaluate PIXIE against two recent test-time optimization methods: DreamPhysics [14] and OmniPhysGS [26], and LLM method NeRF2Physics [42]. DreamPhysics optimizes Youngs modulus field, requiring users to specify other values including material ID, Poissons ratio, and density. OmniPhysGS, on the other hand, selects hyperelastic energy density function and return mapping model, which, in combination, specifies material ID for each point in the field, requiring other physics parameters to be manually specified. Both methods rely on user prompt such as \"a tree swing in the wind\" and generative video diffusion model to optimize motion distillation loss. PIXIE, in contrast, infers all discrete and continuous parameters jointly  (Fig. 16)  . NeRF2Physics first captions the scene and query LLM for all plausible material types (e.g., metal\") along with the associated continuous values. Then, the material semantic names are associated with 3D points in the CLIP feature field, and physical properties are thus assigned via weighted similarities. This method is similar to our dataset labeling in principle with some crucial differences as detailed in Appendix and C, allowing PIXIEVERSE to have much more high-quality labels. PIXIE was trained on 12 NVIDIA RTX A6000 GPUs, each with batch size of 4, in one day using the Adam optimizer [20] while prior test-time methods do not require training. For training PIXIE and computing metrics, we apply log transform to and ρ, and normalize all log E, ν, log ρ values to [1, 1] based on max/min statistics from PIXIEVERSE. Evaluation Metrics We utilize state-of-the-art vision-language model, Gemini-2.5-Pro [36] as the judge. The models are prompted to compare the rendered candidate animations generated using physics parameters predicted by different baselines, and score those videos on scale from 0 to 5, where higher score is better. The prompt is in Appendix D. We also measure the reconstruction quality using PSNR and SSIM metric against the reference videos in the PIXIEVERSE dataset, which are manually verified by humans for quality control. Other metrics our method optimizes including class accuracy and continuous errors over E, ν, ρ are also computed. 6 Table 1: Main Quantitative Results. We report the average reconstruction quality (PSNR, SSIM) against the reference videos in PIXIEVERSE, the VLM score, and five other metrics our method optimizes including material accuracy and continuous errors over E, ν, ρ. Standard errors and 95% CI are also included, and best values are bolded. PIXIE-CLIP is by far the best method across all metrics, achieving 1.62-5.91x improvement in VLM score and 3.6-30.3% gains in PSNR and SSIM. Our CLIP variant is also notably more accurate than RGB and occupancy features as measured by material class accuracy and average continuous MSE on the test set. While our method simultaneously recovers all physical properties, some prior works only predict subset, hence -. Method PSNR SSIM VLM Mat. Acc. Avg. Cont. MSE log err ν err log ρ err DreamPhysics [14] 1 epoch 25 epochs 50 epochs 19.398 1.090 0.880 0.020 2.97 0.31 19.078 0.939 0.881 0.019 2.68 0.24 19.189 0.980 0.880 0.020 2.53 0.24 - - - OmniPhysGS [26] 1 epoch 2 epochs 5 epochs 17.907 0.359 0.882 0.007 0.74 0.10 0.072 0.0511 17.889 0.372 0.882 0.007 1.23 0.19 0.109 0.0704 17.842 0.354 0.883 0.007 0.99 0.12 0.104 0.0681 - - - - - - 2.393 0.123 1.419 0.097 1.387 0.097 - - - - - - - - - - - - - - - NeRF2Physics [42] 18.517 0.644 0.886 0.013 1.09 0.28 0.274 0.001 0.858 0. 1.115 0.165 0.462 0.106 0.997 0.162 PIXIE Occupancy RGB CLIP (ours) 17.887 1.524 0.866 0.027 1.76 0.41 18.652 2.031 0.861 0.035 2.53 0.46 23.256 2.456 0.918 0.023 4.35 0.08 0.643 0.052 0.722 0.061 0.985 0.011 0.126 0.012 0.106 0.015 0.056 0. 0.149 0.023 0.124 0.014 0.105 0.015 0.196 0.032 0.079 0.012 0.045 0.014 0.022 0.004 0.034 0.006 0.112 0."
        },
        {
            "title": "4.1 Synthetic Scene Experiments",
            "content": "Figure 4 (a) plots Gemini score versus runtime. PIXIE achieves VLM realism score of 4.35 0.08 1.46-4.39x improvement over all baselines and tops all other metrics while reducing inference time from minutes or hours to 2 s. per-class breakdown in Fig. 4 (b) shows our lead in most classes. In Table 1, our model improves perceptual metrics such as PSNR and SSIM by 3.6 30.3% and VLM scores by 2.21 4.58x over prior works. Figure 5 visualises eight representative scenes, comparing PIXIE against prior works. DreamPhysics leaves stiff artifacts due to missegmentation or overly high predicted values, OmniPhysGS collapses under force, and NeRF2Physics introduces highfrequency noise, whereas PIXIE generates smooth, class-consistent motion and segment boundaries. In the appendix, Figure 16 qualitatively visualizes the physical properties predicted by our network, showing PIXIEs ability to cleanly and accurately recover both discrete and continuous parameters across diverse sets of objects and continuous value spectrum. In contrast, some prior methods can only recover subset of parameters like or material class."
        },
        {
            "title": "4.2 Zero-shot Generalization to Real-World Scenes",
            "content": "Without any real-scene supervision, PIXIE can zero-shot generalize to many real-world scenes as shown in Fig. 6. For example, our method correctly assigns rigid vase bases and flexible leaves, yielding realistic motion that closely matches human expectation. Our method is surprisingly performant despite significant and non-trivial visual gaps between the training synthetic data versus the out-of-distribution real-world scenes. No other baseline can generalize under this setting."
        },
        {
            "title": "4.3 PIXIE’s Feature Type Ablation",
            "content": "Replacing CLIP with RGB or occupancy features drops VLM score by 40-60 % and nearly doubles parameter MSE (Table 1, rows Occupancy and RGB). We provide more results in the Appendix. Specifically, we show that the material class prediction also dramatically drops across all classes as shown in Fig. 17. Figure 18 shows the failure modes for real scenes, highlighting RGB and occupancys struggle to generalize to unseen data as compared to CLIP."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We presented PIXIE, framework that jointly reconstructs geometry, appearance, and explicit physical material fields from posed RGB images. By distilling rich CLIP features into 3D and training feed-forward 3D U-Net with per-voxel material supervision on our new PIXIEVERSE dataset, PIXIE avoids the expensive test-time optimization required by prior work. Once trained, it produces full material fields in few seconds, improving Gemini realism scores by 1.46-4.39x over DreamPhysics and OmniPhysGS while reducing inference time by three orders of magnitude. PIXIE leverages 7 Figure 5: Qualitative comparison on synthetic scenes. We visualized the predicted material class and predictions (left, right respectively) for PIXIE and Nerf2Physics, for DreamPhysics (right), and the plasticity and hyperelastic function classes predicted by OmniPhysGS. PIXIE produces stable, physically plausible motion while DreamPhysics remains overly stiff due to inaccurate fine-grained prediction or too high (e.g., see tree (C)), OmniPhysGS collapses under load due to unrealistic combination of plasticity and hyperelastic functions, and NeRF2Physics exhibits noisy artifacts. Please see https://pixie-3d.github.io/for the videos. CLIPs strong visual priors, which enables zero-shot transfer to real scenes, even though it is only trained on synthetic data. The method enables realistic, physically plausible 3D scene animation with off-the-shelf MPM solvers. Limitations We take the first step towards learning supervised 3D model for physical material prediction. Like prior art, our work focuses on single object interaction leaving multi-object scenes for future investigation. Another limitation is that while our UNet predict point estimate for each voxel, materials in the real-world contain uncertainty that visual information alone cannot resolve (e.g., tree can be stiff or flexible). promising extension is to learn distribution of materials (e.g., using diffusion) instead. Figure 6: PIXIEs Zero-shot Real-scene Generalization. Trained only on synthetic PIXIEVERSE, PIXIE can predict plausible physic properties, enabling realistic MPM simulation of real scenes. Here, we visualize the material types (left) and Youngs modulus (right) prediction in the first frame, and subsequent frames impacted by wind force. Please see the videos in our website https://pixie-3d.github.io/."
        },
        {
            "title": "References",
            "content": "[1] Jad Abou-Chakra, Krishan Rana, Feras Dayoub, and Niko Suenderhauf. Physically embodied gaussian splatting: realtime correctable world model for robotics. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=AEq0onGrN2. [2] Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. [3] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. Material recognition in the wild with the materials in context database. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 34793487, 2015. [4] Ziang Cao, Zhaoxi Chen, Liang Pan, and Ziwei Liu. Physx: Physical-grounded 3d asset generation. arXiv preprint arXiv:2507.12465, 2025. [5] Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, and Shenlong Wang. Physgen3d: Crafting miniature interactive world from single image. arXiv preprint arXiv:2503.20746, 2025. [6] Chuhao Chen, Zhiyang Dou, Chen Wang, Yiming Huang, Anjun Chen, Qiao Feng, Jiatao Gu, and Lingjie Liu. Vid2sim: Generalizable, video-based reconstruction of appearance, geometry IEEE Conference on Computer Vision and Pattern and physics for mesh-free simulation. Recognition (CVPR), 2025. [7] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects, 2022. URL https://arxiv.org/abs/2212.08051. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pie-nerf: Physics-based interactive elastodynamics with nerf, 2023. [11] Michael Fischer, Iliyan Georgiev, Thibault Groueix, Vladimir Kim, Tobias Ritschel, and Valentin Deschaintre. Sama: Material-aware 3d selection and segmentation. arXiv preprint arXiv:2411.19322, 2024. [12] Minghao Guo, Bohan Wang, Pingchuan Ma, Tianyuan Zhang, Crystal Elaine Owens, Chuang Gan, Joshua B. Tenenbaum, Kaiming He, and Wojciech Matusik. Physically compatible 3d object modeling from single image. arXiv preprint arXiv:2405.20510, 2024. [13] Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, and Shenlong Wang. Autovfx: Physically realistic video editing from natural language instructions. arXiv preprint arXiv:2411.02394, 2024. [14] Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, and Rynson WH Lau. Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors. arXiv preprint arXiv:2406.01476, 2024. [15] Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and Sanja Fidler. gradsim: Differentiable International Conference on simulation for system identification and visuomotor control. Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=c_ E8kFWfhp0. 10 [16] Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, and Yunzhu Li. Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos. arXiv preprint arXiv:2503.17973, 2025. [17] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [19] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1972919739, 2023. [20] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [21] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In Advances in Neural Information Processing Systems, volume 35, 2022. URL https://arxiv.org/pdf/2205.15585.pdf. [22] Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, and Eric Eaton. Articulate-anything: Automatic modeling of articulated objects via vision-language foundation model. arXiv preprint arXiv:2410.13882, 2024. [23] Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang Gan. PAC-neRF: Physics augmented continuum neural radiance fields for geometry-agnostic system identification. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tVkrbkz42vc. [24] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern dynamics. Recognition, pages 2414224153, 2024. [25] Zizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, and Jiajun Wu. Wonderplay: Dynamic 3d scene generation from single image and actions. arXiv preprint arXiv:2505.18151, 2025. [26] Yuchen Lin, Chenguo Lin, Jianjin Xu, and Yadong MU. OmniphysGS: 3d constitutive gaussians for general physics-based dynamics generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=9HZtP6I5lv. [27] Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua Tenenbaum, Tao Du, Chuang Gan, and Wojciech Matusik. Learning neural constitutive laws from motion observations for generalizable pde dynamics. In International Conference on Machine Learning, pages 2327923300. PMLR, 2023. [28] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [29] Himangi Mittal, Peiye Zhuang, Hsin-Ying Lee, and Shubham Tulsiani. Uniphy: Learning unified constitutive model for inverse physics simulation. arXiv preprint arXiv:2505.16971, 2025. [30] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 2: large-scale foundation world model. 2024. URL https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/. 11 [31] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. [32] Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Feature splatting: Language-driven physics-based scene synthesis and editing. arXiv preprint arXiv:2404.01223, 2024. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. [35] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable few-shot language-guided manipulation, 2023. URL https: //arxiv.org/abs/2308.07931. [36] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [37] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. [38] Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, and Lingjie Liu. Physctrl: Generative physics for controllable and physics-grounded video generation. In arXiv preprint, 2025. [39] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. [40] Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, and Shenlong Wang. Video2game: Real-time, interactive, realistic and browser-compatible environment from single video, 2024. [41] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023. [42] Albert Zhai, Yuan Shen, Emily Chen, Gloria Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang. Physical property understanding from language-embedded feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2829628305, 2024. [43] Kaifeng Zhang, Baoyu Li, Kris Hauser, and Yunzhu Li. Particle-grid neural dynamics for learning deformable object models from rgb-d videos. arXiv preprint arXiv:2506.15680, 2025. [44] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William T. Freeman. PhysDreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision. Springer, 2024. [45] Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yunzhu Li. Reconstruction and simulation of elastic objects with spring-mass 3d gaussians. European Conference on Computer Vision (ECCV), 2024."
        },
        {
            "title": "A Preliminaries",
            "content": "This section briefly reviews foundational concepts in 3D scene representation and physics modeling relevant to our work. A.1 Learned Scene Representation Reconstructing 3D scenes from 2D images is commonly achieved by learning parameterized representation, Fθ, optimized to render novel views that match observed images {I (i)}M i=1 given camera parameters {π(i)}M i=1. This typically involves minimizing photometric loss: min θ (cid:88) i=1 ˆI (i)(θ) (i)(cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 , where ˆI (i)(θ) is the image rendered from viewpoint i. Two prominent representations are Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) models. Neural Radiance Fields (NeRF) [28] model scene as continuous function Fθ : (x, d) (cid:55) (c, σ), mapping 3D location and viewing direction to an emitted color and volume density σ. Images are synthesized using volume rendering, integrating color and density along camera rays. This process differentiability allows for end-to-end optimization from images. Gaussian Splatting (GS) [18] represents scenes as collection of 3D Gaussian primitives, each defined by center µi, covariance Σi, color ci, and opacity αi. These Gaussians are projected onto the image plane and blended using alpha compositing to render views. In our work, the principles of neural scene representation, particularly NeRF-like architectures, are leveraged not only for visual reconstruction but also for creating dense 3D visual feature fields. As detailed in Sec. 3.1, we utilize NeRF-based model to distill 2D image features (e.g., from CLIP) into volumetric 3D feature grid. This 3D feature representation, FG, then serves as the primary input to our physics prediction network. For subsequent physics simulation, GS offers convenient particle-based representation. A.2 3D Visual Feature Distillation Details Following [35], we augment the NeRF mapping to produce features alongside color and density σ: Fθ : (x, d) (cid:55) (cid:0) (x), c(x, d), σ(x)(cid:1). Given camera ray r(t) = + td passing through pixel p, color C(p) and features (p) are volume-rendered as (cid:90) tf (cid:90) tf C(p) = (t) σ(cid:0)r(t)(cid:1) c(cid:0)r(t), d(cid:1) dt, (p) = (t) σ(cid:0)r(t)(cid:1) f(cid:0)r(t)(cid:1) dt, (3) tn tn where (t) = exp(cid:0) (cid:82) σ(r(s)) ds(cid:1) is the accumulated transmittance from the ray origin to depth tn t. At each training iteration, batch of rays is sampled from the input views. For each ray (pixel p), we enforce that the rendered color C(p) matches the ground-truth pixel RGB (p), while the rendered feature (p) matches the corresponding CLIP-based feature vector (p) extracted from the image. The loss of the network is: = (cid:88) (cid:13) (cid:13)C(p) (p)(cid:13) 2 2 + λfeat (cid:13) (cid:13) (cid:13)F (p) (p)(cid:13) 2 2 ; (cid:13) (cid:88) the first term enforces color fidelity, while the second aligns the rendered volumetric CLIP features with the dense 2D features extracted from the training images. From trained distilled feature field Fθ, we obtain regular feature grid FG of dimension grid, where = 64 is the grid size and = 768 is the CLIP feature dimension. This is done via voxelization using known scene bounds. For our synthetic dataset, we center and normalize all objects within unit cube. A.3 Material Point Method (MPM) for Physics Simulation To simulate how objects move and deform under applied forces, physics engine requires knowledge of their material properties. These properties are typically defined within the framework of continuum mechanics, which describes the behavior of materials at macroscopic level. The fundamental equations of motion (conservation of mass and momentum) are: ρ"
        },
        {
            "title": "Dv\nDt",
            "content": "= σ + ext = 0 , (4) where ρ is mass density, the velocity field, σ the Cauchy stress tensor, and ext any external force (e.g. gravity or user interactions). The material-specific constitutive laws define how σ depends on the local deformation gradient F. For elastic materials, stress depends purely on the recoverable strain; for plastic materials, yield condition enforces partial flow once strain exceeds threshold. Constitutive Laws and Parameters Most continuum simulations separate the constitutive model into two core components: Eµ : Fe (cid:55) P, Pµ : e,trial (cid:55) e,new , (5) where Fe is the elastic portion of the deformation gradient, is the (First) PiolaKirchhoff stress, and µ represents the set of material parameters (e.g. Youngs modulus E, Poissons ratio ν, yield stress). The elastic law Eµ computes stress from the current elastic deformation, while the return-mapping Pµ projects any trial elastic update e,trial onto the feasible yield surface if plastic flow is triggered. Typically, the constitutive laws i.e., Eµ and Pµ are hand-designed by domain experts. The choice of and jointly define class of material (e.g., rubber). Within material class, additional continuous parameters µ including Youngs modulus, Poissons ratio and density can be specified for more granular control of the material properties (e.g., stiffness of rubber). In our work, PIXIE jointly predicts the discrete material model and the continuous material parameters."
        },
        {
            "title": "B PIXIEVERSE Dataset Details",
            "content": "We heavily curate the dataset to set of 1624 objects after multi-stage filter that removes multi-object scenes, missing textures, duplicated assets, and objects whose material labeling is either ambiguous or physically implausible. The process is semi-automatic with VLM-driven multi-stage pipeline while still imparting substantial human prior and labor. We manually tune the physics parameter ranges for each semantic class (e.g., tree\", rubber toy\") and 3D segmentation query terms, and provide these as in-context examples for the VLM to align them with humans physical understanding. First, we define some object class (e.g., tree\") and some alternative query terms (e.g., ficus, fern, evergreen etc\"). We then use sentence transformer model [39] to compute the cosine similarity between the search terms and the name of each Objaverse object. We select = 500 objects with the highest similarity score for each class, creating an initial candidate pool. However, since Objaverse objects vary greatly in asset quality, lighting conditions, and some scenes contain multiple objects which are not suitable for our material learning, an additional filtering step is needed. The Gemini VLM is prompted to filter out low-quality or unsuitable scenes. distilled NeRF model is fitted to each object. Then, the VLM is provided five multi-view RGB images of an object, and prompted to provide list of the objects semantic parts along with associated material class and ranges for continuous values. The ranges such as {1e4, 1e5} allow us to simulate wider range of dynamics from flexible to more rigid trees. The VLM is also prompted to specify list of constraints such as to ensure that the leafs density is lower than the trunks. We then sample the continuous values from the VLMs specified ranges subject to the constraint via rejection sampling. The semantic parts (e.g., pot\") are used with the CLIP distilled feature field to compute 3D semantic segmentation of the object into parts, and the sampled material properties are applied uniformly to all points within part. This ground-truth material and feature fields are then voxelized into regular grids for use in supervised learning by the PIXIE framework. The following sections provide more details on each step of our semi-automatic labeling process. 14 tree: tree, ficus, fern, oak tree, pine tree, evergreen, palm tree, maple tree, bonsai tree flowers: flower, bouquet, rose, tulip, daisy, lily, sunflower, orchid, flower arrangement, flowering plant, garden flowers, wildflowers, floral rubber_ducks_and_toys: rubber duck, bath toy, rubber toy, toy duck, squeaky toy, floating toy, plastic duck, childrens bath toy, yellow duck toy, rubber animal toy soda_cans: soda can, aluminum can, beverage can, cola can, soft drink can, metal can, canned drink, pop can, fizzy drink can sport_balls: basketball, soccer ball, football, tennis ball, baseball, volleyball, golf ball, rugby ball, ping pong ball, cricket ball, bowling ball, beach ball, sports ball sand: sand, beach sand, desert sand, sandy terrain, sand pile, sand dune, sandpit, sand box, sand texture, grainy sand shrubs: shrub, bush, hedge, ornamental bush, garden shrub, boxwood, flowering bush, evergreen shrub, decorative plant, landscaping shrub metal_crates: metal crate, steel box, metal container, shipping crate, metal storage box, industrial container, metal chest, storage crate, metallic box grass: grass, lawn, turf, grassland, meadow, grassy field, green grass, grass patch, tall grass, wild grass, pasture snow_and_mud: snow, mud, snowy ground, muddy ground, wet mud, fresh snow, packed snow, snowy terrain, muddy terrain, snow patch, mud puddle, snowdrift, muddy path, snowy surface, muddy surface, slush, wet snow, dirty snow, muddy water, snowy landscape Figure 7: Objaverse Class Selection Keywords. The keywords for matching semantic class with an objaverse assets name. B.1 Object Selection from Objaverse We use the all-MiniLM-L6-v2 [39] sentence transformer to compute the cosine similarity between an objaverse assets name and some search terms for each object class. The search terms are in Fig. 7. The top = 500 objects with the highest similarity score are selected for each class. B.2 Object Filtering Next, we prompt Gemini to filter out low-quality assets. The system instruction is given in Fig. 8. Then, human quickly scans through the VLM results organized in our web interface as shown in Fig. 9 to correct any mistakes. B.3 CLIP-Driven 3D Semantic Segmentation From distilled CLIP feature field of the object [35], we can perform 3D semantic segmentation by providing list of the objects parts (e.g., pot, trunk, leaves\"). These query terms are used to compute the cosine similarity between each CLIP feature at given 3D coordinate against the terms, and the part with highest similarity is assigned to that point. The choices of query terms (e.g., pot, trunk, leaves\" vs base, stem, leaf\") greatly affect the segmentation quality, and is not obvious. high-performing query list in one object is not guaranteed to yield high performance in another object, e.g., see Fig. 10. Thus, we prompt VLM actor to generate several candidate queries for each object, render all candidates, and prompt another VLM critic to select the best query terms from the rendered 3D segmentation images, as detailed Sec. B.4. B.4 VLM Actor-Critic Labeling Current VLMs might not have robust physical understanding for generating high-quality labels for PIXIEVERSE zeroshot. Thus, we first manually tune the physic parameters for each semantic object class (e.g., tree\", rubber toy\"). condensed version of these examples is provided in Fig. 12. We also provide examples of different search terms (e.g., pot, trunk, leaves\" vs base, stem, leaf\"). These We need to select some images of the classes: {class_name}. This class includes objects like {search_terms}. We will provide you some images rendered from the 3D model. You need to either return True or False. Return False to reject the image as inappropriate for the video game development. Some common reasons for rejection: The image doesnt clearly depict the object class The image is too dark or too bright or too blurry or has some other low quality. Remember, we want highquality training data. The image contains other things in addition to the object. REMEMBER, we only want images that depict cleanly ONE SINGLE OBJECT belonging to one of the classes. But you also need to use your common sense and best judgement. For example, for class like \"flowers\", the object might include vase of flowers (you rarely see single flower in the wild). So you should return True in this case. We do want diversity in our dataset collection. So even if the texture of the object is bit unusual, as long as you can recognize it as belonging to the class / search terms, you should return True. Only remove lowquality assets. The return format is: json { \"is_appropriate\": true (or false), \"reason\": \"reason for the decision\" } Well be using the 3d models to learn physic parameters like material and young modulus to simulate the physics of the object. E.g., the tree swaying in the wind or thing being dropped from height. Therefore, you need to decide if the image depicts an object that is likely to be used in physics simulation. Figure 8: Object Filtering System Prompt. Prompt for VLM to filter out low-quality assets. in-context examples are provided to VLM actor that simultaneously proposes physics parameters and semantic segmentatic queries for that object from multi-view images of that object as illustrated in Fig. 11. The full system prompt for the VLM is provided in Fig. 13 and the full in-context examples in Listing 1. We render an image representing 3D semantic segmentation masks for each query proposal as shown in Fig. 10. VLM critic is then prompted to select the best segmentation queries from the rendered images. The critics system prompt is provided in Fig. 14. Additionally, materials in the real-world contain uncertainty that visual information alone cannot resolve (e.g., tree can range from stiff to flexible). Thus, instead of specifying one physics parameter per part, we prompt the VLM actor to output plausible range (e.g., {1e4, 1e5} see Fig. 11, 12). We then sample value uniformly from each range to build our training dataset. To further ensure that the sampled values are consistent, the VLM is also prompted to specify list of constraints (e.g., the density of leaves must be lower than that of the trunk). Rejection sampling is used to ensure that the final dataset respects the constraints. 16 Figure 9: Manual correction for object filtering. The web interface for quickly inspecting and manually correcting VLM results. Figure 10: CLIP Semantic Segmentation. CLIP features can be noisy for various objects and different text queries vary greatly in segmentation quality. Thus, we prompt VLM actor to generate several candidate queries for each object, render all candidates, and prompt another VLM critic to select the best query terms from the rendered 3D segmentation images. Some candidates are provided and proposals chosen by the critic are highlighted. Note that high-performing query proposal (e.g., leaves,pot,trunk\") in one object is not necessary high-performant in another. The PCA visualization of the CLIP feature fields is also provided. Figure 11: VLM Actors Physics and Segmentation Proposal. 18 tree: pot: {density: 400, E: 2e8, nu: 0.4, material: \"rigid\"} trunk: {density: 400, E: 2e6, nu: 0.4, material: \"elastic\"} leaves: {density: 200, E: 2e4, nu: 0.4, material: \"elastic\"} flowers: vase: {density: 500, E: 1e6, nu: 0.3, material: \"rigid\"} flowers: {density: 100, E: 1e4, nu: 0.4, material: \"elastic\"} shrub: stems: {density: 300, E: 1e5, nu: 0.35, material: \"elastic\"} twigs: {density: 250, E: 6e4, nu: 0.38, material: \"elastic\"} foliage: {density: 150, E: 2e4, nu: 0.40, material: \"elastic\"} grass: blades: {density: 80, E: 1e4, nu: 0.45, material: \"elastic\"} soil (if visible): {density: 1200, E: 5e5, nu: 0.30, material: \"rigid\"} rubber_ducks_and_toys: toy: {density: [80, 150], E: [3e4, 5e4], nu: [0.4, 0.45], material: \"elastic\"} sport_balls: ball: {density: [80, 150], E: [3e4, 5e4], nu: [0.4, 0.45], material: \"elastic\"} soda_cans: can: {density: [2600, 2800], E: [5e10, 8e10], nu: [0.25, 0.35], material: \" metal\"} metal_crates: crate: {density: [2500, 2900], E: [8e7, 1.2e8], nu: [0.25, 0.35], material: \" metal\"} sand: sand: {density: [1800, 2200], E: [4e7, 6e7], nu: [0.25, 0.35], material: \"sand\"} jello_block: jello: {density: [40, 60], E: [800, 1200], nu: [0.25, 0.35], material: \"elastic \"} snow_and_mud: snow_and_mud: {density: [2000, 3000], E: [8e4, 1.2e5], nu: [0.15, 0.25], material: \"snow\"} Figure 12: In-Context Physics Condensed Examples. Material properties for each object class used in the VLM prompting. Density is in kg/m³, (Youngs Modulus) is in Pa, nu (Poissons ratio) is dimensionless. 19 We are trying to label 3D object with physical properties. The physical properties are: Density Youngs Modulus Poissons Ratio Material model where the material model is one of the following: {material_list_str} We have an automatic semantic segmentation model that can segment the object into different parts. Well assume that each part has the same material model. Your job is to come up with the part query to pass to the semantic segmentation model, and the associated material properties for each part. {special_notes} For example, for {class_name_for_example}, the return is json {example_material_dict_str} {example_explanation} Note that there are many different valid values for the material properties including E, nu, and density that would influence how the object behaves. Thus, instead of actual values, you should return range of values like \"E\": [2e4, 2e6 ]. Also, provide reasoning and constraints on the values when appropriate. So the output should be json with the following format: json {{ \"material_dict\": {{ ... similar to example_dict with ranges ... }}, \"reasoning\": \"...\", \"constraints\": \"...\", \"all_queries\": \"...\" }} Remember to write constraints in the form of python code. For example, python {example_constraints_str} Note that youve been asked to generate material range so material_dict[\" leaves\"][\"density\"] is range of values. But for the purpose of the constraints writing, you can assume that the material_dict[\"leaves\"][\"density\"] is single value, and generate the python code similar to the example above. This is important because we will first sample value from the range, then invoke your constraints code. So instead of writing something like python assert material_dict[\"leaves\"][\"density\"][0] ... you must write something like python assert material_dict[\"leaves\"][\"density\"] ... Note that the correct code doesnt have the bracket because material_dict[\" leaves\"][\"density\"] will be already reduced to single value by our sampler. You will be provided with images of the object from different views or single view. Please try your best to come up with appropriate part queries as well. For example, if the object doesnt have visible trunk or pot, then you should NOT include them in the material_dict. Only segment parts that are visible in the image. Also, because our CLIP segmentation model is not perfect, you should come up with alternative queries as well including the original queries in the all_queries list. For example, json {example_all_queries_str} In total, you need to provide {num_alternative_queries} alternative queries. Tips: {tips_str} Make sure that each element in the all_queries list is in the exact same order as the material_dict keys. 20 Figure 13: VLM Actor System Prompt. You are segmentation quality critic. Your task is to evaluate the quality of segmentation results produced by CLIPbased segmentation model. You will be shown: 1. set of original RGB images of 3D object from different views 2. Segmentation results for different part queries Your job is to: 1. Evaluate each segmentation query based on how well it separates the object into meaningful parts 2. Score each query on scale of 110 (10 being perfect) 3. Provide reasoning for your scores 4. Suggest improvements to the queries if needed Consider the following factors in your evaluation: Does the segmentation properly separate the object into distinct, semantically meaningful parts? Are the boundaries of the segments accurate and clean? Is any important part of the object missed or incorrectly segmented? IMPORTANT: note that our imperfect CLIP segmentation model is heavily dependent on the choice of part queries. Thus, even if query might not be semantically correct, as long as it is useful for separating the object into distinct parts, you should score it high. Bad queries would result in bad segmentation that are noisy or different parts are not correctly and/or clearly separated. Your output should be JSON in the following format: json { \"query_evaluations\": { \"query_0\": { \"score\": 8, \"reasoning\": \"This query effectively separates the object into functionally distinct parts. The boundaries are clean and consistent across different views.\" }, \"query_1\": { \"score\": 3, \"reasoning\": \"This query fails to distinguish important parts of the object, making it unsuitable for physical property assignment.\" }, ... }, \"best_query\": \"query_1\", \"suggested_improvements\": \"Consider using more specific terms like ceramic pot instead of just pot to improve segmentation boundaries.\" } where query_{i} is the ith query in the \"all_queries\" list. Be detailed in your reasoning and make concrete suggestions for improvements. Figure 14: VLM Critic System Prompt. System instruction for evaluating segmentation quality and suggesting improvements. 21 { \" tree \": { Listing 1: In-context Physics Examples \" as _n am _f r_ xa le \": \" ficus tree \" , \" special_notes \": \"\" , \" xam le _m at er ia l_ ic \": { \" pot \": {\" density \": 400 , \" \": 2 e8 , \" nu \": 0.4 , \" material_id \": get_material_id (\" rigid \") } , \" trunk \": {\" density \": 400 , \" \": 2 e6 , \" nu \": 0.4 , \" material_id \": get_material_id (\" elastic \") } , \" leaves \": {\" density \": 200 , \" \": 2 e4 , \" nu \": 0.4 , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": textwrap . dedent (\"\"\" For this , we assume that the pot is stationary , while the trunk and leaves are made of \" elastic \" , which will make them sway in the wind . The stiffness ( Young Modulus ) of the trunk is much higher than that of the leaves . \"\"\") , \" exam ple_al l_qu erie \": [[\" leaves \" , \" trunk \" , \" pot \"] , [\" green \" , \" orange \" , \" reddish - brown \"]] , \" tips \": [ \" In scene , typically there stationary part that will serve to fix the object to the ground . Usually , it the pot , or some base of the tree . You must set the material_id of the stationary part to 6. If there no stationary part , then never mind .\" , \" The higher the is , the stiffer the object is . . . , so tree would sway less in the wind .\" , ] } , \" flowers \": { \" as _n am _f r_ xa le \": \" flowers in vase \" , \" special_notes \": \"\" , \" xam le _m at er ia l_ ic \": { \" vase \": {\" density \": 500 , \" \": 1 e6 , \" nu \": 0.3 , \" material_id \": get_material_id (\" rigid \") } , \" flowers \": {\" density \": 100 , \" \": 1 e4 , \" nu \": 0.4 , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": textwrap . dedent (\"\"\" Here , the vase is designated as stationary ( material_id =6) , indicating it should not move or sway . The flowers are set to more pliable or flexible material ( like \" elastic \" = 0) , so that they can sway if there wind or slight motion . The stiffness ( Young Modulus ) of the vase is much higher than that of the flowers , making the vase rigid and the flowers more flexible . \"\"\") , \" exam ple_al l_qu erie \": [[\" vase \" , \" flowers \"] , [\" ceramic base \" , \" petals \"] , [\" blue vase \" , \" pink flower \"]] , \" tips \": [ \" In typical flower arrangement , the vase ( or base ) is stationary , so give that part material_id =6 if present .\" , \" The higher the , the stiffer the part . So the vase should have higher range than the flowers .\" , ] } , \" shrub \": { \" as _n am _f r_ xa le \": \" typical three - part shrub \" , \" special_notes \": textwrap . dedent (\"\"\" Dataset note : Shrubs in our dataset stand by themselves - - - there is no planter or base . 22 You should therefore return only the shrub structural parts and none of them are stationary . \"\"\") , \" xam le _m at er ia l_ ic \": { \" stems \": { \" density \": 300 , \" \": 1 e5 , \" nu \": 0.35 , \" material_id \": get_material_id (\" elastic \") } , \" twigs \": { \" density \": 250 , \" \": 6 e4 , \" nu \": 0.38 , \" material_id \": get_material_id (\" elastic \") } , \" foliage \": { \" density \": 150 , \" \": 2 e4 , \" nu \": 0.40 , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": textwrap . dedent (\"\"\" Return * ranges * instead of single values and accompany them with reasoning , Pythonic constraints , and alternative query lists . \"\"\") , \" exam ple_al l_qu erie \": [ [\" stems \" , \" twigs \" , \" foliage \"] , [\" woody stems \" , \" thin branches \" , \" leaves \"] , [\" brown sticks \" , \" small branches \" , \" green leaves \"] ] , \" tips \": [ \" Provide exactly the parts visible ( usually stems / twigs + foliage ) .\" , \"1 e4 <= <= 1 e6 .\" , \" Stems should be stiffest > twigs > foliage .\" , \" No part uses material_id 6 because nothing is fixed to the ground .\" , ] } , \" grass \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" ** Dataset note :** Grass patches are usually isolated ; occasionally visible soil patch is underneath . Include \" soil \" part only if it is visible . \"\"\") , \" xam le _m at er ia l_ ic \": { \" blades \": { \" density \": 80 , \" \": 1 e4 , \" nu \": 0.45 , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": textwrap . dedent (\"\"\" Example ( typical isolated grass - - - no stationary part ) : json { \" blades \": { \" density \": 80 , \" \": 1 e4 , \" nu \": 0.45 , \" material_id \": get_material_id (\" elastic \") } } Example ( grass with visible soil ) : json { \" soil \": { \" density \": 1200 , \" \": 5 e5 , \" nu \": 0.30 , \" material_id \": get_material_id (\" rigid \") } , material_id \": get_material_id (\" elastic \") } \" blades \": { \" density \": 80 , \" \": 1 e4 , \" nu \": 0.45 , \" } Return * ranges * , reasoning , constraints , and alternative query lists . \"\"\") , \" exam ple_al l_qu erie \": [ [\" blades \"] , [\" grass \"] , 23 [\" green stalks \"] ] , \" tips \": [ \" Segment only the visible parts ( sometimes just \" blades \") .\" , \" If * no * soil visible : nall_queries : [[\" blades \"] ,[\" grass \"] ,[\" green stalks \"]]\" , \" If soil * is * visible : nall_queries : [[\" soil \" , \" blades \"] ,[\" dirt \" , \" grass \"] ,[\" brown base \" , \" green grass \"]]\" , \"1 e4 <= <= 1 e6 .\" , \" If soil present -> give it material_id 6 and ensure E_soil > E_blades .\" , \" If soil absent -> no stationary part ; material_id should not appear .\" , ] } , \" ru bbe r_d uck _a nd _ to ys \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For rubber ducks and toys , we want to treat the entire object as single part . Do not attempt to segment it into multiple parts . The object should be treated as single , bouncy rubber - like object . \"\"\") , \" xam le _m at er ia l_ ic \": { \" toy \": {\" density \": [80 , 150] , \" \": [3 e4 , 5 e4 ] , \" nu \": [0.4 , 0.45] , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" toy \"] , [\" rubber toy \"] , [\" yellow duck \"] , [\" plastic toy \"]] , \" tips \": [ \" Always use material_id =0 ( jelly ) for bouncy rubber - like behavior \" , \" Keep relatively low ( around 1 e3 ) for good bounce \" , \" Density should be in the range of typical rubber / plastic toys \" , \" Poisson ratio should be around 0.35 for rubber - like behavior \" , \" Make sure all queries in all_queries list are single - part queries \" ] } , \" sport_balls \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For sport balls , we want to treat the entire ball as single part . Do not attempt to segment it into multiple parts ( like surface patterns or seams ) . The ball should be treated as single , bouncy object . \"\"\") , \" xam le _m at er ia l_ ic \": { \" ball \": {\" density \": [80 , 150] , \" \": [3 e4 , 5 e4 ] , \" nu \": [0.4 , 0.45] , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" ball \"] , [\" sport ball \"] , [\" basketball \"] , [\" round ball \"]] , \" tips \": [ \" Always use material_id =0 ( jelly ) for bouncy behavior \" , \" Keep relatively low ( around 1 e3 ) for good bounce \" , \" Density should be in the range of typical sport balls \" , \" Poisson ratio should be around 0.35 for rubber - like behavior \" , 24 \" Make sure all queries in all_queries list are single - part queries \" ] } , \" soda_cans \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For soda cans , we want to treat the entire can as single part . Do not attempt to segment it into multiple parts ( like the top , body , or label ) . The can should be treated as single , rigid metal object . \"\"\") , \" xam le _m at er ia l_ ic \": { \" can \": {\" density \": [2600 , 2800] , \" \": [5 e10 , 8 e10 ] , \" nu \": [0.25 , 0.35] , \" material_id \": get_material_id (\" metal \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" can \"] , [\" soda can \"] , [\" aluminum can \"] , [\" metal can \"]] , \" tips \": [ \" Always use material_id =1 ( metal ) for rigid metal behavior \" , \" Keep relatively high ( around 1 e8 ) for metal stiffness \" , \" Density should be in the range of typical aluminum ( around 2700 kg / ^3) \" , \" Poisson ratio should be around 0.3 for metal behavior \" , \" Make sure all queries in all_queries list are single - part queries \" ] } , \" metal_crates \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For metal crates , we want to treat the entire crate as single part . Do not attempt to segment it into multiple parts ( like the sides , top , or bottom ) . The crate should be treated as single , rigid metal object . \"\"\") , \" xam le _m at er ia l_ ic \": { \" crate \": {\" density \": [2500 , 2900] , \" \": [8 e7 , 1.2 e8 ] , \" nu \": [0.25 , 0.35] , \" material_id \": get_material_id (\" metal \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" crate \"] , [\" metal crate \"] , [\" metal box \"] , [\" steel crate \"]] , \" tips \": [ \" , \" Always use material_id =1 ( metal ) for rigid metal behavior \" Keep relatively high ( around 1 e8 ) for metal stiffness \" , \" Density should be in the range of typical metal ( around 2700 kg / ^3) \" , \" Poisson ratio should be around 0.3 for metal behavior \" , \" Make sure all queries in all_queries list are single - part queries \" ] } , \" sand \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For sand objects , we want to treat the entire object as single part . Do not attempt to segment it into multiple parts . The sand should be treated as single , granular material . \"\"\") , \" xam le _m at er ia l_ ic \": { \" sand \": {\" density \": [1800 , 2200] , \" \": [4 e7 , 6 e7 ] , \" nu \": [0.25 , 0.35] , \" material_id \": get_material_id (\" sand \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" sand \"] , [\" sand pile \"] , [\" sand mound \"] , [\" granular material \"]] , \" tips \": [ \" Always use material_id =2 ( sand ) for granular behavior \" , \" Keep relatively high ( around 5 e7 ) for sand stiffness \" , \" Density should be in the range of typical sand ( around 2000 kg / ^3) \" , \" Poisson ratio should be around 0.3 for sand behavior \" , \" Make sure all queries in all_queries list are single - part queries \" ] } , \" jello_block \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For jello blocks , we want to treat the entire object as single part . Do not attempt to segment it into multiple parts . The jello block should be treated as single , soft , bouncy object . \"\"\") , \" xam le _m at er ia l_ ic \": { \" jello \": {\" density \": [40 , 60] , \" \": [800 , 1200] , \" nu \": [0.25 , 0.35] , \" material_id \": get_material_id (\" elastic \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" jello \"] , [\" jello block \"] , [\" gelatin \"] , [\" bouncy block \"]] , \" tips \": [ \" Always use material_id =0 ( jelly ) for soft , bouncy behavior \" , \" Keep relatively low ( around 1000) for good bounce and jiggle \" , \" Density should be in the range of typical jello ( around 50 kg / ^3) \" , \" Poisson ratio should be around 0.3 for jello - like behavior \" , \" Make sure all queries in all_queries list are single - part queries \" ] } , \" snow_and_mud \": { \" as _n am _f r_ xa le \": \"\" , \" special_notes \": textwrap . dedent (\"\"\" IMPORTANT : For combined snow & mud objects , we treat the entire mixture as single deformable part . Do ** not ** attempt to split it into separate snow and mud regions - - - the simulation will use one MPM material . \"\"\") , \" xam le _m at er ia l_ ic \": { \" snow_and_mud \": {\" density \": [2000 , 3000] , \" \": [8 e4 , 1.2 e5 ] , \" nu \": [0.15 , 0.25] , \" material_id \": get_material_id (\" snow \") } } , \" exam ple_ex plan atio \": \"\" , \" exam ple_al l_qu erie \": [[\" snow and mud \"] , [\" slush \"] , [\" muddy snow \"] , [\" wet snow \"]] , \" tips \": [ \" Always set material_id = 5 ( snow ) so the simulator uses the appropriate elasto - plastic snow model .\" , \" Keep around 1 e5 ( the config value ) to match the intended softness .\" , \" Density is markedly higher than fluffy snow because of the mud / water content - - - use roughly 2 -3 / cm ^3 (2000 -3000 kg / ^3) .\" , \" Make sure every list in all_queries contains ** one ** phrase because this is single - part object .\" ] } , }"
        },
        {
            "title": "C The Effects of Human Prior on PIXIEVERSE",
            "content": "PIXIEVERSE is labeled via VLMs using in-conext physics examples manually tuned by humans. condensed version of these in-context examples is provided in Fig. 12 and the full prompt in Listing 1. These examples align the VLMs physical understanding with humans. In our ablation result, we found that removing these examples significantly results as shown in Tab. 2. The main differences between PIXIEVERSE labeling and NeRF2Physics are 1. We use VLM to propose object-dependent segmentation while NeRF2Physics using LLM is essentially blind. Specifically, ur VLM actor proposes segmentation queries based on set of mutli-view images of the object as shown in Fig. 11. 2. We use semantic proposals (e.g., \"pot\", \"trunk\") instead of material proposals (e.g., \"leather\", \"stone\") like NeRF2Physics did. Computing similarity directly between material name and CLIP features yields inaccurate and noisy segmentation as shown in Fig. 10. This also limits the generality of the NeRF2Physics since one material type (e.g., elastic\") can only have fixed set of parameters in scene. In contrast, PIXIE enables spatially-varying parameter specification: the leaves and the trunk of tree while both belonging to the same elastic\" class can have vastly different young modulus, Poisson ratio and density as shown in Fig. 16. 3. We proposes multiple candidates (e.g., \"pot,leaves\" vs \"base,folliage\") and use VLM critic to select the best based on CLIP-based segmentation while NeRF2Physics does not have any selection mechanism. Figure 10 show the dramatic segmentation quality across different queries, highlighting the need for multiple candidates and selection. 4. We also provide manually tuned in-context physics parameter examples. These crucial differences contribute to much higher quality dataset labeling as shown in Tab. 2."
        },
        {
            "title": "D VLM As a Physics Judge",
            "content": "We utilize VLM to evaluate the realism of different candidate videos. The videos are scored on the scale 1-5, and an optional reference video and the prompt describing the video (e.g,. tree swaying in the wind\") is provided. We also use Cotracker [17] to annotate the videos with motion traces. The system prompt is provided in Fig. 15. Table 2: PIXIEVERSE Ablation. The effect of in-context physics examples on data quality. We include the executionability rate, which computes the fraction of times that physic simulation can be successfully run without numerical explosion, and the realism score judged by Gemini. Method Exec. Rate VLM Score W/ In-context Examples (Ours) W/o In-context Examples NeRF2Physics [42] 100.0% 62.5% 45.0% 4.83 0.09 1.34 0.30 1.09 0.28 27 You are physicsrealism judge for animation videos. You will be shown several candidate animations of the SAME 3D object responding to the SAME textual prompt that describes its intended physical motion. Your tasks: 1. Carefully watch each candidate animation. 2. Describe whats going on in the animation. 3. Evaluate how physically realistic the motion looks (05 scale). 4. Identify concrete pros / cons affecting the score (e.g. energy conservation errors, temporal jitter, incorrect response to gravity, static etc.). 5. Suggest specific improvements. 6. Pick the overall best candidate. Please output ONLY valid JSON with the following schema: { \"candidate_evaluations\": { \"candidate_0\": {\"description\": str, \"score\": float, \"pros\": str, \"cons\": str, \"suggested_improvements\": str}, \"candidate_1\": { ... }, \"candidate_2\": { ... } }, \"best_candidate\": \"candidate_i\", // the key of the best candidate \"general_comments\": str // any overall remarks (optional) } NOTE: ignore missing videos. Still return score for candidate_{idx} that are present. NOTE: to make your job easier, we have also annotated the video with the Co Tracker. Cotracker is motion tracker algorithm to highlight the moving parts in the videos. Pay close attention to the motion traces annotated in the videos to gain information on how the object is moving. Note that for objects that barely move, there will still be dots in the Co Tracker video, but the motion (lines) will be very short or nonexistent, indicating that the points are not moving. Cotracker can sometimes produce noisy traces so only use it as reference, and consider the motion of the object as whole, and other visual cues. Figure 15: VLM Evaluators System Prompt."
        },
        {
            "title": "E Model architecture",
            "content": "E.1 Overview We employ 3D UNet-based architecture for both discrete material segmentation and continuous material parameter regression. The architecture consists of two main components: (1) feature projector for dimensionality reduction, and (2) 3D UNet backbone for spatial processing. E.2 Feature Projector The feature projector is used when the input feature dimension differs from the conditioning dimension: Input features: The model supports three input modalities: RGB features: RN 3DHW CLIP features: RN 768DHW Occupancy features: RN 1DHW Projection: Features are projected to unified conditioning dimension of 32 channels using feature projector with hidden dimension of 128 (when input channels > 32). The projector consists of three layers of Conv3D, GroupNorm and SiLU activation. E.3 3D UNet Architecture We employ U-Net architecture [9, 34] operating on 3D feature grids of shape RN 32646464. The network follows standard encoder-decoder structure with skip connections, using base channel dimension of 64 and channel multipliers of [1, 1, 2, 4] across four resolution levels. The encoder begins with 3D convolution that projects the 32-dimensional input features to 64 channels. The encoder then processes features through four resolution levels, each containing three residual blocks. The first two levels maintain 64 channels while progressively reducing spatial dimensions from 643 to 323. The subsequent levels double the channel count at each downsampling step, reaching 128 channels at 163 resolution and 256 channels at 83 resolution. Downsampling between levels is performed using strided 3D convolutions with stride 2. At the bottleneck, the network processes the lowest resolution features through sequence of residual block, attention block, and another residual block, all operating at 83 spatial resolution with 256 channels. Note that in our implementation, attention blocks are disabled by setting attention resolutions to empty. The decoder symmetrically reverses the encoder path, utilizing skip connections from corresponding encoder levels. Upsampling is achieved through nearest-neighbor interpolation with scale factor of 2, followed by 3D convolution. Each decoder level matches the channel dimensions and number of residual blocks of its corresponding encoder level. Each residual block follows the formulation ResBlock(x) = + (x), where consists of layer normalization, LeakyReLU activation with negative slope 0.02, 3D convolution with kernel size 3, another layer normalization and activation, dropout, and final zero-initialized 3D convolution. When input and output channels differ, the skip connection employs 1 1 1 convolution for channel matching. The final output layer applies layer normalization, LeakyReLU activation, and 3D convolution that projects to either 8 channels for discrete material classification or 3 channels for continuous material parameter regression."
        },
        {
            "title": "F Additional Results",
            "content": "We visualize the physics predictions by our model in Fig. 16. Figure 17 breaks down the material accuracy across semantic classes of PIXIEVERSE between our PIXIE CLIP versus two ablated versions using RGB and occupancy input features. Figure 18 qualitatively compare the ablated methods on the real-world scenes. 29 Figure 16: PIXIE Prediction Visualization. PIXIE simultaneously recovers discrete material class, continuous Youngs modulus (E), Poissons ratio (ν), and mass density (ρ) with high degree of accuracy. For example, the model correctly labels foliage as elastic and the metal can as rigid, while recovering realistic stiffness and density gradients within each object. Figure 17: PIXIE Ablations Per-class Accuracy on synthetic scenes. CLIP features generalizes in synthetic scenes, outperforming RGB and occupancy on all classes. 30 Figure 18: PIXIEs Feature Type Ablation on Real Scenes. Replacing CLIP features with RGB or occupancy severely degrades the material prediction. Incorrect predictions such as leave mislaballed 31 as metal or Youngs modulus being uniform within an object are marked with question marks. This highlights the power of pretrained visual features in bridging the sim2real gap."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "University of Pennsylvania"
    ]
}