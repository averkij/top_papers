{
    "paper_title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models",
    "authors": [
        "Yuhao Wu",
        "Yushi Bai",
        "Zhiqiang Hu",
        "Juanzi Li",
        "Roy Ka-Wei Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation."
        },
        {
            "title": "Start",
            "content": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models Yuhao Wu1, Yushi Bai2, Zhiqiang Hu1, Juanzi Li2, Roy Ka-Wei Lee1 1Singapore University of Technology and Design, Singapore 2Tsinghua University, Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "Long-form text generation remains significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinkingthrough planning and refinement stagesinto the generation pipeline, guiding the model to follow more deliberate and cognitively grounded process akin to that of professional writer. Based on this framework, we construct supervised fine-tuning dataset to train 7B SuperWriter-LM. We further develop hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation. Our code & models are at: https://github.com/mozhu621/SuperWriter. 5 2 0 2 4 ] . [ 1 0 8 1 4 0 . 6 0 5 2 : r Figure 1: Current LLMs directly generate long text in single pass, while human writers follow an iterative process of thinking, outlining, writing, and refining to ensure coherence and quality. Equal contribution. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Effective long-form text generation remains fundamental challenge for advancing large language models (LLMs) [66, 45, 56]. Unlike short-form generation tasks, where LLMs have demonstrated remarkable success, generating extended sequences often lead to degraded coherence and logical consistency as the length grows. Previous studies on long-form writing [6, 35] typically employ LLMs to generate text in single pass without structured intermediate thinking or explicit planning. Consequently, the generated texts often appear plausible initially but lack sustained coherence and can exhibit logical contradictions over longer spans [17, 21, 41, 28, 59]. In practice, human writers rarely produce complex or lengthy documents in single uninterrupted attempt. Instead, they commonly utilize iterative processes, including outlining, drafting, reviewing, and revising their writing to maintain coherence and logical consistency [14, 7, 18], as shown in Fig 1. Moreover, such writing processes are often accompanied by extensive cognitive activities, such as planning, foreshadowing, and strategic structuringespecially in tasks like writing detective novels or academic papers [14]. Building on this observation, we propose SuperWriter-agent, novel agent pipeline explicitly designed to embed structured thinking paradigms within long-form text generation. By incorporating explicit intermediate cognitive steps, SuperWriter-agent simulates the human writing process through coordinated agent-based framework. Formally, SuperWriter-Agent generates training data through structured three-stage framework: Planning Writing Refining. In the Planning stage, two agents collaboratively reflect and outline key arguments, decompose complex ideas, and establish logical connections. During the Writing stage, each paragraph is composed based on the structured plan, incorporating explicit thinking steps into the content. In the Refining stage, the text is carefully reviewed and revised to ensure clarity and structural integrity. Unlike previous SFT datasets for long-form generation [6, 35], our approach explicitly segments the data into these three stages, each enriched with thinking-oriented supervision. To further guide the SuperWriter-LMs writing capabilities, we apply hierarchical Direct Preference Optimization by comparing pairs of finalized response [39] and propagate the feedback to each step through Monte Carlo Tree Search (MCTS). Through the integration of structured thinking signals and hierarchical feedback, SuperWriter-LM demonstrates notable improvements in fluency, coherence, and overall textual quality. Finally, we validate the effectiveness of our trained SuperWriter-LM through both human and automatic evaluations on the WritingBench benchmark [60]. Extensive ablation studies further underscore the critical role of structured thinking data and hierarchical Direct Preference Optimization (DPO) in enabling high-quality text generation. Our contributions are summarized as follows: We introduce SuperWriter-agent, an agent-based framework with structured PlanningWriting Refining workflow that simulates human writing processes by injecting intermediate thinking steps into long-form text generation. We construct thinking-supervised, stage-segmented dataset and train the SuperWriter-LM on it, then apply hierarchical DPO over final output comparisons to optimize each generation step. We empirically demonstrate the effectiveness of our approach, showing substantial improvements in fluency, coherence, and logical consistency compared to existing methods, as evaluated on WritingBench [60]."
        },
        {
            "title": "2 SuperWriter-Agent",
            "content": "While current LLM training corpora provide abundant supervision data reflecting intermediate thinking processessuch as mathematical reasoning and code generation [12]; however, they contain remarkably little data of this kind for writing tasks [48]. Most pretraining data [33, 57] consists of finished texts like articles and books, which largely omit the underlying process of planning, structuring, and thinking involved in writing. However, writingparticularly long-form writinginherently presents complex cognitive task, where explicit thinking steps are crucial for maintaining coherence, logical flow, and structural consistency. To address this gap, we propose the SuperWriter-agent (illustrated in Figure 2), framework designed to generate high-quality, thought-enriched supervised fine-tuning data for writing. The SuperWriter2 Figure 2: This figure illustrates three-stage agent framework for long-form generation. In Stage 1 (Plan), the framework proposes structured writing plan through discussions between AI commentators and writer. In Stage 2 (Write), the text is incrementally generated using thinker-writer collaboration, and in Stage 3 (Refine), checker and refiner iteratively improve the generated text to enhance coherence and quality. agent enables structured content generation through three coordinated stages: careful planning, targeted paragraph-level writing, and iterative refinement. This process explicitly embeds intermediate thinking signals into the writing pipeline, thereby enhancing the fluency, coherence, and narrative consistency of the generated text. 2.1 Stage 1: Plan Inspired by the widely adopted pedagogical technique in writing education known as the Story Workshop2 [46, 51, 47], Stage 1 of SuperWriter-agent begins with oral narration and iterative dialogue aimed at distilling and expanding initial ideas. In practice, this planning stage guides discussion agents to articulate core themes, central arguments, character background settings (for genres like fiction), and paragraph-level content structurescollectively forming the Background component. By systematically allocating word counts and associating key ideas with specific paragraph units, this step builds comprehensive and detailed outline for downstream writing. This structured process significantly enhances the overall coherence and organization of the text. With such framework in place, discussion agents can strategically develop and refine their ideas, resulting in more focused, coherent, and well-developed written outputs. Appendix A.1 provides the detailed prompt for the planning stage. 2.2 Stage 2: Write Motivated by recent advancements in reasoning-oriented LLMsnotably their remarkable scalability during inference in tasks such as mathematical reasoning and code generationwe build upon paradigms exemplified by reasoning models like OpenAI o1 and DeepSeek-R1. [29, 12]. These models typically engage in an explicit reasoning and planning phase prior to generating final responses, which has proven highly effective in enhancing output quality. Following this paradigm, we propose two-stage generation framework that simulates the process of thinking before writing, aiming to produce structurally coherent and logically consistent paragraphs. Appendix A.2 provides the detailed prompt for the actual paragraph write stage. Thinker Step: In this initial phase, the model refrains from generating surface-level text. Instead, it identifies and organizes key ideas, thematic elements, and logical structures relevant to the paragraph. This explicit reasoning process provides clear directional scaffold for subsequent text generation. 2The Story Workshop is writing instruction method developed at Columbia College Chicago. It emphasizes oral storytelling, collaborative discussion, and reflective dialogue to help writers generate ideas, structure narratives, and refine language through an interactive process. Writer Step: Building on the structured outline from the Thinker stage and incorporating the preceding paragraph (i.e., the (n 1)-th paragraph) as contextual input, the model proceeds to generate the current paragraph. This use of prior context ensures smooth transitions between paragraphs and contributes to the overall logical flow of the document. 2.3 Stage 3: Refine The final refinement stage goes beyond superficial edits by systematically evaluating the overall quality of the generated text and identifying specific paragraphs that require targeted revisions. Specifically, the refinement workflow consists of two key steps: Checker Step: The model conducts comprehensive assessment of each paragraph, identifying issues such as logical inconsistencies, unclear expressions, or deviations from the intended narrative structure. Editor Step: Based on the feedback from the Checker stage, the model performs precise and targeted modifications to improve textual accuracy, fluency, and structural coherence. This iterative and structured refinement process ensures that the final output not only accurately conveys the original intent and narrative objectives but also meets the rigorous standards expected in academic writing. Appendix A.3 provides the detailed prompts for paragraph refine stage."
        },
        {
            "title": "3 SuperWriter-LM",
            "content": "Following the development of the SuperWriter-agent, which introduces structured thinking and iterative discussion mechanisms to significantly enhance text generation quality, we are motivated to explore central research question: Can large language models, when guided by the thinking paradigm provided by the SuperWriter-agent data, internalize the ability to generate high-quality long-form content through substantially fewer inference stepsrather than relying on 30 to 40 separate agent calls per sample? To answer this question, we conduct targeted model training experiments. Our objective is not merely to extend output length, but to fundamentally improve coherence, relevance, and depth by distilling the agents structured thinking process directly into the model itself. In the following sections, we describe the construction of our high-quality training dataset and the strategic methodology for training LLMs to acquire and internalize the SuperWriter-agents structured thinking and writing capabilities. 3.1 SFT-Training Our training dataset is sourced from two real-world instruction tuning datasets: English-language and Chinese-language instructions from WildChat-1M [68] and LMSYS-Chat-1M [69]. To ensure the quality and relevance of the selected instructions for long-form writing tasks, we apply filtering process using the DeepSeek-R1-Distill-Qwen-32B model [12] (the filtering method is detailed in Appendix A.7). We generate SFT data using the SuperWriter-agent (powered by GPT-4o-2024-08-06 [31]) based on 4,000 filtered instructions. Each data instance follows structured pipeline: query outline draft final output. We explicitly segment this pipeline into three stages that align with the internal structure of the SuperWriter-agent: plan (query outline), write (outline draft), and refine (draft final output). Rather than training the model directly on full instruction-to-answer SFT pairs, we adopt stage-wise training (illustrated in Fig 3) for two key reasons. First, it better accommodate to real-world user workflows, where users may wish to review and revise intermediate results (e.g., the outline) before progressing to subsequent stages. Second, the complete outputs generated by the agent can be extremely longsome exceeding 100K tokensposing significant challenges for existing long-context models. By breaking the generation process into stages, we ensure that each training sample remains within 32K tokens, making it more tractable for current models. Putting them all together, each of the three stagesplan, write, and refinecontains exactly 4,000 data instances, resulting in total of 12,000 high-quality training data. During inference, the model performs the generation in three sequential stages to produce the final output. Figure 3: Format of the SFT dataset constructed by the SuperWriter-agent. The agent generates training data across three stagesplanning, writing, and refiningeach incorporating intermediate Think steps. During inference, the output of Stage 3 is used as the final answer. We train our SuperWriter-LM model based on Qwen2.5-7B [63], which supports context window of up to 128K tokens, making it well-suited for long-form text generation. To improve training efficiency, we adopt the packing-based training strategy with loss weighting, as proposed by Bai et al. [5]. Training is conducted on single node equipped with 8H800 80GB GPUs using DeepSpeed with ZeRO-3 and CPU offloading [42]. The training setup includes batch size of 32, learning rate of 2 105, and context window of 32K tokens. We train the model for four epochs in total. 3.2 Hierarchical DPO for Multi-Stage Generation Direct Preference Optimization (DPO) has demonstrated effectiveness in aligning policies directly with pairwise human (or proxy-model) preferences for single-pass generation tasks [40]. However, in our scenario, the writing process unfolds sequentially over three distinct stages: planning, drafting, and refinement. Applying conventional DPO exclusively to final outputs neglects the valuable preference signals inherently present in earlier stages. To address this gap, we introduce hierarchical multi-stage DPO framework that combines structured preference-data construction with systematic evaluation, inspired by process-annotation methods such as TPO [26], CHIP [15], and MathShepherd [54]. Specifically, as depicted in Figure 4, the writing process is structured as tree explored through Monte-Carlo Tree Search. Each path through this tree, labeled (i, j, k), corresponds sequentially to Stage-1 (plan i), Stage-2 (draft j), and Stage-3 (refinement k). We embed two key assumptions: well-structured initial plans lead to higher-quality draft (Stage-1 Plan Stage-2 Write) and wellrefined drafts typically yield better final outputs (Stage-2 Write Stage-2 Refine). Consequently, we back-propagate quality signals from leaf nodes (final outputs) upwards through intermediate stages, ensuring the policy learns from decisions at every level rather than only from final outcomes. Structured evaluation (Write-judge). To score the final output at each leaf, we introduce Writejudge3, six-dimension rubric (010 each) chosen from larger pool of twenty dimensions according to the instruction type (e.g. creativity vs. logical coherence). Following best practice to curb evaluation bias, we use the QwQ-32B model [53] to score every output three times under the same temperature settings and take the average. Then we propagate scores from the leaf nodes upward to construct DPO pairs as follows. Step 1: Leaf-score discretization. Let sijk denote the averaged raw evaluation score assigned to leaf (i, j, k), and let represent the total number of leaf nodes. We define the ranking and percentile 3Pipeline details and prompts appear in App. A.4. 5 Figure 4: The MCTS begins with 5 distinct writing plans, each leading to 4 written drafts, totaling 20 initial outputs. Each draft is then refined 3 times, resulting in 60 unique final outputs for single root. judge LLM ranks these final outputs, and the rankings are back-propagated through the refinement and planning stages. This scoring mechanism helps identify the most effective planning and writing strategies, thereby optimizing the overall writing process. for each leaf node as follows: rankijk = 1 + (cid:12) (cid:12){(p, q, r) spqr > sijk}(cid:12) (cid:12), πijk = rankijk . The percentile scores πijk are subsequently discretized into ordinal rewards rijk: rijk = 0, +2, πijk 1 6 , 1 +1, 2 6 < πijk 2 6 , 6 < πijk 4 6 , 6 < πijk 5 6 , 4 1, 2, πijk > 5 6 . Step 2: Reward aggregation. Ordinal rewards are propagated upwards through the tree by averaging across child nodes at each intermediate stage: ˆrij = ˆri = 1 {k} 1 {j} (cid:88) (cid:88) rijk (Stage 2 node) ˆrij (Stage 1 node) Step 3: Preference-pair construction. We harvest pairwise preferences at every hierarchy level, ensuring that each decision contributes to the optimization at every step. L1. Stage 1 (Plan). Let = arg maxi ˆri be the best plan and W1, W2 the worst two. P1 = {(B, W1), (B, W2)} L2. Stage 2 (Write). For the best two plans {B, S} (with being the second best), P2(i) = (cid:0)arg max ˆrij, arg min (cid:1), ˆrij P2 = (cid:91) P2(i). L3. Stage 3 (Refine). For each (i, j) P2, P3(i, j) = (cid:0)arg max sijk, arg min sijk (cid:1), P3 = (cid:91) (i,j) P3(i, j). The complete preference dataset is therefore DDPO = P1 P2 P3. 6 Models Proprietary LLMs ChatGPT-4o-latest o1-Preview Claude-3.5-Sonnet Gemini-1.5-Pro Qwen-Max Open-source LLMs Deepseek-R1 Deepseek-V3 Mistral-Large-Instruct Qwen-2.5-72B-Instruct Qwen-2.5-7B-Instruct Llama-3.3-70B-Instruct Llama-3.1-8B-Instruct Capability-enhanced LLMs Suri-I-ORPO LongWriter-8B Writing-Model-Qwen-7B Writing-Model-Llama-7B SuperWriter-LM (ours) Avg 8.16 8.15 7.71 7.78 8.37 8.55 7.95 7.64 7.90 7.43 7.01 6.35 4.97 7.91 8.49 8.49 8. Languages Domains Requirements ZH EN D1 D2 D3 D4 D5 D6 C R2 R3 8.3 8.1 7.7 7.8 8. 8.7 8.0 7.6 8.0 7.3 6.7 5.7 4.4 7.9 8.6 8.6 8.6 8.1 8.2 7.7 7.7 8.3 8.5 7.9 7.7 7.9 7.5 7.3 6.9 5.5 7.9 8.4 8.4 8.5 8.1 8.0 7.6 7.7 8. 8.5 7.9 7.7 8.0 7.7 7.0 6.6 5.6 8.0 8.4 8.5 8.6 8.1 8.1 7.5 7.5 8.3 8.5 7.8 7.6 7.8 7.4 6.9 6.4 5.3 8.1 8.4 8.4 8.7 8.2 8.2 7.6 7.8 8. 8.6 8.0 7.8 8.1 7.6 7.0 6.1 5.0 8.1 8.6 8.6 8.7 8.1 8.2 7.7 7.9 8.4 8.6 7.8 7.3 7.7 6.9 6.8 6.0 4.1 7.7 8.4 8.4 8.2 8.4 8.4 7.9 8.0 8. 8.7 8.2 7.9 8.2 7.8 7.3 6.7 5.0 8.1 8.6 8.6 8.7 8.1 8.1 8.0 7.9 8.4 8.6 8.0 7.6 7.8 7.3 7.3 6.6 5.1 7.6 8.5 8.5 8.2 8.3 8.2 7.9 7.9 8. 8.7 8.1 7.7 8.0 7.5 7.1 6.4 4.8 7.9 8.6 8.6 8.4 8.7 8.6 8.5 8.6 8.7 8.9 8.6 8.2 8.3 7.9 7.8 7.0 5.2 8.2 8.8 8.8 8.5 8.2 8.2 7.7 7.9 8. 8.6 8.0 7.7 8.0 7.6 7.1 6.4 5.0 8.1 8.5 8.5 8.6 8.9 8.8 8.5 8.8 9.0 9.0 8.9 8.7 8.8 8.6 8.2 7.6 5.4 8.8 9.0 8.9 8.4 8.2 8.2 7.9 7.9 8. 8.6 8.0 7.7 7.9 7.4 7.0 6.3 4.5 7.7 8.5 8.5 8.0 8.3 8.2 8.0 8.0 8.5 8.7 8.2 7.9 8.0 7.5 7.2 6.4 4.0 7.7 8.6 8.5 6.3 Table 1: WritingBench performance of different LLMs across 6 domains and 3 writing requirements evaluated with our critic model (scale: 1-10). The six domains include: (D1) Academic & Engineering, (D2) Finance & Business, (D3) Politics & Law, (D4) Literature & Art, (D5) Education, and (D6) Advertising & Marketing. The three writing requirements assessed are: (R1) Style, (R2) Format, and (R3) Length. Here, indicates category-specific score. Optimization objective. Finally, we optimize the policy πθ using the standard DPO loss: (cid:104) log σ(cid:0) β [ sθ(x, y+) sθ(x, y)](cid:1)(cid:105) LDPO = E(x,y+,y)DDPO . Using the above method, we obtained DPO preference dataset and employed 360-LLaMAfactory [20] to continue context-parallel DPO training of the supervised fine-tuned SuperWriter-LM with batch size of 32 and learning rate of 1 106."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Setup Inference Setup. We utilize the SGLang [71] system, which optimizes key-value (KV) cache memory for efficient large-scale inference. This system is crucial for handling long-form generation efficiently, reducing memory overhead, and maximizing inference throughput. Inferences are performed using BFloat16 precision on 8NVIDIA H800 GPUs. This setup ensured consistency and efficiency in the inference process. Benchmark Setup. We conduct our experiments on two datasets. The first is WritingBench [60], comprehensive benchmark designed to evaluate large language models across six major writing domains and 100 sub-domains, encompassing creative, persuasive, informational, and technical tasks. The benchmark comprises 1,200 real-world writing prompts, each paired with five instance-specific evaluation criteria. WritingBench adopts query-dependent evaluation framework and leverages Qwen2.5-7B critic model fine-tuned on 50K human-labeled samples to produce fine-grained assessments of style, format, and length, achieving an 83% agreement rate with human judgments. The second dataset includes approximately 200 real-world user instructions manually curated by us. We evaluate model performance through pairwise comparisons across several baselines (Win-rate), including LongWriter-8B [6], DeepSeek-R1-Distill-Qwen-7B [12], Writing-Model-Qwen-7B [60]4, 4This refers to the SFT model obtained via rejection sampling using the WritingBench critic model. Figure 5: This figure presents eight donut charts comparing the win rates of our model against seven different baselines. The win-rate in the 8th chart is evaluated by human (Ours vs. Writing-ModelQwen-7B) while the rest are evaluated by GPT-4.1. Each chart shows the proportions of wins, losses, and ties, with the central percentage indicating the overall win rate. When calculating the win rate, tie is counted as 0.5 win for both sides. Qwen3 [52], LLaMA-4-Scout [1], DeepSeek-V3 [13], and DeepSeek-R1 [12]. All models are evaluated using consistent decoding configurations (temperature = 0.6, top-p = 0.95), and win rates are calculated to quantify comparative performance. 4.2 WritingBench Result We evaluate our 7B sized SuperWriter-LM model on the WritingBench benchmark. As shown in Tab 1, SuperWriter-LM achieves an overall performance (Avg) of 8.51second-best among all models, closely following DeepSeek-R1 [12]. It demonstrates strong capabilities in both Chinese8.6 and English8.5, even matching DeepSeek-R1 in English performance. Across different domains, SuperWriter-LM achieves the highest scores in three major areas: (D1) Academic & Engineering8.6, (D2) Finance & Business8.7, (D3) Politics & Law8.7 and (D5) Education8.7, even slightly outperforming the DeepSeek-R1 model. Additionally, SuperWriter-LM satisfies various special writing requirements, except the length_C setting. This discrepancy is primarily due to the nature of agent-generated data, which tends to produce longer outputs even for short-text tasksan issue that does not affect long-form generations. 4.3 Win-Rate Result WritingBench adopts critic model to evaluate model outputs by assigning scores (ranging from 1 to 10) across 45 distinct dimensions. However, this evaluation approach has several limitations. First, due to the relatively small size of the critic model, it may be vulnerable to some word/sentence hacking. Second, we observe that WritingBench primarily focuses on formal or professional writing taskssuch as summaries and reportswhereas our analysis of real-world data from WildChat [68] and LMSYS-Chat-1M [69] reveals that creative writing (e.g., storytelling, fiction) constitutes significant portion of user queries. To address these concerns, we adopt more direct and interpretable evaluation metric: win-rate. We evaluate model performance on nearly 200 real-world user queries collected from the aforementioned datasets. For each query, responses are generated by SuperWriterLM and six baseline models. LLM Evaluation. For automatic evaluation, we adopt LLM-as-a-judge [4, 70] to perform pairwise comparison using GPT-4.1-2025-04-14 [30]. To mitigate positional bias, we conduct two evaluations per pair by swapping the response order: Evaluation_Prompt + + and Evaluation_Prompt + + A. Based on the two judgments, we categorize the results into win, loss, or tie and compute win-rate results5. 5Evaluation prompts are provided in Appendix A.5. 8 As shown in Figure 5, SuperWriter-LM demonstrates substantial performance lead among models of the same size (models 1, 2, and 3). Furthermore, in comparisons against larger-sized models (models 4, 5, 6, and 7), SuperWriter-LM remains competitive and, in some cases, even slightly outperforms state-of-the-art LLMs. Taken together, these results suggest that SuperWriter-LM sets new performance benchmark among 7B-scale models and even challenges the capabilities of existing state-of-the-art systems. We also present several case studies in Appendix A.6. Human Evaluation. To mitigate potential inaccuracies in automatic evaluation, we conduct an human supplementary assessment on approximately 200 real-world user queries, comparing SuperWriter-LM with Writing-Model-Qwen-7B [60]. For each query, three independent annotators with undergraduate degrees. were tasked with evaluating and determining the preferred response, with outcomes categorized as win, loss, or tiefollowing the same standard as the automatic evaluation. The aggregated results are shown in Figure 5 (8), and SuperWriter-LM demonstrates stronger performance under human judgment. However, due to the annotators tendency to assign tie when the differences between two responses are subtle, the overall win rate appears slightly lower. 4.4 Ablation Study Avg Model ZH EN Qwen2.5-7B-Instruct + SuperWriter final output + Three-Stage + Hierarchical DPO Finally, we conduct an ablation study comprising four different setups evaluated on the WritingBench benchmark. The first setup uses the base model, Qwen2.5-Instruct, as the performance baseline. The second setup, SuperWriterfinal-answer, takes the user query as input and produces the final output from the Stage-3 refine step of the SuperWriter-agentthis is onepass generation without any explicit thinking process and achieves an average score of 8.21 (ZH: 8.3, EN: 8.2). The third setup, +Three-Stage, corresponds to our SFT-trained model, which explicitly performs planning, drafting, and refining in chained, multi-stage manner, incorporating structured thinking. This setup further improves performance to 8.47 (ZH: 8.5, EN: 8.4). The final setup is the full model further enhanced with our hierarchical DPO optimization, reaching the highest score of 8.51 (ZH: 8.6, EN: 8.5). As shown in Table 6, each additional component leads to consistent performance improvements, demonstrating the effectiveness of our proposed approach in structured writing tasks. Figure 6: Performance comparison on WritingBench [60] Avg, ZH and EN. 7.43 8.21 8.47 8.51 7.5 8.2 8.4 8.5 7.3 8.3 8.5 8."
        },
        {
            "title": "5 Related Work",
            "content": "Long-context Language Models Recent research on long-context language models (LLMs) has primarily focused on extending the input context length, enabling models to process substantially longer inputs. Approaches in this direction generally fall into two categories. The first involves zeroshot techniques, aiming to expand the models receptive field without additional training [19, 61, 67, 22, 2]. The second category encompasses fine-tuning-based methods, where models undergo training on extended sequences to explicitly enhance their ability to handle long contexts [8, 34, 62, 9, 5, 16]. However, existing studies have predominantly concentrated on input expansion, often overlooking the critical necessity for corresponding output capabilities. Recent empirical findings by Bai et al. [6] and Quan et al. [36] have shown significant mismatch between maximum input lengths (over 100K tokens) and achievable output lengths (approximately 2K words). Long-form Text Generation Recent advancements emphasize architectural innovations and specialized training paradigms [43, 37, 27, 25]. For instance, Re3 employs recursive prompting strategy, effectively maintaining narrative coherence in extended storytelling tasks [64]. Other approaches, such as DOC [65] and hierarchical outlining methods [55], leverage structured task decomposition to improve overall narrative structure and content coherence. Recently, personalization has emerged as another crucial dimension, with models like LongLaMP [23] and reasoning-enhanced self-training approaches [44] adapting outputs according to individual user preferences. Additionally, long-form question-answering techniques have gained attention for addressing complex and detailed queries with comprehensive responses [11, 49, 24, 50]. Despite these advancements, existing methods often 9 suffer from limitations such as inconsistent coherence, lack of theoretical grounding, and challenges in generating consistently high-quality content at large scales [58, 37]. In response, our work addresses these limitations by proposing theoretical agent framework for long-form writing, accompanied by novel training methodologies designed specifically to enhance the models capability in generating coherent and extended texts."
        },
        {
            "title": "6 Conclusion",
            "content": "SuperWriter addresses the challenge of long-form text generation by introducing structured writing processplanning, writing, and refiningguided by the SuperWriter-agent. This approach teaches the model to think before writing and produces high-quality supervision signals. Combined with hierarchical DPO strategy, the model learns to align its output across all writing stages. Experiments show strong results: SuperWriter-LM outperforms all same-size models on WritingBench and even exceeds the 671B DeepSeek-R1 model [12] in key domains. It also wins over 98% of real-user comparisons against top open-source baselines. These results confirm the value of multi-stage generation and structured preference learning for improving writing quality."
        },
        {
            "title": "7 Limitation",
            "content": "While SuperWriter-LM demonstrates strong performance in long-form text generation, several limitations remain: (1) Inference latency. Compared to single-pass generation models such as LongWriter [6] or Suri [35], our method incurs additional inference time due to its three-stage Framework. Although this is significantly more efficient than multi-round agent-based pipelines (e.g., requiring 3040 calls per output), the structured plan write refine process still requires three sequential forward passes, which may increase user-perceived latency in real-world applications. (2) Model scale. Our current implementation is built on 7B parameter backbone (Qwen2.5)[38], which strikes balance between performance and cost. However, this moderate scale may limit the models internal world knowledge, particularly in knowledge-intensive or specialized writing scenarios (e.g., legal, medical, and scientific domains). In our qualitative analysis, some outputs exhibited shallow factual grounding or subtle reasoning errors. (3) Lack of online reinforcement learning. Lack of online reinforcement learning. Our alignment stage relies solely on offline Direct Preference Optimization (DPO) [40], trained from static preference pairs. While effective, this setup lacks the adaptivity of online Reinforcement Learning from Human Feedback (RLHF) [10], which allows models to continually refine outputs through exploration. The key bottleneck is the high rollout cost when applying general-purpose reward models to long outputs. Designing scalable, low-latency reward models or reward distillation methods [3, 32] for long-form tasks is thus promising direction for future research."
        },
        {
            "title": "References",
            "content": "[1] Meta AI. multimodal ai llama-4-multimodal-intelligence/. The llama 4 herd: innovation, April 2025. The beginning of new era of natively URL https://ai.meta.com/blog/ [2] Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463, 2024. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [4] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-anexaminer. Advances in Neural Information Processing Systems, 36:7814278167, 2023. 10 [5] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: recipe for long context alignment of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13761395, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.74. URL https://aclanthology.org/2024.findings-emnlp. 74. [6] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. [7] Anne Becker. Revision: History, theory, and practice, pages 2549, 2006. [8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [9] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [10] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/ abs/1706.03741. [11] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Proc. of NAACL, pages 45994610, 2021. [12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [13] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. [14] Flower. cognitive process theory of writing. Composition and communication, 1981. [15] Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, and See-Kiong Ng. Chip: Cross-modal hierarchical direct preference optimization for multimodal llms. arXiv preprint arXiv:2501.16629, 2025. [16] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024. [17] Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. Plan, write, and revise: an interactive system for open-domain story generation, 2019. URL https://arxiv.org/ abs/1904.02357. [18] Allan Gollins and Dedre Gentner. framework for cognitive theory of writing. In Cognitive processes in writing, pages 5172. Routledge, 2016. [19] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [20] Shousheng Jia Haosheng Zou, Xiaowei Lv and Xiangzheng Zhang. 360-llama-factory, 2024. URL https://github.com/Qihoo360/360-LLaMA-Factory. [21] Xinyu Hua and Lu Wang. Pair: Planning and iterative refinement in pre-trained transformers for long text generation, 2020. URL https://arxiv.org/abs/2010.02301. [22] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024. [23] Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. Longlamp: benchmark 12 for personalized long-form text generation, 2024. URL https://arxiv.org/abs/2407. 11016. [24] Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae Lee. QASA: Advanced question answering on scientific articles. In Proc. of ICML, pages 1903619052, 2023. [25] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. Teach llms to personalize - an approach inspired by writing education. ArXiv, 2023. [26] Weibin Liao, Xu Chu, and Yasha Wang. TPO: Aligning large language models with multibranch & multi-step preference trees. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=O0sQ9CPzai. [27] Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, and Rada Mihalcea. Taskadaptive tokenization: Enhancing long-form text generation efficacy in mental health and beyond. In Proc. of EMNLP, pages 1526415281, 2023. [28] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey, 2024. URL https:// arxiv.org/abs/2402.06196. [29] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [30] OpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/ gpt-4-1/. Accessed: 2025-05-02. [31] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, 13 Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [33] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Data, data everywhere: guide for pretraining dataset construction. arXiv preprint arXiv:2407.06380, 2024. [34] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. [35] Chau Minh Pham, Simeng Sun, and Mohit Iyyer. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371, 2024. [36] Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate long texts. arXiv preprint arXiv:2410.23933, 2024. [37] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, and Kai Chen. Hellobench: Evaluating long text generation capabilities of large language models, 2024. URL https://arxiv.org/abs/2409.16191. 14 [38] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9. [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [41] Zeeshan Rasheed, Muhammad Waseem, Kai Kristian Kemell, Aakash Ahmad, Malik Abdul Sami, Jussi Rasku, Kari Systä, and Pekka Abrahamsson. Large language models for code generation: The practitioners perspective, 2025. URL https://arxiv.org/abs/2501. 16998. [42] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. [43] Alireza Salemi, Julian Killingback, and Hamed Zamani. Expert: Effective and explainable evaluation of personalized long-form text generation, 2025. URL https://arxiv.org/ abs/2501.14956. [44] Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Tao Chen, Zhuowan Li, Michael Bendersky, and Hamed Zamani. Reasoning-enhanced self-training for long-form personalized text generation, 2025. URL https://arxiv.org/abs/2501.04167. [45] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants, 2025. URL https://arxiv.org/abs/2501.04227. [46] John Schultz. The story workshop method: Writing from start to finish. College English, 39(4): 381389, 1977. [47] Betty Sheflett. Story workshop as method of teaching writing. College English, 35(2):141160, 1973. [48] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, and Nolan Dey. version Hestness, plicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. SlimPajama-627B. https://huggingface.co/datasets/cerebras/ Joel 627B token cleaned and deduhttps://www.cerebras.net/blog/ Jacob Steeves, of RedPajama. SlimPajama: URL [49] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-form answers. In Proc. of EMNLP, pages 82738288, 2022. [50] Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. ProxyQA: An alternative framework for evaluating long-form text generation with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68066827, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.368. URL https://aclanthology.org/2024.acl-long.368. 15 [51] Pospelova Tatiana. The collaborative discussion model: Developing writing skills through prewriting discussion. Journal of Language and Education, 7(1 (25)):156170, 2021. [52] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https: //qwenlm.github.io/blog/qwen2.5/. [53] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [54] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/ 2024.acl-long.510/. [55] Qianyue Wang, Jinwu Hu, Zhengping Li, Yufeng Wang, daiyuan li, Yu Hu, and Mingkui Tan. Generating long-form story using dynamic hierarchical outlining with memory-enhancement, 2024. URL https://arxiv.org/abs/2412.13575. [56] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. Autosurvey: Large language models can automatically write surveys. In Proc. of NeurIPS, 2024. [57] Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. [58] Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee. Longgenbench: Benchmarking long-form generation in long context llms. arXiv preprint arXiv:2409.02076, 2024. [59] Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, and Roy KaWei Lee. Shifting long-context llms research from input to output, 2025. URL https: //arxiv.org/abs/2503.04723. [60] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. Writingbench: comprehensive benchmark for generative writing, 2025. URL https://arxiv.org/abs/2503.05244. [61] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [62] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, 2024. [63] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [64] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Proc. of EMNLP, pages 43934479, 2022. 16 [65] Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. DOC: Improving long story coherence with detailed outline control. In Proc. of ACL, pages 33783465, 2023. [66] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Planand-write: Towards better automatic storytelling, 2019. URL https://arxiv.org/abs/ 1811.05701. [67] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. SoararXiv preprint ing from 4k to 400k: Extending llms context with activation beacon. arXiv:2401.03462, 2024. [68] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. [69] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023. [70] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [71] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs. arXiv preprint arXiv:2312.07104, 2023. doi: 10.48550/arXiv.2312.07104. URL https://arxiv.org/abs/2312.07104."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Stage-1 Plan Prompt This appendix provides brief overview of the prompt modules used in SuperWriter-Agent Stage-1 Plan. There are total of 6 modules, each serving specific role in the writing and evaluation pipeline: BrainStorm: Generates an initial in-depth thinking process to analyze and develop preliminary writing plan for given task, ensuring comprehensive and thorough design. BrainStorm Review: Critically evaluates the task design, raising questions about potential flaws, ambiguities, or unclear requirements to refine the tasks overall logic and readability. BrainStorm Refine: Integrates reviewer feedback into the task design by applying editorial judgment to revise or completely rewrite the task, ensuring it is rigorous and well-structured. Outline: Constructs structured article outline based on the task design, including the estimated word count per paragraph and logical framework to guide the writing process. Check outline: Acts as reviewer evaluating the logical structure and completeness of the outline, pointing out logical gaps or missing elements to ensure it aligns with the intended objectives. Refine outline: Edits and improves the previously generated outline based on reviewer feedback, ensuring clarity, completeness, and alignment with the writing objectives. The following sections provide the detailed prompt templates and usage notes for each module. Think template 1. Define the Purpose and Type of Writing - Purpose: Clearly establish the objective of the piece (e.g., to inform, persuade, or inspire), setting the tone and direction accordingly. - Type: Choose an appropriate writing style (e.g., argumentative, expository, or business writing) that aligns with the intended format and structure. Clearly identify the genre and describe its stylistic characteristics. 2. Plan Content and Structure - Key Points: Outline the essential information to ensure clear and focused topic. - Structure: Develop coherent framework that maintains logical flow throughout the piece. 3. Characters and Plot (for Narrative Writing) - Character Development: Define the traits and motivations of all characters. Provide detailed descriptions, including specifics such as names, gender, and relationships. - Plot Development: Establish pivotal plot points and emotional cues to drive the narrative. Offer detailed explanation based on the chosen structure. 4. Additional Guidelines - Formatting Requirements: Automatically select an appropriate output format (e.g., Markdown, bullet points) based on content and presentation needs to enhance visual clarity and appeal. - Other Key Elements: Include any genreor task-specific components. For instance, in summaries, step-by-step approach is sufficient due to their simpler logic. BrainStorm You are professional writer responsible for creating an initial design based on the thinking template below. Please use the template to thoroughly analyze and develop detailed preliminary plan for the task topic. Carefully examine each point in the template to ensure all aspects are fully considered, providing comprehensive and complete writing plan with no vague information. At the same time, make sure the structure remains manageable and not overly complex. Thinking Template: think_template. At this stage, you do not need to provide an outlinejust complete the in-depth thinking required for task design. Please respond: 18 BrainStorm Review You are critical reviewer, specializing in identifying issues and flaws in task design. Please evaluate the following task design and raise at least two questions to ensure it meets the requirements of topic. These questions should be carefully considered and clarified before the actual writing begins, highlighting any logical flaws, ambiguities, or areas that might confuse the reader. Depending on the type of task, you will analyze it from various anglesfor example, whether speech is engaging, story is original, or an academic paper is rigorous. Task Topic: topic. Task Design: task_output. Provide detailed questions along with specific and practical suggestions for improving the task design: BrainStorm Refine You are an experienced editor responsible for revising the task based on the reviewers feedback. Original Task Design: task_output. Feedback: feedback. Using this feedback, provide detailed and specific revision of the current task design. Apply your own judgment rather than simply implementing the feedback verbatim to address all identified issues. If necessary, rewrite the original task design entirely. Please provide the revised task design: Outline Thoroughly understand the task design of task_define_result. Based on the task design, determine suitable article title and generate detailed, structured outline. The outline should not exceed 20 paragraphs, and each paragraph must include detailed description along with specific word count. The total word count should be appropriate for the task but must not exceed 16,000 words. Allocate the total word count to each paragraph according to the complexity of the task. Then, provide the full outline along with the word count for each paragraph. Ensure that each paragraph description includes the expected content, maintains clear logic, and aligns with the users objective: topic. Check outline You are reviewer. Your task is to evaluate the following outline and assess whether its logical structure is complete and aligned with the intended objectives. Task Design: task_define_result Outline: outline Please provide detailed feedback, pointing out any logical gaps or missing elements. Refine outline You are professional outline editor. Based on the following feedback, revise the outline to ensure it includes all necessary components: Feedback: check_output Current Outline: outline. Please provide the revised article title and the updated outline content: A.2 Stage-2 Write Prompt This appendix provides an overview of the prompt modules used in SuperWriter-Agent Stage-2 Write. There are total of 2 modules, each serving specific role in the writing pipeline: Write-thinker: This module is designed to guide the planning phase for each paragraph. It takes the structured outline, the previous paragraphs, and the key point for the current paragraph as input. The module then prompts the model to develop detailed thought process covering the paragraphs purpose, structure, transitions, details and examples, language style, and other relevant aspects. It ensures that each paragraph is planned thoroughly before the actual writing begins. Write: This module transforms the thought process from the Write-thinker stage into the actual written paragraph. It uses the same structured outline, previous paragraphs, key point, and the generated thought process as guidance to produce coherent and logically sound paragraph. The output is formatted with delimiters (e.g., $$content$$) to clearly separate the paragraph from other text. The following sections provide the detailed prompt templates and usage notes for each module. Write-thinker You are writing expert skilled in thoughtful planning before generating each paragraph. Outline: outline Previous Paragraphs: previous_paragraphs Key Point for the Current Paragraph: key_point Please carefully develop writing plan for the new paragraph. You may consider the following aspects: 1. Purpose: What is the main objective of this paragraph? What message or emotion should it convey? 2. Structure: How should the content of this paragraph be organized? What logical sequence would best ensure clarity and coherence, and how will it connect tightly with the previous content? 3. Transitions: How will this paragraph naturally link to the one before it? Are there specific transition sentences or bridging techniques that can be used? 4. Details and Examples: What details, facts, or examples are needed to support the main idea? How should these be arranged for maximum impact? 5. Language Style and Techniques: What kind of language style should be used to achieve the goal? Are there rhetorical devices (such as metaphors or analogies) that could enhance the paragraphs impactwhile still being clear, readable, and easy to understand for the audience? 6. Markdown Format: Use Markdown to structure the output neatly, including headings, bullet points, or bold text to improve readability. Based on the outline and the key point for this paragraph, construct detailed writing plan. Add any other relevant considerations as needed, and keep the word count requirements in mind. Only the thought process behind the paragraph is needed, not the paragraph itself. Writer You are an exceptional writing expert, skilled at completing writing tasks in clear, accessible, and logically sound manner. Outline: outline Previous Paragraphs: previous_paragraphs Key Point for the New Paragraph: key_point Thought Process: thought_response Based on the thought process above and the existing paragraphs, write the next paragraph, ensuring it meets the word count requirement. Only provide the full content of the new paragraph. Enclose the paragraph content with $$content$$. A.3 Stage-3 Refine Prompt In Stage-3, the system focuses on reviewing and revising the paragraphs to ensure high quality and alignment with the overall document structure. This stage comprises two main steps: Paragraph Review: This module acts as meticulous reviewer. It analyzes the entire document, ensuring that each paragraph is logically consistent, complete, and coherent within the context of the overall text. The reviewer provides at least two specific revision suggestions to address logical issues, missing details, or awkward transitions. Paragraph Modification: This module takes the reviewers feedback and applies it to revise the paragraph. It strictly follows the feedback to ensure all identified issues are resolved. The 20 revised paragraph is provided in isolation, enclosed with delimiters (e.g., $$content$$) to clearly separate it from other text. Together, these modules ensure that the document achieves high logical coherence, completeness, and clarity throughout. Stage-3 Refine: Paragraph Review As meticulous document reviewer, your task is to carefully read the entire document, understand its overall structure and logic, and then conduct detailed review of paragraph idx+1, providing revision suggestions: combined_document When reviewing paragraph idx+1, you may refer to the following points: 1. Logical Consistency: Is this paragraph logically consistent with the rest of the document? Are there any illogical transitions or abrupt shifts? 2. Completeness: Does this paragraph provide enough information to support its main idea? Are there any important missing details? 3. Coherence: Does this paragraph connect smoothly with the surrounding paragraphs? Would transitional sentences help improve the flow? Please provide at least two specific improvement suggestions. Focus on offering detailed revision suggestions for paragraph idx+1. Only provide suggestions and possible ways to improvedo not rewrite the paragraph itself. Suggestions for improvement: Stage-3 Refine: Paragraph Modification As text editor, your task is to revise the paragraph based on the following feedback: review_feedback Original Paragraph: updated_document[idx] Ensure the revision strictly follows the specific suggestions in the feedback. Only provide the revised paragraph. Enclose the paragraph content with $$, like: $$content$$ Revised Paragraph: A.4 Evaluation Prompt for Hierarchical DPO To support structured preference data construction for Direct Preference Optimization (DPO), we design and apply sequence of modular prompts. Each serves specific role within the evaluation pipeline. Below is an overview of their usage: The evaluation pipeline comprises the following steps: 1. Rubric Definition (evaluation_criteria): Defines the complete set of General and Special evaluation dimensions. This rubric is reused across all queries. 2. Criterion Selection Schema (format_query): Specifies the JSON format for selecting six criteria (three General, three query-relevant Special) and rewriting their Definitions and Standards to match the specific query context. 3. Criterion Selection Prompt: Combines the rubric and schema to instruct the model to select and customize criteria. The output is JSON object referred to as evaluate_standard. 4. Scoring Format Schema (format_eval): Specifies the expected evaluation output format: for each selected criterion, the model must return an Analysis string and numeric Score. 5. Final Scoring Prompt: Provides the model with query, its generated result, the customized evaluate_standard, and the format_eval schema. The model performs criterion-wise evaluation and outputs structured JSON. Outcome: This pipeline yields structured, query-specific evaluations that are interpretable, machineparsable, and suitable for training with DPO loss. 21 evaluation_criteria Prompt Evaluation Criteria 1. General Criteria (Applicable to All Genres) 1.1 Relevance Definition: How well the content matches the users request, and whether it addresses the intended purpose or topic. Standards: 10: Fully aligned with the users needs, highly relevant to the request. 79: Mostly relevant, with some minor deviations or less-than-perfect alignment. 46: Partially relevant, with the majority of the content not matching the users request. 13: Completely irrelevant, fails to meet the users needs. 1.2 Coherence Definition: The clarity of the structure, and the logical flow of ideas and transitions between paragraphs and sentences. Standards: 10: Clear structure, strong logical progression, and smooth flow of content with natural transitions. 79: Mostly coherent, with minor lapses or jumps in logic. 46: Structure is somewhat unclear, and there are noticeable gaps or jumps in logic. 13: Disorganized and confusing, with no clear structure or logical connections. 1.3 Clarity Definition: How clear and easy the content is to understand, and whether it includes sufficient detail. Standards: 10: Clear expression, rich in detail, and easy to understand. 79: Mostly clear, but may have some lengthy or unclear parts. 46: Expression is somewhat muddled, lacks detail, and is difficult to understand. 13: Extremely vague, minimal information, and difficult to comprehend. 2. Special Criteria (Applicable to Specific Genres) 2.1 Creativity and Uniqueness Definition: Whether the content is innovative, offering new perspectives, or showcasing original expression. Standards: 10: Highly creative and unique, presenting entirely new or unconventional ideas or perspectives. 79: Creative, but some parts are more conventional or lack originality. 46: Limited creativity, mostly traditional content with little innovation. 13: Lacks creativity, offering conventional or uninspired content. ..... format_eval Prompt The final output should be in JSON format, structured as follows: json { \"Criterion 1\": { \"Analysis\": \"Analysis content\", \"Score\": }, \"Criterion 2\": { \"Analysis\": \"Analysis content\", \"Score\": }, ... } format_query Prompt After think give final answer in JSON format. Structured as follows: json { \"Selected Criterion 1\": { \"Definition\": \"Criterion definition\", \"Standards\": \"Scoring standards for the query, give simple rubric from 1 to 10, 10: ..., 7-9: ..., 4-6: ..., 22 1-3: ...\" }, \"Selected Criterion 2\": { \"Definition\": \"Criterion definition\", \"Standards\": \"...\" }, ... } prompt Please refer to the evaluation criteria outlined below: {evaluation_criteria} Task: You are tasked with evaluating the query: {query}. From the \"Special Criteria\" section, select 3 relevant criteria, and from the \"General Criteria\" section, select all criteria (Relevance, Coherence, Clarity), for total of 6 criteria. Be sure to: 1. Think step-by-step about why each criterion is relevant to the query. 2. Think step-by-step through the query and how each criterion applies. 3. Provide brief analysis for each selected criterion on how it applies to the query. 4. Integrate the above reasoning into the Definition and Standards sections of each criterion. {format_query} prompt ### Query: {query} ### Result: <start> {clean_res} <end> ### ensure_ascii=False)} Based on the provided info, perform rigorous evaluation. {format_eval} Evaluation Standard: {json.dumps(evaluate_standard, A.5 Evaluation Prompt for Win-Rate Judgment SYSTEM_PROMPT for GPT-4.1 Win-Rate Evaluation Please act as an impartial judge and evaluate the quality of the written responses provided by two AI assistants to the users writing prompt below. You will be given Assistant As response and Assistant Bs response. Your job is to determine which assistants writing is superior. Evaluate them on the following criteria: 1. Relevance and Completeness: Does the assistant fully respond to the writing prompt? Does the length meet the users query expectations? Is the content relevant to the topic, and does it provide sufficient depth, length, and detail, rather than drifting off-topic or simplistic? 2. Writing Quality: Evaluate whether the assistants writing is clear, fluent, and free of obvious grammatical errors. The overall quality of the writing is high, with elegant. 3. Creativity and Originality: If applicable, assess the creativity of the response. Does the assistant offer fresh perspectives, unique insights, or demonstrate certain level of originality? 4. Specificity and Detail: Determine whether the assistant provides concrete examples or detailed explanations. Properly justified repetition is permissible. 5. Tone and Style: Is the tone appropriate for the writing prompt? Is the writing style consistent throughout? Consider whether it aligns with the expectations of the intended audience or writing purpose. After evaluating each response, determine which one is superior based on the factors above. Provide your explanation and then select one of the following final verdicts: Assistant is significantly better: [[AB]] Assistant is slightly better: [[A>B]] Tie, relatively the same: [[A=B]] Assistant is slightly better: [[B>A]] 23 Assistant is significantly better: [[BA]] Example output: My final verdict is tie: [[A=B]]. A.6 Case Study Research on the Impact of Digital Inclusive Finance on Household Financial Asset Allocation Query: Research on the Impact of Digital Inclusive Finance on Household Financial Asset Allocation. SuperWriter-LM Response: Digital inclusive finance (DIF) has become cornerstone of global financial innovation, transforming access to financial services for millions. Over 70% of SubSaharan Africa now uses mobile money platforms like Kenyas M-Pesa, while Indias Digital India initiative has spurred 40% rise in crypto adoption since 2020. These advancements highlight DIFs transformative potential, yet existing research has fallen short in analyzing three critical dimensions: the mechanisms driving shifts in household asset allocation, the equity trade-offs between access and exclusion, and the risk mitigation required to balance innovation with stability. Most studies focus on access expansion or behavioral finance theories in isolation, neglecting how DIFs designsuch as gamified investing apps or frictionless cross-border transfersaffects portfolio composition, risk tolerance, or wealth gaps. Methodological gaps persist too: cross-sectional data often conflate correlation with causation, and equity analyses rarely account for cultural or infrastructural moderators like trust in institutions or gender norms. This study addresses these gaps by rigorously examining how DIF influences household financial asset allocation, including diversification, risk-return preferences, and equity access. Grounded in behavioral finance (e.g., FUDT), financial inclusion theory, and institutional economics, it employs mixed-methods approach combining quantitative surveys (n=10,000 households in Nigeria and Indonesia), qualitative interviews with users and regulators, and ethnographic observations. Grounded in these theories, the studys five hypotheses guide the analysis: (1) DIF adoption increases formal market participation, (2) behavioral nudges shift portfolios toward riskier assets, (3) DIF reduces wealth gaps for underserved groups, (4) geographic/cultural factors moderate these effects, and (5) DIF amplifies exposure to volatile/high-risk products. By bridging interdisciplinary insightsfrom behavioral biases to institutional trustthe study offers actionable strategies for policymakers and fintech firms to design equitable, risk-aware systems. Its findings aim to inform global debates on how to harness DIFs potential while mitigating its pitfalls, ensuring financial innovation serves as ladder out of poverty rather than trap into risk. Empirical studies have highlighted DIFs role in expanding financial access. For instance, Kenyas M-Pesa reduced informal savings reliance by 20% among rural households (Ratha, 2012), while Indias Digital India initiative spurred 40% rise in crypto adoption since 2020 (Reserve Bank of India, 2023). These findings underscore DIFs potential to bridge wealth gaps and democratize financial inclusion. However, such studies focus predominantly on access metricssuch as account ownership or transaction frequencyrather than how this access translates to changes in household financial behavior, such as asset allocation or risk tolerance. By including unregulated peer-to-peer platforms in DIF metrics, prior studies may overstate the services equity impact, as informal tools often lack safeguards against predatory practices or volatility. This conflation also masks how DIFs designsuch as gamified apps or crossborder transfersaffects risk tolerance or wealth gaps, limiting insights into its dual role as both an equity tool and risk amplifier. This focus on access metrics neglects the mechanisms through which DIF alters household financial behaviorsuch as shifts in asset allocation or risk toleranceand fails to assess how these changes interact with equity trade-offs or systemic risks, such as volatility exposure. To address these dual challenges of equity and risk, theoretical frameworks like Financial Decision-Making Under Uncertainty (FUDT) and Modern Portfolio Theory (MPT) provide critical lenses for understanding how DIFs design shapes household financial behavior.......) 24 艾琳达民俗志 Query: 请帮助写出一个有趣的故事2000字左右开头是在遥远平行世界的一角 你作为一名异世界的民俗学家踏上了一片充满异域风情的土地 SuperWriter-LM Response: 实验室的白炽灯在艾琳达民俗志泛黄的书页上投下斑 驳光影林小夏的指尖悬在笔记本边缘激光笔的光斑正沿着岩灵祭祀需三色岩盐的 注释游移突然那道光斑在纸面某处骤然凝固符号边缘泛起幽蓝荧光这光芒 并非单纯视觉现象而是艾琳达民俗志中记载的裂隙激活符当三色岩盐的符 号被激光笔的光谱激发时会形成跨维度的能量通道符号边缘的蓝光在纸面形成类 似DNA链的缠绕结构每个节点都与冰晶裂纹的走向完全重合 奇怪. . . . . . 她轻声呢喃凑近观察时书页竟开始自行翻动那些发光的萤火藤实为 岩灵祭祀的灵脉指示器当能量通道开启时被封印的祭祀参与者会以虚影形式显 现倒影在符号中的冰封峡谷骤然清晰冰面下封印着艾琳达大陆的岩灵祭祀残卷 雾气中浮现出雾之族少女的轮廓 竹哨的呜咽声混着实验室的电子嗡鸣那频率与艾琳达民俗志第17章记载的岩灵 共鸣频率完全吻合林小夏的耳膜传来刺痛这声音的波长恰好是三色岩盐在特定 温度下共振的频率她下意识握紧父亲遗留的铜牌铜牌表面的纹路正与冰晶共鸣 指尖突然发烫她想起父亲临终前将一枚刻有三色岩盐纹的铜牌塞进她手心那枚 铜牌此刻正贴着掌心与冰晶共鸣的频率完全一致 等等她后退半步符号光芒暴涨将整张书桌吞没当视线再次聚焦时她握着的 笔记本已变成一片冰晶而窗外的竹林深处传来与童年祭祖时如出一辙的竹哨声 实验室的通风管道开始渗出细小的雾气凝结成与冰晶相同的六棱结晶这些雾气在 地面形成微小的漩涡如同被撕裂的时空裂缝在现实世界投下的投影 冰凉的触感爬上后颈那不是汗珠而是雾之族少女的雾气渗入实验室的证明林 小夏低头发现所有笔记都被双螺旋符号覆盖唯有窗台残留着竹哨的余韵与三百 年前父亲唤醒祖先灵位的调子完美重合那些符号并非静态图案而是以书页为基底 不断重组就像艾琳达大陆的岩灵祭祀在重演 雾气从莉娅指尖溢出时林小夏闻 到了竹子的清香少女的皮肤如流动的云雾随着她向前走动那些雾气在冰面下浮 现出与艾琳达民俗志第12章记载的\"岩灵印记\"相同的螺旋纹路这是雾之族萨满 在举行祭祀时通过雾气凝结形成的古老符号 \"您父亲用的竹哨与三百年前萨满唤醒土地的仪式完全一致\"莉娅的声音裹着雾气 如同三百年前岩灵祭祀的咒文\"信仰不是跪拜的姿态而是让岩盐结晶时能听见土 地在冰层下低语的耳朵\"她跪坐在冰封的溪流旁指尖划过冰面泛起的涟漪竟让整 片区域的雾气凝结成冰晶 林小夏盯着那些冰晶中的纹样突然意识到它们与艾琳达民俗志中\"季节仪式\"的记 载完全重合那不是魔法符号而是岩灵祭祀在冰层下刻下的伤疤她举起激光笔 对准冰面试图用光斑测量裂纹角度\"这不过是自然魔法的伪科学现象仪式失败的 原因是你们的信仰不够纯粹\" \"信仰\"莉娅的雾气突然收缩成剑锋形状皮肤在雾气收缩时泛起蛛网状裂纹如同被 暴雨击碎的瓷器她的声音混着雾气的震颤\"您父亲用竹哨唤醒祖先灵位时是否听 见了土地的低语\" 林小夏的指尖猛地刺痛那不是幻觉冰面下传来竹哨的呜咽混着雾气的震颤 那声音与父亲临终前对她说的\"有些伤痕需要文化之光来治愈\"重叠父亲站在竹林深处 的画面突然在冰晶中闪现他闭着眼睛让竹哨声与山风共鸣而她却总在反驳\"竹哨 只是乐器\" \"爸爸竹哨只是竹子做的乐器. . . . . . \"八岁的林小夏曾这样反驳父亲却闭上眼睛让 竹哨声与山风共鸣\"林小夏听土地在说话\" \"你父亲的竹哨\"莉娅的声音突然尖锐如岩浆\"唤醒了不该被惊醒的沉睡仪式的 火焰需要木柴但你的火种早已枯萎三色岩盐在你手中不过是实验室的化学试 剂\"她的雾气化作冰针刺入溪流冰面下浮现的岩盐结晶在蓝光中闪烁 . . . . . . 林小夏握紧被莉娅血脉之力染蓝的玉璧听见窗外传来竹哨声那声音既熟悉又陌 生如同被时光折叠的回响正穿透实验室的玻璃唤醒另一个维度的古老契约 25 A.7 Predicting Users Length Requirement with DeepSeek-R1-distill-Qwen-32B We describe method for predicting the length of users input requirement using the R1-distillQwen-32B model for few-shot learning. The process involves two main steps: predicting whether the input exceeds 2,000 words, and predicting the exact length requirement based on the first prediction. 1. Step 1: Predicting Length Exceedance (Prompt 1): The first prediction is made by checking whether the input exceeds 2,000 words. carefully crafted prompt (Prompt 1) is provided to the model to predict if the contents expected word count will surpass the 2K threshold. The model utilizes few-shot learning with example inputs to classify the task into either above 2K or below 2K based on the nature of the input. 2. Step 2: Predicting Exact Length Requirement (Prompt 2): Once the model predicts whether the task exceeds 2,000 words, second prediction is made to determine the exact length category. Based on the result from Step 1, Prompt 2 is designed to predict whether the content is in the 2K-4K, 4K-8K, 8K-16K, or 16K+ category. The model provides the final prediction by analyzing the contextual hints and the input length characteristics. PromptGuidelines: To determine whether the expected output will exceed 2000 words, consider the following factors: 1. Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or comprehensive coverage of complex topics? 2. Scope and Breadth: Does the task cover multiple subtopics, perspectives, or extensive subject matter? 3. Structure and Sections: Does the output need to include multiple sections such as introductions, literature reviews, methodologies, results, discussions, and conclusions? 4. Research and References: Does the task require extensive research, citations, and referencing of multiple sources? Response Format: Answer with either #*# Yes or #*# No. Provide concise justification based on the guidelines above. Example 1: Query: Is Sanskrit the oldest language? Answer: This question requires concise factual answer, not an extensive output. #*# No *** END Example 2: Query: Create detailed business plan for new cat litter product. Answer: Creating detailed business plan involves multiple sections such as market research, product development, financial projections, marketing strategy, and competitive analysis, all of which require in-depth exploration and explanation. #*# Yes *** END ..... ..... Assess the following statement and decide whether the expected response is likely to require more than 2000 words. Answer with either #*# Yes or #*# No, and include brief justification, like above example. Query: User Query Answer: Prompt-2 Guidelines: To estimate the expected length of the output, consider the following factors: 1. Depth and Complexity: Does the task require detailed explanations, in-depth analysis, or complex reasoning? 2. Scope and Breadth: Does the task cover multiple subtopics, perspectives, or an extensive subject matter? 26 3. Structure and Sections: Does the output require multiple sections (e.g., introduction, literature review, methodologies, results, discussions, conclusions)? 4. Research and References: Does the task require significant research, citations, or references to multiple sources? 5. Detail Level: Is the task expected to be highly detailed, or can it be summarized concisely? Response Format: - Choose the most likely word count category: Less than 2000 words, 2000 words, 4000 words, 8000 words, or 16000 words. Using (### Category: Chosen category) as the response format. - Provide brief justification based on the guidelines above. Example 1: Less than 2000 words Query: Is Sanskrit the oldest language? Answer: This is factual question requiring brief answer with no complex analysis or subtopics. Likely to be less than 2000 words. ### Category: Less than 2000 words Explanation: Similar to short blog post or brief news article, this task needs minimal detail and is concise. *** END Example 2: 2000 words (2000 to 4000 words) Query: Describe the key differences between classical and quantum computing. Answer: This question requires moderate detail, comparing classical and quantum computing without exhaustive technical exploration. Likely to be around 2000 words. ### Category: 2000 words Explanation: Similar to moderate-length essay or detailed blog post, this task covers key points with enough depth but remains manageable. *** END ..... ..... Example 5: More than 16000 words Query: Write full-length book on the history of the Industrial Revolution, covering all major events, technological innovations, and global impacts. Answer: This would require an in-depth exploration of the entire history of the Industrial Revolution, with detailed analysis across multiple chapters. Likely to be more than 16000 words. ### Category: 16000 words Explanation: Similar to book-length content, such as thesis or encyclopedia entry, requiring substantial detail and coverage over multiple sections or chapters. *** END Assess the following statement and decide what the expected output length is. Answer with the appropriate word count category and provide brief justification. Query: User Query Answer:"
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design, Singapore",
        "Tsinghua University, Beijing, China"
    ]
}