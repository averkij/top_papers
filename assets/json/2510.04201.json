{
    "paper_title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
    "authors": [
        "Moo Hyun Son",
        "Jintaek Oh",
        "Sun Bin Mun",
        "Jaechul Roh",
        "Sehyun Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\\footnote{https://github.com/mhson-kyle/World-To-Image}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 1 0 2 4 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "WORLD-TO-IMAGE: GROUNDING TEXT-TO-IMAGE GENERATION WITH AGENT-DRIVEN WORLD KNOWLEDGE Moo Hyun Son1, Jintaek Oh1, Sun Bin Mun2, Jaechul Roh3, Sehyun Choi4 1The Hong Kong University of Science and Technology, 2Georgia Institute of Technology 3University of Massachusetts Amherst, 4TwelveLabs {mhson, johaa}@connect.ust.hk, smun6@gatech.edu jroh@cs.umass.edu, schoiaj98@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce WORLD-TOIMAGE, novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that WORLD-TO-IMAGE substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-image (T2I) diffusion models have rapidly advanced, producing high-fidelity, stylistically rich images from natural-language prompts and broadening access to creative tools (Liu et al., 2024; Gao et al., 2025; Black-Forest-Labs et al., 2025). Recent models are even capable of generating more photorealistic images that adhere to common artistic conventions (Imagen-Team-Google et al., 2024; Blattmann et al., 2023). Despite this progress, persistent failure mode remains: models frequently misinterpret prompts that reference novel concepts, long-tail entities, or domain-specific terminology that fall outside their pretraining distribution (Rege et al., 2025; Zhao et al., 2025). As such failure modes are manifestations of evolving world knowledge, static pretrained representations will inevitably lag behind, establishing clear mandate for research in this direction. Potential solutions include scaling training or fine-tuning, but it is expensive and ill-suited for rapidly emerging or long tail concepts (Li et al., 2024; Arar et al., 2024). Another solution could be optimizing the prompts rather than the model weights directly, so that the input is formulated in way that best understood by the model. However, current prompt-optimization approaches improve image aesthetics and prompt consistency but largely operate at the text surface (Hao et al., 2022; Manas et al., 2024). When model lacks the underlying semantic grounding for concept, adding descriptors like highly detailed, 8K, award-winning does not induce the correct depiction (Khan et al., 2025). We propose to systematically mitigate promptmodel misalignment where the root cause is missing world knowledge, without retraining or extending the base models capabilities directly. To this Equal Contribution. 1https://github.com/mhson-kyle/World-To-Image"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of WORLD-TO-IMAGE. end, we employ the framework of prompt optimization and extend it as an agentic decision process that (i) diagnoses whether generation failure is due to rendering limitations versus conceptcomprehension failures, and (ii) conditionally invokes targeted strategies that incorporate external world knowledge. Concretely, our system  (Fig. 1)  integrates web interaction for evidence gathering, semantic decomposition and concept substitution for text reformulation, and multi-modal grounding via image retrieval and reference image-based conditioning. Rather than hoping the model will infer unseen concepts from adjectives, the agent supplies multimodal evidence that steers generation toward semantically faithful outputs. By formulating the input prompt optimization as means to instill world knowledge, we leave the base model unchanged and leverage the full potential of the existing capabilities. Given user prompt, the agent first conducts lightweight failure analysis using probe generations and concept coverage checks. If signals indicate comprehension risk due to novel concepts present in the prompt, the agent retrieves concise textual definitions and representative reference images from the web, then performs: (1) semantic decomposition to isolate atomic concepts; (2) concept substitution to map obscure terms to model-familiar paraphrases while preserving meaning; and (3) visual grounding that conditions the generator with retrieved references. To best study the novel/long-tail entities and compositional attributes, we curated dataset containing prompts with novel concepts outside the training of the base model. Across popular benchmarks and our proposed dataset, our framework, W2I, consistently improves semantic faithfulness and prompt adherence over strong text-only prompt optimizers, while maintaining competitive aesthetic quality. Our main contributions are two-fold: 1. Agentic optimization framework. We propose diagnosis-and-selection agent that chooses among semantic decomposition, concept substitution, and multi-modal grounding with web-sourced evidence (Fig. 1, Sec. 3). 2. World-knowledge infusion for T2I. We extend prompt optimization beyond text by integrating image retrieval and conditioning to handle novel concepts, yielding state-of-the-art semantic faithfulness without retraining (Sec. 4.1)."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Prior work has explored diverse strategies, including iterative prompt optimization to emphasize salient semantic components for improved image quality, fine-tuning of model parameters to enhance generative performance, and augmentation with external knowledge sources to overcome the limitations of fixed pretrained image datasets."
        },
        {
            "title": "2.1 PROMPT OPTIMIZATION IN TEXT-TO-IMAGE",
            "content": "Recent research has increasingly focused on automating prompt engineering to enhance the quality, control, and reliability of text-to-image (T2I) models. dominant approach involves leveraging large language models (LLMs) and reinforcement learning to automatically discover superior prompts, optimizing for aesthetic quality and semantic alignment without requiring manual iteration. These methods range from reward-agnostic, test-time optimization in the embedding space (Kim et al., 2025) to Multi-stage fine-tuning frameworks for LMs multi-stage frameworks using fine-tuned Language Models (Wang et al., 2023a), and even dynamic systems that adjust prompt weights online during the generation process (Mo et al., 2024). Beyond general performance, this optimization paradigm is being extended to address critical concerns such as safety and fairness, with studies proposing universal optimizers for reliable generation (Wu et al., 2024) and techniques to improve the representation of minority groups (Um & Ye, 2025). Complementing these automated approaches, interactive systems like PromptMagician (Feng et al., 2023) focus on human-in-the-loop optimization, providing visual analytics to empower users in the creative refinement process. Collectively, this body of work signifies shift from manual prompt crafting to systematic, goal-driven optimization frameworks for T2I synthesis."
        },
        {
            "title": "2.2 WORLD KNOWLEDGE DRIVEN TEXT-TO-IMAGE",
            "content": "A growing body of literature has focused on creating benchmarks to probe the knowledge-grounding capabilities of T2I models. For instance, WorldGenBench (Zhang et al., 2025) introduces benchmark to test the grounding of prompts containing explicit and implicit cultural, factual, and inferential knowledge. Using proposed Knowledge Checklist Score, they find that while diffusion models are competent, newer autoregressive systems like GPT-4o demonstrate superior reasoning. Similarly, WISE (Niu et al., 2025) presents an extensive evaluation framework with over 1,000 prompts across 25 knowledge domains. Their WiScore metric reveals deep limitations in current models ability to handle complex semantic, factual, and inferential concepts. Complementing these broadknowledge benchmarks, the Commonsense-T2I challenge (Fu et al., 2024) specifically investigates whether models possess human-like commonsense reasoning. Through adversarial prompt pairs, their work highlights significant gap between model-generated outputs and commonsense expectations, underscoring the need for improved reasoning capabilities. Collectively, these evaluation frameworks establish clear consensus: even state-of-the-art T2I models struggle to consistently and accurately reflect nuanced world knowledge and commonsense, gap our work aims to address."
        },
        {
            "title": "GENERATION",
            "content": "The goal of this work is to enable T2I models to incorporate external world knowledge, thereby extending regions of the embedding space that were not observed during pretraining. Since the model has not been exposed to novel concepts during training, its performance on prompts that introduce such concepts often degrades, requiring additional time and iterations to produce meaningful images. To address this limitation, we propose WORLD-TO-IMAGE (W2I), an iterative, agent-based T2I generation optimization framework that dynamically utilizes world knowledge. Given an initial prompt p0 = p, the system first generates baseline image I0 = T2I(p0, ϕ(E0)) with no exemplars (E0 = ). At each iteration t, the framework is coordinated by an Orchestrator Agent that receives the state (pt1, It1, Et1, st1), where st1 = (It1, p, Et1) is the evaluation score combining semantic alignment and aesthetic quality. Based on this state, the Orchestrator decides whether to activate the Prompt Optimizer Agent (POA) or the Image Retriever Agent (IRA). As illustrated in Figure 2, if invoke-POA = 1, the POA refines the prompt pt1 into pt by augmenting its descriptive content (e.g., replacing domain-specific jargon or reformulating cultural references), while keeping the exemplar set unchanged (Et = Et1). Conversely, if invoke-IRA = 1, the IRA retrieves an updated exemplar set Et conditioned on (Et1, pt, It1), grounding novel concepts such as unseen entities or styles, while leaving the prompt unchanged (pt = pt1). Finally, the framework supports joint activation where both agents operate sequentially. In this mode, the"
        },
        {
            "title": "Preprint",
            "content": "POA first generates an optimized prompt pt, which is then immediately used by the IRA to retrieve more contextually-aware set of exemplars Et. This allows for comprehensive update to both the language and vision inputs in single iteration. The updated promptexemplar pair (pt, Et) is then passed to the generator, producing new image It = T2I(pt, ϕ(Et)). The image is evaluated by st = (It, p, Et), and the loop continues until convergence. Convergence is defined either when st τ , yielding = It, or when the maximum iteration budget Tmax is reached, in which case the best image across all iterations is returned: = arg max tTmax (It, p, Et). We decompose st = (It, p, Et) into semantic alignment, keyword coverage (graded by an LLM), and aesthetic quality: st = α Ssem + β Kt + γ At , α, β, γ 0. Keyword set. From the prompt (and, when applicable, reference descriptors extracted from Et), we form canonical set of required tokens = {ki}m i=1 including entities, attributes, relations, styles, and constraints (e.g., character name, location, palette, era, camera). We obtain by rule-based parsing (POS/NER) composed with an LLM pass that merges synonyms and prunes redundancies. LLM keyword grading. An LLM receives (prompt p, references Et, visual analysis of It) and returns per-keyword judgments gi {1, 1 2 , 0} for {present, partially present, missing}, with short rationales. The keyword coverage score is Figure 2: Illustration of case where the Orchestrator Agent invokes the Image Retriever Agent (invoke-IRA=1). Kt = 1 m (cid:88) i=1 gi [0, 1], optionally weighted if some keywords are marked critical by the Orchestrator (weights renormalized to 1). Aesthetic quality. At [0, 1] measures perceptual appeal (e.g., composition, lighting, color harmony). It may be computed by an automatic quality model or an LLM aesthetic rubric; scores are normalized to [0, 1]. In this way, we integrate both language-space optimization (via prompt refinement) and vision-space optimization (via exemplar retrieval), enabling T2I models to adapt to novel concepts during inference. We hypothesize that such joint optimization of the language and vision space complements each other and generates strong synergy. We formerly illustrate our method in Algorithm 1 of Appendix A."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "This section first describes our experimental settings (4.1), then presents results analysis (4.2), aligning them with our hypotheses."
        },
        {
            "title": "4.1 EXPERIMENT SETTING",
            "content": "Models. We compare seven systems: Stable Diffusion 1.4 (Rombach et al., 2022), Stable Diffusion 2.1 (Rombach et al., 2022), Stable Diffusion XL (Base) (Podell et al., 2024), OmniGen2 (Wu et al.,"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Qualitative comparison of text-to-image generation results across seven models. Our model consistently demonstrates stronger semantic alignment (e.g., Doomer Doge staring at TikTok stock crash), accurate identity grounding (e.g., Kai Cenat streaming from spaceship), and faithful concept representation (e.g., mommy AI), outperforming baselines in both fidelity and prompt adherence. 2025), the Promptist prompt-optimization pipeline with Stable Diffusion XL (Base) and OmniGen2 (Hao et al., 2022), and World-To-Image, our agentic pipeline. SDXL-Base marginally outperforms OmniGen2 on general prompts  (Table 1)  . However, in reference-conditioned settings, where prompts require grounding to unfamiliar entities or finegrained attributes, OmniGen2 demonstrates stronger conditioning fidelity and stability, yielding higher Accuracy-to-Prompt. Accordingly, we adopt OmniGen2 as the generator backbone for our agentic pipeline, while reporting SDXL-Base, SD2.1, SD1.4, and Promptist as baselines for completeness. We include SDXL-Base, SD2.1, and SD1.4 because they remain widely adopted, strong baselines in the image-generation community and provide representative benchmark for comparing modern systems. Datasets. To evaluate our agentic image generation pipeline, where the system issues API calls to fetch reference images for concepts the base generator is unlikely to comprehend, we use three datasets: Lexica (Shen et al., 2024), DiffusionDB (Wang et al., 2023b), and our curated NICE (Niche Concept Evaluation) benchmark. While existing benchmarks largely focus on generic prompts, NICE specifically targets rare, compositional, and time-sensitive concepts, providing challenging setting to stress-test retrieval and grounding capabilities. For each subcategory, we searched for trending and emerging topics and refined them into high-quality prompts using GPT-5 to ensure clarity and diversity. General-purpose baselines. Lexica and DiffusionDB are widely used for benchmarking text-toimage systems on broad, in-distribution prompts. While they contain occasional IP or celebrity mentions, such instances are incidental rather than the main focus of these corpora; consequently, they underrepresent the long-tail, time-sensitive, or compositional concepts our pipeline targets. Curated NICE Benchmark. To stress test retrieval, we construct 100-prompt evaluation set spanning five sub-categories: (1) Memes, (2) Real-Time News & Events, (3) Pop Culture & IP, (4) Artists/Celebrities/Influencers, and (5) Niche Concepts (20 prompts each). Prompts are built to (i) mix two distinct concepts or (ii) reference post-2024 entities and events, creating out-ofdistribution cases that require external visual evidence. This design forces the Orchestrator to invoke image-retrieval via API and ground generation on retrieved exemplars."
        },
        {
            "title": "Preprint",
            "content": "Evaluation Metrics. We evaluate our retrieval-augmented, agentic pipeline on hard/niche prompts that are typically out-of-distribution for the base generator. To capture semantic faithfulness and human-perceived quality at scale, we report an LLM Grader (Hao et al., 2022) and Human Preference Rewards (Promptist Reward (Hao et al., 2022) and ImageReward (Xu et al., 2023)), and HPSv2 (Wu et al., 2023). LLM Grader (Hao et al., 2022). Following (Hao et al., 2022), an LLM-based judge scores five dimensions, Accuracy-to-Prompt, Creativity & Originality, Visual Quality & Realism, Consistency & Cohesion, and Emotional/Thematic Resonance with an overall aggregate. This is our primary indicator of semantic alignment on rare, compositional, or time-sensitive concepts that benefit from retrieval. Human-Preference. Promptist Reward (Hao et al., 2022) and ImageReward (Xu et al., 2023) are learned reward models trained on human preference data for textimage pairs; we report their sum as the Human Preference Reward. HPSv2 (Wu et al., 2023) is another human-preference-based scoring model. These serve as automatic proxies for perceptual quality and user favorability, complementing the LLM Grader for large-scale, reproducible comparisons. Implementation Details. All agents in our pipeline use gpt-4o as their backbone model. We perform two optimization iterations by default, using OmniGen2 as the base image generator. For image retrieval, we leverage the Google SERP API to fetch relevant reference images for grounding. The Orchestrator Agent monitors progress and may terminate the loop early if no further improvements are expected; otherwise, it executes the full two-iteration optimization schedule. Table 1: Comparison of LLM-based evaluation metrics across datasets and models. Bold values indicate the best performance within each dataset group. For our main metrics, Accuracy-to-Prompt and Overall, we additionally report the relative improvement (in %) over the next best-performing model within the same dataset group. Dataset Metric W2I (Ours) OmniGen2 Promptist Promptist SDXL-Base SD2.1 SD1.4 OmniGen SDXL-Base NICE DiffusionDB Lexica Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall 87.5 88.9 91.3 84.5 86.8 ( 8.1 %) 87.8 ( 4.5 %) 87.3 92.3 94.1 85.0 87.4 ( 3.4 %) 89.3 ( 2.1 %) 88.6 92.7 95.2 86.3 89.8 ( 6.0 %) 90.5 ( 2.8 %) 73.8 86.0 90.1 77.0 75.5 80.5 81.8 89.7 92.8 81.6 81.9 85. 81.6 90.3 94.2 79.8 83.6 85.9 80.7 85.6 90.6 84.0 79.0 84.0 84.6 89.8 94.1 86.2 82.8 87.5 86.1 90.3 93.3 85.5 84.7 88.0 80.5 84.9 88.7 81.3 79.7 83.0 84.4 88.7 93.4 85.0 84.3 87. 85.2 89.2 93.1 85.2 84.4 87.5 79.3 85.1 86.1 79.4 79.4 81.8 83.7 89.4 90.0 83.1 84.5 86.2 83.4 88.0 89.2 83.4 83.8 85.6 68.2 75.4 74.6 69.8 69.7 71.5 77.5 82.5 81.4 77.1 79.0 79. 76.9 82.5 83.0 77.7 79.4 79.9 66.3 71.6 74.6 69.9 67.3 69.9 75.8 80.0 79.2 76.2 76.9 77.6 75.9 81.7 79.1 76.4 77.0 78."
        },
        {
            "title": "4.2 RESULTS",
            "content": "Our main results are summarized in Table 1. Across all three studied datasets, our proposed method, W2I, consistently outperforms all baselines. The overall performance gains are most significant on our curated NICE (+5.8%), compared to the broader DiffusionDB (+2.4%) and Lexica (+3.4%) benchmarks. This confirms that our agentic pipeline is particularly effective for the out-of-distribution prompts it was designed to address. The improvements are most pronounced on Accuracy-to-Prompt, where W2I increases the score by substantial +8.1% on our set, versus +3.4% on DiffusionDB and +6.4% on Lexica. This aligns with our central hypothesis that prompts involving novel concepts benefit most from multimodal grounding, which W2I achieves by jointly leveraging retrieval and textual optimization. Image Quality and Human Preference In Table 2, we study the impact of our multi-modal prompt optimization on image quality. We focus on both objective image quality scores and human"
        },
        {
            "title": "Preprint",
            "content": "(a) \"Yondu udonta from guardians of the galaxy (b) \"production photo of Danny Divito as the Scarlett witch\" (c) \"portrait of mel medarda from arcane.\" Figure 4: Qualitative comparison of image generations across models for diverse prompts. Each row corresponds to one prompt, with columns showing outputs from left to right: Ours, OmniGen2, Promptist OmniGen2, Promptist SDXL-Base, SDXL-Base, SD2.1, and SD1.4. Table 2: Comparison of Human-Preference evaluation metrics across datasets and models. Bold values indicate the best performance within each dataset group. Dataset Metric W2I (Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1.4 NICE DiffusionDB Lexica Human Preference Reward ImageReward HPSv2 Human Preference Reward Image Reward HPSv2 Human Preference Reward Image Reward HPSv2 2.761 1.271 0. 2.817 1.271 0.304 2.947 1.376 0.309 2.259 0.775 0.283 2.364 0.993 0.297 2.738 1.176 0.302 2.4040 0.8119 0. 2.6854 1.0357 0.2977 2.8673 1.2208 0.2998 2.156 0.601 0.278 2.331 0.695 0.286 2.420 0.766 0.287 2.005 0.550 0. 2.233 0.696 0.281 2.303 0.766 0.283 1.609 0.239 0.256 1.639 0.224 0.252 1.647 0.210 0.247 1.305 -0.022 0. 1.409 0.033 0.241 1.528 0.122 0.241 preference-based evaluations. As shown, W2I maintains strong performance across both dimensions, outperforming all other baselines. These findings indicate that our method does not sacrifice visual fidelity in pursuit of semantic accuracy, but instead achieves strong balance between the two. Performance on Novel Concepts To further validate our frameworks effectiveness with out-ofdistribution prompts, we analyzed its performance across the five distinct subcategories of our NICE benchmark. As illustrated in Figure 5, our method consistently outperforms all baselines, including the strong Promptist optimizer and the base OmniGen2 model, in each categoryfrom memes and real-time events to niche intellectual property. This result demonstrates the frameworks robustness and confirms that its superior performance is driven by specialized ability to handle wide range of previously unseen concepts through agentic retrieval and grounding. Ablation Study To disentangle the contributions of different components within our optimization pipeline, we coablated each component of the optimization pipeline  (Table 3)  . Across the board, our full pipeline yields the best results on our proposed dataset. Relying exclusively on image retrieval can fail for more complex prompts, as the generation process may become overly conditioned on the reference without fully aligning to the task specification. Conversely, prompt optimization only improves alignment with textual instructions but image conditioning can provide the model with more concrete reference. The synergy of combining both components produces significant gains"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: LLM Grader overall scores across subcategories. Our method consistently outperforms all baselines. Table 3: Ablation study on our dataset. Each column shows performance when specific component is removed to quantify its contribution. Prompt Optimizer indicates that only the Prompt Optimizer (with image retrieval disabled) was used. Image Retrieval indicates that only the Image Retrieval module was used. w/o Agent represents variant with no agents. Bold values indicate the best performance."
        },
        {
            "title": "Metric",
            "content": "W2I (Ours) Prompt Optimizer Image Retrieval w/o Agent Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Score Human Preference Reward ImageReward HPSv2 87.5 88.9 91.3 84.5 86.8 87.8 2.761 1.271 0. 74.6 85.3 89.8 76.6 76.4 80.5 2.624 1.098 0.299 79.3 84.2 89.1 80.6 79.7 82.6 2.319 0.853 0.288 73.8 86.0 90.1 77.0 75.5 80.5 2.259 0.775 0. across all metrics, indicating that while each method individually emphasizes different axes of improvement, only their combination unlocks the full potential of the base model. Impact of increasing optimization steps We also analyzed the impact of extending the optimization schedule up to 10 steps, and plot the per-iteration improvement traces in Figure 6. Performance improves consistently across iterations, with the sharpest increase shown in the first 2 iterations. This supports our decision to use 2-step iterations by default, striking balance between performance and efficiency. We also observe that IRA is often invoked in the early iterations and POA predominantly in the later iterations, suggesting that image retrieval provides strong early boost, while subsequent prompt optimization refines outputs for further gains."
        },
        {
            "title": "5 DISCUSSIONS",
            "content": "Our findings raise several important discussion points. The strong gains on novel concepts highlight that pretrained generative models often already possess latent capacity to represent new entities, but require the right multimodal signals to activate them. This suggests broader opportunity: instead of scaling models alone, improving interface mechanisms, such as retrieval and adaptive prompting, may unlock substantial gains. Moreover, our ablation study shows strong synergy between text and image-based optimization, effectively expanding the horizon of prompt optimization to multimodal prompts to harness their complementary strengths."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: LLM-Grader sub-scores and overall score across optimization steps. The dotted line shows the overall score; solid lines represent individual dimensions. Future work may explore the scalability of World-To-Image with respect to the number of novel concepts in the input prompt. Preliminary results suggest that World-To-Image can consolidate novel concepts from various sources (text, image) and effectively incorporate the knowledge into the generation process through multiple iterations, which may lead to its ability of compositional generalization over multiple novel concepts simultaneously. While W2I demonstrates consistent improvements, several limitations remain. First, the reliance on external image retrieval assumes access to relevant, high-quality references; in domains with sparse or noisy imagery, performance may degrade. Second, our method focuses on optimizing prompts and retrieval rather than modifying the base generative model, which means it cannot introduce fundamentally new capabilities. This is compromise that in turn enabled efficient and modelagnostic framework, which lets us leverage the capabilities of the base model otherwise locked due to the limitations in the prompt comprehension. Finally, iterative optimization introduces additional test-time computational overhead compared to single-pass baselines, while our framework provides flexible control knobs to balance the efficiency-quality trade-off."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present World-To-Image, an agentic framework that solves the prompt-model mismatch problem rooted in missing world knowledge by multimodal prompt optimization. By deploying an Orchestrator agent to dynamically select between language-space prompt refinement and vision-space visual grounding via web retrieval, W2I instills timely world knowledge into the generation process without modifying the base model. Our experiments demonstrated that this approach significantly outperforms existing methods, achieving +8.1% improvement in accuracy-to-prompt on our challenging NICE benchmark containing diverse novel concepts. Our findings provide evidence that the path toward more capable generative models lies not only in model size scaling but also in improving the interfaces. The strong performance of W2I shows that by dynamically searching external knowledge and instilling them through multimodal interface, we can unlock the latent capabilities of existing models and bridge the gap between their static training and the evolving world. As result, World-To-Image introduces new axis of improvement for T2I generation, while also providing flexible framework for future research into more efficient retrieval strategies and more sophisticated agentic reasoning."
        },
        {
            "title": "REFERENCES",
            "content": "Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, and Ariel Shamir. Palp: Prompt aligned personalization of text-to-image models, 2024. URL https://arxiv.org/abs/2401.06105. Black-Forest-Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia Wang, Yuhong Lu, Minfeng Zhu, Baicheng Wang, and Wei Chen. Promptmagician: Interactive prompt engineering for text-toimage creation. IEEE Transactions on Visualization and Computer Graphics, pp. 111, 2023. ISSN 2160-9306. doi: 10.1109/tvcg.2023.3327168. URL http://dx.doi.org/10.1109/ TVCG.2023.3327168. Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense? arXiv, abs/2406.07546, 2024. URL https://arxiv.org/abs/2406.07546. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xuanda Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, and Weilin Huang. Seedream 3.0 technical report, 2025. URL https://arxiv.org/abs/2504. 11346. Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. Neurips2023, 2022. doi: 10.48550/arXiv.2212.09611. URL https://arxiv.org/abs/ 2212.09611. Imagen-Team-Google, :, Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach EatonRosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio Gomez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Rory Lawton, Shixin Luo, Soˇna Mokra, Henna Nandwani, Yasumasa Onoe, Aaron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao Xiong, Keyang Xu, Marc van Zee, Junlin Zhang, Katie Zhang, Wenlei Zhou, Konrad Zolna, Ola Aboubakar, Canfer Akbulut, Oscar Akerlund, Isabela Albuquerque, Nina Anderson, Marco Andreetto, Lora Aroyo, Ben Bariach, David Barker, Sherry Ben, Dana Berman, Courtney Biles, Irina Blok, Pankil Botadra, Jenny Brennan, Karla Brown, John Buckley, Rudy Bunel, Elie Bursztein, Christina Butterfield, Ben Caine, Viral Carpenter, Norman Casagrande, Ming-Wei Chang, Solomon Chang, Shamik Chaudhuri, Tony Chen, John Choi, Dmitry Churbanau, Nathan Clement, Matan Cohen, Forrester Cole, Mikhail Dektiarev, Vincent Du, Praneet Dutta, Tom Eccles, Ndidi Elue, Ashley Feden, Shlomi Fruchter, Frankie Garcia, Roopal Garg, Weina Ge, Ahmed Ghazy, Bryant Gipson, Andrew Goodman, Dawid Gorny, Sven Gowal, Khyatti Gupta, Yoni Halpern, Yena Han, Susan Hao, Jamie Hayes, Jonathan Heek, Amir Hertz, Ed Hirst, Emiel Hoogeboom, Tingbo Hou, Heidi Howard, Mohamed Ibrahim, Dirichi Ike-Njoku, Joana Iljazi, Vlad Ionescu, William Isaac, Reena Jana, Gemma Jennings, Donovon Jenson, Xuhui Jia, Kerry Jones, Xiaoen Ju, Ivana Kajic, Christos Kaplanis, Burcu Karagol Ayan, Jacob Kelly, Suraj Kothawade, Christina Kouridi, Ira Ktena, Jolanda Kumakaw, Dana Kurniawan, Dmitry Lagun, Lily Lavitas, Jason Lee, Tao Li, Marco Liang, Maggie Li-Calis, Yuchi Liu,"
        },
        {
            "title": "Preprint",
            "content": "Javier Lopez Alberca, Matthieu Kim Lorrain, Peggy Lu, Kristian Lum, Yukun Ma, Chase Malik, John Mellor, Thomas Mensink, Inbar Mosseri, Tom Murray, Aida Nematzadeh, Paul Nicholas, Signe Nørly, Joao Gabriel Oliveira, Guillermo Ortiz-Jimenez, Michela Paganini, Tom Le Paine, Roni Paiss, Alicia Parrish, Anne Peckham, Vikas Peswani, Igor Petrovski, Tobias Pfaff, Alex Pirozhenko, Ryan Poplin, Utsav Prabhu, Yuan Qi, Matthew Rahtz, Cyrus Rashtchian, Charvi Rastogi, Amit Raul, Ali Razavi, Sylvestre-Alvise Rebuffi, Susanna Ricco, Felix Riedel, Dirk Robinson, Pankaj Rohatgi, Bill Rosgen, Sarah Rumbley, Moonkyung Ryu, Anthony Salgado, Tim Salimans, Sahil Singla, Florian Schroff, Candice Schumann, Tanmay Shah, Eleni Shaw, Gregory Shaw, Brendan Shillingford, Kaushik Shivakumar, Dennis Shtatnov, Zach Singer, Evgeny Sluzhaev, Valerii Sokolov, Thibault Sottiaux, Florian Stimberg, Brad Stone, David Stutz, YuChuan Su, Eric Tabellion, Shuai Tang, David Tao, Kurt Thomas, Gregory Thornton, Andeep Toor, Cristian Udrescu, Aayush Upadhyay, Cristina Vasconcelos, Alex Vasiloff, Andrey Voynov, Amanda Walker, Luyu Wang, Miaosen Wang, Simon Wang, Stanley Wang, Qifei Wang, Yuxiao Wang, Agoston Weisz, Olivia Wiles, Chenxia Wu, Xingyu Federico Xu, Andrew Xue, Jianbo Yang, Luo Yu, Mete Yurtoglu, Ali Zand, Han Zhang, Jiageng Zhang, Catherine Zhao, Adilet Zhaxybay, Miao Zhou, Shengqi Zhu, Zhenkai Zhu, Dawn Bloxwich, Mahyar Bordbar, Luis C. Cobo, Eli Collins, Shengyang Dai, Tulsee Doshi, Anca Dragan, Douglas Eck, Demis Hassabis, Sissie Hsiao, Tom Hume, Koray Kavukcuoglu, Helen King, Jack Krawczyk, Yeqing Li, Kathy Meier-Hellstern, Andras Orban, Yury Pinsky, Amar Subramanya, Oriol Vinyals, Ting Yu, and Yori Zwols. Imagen 3, 2024. URL https://arxiv.org/abs/2408.07009. Mohammad Abdul Hafeez Khan, Yash Jain, Siddhartha Bhattacharyya, and Vibhav Vineet. Testtime prompt refinement for text-to-image models, 2025. URL https://arxiv.org/abs/ 2507.22076. Semin Kim, Yeonwoo Cha, Jaehoon Yoo, and Seunghoon Hong. Reward-agnostic prompt optimization for text-to-image diffusion models, 2025. URL https://arxiv.org/abs/2506. 16853. Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation, 2024. URL https://arxiv.org/abs/2404.02883. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models, 2024. URL https://arxiv.org/ abs/2409.10695. Oscar Manas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency via automatic prompt optimization, 2024. URL https://arxiv.org/abs/2403. 17804. Wenyi Mo et al. Dynamic prompt optimizing for text-to-image generation via prompt auto-editing and online weight & time-step adaptation. Proceedings of CVPR 2024, 2024, 2024. arXiv preprint arXiv:2404.04095. Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv, abs/2503.07265, 2025. URL https://arxiv.org/abs/2503.07265. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations (ICLR) 2024, 2024. doi: 10. 48550/arXiv.2307.01952. URL https://openreview.net/forum?id=di52zR8xgf. Aniket Rege, Zinnia Nie, Mahesh Ramesh, Unmesh Raskar, Zhuoran Yu, Aditya Kusupati, Yong Jae Lee, and Ramya Korlakai Vinayak. Cure: Cultural gaps in the long tail of text-to-image systems, 2025. URL https://arxiv.org/abs/2506.08071. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022."
        },
        {
            "title": "Preprint",
            "content": "Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. Prompt stealing attacks against textto-image generation models. In Proceedings of the USENIX Security Symposium 2024, pp. , 2024. doi: 10.48550/arXiv.2302.09923. URL https://arxiv.org/abs/2302.09923. arXiv preprint arXiv:2302.09923, revised version v2. Soobin Um and Jong Chul Ye. Minorityprompt: Minority-focused text-to-image generation via prompt optimization. In CVPR 2025, 2025. arXiv:2410.07838. Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit Dhillon, and Sanjiv Kumar. Promot: Prompt tuning with model tuning, two-stage framework to reduce In NeurIPS 2023, 2023a. Abstract / Poster: Two-stage LM specialization in lm fine-tuning. fine-tuning. Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. ACL 202, 2023b. doi: 10.48550/arXiv.2210.14896. URL https://arxiv. org/abs/2210.14896. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, and Suhang Wang. Posi: Universal In NAACL-HLT 2024 (Long Papers), pp. prompt optimizer for safe text-to-image generation. 63406354, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977, 2023. doi: 10.48550/arXiv.2304.05977. URL https:// arxiv.org/abs/2304.05977. Daoan Zhang, Che Jiang, Ruoshi Xu, Biaoxiang Chen, Zijian Jin, Yutian Lu, Jianguo Zhang, Liang Yong, Jiebo Luo, and Shengda Luo. Worldgenbench: world-knowledge-integrated benchmark for reasoning-driven text-to-image generation. arXiv, abs/2505.01490, 2025. URL https: //arxiv.org/abs/2505.01490. Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, and Yunhao Ge. Dreamdistribution: Learning prompt distribution for diverse in-distribution generation, 2025. URL https://arxiv.org/abs/2312.14216."
        },
        {
            "title": "Preprint",
            "content": "A WORLD-TO-IMAGE ALGORITHM Algorithm 1: World-To-Image: Agentic Framework for Optimizing Novel-Concept T2I Generation Legend. p: initial user prompt. pt: refined prompt at iteration t. It: generated image at iteration t. : final selected image. Et: set of external exemplars (retrieved reference images) at iteration t. ϕ(Et): embedding/conditioning function applied to exemplars. (It, p, Et): evaluation function (e.g., LLMGrader, CLIP similarity, or aesthetic score). τ : score threshold for convergence. Tmax: maximum iteration budget. OrchestratorAgent: decides whether to invoke sub-agents. PromptOptimizerAgent: refines/augments prompts. ImageRetrieverAgent: retrieves external exemplars. invoke-POA, invoke-IRA: binary flags from the Orchestrator. Input: Initial prompt p; threshold τ ; maximum iterations Tmax Output: Final image p0 p, E0 ; I0 T2I(p0, ϕ(E0)) ; for 1 to Tmax do // Step 1: Orchestration (invoke-POA, invoke-IRA) OrchestratorAgent(pt1, It1, Et1) ; // Step 2: Prompt Optimization (if selected) if invoke-POA = 1 then pt PromptOptimizerAgent(pt1, It1) else pt pt1 // Step 3: Image Retrieval (if selected) if invoke-IRA = 1 then Et ImageRetrieverAgent(Et1, pt, It1) else Et Et1 // Step 4: Candidate Generation & Scoring It T2I(pt, ϕ(Et)) ; st (It, p, Et) ; if st τ then It ; break return"
        },
        {
            "title": "B EXTENDED VISUAL COMPARISONS",
            "content": "buff jesus christ welcoming you into heaven punching contest in glass of milk beautiful concept inside of oneill cylinder Shrek in the Backrooms playing basketball Figure 7: Qualitative comparison of image generations across models for diverse prompts. Each row corresponds to one prompt, with columns showing outputs from left to right: Ours, OmniGen2, Promptist OmniGen2, Promptist SDXL-Base, SDXL-Base, SD2.1, and SD1.4."
        },
        {
            "title": "Preprint",
            "content": "Pepe as chess grandmaster with NFT crown 2025 Paris Olympics esports finals Tesla Optimus robot dancing at Gigafactory Samsung Galaxy Ring product launch Wednesday Addams in Tokyo 2025 An ultradetailed illustration of cthulu destroying fleet of battleships wooper portrait of Gandalf dressed up like hello kitty Figure 8: Qualitative comparison of image generations across models for diverse prompts. Each row corresponds to one prompt, with columns showing outputs from left to right: Ours, OmniGen2, Promptist OmniGen2, Promptist SDXL-Base, SDXL-Base, SD2.1, and SD1.4."
        },
        {
            "title": "C EXTENDED TABLE",
            "content": "Table 4: Comparison of Scores for Meme subgroup across different models. The best scores are highlighted in bold. Metric W2I(Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1. Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Human Preference Reward ImageReward HPSv2 86.0 91.0 89.0 83.5 87.0 87.3 3.032 1.546 0.313 68.0 87.5 91.0 75.0 72.5 78.8 2.750 1.279 0. 73.0 85.0 92.0 83.5 73.0 81.3 2.783 1.172 0.299 73.5 84.0 90.5 81.5 73.0 80.5 2.377 0.732 0.280 70.0 82.5 83.0 77.0 69.5 76.4 2.033 0.582 0. 51.0 70.0 71.5 58.5 54.0 60.9 52.6 70.0 72.1 64.7 59.5 63.8 1.309 -0.008 0.252 0.958 -0.315 0.236 Table 5: Comparison of Scores for Real-time News & Events subgroup across different models. The best scores are highlighted in bold. Metric W2I(Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1.4 Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Human Preference Reward ImageReward HPSv 85.0 86.5 92.5 79.0 86.5 85.9 2.615 1.179 0.284 75.0 83.5 88.5 75.5 78.0 80.1 1.712 0.297 0.258 77.9 86.8 89.5 78.4 77.4 82.0 2.131 0.636 0. 78.5 81.5 85.0 75.5 78.0 79.4 1.632 0.210 0.252 80.5 86.5 84.5 76.5 81.5 81.9 1.846 0.429 0.265 65.0 69.5 69.5 65.5 67.5 67.4 65.0 68.5 71.5 66.0 66.0 67. 1.628 0.309 0.245 1.264 -0.047 0.229 Table 6: Comparison of Scores for the Pop Culture & IP subgroup across different models. The best scores are highlighted in bold. Metric W2I(Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1.4 Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Human Preference Reward ImageReward HPSv2 90.5 89.5 95.0 81.5 89.5 89.2 2.218 0.712 0. 80.0 87.5 91.5 77.5 81.0 83.5 1.974 0.441 0.270 83.0 83.5 89.5 83.0 80.5 83.9 1.981 0.375 0.264 88.0 87.5 90.0 83.5 86.5 87.1 2.017 0.471 0. 87.5 84.0 89.0 82.0 86.0 85.7 1.735 0.276 0.273 81.5 77.5 78.0 75.5 81.5 78.8 76.0 73.5 75.5 73.5 75.5 74.8 1.610 0.219 0.259 1.310 -0.017 0. Table 7: Comparison of Scores for the Artists, Celebrities, Influencers subgroup across different models. The best scores are highlighted in bold. Metric W2I(Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1. Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Human Preference Reward ImageReward HPSv2 85.5 88.0 89.0 84.0 81.5 85.6 2.826 1.344 0.293 69.5 84.5 89.0 76.0 69.5 77.7 2.187 0.707 0. 82.0 85.5 91.0 83.0 80.0 84.3 2.511 0.885 0.282 84.2 86.3 89.4 83.2 83.7 85.4 2.276 0.711 0.295 77.9 85.8 85.8 76.3 80.5 81.3 2.050 0.617 0. 73.0 78.5 75.5 75.5 73.5 75.2 1.740 0.354 0.263 70.5 71.0 75.0 73.5 68.5 71.7 1.549 0.239 0."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Comparison of Scores for Niche Concepts subgroup across different models. The best scores are highlighted in bold. Metric W2I(Ours) OmniGen2 Promptist Promptist SDXL-Base OmniGen2 SDXL-Base SD2.1 SD1. Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Human Preference Reward ImageReward HPSv2 90.5 89.5 91.0 94.5 89.5 91.0 3.115 1.574 0.312 76.5 87.0 90.5 81.0 76.5 82.3 2.670 1.150 0. 87.5 87.0 91.0 92.0 84.0 88.3 2.60 0.982 0.296 78.5 85.5 88.5 83.0 77.5 82.6 2.480 0.883 0.290 80.5 86.5 88.0 85.0 79.5 83.9 2.362 0.844 0. 70.5 81.5 78.5 74.0 72.0 75.3 1.757 0.322 0.262 66.5 75.0 79.0 71.5 66.5 71.7 1.443 0.029 0.251 Table 9: Comparison of Promptist Reward and Aesthetic Score across our model and OmniGen2 on three datasets. Lower is better for Promptist Reward; higher is better for Aesthetic Score. Metric NICE DiffusionDB Lexica W2I OmniGen2 W2I OmniGen2 W2I OmniGen2 Promptist Reward Aesthetic Score -0.143 5.961 -0.285 5.936 -0.178 6.184 -0.259 6. -0.117 6.284 -0.210 6.246 Table 10: Performance of World-To-Image on NICE benchmark subgroups. We report LLM-Grader and Human Preference metrics. Metric Meme Real-Time News & Events Pop Culture & IP Artists, Celebrities, Influencers Niche Concept Emotional / Thematic Resonance Consistency & Cohesion Visual Quality & Realism Creativity & Originality Accuracy-to-Prompt Overall Score Human Preference Reward ImageReward HPSv2 86.0 91.0 89.0 83.5 87.0 87.3 3.032 1.546 0.313 85.0 86.5 92.5 79.0 86.5 85.9 2.615 1.179 0.284 90.5 89.5 95.0 81.5 89.5 89. 2.218 0.712 0.279 85.5 88.0 89.0 84.0 81.5 85.6 2.826 1.344 0.293 90.5 89.5 91.0 94.5 89.5 91.0 3.115 1.574 0."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "D.1 ORCHESTRATOR AGENT"
        },
        {
            "title": "Orchestrator Agent",
            "content": "You are an expert orchestrator for multimodal generation model. Your job is to: 1. Analyze the provided image, prompt, scores, and optimization history. 2. Decide the most suitable generation task type: (This is in order of preference) - text image to image: Use reference image + prompt for improved fidelity. (Most recommended) - text to image: Generate image purely from text prompt. - image editing with prompt and reference: Modify the currently generated image according to the prompt and reference image. - image editing with prompt: Modify the currently generated image according to the prompt (inpainting, style transfer, attribute edit)."
        },
        {
            "title": "GUIDELINES",
            "content": "- Image editing is the least recommended task type. - You should only choose image editing if the generated image is very good and you are confident that the prompt is not enough to improve the image."
        },
        {
            "title": "INPUTS",
            "content": "Original Prompt: {original prompt} Current Opimtized Prompt: {current prompt} Detailed Scores: {json.dumps(current scores, indent=2)} Optimization History: {json.dumps(optimization history, indent=2)} Visual Analysis: {visual analysis}"
        },
        {
            "title": "TASK CLASSIFICATION RULES",
            "content": "- text to image: Prompt is self-sufficient; no celebrity/IP likeness, no niche style, no need to preserve an existing image. - text image to image: Prompt includes niche entities (celebrity/IP/meme), rare styles, or ambiguous visuals retrieve TWO references. - image editing with prompt: previously generated image exists AND the new text indicates incremental change (style tweak, color, local edit) without needing specific external reference. - image editing with prompt and reference: previously generated image exists AND the new text implies matching specific look/scene/face/style from known IP or example retrieve ONE reference. DISAMBIGUATION (TEXT-ONLY PROMPTS THAT MIGHT BE EDITS) - If OPTIMIZATION HISTORY shows recent successful generation (e.g., within last step) and DETAILED SCORES indicate high content alignment but style mismatch prefer image editing with prompt. - If the text asks to match specific world/IP/location/face (e.g., Shrek swamp, Monicas apartment, Van Gogh brushwork) prefer image editing with prompt and reference. - If structural changes are large (pose/layout/object count), or prior image is low-quality/incorrect content prefer text image to image (with references if niche) or text to image. - Reference needed should just be simple keyword or list of keywords. STRATEGY SELECTION - text to image [prompt optimizer] - text image to image [prompt optimizer, image retrieval] - image editing with prompt [prompt optimizer] - image editing with prompt and reference [prompt optimizer, image retrieval]"
        },
        {
            "title": "Output Format",
            "content": "Return JSON object: { task type: text to image text image to image image editing with prompt image editing with prompt and reference, strategies: references needed: draft prompt: [prompt optimizer, image retrieval], [reference image 1, reference image 2], Draft prompt for the prompt optimizer to optimize with reference image index not REF., reasoning: Step-by-step reasoning why this task type and strategies were chosen., score analysis: Interpretation of each score and threshold violations., keyword analysis: Which keywords are crucial/missing and how they influence strategy choice., confidence: 0. }"
        },
        {
            "title": "Few Shot Examples",
            "content": "FEW-SHOT EXAMPLES EXAMPLE 1 (TEXT IMAGE TO IMAGE; HARD IP) Prompt: Squid Game S3 teaser poster, Gi-hun in rain-soaked street, neon green mask reflections Output: { task type: strategies: references needed: draft prompt: text image to image, [prompt optimizer, image retrieval], [squid game poster, gi-hun], The poster based on image 1, man from image 2 in rain-soaked street, neon green mask reflections, reasoning: IP + character likeness + specific aesthetic needs two references (Gi-hun, official poster style) to anchor identity and tone., score analysis: clip score low; face similarity target absent; style consistency uncertain retrieval to ground likeness/style., keyword analysis: Squid Game, Gi-hun, neon mask are niche; require grounding., confidence: 0.93 } EXAMPLE 2 (TEXT TO IMAGE; GENERIC BUT DESCRIPTIVE) Prompt: Pixel art of golden retriever surfing giant wave at sunset Output: { text to image, [prompt optimizer], [], task type: strategies: references needed: draft prompt: wave at sunset, reasoning: action, style., Pixel art of golden retriever surfing giant No niche entities; text fully specifies subject, score analysis: image constraints., keyword analysis: are common., semantic alignment expected adequate; no prior pixel art, retriever, surfing, sunset confidence: 0. }"
        },
        {
            "title": "Preprint",
            "content": "Few Shot Examples (cont.) EXAMPLE 3 (IMAGE EDITING WITH PROMPT; TEXT-ONLY PROMPT BUT EDIT PRIOR IMAGE) Context: valid image was just generated (step t-1) of street portrait, female runner mid-stride. Prompt (text-only): Give it 90s VHS sitcom vibe with warm halation and grain; keep the same pose and outfit Output: { task type: strategies: references needed: draft prompt: [], image editing with prompt, [prompt optimizer], Give it 90s VHS sitcom vibe with warm halation and grain; keep the same pose and outfit, reasoning: Text suggests incremental style change to the most recent image while preserving pose/outfit. No specific external reference required., score analysis: prior image available=true; content alignment high=0.86; style mismatch=0.41; edit intent detected=true style-only edit is appropriate., keyword analysis: 90s VHS, grain, halation are style modifiers without named IP no retrieval., confidence: 0.95 } EXAMPLE 4 (IMAGE EDITING WITH PROMPT AND REFERENCE; TEXT-ONLY PROMPT BUT NEEDS IP/BACKGROUND MATCH) # The original image will always be image 1. And there will be only one reference image which is image 2. # Only retrieve one reference image. Context: valid image was just generated (step t-1) of ogre-like character standing in forest clearing. Prompt (text-only): Keep the current pose and lighting but move her to the Shrek swamp and match the movies green tint and fog Output: { task type: strategies: references needed: draft prompt: image editing with prompt and reference, [prompt optimizer, image retrieval], [shrek], Keep the current pose and lighting but move her to the green ogre in image 1 and match the movies green tint and fog, reasoning: User wants to retain existing composition but match specific IP location and look. External visual target needed for accurate palette/props/fog., score analysis: prior image available=true; content alignment high=0.83; location specificity=Shrek swamp; style target=movies green tint requires one reference to lock scene aesthetics., keyword analysis: Shrek swamp, movies green tint, fog IP-scene keywords necessitate reference., confidence: 0.96 }"
        },
        {
            "title": "Preprint",
            "content": "D.2 PROMPT OPTIMIZER AGENT"
        },
        {
            "title": "Prompt Optimizer Agent",
            "content": "ROLE You are the Prompt Optimizer Agent. Rewrite the users request into clean, actionable instruction string for the selected task type. Always produce single JSON object with the following variables: - single string variable named prompt - negative prompts comma-separated string TASK TYPE {task type} INPUTS - ORIGINAL PROMPT: {original prompt} - CURRENT OPTIMIZED PROMPT: {current prompt} - VISUAL ANALYSIS: {visual analysis} - CURRENT SCORES: {score summary} - RECENT OPTIMIZATION HISTORY: {history block} - ORCHESTRATOR REASONING: {reasoning} OBJECTIVES - Preserve essential subject(s), action/intent, and any crucial style/medium cues. - If there are any unclear or ambiguous concepts that the image generator might not know try explaining them in the prompt. - Clarify composition, lighting, lens/camera, time-of-day only when helpful. - Keep wording compact, natural, and non-contradictory. - Append concise negatives if artifacts are likely (e.g., no text artifacts, natural hands). - If concept is niche/ambiguous (celebrity, brand, rare object/place/style) - Always refer to the reference image(s) with image index in the prompt for higher performance."
        },
        {
            "title": "Prompt Optimizer Agent",
            "content": "OUTPUT RULES (CHOOSE EXACTLY ONE CASE BASED ON TASK TYPE) A) text to image { prompt: negative prompts: <refined prompt string>, term1, term2, term3, } Guidelines: - One complete directive (Subject Action/Intent Composition Lighting/Camera Style/Medium). - Rich but controlled descriptors; avoid long enumerations or conflicting specs. B) text image to image { prompt: negative prompts: term1, term2, term3 <composite instruction referencing the reference(s)>, } Guidelines: - Assume the Image Retrieval Agent provides reference image(s) for the niche concept(s). - Instruction should state the intended composition/edit/compositing with those references. - For example Add the cat in image 1 to the background in image 2. - Always refer to the reference image(s) with image index in the prompt for higher performance. C) image editing with prompt { <instruction to improve the current image>, prompt: negative prompts: term1, term2, term3, } D) image editing with prompt and reference { prompt: negative prompts: term1, term2, term3, <instruction to improve using reference(s)>, } Guidelines for Image Editing: - Youre improving an EXISTING image to better match the SAME prompt - Analyze whats wrong with current image (from scores/visual analysis) - For prompt-only editing: focus on lighting, color, style, composition improvements - For reference editing: identify specific elements that need external reference - Keep the core subject/scene but improve quality/accuracy STYLE HEURISTICS - Prioritize: Subject Action/Intent Composition Lighting/Camera Style/Medium. - Use concrete, photography/film/art vocabulary over vague adjectives. - Avoid contradictions (e.g., harsh noon sun + soft night ambience). - If scores/history imply distortions, add short negatives (hands, faces, watermarks, banding, text)."
        },
        {
            "title": "Few Shot Examples",
            "content": "CASE A: TEXT TO IMAGE Original propmt: Sunrise garden macro photography { prompt: The sun rises slightly; clear dew on rose petals; crystal ladybug crawls toward dew bead; early-morning garden backdrop; macro lens., negative prompts: (((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor bar } CASE B1: TEXT IMAGE TO IMAGE Original prompt: Dr Strange in backroom { prompt: Compose scene with the character (Dr Strange) from image 1 standing in dim, fluorescent backrooms corridor from image 2; center-frame, medium shot; flat overhead lighting, subtle fog; emphasize iconic outfit and cape motion., negative prompts: warped hands, banding } text artifacts, over-smoothing, waxy skin, CASE B2: TEXT IMAGE TO IMAGE Original prompt: kids toy in parking lot. { prompt: Place the toy from image 1 into the hands of the person in image 2 in parking-lot setting; align scale and grip; match lighting direction and color temperature., negative prompts: (((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor bar } CASE C: IMAGE EDITING WITH PROMPT Original prompt: Dr Strange in backroom Current image issues: Low lighting quality, poor color balance { prompt: Improve the lighting and color balance of the current character (Dr Strange) in backroom scene; enhance contrast and fix dim areas; maintain character pose and backroom atmosphere, negative prompts: overexposure, harsh shadows, color banding, washed out colors, } CASE D: IMAGE EDITING WITH PROMPT AND REFERENCE Original prompt: Dr Strange in backroom Current image issues: Character face doesnt look like Dr Strange { prompt: Fix the characters face in the current backroom scene to match image 2 (character (Dr Strange)); maintain the existing pose and backroom setting in image 1; improve facial accuracy, negative prompts: wrong face, generic face, blurry features, face artifacts, } Note: Emit exactly one case per call based on task type. No extra text outside the JSON object."
        },
        {
            "title": "Preprint",
            "content": "D."
        },
        {
            "title": "Image Retrieval Agent",
            "content": "You are an expert visual analyst evaluating reference images for text-to-image generation. CONTEXT: - Original prompt: {original prompt} - Search query: {query} - Category: {category} - Purpose: Select the best reference images to guide AI image generation. - You must select at least one image. TASK: Analyze each provided image and evaluate how well it matches the search query and would help generate the target prompt. For {category} category: - CONTENT: Look for objects, people, locations, compositions that match the query - STYLE: Look for artistic styles, visual aesthetics, color palettes, techniques - CONTEXT: Look for environmental context, mood, atmosphere, setting details EVALUATION CRITERIA: 1. Query Match: How well does the image match the specific search query? 2. Visual Quality: Is the image clear, well-composed, and visually appealing? 3. Usefulness: Would this image provide good visual guidance for AI generation? 4. Distinctiveness: Does it offer unique visual information not found in other candidates? INSTRUCTIONS: - Rate each image from 0.0 to 1.0 (higher = better) - Select up to {max selections} best images - Provide brief reasoning for each selection Respond with ONLY JSON object in the following format (this is an example): { selections: [ { image index: score: reasoning: 0.85, 0, Excellent match for query, high visual quality, provides clear guidance }, { image index: score: reasoning: 0.72, 1, Good secondary option with different angle/perspective } ] } Only include images you would actually select (score 0.6). If you are not sure about the images, you can select multiple images. Low scores are allowed."
        },
        {
            "title": "Query Rewriting Prompt",
            "content": "You are an expert at creating image search queries. search query failed to return any images from an image search API. CONTEXT: - Original text prompt: original prompt - Failed search query: original query - Goal: Find reference images to help generate the target prompt TASK: Create better, more searchable query that is likely to return relevant images. Consider: - Simplify complex terms: Replace uncommon/specific terms with more common alternatives - Add descriptive keywords: Include visual descriptors that would help find relevant images - Use popular terms: Replace niche concepts with mainstream equivalents - Consider synonyms: Use alternative words that mean the same thing - Focus on visual elements: Emphasize what the image should look like rather than abstract concepts EXAMPLES: - Dr Strange Marvel superhero with cape or sorcerer with magic - backroom yellow fluorescent office space or liminal empty rooms - cyberpunk hacker futuristic computer user neon lights - medieval knight armored warrior with sword Respond with ONLY the modified search query, nothing else. Make it 2-6 words that would likely return relevant images."
        },
        {
            "title": "Visual Analysis Prompt",
            "content": "You are an expert at analyzing images and detecting AI-generated artifacts. Provide concise, focused analysis. Analyze this image and compare it with the text: prompt. Focus on: 1) What the text describes well vs. what it misses 2) Any hallucinations or distorted details that dont match the prompt. 3) Any elements that are not shown in the text but should be added. 4) Visual enhancements for better generation quality Be specific about enhancement opportunities that dont conflict with the original intent."
        },
        {
            "title": "Preprint",
            "content": "D.4 LLM GRADER"
        },
        {
            "title": "LLM Grader Prompt",
            "content": "You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. 1. Key Evaluation Aspects and Scoring Criteria: For each aspect, provide score from 0 to 10 (0 = poor, 10 = excellent) and short justification (12 sentences). - Accuracy to Prompt Assess how well the image matches the prompt: elements, objects, and setting. - Creativity and Originality Judge whether the image shows imagination beyond literal interpretation. - Visual Quality and Realism Evaluate resolution, detail, lighting, shading, and perspective. - Consistency and Cohesion Check whether all elements are coherent and free from anomalies. - Emotional or Thematic Resonance Assess whether the image conveys the intended mood or tone. 2. Overall Score: Provide an overall score as weighted or simple average of all aspects. Please evaluate the following image based on the prompt: \"{prompt}\" Respond with JSON object in this exact format: { } \"accuracy_to_prompt\": { \"score\": <0-10>, \"explanation\": \"<1-2 sentence explanation>\" }, \"creativity_and_originality\": { \"score\": <0-10>, \"explanation\": \"<1-2 sentence explanation>\" }, \"visual_quality_and_realism\": { \"score\": <0-10>, \"explanation\": \"<1-2 sentence explanation>\" }, \"consistency_and_cohesion\": { \"score\": <0-10>, \"explanation\": \"<1-2 sentence explanation>\" }, \"emotional_or_thematic_resonance\": { \"score\": <0-10>, \"explanation\": \"<1-2 sentence explanation>\" }, \"overall_score\": <0-10>"
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "The Hong Kong University of Science and Technology",
        "TwelveLabs",
        "University of Massachusetts Amherst"
    ]
}