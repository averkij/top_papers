{
    "paper_title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B",
    "authors": [
        "Sen Xu",
        "Yi Zhou",
        "Wei Wang",
        "Jixin Min",
        "Zhibin Yin",
        "Yingwei Dai",
        "Shixi Liu",
        "Lianyu Pang",
        "Yirong Chen",
        "Junlin Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research."
        },
        {
            "title": "Start",
            "content": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B Sen Xu, Yi Zhou, Wei Wang, Jixin Min, Zhibin Yin, Yingwei Dai, Shixi Liu, Lianyu Pang, Yirong Chen, Junlin Zhang Sina Weibo Inc. OpenAIs o1 model has established new reasoning paradigm through Long Chain-of-Thought, marking significant progress in reasoning technology. The prevailing approach continues to rely on scaling model parameters to enhance capabilitiesfor example, DeepSeek R1 reaches 671B parameters, and Kimi k2 exceeds 1T. mainstream consensus holds that small models inherently lack robust reasoning capabilities. This technical report challenges that notion, demonstrating that this assumption may be incorrect. We introduce VibeThinker1.5B, 1.5B-parameter dense model developed using an innovative post-training methodology centered on the Spectrum-to-Signal Principle (SSP). This framework systematically enhances output diversity by first employing Two-Stage Diversity-Exploring Distillation in the SFT phase to generate broad spectrum of solutions, followed by the MaxEnt-Guided Policy Optimization (MGPO) framework in the RL phase to amplify the correct signal. With total training cost of $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models Magistral Medium and Claude Opus 4, while achieving performance on par with open-source models like GPT OSS-20B Medium. Most remarkably, it surpasses the initial DeepSeek R1 modelwhich is over 400 times largeracross three challenging mathematical benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This marks substantial improvement over its base model, which scored 6.7 on AIME24, 4.3 on AIME25, and 0.6 on HMMT25. Similarly, on the LiveCodeBench V6 coding benchmark, VibeThinker-1.5B achieves score of 51.1, slightly outperforming Magistral Mediums 50.3 and substantial improvement over the base models 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing the associated costs for training and inference and thereby democratizing access to advanced AI research and accelerating technological progress. We release our post-trained model checkpoint to support future research. Date: Nov. 7, 2025 Github: https://github.com/WeiboAI/VibeThinker HuggingFace: https://huggingface.co/WeiboAI/VibeThinker-1.5B Mail: { xusen1, junlin6 }@staff.weibo.com 5 2 0 1 2 2 6 0 . 1 1 5 2 : r Figure 1. Performance of VibeThinker-1.5B versus competing models on representative benchmarks. 1 Tech Report, VibeThinker-1.5B, Weibo"
        },
        {
            "title": "1 Introduction",
            "content": "OpenAI o1[20] pioneered the Large Reasoning Model (LRM) paradigm, significantly enhancing the logical reasoning capabilities of large language models through reinforcement learning and extended chain-of-thought processes. It achieved human-level performance in complex domains such as mathematical theorem proving, clinical diagnosis, and competitive programming, reaching expert-level proficiency in scientific reasoning and significantly surpassing previous state-of-the-art models. Subsequent open-source projects, including DeepSeek-R1[12], Qwen3[35], and GLM 4.5[37], further advanced these capabilities through optimized training data selection strategies and improved reinforcement learning (RL) algorithms. These efforts established both RL Scaling and test-time scaling as key optimization strategies. By allocating more computational resources during training and inferenceguided by refined reward models and multi-path explorationmodel accuracy on real-world problems consistently improves. The LRM paradigm has thus redefined scaling laws for reasoning-centric training, accelerating progress toward robust, generalist AI systems capable of tackling open-ended intellectual challenges. Industry consensus holds that scaling model parametersexemplified by DeepSeek R1 (671B)[12], Kimi-K2[28], and Qwen3Max[29] (>1T)is essential for enhancing capabilities like logical reasoning, with small models deemed significantly inferior. However, we challenge this view by investigating whether compact models (e.g., 1.5B \"Tiny models\") can match the reasoning performance of state-of-the-art large models. An affirmative answer would imply that the industry need not rely solely on extreme parameter scaling but could prioritize advancing small models, drastically reducing training/inference costs, energy consumption, and environmental impact. While recent small models like DeepscaleR[18], ProRL[17], and Qwen3-1.7B[35] show promise, they have yet to fully exploit the potential of logical reasoning. This technical report demonstrates that 1.5B model, with appropriate training, can achieve reasoning parity with todays largest models. We open-source VibeThinker-1.5B not as deployable solution but to prove that small models possess far greater reasoning capabilities than previously assumed. In this report, we introduce VibeThinker-1.5B, tiny dense language model with powerful reasoning capabilities, whose development is guided by the Spectrum-to-Signal Principle (SSP). This principle redefines the post-training pipeline by decoupling the objectives of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) into two distinct, synergistic phases. First, the SFT stage operates as the \"Spectrum Phase,\" where we employ Diversity-Exploring Distillation methodology to cultivate broad spectrum of diverse solutions, rather than solely maximizing single-shot accuracy. Subsequently, the RL stage functions as the \"Signal Phase,\" utilizing the MaxEnt-Guided Policy Optimization (MGPO) framework to identify and amplify the most effective reasoning paths from this pre-established spectrum. MGPO dynamically prioritizes training on problems where the model exhibits high uncertainty, thereby maximizing learning efficiency. By systematically integrating these two phases, our approach establishes diversity as the central technical design principle, enabling VibeThinker-1.5B to achieve robust performance that surpasses conventional training paradigms. VibeThinker-1.5B redefines the efficiency frontier for reasoning models, achieving state-of-the-art performance in mathematical and coding tasks with only 1.5B parameters100 to 600 smaller than giants like Kimi K2 (1000B+) and DeepSeek R1(671B). Trained for under $8,000, VibeThinker-1.5B, the leading model in the sub-3B category, demonstrates superior performance on challenging mathematical and coding benchmarks, surpassing many significantly larger and more powerful models. Our model exhibits exceptional performance, consistently exceeding both larger reasoning models and state-of-the-art non-reasoning models. On the demanding AIME24, AIME25, and HMMT25 mathematical tests, it surpasses the massive DeepSeek R1-0120 (over 400x larger) with scores of 80.3 vs. 79.8, 74.4 vs. 70.0, and 50.4 vs. 41.7, respectively. This competitive edge extends to non-reasoning models, where it significantly outperforms Kimi-K2-Instruct on the AIME24 math benchmark (80.3 vs. 69.6) and GPT-4.1 on the LiveCodeBench v6 coding benchmark (51.1 vs. 44.7). Our key insight is that meticulous algorithmic design enables compact models (e.g., 1.5B parameters) to achieve logical reasoning capabilities in mathematics, code, and scientific tasks comparable to models tens to hundreds of times larger. This reveals the underestimated potential of small-scale models in reasoning. While the study by Belcak et al. (2025) [8] proposes small models as the future of autonomous agents, these remain theoretical. We substantiate this perspective empirically by developing 1.5B parameter \"Tiny\" model with strong reasoning performance. Powerful small models not only significantly reduce the costs associated with training large models and performing online inferencethereby promoting broader adoption of AI applicationsbut also address critical issue of research accessibility. The current emphasis on scaling model parameters has concentrated cutting-edge research within handful of technology companies (e.g., OpenAI, Anthropic, Google, xAI) that possess vast computational resources. This monopolization marginalizes many corporations and universities, which often host abundant high-quality research talent but lack sufficient hardware, preventing them from contributing to the frontier of large model development. If small models demonstrate competitive Tech Report, VibeThinker-1.5B, Weibo Figure 2. VibeThinker-1.5B demonstrates remarkable efficiency, surpassing much larger and stronger models on the challenging AIME25 benchmark. It achieves score of 74.4, outperforming strong baselines such as GPT-OSS-20B-Medium (72.1/20B), DeepSeek-R1-0120 (70.0/671B), and Seed-Thinking v1.5 (74.0/200B). performance with large models across multiple domains, the significantly lower development costs would broaden participation, enabling wider research community to contribute and thus accelerating progress in large model technology. This would prevent core advancements from being concentrated in few commercial entities. The advancement of compact model research therefore holds profound, albeit often implicit, significance. Although we have identified considerable performance gap between small and large models in general knowledge benchmarks, we believe this technical challenge can be addressed through efficient methodological improvements."
        },
        {
            "title": "2 Preliminaries",
            "content": "Supervised Fine-Tuning (SFT). Supervised Fine-Tuning (SFT) adapts pre-trained language model, parameterized by 洧랚 , to downstream tasks using labeled dataset = {(洧논, 洧녽)} , where 洧녽 denotes the reference response for given input 洧논 . The model defines an autoregressive conditional distribution 洧랢洧랚 (洧녽洧논) over response sequences. The training objective is to minimize the cross-entropy loss: LSFT(洧랚 ) = E(洧논,洧녽)D [ log 洧랢洧랚 (洧녽洧논)] which is equivalent to maximizing the likelihood of the target responses under the model distribution. This process enhances task-specific alignment while preserving pre-trained knowledge. Group Relative Policy Optimization (GRPO). Group Relative Policy Optimization (GRPO) [26] is reinforcement learning algorithm that extends Proximal Policy Optimization (PPO) [25] by replacing the critic-based advantage estimation with group-relative mechanism. For given query 洧, group of 洧냨 responses {洧녽洧녰 }洧냨 洧녰=1 is sampled from the old policy 洧랢洧랚old (洧). Each response 洧녽洧녰 is assigned reward 洧洧녰 = 洧 (洧, 洧녽洧녰 ). The advantage for each token 洧노 in response 洧녽洧녰 is then computed relative to the groups reward statistics: A洧녰,洧노 (洧) = 洧洧녰 洧랞 洧랥 Tech Report, VibeThinker-1.5B, Weibo where 洧랞 and 洧랥 are the mean and standard deviation of the rewards within the group. This approach reduces variance and eliminates the need for an external critic model. The optimization objective is formulated as clipped surrogate loss, averaged over tokens and responses within the group: JGRPO(洧랚 ) = E(洧,洧녽)D (cid:34) {洧녽洧녰 }洧냨 洧녰=1洧랢洧랚 old ( 洧) (cid:34) 1 洧냨 洧냨 洧녰=1 1 洧녽洧녰 洧녽洧녰 洧노 =1 (cid:0)min (cid:0)洧洧녰,洧노 (洧랚 )A洧녰,洧노 (洧), clip (cid:0)洧洧녰,洧노 (洧랚 ), 1 洧, 1 + 洧(cid:1) A洧녰,洧노 (洧)(cid:1)(cid:1) (cid:35) (cid:35) where 洧洧녰,洧노 (洧랚 ) = divergence penalty relative to reference policy is often added as regularizer. (洧녽洧녰,洧노 洧,洧녽洧녰,<洧노 ) is the token-level probability ratio, and 洧 controls the clipping range. For stability, KLold 洧랢洧랚 (洧녽洧녰,洧노 洧,洧녽洧녰,<洧노 ) 洧랢洧랚 The Relationship Between Pass@K and Diversity. Output diversity in large language models (LLMs) refers to the variability in generated responses for given input, which is crucial for enhancing the models problem-solving robustness and creativity. High diversity enables the exploration of multiple reasoning paths and perspectives, preventing the model from overfitting to narrow solution patterns and increasing the likelihood of discovering novel and effective solutions. In contrast, low diversity often leads to repetitive or suboptimal outputs, limiting the models ability to handle complex tasks requiring adaptive reasoning. This capability is particularly vital in domains like mathematical problem-solving or code generation, where exploring alternative approaches significantly improves performance. Current research[10] commonly adopts the Pass@K metric as key indicator for assessing the diversity of outputs generated by large language models (LLMs). Pass@K measures the probability that at least one of independently generated solutions passes verification test (e.g., functional correctness for code or factual accuracy). Formally, for problem 洧논 and model 洧랢洧랚 , it is defined as: Pass@K = E洧논D,{洧녽洧녰 }洧녲 洧녰=1洧랢洧랚 ( 洧논 ) [max{洧녠(洧논, 洧녽1), . . . , 洧녠(洧논, 洧녽洧녲 )}] where 洧녠(洧논, 洧녽) is binary reward function indicating correctness. Diversity directly contributes to higher PASS@K scores, as varied set of solutions broadens the exploration space and reduces the risk of all outputs failing. Empirical studies show strong positive correlation between solution diversity and potential performance gains after reinforcement learning (RL) training, underscoring that diversity enhances the models capacity to achieve verifiable success. Consequently, optimizing for Pass@K during supervised fine-tuning (SFT) is critical, as it encourages the development of diverse solution repertoire, thereby improving both exploration and eventual task performance."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 The Spectrum-to-Signal Principle for SFT-RL Synergy The sequential training pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) is cornerstone of modern large language model development. However, the optimal interface between these two stages remains critically under-explored area. The prevailing, yet implicit, assumption is to select the SFT checkpoint that maximizes single-shot accuracy (Pass@1) and then use RL to further refine this same metric. We posit that this approach is suboptimal as it artificially constrains the potential performance ceiling for the subsequent RL phase. To address this, we introduce the \"Spectrum-to-Signal Principle (SSP)\", theoretical framework that redefines the roles of and the synergy between SFT and RL. In SSP, the two stages are assigned distinct, complementary objectives: - The Spectrum Phase (SFT): The primary goal of SFT is not to converge on single optimal answer, but to generate rich and diverse \"spectrum\" of plausible solutions. This phase maximizes the models Pass@K metric, effectively creating broad \"candidate space\" of correct answers. model with high Pass@K provides richer foundation for exploration, thereby raising the upper bound of what RL can achieve. - The Signal Phase (RL): The role of RL is then to identify and amplify the correct \"signal\" from within this pre-established spectrum. By receiving reward signals, the RL phase learns to increase the generation probability of the most correct and effective answers from the diverse pool provided by the SFT phase. This principle posits that an SFT checkpoint optimized for diversity (Pass@K) is superior prerequisite for RL, as it presents the RL algorithm with more fertile ground for optimization compared to narrow, Pass@1-optimized model. 3.2 Training Pipeline Tech Report, VibeThinker-1.5B, Weibo Figure 3. The Training Pipeline of VibeThinker-1.5B Our post-training process is guided by the \"Spectrum-to-Signal Principle\" (SSP), which conceptualizes Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) as sequential two-stage optimization. Unlike conventional approaches that prioritize Pass@1 in SFT, the initial stage is dedicated to cultivating broad \"spectrum\" of diverse solutions. The subsequent RL phase then amplifies the correct \"signal\" from this pool. By intentionally fostering diversity first, this method provides richer foundation for RL, leading to more robust reasoning and enhanced problem-solving capabilities. The SFT stage, designated as the \"Spectrum Phase\", implements this principle through \"Two-Stage Diversity-Exploring Distillation\" methodology. Initially, \"Domain-Aware Diversity Probing\" is conducted to analyze broad domains (e.g., mathematics, code) and identify sub-domains. This process pinpoints specialist SFT checkpoints that exhibit the highest diversity, as measured by Pass@K, within each sub-domain. Subsequently, \"Expert Model Fusion\" consolidates these optimal checkpoints using techniques like model merging. The result is unified SFT model that embodies maximized spectrum of plausible solutions, providing fertile ground for the subsequent RL phase. The RL stage, designated as the \"Signal Phase\", is guided by the \"MaxEnt-Guided Policy Optimization (MGPO)\" framework. MGPO leverages information-theoretic principles to dynamically prioritize the most pedagogically valuable problems for on-policy learning, specifically those where the policys performance exhibits the highest uncertainty, representing critical learning frontier. The RL phase is structured into sub-stages, beginning with mathematical reasoning within 16K context window, expanding to 32K, and followed by code generation. MGPO enhances the exploration capacity of RL by modifying the advantage calculation to incentivize the increased generation probability of low-probability yet correct reasoning traces sampled during rollouts. This allows the model to effectively identify and amplify the correct signal from the diverse solution spectrum established by SFT. By operationalizing the Spectrum-to-Signal Principle across the entire training pipeline, we successfully construct VibeThinker1.5B, small-scale model with powerful reasoning capabilities. This deliberate focus on diversity, from spectrum generation in SFT to signal amplification in RL, serves as the central theme and technical design principle of our optimization strategy, unlocking performance that surpasses the constraints of traditional training paradigms. 3.3 Diversity-Exploring Distillation Two-Stage Diversity-Exploring Distillation To implement the spectrum phase of SSP, we propose two-stage methodology: \"Domain-Aware Diversity Probing\" to identify specialist models, followed by \"Expert Model Fusion\" to synthesize unified, diversity-maximized SFT model. - Domain-Aware Diversity Probing To operationalize the spectrum phase, we first partition the mathematical knowledge space into 洧녜 distinct subdomains, = {洧녡1, 洧녡2, . . . , 洧녡洧녜 }. For instance, in our work on mathematical reasoning, we define 洧녜 = 4 Tech Report, VibeThinker-1.5B, Weibo with = {洧녡algebra, 洧녡geometry, 洧녡calculus, 洧녡statistics}. For each subdomain 洧녡洧녰 , we employ capable LLM to automatically construct specialized probing set, 洧냥洧녰 = {(洧륋롐 洧녱, 洧녩洧녰 洧녱 ) 洧녱 = 1, . . . , 洧냥洧녰 }, where 洧륋롐 洧녱 is problem and 洧녩洧녰 洧녱 its ground-truth answer. During SFT training, we periodically evaluate intermediate model checkpoints 洧洧노 (saved every 洧녲 steps) on each probing set 洧냥洧녰 using the Pass@K metric, yielding score 洧녞洧녰 (洧노). The checkpoint that maximizes this metric for given subdomain is selected as the specialist model: 洧 洧녰 = arg max 洧노 洧녞洧녰 (洧노) This process yields set of 洧녜 diversity-maximizing specialist models, {洧 1 solutions within its respective mathematical subdomain. , 洧 2 , . . . , 洧 洧녜 }, each excelling at generating diverse - Expert Model Fusion Having identified the specialist models, we synthesize them into single, comprehensive SFT Merge, is constructed as weighted linear combination of the model optimized for the spectrum phase. This fused model, MSFT specialist model parameters: MSFT Merge = 洧녜 洧녻洧녰洧 洧녰 洧녰=1 where the weights 洧녻洧녰 are non-negative and sum to unity ((cid:205)洧녜 洧녰=1 implementation for VibeThinker-1.5B, we employ an unweighted averaging scheme where 洧녻洧녰 = 1 integration of the diverse capabilities from all subdomains. 洧녻洧녰 = 1) to preserve the models parameter scale. In our 洧녜 for all 洧녰, ensuring equitable Empirically, our findings confirm the core tenets of the Spectrum-to-Signal Principle. The model MSFT Merge, synthesized via Pass@K maximization, demonstrates remarkable dual optimization: it attains state-of-the-art performance on both the Pass@K (diversity) and Pass@1 (accuracy) metrics. This result indicates that the optimization of models generative spectrum is not at the expense of its primary signal strength. On the contrary, broader spectrum appears to reinforce the most correct pathways. This establishes powerful paradigm where the SFT spectrum phase creates synergistic foundation, maximizing the potential for performance gains in the subsequent RL signal phase. 3.4 MaxEnt-Guided Policy Optimization (MGPO) In reinforcement learning from human feedback (RLHF), particularly for complex reasoning tasks, the selection of training data is paramount. static dataset often presents uniform challenge, failing to adapt to the evolving capabilities of the policy model. We propose \"MaxEnt-Guided Policy Optimization (MGPO)\", novel framework that leverages information-theoretic principles to dynamically identify and prioritize the most pedagogically valuable problems for on-policy learning. Our core hypothesis is that problems utility for training is maximized when the policys current performance on it exhibits the highest uncertainty, as this signifies critical learning frontier where the model is most receptive to exploration and improvement. Maximum Entropy as an Ideal for Exploration. This process induces binary distribution over the outcomes (correct vs. incorrect) for question 洧. Let 洧녷洧녫 (洧) be the empirical probability of correct answer, derived from the 洧냨 rollouts: 洧녷洧녫 (洧) = 洧냨 I(洧洧녰 = 1) 1 洧냨 洧녰=1 where I() is the indicator function. According to the principle of maximum entropy, this distribution is most \"uninformed\" or uncertain when its entropy is maximized. For binary distribution, the maximum entropy occurs when 洧녷洧녫 (洧) = 0.5. In this state, the model is completely uncertain about the correct answer; it is neither consistently correct nor consistently wrong. We argue that this state of maximum uncertainty represents problem with optimal \"exploratory value\". Such problem lies at the very edge of the models current capabilities, making it an ideal candidate for policy optimization. Entropy Deviation Regularization. While directly using the Shannon entropy 洧냩 (洧) is an intuitive approach, we propose more targeted weighting scheme that explicitly measures and penalizes deviation from the ideal maximum-entropy state. We term this \"Entropy Deviation Regularization\". The core idea is to define \"distance\" from the ideal distribution and use this distance to modulate the learning signal. We define the \"Max-Entropy Deviation Distance\", 洧냥ME (洧녷洧녫 (洧)洧녷0), as the Kullback-Leibler (KL) divergence between the observed accuracy 洧녷洧녫 (洧) and the target maximum-entropy distribution 洧녷0 = 0.5. This metric effectively quantifies how much the models performance has deviated from the state of optimal uncertainty. The distance is calculated as: Tech Report, VibeThinker-1.5B, Weibo 洧녷洧녫 (洧) 洧녷0 Using this distance, we construct weighting function, 洧녻ME, that assigns the highest weight to questions where the accuracy is closest to 0.5 and exponentially suppresses the weight as the accuracy moves towards 0 or 1: 洧냥ME (洧녷洧녫 (洧)洧녷0) = 洧녷洧녫 (洧) log 1 洧녷洧녫 (洧) 1 洧녷 + (1 洧녷洧녫 (洧)) log 洧녻ME (洧녷洧녫 (洧)) = exp(洧랝 洧냥ME (洧녷洧녫 (洧)洧녷0)), where 洧녷0 = 0.5, 洧랝 0 Here, 洧랝 is regularization coefficient that controls the sharpness of the weighting. When 洧랝 = 0, 洧녻ME = 1 and the algorithm degrades to standard GRPO. As 洧랝 increases, the penalty for deviating from the 0.5 accuracy becomes more severe, focusing the training more aggressively on the most uncertain problems. This weighting function is applied directly to the advantage term within the GRPO objective. The updated advantage for each rollout 洧녱 in group for question 洧 is: 洧녱 (洧) = 洧녻ME (洧녷洧녫 (洧)) 洧녱 (洧) The MGPO Optimization Objective MGPO integrates this entropy deviation weight directly into the GRPO optimization process. The modified objective function, JMGPO (洧랚 ), is formulated as: JMGPO (洧랚 ) = E(洧,洧녽)D (cid:34) {洧녽洧녰 }洧냨 洧녰=1洧랢洧랚 old ( 洧) (cid:34) 1 洧냨 洧냨 洧녰=1 1 洧녽洧녰 洧녽洧녰 洧노 =1 (cid:0)min (cid:0)洧洧녰,洧노 (洧랚 )A 洧녰,洧노 (洧), clip (cid:0)洧洧녰,洧노 (洧랚 ), 1 洧, 1 + 洧(cid:1) 洧녰,洧노 (洧)(cid:1)(cid:1) (cid:35) (cid:35) 洧녱 (洧). This creates an implicit In this formulation, the standard GRPO objective is modulated by the entropy deviation weight curriculum learning mechanism where the model is automatically steered towards focusing its gradient updates on questions for which its current performance is most ambiguous. By doing so, MGPO ensures that the computational budget is spent on the most impactful learning opportunities, fostering more robust exploration and efficient policy improvement grounded in the principle of maximizing uncertainty. 3.5 Training Data & Decontamination For model training, the majority of data was derived from publicly available open-source datasets, while minor portion originated from proprietary synthetic data generated internally to enhance domain-specific coverage and robustness. To ensure the impartiality of model evaluation and the authenticity of generalization capabilities, we implemented rigorous data decontamination procedures on the training data during both the Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages. The primary objective of this process is to eliminate semantic overlap or information leakage risks between the training data and evaluation sets, thereby preventing assessment biases caused by data contamination. The specific operations include: (1). Text Standardization and Preprocessing: Prior to matching, texts were normalized by removing irrelevant punctuation, symbols, and unifying letter cases to reduce noise interference and enhance matching accuracy. (2). Semantic Decontamination: We employed 10-gram matching to identify and exclude training samples potentially overlapping semantically with evaluation sets. By reducing the n-gram length, we increased matching sensitivity to capture local semantic similarities more precisely, thereby strengthening the rigor of decontamination. These measures significantly mitigate the risks of information leakage, ensuring that the performance of the model in the core evaluations, such as mathematical reasoning (for example, AIME24 / AIME25 [5]) and code generation (e.g., LiveCodeBench[14])faithfully reflects its true generalization and reasoning capabilities. There is an ongoing debate regarding whether certain base models have undergone adequate data decontamination. Some study by Wu et al. (2025)[32] attempted to demonstrate that the Qwen 2.5-7B[36] model suffers from MATH500[16] data leakage, arguing that this could explain why even incorrect RL reward signals might lead to seemingly good results. In contrast, another study by Wu et al. (2025)[31] contends that data leakage is not the primary factor; instead, it emphasizes the critical role of model-task alignmentdefined as the congruence between models inherent capabilities and the requirements of task. According to this view, strong innate model abilities can be effectively activated with minimal or even noisy training signals, especially within the models domain of competence. Tech Report, VibeThinker-1.5B, Weibo The conclusions of Study[31] are more consistent with our findings. Even assuming the validity of the data leakage concerns raised in Study[32], they do not adequately account for the advanced logical reasoning abilities exhibited by our VibeThinker1.5B model. Our model is built upon Qwen2.5-Math-1.5B[36], base model released in September 2024. Despite this, VibeThinker1.5B demonstrated strong performance on multiple benchmarks released in 2025, including scores of 74.4 on the AIME25[5] (surpassing DeepSeek R1s 70.0) and 50.4 on the HMMT25[7] (outperforming DeepSeek R1s 41.7). This performance strongly indicates that the results are not product of training data contamination, as both the AIME25 and HMMT25 benchmarks were not publicly released until 2025. This timeline precludes their inclusion in the training data of any model finalized prior to that date, including our base mode. Furthermore, the base model itself showed very weak capabilities in hard coding tasks, scoring 0.0 on both LiveCodeBench v5 and v6[14]. Through our innovative post-training methodology, we significantly improved these scores to 55.9 on LiveCodeBench v5 and 51.1 on v6the latter even surpassing Magistral medium[4]s score of 50.3 on LiveCodeBench v6. These marked improvements across multiple new benchmarks and domains reinforce that factors beyond base model data contamination, such as targeted alignment and enhanced training techniques, are driving the performance gains. 3.6 Training Cost Table 1. Comparison of Post-Training Costs Models DeepScaleR MiniMax-M1 DeepSeek-R1 VibeThinker Size AIME25 Score GPU Type GPU Hours Total Cost 1.5B 456B 671B 1.5B $4.5K $535K $294K $7.8K A100 H800 H800 3.8K 258K 147K 3.9K 31.5 74.6 70.0 74.4 We emphasize the exceptionally low post-training cost of VibeThinker-1.5B, which is directly attributable to its compact architecture of 1.5 billion parameters. Throughout the supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR) stages, the entire training process consumed approximately 3900 GPU hours on NVIDIA H800 GPUs. Based on market rental rate of $2 per GPU hour for H800 instances, the total computational cost amounted to less than $8K. Our 1.5B model demonstrates exceptional performance on the AIME25 benchmark, exceeding DeepSeek-R1-0120[12] and achieving results comparable to MiniMax-M1[9]. Although gap to the best-in-class SOTA models persists, it has narrowed to point where it is no longer considered an inherent limitation of small-scale models. The post-training cost of our 1.5B model is lower by one to two orders of magnitude compared to SOTA large reasoning models. While models like DeepSeek R1 and MiniMax-M1[9] incur post-training expenses of $294K and $535K respectively, our models expenditure is only 1/30 to 1/60 of these figures  (Table 1)  , demonstrating significant breakthrough in cost-effective training. When further considering inference service expenses, the 1.5B parameter model not only supports deployment on edge devicessuch as mobile phones and vehicle-embedded systemsbut also reduces inference costs by 20 to 70 times compared to state-of-the-art large-scale models. This starkly underscores the significant advantages of small models in terms of training efficiency, deployment flexibility, and overall cost-effectiveness."
        },
        {
            "title": "4 Evaluation and Analysis",
            "content": "4.1 Experimental Setup Benchmarks. To comprehensively evaluate the quality of our reasoning models, we employed automated benchmarking across the following key domains: -Mathematics: To assess mathematical reasoning capabilities, we employ suite of challenging mathematical benchmarks, including MATH-500 [16], HMMT 2025 [7], AIME 2024, and AIME 2025[5]. To quantify model performance on the problems, we report the average pass rate across 64 repeated sampling trials as the definitive metric. -Coding: We assess general programming proficiency using LiveCodeBench V5 and V6 [14]. V5 comprises 279 problems from August 2024 to February 2025. Notably, V6 has two definitions: our evaluation uses the first (131 problems, February 2025May 2025), while some comparative models may have used the second (454 problems, August 2024May 2025), which generally yields higher scores. For both benchmarks, the final score is the average pass rate from 8 sampled outputs. Table 2. Performance of VibeThinker-1.5B on Core Benchmarks (Small Reasoning Models) Model Mathematics Coding Knowledge Params Institution AIME 2024 AIME 2025 MATH 500 HMMT25 LCB v5 LCB v6 GPQA Diamond Tech Report, VibeThinker-1.5B, Weibo Name L1-Max STILL-3 DeepScaleR 1.5B 1.5B 1.5B DeepSeek-Distill-Qwen 1.5B 1.5B FastCURL-v3 1.5B ProRL 1.8B Hunyuan-0729 1.7B Qwen3 3B SmolLM CMU RUC UC Berkeley DeepSeek Tencent NVIDIA Tencent Alibaba HuggingFace Base Model VibeThinker-1.5B 1.5B 1.5B Weibo 27.7 34.7 43. 28.5 49.6 48.1 56.7 48.3 6.7 80.3 21.0 24.0 31.5 22.7 34.4 33.3 36.8 36.7 4.3 74.4 84.7 86.6 87. 82.9 90.5 91.9 86.0 93.4 87.5 58.5 95.0 9.9 13.9 19.0 13.6 21.5 20.5 23.6 26.0 0.6 50.4 16.8 23.8 31.5 33.2 27.6 0.0 55.9 16.3 12.8 20.6 26.9 29.1 0.0 51.1 20. 15.9 23.2 41.8 35.8 40.1 41.7 16.4 46.7 -Knowledge: To quantify expertise in specialized domains and complex reasoning abilities, we utilize GPQA-Diamond[24]a graduate-level benchmark comprising 198 Ph.D.-level questions across biology, physics, and chemistry. Baselines. To evaluate the reasoning capabilities of VibeThinker-1.5B, we compare its performance against three distinct groups of SOTA models: (1). The most powerful sub-3B open-source reasoning models from both academia and companies, including STILL-3[19], L1-Max[3], DeepscaleR[18], FastCURL[27], ProRL[17], Qwen3-1.7B[35], Hunyuan 1.8B[30] and SmolLM-3 3B[1]. (2). Advanced reasoning models featuring Long-CoT capabilities are developed by both proprietary and open-source communities. Notable proprietary models include Magistral Medium (Mistral AI)[4], Claude Opus 4 thinking (Anthropic)[6], Gemini 2.5 Flash thinking (Google)[11], and OpenAI o3-mini-Medium[23]. In the open-source domain, key examples comprise MiMo-7B-RL (Xiaomi)[34], Phi-4 Reasoning (14.7B) (Microsoft)[2], Qwen3 32B thinking (Alibaba)[35], DeepSeek R1 (671B)[12], GPT-OSS-20B-Medium[22], and MiniMax-M1[9], among others. (3). Top-Tier non-reasoning models, comprising the largest and highest-performing open-source and commercial models available, such as Deepseek-V3-0324, Qwen3-235B-A22B[35], Kimi k2 Instruct[28], Claude Opus 4[6], GPT 4.1[21], and Gemini 2.5 Flash[11]. This structured comparison ensures comprehensive assessment of VibeThinker-1.5Bs reasoning performance across diverse model categories and scales. Evaluation Settings. We use vLLM[15] as the inference backend, with sampling temperature of 0.6 in our code (the mathematical temperature is 1.0), nucleus sampling [13] with top_p = 0.95, and maximum response length of 40k tokens. For mathematical reasoning, code generation, and domain-specific knowledge tasks, we compute Pass@1 estimates from 64, 8, and 16 samples per benchmark prompt, respectively, using strictly binary rewards. Metrics for other open-source models are sourced from evaluation results reported in their original publications or cited literature. 4.2 Evaluation Results Comparison with Small Reasoning Models. We first compare VibeThinker-1.5B against selection of the most powerful sub-3B reasoning models from both academia and industry. As shown in Table 2, models developed by tech companies generally outperform those from academic open-source efforts. Among 1.5B-scale models, FastCURL[27] and ProRL[17] deliver the strongest results, while larger models such as Qwen3-1.7B[35] and Hunyuan 1.8B[30] exhibit further improvements, particularly in coding taskshighlighting the resource disparities between academic and industrial settings. VibeThinker-1.5B significantly outperforms its base model, Qwen2.5-Math-1.5B[36], across diverse reasoning domains. In mathematics, its AIME25 score increases from 4.3 to 74.4, while its HMMT25 score surges from 0.6 to 50.4. In coding, it achieves 55.9 on LiveCodeBench V5, up from baseline of 0. Critically, it also demonstrates substantial improvement in professional knowledge, with its GPQA score climbing from 16.4 to 46.7, highlighting the models versatile potential. Tech Report, VibeThinker-1.5B, Weibo Table 3. Performance of VibeThinker-1.5B on Core Benchmarks (Large Reasoning Models) Model Mathematics Coding Knowledge Name Params Institution AIME 2024 AIME 2025 HMMT 2025 LCB V5 LCB V6 GPQA Diamond Magistral-Medium-2506 Claude Opus 4 Gemini 2.5 Flash OpenAI o3-mini-Medium MiMo-7B-RL Skywork-OR1-7B AceReason-Nemotron-1.1 GLM-Z1 Phi-4 Reasoning Ring-Lite GPT-OSS-20B-Medium Magistral-Small-2506 Qwen3-32B Llama-Nemotron-Super v1 DeepSeek-R1-Distill-Llama Seed-Thinking v1.5 Llama-Nemotron-Ultra v1 MiniMax-M1 DeepSeek R1-0120 VibeThinker-1.5B N/A N/A N/A N/A 7B 7B 7B 9B 14.7B 16.8B 20B 24B 32B 49B 70B 200B 253B 456B 671B 1.5B Mistral.AI Anthropic Google OpenAI XiaoMi KunLun NVIDIA ZhiPu.AI Microsoft Ant OpenAI Mistral.AI Alibaba NVIDIA DeepSeek ByteDance NVIDIA MiniMax DeepSeek 73.6 76.0 80.4 79.6 68.2 70.2 72.6 75.6 74.6 76.6 80.0 70.7 81.4 67.5 70.0 86.7 80.8 83.3 79.8 64.9 69.2 72.0 74.8 55.4 54.6 64.8 55.4 63.1 69.1 72.1 62.8 72.9 60.0 74.0 72.5 74.6 70.0 53. 38.3 32.0 42.9 43.8 43.5 50.4 41.7 59.4 56.6 61.4 66.3 57.8 47.6 57.2 49.1 53.8 60.7 55.8 65.7 45.5 57.5 64.9 66.3 62.3 65.9 50.3 49.3 42.7 52.1 42.3 54.9 47.4 60.1 70.8 79.6 82.8 76. 54.4 67.1 61.1 66.0 68.2 68.4 66.7 65.2 77.3 76.0 69.2 71.5 Weibo 80.3 74.4 50.4 55. 51.1 46.7 VibeThinker-1.5B also redefines the performance frontier for small-scale models, achieving results that surpass not only its peers but also larger models. It more than doubles the score of the 3B SmolLM on the AIME25 benchmark (74.4 vs. 36.7) and maintains similar large margins on HMMT25 (50.4 vs. 26.0) and LiveCodeBench V5 (55.1 vs. 27.6). Against its closest peer, Qwen3-1.7B, VibeThinker-1.5B also demonstrates substantial advantage on AIME25 (74.4 vs. 36.8) and LiveCodeBench V6 (51.1 vs. 26.9), solidifying its position as the most capable model under 3B parameters. It is important to note that all compared models represent the top tier of reasoning capabilities. Numerous academic efforts fine-tuning Qwen2.5-Math-1.5B[36] report AIME24 scores significantly below 20for instance, the Dynamic Fine-Tuning (DFT) method[33], which attracted significant attention for its theoretical innovation, achieved only 6.87 on AIME24. This is not an isolated case; many improved variants of this architecture similarly struggle to surpass single-digit scores on challenging benchmarks like AIME24. Furthermore, the consistently low AIME24 scores of these Qwen2.5-Math-1.5B-based models suggest an absence of data contamination in the original Qwen2.5-Math-1.5B model, as its knowledge does not appear to overlap with such high-difficulty test items. Comparison with Large Reasoning Models. We compare VibeThinker-1.5B against several state-of-the-art reasoning models, including open-source counterparts such as Phi-4 Reasoning (14.7B), GPT-OSS-20B-Medium, MiniMax-M1, and DeepSeek R1, as well as proprietary models like Claude Opus (thinking) and OpenAI O3-mini-Medium. Despite the substantial parameter gapranging from 10 to hundreds of times larger than VibeThinker-1.5Bthis comparison is highly illustrative. It serves to demonstrate how meticulously designed small-scale model can challenge the conventional wisdom that performance in logical reasoning is dictated primarily by model size. The results in Table 3 demonstrate that VibeThinker-1.5B achieves competitive performance on complex mathematical benchmarks, rivaling models with significantly larger parameter counts. Compared to proprietary models, its scores are comparable to those of O3-mini-Medium and Gemini 2.5 Flash and exceed those of Magistral Medium and Claude Opus 4 on the AIME24 and AIME25 benchmarks. When evaluated against open-source models, VibeThinker-1.5B shows consistent superiority, surpassing DeepSeek R1-0120 across all three datasets. Its performance is also closely aligned with MiniMax-M1 and superior to other models like MiMo 7B and Phi-4 Reasoning. This evidence directly challenges the long-held belief that reasoning performance is dictated primarily by model size, underscoring the untapped potential of small-scale, expertly designed architectures. Tech Report, VibeThinker-1.5B, Weibo Table 4. Performance of VibeThinker-1.5B on Core Benchmarks (Top-Tier Non-Reasoning Models)"
        },
        {
            "title": "Params Type",
            "content": "AIME 2024 AIME 2025 LCB v5 LCB v6 GPQA Diamond Kimi K2 Deepseek V3-0324 Qwen3-235B-A22B GPT-4.1 Claude Opus 4 Gemini 2.5 Flash 1.09T 671B 235B N/A N/A N/A Open-Source Open-Source Open-Source"
        },
        {
            "title": "Proprietary\nProprietary\nProprietary",
            "content": "VibeThinker-1.5B 1.5B Open-Source 69.6 59.4 40.1 46.5 48.2 61.3 80.3 49.5 46.7 24. 37.0 33.9 46.6 74.4 49.2 53.7 46.9 37.0 44.7 47.4 44. 55.9 51.1 75.1 68.4 62.9 71.5 81.0 71.1 46.7 On the challenging code generation benchmarks, VibeThinker-1.5B also demonstrates competitive performance, though the gap with larger models is slightly more pronounced compared to mathematics. It achieves performance level comparable to Magistral Medium and Claude Opus 4. We attribute this disparity primarily to our base model, Qwen-2.5-math 1.5B, which was pre-trained predominantly on mathematical data and thus had limited exposure to code. We posit that this gap is bridgeable; by strengthening the foundational code capabilities of the base model, the performance of VibeThinker could be significantly elevated. However, it is crucial to acknowledge that on the knowledge benchmark GPQA, substantial performance gap of 20-40 points persists between VibeThinker-1.5B and the current leading models. This suggests that smaller parameter scales may inherently limit models capacity to handle broad, encyclopedic general knowledge. We therefore call upon the research community to prioritize enhancing the general knowledge capabilities of small models as critical research direction, which is essential for accelerating their widespread adoption and real-world application. Comparison with Top-Tier Non-Reasoning Models. We next compare VibeThinker-1.5B against the most powerful non-reasoning models, including open-source models like Kimi K2 Instruct[28], Deepseek V3-0324, and Qwen3-235B-A22B[35] (with parameter scales ranging from 235B to 1T), as well as commercial models such as GPT-4.1[21], Claude Opus 4[6], and Gemini 2.5 Flash[11]. Although comparing reasoning models with non-reasoning models may seem inherently uneven, this comparison is motivated by two key considerations: first, VibeThinker-1.5Bs parameter count is merely fraction (often 1/100th or less) of these non-reasoning models; second, these large models themselves have undergone extensive reinforcement learning training with substantial math and coding data, albeit without explicit Chain-of-Thought (CoT) reasoning processes. This juxtaposition aims to highlight the significant potential of small models in reasoning tasks. As shown in Table 4, despite its drastically smaller size, VibeThinker-1.5B surpasses all compared non-reasoning models on challenging mathematical benchmark sets and outperforms the majority in code generation tasks. These results robustly demonstrate that small models possess far greater potential in logical reasoning than previously assumed by consensus. However, considerable performance disparity persists on the general knowledge benchmark GPQA, reaffirming that smaller models still face inherent limitations in handling broad domain knowledge compared to their larger counterparts."
        },
        {
            "title": "5 Conclusion",
            "content": "This report introduces VibeThinker-1.5B, compact 1.5B-parameter model that challenges the prevailing scaling paradigm by achieving state-of-the-art reasoning performance at fraction of the cost. Developed for under $8,000, our model surpasses DeepSeek R1 on the challenging AIME25 benchmark and outperforms other leading large models on key benchmarks. This is accomplished not through extreme parameter scaling, but via innovation in post-training that enhances output diversity during supervised fine-tuning and reinforcement learning. Our results indicate that small models can possess formidable reasoning capabilities, prompting necessary re-evaluation of Scaling Law assumptions."
        },
        {
            "title": "References",
            "content": "[1] 2025. SmolLM 3B. https://smollm3.com/. Tech Report, VibeThinker-1.5B, Weibo [2] M. Abdin, S. Agarwal, A. Awadallah, V. Balachandran, H. Behl, L. Chen, G. de Rosa, S. Gunasekar, M. Javaheripi, N. Joshi, and P. Kauffmann. 2025. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318 (2025). [3] Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697 (2025). [4] Mistral AI. 2025. Magistral. Product Webpage. https://mistral.ai/news/magistral [5] AIME. 2025. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions [6] Anthropic. 2025. Claude Opus 4. Product Webpage. https://www.anthropic.com/claude/opus [7] Mislav Balunovi캖, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi캖, and Martin Vechev. 2025. MathArena: Evaluating LLMs on Uncontaminated Math Competitions. arXiv preprint arXiv:2502.14656 (2025). [8] Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. 2025. Small Language Models are the Future of Agentic AI. arXiv preprint arXiv:2506.02153 (2025). [9] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, and Boji Shan. 2025. MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention. arXiv preprint arXiv:2506.13585 (2025). [10] Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, and Omer Nevo. 2025. Leveraging LLM Inconsistency to Boost Pass@ Performance. arXiv preprint arXiv:2505.12938 (2025). [11] Google DeepMind. 2025. Gemini 2.5 Flash. Product Webpage. https://deepmind.google/models/gemini/flash/ [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, and et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [13] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2020). [14] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations (ICLR). https://openreview.net/forum?id=chfJJYC3iL [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. [16] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Teddy Baker, Jan Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations (ICLR). https://openreview.net/forum?id=v8L0pN6EOi [17] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. 2025. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864 (2025). [18] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL. Notion Blog. [19] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413 (2024). https://arxiv.org/abs/2412.09413 [20] OpenAI. 2024. OpenAI o1. https://openai.com/zh-Hans-CN/o1/. [21] OpenAI. 2025. GPT 4.1. Product Webpage. https://openai.com/index/gpt-4-1/ [22] OpenAI. 2025. GPT-OSS-20B Model. Hugging Face Model. https://huggingface.co/openai/gpt-oss-20b [23] OpenAI. 2025. Introducing o3 and o4-mini. https://openai.com/zh-Hans-CN/index/introducing-o3-and-o4-mini/ Web page, accessed 2024. [24] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [26] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [27] Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. 2025. FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models. arXiv preprint arXiv:2503.17287 (2025). https://arxiv.org/abs/2503.17287 [28] Kimi Team, Yiping Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, et al. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 (2025). [29] Qwen Team. 2024. Qwen 3 Max. https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&from=research.latest-advancements-list. [30] Tencent. 2025. Hunyuan 1.8B Model. GitHub Repository. https://github.com/Tencent-Hunyuan/Hunyuan-1.8B [31] Haoze Wu, Cheng Wang, Wenshuo Zhao, and Junxian He. 2025. Model-Task Alignment Drives Distinct RL Outcomes. arXiv preprint arXiv:2508.21188 (2025). [32] Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, and et al. 2025. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532 (2025). [33] Yongliang Wu, Yizhou Zhou, Ziheng Zhou, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. 2025. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629 (2025). [34] L. L. M. Xiaomi, Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, et al. 2025. MiMo: Unlocking the Reasoning Potential of Language ModelFrom Pretraining to Posttraining. arXiv preprint arXiv:2505.07608 (2025). [35] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505. (2025). [36] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 (2024). [37] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, et al. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471 (2025). Tech Report, VibeThinker-1.5B, Weibo"
        }
    ],
    "affiliations": [
        "Sina Weibo Inc."
    ]
}