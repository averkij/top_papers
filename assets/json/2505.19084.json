{
    "paper_title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling",
    "authors": [
        "Yifeng Xu",
        "Zhenliang He",
        "Meina Kan",
        "Shiguang Shan",
        "Xilin Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 4 8 0 9 1 . 5 0 5 2 : r Jodi: Unification of Visual Generation and Understanding via Joint Modeling Yifeng Xu1,2, Zhenliang He1, Meina Kan1,2, Shiguang Shan1,2, Xilin Chen1,2 1State Key Lab of AI Safety, Institute of Computing Technology, CAS, China 2University of Chinese Academy of Sciences, China yifeng.xu@vipl.ict.ac.cn, {hezhenliang,kanmeina,sgshan,xlchen}@ict.ac.cn Figure 1: Our Jodi framework is capable of performing (a) joint generation, (b) controllable generation, and (c) image perception in unified diffusion model. More visual results can be found in the appendix."
        },
        {
            "title": "Abstract",
            "content": "Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon linear diffusion transformer along with role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi."
        },
        {
            "title": "Introduction",
            "content": "Visual generation [47, 32, 43, 23, 89, 26, 82, 83, 40, 84, 64, 65, 2] and understanding [55, 52, 81, 86, 38, 31, 11, 76, 37, 48] have long been regarded as two separate research fields, each addressed by specialized models. However, from the perspective of human cognition [24, 51, 12, 27], profound understanding of visual scene/object is fundamental to its creation; conversely, the process of creating that scene/object can further enhance and refine our understanding of it. In other words, generation and understanding are two sides of the same coin and deeply interdependent. Therefore, exploring the unification of visual generation and understanding within single foundation model, analogous to the human brain, might be promising avenue toward human-level artificial intelligence. Theoretically, generation and understanding can be associated through the joint distribution. Let denote the image domain and denote the label domain, generation tasks are typically formulated as learning p(x) for unconditional generation and p(x y) for conditional generation, whereas understanding tasks are commonly represented as p(y x). It is theoretical fact that, once we have the joint distribution p(x, y), we can derive any of the corresponding marginal distributions p(x) and p(y), as well as the conditional distributions p(x y) and p(y x)1. This implies that the joint distribution inherently encodes the interdependence between generation tasks and understanding tasks. Inspired, an intriguing idea arises: Is it possible to achieve the unification of visual generation and understanding by jointly modeling the image domain and the label domain? In this paper, we propose Jodi (Joint Diffusion), diffusion model that jointly learns the distributions over the image domain and multiple label domains y1, y2, . . ., including depth, normal, albedo, edge, line art, segmentation, and human skeleton. During the training process, each domain has the chance to serve as one of three roles: as generation target, as condition input, or to be ignored. As result, our unified model simultaneously learns three types of probability distributions, including: 1) p(x, y1, y2, ), joint generation, where the model simultaneously generates both the image and the corresponding labels of different domains; 2) p(x y1, y2, ), controllable generation, where the images are generated conditioned on any combination of the label domains; 3) p(y1, y2, x), image perception, where the model accepts an input image and predicts multiple labels at once. In word, the proposed model is capable of performing both image generation and understanding, as shown in Figure 1. To effectively capture the correspondence and model the consistency among different visual domains, we employ the powerful attention mechanism [91, 71]. However, as the number of domains increases, the computational burden of full attention grows quadratically in terms of both time and space, making the training inefficient or even infeasible. To address this issue, we adopt the linear diffusion transformer [44, 99] and design masked variant to accommodate our role switch mechanism, which achieves linear time and space complexities relative to the number of domains. To further enhance the inter-domain consistency, we introduce domain-invariant positional embeddings to provide an explicit cue for the spatial alignment between visual domains. As result, our framework is capable of modeling as many as 8 visual domains simultaneously with high consistency. Our contributions are summarized below: 1. Inspired by the theoretical fact that the joint distribution intrinsically connects generation and understanding, we propose to jointly model the image domain and multiple label domains, achieving unification of visual generation and understanding. As result, our framework is capable of joint generation, controllable generation, and image perception in unified diffusion model. We believe that the proposed paradigm is positive and promising attempt toward unified and general artificial intelligence. 2. Our model effectively and efficiently captures complex inter-domain relationships through the linear attention, and achieves high consistency across different domains by using the proposed domain-invariant positional embeddings. 3. Comprehensive experiments demonstrate the superiority of our model in various image generation and understanding tasks, despite using significantly less data and computational resources. Our model also supports novel applications not supported by previous unified models, such as joint generation of images and labels, multi-conditional generation, and performing multiple understanding tasks simultaneously. 1p(x) = (cid:82) p(x, y) dy, p(x y) = p(x, y) / p(y), p(y x) = p(x, y) / p(x)"
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Models for Image Generation Diffusion models [82, 83, 40, 84, 64, 65, 2] have made remarkable progress in image generation, with large-scale text-to-image (T2I) models [75, 78, 5, 57, 14, 25, 7, 99] excelling in generating both photorealistic and imaginative scenes. To enhance the controllability, conditional diffusion models [60, 111, 68, 87, 113] introduce spatial conditions to enable more fine-grained control over the generated images. Subsequent methods [115, 73, 104] further improve the efficiency by unifying different types of conditions within single model. Moreover, several studies incorporate reference images [77, 107, 53, 34, 79, 16] or face identities [61, 94, 97, 19, 33] as controlling conditions, broadening the application scope of diffusion models. Diffusion Models for Image Understanding In addition to generation tasks, diffusion models have also exhibited superior performance in image understanding tasks, such as geometry estimation [46, 29, 56, 106, 109, 102, 36], segmentation [103, 116, 72, 119], and edge detection [108]. These methods either use pretrained diffusion models as feature extractors or reformulate the prediction objectives with diffusion frameworks. Furthermore, recent work called Diception [114] unifies wide range of image understanding tasks into single diffusion model, demonstrating the capability of diffusion models in complicated image understanding. Diffusion Models for General Purposes Recent efforts [63, 98, 54, 17, 28] have developed generalist diffusion models to handle various tasks of both image generation and understanding within single model. Typically, these methods achieve general capabilities by training diffusion models on large-scale datasets that span diverse visual tasks. However, these methods do not investigate the relationships among different tasks, and each task requires separate inference process. In contrast, our work emphasizes and models the correspondence and consistency among various visual domains (tasks), enabling novel applications unattainable with previous generalist methods, such as joint generation of image and multiple labels for data synthesis, multi-conditional generation, and simultaneously performing multiple understanding tasks. Concurrent to our work, MMGen [92] also explores the joint modeling of multiple visual domains. However, this approach is limited to only 4 domains (image, depth, normal, and segmentation) and is trained only on ImageNet [22] scale with 256 image resolution. In contrast, our method is built upon text-to-image foundation model [99] and incorporates as many as 8 visual domains with image resolutions of approximately 10241024 pixels, making it significantly more versatile in real-world applications. Multi-modal Generation and Understanding In the context of multi-modal learning, previous works have explored the unification of vision and language by jointly modeling images and texts with autoregressive [88, 95, 96, 18], diffusion [6, 62], or hybrid frameworks [118, 100, 18]. These methods are capable of various cross-modality tasks, such as image-text mixed generation, text-to-image generation, and visual question-answering. In contrast to these methods that focus on unifying vision and language modalities, our work concentrates on the unification of pure visual domains within single diffusion framework."
        },
        {
            "title": "3 Method",
            "content": "Overview In this section, we present the details of our Jodi framework, which unifies visual generation and understanding within single diffusion model by jointly modeling the image domain and multiple label domains. As shown in Figure 2, our Jodi mainly consists of four parts: Deep Compression Autoencoder (DC-AE) [15], role assignment mechanism, Switch module, and linear diffusion transformer backbone [99]. Specifically, all of the image domain and the label domains are first compressed into set of tokens by DC-AE with downsampling factor of 32. Then, each domain is randomly assigned one of three roles: as generation target, as condition input, or to be ignored. Depending on the roles, the Switch module further processes the tokens in one of the following ways: adding noise, preserving their values, or setting them to zero. Subsequently, tokens from all domains are concatenated and fed into the linear diffusion transformer, which facilitates interactions across these domains and predicts the velocity field as in Rectified Flow [65]. Please refer to the appendix for more details on the framework architecture. 3 Figure 2: Overview of our Jodi framework. For the sake of clarity, only four domains are illustrated. 3."
        },
        {
            "title": "Joint Modeling",
            "content": "Role Assignment Let y0 = denote the tokens of image domain and y1, y2, . . . , yM denote the tokens of distinct label domains. At each training iteration, each domain is randomly assigned one of three roles: 1) [G], which means the model will learn to generate this domain; 2) [C], which means the model will use this domain as condition; 3) [X], which means this domain will be ignored. In this manner, our model learns class of probability distributions as follows: p(cid:0){ym rolem=[G]} (cid:12) (cid:12) {ym rolem=[C]}(cid:1). (1) Since each domain can be an outcome, condition, or be ignored in Eq. (1), our model learns diverse distributions, including three most typical ones: 1) p(x, y1, y2, ), joint generation, where the model simultaneously generates both the image and the corresponding labels of different domains; 2) p(x y1, y2, ), controllable generation, where the images are generated conditioned on any combination of the label domains; 3) p(y1, y2, x), image perception, where the model accepts an input image and predicts multiple labels at once. In word, our method unifies various distributions related to both image generation and understanding within single model. Switch Module Depending on the roles assigned, the Switch module processes the tokens in different ways, as shown on the right of Figure 2. Specifically, at diffusion time step t, the [G] tokens are linearly interpolated with noise ϵm (0, I) as in Rectified Flow [65], the [C] tokens remain 0 = ym, this process is formulated as follows: unchanged, and the [X] tokens are set to zero. Let ym ym = 0 + tϵm (1 t)ym ym 0 if rolem = [G] if rolem = [C] if rolem = [X] (2) Objective Function Given the processed tokens in Eq. (2), we optimize our model by flow matching [64, 65]. Specifically, our model learns to predict the velocity field of [G] tokens conditioned on [C] tokens, with the following objective function: = tU (0,1), ϵ0:M (0,I), y0:M 0 (cid:34) (cid:88) m: rolem=[G] (cid:13) (cid:13)vm θ (y0 , , yM , t) (ϵm ym 0 )(cid:13) 2 (cid:13) (cid:35) , (3) where vθ() is the velocity predictor with linear transformer architecture, introduced in Section 3.2."
        },
        {
            "title": "3.2 Model Architecture",
            "content": "Linear Diffusion Transformer We employ the attention mechanism [91] to model the interaction among different visual domains and predict the velocity field in Equation (3). However, as the number of domains increases, we need to carefully consider the computational complexity. Suppose we have visual domains in total, each domain contains tokens, and each token is D-dimensional. In this setting, the full attention mechanism exhibits time complexity of O(M 2N 2D + D2) and space complexity of O(M 2N 2 + D), both scaling quadratically with respect to the number of 4 domains . In consequence, training our model with full attention diffusion transformer [25, 7] is computationally inefficient or even infeasible. To solve this problem, we choose Sana [99] as our backbone, which adopts linear transformer [44] for efficient text-to-image generation. Using linear transformer, the time complexity is reduced to O(M D2) and the space complexity to O(M D), both of which are linear with respect to . As result, our model can efficiently handle as many as 8 visual domains. An empirical comparison on the computational cost can be found in the appendix. When domain is assigned the role [X], i.e., to be ignored, the corresponding tokens should not participate in the attention computation. To this end, we design masked version of linear attention. For single attention head, let Qi, Ki, Vi R1d denote the query, key, and value of the ith token, and mi {0, 1} indicate whether to ignore this token, the masked linear attention is designed as: Oi = (cid:16)(cid:80)M ReLU(Qi) j=1 ReLU(mjKj)T Vj ReLU(Qi) (cid:16)(cid:80)M j=1 ReLU(mjKj)T (cid:17) (cid:17) , = 1, 2, . . . , N. (4) When mj = 0 in Eq. (4), the jth token vanishes from both the denominator and numerator, which means it is excluded from the attention computation. Domain-invariant Positional Embeddings notable feature of our backbone Sana [99] is that it does not use explicit positional embeddings (NoPE) [35, 45]. However, in our multi-domain scenario, there is strong spatial correspondence between the visual domains. Therefore, it is necessary to explicitly indicate the spatial positions to facilitate precise spatial alignment across different domains. To this end, we add domain-invariant sinusoidal positional embeddings to the tokens of each visual domain, where the same positions in different visual domains share identical positional embeddings, providing an explicit cue for the spatial alignment. Besides, we also introduce domain embeddings and role embeddings to help the model distinguish the domains and the roles of the tokens."
        },
        {
            "title": "3.3 Data Construction",
            "content": "To support joint modeling of multiple visual domains, we require large-scale dataset containing high-quality images and corresponding labels of various domains. We construct the dataset from two kinds of sources: 1) images with predicted labels and 2) images with ground-truth labels. First, we collect images with high quality and diversity from several publicly available sources, including Subjects200K [87], Aesthetic-4K [110], and Pexels [70, 30]. All of these images have resolutions over 10241024, which is advantageous for training high-resolution generative model. As these datasets lack labels, we use state-of-the-art predictors to automatically annotate the data with labels corresponding to 7 specific domains. Specifically, we employ Informative Drawings [13] to generate line arts, PiDiNet [85] to extract edge maps, Depth Anything V2 [105] and Lotus [36] to estimate depth maps, Lotus [36] to estimate normal maps, RGB2X [109] to estimate albedos, Oneformer [42] to predict segmentation colormaps, and Openpose [8] to predict human skeletons. In this manner, we construct dataset containing 200,000 images with corresponding 7200,000 predicted labels. We name this dataset Joint-1.6M, and it will be made publicly available. However, the predicted labels may lack sufficient accuracy, especially for in-the-wild images. To this end, we also employ datasets with ground-truth labels. Specifically, we use BSDS500 [4] for edge maps, Hypersim [74] for depth, normal, and albedo maps, and ADE20K [117] for semantic segmentation maps. These datasets encompass total of 90,000 images. Furthermore, we use BLIP2-OPT-2.7b [58] and Qwen2-VL-7b-Instruct [93] to generate captions. The former tends to provide concise description of the main subject in the image, while the latter tends to give long paragraph that describes the subject, background, and the overall atmosphere in detail. During the training process, one of these two captions is randomly selected for each image. Table 1: Comparison of training costs among unified models. We use much less data and resources. Method Base Model Parameters Dataset Size Training GPU OmniGen [98] PixWizard [63] OneDiffusion [54] Jodi (ours) Phi-3 [1] Lumina-Next-T2I [120] (from scratch) Sana [99] 3.8B 2B 2.8B 1.6B 104A800 100M 30M - 75M TPU v3-256, 64H100 8A6000 290K"
        },
        {
            "title": "4.1 Setup",
            "content": "Training Details We adopt Sana [99] as our base model. We train our model using the CAME-8bit optimizer [67] for 130K steps, with learning rate of 4 105, batch size of 32, and BF16 mixed-precision, which takes around 535 hours on 8 RTX A6000. Since our dataset contains images with various aspect ratios, we use ratio bucketing strategy [69] during training to prevent important contents from being cropped. This also allows users to generate images with wide range of aspect ratios during inference. It is worth noting that we use significantly less data and computational resources than the other unified models, as shown in Table 1. Sampling Details We employ Flow-DPM-Solver [99], variant of DPM-Solver++ [66] adapted for rectified flow. The classifier-free guidance [41] scale is set to 4.5. For joint generation and controllable generation, we use 20 sampling steps. For image perception, we use 10 sampling steps since increasing the steps leads to little performance gain."
        },
        {
            "title": "4.2 Visual Generation and Understanding",
            "content": "Joint Generation In Figure 3, we illustrate the capability of our Jodi to simultaneously generate high-quality images of various aspect ratios along with corresponding labels, including depth, normal, albedo, edge, lineart, segmentation, and openpose. The generated images and the generated labels are semantically consistent and spatially aligned, credited to the linear attention and domain-invariant positional embeddings. Please refer to the appendix for more results. Controllable Generation To demonstrate Jodis performance in controllable generation, we first generate images using existing labels as input conditions and evaluate their fidelity using FID scores [39]. Then, to evaluate the faithfulness of the generated images to the input conditions, we re-extract the conditions from the generated images and compare them to the input conditions using LPIPS [112]. As shown in Table 2 and Figure 4, our Jodi achieves superior performance for all conditions compared to existing unified models as well as generation-only specialist models. Figure 3: Joint generation of images and labels across wide range of aspect ratios. Table 2: Quantitative comparison of controllable generation. Method ControlNet [111] UniControl [73] EasyControl [113] OmniGen [98] PixWizard [63] OneDiffusion [54] Jodi (ours) Depth Normal Edge Lineart Openpose LPIPS FID LPIPS FID LPIPS FID LPIPS FID LPIPS FID 0.29 0.29 0.27 0.31 0.23 0.24 0. 19.5 18.8 19.5 20.4 14.4 15.9 13.6 0.35 0.35 - 0.33 0.27 0.41 0.27 28.0 22.5 - 24.9 16.7 21.6 13. 0.23 0.31 0.31 0.25 0.29 0.26 0.20 18.9 39.1 20.0 23.3 22.9 40.5 13.7 0.33 - - 0.35 0.22 0.40 0. 15.9 - - 102.7 14.6 37.2 11.3 0.11 0.11 0.12 0.22 0.16 - 0.15 32.0 26.8 33.9 33.5 31.7 - 23. * First block: specialist models, second block: unified models. 6 Figure 4: Visual comparison of controllable generation. Image Perception We assess the visual understanding ability of our Jodi on four image perception tasks: depth estimation, normal estimation, albedo estimation, and edge detection. For depth estimation, we evaluate our model on NYUv2 [80], ScanNet [20], and DIODE [90] datasets using absolute mean relative error. For normal estimation, we evaluate our model on NYUv2 [80], ScanNet [20], and iBims [49] datasets using mean angular error. For albedo estimation, we evaluate our model on the Hypersim [74] test set using PSNR and LPIPS [112]. For edge detection, we evaluate our model on the BSDS500 [4] test set, using F-scores at Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS) as evaluation metrics. Besides, given the stochastic nature of diffusion models, we also report the ensemble performance by sampling five times for each input image and averaging the results. As shown in Table 3, Table 4, Table 5, Table 6, and Figure 5, our method consistently achieves superior or comparable results to the other unified models and specialist models. Table 3: Quantitative comparison of depth estimation with absolute mean relative error . Method NYUv2 ScanNet DIODE Table 4: Quantitative comparison of surface normal estimation with mean angular error . Method NYUv2 ScanNet iBims Marigold [46] GeoWizard [29] Lotus-D [36] OmniGen [98] PixWizard [63] OneDiffusion [54] Jodi (ours) Jodi (ours, ensemble) 5.5 5.6 5. 9.2 7.0 8.9 10.1 8.3 6.4 6.4 5.5 10.1 7.9 9.7 12.1 9.9 30.8 33.5 22.8 30.6 25.4 25.2 25.9 25.8 GeoWizard [29] GenPercept [102] StableNormal [106] Lotus-D [36] OmniGen [98] PixWizard [63] Jodi (ours) Jodi (ours, ensemble) 18.9 18.2 18.6 16.2 28.9 23.5 21.1 18.6 17.4 17.7 17.1 14.7 28.9 26.6 24.3 20.3 19.3 18.2 18.2 17. 31.3 22.5 20.1 18.2 * First block: specialist models, second block: unified models. * First block: specialist models, second block: unified models. Table 5: Quantitative comparison of albedo estimation on the Hypersim [74] test set. Table 6: Quantitative comparison of edge detection on the BSDS500 [4] test set. Method PSNR LPIPS Method Ordinal Shading [9] Kocsis et al. [50] Careaga and Aksoy [10] RGB2X [109] Jodi (ours) Jodi (ours, ensemble) 15.6 11.3 15.7 20.6 15.5 16. 0.34 0.49 0.36 0.18 0.31 0.33 HED [101] PiDiNet [85] OmniGen [98] PixWizard [63] OneDiffusion [54] Jodi (ours) ODS 0.788 0. 0.767 0.605 0.682 0.826 IDS 0.808 0.823 0.781 0.633 0.691 0.851 * First block: specialist models, second block: unified models. * First block: specialist models, second block: unified models. 7 Figure 5: Visual comparison of image perception tasks on in-the-wild images."
        },
        {
            "title": "4.3 Analysis",
            "content": "Effect of Domain-invariant Positional Embeddings As described in Section 3.2, we introduce domain-invariant positional embeddings to encourage the spatial alignment across different visual domains. To validate the effect, we compare our models trained for 10K steps with and without positional embeddings, by observing whether the joint generated images and labels are spatially aligned. As shown in Figure 6, our model aligns the image domain and label domains significantly better with positional embeddings, whereas obvious misalignment is observed without positional embeddings. Attention Map Visualization To further investigate how the tokens from different visual domains align and interact with each other, we pick two query tokens from the image domain and visualize the corresponding attention maps in Figure 7. As can be seen, most domains show strong activation at the same spatial location as the query token, demonstrating good alignment between these domains. Interestingly, attention maps of different domains also reveal their own unique structural patterns. For example, the segmentation domain exhibits strong activation along semantic boundaries, and the openpose domain focuses more on the human figure. Joint Consistency In Figure 8, we illustrate the consistency of our unified model across joint generation, controllable generation, and image perception tasks. We first perform joint generation based on the input prompt to produce samples covering all of the image and label domains. According to each generated label, we then apply controllable generation to generate new images that comply with these labels. Besides, we perform image perception on the image generated in the first step to detect all its labels. As can be observed, three types of tasks produce visually consistent results. Extension to New Domains Our well-trained Jodi model can be readily extended to one or more new domains by appending the corresponding tokens to the existing ones. Figure 9 presents the joint generation results after fine-tuning the model on the doodle sketch domain [3] as well as simultaneous fine-tuning on the pixel, irradiance, and canny domains. Figure 6: Effect of positional embeddings. Generated labels are overlaid on images for better view. Figure 7: Visualization of attention map. Figure 8: Jodi shows consistency among joint generation, controllable generation, image perception. Figure 9: Joint generation results of our model extended to new domains."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "Motivated by the interdependence between generation and understanding inherent in the joint distribution, we propose Jodi, diffusion framework that jointly models the image domain and multiple label domains, unifying the visual generation and understanding. We design role switch mechanism that allows the model to simultaneously learn joint generation, controllable generation, and image perception. Furthermore, to facilitate the interaction and alignment between tokens from different visual domains, we introduce masked linear attention and domain-invariant positional embeddings. As result, our Jodi is capable of both generation and understanding tasks across the image domain and 7 distinct label domains. We also introduce the Joint-1.6M dataset, which will be publicly released to advance this research area. While Jodi achieves impressive performance, it still comes with certain limitations. First, due to the limited size of our training dataset, the generated images may exhibit structural distortions, especially for human figures. Second, we simply represent each domain in RGB space. As consequence, our model is currently limited to handling 12 clustered classes for the segmentation domain (see the appendix for details), as increasing the number of classes makes the RGB representations of the segments too similar to be reliably distinguished. Similarly, the RGB space is also not the ideal choice for the openpose domain, where the keypoints are better represented by coordinates. These problems may be resolved by incorporating more data and designing specific encoders and decoders for each visual domain, which we leave for future work. It is important to note that, as with all generative models, Jodi may inherit biases present in the training dataset and could be misused to generate malicious or unintended content. Users should remain vigilant and comply with the usage policies to mitigate these risks."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. [3] Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, and Yael Vinker. Swiftsketch: diffusion model for image-to-vector sketch generation. arXiv preprint arXiv:2502.08642, 2025. [4] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence, 33(5):898916, 2010. [5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [6] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pages 16921717. PMLR, 2023. [7] BlackForestLab. Flux.1. https://blackforestlabs.io/flux-1/. [8] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person IEEE Transactions on Pattern Analysis and Machine 2d pose estimation using part affinity fields. Intelligence, 2019. [9] Chris Careaga and Yagız Aksoy. Intrinsic image decomposition via ordinal shading. ACM Transactions on Graphics, 43(1):124, 2023. [10] Chris Careaga and Yagız Aksoy. Colorful diffuse intrinsic image decomposition in the wild. ACM Transactions on Graphics (TOG), 43(6):112, 2024. [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213229, 2020. [12] Rebecca Chamberlain, Jennifer Drake, Aaron Kozbelt, Rachel Hickman, Joseph Siev, and Johan Wagemans. Artists as experts in visual cognition: An update. Psychology of Aesthetics, Creativity, and the Arts, 13(1):58, 2019. [13] Caroline Chan, Frédo Durand, and Phillip Isola. Learning to generate line drawings that convey geometry and semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79157925, 2022. [14] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [15] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [16] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65936602, 2024. [17] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [18] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [19] Siying Cui, Jia Guo, Xiang An, Jiankang Deng, Yongle Zhao, Xinyu Wei, and Ziyong Feng. Idadapter: Learning mixed features for tuning-free personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 950959, 2024. [20] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [21] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:16344 16359, 2022. [22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [23] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [24] Melissa Ellamil, Charles Dobson, Mark Beeman, and Kalina Christoff. Evaluative and generative modes of thought during the creative process. Neuroimage, 59(2):17831794, 2012. [25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. [26] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, synthesis. pages 1287312883, 2021. [27] Myra Fernandes, Jeffrey Wammes, and Melissa Meade. The surprisingly powerful influence of drawing on memory. Current Directions in Psychological Science, 27(5):302308, 2018. [28] Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, and Yinfei Yang. Univg: generalist diffusion model for unified image generation and editing. arXiv preprint arXiv:2503.12652, 2025. [29] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pages 241258. Springer, 2024. [30] gaunernst. pexels-portrait. https://huggingface.co/datasets/gaunernst/pexels-portrait. [31] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580587, 2014. [32] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [33] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37:36777 36804, 2024. [34] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: In Proceedings of the IEEE/CVF International Compact parameter space for diffusion fine-tuning. Conference on Computer Vision, pages 73237334, 2023. [35] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 13821390, 2022. [36] Jing He, Haodong LI, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. In The Thirteenth International Conference on Learning Representations, 2025. [37] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 11 [38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [39] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [41] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [42] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 29892998, 2023. [43] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, networks. pages 44014410, 2019. [44] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. [45] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. [46] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [47] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [48] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [49] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based singleIn Proceedings of the European Conference on Computer Vision image depth estimation methods. (ECCV) Workshops, pages 00, 2018. [50] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor singleview material estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51985208, 2024. [51] Aaron Kozbelt. Artists as experts in visual cognition. Visual cognition, 8(6):705723, 2001. [52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [53] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [54] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318, 2024. [55] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [56] Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78617871, 2024. 12 [57] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [58] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [59] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. [60] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. [61] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: In Proceedings of the IEEE/CVF Customizing realistic human photos via stacked id embedding. conference on computer vision and pattern recognition, pages 86408650, 2024. [62] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. [63] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Peng Gao, and Hongsheng Li. Pixwizard: Versatile image-to-image visual assistant with open-language instructions. In The Thirteenth International Conference on Learning Representations, 2025. [64] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [65] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. [66] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [67] Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. Came: ConfidenceIn Proceedings of the 61st Annual Meeting of the guided adaptive memory efficient optimization. Association for Computational Linguistics (Volume 1: Long Papers), pages 44424453, 2023. [68] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 42964304, 2024. [69] NovelAI. Novelai improvements on stable diffusion, 2022. [70] opendiffusionai. pexels-photos-janpf. https://huggingface.co/datasets/opendiffusionai/ pexels-photos-janpf. [71] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [72] Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, and David Jacobs. Ld-znet: latent diffusion approach for text-based image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41574168, 2023. [73] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. Advances in Neural Information Processing Systems, 36, 2024. [74] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. [75] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 13 [76] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [77] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [78] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [79] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2024. [80] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. [81] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [82] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [83] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [84] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [85] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietikäinen, and Li Liu. Pixel difference networks for efficient edge detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51175127, 2021. [86] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19, 2015. [87] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [88] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [89] Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. [90] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. [91] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [92] Jiepeng Wang, Zhaoqing Wang, Hao Pan, Yuan Liu, Dongdong Yu, Changhu Wang, and Wenping Wang. Mmgen: Unified multi-modal image generation and understanding in one go. arXiv preprint arXiv:2503.20644, 2025. [93] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [94] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [95] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [96] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. VILA-u: unified foundation model integrating visual understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. [97] Yi Wu, Ziqiang Li, Heliang Zheng, Chaoyue Wang, and Bin Li. Infinite-id: Identity-preserved personalization via id-semantics decoupling paradigm. In European Conference on Computer Vision, pages 279296. Springer, 2024. [98] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [99] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. [100] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. [101] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision, pages 13951403, 2015. [102] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks? In The Thirteenth International Conference on Learning Representations, 2025. [103] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. [104] Yifeng Xu, Zhenliang He, Shiguang Shan, and Xilin Chen. CtrloRA: An extensible and efficient framework for controllable image generation. In The Thirteenth International Conference on Learning Representations, 2025. [105] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [106] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 43(6):118, 2024. [107] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [108] Yunfan Ye, Kai Xu, Yuhang Huang, Renjiao Yi, and Zhiping Cai. Diffusionedge: Diffusion probabilistic model for crisp edge detection. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 66756683, 2024. [109] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, LingQi Yan, and Miloš Hašan. Rgbx: Image decomposition and synthesis using materialand lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, SIGGRAPH 24, New York, NY, USA, 2024. Association for Computing Machinery. [110] Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, and Di Huang. Diffusion-4k: Ultra-high-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [111] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836 3847, 2023. 15 [112] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [113] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [114] Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, and Chunhua Shen. Diception: generalist diffusion model for visual perceptual tasks. arXiv preprint arXiv:2502.17157, 2025. [115] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [116] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57295739, 2023. [117] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. [118] Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth International Conference on Learning Representations, 2025. [119] Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, and Chunhua Shen. Unleashing the potential of the diffusion model in few-shot semantic segmentation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [120] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Lirui Zhao, Si Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-next : Making lumina-t2x stronger and faster with next-dit. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024."
        },
        {
            "title": "A Detailed Architecture",
            "content": "Figure 10 shows the detailed architecture of our Jodi. To incorporate the label domains into our backbone model Sana [99], we add new patch embedding layer and new final layer for each label domain. The patch embedding layers are responsible for projecting the encoded tokens to match the input dimension of the backbone model, and the final layers project them back to match the input dimension of the decoder. The patch embedding layers of the label domains are initialized from the pretrained Sana weights of the image domain, while the new final layers are randomly initialized. We find that this initialization strategy leads to the best convergence. The backbone is stack of linear transformer blocks, where each block is composed of AdaLN-Zero layers [71], linear attention layer [44], cross attention layer [75], and mix FFN layer [99]. The scale, shift, and gate parameters of the AdaLN-Zero layers are obtained via an MLP that takes the role embeddings, domain embeddings, and timestep embeddings as input; therefore, these parameters are tailored for each domain, helping the model distinguish the roles and domains. Figure 10: Detailed architecture of Jodi. For the sake of clarity, only four domains are illustrated. 17 Relationships among Text, Image, and Labels For joint generation and controllable generation, we can always assume proper input text describing the content of the image. However, for image perception tasks, the labels are generally determined by the given image alone, regardless of the text description. In the context of graphical models, the labels and the text are conditionally independent given the image, as illustrated by the probabilistic graph in Figure 11. Accordingly, we set the text input empty for image perception tasks during both training and inference. Figure 11: The probabilistic graph of text, image, and labels."
        },
        {
            "title": "C Computational Cost",
            "content": "In Section 3.2 of the main paper, we analyze the theoretical computational complexity of using linear attention versus full attention. In Figure 12, we present the actual VRAM usage, training time, and inference latency when using vanilla full attention [91], flash attention [21], and linear attention [44] in practice. As the number of domains increases, the VRAM usage of vanilla full attention quickly exceeds the memory limits of an RTX A6000 GPU, making our training infeasible. Although flash attention reduces memory usage, its training time is over twice as long as that of linear attention when handling 8 domains, resulting in lower efficiency. Figure 12: Comparison of actual computational cost among three types of attention."
        },
        {
            "title": "D Notes on Segmentation Domain",
            "content": "As discussed in the limitation part in Section 5 of the main paper, we represent each visual domain in RGB space, which is not suitable for the semantic segmentation domain. Specifically, the segmentation dataset ADE20K [117] contains as many as 150 semantic classes, where some of the classes are assigned similar or even the same colors in RGB space, causing confusion for the model. To mitigate this problem, we group the 150 classes into 12 manually defined superclasses as shown in Table 7, where the RGB values assigned to different superclasses are set to be as far apart as possible. However, this is apparently not the optimal solution because it decreases the number of distinguishable classes. In the future, we plan to extend our model beyond the RGB space to better accommodate special domains like the segmentation domain. Table 7: 12 superclasses and the corresponding RGB colors. Superclass Person Animal Plant Water Mountain Sky Building Vehicle Wall Road Furniture Others Color FF7FFF FF7F00 7FFF00 007FFF 00FF7F 7FFFFF FF007F 7F00FF FFFF7F 7F 007F00 00007F"
        },
        {
            "title": "E Additional Quantitative Results",
            "content": "Semantic Segmentation In Table 8, we report the Intersection-over-Union (IoU) for each superclass except Others, as well as their mean IoU (mIoU). We also report the ensemble performance by sampling five times for each input image and performing majority voting. For the methods trained on the original 150 classes of ADE20K [117], we map their predictions to our 12 superclasses before computing IoUs. It is worth noting that such comparison is somewhat unfair to the other methods, because these methods are trained to predict 150 classes, which is more challenging task than predicting our 12 superclasses. Table 8: Quantitative comparison on semantic segmentation (12 classes) on ADE20K [117] test set. Method Person Animal Plant Water Mountain Sky Building Vehicle Wall Road Furniture IoU per class Uniformer [59] Oneformer [42] PixWizard [63] Jodi (ours) Jodi (ours, ensemble) 78.0 87.3 47.1 74.0 79.5 62.9 65.4 0.0 14.7 1.9 75.8 81. 53.0 55.7 65.6 64.9 88.4 25.4 50.7 60.9 61.7 69.7 14.4 37.9 38.9 93.2 95. 85.1 90.9 92.4 87.1 90.8 50.3 67.0 78.0 76.2 86.2 29.7 52.5 66.4 87.9 90. 66.1 72.4 79.4 74.6 82.7 38.9 61.0 67.5 81.5 86.0 24.5 56.2 65.4 mIoU 76.7 83.9 39.5 57.5 63.3 * First block: specialist models, second block: unified models. Full Results of Depth and Normal Estimation We present the full evaluation results of depth estimation in Table 9 and normal estimation in Table 10, where additional methods and metrics are included for comprehensive comparison. The detailed description of these metrics can be found in Appendix A.2 of the Lotus paper [36]. Table 9: Quantitative comparison on depth estimation. Method Marigold [46] GeoWizard [29] GenPercept [102] Lotus-D [36] OmniGen [98] PixWizard [63] OneDiffusion [54] Jodi (ours) Jodi (ours, w/ ensemble) NYUv2 [80] ScanNet [20] DIODE [90] AbsRel δ1 5.5 5.6 5.6 5.1 9.2 7.0 8.9 10.1 8.3 96.4 96.3 96.0 97.2 91.8 95.0 92.0 89.6 92.7 δ2 99.1 99.1 99.2 99. 98.6 99.1 98.2 97.9 98.8 AbsRel δ1 6.4 6.4 6.2 5.5 10.1 7.9 9.7 12.1 9.9 95.2 95.0 96.1 96.5 90.0 93.7 90.7 84.7 89. δ2 98.8 98.4 99.1 99.0 98.2 98.8 98.0 96.4 97.8 AbsRel δ1 30.8 33.5 35.7 22.8 30.6 25.4 25.2 25.9 25. 77.3 72.3 75.6 73.8 71.0 72.1 72.2 69.0 71.0 δ2 88.7 86.5 86.6 86.2 85.8 85.0 85.3 84.1 84.9 * First block: specialist models, second block: unified models. * sourced from Lotus [36], evaluated by ourselves following the Lotus protocol. Table 10: Quantitative comparison on normal estimation. Method NYUv2 [80] ScanNet [20] iBims [49] mean 11.25 30 mean 11.25 30 mean 11.25 30 GeoWizard [29] GenPercept [102] StableNormal [106] Lotus-D [36] OmniGen [98] PixWizard [63] Jodi (ours) Jodi (ours, w/ ensemble) 18.9 18.2 18.6 16.2 28.9 23.5 21.1 18.6 50.7 56.3 53.5 59.8 18.1 33.9 47.7 50.5 81.5 81.4 81.7 83. 64.5 72.6 77.7 80.4 17.4 17.7 17.1 14.7 28.9 26.6 24.3 20.3 53.8 58.3 57.4 64.0 17.7 25.5 41.3 46.2 83.5 82.7 84.1 86. 64.7 65.3 73.9 78.0 19.3 18.2 18.2 17.1 31.3 22.5 20.1 18.2 63.0 64.0 65.0 66.4 18.3 40.1 60.0 61.8 80.3 82.0 82.4 83. 63.1 78.3 79.6 81.0 * First block: specialist models, second block: unified models. * sourced from Lotus [36], evaluated by ourselves following the Lotus protocol. 19 Multi-conditional Controllable Generation In Table 11, we compare our performance of singleconditional and multi-conditional controllable generation. Specifically, we evaluate controllable generation conditioned individually on each of Depth, Normal, Edge, and Lineart, as well as conditioned on all of them together. Since multiple conditions provide more information than single condition, it is natural that the former presents better controllability. Table 11: Comparison between single and multi-conditional controllable generation. Method Depth Normal Edge Lineart LPIPS FID LPIPS FID LPIPS FID LPIPS FID Jodi (single) Jodi (multi) 0.23 0.22 13.6 10.2 0.27 0.22 13.6 10.2 0.20 0. 13.7 10.2 0.20 0.20 11.3 10.2 Multi-label Image Perception One of the notable features of our Jodi is that it can simultaneously predict multiple types of labels for given image. In Table 12, we compare the performance of predicting all types of labels at once to predicting one label at time. As can be seen, the performance of multi-label prediction is slightly inferior to that of single-label prediction, which we attribute to the absence of ground-truth labels for learning multi-label prediction (we use predicted labels as surrogates). Despite slightly lower performance, predicting all labels at once significantly saves inference time compared to predicting them one by one. For example, performing multi-label prediction 5 times still takes no more inference time than predicting 5 labels individually. Therefore, we can ensemble these 5 repeats of multi-label prediction to achieve better performance, which outperforms single-label prediction in most cases. Table 12: Comparison between single and multi-label image perception. Method Depth (NYUv2 [80]) Normal (NYUv2 [80]) Albedo (Hypersim [74]) Edge (BSDS500 [4]) Seg. (ADE20K [117]) AbsRel δ1 δ2 mean 11.25 30 PSNR LPIPS ODS IDS mIoU Jodi (single) Jodi (multi) Jodi (multi, ensemble) 10.1 11.8 9. 89.6 97.9 85.9 97.0 90.4 98.3 21.1 22.1 19.6 47.7 44.5 46.9 77.7 76.1 79.0 15.5 13.9 15.1 0.31 0.44 0. 0.826 0.765 - 0.851 0.782 - 57.5 57.1 62."
        },
        {
            "title": "F Additional Visual Results",
            "content": "In this part, we provide additional visual results of our Jodi, including Figure 13 for joint consistency, Figure 14 for joint generation, Figure 15, Figure 16, and Figure 17 for controllable generation, and Figure 18 and Figure 19 for image perception."
        },
        {
            "title": "G Licenses and Sources",
            "content": "Licenses and sources of datasets and models used in our paper are listed in Table 13 and Table 14. Table 13: Datasets used in this paper. Table 14: Models used in this paper. Dataset License Source Model Aesthetic-4K [110] Pexels-photos [70] Pexels-portrait [30] Subjects200K [87] ADE20K [117] BSDS500 [4] Hypersim [74] MIT Pexels Pexels Apache-2.0 BSD-3-Clause - CC BY-SA 3.0 HuggingFace HuggingFace HuggingFace HuggingFace Official Website Official Website GitHub Sana-1600M-1024px-BF16 [99] BLIP2-OPT-2.7b [58] Qwen2-VL-7b-Instruct [93] Depth Anything V2 [105] Informative Drawings [13] Lotus [36] Oneformer [42] Openpose [8] PiDiNet [85] RGB2X [109] License NVIDIA MIT Apache-2.0 CC BY-NC 4.0 MIT Apache-2.0 MIT Openpose PiDiNet Adobe Source GitHub GitHub GitHub GitHub GitHub GitHub GitHub GitHub GitHub GitHub 20 Figure 13: Jodi shows consistency among joint generation, controllable generation, image perception. Figure 14: Additional visual results of joint generation. 22 Figure 15: Additional visual comparisons of controllable generation. 23 Figure 16: Additional visual results of controllable generation using depth, normal, or albedo as input. Figure 17: Additional visual results of controllable generation using edge, lineart, segmentation as input. 25 Figure 18: Additional visual comparisons of single-label perception on in-the-wild images. 26 Figure 19: Additional visual results of multi-label perception on in-the-wild images."
        }
    ],
    "affiliations": [
        "State Key Lab of AI Safety, Institute of Computing Technology, CAS, China",
        "University of Chinese Academy of Sciences, China"
    ]
}