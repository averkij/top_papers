{
    "paper_title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
    "authors": [
        "Tianhao Peng",
        "Haochen Wang",
        "Yuanxing Zhang",
        "Zekun Wang",
        "Zili Wang",
        "Gavin Chang",
        "Jian Yang",
        "Shihao Li",
        "Yanghai Wang",
        "Xintao Wang",
        "Houyi Li",
        "Wei Ji",
        "Pengfei Wan",
        "Steven Huang",
        "Zhaoxiang Zhang",
        "Jiaheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 2 0 5 2 7 0 . 1 1 5 2 : r MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs Tianhao Peng1,4, Haochen Wang2, Yuanxing Zhang3, Zekun Wang3, Zili Wang4, Gavin Chang4, Jian Yang4, Shihao Li1, Yanghai Wang1, Xintao Wang3, Houyi Li4, Wei Ji1, Pengfei Wan3, Steven Huang4, Zhaoxiang Zhang1,2, Jiaheng Liu,1 1Nanjing University, 2CASIA, 3Kuaishou Technology, 4M-A-P Abstract The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
        },
        {
            "title": "1 Introduction",
            "content": "The rise of Large Language Models (LLMs) has enabled numerous groundbreaking applications across various domains. For instance, conversational agents like ChatGPT have revolutionized how we interact with technology by providing coherent and contextually relevant responses in natural language (OpenAI, 2024). These models have also shown significant improvements in tasks such as knowledge-based question-answering (Hendrycks et al., 2021; Rein et al., 2023; Du et al., 2025; Chen et al., 2025), mathematics (Wu et al., 2024; Cobbe et al., 2021; Yue et al., 2023), and code generation (Chai et al., 2025; Li et al., 2023c; Liu et al., 2025a). Multimodal Large Language Models (MLLMs) extend this capability to visual modalities and MLLMs are trained to integrate visual inputs to understand and interpret images and videos (Zhang et al., 2025b; Ma et al., 2024; Bai et al., 2025). Recently, to evaluate the capability of existing MLLMs for video understanding, many benchmarks have been proposed (Wu et al., 2025; Li et al., 2024b; Fu et al., 2024). However, most of the video understanding benchmarks primarily take single video as input, which neglects the crucial need for multi-video understanding. This limitation becomes particularly evident in complex real-world scenarios such as summarization on multiple retrieved relevant videos, sports analytics using various camera angles, or autonomous driving requiring information from multiple cameras. To approximate real-world scenarios more accurately, as shown in Figure 1, we introduce the first Multi-Video Understanding benchmark called MVU-Eval 1, which comprehensively assesses eight core perception and reasoning abilities through 1,824 carefully curated QA pairs spanning 4,959 distinct videos from various domains (e.g., life, movie, gaming, autonomous driving). Specifically, the fundamental multi-video perception tasks include Object Recognition (OR), Spatial Understanding (SU), Counting, and Comparison, which aim to evaluate the models ability to accurately extract the vision feature and identify specific content across multiple videos. Second, the high-order multi-video reasoning tasks include Knowledge-Intensive Reasoning (KIR), In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and Temporal Reasoning (TR), which aim to evaluate the models ability to analyze and infer valuable information based on the multi-video input. Based on our MVU-Eval, we provide detailed evaluation of both open-source and closed-source models, highlighting the current limitations and performance discrepancies in multi-image understanding. Specifically, several insightful findings are as follows: * Equal Contribution. 1https://github.com/NJU-LINK/MVU-Eval Corresponding Author. 1 Figure 1: Illustration of several representative examples in our MVU-Eval. The multi-video understanding abilities of MLLMs still have significant room for improvement. For example, the top-performing closed-source Gemini 2.5 Pro only achieves 56.6% accuracy on MVU-Eval. Moreover, except for Qwen2.5-VL series, the accuracies of most open-sourced MLLMs are lower than 50%. The capabilities of different models on different subtasks are imbalanced. For example, the models with the highest accuracies on three perception tasks (i.e., OR, SU, Counting) are Qwen2.5-VL-72B, Qwen2.5-VL-32B, Gemini 1.5 Pro, respectively. Larger models result in better performance. For both Qwen2.5VL series and InternVL series, significant improvements are achieved when scaling the model size. It is critical to support longer contexts (e.g., more frames and higher resolutions) for MLLMs. For VideoLLaMA3-7B, the performance improves lot when increasing the number of frames and the input resolution. To summarize, the contributions of this paper are as follows: we introduce the first multi-video understanding benchmark MVU-Eval covering various subtasks from different real-world application domains, filling critical gap in the evaluation of MLLMs. Then, based on extensive experiments on MVU-Eval, we underscore the challenges and potential directions for improvement of handling and reasoning over multiple videos, offering roadmap for future research and development."
        },
        {
            "title": "2 Related Works",
            "content": "Multimodal LLMs The field of Multimodal Large Language Models (MLLMs) has witnessed substantial advancements. These MLLMs usually combine LLM backbone with visual encoders, employing visionlanguage alignment techniques to strengthen cross-modal comprehension (Liu et al., 2023; 2024; Zhu et al., 2023; Ma et al., 2024; Zhang et al., 2024a; Bai et al., 2025; Wang et al., 2024a; 2025a). Recently, many MLLMs have been proposed for video understanding(Cheng et al., 2024; Li et al., 2023b; 2024d). For example, Video-LLaMA (Zhang et al., 2023) implements dual-path encoding with ViT (Dosovitskiy et al., 2020) and Q-Former (Li et al., 2023a) for spatiotemporal modeling. The recent Mavors (Shi et al., 2025) extracts the multi-granularity video representation to process raw video while preserving both spatial fidelity and temporal coherence. Video Benchmarks The landscape of video understanding benchmarks has undergone significant improvements (Wang et al., 2023; Wu et al., 2024; Xiao et al., 2021; Han et al., 2023; Li et al., 2025a;b; Pan et al., 2025). For example, MVBench (Li et al., 2024b) evaluates the multimodal understanding abilities through concise video QA tasks, while the MLVU (Zhou et al., 2024) and the LongVideoBench (Wu et al., 2025) are proposed to provide comprehensive and in-depth analysis for MLLMs long-video understanding performance. Video-MME (Fu et al., 2024) establishes multi-scale evaluation system 2 Table 1: Comparisons between our MVU-Eval and other related video benchmarks. Here, and are questions and videos, respectively. Benchmark #Q #V Multi-Video Annotation Video Source MVBench (Li et al., 2024b) LongVideoBench (Wu et al., 2025) Video-MME (Fu et al., 2024) MovieChat-1K (Song et al., 2024) Video-MMLU (Song et al., 2025) MLVU (Zhou et al., 2024) 4,000 6,678 2,700 13,000 15, 3,102 4,000 3,763 900 1,000 1,065 1,730 MVU-Eval (Ours) 1,824 4, Auto Human Human Human Human, Auto Human Life, Human Action, Movie Life, Movie, Knowledge, News Life, Movie & TV, Sports, Knowledge Movie Tutorial Life, Movie & TV, Sports, Knowledge, Surveillance, Simulation Human, Auto Life, Human Action, Movie & TV, Room, Animation, Sports, AIGC, Gaming, Autonomous Driving spanning durations from seconds to an hour, incorporating audio processing alongside visual analysis capabilities. Video-MMLU (Song et al., 2025) is massive benchmark designed to evaluate the capabilities of LMMs in understanding multi-disciplinary lectures. However, existing video benchmarks usually take single video as input, and the multi-video understanding is neglected. In this work, we propose the first multi-video understanding benchmark MVU-Eval to address this critical limitation."
        },
        {
            "title": "3.1 Overview",
            "content": "MVU-Eval is designed to address critical gap in multimodal evaluation by establishing the first comprehensive benchmark for multi-video perception and reasoning. Unlike conventional video understanding benchmarks that focus on single-video analysis such as Video-MME Fu et al. (2024), our framework specifically evaluates MLLMs capacity to aggregate, correlate, and reason across multiple video sources - capability essential for real-world applications. Comprising 1,824 carefully curated QA pairs with 4,959 total videos, each question in MVU-Eval requires cross-video integration, demanding not just accurate perception but contextual synthesis of temporal and spatial relationships across disparate visual sequences. Our MVU-Eval systematically evaluates 8 core competencies through 1,824 distinct questions, organized into two progressive protocols, i.e., Perception and Reasoning, with representative examples demonstrated in Figure 1. comprehensive comparison between our MVU-Eval and other related benchmarks is provided in Table 1. It has three key features: (1) supports multiple video inputs with meticulously curated questionanswer pairs, (2) supports unique real-world tasks that naturally require multi-video inputs, and (3) includes variety of video sources for robust evaluation of different domains."
        },
        {
            "title": "3.2 Evaluation Tasks",
            "content": "Perception evaluates the models ability to accurately see and identify specific content, which is one of the basic capabilities of directly extracting and interpreting visual information from each video. They primarily focus on recognizing objects, people, scenes, and spatial relationships. It includes: Object Recognition (OR) evaluates models ability to identify and track identical objects across non-overlapping video sequences, testing cross-modal consistency in dynamic environments. Spatial Understanding (SU) measures models capacity of modeling spatial layout from complementary camera angles, requiring geometric comprehension beyond single viewpoint. Counting assesses models precision in aggregating transient objects appearing across asynchronous videos, addressing real-world challenges like occlusions and partial observations. Comparison probes models aptitude for cross-video feature differentiation, demanding finegrained attribute analysis. Replacement tests the ability to identify and distinguish changes when specific elements in video are substituted with semantically similar or dissimilar alternatives. Removal evaluates how effectively the model detects the absence of key features or objects in one video compared to reference, requiring precise attention to detail. Addition challenges the model to recognize and analyze newly introduced elements in video, ensuring robustness in detecting incremental changes. Figure 2: The overall data construction pipeline of MVU-Eval. Reasoning evaluates the ability to analyze and infer meaningful conclusions beyond simple visual recognition. These tasks require models to engage in higher-order cognitive functions. It includes: Knowledge-Intensive Reasoning (KIR) tests integration of domain knowledge such as sports rules with multi-video evidence to resolve ambiguities invisible in isolated clips. Action Classification challenges models to infer action types by combining visual information from the videos with relevant sports knowledge. Difficulty Measuring requires models to evaluate the difficulty level of specific actions. Score Judging further challenges models to judge the athletic performance based on the inferred actions and their assessed difficulty. In-Context Learning (ICL) challenges models to adapt reasoning strategies learned from limited examples to novel cross-video scenarios, mimicking human-like analogical transfer (Liu et al., 2025b). Retrieval-Augmented Generation (RAG) evaluates selective attention mechanisms for identifying and synthesizing relevant visual evidence from redundant multi-video inputs. Temporal Reasoning (TR) benchmarks temporal logic capabilities by requiring chronological alignment of discontinuous events across videos with varying capture timelines. Temporal Ordering requires models to arrange shuffled video clips into their correct chronological sequence. Temporal Grounding assesses models ability to map specific event descriptions to the corresponding video segments. Temporal Caption Filling challenges models to infer missing events to complete videos event sequence."
        },
        {
            "title": "3.3 Data Collection",
            "content": "As demonstrated in Figure 2, the data collection process for MVU-Eval includes both automated construction and human verification. We first sample video pairs based on specific rules designed for different tasks. Next, question-answer pairs are generated either automatically or using carefully designed templates. Subsequently, we remove possible leaked information in those generated options and filter easy questions using MLLMs. Finally, human annotators are incorporated to ensure the utility and the correctness of each QA pair. Video Pairs Construction. To ensure inter-video relationship, we sample diverse videos following specific rules from variety of open-sourced datasets. The data sources corresponding to each task are listed in Appendix B.1. For instance, for RAG, we initially sampled an anchor video from large-scale video pool with detailed captions. Subsequently, we sample 3-5 videos similar to the anchor, where we take the Jaccard similarity (Jaccard, 1912) between text captions as the similarity metric. Other examples like counting, where we sample videos that have the same objects from video detection datasets. For comparison, we manually curate dataset of 130 samples derived from real-world use cases of the multimodal video editing feature on Kling.AI2. To ensure privacy and copyright protection, samples containing real human faces or copyright-sensitive content are excluded. Detailed construction rules for different data sources are provided in the supplemental material. Question-Answer Pairs Generation. This process includes automatic generation using MLLMs with reject sampling and template-based generation for evaluating specific subtasks. For instance, when 2https://app.klingai.com/cn/ 4 constructing knowledge-intensive reasoning tasks, we randomly select videos from the benchmark dataset and leverage ground truth metadata (e.g., athletes action type and difficulty level) to formulate knowledge-intensive questions. We then use templates to create questions and make them more challenging by adding distractors, such as swapping metadata details like difficulty levels between similar actions, to test how well the model can handle confusion."
        },
        {
            "title": "3.4 Quality Control",
            "content": "First Round Quality Control: Leaked Information Removal. The first round of quality control focuses on removing the leaked information by options, including (1) content leakage and (2) format leakage, as we find that the model sometimes could infer the correct answer without watching the provided videos. An instance of content leakage is when the question is Which video contains the most chairs? and the generated correct option is The classroom in Video 1. This additional text description, i.e., classroom here, helps the model to guess the correct answer without watching the video. Therefore, we rewrite options with additional text information. Specifically, we simply omit additional information and rewrite The classroom in Video 1 into Video 1. As for the format leakage, we observe that the distractors generated by LLMs often follow certain format priors, such as three short wrong options and one long correct option. To address this, we first use multiple LLMs to filter out questions that can be answered correctly without watching the video. Then, we prompt the LLMs to strictly control the format of the options and regenerate them until the accuracy rate without video input approaches random chance. Second Round Quality Control: Human Checking. The second round focuses on checking the utility of generated questions and the correctness of answers. First, we simply filter easy questions that both Gemini 2.5 Pro (Team et al., 2024), Gemini 2.0 Flash (Team et al., 2024), Qwen2.5-VL-72B (Bai et al., 2025) can answer correctly. Following automated data collection, we employ human verification to enhance dataset quality. By reviewing each question-answering pair and the corresponding videos, human annotators are required to check (1) the utility of the generated question and (2) the correctness of the answer. Here, the utility includes (1) the question must be answerable and challenging, and (2) the question is answerable only after checking all the given videos. The corresponding sample is discarded if the annotator considers the question-answering pair invalid. Moreover, we manually balance the distribution of ground-truths as we empirically found that the generated options are not balanced (usually biased to and B). In quality control, many low-quality question-answer pairs are discarded, and many wrong answers are corrected by humans. Specifically, 4,187 video pairs were initially sampled, and then, question-answering pairs were generated for each video pair. After difficulty evaluation through testing with different models, roughly 2,710 pairs are retained, with about 35% of the easier data being discarded. Subsequently, another 563 samples are removed after rule-based verification in the human checking process, which means that only about 51% of the original generated data remains. Finally, after thorough and rigorous manual review, only about 1,824 samples are kept, which is approximately 46% of the original dataset. Figure 3: The histogram of #videos."
        },
        {
            "title": "3.5 Dataset Statistics",
            "content": "Table 2 presents the statistics of MVU-Eval. With total of 1,824 samples, the data distribution across the 8 primary topics in MVU-Eval is relatively balanced. Furthermore, as demonstrated in Figure 3, our dataset exhibits significant distribution of video content associated with each question, with an average number of videos per question of 4.7, indicating rich and diverse set of visual data that must be thoroughly analyzed for accurate comprehension. The majority of questions are accompanied by 4-6 videos. It is noteworthy that the dataset includes questions with up to 13 videos, although such cases are rare. Moreover, the ground-truth options in our MVU-Eval are relatively balanced, with the distribution being: 25.5% for option A, 25.8% for option B, 22.7% for option C, 20.4% for option D, and 5.6% for other options. The distribution of video categories is shown in Figure 4. The video source of MVU-Eval is diverse, with total of 4,959 videos. 5 Figure 4: The distribution of video categories in MVU-Eval. Table 2: Data statistics of our MVU-Eval. Note that each video is resized such that its longer side is limited to 720 pixels and the other side is scaled proportionally. Following Qwen2.5-VL, the patch size is set to 28 28. Statistics Number Statistics Number Statistics Number Perception Topics - Object Recognition - Spatial Understanding - Counting - Comparison - Replacement - Removal - Addition - Others 667 126 179 227 135 40 24 40 31 Reasoning Topics - Knowledge-Intensive Reasoning - In-Context Learning - Retrieval-Augmented Generation - Temporal Reasoning - Temporal Ordering - Temporal Grounding - Temporal Caption Filling - Others 1,157 281 164 339 373 152 164 27 30 Video Token Length - maximum length - minimum length - averaged length 154,336 8,324 51,013 Question Length - maximum length - minimum length - averaged length 892 47 111 Table 3: Category-wise model performance on MVU-Eval. OR: object recognition. SU: spatial understanding. KIR: knowledge-intensive reasoning. ICL: in-context learning. RAG: retrieval-augmented generation. TR: temporal reasoning. The best performance and the second best performance are highlighted in green and yellow , respectively. Overall Perception Reasoning Random Choice Human Gemini 2.5 Pro (Team, 2025) Gemini 1.5 Pro (Team et al., 2024) Gemini 2.0 Flash (Team, 2024) GPT-4o (OpenAI, 2024) Model Size > 40B Qwen2.5-VL-72B (Bai et al., 2025) InternVL3-78B (Zhu et al., 2025) InternVL2.5-78B (Chen et al., 2024b) LLaVA-OneVision-72B (Li et al., 2024a) 8B < Model Size 40B Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-38B (Zhu et al., 2025) InternVL2.5-38B (Chen et al., 2024b) 4B < Model Size 8B Qwen2.5-VL-7B (Bai et al., 2025) VideoChat-Flash-7B (Li et al., 2024c) VideoLLaMA3-7B (Zhang et al., 2025a) InternVideo2.5-8B (Wang et al., 2025b) mPLUG-Owl3-7B (Ye et al., 2024) InternVL3-8B (Zhu et al., 2025) InternVL2.5-8B (Chen et al., 2024b) LLaVA-OneVision-7B (Li et al., 2024a) MiniCPM-o (Yao et al., 2024) Slow-Fast-MLLM-7B (Zhou et al., 2025) MiniCPM-V (Yao et al., 2024) LLaVA-Video-7B (Zhang et al., 2024c) LLaVa-NeXT-Video-7B (Zhang et al., 2024b) Qwen2-7b-LongVILA-1M (Chen et al., 2024a) Video-XL-2-8B (Qin et al., 2025) Model Size 4B Qwen2.5-VL-3B (Bai et al., 2025) InternVL2.5-4B (Chen et al., 2024b) Video-XL-Pro-3B (Liu et al., 2025c) 4 Experiments 26.0 93.6 58.4 57.3 56.3 55.9 57.1 50.6 48.7 44.6 55.6 48.4 44.5 51.9 48.5 47.5 46.4 45.0 41.7 41.1 40.4 40.6 38.7 37.9 27.4 26.8 32.7 43.7 46.2 37.3 39. OR 25.5 98.4 SU 25.3 96.6 Counting Comparison 24.3 100. 13.6 100.0 Closed-Sourced Models 47.6 51.6 46.0 54.7 54.7 55.3 52.0 57.7 65.6 66.1 45.4 58.9 Open-Sourced Models 52.4 42.9 44.4 31.7 48.4 46.0 37.3 50.8 48.4 48.4 45.2 48.4 41.3 38.1 40.5 31.0 44.4 34.1 26.2 22.2 27.0 34.1 46.0 32.5 38.9 56.4 56.4 47.5 50.8 57.0 46.4 40. 55.3 55.9 50.3 43.0 53.6 44.1 40.8 36.3 45.3 38.5 41.3 26.3 29.1 41.3 41.3 45.8 40.2 40.2 58.1 49.8 45.8 44.5 59.5 47.1 40.1 62.1 55.5 52.9 44.9 50.2 31.3 28.2 36.6 37.9 37.4 32.6 35.7 23.8 31.3 36.4 44.1 28.2 31. 76.3 67.4 75.6 74.6 77.8 72.6 72.6 61.5 71.1 69.6 67.4 65.2 67.4 60.0 63.7 50.4 54.8 54.8 45.9 63.7 54.8 45.9 43.0 20.7 30.4 64.4 46.7 45.2 38.5 KIR 25.0 82.9 50.2 43.1 53.7 36.3 43.8 43.8 38.1 37.4 43.4 42.0 40.2 32.4 38.1 37.0 37.7 29.5 34.5 36.9 29.9 26.7 20.3 26.3 7.9 27.8 31.7 35.6 36.3 33.8 35. ICL 25.0 66.5 34.8 47.6 45.1 38.9 35.4 34.1 28.7 26.2 28.7 30.5 28.0 29.3 25.0 29.9 28.7 24.4 26.8 28.0 28.0 21.3 24.4 23.2 22.0 12.8 26.2 28. 27.4 17.7 20.7 RAG 25.0 100.0 43.7 44.0 44.5 42.0 48.1 49.0 48.1 44.5 48.4 42.8 43. 49.3 43.1 44.0 48.1 41.6 43.7 44.5 45.1 42.5 46.9 43.7 18.9 28.9 36.6 48.7 46.3 42.8 44.5 TR 34.0 99.2 83.1 78.6 79.1 74.6 78.6 56.8 61.4 53. 76.9 61.1 54.7 66.8 57.1 57.1 56.0 58.2 52.5 51.1 51.5 52.0 44.5 47.7 42.4 34.9 32.0 53.6 63.3 46.4 49.3 We evaluate numerous closed-source MLLMs: Gemini 2.5 Pro, Gemini 2.0 Flash, and Gemini 1.5 Pro. For open-source models, we select 22 representative MLLMs, including Qwen2.5 series (Bai et al., 2025), InternVL2.5 series (Chen et al., 2024b), InternVL3 series (Zhu et al., 2025), InternVideo2.5 series (Wang et al., 2025b), VideoChat-Flash series (Li et al., 2024c), VideoLlama3 series (Zhang et al., 2025a), mPLUG-Owl3 series (Ye et al., 2024), LLaVA-Video series (Zhang et al., 2024c), LLaVA-Onevision series (Li et al., 2024a), LLaVa-NeXT-Video series (Zhang et al., 2024b), MiniCPM series (Yao et al., 2024), Slow-Fast-MLLM series (Zhou et al., 2025). Evaluation. We adopt accuracy as the evaluation metric based on zero-shot setting. For each model, we adopt uniform sampling strategy to process video frames, setting the number of frames to 32. Each video is resized before input to models that the longer side is limited to 720 pixels and the other side is scaled proportionally. More details are described in Appendix B.2.1. For the prompts, we provide examples for the eight tasks of MVU-Eval in Appendix B.2.5."
        },
        {
            "title": "4.1 Main Results",
            "content": "In Table 3 and Figure 5, we provide the performance results of different LLMs on our MVUEval, and we have the following insightful and interesting observations: (1) MVU-Eval is very challenging. The top-performing closed-source MLLM (i.e., Gemini 2.5 Pro) achieves the best performance on MVU-Eval, which is inferior to the performance of human experts lot. Besides, the results of Qwen2.5-VL-72B are close to Gemini 2.5 Pro, which is also better than several closedsourced MLLMs (e.g., Gemini 2.0 Flash). (2) Results on different subtasks vary lot. For example, Gemini 2.5 Pro is better than Qwen2.5-72B-VL on counting and knowledge-intensive reasoning lot. However, on the RAG setting, Qwen2.5-72B-VL achieves 48.1% accuracy, which is better than Gemini 2.5 Pro (43.7%). (3) Scaling property is well preserved. For Qwen2.5-VL series (3B/7B/32B/72B) and InternVL3 series (8B/38B), larger model leads to better performance. (4) Some smaller models (e.g., Qwen2.5-VL-3B) outperform larger ones (e.g., LLaVA-OneVision-7B), likely due to better architectural design or more effective data strategies. (5) Through further analysis of the model outputs, we find that some MLLMs (e.g., LLaVA-Video-7B) on some specific tasks (e.g., KIR) often fail to follow the instructed output format and do not provide the required answers, instead generating free-form descriptive text. Figure 5: Model scaling of MLLMs on MVU-Eval."
        },
        {
            "title": "4.2 Further Analysis",
            "content": "Effectiveness of vision information. To evaluate the impact of visual information in MVU-Eval, we compare model performance across five inference settings: (1) Multi-video: Multiple videos are input separately into the MLLMs. (2) Single-video: One video is randomly sampled from each multi-video QA pair. This experiment is repeated five times for consistency. (3) Multi-image: One frame is randomly sampled from each video. This experiment is also repeated five times. (4) Textual descriptions of videos: Each video is input into the MLLM to generate textual description. These descriptions are then used in place of visual input to complete the QA tasks with the same MLLM. (5) No-video: Only the textual content of the questions and answer options is provided, and no visual information is used. is to tied that task multi-video From the results of VideoLLaMA3-7B shown in Table 4, three key observations can be made from the results: (1) The results demonstrate that model performance consistently degrades as the amount of visual information provided decreases. This trend highlights the well-designed nature of the MVU-Eval tasks, ensuring contexts. closely each (2) Models under the text-description setting exhibit better performance compared to the multi-images setting. One possible explanation for this is that the multi-image setting only provides single frame for each video, losing temporal information. In contrast, the text-description setting captures the entire process of each video, thus preserving more detailed information. (3) Models under some settings (e.g., video-disabled) show inferior performance to random guess. Through analyzing the responses of the models, we find that models would refuse to answer when the important visual information is missing (See the results of single-video and video-disabled settings in Table 4). Table 4: Comparison of VideoLLaMA3-7B performance under different vision information types. indicate the change relative to Multi-video. Our MVU-Eval necessitates comprehensive analysis of all videos, as using only single frame from the video results in significant degradation. Single-video Multi-image Text-description No-video 47.5 24.9 22.6 34.6 12.9 41.0 6.5 16.0 31.5 Multi-video ACC (%) Methods Effectiveness of number of frames. On the left of Figure 7, VideoLLaMA3 generally exhibits improved performance with an increasing number of frames. However, when the number of frame reaches 64, we observe performance degradation due to excessive input tokens overwhelming the models processing capacity. 7 Figure 6: Effectiveness of different numbers of videos. higher number of involved video clips in question correlates with increased difficulty. Effectiveness of input resolution. On the right of Figure 7, input resolution exhibits similar pattern to the number of frames, where model performance improves with higher resolutions up to 720. However, performance begins to degrade when the input resolution is 960, primarily due to the increased number of input tokens that exceeds the models optimal processing capacity. Effectiveness of different numbers of videos. In Figure 6, we investigate the effectiveness of different numbers of videos. Given that the number of videos in 93.8% of QA pairs ranges from two to six, we assess the accuracy of eight representative MLLMs on this subset of MVU-Eval. The results reveal that as the number of videos increases, most models experience decline in performance. This observation further highlights the well-designed features of MVU-Eval, as it suggests that the models ability to answer QA pairs depends on considering all the provided videos. Figure 7: Effectiveness of input resolution and number of frames on MVU-Eval. The performance degradation is possibly due to excessive input tokens. Table 5: Comparison of Qwen2.5-VL-7B performance under different input formats. Effectiveness of input format. The Qwen2.5-VL series consistently outperforms other models of comparable size. One possible explanation is its ability to support naive multiple video inputs. To explore how input format affects the performance of Qwen2.5 models, we propose two alternative input formats based on Qwen2.5-VL-7B. For Multi-image (per video), we directly sample 32 frames uniformly for each video to construct the input of Qwen2.5-VL-7B. For Merged-video, we merge multiple videos into single input by concatenating them with black frames inserted between each segment. The prompt templates are provided in Appendix B.2.4. As shown in Table 5, different input formats can significantly influence the models performance for multi-video understanding. Multi-video Multi-image (per video) Merged-video 51.9 45.2 6.7 44.6 7.3 ACC (%) Methods Failure Cases. To better understand the limitations of MLLMs in multi-video understanding tasks, we analyze specific failure cases and derive two key observations. (1) For perception tasks, while MLLMs generally perform well in detecting the presence of objects, they struggle to interpret object status or function. For example, most models fails to distinguish whether shovel is being actively used or merely held by person. Moreover, models exhibit difficulty in understanding spatial relationships across multiple video perspectives, such as interpreting scenes from cameras positioned at different angles on the same moving vehicle. (2) For reasoning tasks, MLLMs struggle with reasoning that involves domainspecific knowledge, filtering out irrelevant information across videos, and understanding temporal and causal relationships. For example, while models can often understand what happens, they fail to explain why it happens. These observations highlight the shortcomings of MLLMs and underscore the necessity of developing comprehensive benchmark to evaluate their multi-video understanding capabilities. Additionally, we present two representative failure cases in Figure 8. In the failure case of counting task, the models are required to recognize the object Table and count its occurrences. Although this task is relatively easy for human experts, we conduct detailed analysis of each video. possible explanation for the failure is that Video 2 is significantly longer than the other two (1 minute vs. 10 seconds), and the tables within it are highly similar, making them difficult for the models to distinguish. In the case of spatial understanding, the models are expected to understand spatial relationships and reason about 8 Figure 8: Visualization of several failure cases in MVU-Eval. temporal changes on the right side of the environment. Specifically, the left side of the vehicle corresponds to option B, while the right side aligns with option D. likely reason for the failure is that models selecting option retained the ability to perform temporal reasoning but struggled with spatial understanding. These examples highlight the crucial need to enhance models multi-video understanding capabilities."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present MVU-Eval, the first and comprehensive benchmark for evaluating MLLMs on multi-video understanding, spanning eight core competencies for basic perception and advanced reasoning tasks. Through extensive experiments on multiple MLLMs, we provide several insightful findings, which highlight the need for improved architectures and training data strategies to tackle complex multi-video scenarios in practical applications."
        },
        {
            "title": "6 Future Works",
            "content": "Building on the limitations identified in MVU-Eval, we outline several promising directions for future research in multi-video understanding for MLLMs as follows: Cross-video Visual Alignment: Addressing the challenge where different videos may not start at the same temporal moment or have their frames aligned on shared timeline, which is crucial given that most videos in MVU-Eval are asynchronous except for small portion in Spatial Understanding tasks. Cross-video Spatial Understanding: Developing the ability to identify the same objects across multiple videos as anchor points to facilitate comprehensive spatial comprehension, as required by the Spatial Understanding task that demands geometric comprehension beyond single viewpoint. Temporal Reasoning in Asynchronous Multi-Video Scenarios: Enhancing models capacity to infer temporal relationships across unaligned video streams, which is vital for tasks like Temporal Reasoning in MVU-Eval and becomes more challenging as the number of videos increases . Scalable Multi-Modal Fusion for High-Cardinality Inputs: Exploring efficient fusion strategies to handle more videos without exceeding token limits, as current MLLMs face performance degradation when processing excessive tokens from too many videos, frames, or high resolution. Generalization to Out-of-Distribution Multi-View Scenarios: Enhancing model robustness across diverse video sources in MVU-Eval, including indoor, outdoor, gaming, AIGC, and movie and TV, to ensure performance in unseen real-world scenarios. 9 References Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, JinKe, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, Noah Wang, Boyang Wang, Xianjie Wu, Bing Wang, Tongliang Li, Liqun Yang, Sufeng Duan, Zhaoxiang Zhang, and Zhoujun Li. Mceval: Massively multilingual code evaluation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=UunCPtPOlZ. Xinran Chen, Yuchen Li, Hengyi Cai, Zhuoran Ma, Xuanang Chen, Haoyi Xiong, Shuaiqiang Wang, Ben He, Le Sun, and Dawei Yin. Multi-agent proactive information seeking with adaptive llm orchestration for non-factoid question answering. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 43414352, 2025. Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv e-prints, art. arXiv:2010.11929, October 2020. doi: 10.48550/arXiv.2010.11929. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv preprint arXiv:2312.10300, 2023. 10 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Paul Jaccard. The distribution of the flora in the alpine zone. 1. New phytologist, 11(2):3750, 1912. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. URL https://arxiv.org/abs/1705.06950. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, and Jiaheng Liu. Omnivideobench: Towards audiovisual understanding evaluation for omni mllms, 2025a. URL https://arxiv.org/abs/2510.10689. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, , and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024b. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023c. Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, and Jiaheng Liu. If-vidcap: Can video caption models follow instructions?, 2025b. URL https://arxiv.org/abs/2510. 18726. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024c. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pp. 323340. Springer, 2024d. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. Jiaheng Liu, Ken Deng, Congnan Liu, Jian Yang, Shukai Liu, He Zhu, Peng Zhao, Linzheng Chai, Yanan Wu, JinKe JinKe, Ge Zhang, Zekun Moore Wang, Guoan Zhang, Yingshui Tan, Bangyu Xiang, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng. M2RC-EVAL: Massively multilingual repository-level code completion evaluation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1566115684, Vienna, Austria, July 2025a. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long.763. URL https://aclanthology.org/2025.acl-long.763/. Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025b. Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025c. 11 Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing hallucination in video language models via equal distance to visual tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1315113160, 2024. OpenAI. Gpt-4o, 2024. Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, and Jiaheng Liu. Mt-video-bench: holistic video understanding benchmark for evaluating multimodal llms in multi-turn dialogues, 2025. URL https://arxiv.org/abs/2510.17722. Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 45424550, 2024. Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, and Zheng Liu. Video-xl-2: Towards very long-video understanding through task-aware kv sparsification. arXiv preprint arXiv:2506.19225, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, and Di Zhang. Mavors: Multi-granularity video representation for multimodal large language model, 2025. URL https://arxiv.org/abs/2504.10068. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1822118232, 2024. Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, and Gaoang Wang. Video-mmlu: massive multi-discipline lecture understanding benchmark. arXiv preprint arXiv:2504.14693, 2025. Gemini Team. Introducing gemini 2.0: our new ai model for the agentic era, December 2024. URL https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/. Gemini Team. Build rich, interactive web apps with an updated gemini 2.5 pro, March 2025. URL https://blog.google/products/gemini/gemini-2-5-pro-updates. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024a. Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025a. Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024b. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025b. Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu, Jaemin Cho, Zineng Tang, Mohit Bansal, and Heng Ji. Paxion: Patching Action Knowledge in Video-Language Foundation Models. arXiv e-prints, art. arXiv:2305.10683, May 2023. doi: 10.48550/arXiv.2305.10683. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. STAR: Benchmark for Situated Reasoning in Real-World Videos. arXiv e-prints, art. arXiv:2405.09711, May 2024. doi: 10.48550/arXiv.2405.09711. 12 Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37: 2882828857, 2025. Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, ZhiqiBai ZhiqiBai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, and Bo Zheng. ConceptMath: bilingual concept-wise benchmark for measuring mathematical reasoning of large In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the language models. Association for Computational Linguistics: ACL 2024, pp. 68156839, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.407. URL https://aclanthology.org/2024.findings-acl.407/. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions. arXiv e-prints, art. arXiv:2105.08276, May 2021. doi: 10.48550/arXiv. 2105.08276. Jinglin Xu, Yongming Rao, Xumin Yu, Guangyi Chen, Jie Zhou, and Jiwen Lu. Finediving: fine-grained dataset for procedure-aware action quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 29492958, 2022. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. URL https://arxiv.org/abs/2408.04840. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv: 2309.05653, 2023. URL https://arxiv.org/abs/2309.05653v1. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024a. Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token. arXiv preprint arXiv:2501.03895, 2025b. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https: //llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024c. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Yifei Zhou, Jiaming Zuo, Chen Change Loy, Chongyang Zhong, Xin Wang, Qi Wu, Weidong Cai, Xiaodong He, Qingzhong Wang, Lei Zhang, Marcelo H. Ang Jr, Boyang Li, Yanfeng Wang, Qinghai He, Fengbei Liu, Liangchen Luo, Jingdong Wang, Conghui He, and Wenhai Wang. Slow-fast architecture for video multi-modal large language models, 2025. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 13 Table 6: Data source of MVU-Eval. Task Subtask Source Perception Reasoning Object Recognition Spatial Understanding Counting Comparison Kinetics-400 Kay et al. (2017), nuScenes (Qian et al., 2024), ScanNet (Dai et al., 2017) Kinetics-400 Kay et al. (2017), nuScenes (Qian et al., 2024), ScanNet (Dai et al., 2017) Kinetics-400 Kay et al. (2017), nuScenes (Qian et al., 2024), ScanNet (Dai et al., 2017) FineDiving (Xu et al., 2022) Knowledge-intensive Reasoning In-context Learning Retrieval-augmented generation Temporal Reasoning FineDiving (Xu et al., 2022), YouCook2 (Zhou et al., 2018), Kinetics-400 Kay et al. (2017), nuScenes (Qian et al., 2024) FineDiving (Xu et al., 2022) Vchitect-2.0 (Fan et al., 2025) YouCook2 (Zhou et al., 2018), Kinetics-400 Kay et al. (2017), nuScenes (Qian et al., 2024), DREAM-1K (Wang et al., 2024b) Ethical, Technical, and Resource Statements A.1 Ethics Statement Our work introduces MVU-Eval, benchmark for evaluating multi-video understanding in MLLMs, and does not pose direct ethical concerns. All videos and annotations are either synthetically generated or sourced from publicly available datasets, containing no personally identifiable information or sensitive content. No human participants were involved in data collection or experimentation. A.2 Limitations Despite the strengths of our proposed MVU-Eval, there are still several limitations to consider. First, the video samples used in the benchmark are relatively short in duration and do not reach movie-level lengths. This constrains the benchmarks ability to assess long-range temporal reasoning and narrative comprehension over extended video sequences. Second, MVU-Eval currently focuses on visual inputs and does not support the evaluation of models on audio. As many real-world scenarios involve rich auditory information, future extensions of MVU-Eval will aim to incorporate audio-based assessments to enable more comprehensive multimodal understanding. A.3 Broader Impacts Our work establishes benchmark for multi-video understanding, focusing on advancing core technical capabilities without direct ties to specific applications or deployments. As dataset designed purely for research purposes, it primarily contributes to the development of robust and generalizable models for multi-video understanding tasks."
        },
        {
            "title": "B Experimental Settings",
            "content": "B.1 Data Source The data source of the eight tasks in MVU-Eval is shown in Table 6. B.2 Evaluation Settings B.2.1 Frames and Resolution The experimental details regarding the number of frames and resolution are provided as follows: For most models, we sample 32 frames per video. Each frame is resized such that its longer side is limited to 720 pixels, with the shorter side scaled proportionally. The following models adopt this setting: Qwen2.5 series (Bai et al., 2025), VideoChat-Flash series (Li et al., 2024c), VideoLlama3 series (Zhang et al., 2025a), mPLUG-Owl3 series (Ye et al., 2024), LLaVA-Video series (Zhang et al., 2024c), LLaVA-Onevision series (Li et al., 2024a), LLaVA-NeXT-Video series (Zhang et al., 2024b), MiniCPM series (Yao et al., 2024), and Slow-Fast-MLLM series (Zhou et al., 2025). For the InternVL2.5 (Chen et al., 2024b) and InternVL3 (Zhu et al., 2025) series, we sample 32 frames per video and set the resolution to 448 448, following the model-specific requirements. For the InternVideo2.5 series (Wang et al., 2025b), we sample 32 frames and set the resolution to 728 728, in order to minimize the impact of varying experimental settings. 14 For the closed-source models, we also sample 32 frames per video. Each frame is resized such that its longer side is limited to 720 pixels, with the other side scaled proportionally, before prompting the models to complete the QA tasks. B.2.2 Human Evaluation We invited five domain experts with rich experience in multimodal video understanding to participate in the evaluation. The entire process consists of two key phases: In the first phase, all five experts were required to independently complete MVU-Eval, covering all eight subtasks. Each expert received detailed task instructions and was asked to provide answers based solely on the video content and their own knowledge, with no communication between experts during this phase. This ensured that each answer was an independent judgment. In the second phase, we first conducted consistency analysis of the answers from the first phase. For samples where there was disagreement (i.e., where two or more experts gave different answers), we organized joint review meeting. During this meeting, experts presented their reasoning processes, discussed the key details in the videos, and clarified any ambiguities in the task requirements. Through in-depth discussions, we aimed to reach consensus on the correct answers for these controversial samples. For cases where consensus could not be reached after full discussion, we adopted the majority opinion (with at least three experts agreeing) as the final answer, while noting the divergence to highlight potential high-difficulty samples. B.2.3 Evaluation Protocol We adopt accuracy as the evaluation metric based on zero-shot setting. Although models are prompted to respond with the option letter directly, some models (e.g., Qwen2.5-VL-32B) still generate intermediate reasoning. To ensure fair and comprehensive evaluation, we design systematic, rule-based pipelines that mitigate the potential influence of such intermediate content. Specifically, we construct robust regular expressions and develop response-processing workflows to extract answer candidates following identifiable patterns (e.g., The answer is A., A). If no valid answer is found, we default to using the first letter of the models response as its answer. response is considered correct only if the extracted answer exactly matches the ground truth. B.2.4 Prompt Templates for Video Information For the experiments presented in Table 5, we preprocess the original videos into two input formats, including multi-image (per video) and merged-video. Corresponding to each format, we adopt different prompt templates, as detailed below. Multi-image (per video) The following are 32 frames of the Video 1. <Frame_1><Frame_2> . . . <Frame_32> The following are <number_of_frames> frames of the Video 2. <Frame_1><Frame_2> . . . <Frame_32> . . . <question> <options> Please select the correct answer from the options. Answer with the options letter directly. Merged-video The following are one merged video that concatenated by <number_of_videos> videos in order, separated by the black delimiter frame between every two videos. <video> <question> <options> Please select the correct answer from the options. Answer with the options letter directly. B.2.5 Prompt Templates for Tasks In this section, we provide the prompt templates for the eight tasks in MVU-Eval. 15 Object Recognition Which athlete used the most complex technique while climbing the rope in these videos? A. The athlete in the Video 1 B. The athlete in the Video 2 C. The athlete in the Video 3 D. The athlete in the Video 4 Please select the correct answer from the options. Answer with the options letter directly. Spatial Understanding How does the traffic condition on the left side of the vehicle affect the drivers visibility during night driving? A. The left side of the vehicle passes junction with vehicles, requiring careful observation. B. The left side of the road is flat with no obstacles, providing good visibility. C. There are pedestrians on the left side of the vehicle, requiring extra attention. D. There are streetlights on the left side of the vehicle, affecting visibility. Please select the correct answer from the options. Answer with the options letter directly. Counting Which of the three videos has the most number of sofas? A. Video 1 B. Video 2 C. Video 3 Please select the correct answer from the options. Answer with the options letter directly. Comparison Which vehicles may pose potential threat to the current vehicle during driving? Please analyze using multiple camera perspectives. A. Compact car B. Bicycle C. Multiple large trucks and school buses D. Motorcycle Please select the correct answer from the options. Answer with the options letter directly. Knowledge-Intensive Reasoning Which of these 6 videos has the lowest difficulty in terms of technical skills? A. Video 3 B. Video 4 C. Video 5 D. Video 2 Please select the correct answer from the options. Answer with the options letter directly. In-contxt Learning The answer of the Video 1 is 42.75, the answer of the Video 2 is 36.45, the answer of the Video 3 is 56.4. Based on the above video and answers, infer what the question is, and then answer the question about Video 4. A. 76.8 B. 72.9 C. 104.4 D. 68.15 Please select the correct answer from the options. Answer with the options letter directly. 16 Retrieval-augmented Generation What changes can be seen in the mans gestures from the video? A. He keeps pointing at the audience with his finger. B. He starts with his hands crossed in front of him and then begins to gesture. C. He keeps tapping the table with his fingers. D. He keeps gesturing with one hand. Please select the correct answer from the options. Answer with the options letter directly. Temporal Reasoning Sort the six videos in the correct temporal order. A. Video 5, Video 6, Video 3, Video 1, Video 4, Video 2 B. Video 3, Video 6, Video 2, Video 5, Video 1, Video 4 C. Video 6, Video 1, Video 5, Video 4, Video 3, Video 2 D. Video 6, Video 1, Video 2, Video 4, Video 3, Video 5 Please select the correct answer from the options. Answer with the options letter directly."
        },
        {
            "title": "C Further Analysis",
            "content": "C.1 Instruction-Following Behavior We employ robust regular expressions to parse model outputs for automated evaluation. However, some MLLMs fail to follow the instructed output format, leading to missing or unstructured answers. For instance, over 98% of LLaVA-Video-7Bs responses were generated as free-form paragraphs rather than the requested single-letter answers. Even when explicitly prompted to select an option (e.g., Which video has the lowest difficulty?), the model still produced descriptive text instead of providing concise answer. Example Prompt (KIR Task) Which of these 6 videos has the lowest difficulty in terms of technical skills? A. Video 3 B. Video 4 C. Video 5 D. Video 2 Please select the correct answer from the options. Answer with the options letter directly. LLaVA-Video-7B Output The video showcases series of synchronized and individual dives performed by divers at the 2016 European Championships in London. The athletes demonstrate impressive acrobatic skills, including flips and twists, as they descend from high diving platforms into the pool. The sequence captures the precision and athleticism required for competitive diving, with close-up shots highlighting the water entry and the splash created by the divers impact. The event is broadcasted live on Eurosport, as indicated by the logo visible in the top right corner of the frames. While the generated text is often semantically relevant, it completely ignores the task-specific question. Through further analysis of the model outputs, we found that certain MLLMs (e.g., LLaVA-Video-7B) frequently fail to follow the instructed output format and do not provide the required answers, instead generating free-form descriptive text. This phenomenon highlights critical limitation in instructionfollowing and output controllability, which we explicitly discuss in this revised version. C.2 Performance of Long-Video MLLMs To further strengthen the evaluation, we additionally incorporate several MLLMs specialized for longvideo understanding into the main results table, including Video-XL-Pro-3B (Liu et al., 2025c), Qwen27B-LongVILA-1M (Chen et al., 2024a), Video-XL-2-8B (Qin et al., 2025), and mPLUG-Owl3-7B (Ye et al., 2024). These models are designed to process extremely long temporal contexts through reconstructive 17 token compression and key-value sparsification mechanisms, which theoretically should benefit multivideo understanding. However, as shown in Table 3, their performance on MVU-Eval remains modest. Despite stronger temporal modeling, these models exhibit only limited gains on temporal reasoning tasks and even underperform in spatially grounded subtasks, suggesting that f-context modeling alone is insufficient for handling cross-video reasoning, which additionally requires effective inter-video fusion and alignment mechanisms."
        }
    ],
    "affiliations": [
        "CASIA",
        "Kuaishou Technology",
        "M-A-P",
        "Nanjing University"
    ]
}