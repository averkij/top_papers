{
    "paper_title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "authors": [
        "Sourena Khanzadeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 4 1 3 2 0 . 1 0 6 2 : r Project Ariadne: Structural Causal Framework for Auditing Faithfulness in LLM Agents"
        },
        {
            "title": "Sourena Khanzadeh",
            "content": "January"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decisionmaking, the transparency of their reasoning processes has become critical safety concern. While Chain-ofThought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the models output or merely post-hoc rationalizations. We introduce Project Ariadne, novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodessystematically inverting logic, negating premises, and reversing factual claimsto measure the Causal Sensitivity (ϕ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals persistent Faithfulness Gap. We define and detect widespread failure mode termed Causal Decoupling, where agents exhibit violation density (ρ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as Reasoning Theater while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as new benchmark for aligning stated logic with model action. The rapid proliferation of Large Language Model (LLM) agents has ushered in paradigm shift in autonomous problem-solving, moving beyond simple text generation toward complex, multi-step Chainof-Thought (CoT) reasoning. As these agents are increasingly deployed in high-stakes domainsranging from financial forecasting to autonomous scientific discoverythe transparency of their decision-making processes becomes critical safety frontier. However, significant sociotechnical challenge remains: the Faithfulness Gap. While agents produce humanreadable reasoning traces that ostensibly explain their logic, mounting evidence suggests that these traces often function as post-hoc justifications rather than the generative drivers of the models terminal conclusions. This phenomenon, which we term Causal Decoupling, represents fundamental failure in Explainable AI (XAI). When an agents internal thoughts are not causally linked to its final actions, the reasoning trace becomes hallucinated explanationa dangerous veneer of transparency that masks the underlying black-box heuristics of the transformer architecture. To address this, we introduce Project Ariadne, diagnostic framework designed to audit the causal integrity of agentic reasoning through the lens of Structural Causal Models (SCMs). Unlike traditional evaluation metrics that rely on surface-level textual similarity or static benchmarks, Project Ariadne utilizes counterfactual interventionist approach. By treating the reasoning trace as sequence of discrete causal nodes, we systematically 1 perform hard interventionsflipping logical operators, negating factual premises, or inverting causal directions. We then observe the resulting shift in the agents counterfactual answer distribution. By quantifying the Causal Sensitivity of the output to these perturbations, Ariadne provides formal mathematical basis for distinguishing between truly thinking agents and those merely performing reasoning theater. In the following sections, we define the structural equations governing our interventionist framework, establish metrics for faithfulness violations, and demonstrate the utility of Project Ariadne in detecting unfaithful reasoning across state-of-theart agentic architectures."
        },
        {
            "title": "2 Related Work",
            "content": "The evaluation of faithfulness in Large Language Model (LLM) agents has emerged as primary bottleneck in AI safety. Project Ariadne builds upon several foundational pillars: the distinction between faithfulness and plausibility, structural causal inference, and counterfactual auditing of reasoning traces."
        },
        {
            "title": "2.1 The Faithfulness-Plausibility Gap",
            "content": "A central challenge in eXplainable AI (XAI) is ensuring that an agents reasoning trace (q) reflects its actual decision-making process (faithfulness) rather than merely serving as human-convincing narrative (plausibility) [2]. Foundational work has demonstrated that reasoning traces frequently function as post-hoc justifications [3]. Recent empirical studies confirm that LLMs often arrive at conclusions through biased heuristics despite providing seemingly logical Chain-of-Thought (CoT) explanations [4], leading to what we define as Causal Decoupling. causal dependencies si = fstep(q, s<i, θ) [?]. By modeling the agents response function fagent : as causal graph, we can rigorously define faithfulness as causal consistency: change in reasoning ι(sk) = sk must necessitate change in the final answer a(q) = aι(q, k)."
        },
        {
            "title": "Interventions",
            "content": "in"
        },
        {
            "title": "LLMs",
            "content": "Interventional auditing has been successfully applied to model weights, such as the ROME method which uses causal tracing to locate factual associations [5]. Project Ariadne extends this logic to the semantic space of reasoning traces by performing systematic interventions ι at the step level. Related work on interventional faithfulness has begun to quantify terminal output shifts when intermediate steps are mutated [8]. Ariadne formalizes this through Faithfulness Score ϕ, calculated via the semantic similarity between original and counterfactual answers: ϕ = 1 S(a, aι)."
        },
        {
            "title": "2.4 Benchmarking Agentic Reasoning",
            "content": "As LLMs evolve into autonomous agents, benchmarks have been developed to measure tool-use and multi-step logic [6]. Project Ariadne contributes to this ecosystem by providing diagnostic for faithfulness violations detected when an agents answer remains invariant despite contradictory reasoning. This framework enables batch auditing to compute aggregate statistics such as Violation Rate Vrate and Average Faithfulness ϕ across diverse task domains ."
        },
        {
            "title": "2.2 Causal Interpretability and SCMs",
            "content": "Project Ariadne utilizes Structural Causal Models (SCMs) to move from correlational interpretability to interventional proof [7]. This methodology is grounded in the do-calculus framework proposed by Pearl [1], treating the reasoning process as series of To rigorously audit the causal dependency between an agents reasoning trace and its final output, we developed the Project Ariadne framework. As illustrated in Figure 1, the methodology treats the agents generation process as Structural Causal Model (SCM). 2 The framework proceeds in two stages. First, an original trace is generated (top row of Figure 1). Second, controlled counterfactual intervention, denoted by the do-operator, is applied to specific target step sk. This forces the agent down an alternative causal path (bottom row), resulting in counterfactual answer a. By quantitatively comparing the semantic distance between the original answer and the counterfactual answer a, we derive the Causal Faithfulness Score ϕ. As detailed in section 4, high similarity score S(a, a) resulting in low faithfulness score ϕ indicates Causal Decoupling, proving the intervention on the reasoning trace had negligible effect on the outcome."
        },
        {
            "title": "4 Mathematical Framework",
            "content": "To formalize the audit process for Agentic Reasoning, we present framework grounded in Structural Causal Models (SCMs) and counterfactual logic. This framework treats the agents reasoning process as directed computational graph and quantifies faithfulness through controlled semantic interventions."
        },
        {
            "title": "4.1 The Structural Causal Model",
            "content": "(SCM) of Reasoning We define the agentic process as an SCM denoted by = U, V, F, where: = {q, θ} represents exogenous variables: the input query and the model parameters θ."
        },
        {
            "title": "4.1.1 Stepwise Dependency",
            "content": "Each reasoning step si is generated conditioned on the query and the preceding reasoning history: si = fi(q, s<i; θ) + ϵi (1) where s<i = {s1, . . . , si1} and ϵi represents the stochastic noise inherent in LLM autoregressive sampling."
        },
        {
            "title": "4.1.2 The Answer Function",
            "content": "The final answer is the terminal node in the causal chain, determined by the query and the complete reasoning trace: = fa(q, (q); θ) (2)"
        },
        {
            "title": "4.2 Counterfactual Interventions",
            "content": "Project Ariadne evaluates causal faithfulness by performing hard interventions on the reasoning trace. Following Pearls do-calculus notation, an intervention on step is represented as do(sk = k), where is counterfactual thought generated to contradict the original reasoning. 4.2.1 The Intervened Distribution When an intervention ι is applied to step sk, we generate counterfactual answer by re-executing the agent from the point of intervention: = askι(sk)(q) = fa(q, {s1, . . . , ι(sk), . . . , n}; θ) (3) Note that subsequent steps for > are rej sampled and may deviate from the original trace due to the causal shift introduced by ι(sk). 4.2.2 Intervention Modalities = {s1, s2, . . . , sn, a} represents endogenous variables: the sequence of reasoning steps (the trace ) and the final answer A. We define an intervention operator : that maps reasoning step to its contradictory counterpart based on type τ : is set of structural equations such that each is function of its causal parents pa(v). where ιτ (sk) = fcritic(sk, τ, θcritic) (4) 3 The Original Causal Path (T ) The Agents Internal SCM (f, θ) Query (q) s1 . . . Target Step sk . . . Original Answer (a) Compare Semantic Similarity Scorer (S) Faithfulness Score ϕ = 1 S(a, a) Intervention do(sk k) Counterfactual Thought (s k) Rerun (. . . , n) Counterfactual Answer (a) Compare The Intervened Path Figure 1: The Project Ariadne Causal Audit Framework. The diagram illustrates the generation of an original reasoning trace (top) and counterfactual trace resulting from hard intervention on step sk (bottom). The semantic divergence between the resulting answers (a and a) quantifies the causal faithfulness of the reasoning process. τ {LogicFlip, FactReversal, PremiseNegation, CausalInversion}."
        },
        {
            "title": "Faithfulness",
            "content": "and"
        },
        {
            "title": "Causal Decoupling",
            "content": "The core metric of the Ariadne framework is the Causal Sensitivity Score ϕ, measuring the degree to which the terminal answer is functionally dependent on the intermediate reasoning steps. 4.3.1 Causal Sensitivity Score Let S(a, a) be semantic similarity function in the interval [0, 1]. The faithfulness score ϕ for query and intervention ι at step is defined as: ϕ(q, k, ι) = 1 S(a, a) (5) 4.3.2 Violation Detection An agent exhibits Causal Decouplinga faithfulness violationif the answer remains invariant (S 1) despite substantive contradiction in the reasoning chain. We define binary violation indicator : (q, k, ι) = 1 0 if S(a, a) > τsim and Strength(ι, sk) > λ otherwise (6) where τsim is the similarity threshold and λ is the minimum intervention strength required to expect change in a."
        },
        {
            "title": "4.4 Aggregate Metrics",
            "content": "For dataset of queries, we define the Expected Faithfulness (EF) and Violation Density (ρ): EF (θ) = EqD[1 S(a, a)] ρ = 1 (cid:88) i= (qi, ki, ιi) (7) (8)"
        },
        {
            "title": "5 Experiments and Results",
            "content": "To evaluate the causal faithfulness of state-of-the-art LLM agents, we conducted series of audits using the Project Ariadne framework. Our experiments focus on identifying Causal Decouplinginstances where the agents final answer remains invariant despite significant logical perturbations in its reasoning trace."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "We utilized dataset of 500 queries spanning three distinct categories: General Knowledge (e.g., geography, history), Scientific Reasoning (e.g., climate science, biology), and Mathematical Logic (e.g., arithmetic, symbolic logic). For each query, we extracted an initial reasoning trace and terminal answer using GPT-4o-based agent. Interventions were applied using the τf lip (Logic Flip) modality at the initial reasoning step (s0) to maximize the potential for downstream effects. Semantic similarity S(a, a) was computed using secondary Claude 3.7 Sonnet instance as the scoring judge to ensure nuanced understanding of answer equivalence."
        },
        {
            "title": "5.2 Quantitative Results: The Faith-",
            "content": "fulness Gap Our results reveal stark discrepancy between the presence of reasoning trace and its causal utility. As shown in Table 1, the majority of audited responses exhibited high semantic similarity despite contradictory reasoning. The Violation Density (ρ) was highest in Scientific Reasoning (ρ = 0.96), suggesting that models rely heavily on parametric memory for well-known facts, rendering the reasoning trace largely performative. In contrast, Mathematical Logic tasks showed significantly higher sensitivity ( ϕ = 0.329), indicating that computation-heavy tasks are more causally grounded in their intermediate steps. generative driver. The model knows the culturally or factually expected answer and effectively bypasses its own internal logic to reach it."
        },
        {
            "title": "Length",
            "content": "We further analyzed whether the length of the reasoning trace correlates with faithfulness. Our data suggests that longer traces do not necessarily lead to higher causal grounding. In fact, for General Knowledge queries, increased trace length was positively correlated with higher similarity (S), suggesting that longer chains of thought may provide more opportunities for the model to correct its path back toward its original parametric bias, regardless of the intervention. Figure 2: Distribution of Faithfulness Scores (ϕ) across task domains."
        },
        {
            "title": "5.5 Discussion: The Robustness of",
            "content": "tion"
        },
        {
            "title": "Parametric Priors",
            "content": "A qualitative analysis of the audit logs reveals the Hallucinated Explapersistent failure mode: in audit 7152213f (Global nation. For example, Warming), the agent was forced to accept an initial premise negating human-induced climate change. Despite this, the agent arrived at final answer functionally identical to its original version (S = 0.9698). This confirms that the agent utilizes the reasoning trace as post-hoc justification layer rather than Our audit of 30 distinct reasoning traces reveals significant Causal Resilience to intervention, with violation density of ρ = 0.767. Qualitative analysis of the intervened traces suggests that state-of-the-art models possess an implicit error-correction mechanism. When counterfactual logic node is introduced via Project Ariadne, the agent often identifies the contradiction in subsequent steps (s k+1) and reverts to its high-probability parametric prior. 5 Table 1: Summary of Causal Audit Results across Task Categories"
        },
        {
            "title": "Category",
            "content": "Mean Faithfulness ( ϕ) Similarity (S) Violation Rate (ρ)"
        },
        {
            "title": "General Knowledge\nScientific Reasoning\nMathematical Logic",
            "content": "0.062 0.030 0.329 0.938 0.970 0.671 92% 96% 20% This behavior, while beneficial for accuracy, is catastrophic for faithfulness. It confirms that the reasoning trace is not generative constraint but fluid narrative layer. Mathematically, the transition probability (aq, k) is nearly identical to (aq, sk), proving that the intermediate reasoning state is nonessential for terminal decision-making in factual retrieval tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "This research has formalized and evaluated the causal integrity of agentic reasoning through the Project Ariadne framework. By leveraging Structural Causal Model (SCM) approach and the principles of do-calculus, we have moved beyond surface-level textual evaluation to provide rigorous mathematical audit of LLM faithfulness. Our empirical results, specifically the high Violation Density (ρ = 0.767) across thirty distinct audits, highlight critical failure mode in current autoregressive architectures: Causal Decoupling. The data demonstrates that while Large Language Models produce sophisticated reasoning traces, these traces often function as narrative veneer or Reasoning Theater. In these instances, the terminal decision-making is driven by internal parametric priors rather than the intermediate logical steps. Project Ariadne provides the XAI community with the diagnostic tools necessary to distinguish between agents that truly derive solutions and those that merely provide post-hoc justifications. As agentic systems take on more autonomous roles in society, ensuring that their stated logic is the true cause of their actions is fundamental requirement for AI safety, reliability, and alignment."
        },
        {
            "title": "7 Future Work",
            "content": "The findings from this study open several promising avenues for enhancing the faithfulness of machine reasoning: Multi-Step and Path-Specific Interventions: While the current framework focuses on single-node perturbations (do(sk)), future iterations will explore Path-Specific Effects. By simultaneously perturbing multiple nodes in reasoning chain, we can map the logical threshold at which model is forced to abandon its parametric bias in favor of contextual logic. Causal Faithfulness as Training Objective: We propose using the Faithfulness Score (ϕ) as reward signal in Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO). By penalizing decoupled responses during the fine-tuning phase, we can potentially bridge the Faithfulness Gap. Benchmarking System 2 Architectures: key question for future research is whether increased thinking time in models utilizing test-time compute (e.g., OpenAIs o1) leads to higher causal faithfulness or simply more elaborate post-hoc justifications. Automated Saliency Mapping for Audits: To increase audit efficiency, we intend to implement Automated Saliency Detection. By using attention weights or gradient-based methods, the system can identify load-bearing steps in trace and target them for intervention automatically."
        },
        {
            "title": "References",
            "content": "[1] J. Pearl, Causality: Models, Reasoning, and Inference, Cambridge University Press, 2009. [2] A. Jacovi and Y. Goldberg, Towards Faithfully Interpretable NLP Systems, Proc. of ACL, 2020. [3] S. Wiegreffe and A. Marasovic, Explainability for Natural Language Processing: Survey, arXiv:2102.12451, 2021. [4] M. Turpin et al., Language Models Dont Always Say What They Think: Unfaithful Explanations in CoT, NeurIPS, 2023. [5] K. Meng et al., Locating and Editing Factual Associations in GPT, NeurIPS, 2022. [6] TIR-Bench: Comprehensive Benchmark for Agentic Thinking, ICLR, 2026. [7] Geiger, A., Ibeling, D., Zur, A., Chaudhary, M., Chauhan, S., Huang, J., ... & Icard, T. (2025). Causal abstraction: theoretical foundation for mechanistic interpretability. Journal of Machine Learning Research, 26(83), 1-64. [8] Pelosi, D., Cacciagrano, D., & Piangerelli, M. (2025). Explainability and interpretability in concept and data drift: systematic literature review. Algorithms, 18(7), 443."
        }
    ],
    "affiliations": []
}