{
    "paper_title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float",
    "authors": [
        "Tianyi Zhang",
        "Yang Sui",
        "Shaochen Zhong",
        "Vipin Chaudhary",
        "Xia Hu",
        "Anshumali Shrivastava"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 5 6 1 1 . 4 0 5 2 : r 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float Tianyi Zhang 12 , Yang Sui 1 , Shaochen Zhong Anshumali Shrivastava 1 , Vipin Chaudhary 3 , Xia Hu 1 , and 1 Department of Computer Science, Rice University xMAD.ai Department of Computer and Data Sciences, Case Western Reserve University"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.938.8 higher throughput in token generation. With fixed GPU memory budget, DFloat11 enables 5.313.17 longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on single node equipped with 880GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across wide range of natural language processing (NLP) tasks [38]. However, their rapidly increasing sizes create substantial obstacles to efficient deployment and inference, especially in environments with limited computational or memory resources. For example, Llama-3.1-405B [11] has 405 billion parameters in 16-bit Brain Float (BFloat16) format and requires about 810 GB of memory for full inference, exceeding the capacity of typical high-end GPU server (e.g., DGX A100/H100 with 880GB GPUs). As result, deploying this model requires multiple nodes, making it expensive and inaccessible. In Preprint. Under review. Table 1: Comparison of running LLMs on specific GPUs using BFloat16 and our proposed DFloat11 format. Note that DFloat11 produces outputs that are 100% identical to those of the original BFloat16 models. Model Successfully Run? Required Memory GPU Type Method Llama-3.1-405B-Instruct 8H100-80G Llama-3.3-70B-Instruct 1H200-141G QwQ-32B 1A6000-48G Qwen2.5-32B-Instruct 1A6000-48G DeepSeek-R1-Distill-Llama-8B 1RTX 5080-16G BFloat16 DFloat11 (Ours) BFloat16 DFloat11 (Ours) BFloat16 DFloat11 (Ours) BFloat16 DFloat11 (Ours) BFloat16 DFloat11 (Ours) 811.71 GB 551.22 GB 141.11 GB 96.14 GB 65.53 GB 45.53 GB 65.53 GB 45.53 GB 16.06 GB 11.23 GB this work, we present solution that compresses any BFloat16 model to approximately 70% of its original size while preserving 100% of its accuracy on any task. 1.1 Lossy Compression Degrades Output Quality, and Lossless Compression Lacks GPU Efficiency To address the growing model size of LLMs, quantization techniques [7, 21] are commonly employed, converting high-precision weights into lower-bit representations. This significantly reduces memory footprint and computational requirements, facilitating faster inference and deployment in resourceconstrained environments. However, quantization is inherently lossy compression technique, introducing fundamental drawback: it inevitably alters the output distribution of LLMs, thus impacting model accuracy and reliability. In contrast, lossless compression techniques preserve the exact original weights of large-scale LLMs while effectively reducing their size, ensuring the models output distribution remains identical to that of the uncompressed representation (e.g., BFloat16). However, existing lossless methods primarily focus on improving storage efficiency for LLMs, such as shrinking model checkpoints [13, 16] or optimizing performance for specialized hardware like FPGAs [40]. While these methods indeed benefit scenarios like efficient checkpoint rollbacks during training [32] or accelerated downloads from model repositories such as HuggingFace [16], their advantages typically do not extend effectively to general-purpose GPU-based LLM inference. 1.2 Dynamic-Length Float (DFloat): Data Format for Lossless GPU Inference Our key insight is that the BFloat16 (BF16) format used to store LLM weights is informationinefficient. Through an analysis of the frequency distribution of BFloat16 components (sign, exponent, and mantissa) in recent LLMs, we leverage the observation that BFloat16 exponents carry significantly less information than their allocated bit width, with an entropy of approximately 2.6 bits compared to the assigned 8 bits [13]. This highlights an opportunity for substantial lossless compression. Leveraging entropy coding techniques such as Huffman coding [17], which assign shorter codes to more frequent symbols, we can achieve roughly 30% compression without any loss in information. However, such compressed representation introduces considerable challenges for efficient GPU inference. Although theoretically appealing, efficient inference with entropy-coded weights on GPUs presents significant challenges. During inference, losslessly compressed weight matrices must be decompressed on-the-fly back into their original BFloat16 format for matrix multiplications, and this decompression step introduces critical performance bottleneck. Traditional Huffman decoding algorithms rely on sequential, bit-by-bit traversal of Huffman tree, making them inherently unsuitable for GPUs massively parallel architecture. Assigning single GPU thread for decompression, however, results in severe under-utilization of GPU resources and increased latency. Existing entropy coding-based compression methods, ranging from established CNN-focused approaches [13] to recent 2 adaptations for LLMs [16, 14, 40], have yet to deliver significant inference efficiency improvements in GPU-based scenarios relevant to most end-users. To enable efficient inference with entropy-coded weights on GPUs, we introduce novel data representation called Dynamic-Length Float (DFloat), along with GPU kernel designed for fast, online decompression of 11-bit DFloat weights (DFloat11) in massively parallel manner. The DFloat11 decompression kernel comprises three core components: ➊ Efficient decoding of entropy-coded weights using compact lookup tables (LUTs) stored in GPU shared memory (SRAM). single, monolitic LUT for decoding 32-bit Huffman codes would require roughly 4.29 billion entries (as discussed in Section 3.3.1), making it prohibitively memory-intensive. To mitigate this, we decompose it into multiple compact LUTs that fit within GPU SRAM to enable fast access. ➋ Precise and efficient identification of READ positions in encoded weights and WRITE positions for decoded weights. Since entropy-encoded weights have variable bit widths and are tightly packed, determining correct read/write offsets for each thread is challenging. We resolve this through two-phase kernel design, which employs minimal set of auxiliary variables to efficiently coordinate thread-specific input and output positions. ➌ Performing matrix decompression in batch for improved GPU resource utilization. Decompressing individual weight matrices leads to poor GPU utilization due to their relatively small size. To address this, we decompress weights at the transformer-block level, significantly enhancing throughput and reducing inference latency. We summarize our contributions as follows: We propose Dynamic-Length Float (DFloat11), data format that losslessly compresses BFloat16 weights of LLMs down to approximately 11 bits. By leveraging the information inefficiency of the BFloat16 representation, we achieve roughly 30% free model-size reduction while preserving bit-for-bit identical outputs. We introduce optimized algorithmic designs to enable efficient GPU inference with DFloat11compressed models. By carefully exploiting GPU memory and computational hierarchies, we develop hardware-aware algorithms for efficient online inference of DFloat11-compressed models. We extensively evaluate our method on popular LLMs and large reasoning models, including Llama-3.1, Qwen2.5, QwQ-32B, Mistral, Gemma-2, Gemma-3, DeepSeek-R1-Distill-Qwen, and DeepSeek-R1-Distill-Llama [11, 36, 31, 30, 27, 26, 12]. Experimental results demonstrate that our method consistently achieves about 30% compression without altering outputs. For example, our approach reduces the hardware requirements for running Llama-3.1-405B from two GPU nodes down to single node, equipped with 880GB NVIDIA A100 GPUs, without any loss in accuracy or changes in output distribution."
        },
        {
            "title": "2 Motivation: Is Lossless Compression of LLMs Worth Studying?",
            "content": "Much of the motivation behind our work lies in understanding whether lossless compression of LLMs, which preserves 100% identical output behavior compared to the original uncompressed model, is practical direction worthy of further study. Specifically, how does DFloat11, which compresses LLMs to approximately 11 bits, compare to widely used lossy quantization techniques [7, 21], where models are typically reduced to even lower bit-widths (e.g., 8-bit or 4-bit)? The answer is far more nuanced than simple Yes/No or one-size-fits-all judgment about which approach is better. For instance, existing benchmark studies like [10, 37, 18] often suggest that 8-bit (weight-only or not) quantization is relatively safe compression scheme. Although technically lossy, 8-bit models can often maintain strong task performance across range of standard benchmarks. However, we must note these benchmarks typically focus on narrow set of tasks (e.g., WikiText2 perplexity, MMLU, Commonsense Reasoning), and thus fail to offer comprehensive view of real-world LLM usage, especially from the perspective of end-users. That being said, the argument that current benchmarks fail to capture the performance gap between 8bit compressed and 16-bit uncompressed models is itself constrained by the limitations of the current benchmarking landscape, making it difficult to produce abundant supporting evidence. Nonetheless, some reports have begun to highlight such gaps. For example, human evaluations on LLM Arena1 show notable performance drop between Llama-3.1-405B-Instruct [11] and its 8-bit counterpart 1https://x.com/lmarena_ai/status/1835760196758728898 3 (Llama-3.1-405B-Instruct-FP8), particularly under coding (1293 vs. 1277) and long-query (1282 vs. 1275) tasks. Similarly, quantizing DeepSeek-R1-Distill-Llama-70B [12] from 16 bits to 8 bits results in 23.7% drop on GPQA (from 9.51% to 7.25%).2 Furthermore, reasoning, core capability of modern LLMs, appears especially sensitive to compression loss. Recent benchmark [23] reveals that quantizing DeepSeek-R1-Distill-Qwen-1.5B with 8-bit SmoothQuant [34] (for weight, attention, and KV cache) leads to an average 9.09% drop in reasoning tasks (48.82% to 44.29%) across datasets like AIME, MATH-500, GPQA-Diamond, and LiveCodeBench. We leave more evidence exploring the performance gap between 8-bit quantized and uncompressed model in Appendix D. Although the broader question: Which specific task, on which model, using which quantization technique, under what conditions, will lead to noticeable drop compared to FP16/BF16? is likely to remain open-ended simply due to the sheer amount of potential combinations. It is fair to say that lossy quantization introduces complexities that some end-users would prefer to avoid, since it creates uncontrolled variables that must be empirically stress-tested for each deployment scenario. To eliminate this burden, DFloat11 offers compelling alternative: delivering 100% identical performance to the original model, while consuming only 70% of the memory footprint with many throughput benefits, which is unique and practical offering for resource-constrained deployment settings."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present our framework for lossless compression of LLM weights. We begin with background on BFloat16, entropy coding, and the GPU computation and memory model. Next, we motivate our approach by analyzing the information efficiency of BFloat16 in representing model weights. We then introduce our proposed floating-point format, Dynamic-Length Float (DFloat11), along with its custom decompression kernel designed for efficient GPU inference. 3.1 Preliminary Brain Float (BFloat16) The weights of large language models (LLMs) are typically represented using floating-point number formats. Recent state-of-the-art LLMs predominantly employ the 16-bit Brain Floating Point format (BFloat16 or BF16), which balances numerical precision and memory efficiency. BF16 allocates its 16 bits as follows: 1 sign bit, 8 exponent bits, and 7 mantissa bits. The numerical value represented by BF16 number is computed as: (1)sign 2exponent127 (1.mantissa), (1) where mantissa is interpreted as binary fractional value. Compared to FP32, BF16 uses half the bytes to represent the same number of parameters while having similar numerical range, due to having the same number of exponent bits. In contrast to FP16, BF16 offers wider numerical range, reducing the risk of overflow during model training and inference. Entropy Coding Entropy coding is core technique in lossless data compression that leverages statistical redundancy to reduce data size. Several widely used methods fall under this category, including Huffman coding [17], arithmetic coding [20], and Asymmetric Numeral Systems (ANS) [5]. Among these, Huffman coding is one of the earliest and most widely adopted, which uses variablelength encoding to minimize the size of encoded data. It assigns shorter binary codes to more frequent symbols and longer codes to less frequent ones. The codes are decoded using prefix-free binary tree, known as Huffman tree. Due to the prefix-free property of Huffman codes, no code is prefix of any other, which ensures unique decodability of the encoded bitstream without the need for delimiters. The tree is constructed based on symbol frequencies and is provably optimal for any given frequency distribution. Huffman coding is widely used in file compression and data transmission. However, decoding in massively parallel manner remains challenging due to its inherently sequential nature. GPU Computation and Memory Paradigm GPUs are designed to perform computations in massively parallel manner. modern GPU consists of thousands of threads, which are organized into 2https://huggingface.co/RedHatAI/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8 4 Figure 1: (Left) The allocation of bits for the components of BFloat16. (Right 3) The information content, as measured by Shannon Entropy, of the components (sign, exponent, mantissa) of BFloat16 weights in various LLMs. blocks and executed on streaming multiprocessors (SMs). Each block of threads has access to small, fast on-chip memory known as shared memory or SRAM. This memory offers low latency and high throughput, making it much more efficient for frequent read and write operations compared to the global high-bandwidth memory (HBM), which is off-chip and shared across all SMs. The capacity of shared memory is limited, typically having up to 100 KB per block. In this work, we leverage the fast access characteristics of SRAM to enable efficient on-the-fly decompression of losslessly compressed weights during inference. 3.2 Technical Motivation: BFloat16 Representation is Information Inefficient To motivate the lossless compression of LLM weights, we analyze the compressibility of the BFloat16 components (sign, exponent, and mantissa) in the weights of recent large language models. Specifically, we use Shannon entropy to quantify the information content of all parameters within the linear projection matrices of an LLM. The Shannon entropy H() is defined as: H(X) = (cid:88) xX p(x) log2 p(x) (2) where is discrete random variable with support , and : [0, 1] denotes its probability mass function. We present the computed entropy values in Figure 1. As shown, the entropy of the sign and mantissa components is close to their respective bit widths, indicating limited potential for compression. In contrast, the exponent exhibits significantly lower entropy, approximately 2.6 bits versus its allocated 8 bits, suggesting substantial opportunities for lossless compression. To better understand this discrepancy, we visualize the relative frequency for all values of BFloat16 components in Figure 7 in the Appendix, and plot the ranked frequency distribution of exponent values in Figure 8 in the Appendix. As shown, the sign and mantissa values are distributed relatively uniformly across the entire value ranges. However, the distribution of exponent values is highly imbalanced: out of the 256 possible 8-bit values, only around 40 are used, with the rest never appearing in the weights. Moreover, the ranked frequencies decay rapidly towards zero. These findings explain the low entropy of the exponent and highlight the potential for significant compression. In the following sections, we leverage the low information content of BFloat16 exponents to design lossless compression framework that enables efficient GPU inference. 3.3 Dynamic-Length Float: Lossless LLM Compression Framework for Efficient GPU Inference To address the substantial information inefficiency in the BFloat16 representation of LLM weights, we propose lossless compression framework that encodes floating-point parameters using entropy coding. Specifically, we build Huffman tree based on the distribution of exponents from all BFloat16 weights within the linear projection matrices of an LLM. We then compress the exponents using Huffman coding, while preserving the original signs and mantissas. Exponents are encoded and 5 Figure 2: An illustration of our proposed format Dynamic-Length Float for compressing the BFloat16 weights of LLMs losslessly. tightly bit-packed into byte array, EncodedExponent, while the sign and mantissa are left uncompressed and stored in separate byte array PackedSignMantissa. Figure 2 illustrates Dynamic-Length Float (DFloat11 or DF11), our proposed format for compactly representing floatingpoint model parameters. The Core Challenge: Efficient GPU Inference with Compressed Weights Although DynamicLength Float enables effective lossless compression of LLMs, key challenge remains: how to perform efficient GPU inference with these compressed weights. Entropy-coded weights, due to their variable-length encoding, cannot be directly used in matrix multiplications. Therefore, each weight matrix is decompressed on-the-fly to its original BFloat16 representation when needed during inference, and immediately discarded after the matrix multiplication completes to save memory. Traditional Huffman decoding involves bit-by-bit traversal of the Huffman tree to decode each element, process that is inherently sequential and fits poorly with the massively parallel execution model of GPUs. Naively assigning single thread to decode each value results in severe underutilization of GPU resources and high latency. Overcoming this bottleneck is critical for making compressed inference practical. In the following paragraphs, we present our solution in detail: set of hardware-aware algorithmic designs tailored for low-latency decoding of entropy-coded weights in massively parallel manner. Our approach consists of three key components: 1. Decomposing monolithic prefix-free lookup table (LUT) into multiple compact LUTs that fit within GPU SRAM; 2. Introducing two-phase kernel design that leverages lightweight auxiliary variables to efficiently coordinate read/write operations of threads; 3. Performing decompression at the transformer block level to boost throughput and minimize latency. 3.3.1 Efficient Decoding with Compact LUTs Huffman codes can be efficiently decoded using lookup table (LUT)based approach [35]. We construct lookup table, denoted as LUT, of size 2L, where is the maximum bit length of any Huffman code in the codebook. Each entry in LUT maps an L-bit sequence to the decoded symbol whose Huffman code is prefix of that sequence. Due to the prefix-free property of Huffman codes, this mapping is guaranteed to be unambiguous. To decode, we read the next bits from the encoded bitstream and use them as an index into LUT to retrieve the next decoded symbol. To determine the exact number of bits consumed in the decoding process (i.e., the length of the matched Huffman code), we use second lookup table, 6 Algorithm 1 GPU kernel for decompressing DFloat11 to BFloat16 1: procedure DFLOATTOBFLOAT require: EncodedExponent, PackedSignMantissa: byte arrays LUT1, LUT2, LUT3, LUT4, CodeLengths: 8-bit unsigned integer arrays of size 256 Gaps: 5-bit unsigned integer array (one entry per thread in each block) BlockOutputPos: 32-bit unsigned integer array (one entry per block) Outputs: BFloat16 array, for storing results B, T, n, R: the number of blocks, number of threads, number of bytes in EncodedExponent processed by each thread, reserved value, respectively Divide EncodedExponent into chunks: EncodedExponent1, . . . , EncodedExponentB of size nT bytes each for all 1, . . . , (in parallel across blocks) do Load EncodedExponentb into SRAM Divide EncodedExponentb into chunks: EncodedExponentb,1, . . . , EncodedExponentb,T of size bytes each Load LUT1, LUT2, LUT3, LUT4, CodeLengths into SRAM Initialize integer arrays NumElements[1 . . . ], ThreadOutputPos[1 . . . ] with Read the next 4 bytes of EncodedExponentb,t, starting from the BitOffset-th all 0s for all 1, . . . , (in parallel across threads) do Phase 1: Each thread determines its initial output position BitOffset Gaps[bT + t] while BitOffset < 8n do bit, into Byte1...4 Exponent 1 while Exponent = do Exponent LUTi[Bytei] + 1 end while BitOffset BitOffset + CodeLengths[Exponent] NumElements[t] NumElements[t] + 1 end while Thread Synchronization Barrier ThreadOutputPos[t] BlockOutputPos[b] + (cid:80)t1 Phase 2: Writing decoded BFloat16s to the appropriate positions BitOffset Gaps[bT + t] while BitOffset < 8n do bit, into Byte1...4 Exponent 1 while Exponent = do Exponent LUTi[Bytei] + 1 i=1 NumElements[i] Read the next 4 bytes of EncodedExponentb,t, starting from the BitOffset-th end while Byte PackedSignMantissa(cid:2)ThreadOutputPos[t](cid:3) Sign Byte bitwise_and 0b10000000 Mantissa Byte bitwise_and 0b01111111 Outputs[ThreadOutputPos[t]] (Sign bitwise_left_shift 8) bitwise_or (Exponent bitwise_left_shift 7) bitwise_or Mantissa BitOffset BitOffset + CodeLengths[Exponent] ThreadOutputPos[t] ThreadOutputPos[t] + 1 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: end procedure end for end for end while 7 CodeLengths, which maps each symbol to the length of its Huffman code. We then advance the bitstream by that length and repeat the process to decode subsequent symbols. For decoding the exponents of DFloat11, we constrain the maximum code length to 32 bits for each model, allowing us to read 32 bits from the encoded bitstream in each decoding step. Empirically, we observe that most models naturally produce maximum code lengths 32, or only slightly above this threshold. For models where > 32, we enforce the length constraint by reducing the frequencies of the least common exponents to 1 and rebuilding the Huffman tree. This results in more balanced structure in the tail of the Huffman tree, assigning equal-length codes to the rarest exponents and reducing the maximum code length to 32 bits. However, when = 32, direct lookup table would require 232 4.29 billion entries, which is prohibitively memory-intensive. To address this, we propose partitioning the monolithic LUT into four disjoint and memory-efficient lookup tables. Specifically, we replace the 232-entry array with four 28-entry arrays: LUT1, LUT2, LUT3, and LUT4. Each entry in these tables consumes single byte, resulting in total memory footprint of 4 28 = 1024 bytes. The CodeLengths table adds another 28 = 256 bytes, bringing the total to 1280 bytes. This fits in GPU SRAM and enables fast access. Decoding 32-bit encoded sequence using the four compact LUTs proceeds as follows. We start by reading the first byte (8 bits) of the sequence and using it as an index into LUT1. If the corresponding Huffman code has length of 8 bits or fewer, LUT1 returns valid decoded symbol, and the decoding step is complete. Otherwise, it returns reserved value R, indicating that the code has not yet been fully resolved. In that case, we read the next byte from the sequence and use it to index into LUT2. This process continues with LUT3 and LUT4 as needed, until non-R value is returned, signaling that the Huffman code has been successfully decoded. The first non-R value returned is the decoded symbol corresponding to prefix of the 32-bit encoded sequence. While this approach significantly reduces memory usage, it introduces potential ambiguity. Specifically, if two different Huffman codes with distinct prefixes both map to the reserved value within the same LUT, the remaining bits may no longer be sufficient to unambiguously identify the correct decoded symbol in the subsequent LUTs. Fortunately, such conflicts are rare in practice and can be resolved. Due to the highly imbalanced distribution of exponent frequencies, the resulting Huffman trees are often skewed. As result, each compact LUT typically maps at most one index to the value R, thereby avoiding ambiguity. In the rare cases where ambiguity does arise, it can be resolved by slightly adjusting the frequency distribution. Specifically, we increase the frequency of the more common exponent associated with the conflicting Huffman code and rebuild the Huffman tree. This shortens the code for that exponent, eliminating the conflict and ensuring that each compact LUT maps at most one index to the reserved value R. 3.3.2 Two-Phase Kernel and Lightweight Auxiliary Variables To enable massively parallel decoding of entropy-coded exponents in DFloat11, we assign each thread fixed number of bytes from the encoded sequence to process. However, this approach introduces two key challenges: 1. Because Huffman codes have variable bit widths and are tightly packed, the starting bit position for each thread to begin decoding is unclear. 2. Except for the first thread, the index of the elements being decoded is unknown, making it difficult to determine the correct output location for storing the results. To address the first issue, we use gap array [35] to determine the starting bit position for each thread. The gap array Gaps contains one entry per thread, and each entry specifies the bit offset of the first valid Huffman code relative to the threads assigned starting byte. Since the maximum code length is 32 bits, each offset lies in the range [0, 31]. To maintain memory efficiency, we encode each entry using 5 bits. To address the second issue, the most straightforward approach is to maintain an array that stores the output position of the first decoded element for each thread. However, this incurs substantial storage overhead. Given the large size of weight matrices, each output position must be represented as 32-bit integer. Since decoding typically involves tens of thousands of threads per weight matrix, storing these positions results in significant memory consumption. This overhead undermines the compression ratio achieved by DFloat11. 8 To reduce storage overhead, we store the output position only for the first element of each thread block, rather than for every individual thread. Since each block typically contains hundreds to thousands of threads, this reduces the overhead from one 32-bit integer per thread to one per block, resulting in negligible memory overhead. To make decoding possible with block-level output positions, we adopt two-phase kernel design. In the first phase, all threads within block decode their assigned portions of the encoded sequence in parallel, but without writing any output to global memory. Instead, each thread counts how many elements it will decode. After this pass, we synchronize the threads within each block and compute the output position for every thread by calculating prefix sums over the counts, starting from the known output position of the block. In the second phase, each thread re-decodes the same portion of the encoded sequence, this time writing the decoded results to the HBM at the correct output positions. To avoid redundant memory read access to HBM during the two passes, we load the encoded exponents into SRAM. The pseudocode for the two-phase kernel is presented in Algorithm 1. 3.3.3 Transformer-Block-Level Decompression We now have complete method for decompressing entropy-coded exponents in massively parallel manner. The weights of the LLM are stored in the DFloat11 format, accompanied by lightweight auxiliary data: thread-level gap offsets and block-level output positions, which determine the read and write locations for each thread. Both the compressed weights and auxiliary variables reside entirely on the GPU during inference. When weight matrix is needed for matrix multiplication, it is decompressed on-the-fly into the original BFloat16 format. Once the matrix multiplication is complete, the BFloat16 matrix is immediately discarded to conserve GPU memory. nT In practice, decompressing single weight matrix often underutilizes GPU resources due to its relatively small size. In the DFloat11 decompression kernel, we set the number of bytes processed per thread to = 8, the number of threads per block to = 256, and the number of thread blocks to = EncodedExponent , where EncodedExponent is the total number of bytes in the encoded exponents. As the size of the DFloat11 weights increases, more thread blocks are utilized, resulting in higher decompression throughput. This effect is illustrated in Figure 6, which shows that decompression throughput improves significantly with larger matrix sizes. To capitalize on this, we propose batching the decompression of multiple matrices together to improve throughput and hide latency. Specifically, we batch the decompression of all DFloat11 weight matrices within single transformer block. Before executing any computation in transformer block, we first decompress all its associated weights. This technique significantly reduces decompression latency and improves overall inference efficiency. The latency breakdown for DFloat11-compressed Llama-3.1-8B-Instruct [11] across different batch sizes is shown in Figure 5."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we empirically evaluate the effectiveness of DF11 compression and its inference efficiency on GPUs. We compress wide selection of recent LLMs from their original BFloat16 representation into our proposed DF11 format and report the resulting compression factors. We then compare the inference performance of the DF11-compressed models running on different GPUs with that of uncompressed models. Finally, we conduct an ablation study to analyze the effects of compression. Software and Hardware We implement the DF11 decompression kernel in CUDA and C++, and integrate it into the Transformers [33] inference framework. We evaluate the inference efficiency of our losslessly compressed models against their uncompressed counterparts. For the uncompressed baseline, we use the HuggingFace Accelerate framework to support CPU offloading and multi-GPU inference. To assess the performance of the DF11 kernel across different hardware configurations, we run experiments on multiple machines with varying GPU and CPU setups. The hardware specifications for all experimental machines are provided in Table 4 in the Appendix. 9 Table 2: Lossless compression statistics for various models. Model sizes are shown before and after compression. Original Losslessly Compressed Compression Ratio Compressed Bit Width Model Llama-3.1-8B-Instruct Meta-Llama-3-8B Llama-3.3-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct QwQ-32B Mistral-Nemo-Instruct-2407 Mistral-Small-24B-Instruct-2501 Codestral-22B-v0.1 gemma-2-9b-it gemma-3-12b-it gemma-3-27b-it"
        },
        {
            "title": "24.50 GB → 17.04 GB\n47.14 GB → 32.30 GB\n44.49 GB → 30.24 GB",
            "content": "20.32 GB 14.59 GB 25.55 GB 18.21 GB 56.84 GB 39.95 GB DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B 15.23 GB 10.73 GB 16.06 GB 11.23 GB 69.98% 70.12% 68.13% 67.91% 70.44% 69.48% 69.48% 69.55% 68.52% 67.96% 71.81% 71.27% 70.28% 70.48% 69.95% 11.20 11.22 10.90 10.87 11.27 11.12 11.12 11.13 10.96 10.87 11.49 11.40 11. 11.28 11.19 Table 3: Comparison of accuracy and perplexity for the original (BF16) and losslessly compressed (DF11) models on different benchmarks. DF11 compression results in absolutely no loss in accuracy or perplexity. Accuracy Perplexity Model Llama-3.1-8B-Instruct Qwen2.5-14B-Instruct Data Type BF16 DF11 (Ours) BF16 DF11 (Ours) MMLU TruthfulQA WikiText C4 68.010 0.375 68.010 0.375 36.965 1.690 36.965 1.690 78.885 0.330 78.885 0.330 51.775 1.749 51.775 1. 8.649 8.649 6.661 6.661 21.677 21.677 23.093 23.093 4.1 Results 4.1.1 DF11 Compresses LLMs to 70% Size Table 2 presents the compression factors of DF11 for wide selection of recent LLMs. Specifically, we apply compression to all linear projection layers by converting their parameters from BF16 to DF11. The models we compress include LLaMA 3/3.1/3.3 [11], Qwen 2.5 [36], QwQ [31], Mistral Nemo/Small/Codestral [29, 30, 28], Gemma 2/3 [27, 26], and DeepSeek R1 Distilled [12]. DF11 achieves approximately 70% compression across all models, corresponding to an effective bit width of around 11 bits. 4.1.2 Accuracy and Perplexity Evaluations Confirm DF11 Is Perfectly Lossless We verify the lossless property of DF11 compression through series of accuracy and perplexity evaluations on standard benchmarks. Evaluations are conducted using lm_evaluation_harness [9], reporting accuracy on MMLU [15] and TruthfulQA [22], and word-level perplexity on WikiText [24] and C4 [25]. The results are shown in Table 3. As demonstrated, the compressed models achieve identical accuracy and perplexity to their original BF16 counterparts. To further validate the losslessness property, we compare the decompressed BF16 weight matrices from DF11 against the originals for each model in Table 2, confirming exact bit-level equivalence. 10 Figure 3: Comparison of average latency and throughput for token decoding between the original (BF16) models and their losslessly compressed (DF11) counterparts. Portions of the BF16 models are offloaded to the CPU due to GPU memory constraints. Figure 4: Comparison of GPU memory consumption between the original (BF16) models and their losslessly compressed (DF11) counterparts. The DF11 models support 5.3313.17 longer context lengths by allowing more GPU memory to be used for storing the KV cache. O.O.M. means out of memory. 4.1.3 DF11 Outperforms CPU Offloading in Inference Efficiency We compare the inference efficiency of DF11 and BF16 models across different hardware platforms. The uncompressed BF16 models exceed the memory limits of single GPU, whereas the losslessly compressed DF11 models fit within those limits. For the BF16 models, we retain most of the model and the computation in the GPU while offloading some components and their associated computations to the CPU. To measure latency and throughput, we first perform warm-up run by processing 100 tokens. In the actual evaluation, we decode 100 tokens starting from an empty prompt, using varying batch sizes. Each configuration is run five times, and we report the average latency and throughput across these runs. The results across different models, GPUs, and batch sizes are presented in Figure 3. As shown, DF11 models consistently outperform the BF16 models with CPU offloading, achieving 11 Figure 5: Comparison of latency breakdown for DFloat11 and BFloat16 Llama-3.1-8B-Instruct during GPU inference for different token batch sizes, using one A100-40GB GPU. 1.8538.83 lower latency or higher throughput. For comparison using multiple GPUs, Figure 9 in the Appendix shows the performance of DF11 models running on single GPU versus BF16 models running on two GPUs. 4.1.4 Memory Savings from DF11 Enable Longer Generation Lengths The memory savings provided by DF11 compression not only reduce the number of GPUs required for inference but also enable longer generation lengths. During inference, the KV cache grows lienarly with the number of decoded tokens and quickly becomes bottleneck for GPU memory. In Figure 4, we show the GPU memory consumption of DF11 and BF16 models during inference with batch size of 1, as the number of decoded tokens increases. As shown, DF11 compression significantly extends the token generation length, allowing 5.3313.17 more tokens to be decoded before reaching the GPU memory limit compared to BF16 models. 4.2 Ablation Study 4.2.1 Latency Breakdown Shows Decompression Overhead Is Amortized at Larger Batch Sizes We compare the latency breakdown of Llama-3.1-8B-Instruct in BF16 and DF11 formats using varying token batch sizes on single A100-40GB GPU. For each configuration, we measure the latency of each component during the forward pass over 10 runs and report the average in Figure 5. Compared to the original model, the DF11-compressed version introduces additional latency due to the decompression of transformer blocks and the language modeling head. This decompression adds constant overhead that is independent of the token batch size. As result, increasing the batch size amortizes the decompression cost, leading to substantially smaller gap in overall inference time. 4.2.2 DF11 Decompression Is Significantly Faster Than CPU-to-GPU Transfer and ANS We compare the latency and throughput of the DF11 decompression kernel against two baselines: CPU-to-GPU transfer and ANS (Asymmetric Numeral System) decompression [5] from the NVIDIA nvCOMP library [1], using weight matrices of varying sizes. The motivation is to evaluate two strategies for memory-constrained inference: offloading weight matrices to CPU memory and trans12 Figure 6: Throughput (top two) and latency (bottom two) comparisons between transferring BFloat16 matrices from CPU to GPU and decompressing the same matrices using the NVIDIA nvCOMP ANS library and our proposed DFloat11 kernel, across matrix sizes and GPU types. ferring them to the GPU as needed, or storing compressed matrices on the GPU and decompressing them on demand. For this study, we use the weight matrix of the language modeling head from Llama-3.1-8B-Instruct and slice it into varying sizes. The results are shown in Figure 6. As illustrated, DF11 decompression is significantly more efficient than both baselines, achieving up to 24.87 higher throughput or lower latency than CPU-to-GPU transfer, and up to 15.12 faster than NVIDIA nvCOMP decompression. In addition to speed, DF11 also provides better compression ratio than nvCOMP, reducing model size by approximately 70% compared to around 78% for nvCOMP. Notably, DF11 decompression throughput improves with larger matrix sizes due to increased GPU thread utilization."
        },
        {
            "title": "5 Related Works",
            "content": "Data Formats for Model Weights LLM weights are typically stored in compact floating-point formats such as FP16 or BF16. FP16 allocates 1 sign bit, 5 exponent bits, and 10 mantissa bits, whereas BF16 uses 1 sign bit, 8 exponent bits, and 7 mantissa bits. Compared to FP16, BF16 offers wider dynamic range at the cost of precision, which improves numerical stability and mitigates overflow issues during training [8, 19]. Compressed data formats typically aim for lower bit-widths. For example, FP8which comes in both E4M3 (4 exponent bits, 3 mantissa bits, plus 1 sign bit) and E5M2 configurationshas seen reasonable adoption in LLM training and development. Integer formats like INT8 have also been well explored, as in LLM.int8() [2] and its following works. Formats with stronger emphasis on efficiency, such as FP4, INT4, NF4 [3], and AF4 [39], use only 4 bits. In this work, we primarily focus on formats with 8 bits, as benchmark literature [37, 10, 23] often suggests that 8-bit quantization results in negligible performance dropthough we show in Section 2 that this claim is likely skewed due to evaluation selectiveness and benchmark limitations. Lossless Model Compression While lossy model compression techniques such as pruning and quantization [6, 21, 7] have received widespread attention, lossless model compression remains relatively underexplored area. Upon careful investigation, we identified roughly four prior works that have made meaningful efforts in this space. Deep Compression [13] is foundational work, applying Huffman coding [17] to quantized CNN models and achieving an additional 22% compression gain for model checkpoints. ZipNN [16] extended this idea to language models, comparing its results to 13 classic lossless compression tools such as zlib [4] and zstd3 and demonstrated superior compression gains. However, this line of work is limited in that its efficiency gains only apply to storage (reducing the size of model checkpoints) but offer no benefits during inference. While such storage savings are meaningful in large-scale training settingswhere frequent snapshotting and checkpoint rollbacks are needed [32]they have limited impact for everyday LLM end-users. Model downloading is typically one-time cost, so even if model checkpoint is compressed by 50%, it only cuts the download time at most by half, presumably over the models entire lifecycle of deployment. Furthermore, checkpoints are usually stored on disk, where terabytes of capacity are easily available, making up much looser constraint compared to GPU HBM (High Bandwidth Memory); one of the main resource constraints during inference. We argue that lossless compression technique would be substantially more impactful if it could deliver efficiency gains during inferenceparticularly on GPU-based systems, which is the default setup for LLM serving. In this context, NeuZip [14] is the only prior work we identify that supports GPU inference. NeuZip applies entropy encoding with layer-wise decompression to maintain reduced memory footprint throughout serving. However, it is built on NVIDIAs nvCOMP: highspeed data compression and decompression library optimized for NVIDIA GPUs.4 Unfortunately, nvCOMP is no longer open-source (only binary executables are available), which hinders future research. Moreover, we empirically find that nvCOMPs inference throughput and latency are significantly worse than our proposed DFloat11 kernel, resulting in pipeline that trades memory efficiency for substantial inference overhead (see Figure 6). Another work referencing NeuZip is Huff-LLM [40], which also aims to reduce memory costs while maintaining efficient inference. However, its contributions are specific to FPGA-like architectures and do not apply to GPUs. To the best of our knowledge, the DFloat data format we presented (and its respective kernel support in DFloat11) shall serve as the only GPU-inference-friendly data format with lossless compression benefits."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Dynamic-Length Float (DFloat) as lossless compression data format for LLM weights. To the best of our knowledge, DFloat is the only data format capable of reducing memory footprint while remaining compatible with efficient GPU inference. Specifically, we evaluate several popular LLMs using the 11-bit DFloat format (DF11), alongside custom-developed GPU kernels tailored for this format. Empirical results suggest that DF11-based compression significantly lowers hardware demands for serving LLMs, while introducing reasonable processing overhead in most practical use cases."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Oscar Wu for insightful discussions and xMAD.ai for providing computational resources."
        },
        {
            "title": "References",
            "content": "[1] NVIDIA Corporation. nvCOMP: Gpu-accelerated compression and decompression library. https://developer.nvidia.com/nvcomp, 2025. Accessed: April 11, 2025. [2] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. [3] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:10088 10115, 2023. [4] P. Deutsch and J.-L. Gailly. Rfc1950: Zlib compressed data format specification version 3.3, 1996. 3https://github.com/facebook/zstd 4https://developer.nvidia.com/nvcomp 14 [5] Jarek Duda. Asymmetric numeral systems: entropy coding combining speed of huffman coding with compression rate of arithmetic coding. arXiv preprint arXiv:1311.2540, 2013. [6] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. [7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [8] Kazuki Fujii, Taishi Nakamura, and Rio Yokota. Balancing speed and stability: The trade-offs of fp8 vs. bf16 training in llms. arXiv preprint arXiv:2411.08719, 2024. [9] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [10] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen Zhang, Xianglong Liu, and Dacheng Tao. Llmc: Benchmarking large language model quantization with versatile compression toolkit. arXiv preprint arXiv:2405.06001, 2024. [11] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [14] Yongchang Hao, Yanshuai Cao, and Lili Mou. Neuzip: Memory-efficient training and inference with dynamic compression of neural networks. arXiv preprint arXiv:2410.20650, 2024. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations. [16] Moshik Hershcovitch, Andrew Wood, Leshem Choshen, Guy Girmonsky, Roy Leibovitz, Ilias Ennmouri, Michal Malka, Peter Chin, Swaminathan Sundararaman, and Danny Harnik. Zipnn: Lossless compression for ai models. arXiv preprint arXiv:2411.05239, 2024. [17] David Huffman. method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):10981101, 1952. [18] Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and Deyi Xiong. comprehensive evaluation of quantization strategies for large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1218612215, 2024. [19] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. [20] G. G. Langdon. An introduction to arithmetic coding. IBM Journal of Research and Development, 28(2):135149, 1984. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [22] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, 2022. [23] Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, and Lu Hou. Quantization hurts reasoning? an empirical study on quantized reasoning models. arXiv preprint arXiv:2504.04823, 2025. 15 [24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [26] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [27] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [28] Mistral AI Team. Codestral. https://mistral.ai/news/codestral, May 2024. [29] Mistral AI Team. Mistral NeMo. https://mistral.ai/news/mistral-nemo, July 2024. [30] Mistral AI Team. Mistral Small 3. https://mistral.ai/news/mistral-small-3, January 2025. [31] Qwen Team. QwQ: Reflect Deeply on the Boundaries of the Unknown. https://qwenlm. github.io/blog/qwq-32b-preview/, November 2024. [32] Zhuang Wang, Zhen Jia, Shuai Zhang, Zhen Zhang, Mason Fu, T. S. Eugene Ng, and Yida Wang. Gemini: Fast failure recovery in distributed training with in-memory checkpoints. 2023. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 3845, 2020. [34] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. [35] Naoya Yamamoto, Koji Nakano, Yasuaki Ito, Daisuke Takafuji, Akihiko Kasagi, and Tsuguchika Tabaru. Huffman coding with gap arrays for gpu acceleration. In Proceedings of the 49th International Conference on Parallel Processing, pages 111, 2020. [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [37] Ge Yang, Changyi He, Jinyang Guo, Jianyu Wu, Yifu Ding, Aishan Liu, Haotong Qin, Pengliang Ji, and Xianglong Liu. LLMCBench: Benchmarking large language model compression for efficient deployment. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [38] Jingfeng Yang, Haongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: survey on chatgpt and beyond. 2024. [39] Davis Yoshida. Nf4 isnt information theoretically optimal (and thats good). arXiv preprint arXiv:2306.06965, 2023. [40] Patrick Yubeaton, Tareq Mahmoud, Shehab Naga, Pooria Taheri, Tianhua Xia, Arun George, Yasmein Khalil, Sai Qian Zhang, Siddharth Joshi, Chinmay Hegde, et al. Huff-llm: End-to-end lossless compression for efficient llm inference. arXiv preprint arXiv:2502.00922, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Frequency Distribution of BFloat16 Values Figure 7: Relative frequency distribution of sign, exponent, and mantissa values in the BFloat16 weights of all linear projection layers across various LLMs. Figure 8: Distribution of BFloat16 exponent values across various models. The frequency of exponent values (shown in log scale) decays rapidly with exponent rank."
        },
        {
            "title": "B Hardware for Experiments",
            "content": "Table 4: System specifications of servers used for experiments. GPU Memory CPU GPU CPU Memory 15360MiB Server 1 NVIDIA Tesla T4 24564MiB Server 2 NVIDIA RTX A5000 Server 3 NVIDIA A100 40960MiB Server 4 NVIDIA Quadro RTX 8000 49152MiB Intel Xeon Platinum 8259CL 187GB 504GB AMD EPYC 7513 32-Core 1.48TB AMD EPYC 7742 64-Core 1.48TB AMD EPYC 7742 64-Core Figure 9: Comparison of average latency and throughput for token decoding between the original (BF16) models and their losslessly compressed (DF11) counterparts. The DF11 models are run on single GPU, while the BF16 models require two GPUs due to memory constraints. GPU Inference Efficiency Comparison: BF16 vs. DF11 We present the GPU inference efficiency of BF16 and DF11 models in Figure 9, for various models, GPUs, and batch sizes. Due to GPU memory constraints, the BF16 models are run with two GPUs while the DF11 models are run with single GPU."
        },
        {
            "title": "D Impact of Lossy Quantization",
            "content": "Table 5: INT8 quantization error on different tasks. Math denotes MATH Hard with 2 shots. GPQA CoT is with 2 shots. denotes the error gap via INT8 quantization. Model Data Type Math GPQA CoT Llama-3.1-8B-Instruct BF16 INT8 23.92 19.92 4. 15.18 14.06 1."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Rice University",
        "Department of Computer and Data Sciences, Case Western Reserve University"
    ]
}