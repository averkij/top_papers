{
    "paper_title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
    "authors": [
        "Wenxuan Guo",
        "Xiuwei Xu",
        "Ziwei Wang",
        "Jianjiang Feng",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code is available at \\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}."
        },
        {
            "title": "Start",
            "content": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding Wenxuan Guo1 Xiuwei Xu1 1Tsinghua University Ziwei Wang2 Jianjiang Feng1 2Nanyang Technological University Jie Zhou Jiwen Lu1 {gwx22,xxw21}@mails.tsinghua.edu.cn ziwei.wang@ntu.edu.sg {jfeng,jzhou,lujiwen}@tsinghua.edu.cn 5 2 0 2 4 ] . [ 1 2 9 3 0 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code is available at https://github.com/GWxuan/TSP3D. 1. Introduction Incorporating multi-modal information to guide 3D visual perception is promising direction. In these years, 3D visual grounding (3DVG), also known as 3D instance referencing, has been paid increasing attention as fundamental multi-modal 3D perception task. The aim of 3DVG is *Equal contribution. Corresponding author. Figure 1. Comparison of 3DVG methods on ScanRefer dataset [3]. Our TSP3D surpasses existing methods in both accuracy and inference speed, achieving the first efficient 3DVG framework. to locate an object in the scene with free-form query description. 3DVG is challenging since it requires understanding of both 3D scene and language description. Recently, with the development of 3D scene perception and visionlanguage models, 3DVG methods have shown remarkable progress [16, 22]. However, with 3DVG being widely applied in fields like robotics and AR / VR where inference speed is the main bottleneck, how to construct efficient realtime 3DVG model remains challenging problem. Since the output format of 3DVG is similar with 3D object detection, early 3DVG methods [3, 14, 38, 39] usually adopt two-stage framework, which first conducts detection to locate all objects in the scene, and then selects the target object by incorporating text information. As there are many similarities between 3D object detection and 3DVG (e.g. both of them need to extract the representation of the 3D scene), there will be much redundant feature computation during the independent adoption of the two models. As result, two-stage methods are usually hard to handle real-time tasks. To solve this problem, single-stage methods [22, 35] are presented, which generates the bounding box of the target directly from point clouds. This integrated design is more compact and efficient. However, current single-stage 3DVG methods mainly build on pointbased architecture [25], where the feature extraction contains time-consuming operations like furthest point sampling and kNN. They also need to aggressively downsample the point features to reduce computational cost, which might hurt the geometric information of small and thin objects [37]. Due to these reasons, current single-stage methods are still far from real-time (< 6 FPS) and their performance is inferior to two-stage methods, as shown in Fig. 1. In this paper, we propose new single-stage framework for 3DVG based on text-guided sparse voxel pruning, namely TSP3D. Inspired by state-of-the-art 3D object detection methods [29, 37] which achieves both leading accuracy and speed with multi-level sparse convolutional architecture, we build the first sparse single-stage 3DVG network. However, different from 3D object detection, in 3DVG the 3D scene representation should be deeply interacted with text features. Since the count of voxels is very large in sparse convolution-based architecture, deep multi-modal interaction like cross-attention becomes infeasible due to unaffordable computational cost. To this end, we propose text-guided pruning (TGP), which first utilize text information to jointly sparsify the 3D scene representation and enhance the voxel and text features. To mitigate the affect of pruning on delicate geometric information, we further present completion-based addition (CBA) to adaptively fix the over-pruned region with negligible computational overhead. Specifically, TGP prunes the voxel features according to the object distribution. It gradually removes background features and features of irrelevant objects, which generates text-aware voxel features around the target object for accurate bounding box prediction. Since pruning may mistakenly remove the representation of target object, CBA utilizes text features to query small set of voxel features from the complete backbone features, followed by pruned-aware addition to fix the over-pruned region. We conduct extensive experiments on the popular ScanRefer [3] and ReferIt3D [2] datasets. Compared with previous single-stage methods, TSP3D achieves top inference speed and surpasses previous fastest single-stage method by 100% FPS. TSP3D also achieves state-of-theart accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. To summarize, our main contributions are as follows: To the best of our knowledge, this is the first work exploring sparse convolutional architecture for efficient 3DVG. To enable efficient feature extraction, we propose textguided pruning and completion-based addition to sparsify sparse voxels and adaptively fuse multi-level features. We conduct extensive experiments, and TSP3D outperforms existing methods in both accuracy and speed, demonstrating the superiority of the proposed framework. 2. Related Work 2.1. 3D Visual Grounding 3D visual grounding aims to locate target object within 3D scene based on natural language descriptions [19]. Existing methods are typically categorized into two-stage and single-stage approaches. Two-stage methods follow detect-then-match paradigm. In the first stage, they independently extract features from the language query using pre-trained language models [7, 9, 24] and predict candidate 3D objects using pre-trained 3D detectors [21, 26] or segmenters [4, 17, 32]. In the second stage, they focus on aligning the vision and text features to identify the target object. Techniques for feature fusion include attention mechanisms with Transformers [13, 40], contrastive learning [1], and graph-based matching [10, 14, 39]. In contrast, single-stage methods integrate object detection and feature extraction, allowing for direct identification of the target object. Methods in this category include guiding keypoint selection using textual features [22], and measuring similarity between words and objects inspired by 2D imagelanguage pre-trained models like GLIP [18], as in BUTDDETR [16]. And methods like EDA [35] and G3-LQ [34] advance single-stage 3D visual grounding by enhancing multimodal feature discriminability through explicit textdecoupling, dense alignment, and semantic-geometric modeling. MCLN [27] uses the 3D referring expression segmentation task to assist 3DVG in improving performance. However, existing two-stage and single-stage methods generally have high computational costs, hindering realtime applications. Our work aims to address these efficiency challenges by proposing an efficient single-stage method with multi-level sparse convolutional architecture. 2.2. Multi-Level Convolutional Architectures Recently, sparse convolutional architecture has achieved great success in the field of 3D object detection. Built on the voxel-based representation [5, 8, 33] and sparse convolution operation [6, 11, 36], this kind of methods show great efficiency and accuracy when processing scene-level data. GSDN [12] first adopts multi-level sparse convolution with generative feature upsampling in 3D object detection. FCAF3D [29] simplifies the multi-level architecture with anchor-free design, achieving leading accuracy and speed. TR3D [30] further accelerates FCAF3D by removing unnecessary layers and introducing category-aware proposal assignment method. Moreover, DSPDet3D [37] introduces the multi-level architecture to 3D small object detection. Our proposed method draws inspiration from these approaches, utilizing sparse multi-level architecture with sparse convolutions and an anchor-free design. This allows for efficient processing of 3D data, enabling real-time performance in 3D visual grounding tasks. 3. Method In this section, we describe our TSP3D for efficient singlestage 3DVG. We first analyze existing pipelines to identify current challenges and motivate our approach (Sec. 3.1). We then introduce the text-guided pruning, which leverages text features to guide feature pruning (Sec. 3.2). To address the potential risk of pruning key information, we propose the completion-based addition for multi-level feature fusion (Sec. 3.3). Finally, we detail the training loss (Sec. 3.4). 3.1. Architecture Analysis for 3DVG Top-performance 3DVG methods [31, 34, 35], are mainly two-stage, which is serial combination of 3D object detection and 3D object grounding. This separate calls of two approaches result in redundant feature extraction and complex pipeline, thus making the two-stage methods less efficient. To demonstrate the efficiency of existing methods, we conduct comparison of accuracy and speed among several representative methods on ScanRefer [3], as shown in Fig. 1. It can be seen that two-stage methods struggle in speed (< 3 FPS) due to the additional detection stage. Since 3D visual grounding is usually adopted in practical scenarios that require real-time inference under limited resources, such as embodied robots and VR/AR, the low speed of twostage methods make them less practical. On the other side, single-stage methods [22], which directly predicts refered bounding box from the observed 3D scene, are more suitable choices due to their streamlined processes. In Fig. 1, it can be observed that single-stage methods are significantly more efficient than their two-stage counterparts. However, existing single-stage methods are mainly built on point-based backbone [25], where the scene representation is extracted with time-consuming operations like furthest point sampling and set abstraction. They also employ large transformer decoder to fuse text and 3D features for several iterations. Therefore, the inference speed of current single-stage methods is still far from real-time (< 6 FPS). The inference speed of specific components in different frameworks is analyzed and discussed in detail in the supplementary material. Inspired by the success of multi-level sparse convolutional architecture in 3D object detection [30], which achieves both leading accuracy and speed, we propose to build the first multi-level convolutional single-stage 3DVG pipeline. TSP3D-B. Here we propose baseline framework based on sparse convolution, namely TSP3D-B. Following the simple and effective multi-level architecture of FCAF3D [29], TSP3D-B utilizes 3 levels of sparse convolutional blocks for scene representation extraction and bounding box prediction, as shown in Fig. 2 (a). Specifically, the input pointclouds RN 6 with 6-dim features (3D position and RGB) are first voxelized and then fed into three sequential MinkResBlocks [6], which generates three levels of voxel features Vl (l = 1, 2, 3). With the increase of l, the spatial resolution of Vl decreases and the context information increases. Concurrently, the free-form text with words is encoded by the pre-trained RoBERTa [20] and produce the vanilla text tokens Rld. With the extracted 3D and text representations, we iteratively upsample V3 and fuse it with to generate high-resolution and text-aware scene representation: Ul = l + Vl, = GeSpConv(U l+1) (1) (2) l+1 = Concat(Ul+1, ) where U3 = V3, GeSpConv means generative sparse convolution [12] with stride 2, which upsamples the voxel features and expands their spatial locations for better bounding box prediction. Concat is voxel-wise feature concatenation by duplicating . The final upsampled feature map U1 is concatenated with and fed into convolutional head to predict the objectness scores and regress the 3D bounding box. We select the box with highest objectness score as the grounding result. As shown in Fig. 1, TSP3D-B achieves an inference speed of 14.58 FPS, which is significantly faster than previous single-stage methods and demonstrates great potential for real-time 3DVG. 3.2. Text-guided Pruning Though efficient, TSP3D-B exhibits poor performance due to the inadequate interaction between 3D scene representation and text features. Motivated by previous 3DVG methods [16], simple solution is to replace Concat with crossmodal attention to process voxel and text features, as shown in Fig. 2 (b). However, different from point-based architectures where the scene representation is usually aggressively downsampled, the number of voxels in multi-level convolutional framework is very large1. In practical implementation, we find that the voxels expand almost exponentially with each upsampling layer, leading to substantial computational burden for the self-attention and cross-attention of scene features. To address this issue, we introduce textguided pruning (TGP) to construct TSP3D, as illustrated in Fig. 2 (c). The core idea of TGP is to reduce feature amount by pruning redundant voxels and guide the network to gradually focus on the final target based on textual features. Overall Architecture. TGP can be regarded as modified version of cross-modal attention, which reduces the number of voxels before attention operation, thereby lowering computational cost. To minimize the affect of pruning on the final prediction, we propose to prune the scene representation gradually. At higher level where the number of 1Compared to point-based architectures, sparse convolutional framework provides higher resolution and more detailed scene representations, while also offering advantages in inference speed. For detailed statistics, please refer to the supplementary material. Figure 2. Illustration of TSP3D. TSP3D bulids on multi-level sparse convolutional architecture. It iteratively upsamples the voxel features with text-guided pruning (TGP), and fuses multi-level features via completion-based addition (CBA). (a) to (d) on the right side illustrate various options for feature upsampling. (a) refers to simple concatenation with text features, which is fast but less accurate. (b) refers to feature interaction through cross-modal attention mechanisms, which is constrained by the large number of voxels. (c) represents our proposed TGP, which first prunes voxel features under textual guidance and thus enables efficient interaction between voxel and text features. (d) shows simplified version of TGP that removes farthest point sampling and interpolation, combines multi-modal feature interactions into whole and moves it before pruning. voxels is not too large yet, TGP prunes less voxels. While at lower level where the number of voxels is significantly increased by upsampling operation, TGP prunes the voxel features more aggressively. The multi-level architecture of TSP3D consists of three levels and includes two feature upsampling operations. Therefore, we correspondingly configure two TGPs with different functions, which are referred as scene-level TGP (level 3 to 2) and target-level TGP (level 2 to 1) respectively. Scene-level TGP aims to distinguish between objects and the background, specifically pruning the voxels on background. Target-level TGP focuses on regions mentioned in the text, intending to preserve the target object and referential objects while removing other regions. Details of TGP. Since the pruning is relevant to the description, we need to make the voxel features text-aware to predict proper pruning mask. To reduce the computational cost, we perform farthest point sampling (FPS) on the voxel features to reduce their size while preserving the basic distribution of the scene. Next, we utilize cross-attention to interact with the text features and employ simple MLP to predict the probability distribution ˆM for retaining each voxel. To prune the features Ul, we binarize and interpolate the ˆM to obtain the pruned mask. This process can be expressed as: = Ul Θ(I( ˆM , Ul) σ) ˆM = MLP(CrossAtt(FPS(Ul), SelfAtt(T ))) (3) (4) where is the pruned features, Θ is Heaviside step funcl tion, is matrix dot product, σ is the pruning threshold, and represents linear interpolation based on the positions specified by Ul. After pruning, the scale of the scene features is significantly reduced, enabling internal feature interactions based on self-attention. Subsequently, we utilize self-attention and cross-attention to perceive the relative relationships among objects within the scene and to fuse multimodal features, resulting in updated features . Finally, through generative sparse convolutions, we obtain l1. Supervision for Pruning. The binary supervision mask sce for scene-level TGP is generated based on the centers of all objects in the scene, and the mask tar for targetlevel TGP is based on the target and relevant objects mentioned in the descriptions: sce = (cid:91) i=1 M(Oi), tar = M(Otar) (cid:91) j=1 M(Orel ) (5) where {Oi1 } indicates all objects in the scene. Otar and Orel refer to target and relevant objects respectively. M(O) represents the mask generated from the center of object O. It generates cube centered at the center of to construct the supervision mask , where locations inside the cube is set to 1 while others set to 0. Simplification. Although the above mentioned method can effectively prune voxel features to reduce the computational cost of cross-modal attention, there are some inefficient operations in the pipeline: (1) FPS is time-consuming, especially for large scenes; (2) there are two times of interactions between voxel features and text features, the first is to guide pruning and the second is to enhance the representation, which is bit redundant. We also empirically observe that the number of voxels is not large in level 3. tures are directly added. For voxel features outside the intersection of and Vl which lack corresponding features in the other map, the missing voxel features are interpolated before addition. Due to pruning process, is sparser than Vl. In this way, full addition can fix almost all the pruned region. But this operation is computationally heavy and make the scene representation fail to focus on relevant objects, which deviates the core idea of TGP. (2) Pruning-aware Addition. The addition is constrained to the locations of but not in Vl, interpolation from G is applied to complete the missing locations in Vl. It restricts the addition operation to the shape of the pruned features, potentially leading to an over-reliance on the results of the pruning process. If important regions are over-pruned, the network may struggle to detect targets with severely damaged geometric information. . For voxel in Considering the unavoidable risk of pruning the query target, we introduce the completion-based addition (CBA). CBA is designed to address the limitations of full and pruning-aware additions. It offers more targeted and efficient way to integrating multi-level features, ensuring the preservation of essential details while keeping the additional computational overhead negligible. Details of CBA. We first enhance the backbone features Vl with the text features through cross-attention, obtaining . Then MLP is adopted to predict the probability distribution of target for region selection: tar = Θ(MLP(V ) τ ) (6) where Θ is the step function, and τ is the threshold determining voxel relevance. tar is binary mask indicating potential regions of the mentioned target. Then, comparison of tar with Ul identifies missing voxels. The missing mask mis is derived as follows: = tar mis ( C(U l , Vl)) (7) where C(A, B) denotes the generation of binary mask for based on the shape of B. Specifically, for positions in B, if there are corresponding voxel features in A, the mask for that position is set to 1. Otherwise it is set to 0. Missed voxel features in are interl polated from , filling in gaps identified by the missing mask. The completed feature map cpl + I(U that correspond to mis is computed by: mis , mis cpl = (8) ) where represents linear interpolation on the feature map based on the positions specified in the mask. Finally, the original upsampled features are combined with the backbone features according to the pruning-aware addition, and merged with the completion features to yield updated Ul: Ul = Concat(U Vl, cpl ) (9) where denotes the pruning-aware addition, and Concat means concatenation of voxel features. Figure 3. Illustration of completion-based addition. The upper figure (b) illustrates an example of over-pruning on the target. The lower figure (c) shows the completed features predicted by CBA. To this end, we propose simplified version of TGP, as shown in Fig. 2 (d). We remove the FPS and merge the two multi-modal interactions into one. We also move the merged interaction operation before pruning. In this way, voxel features and text features are first deeply interacted for both feature enhancement and pruning. Because in level 3 the number of voxels is small and in level 2 / 1 the voxels are already pruned, the computational cost of self-attention and cross-attention is always kept at relatively low level. Effectiveness of TGP. After pruning, the voxel count of U1 is reduced to nearly 7% of its original size without TGP, while the 3DVG performance is significantly boosted. TGP serves multiple functions, including: (1) facilitating the interaction of multi-modal features through cross-attention, (2) reducing the feature amount (number of voxels) through pruning, and (3) gradually guiding the network to focus on the mentioned target based on text features. 3.3. Completion-based Addition During the pruning process, some targets may be mistakenly removed, especially for small or narrow objects, as shown in Fig. 3 (b). Therefore, the addition operation between the upsampled pruned features and backbone features Vl described in Equation (1) play an important role to mitigate the affect of over-pruning. There are two alternative addition operation: (1) Full , feaAddition. For the intersecting regions of Vl and Table 1. Comparison of methods on the ScanRefer dataset evaluated at IoU thresholds of 0.25 and 0.5. TSP3D achieves stateof-the-art accuracy even compared with two-stage methods, with +1.13 lead on Acc@0.5. Notably, we are the first to comprehensively evaluate inference speed for 3DVG methods. The inference speeds of other methods are obtained through our reproduction. Method Venue Input Accuracy 0. 0.25 Inference Speed (FPS) Two-Stage Model ScanRefer [3] TGNN [14] InstanceRefer [39] SAT [38] FFL-3DOG [10] 3D-SPS [22] BUTD-DETR [16] EDA [35] 3D-VisTA [41] VPP-Net [31] G3-LQ [34] MCLN [27] ECCV20 AAAI21 ICCV21 ICCV21 ICCV21 CVPR22 ECCV22 CVPR23 ICCV23 CVPR24 CVPR24 ECCV24 Single-stage Model 3D-SPS [22] BUTD-DETR [16] EDA [35] G3-LQ [34] MCLN [27] TSP3D (Ours) CVPR22 ECCV22 CVPR23 CVPR24 ECCV24 3.4. Train Loss 3D+2D 3D 3D 3D+2D 3D 3D+2D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 41.19 37.37 40.23 44.54 41.33 48.82 50.42 54.59 45.90 55.65 56.90 57. 47.65 49.76 53.83 55.95 54.30 56.45 27.40 29.70 30.15 30.14 34.01 36.98 38.60 42.26 41.50 43.29 45.58 45.53 36.43 37.05 41.70 44.72 42.64 46.71 6.72 3.19 2.33 4.34 Not released 3.17 3.33 3.34 2.03 Not released Not released 3.17 5.38 5.91 5.98 Not released 5.45 12.43 The loss is composed of several components: pruning loss for TGP, completion loss for CBA, and objectness loss as well as bounding box regression loss for the head. Pruning loss, completion loss and objectness loss employ the focal loss to handle class imbalance. Supervision for completion and classification losses are the same, which sets voxels near the target object center as positives while leaving others as negatives. For bounding box regression, we use the Distance-IoU (DIoU) loss. The total loss function is computetd as the sum of these individual losses: Ltotal = λ1Lpruning + λ2Lcom + λ3Lclass + λ4Lbbox where λ1, λ2, λ3 and λ4 are the weights of different parts. 4. Experiments 4.1. Datasets We maintain the same experimental settings with previous works, employing ScanRefer [3] and SR3D/NR3D [2] as datasets. ScanRefer: Built on the ScanNet framework, ScanRefer includes 51,583 descriptions across scenes. Evaluation metrics focus on Acc@mIoU. ReferIt3D: ReferIt3D splits into Nr3D, with 41,503 human-generated descriptions, and Sr3D, containing 83,572 synthetic expressions. ReferIt3D simplifies the task by providing segmented point clouds for each object. The primary evaluation metric is accuracy in target object selection. Table 2. Quantitative comparisons on Nr3D and Sr3D datasets. We evaluate under three pipelines, noting that the Two-stage using Ground-Truth Boxes is impractical for real-world applications. TSP3D exhibits significant superiority, with leads of +2.6% and +3.2% on NR3D and SR3D respectively. Method Venue Pipeline Accuracy Nr3D Sr3D InstanceRefer [39] LanguageRefer [28] 3D-SPS [22] MVT [15] BUTD-DETR [16] EDA [35] VPP-Net [31] G3-LQ [34] MCLN [27] InstanceRefer [39] LanguageRefer [28] BUTD-DETR [16] EDA [35] MCLN [27] 3D-SPS [22] BUTD-DETR [16] EDA [35] MCLN [27] TSP3D (Ours) ICCV21 CoRL22 CVPR22 CVPR22 ECCV22 CVPR23 CVPR24 CVPR24 ECCV24 ICCV21 CoRL22 ECCV22 CVPR23 ECCV24 CVPR22 ECCV22 CVPR23 ECCV24 Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (gt) Two-stage (det) Two-stage (det) Two-stage (det) Two-stage (det) Two-stage (det) Single-stage Single-stage Single-stage Single-stage Single-stage 38.8 43.9 51.5 55.1 54.6 52.1 56.9 58.4 59.8 29.9 28.6 43.3 40.7 46.1 39.2 38.7 40.0 45.7 48. 48.0 56.0 62.6 64.5 67.0 68.1 68.7 73.1 68.4 31.5 39.5 52.1 49.9 53.9 47.1 50.1 49.7 53.4 57.1 4.2. Implementation Details TSP3D is implemented based on PyTorch [23]. The pruning thresholds are set at σsce = 0.7 and σtar = 0.3, and the completion threshold in CBA is τ = 0.15. The initial voxelization of the point cloud has voxel size of 1cm, while the voxel size for level features scales to 2i+2 cm. The supervision for pruning uses = 7. The weights for all components of the loss function, λ1, λ2, λ3, λ4, are equal to 1. Training is conducted using four GPUs, while inference speeds are evaluated using single consumer-grade GPU, RTX 3090, with batch size of 1. 4.3. Quantitative Comparisons Performance on ScanRefer. We carry out comparisons with existing methods on ScanRefer, as detailed in Tab. 1. The inference speeds of other methods are obtained through our reproduction with single RTX 3090 and batch size of 1. For two-stage methods, the inference speed includes the time taken for object detection in the first stage. For methods using 2D image features and 3D point clouds as inputs, we do not account for the time spent extracting 2D features, assuming they can be obtained in advance. However, in practical applications, the acquisition of 2D features also impacts overall efficiency. TSP3D achieves stateof-the-art accuracy even compared with two-stage methods, with +1.13 lead on Acc@0.5. Notably, in the singlestage setting, TSP3D achieves fast inference speed, which is unprecedented among the existing methods. This significant improvement is attributed to our methods efficient use of multi-level architecture based on 3D sparse convoluTable 3. Impact of the proposed TGP and CBA. Evaluated on ScanRefer. Table 4. Influence of the two CBAs at different levels. Evaluated on ScanRefer. Table 5. Influence of different feature upsampling methods. Evaluated on ScanRefer. ID TGP CBA (a) (b) (c) (d) Accuracy 0.5 0.25 40.13 55.20 41.34 56. 32.87 46.15 33.09 46.71 Speed (FPS) 14.58 13.22 13.51 12.43 ID (a) (b) (c) (d) CBA (level 2) CBA (level 1) Accuracy 0.5 0.25 Speed (FPS) ID Method Accuracy 0.5 0.25 Speed (FPS) 55.20 55.17 56.45 56. 46.15 46.06 46.71 46.68 13.22 12.79 12.43 12.19 (a) Simple concatenation (b) Attention mechanism Text-guided pruning (c) Simplified TGP (d) 56.27 56.45 40. 32.87 46.58 46.71 14.58 10.11 12.43 tions, coupled with the text-guided pruning. By focusing computation only on salient regions of the point clouds, determined by textual cues, our model effectively reduces computational overhead while maintaining high accuracy. TSP3D also sets benchmark for inference speed comparisons for future methodologies. Performance on Nr3D/Sr3D. We evaluate our method on the SR3D and NR3D datasets, following the evaluation protocols of prior works like EDA [35] and BUTDDETR [16] by using Acc@0.25 as the accuracy metric. The results are shown in Tab. 2. Given that SR3D and NR3D provide ground-truth boxes and categories for all objects in the scene, we consider three pipelines: (1) Two-stage using Ground-Truth Boxes, (2) Two-stage using Detected Boxes, and (3) Single-stage. In practical applications, the Two-stage using Ground-Truth Boxes pipeline is unrealistic because obtaining all ground-truth boxes in scene is infeasible. This approach can also oversimplify certain evaluation scenarios. For example, if there are no other objects of the same category as the target in the scene, the task reduces to relying on the provided ground-truth category. Under the Single-stage setting, TSP3D exhibits significant superiority with peak performance of 48.7% and 57.1% on Nr3D and Sr3D. TSP3D even outperforms previous works under the pipeline of Two-stage using Detected Boxes, with leads of +2.6% and +3.2% on NR3D and SR3D. 4.4. Ablation Study Effectiveness of Proposed Components. To investigate the effects of our proposed TGP and CBA, we conduct ablation experiments with module removal as shown in Tab. 3. When TGP is not used, multi-modal feature concatenation is employed as replacement, as shown in Fig. 2 (a). When CBA is not used, it is substituted with pruning-based addition. The results demonstrate that TGP significantly enhances performance without notably impacting inference time. This is because TGP, while utilizing more complex multi-modal attention mechanism for stronger feature fusion, significantly reduces feature scale through text-guided pruning. Additionally, the performance improvement is also due to the gradual guidance towards the target object by both scene-level and target-level TGP. Using CBA alone has limited effect, as no voxels are pruned. Implementing CBA on top of TGP further enhances performance, as CBA dynamically compensates for some of the excessive pruning by TGP, thus increasing the networks robustness. Influence of the Two CBAs. To explore the impact of CBAs at two different levels, we conduct ablation experiments as depicted in Tab. 4. In the absence of CBA, we use pruning-based addition as substitute. The results indicate that the CBA at level 2 has negligible effects on the 3DVG task. This is primarily because the CBA at level 2 serves to supplement the scene-level TGP, which is expected to prune the background (a relatively simple task). Moreover, although some target features are pruned, they are compensated by two subsequent generative sparse convolutions. However, the CBA at level 1 enhances performance by adapt completion for the target-level TGP. It is challenging to fully preserve target objects from deep upsampling features, especially for smaller or narrower targets. The CBA at level 1, based on high-resolution backbone features, effectively complements the TGP. Feature Upsampling Techniques. We conduct experiments to assess the effects of different feature upsampling techniques, as detailed in Tab. 5. Using simple feature concatenation (Fig. 2 (a)), while fast in inference speed, results in poor performance. When we utilize an attention mechanism with stronger feature interaction, as shown in Fig. 2 (b), the computation exceeds the limits of GPU due to the large number of voxels, making it impractical for real-world applications. Consequently, we employ TGP to reduce the feature amount, as illustrated in Fig. 2 (c), which significantly improves performance and enables practical deployment. Building on TGP, we propose simplified TGP, as shown in Fig. 2 (d), that merges feature interactions before and after pruning, achieving performance consistent with the original TGP while enhancing inference speed. 4.5. Qualitative Results Text-guided Pruning. To visually demonstrate the process of TGP, we visualize the results of two pruning phases, as shown in Fig. 4. In each example, the voxel features after scene-level pruning, the features after target-level pruning, and the features after target-level generative sparse convolution are displayed from top to bottom. It is evident that both pruning stages effectively achieve our intended effect: the scene-level pruning filters out the background and retained object voxels, and the target-level pruning preserves relevant and target objects. Moreover, during the feature upsampling process, the feature amount nearly exponenFigure 4. Visualization of the text-guided pruning process. In each example, the voxel features after scene-level TGP, target-level TGP and the last upsampling layer are presented from top to bottom. The blue boxes represent the ground truth of the target, and the red boxes denote the bounding boxes of relevant objects. TSP3D reduces the amount of voxel features through two stages of pruning and progressively guides the network focusing towards the target. Figure 5. Visualization of the completion-based addition process. The blue points represent the voxel features output by the target-level TGP, while the red points are the completion features predicted by the CBA. The blue boxes indicate the ground truth boxes. CBA adaptively supplements situations where excessive pruning has occurred. tially increases due to generative upsampling. Without TGP, the voxel coverage would far exceed the range of the scene point cloud, which is inefficient for inference. This also intuitively explains the significant impact of our TGP on both performance and inference speed. Completion-based Addition. To clearly illustrate the function of CBA, we visualize the adaptive completion process in Fig. 5. The images below showcase several instances of excessive pruning. TGP performs pruning based on deep and low-resolution features, which can lead to excessive pruning, potentially removing entire or partial targets. This over-pruning is more likely to occur with small, as shown in Fig. 5 (a) and (c), narrow, as in Fig. 5 (b), or elongated targets, as in Fig. 5 (d). Our CBA effectively supplements the process using higher-resolution backbone features, thus dynamically integrating multi-level features. 5. Conclusion In this paper, we present TSP3D, an efficient sparse singlestage method for real-time 3D visual grounding. Different from previous 3D visual grounding frameworks, TSP3D builds on multi-level sparse convolutional architecture for efficient and fine-grained scene representation extraction. To enable the interaction between voxel features and textual features, we propose text-guided pruning (TGP), which reduces the amount of voxel features and guides the network to progressively focus on the target object. Additionally, we introduce completion-based addition (CBA) for adaptive multi-level feature fusion, effectively compensating for instances of over-pruning. Extensive experiments demonstrate the effectiveness of our proposed modules, resulting in an efficient 3DVG method that achieves state-of-the-art accuracy and fast inference speed."
        },
        {
            "title": "References",
            "content": "[1] Ahmed Abdelreheem, Ujjwal Upadhyay, Ivan Skorokhodov, Rawan Al Yahya, Jun Chen, and Mohamed Elhoseiny. 3dreftransformer: Fine-grained object identification in real-world scenes using natural language. In WACV, pages 39413950, 2022. 2 [2] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, pages 422440. Springer, 2020. 2, 6 [3] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, pages 202221. Springer, 2020. 1, 2, 3, 6, 4 [4] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical aggregation for 3d instance segmentation. In ICCV, pages 1546715476, 2021. 2 [5] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. Voxelnext: Fully sparse voxelnet for 3d object detection and tracking. In CVPR, pages 2167421683, 2023. 2 [6] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In CVPR, pages 30753084, 2019. 2, 3 [7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent arXiv preprint neural networks on sequence modeling. arXiv:1412.3555, 2014. [8] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In AAAI, pages 12011209, 2021. 2 [9] Jacob Devlin. Bert: Pre-training of deep bidirectional arXiv preprint transformers for language understanding. arXiv:1810.04805, 2018. 2 [10] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. In ICCV, pages 37223731, 2021. 2, 6, 3 [11] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In CVPR, pages 92249232, 2018. 2 [12] JunYoung Gwak, Christopher Choy, and Silvio Savarese. Generative sparse detection networks for 3d single-shot object detection. In ECCV, pages 297313. Springer, 2020. 2, [13] Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-andrelation aware transformer for fine-grained 3d visual grounding. In ACM MM, pages 23442352, 2021. 2 [14] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In AAAI, pages 16101618, 2021. 1, 2, 6, 3 [15] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multiview transformer for 3d visual grounding. In CVPR, pages 1552415533, 2022. 6 [16] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In ECCV, pages 417433. Springer, 2022. 1, 2, 3, 6, 7 [17] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In CVPR, pages 48674876, 2020. 2 [18] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, pages 1096510975, 2022. [19] Daizong Liu, Yang Liu, Wencan Huang, and Wei Hu. survey on text-guided 3d visual grounding: Elements, arXiv preprint recent advances, and future directions. arXiv:2406.05785, 2024. 2 [20] Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 3, 1 [21] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3d object detection via transformers. In ICCV, pages 29492958, 2021. 2 [22] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. In CVPR, pages 1645416463, 2022. 1, 2, 3, 6 [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019. 6 [24] Jeffrey Pennington, Richard Socher, and Christopher ManIn ning. Glove: Global vectors for word representation. EMNLP, pages 15321543, 2014. 2 [25] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. NeurIPS, 30, 2017. 2, 3, 1 [26] Charles Qi, Or Litany, Kaiming He, and Leonidas Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 92779286, 2019. [27] Zhipeng Qian, Yiwei Ma, Zhekai Lin, Jiayi Ji, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Multi-branch collaborative learning network for 3d visual grounding. In ECCV, pages 381398. Springer, 2025. 2, 6, 3 [28] Junha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. In CoRL, pages 10461056. PMLR, 2022. 6 [29] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Fcaf3d: Fully convolutional anchor-free 3d object detection. In ECCV, pages 477493. Springer, 2022. 2, 3 [30] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Tr3d: Towards real-time indoor 3d object detection. In ICIP, pages 281285. IEEE, 2023. 2, 3 [31] Xiangxi Shi, Zhonghua Wu, and Stefan Lee. Viewpointaware visual grounding in 3d scenes. In CVPR, pages 14056 14065, 2024. 3, 6 [32] Thang Vu, Kookhoi Kim, Tung Luu, Thanh Nguyen, and Chang Yoo. Softgroup for 3d instance segmentation on point clouds. In CVPR, pages 27082717, 2022. 2 [33] Haiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhenguo Li, and Liwei Wang. Cagroup3d: Class-aware grouping for 3d object detection on point clouds. NeurIPS, 35:2997529988, 2022. [34] Yuan Wang, Yali Li, and Shengjin Wang. Gˆ 3-lq: Marrying hyperbolic alignment with explicit semantic-geometric modeling for 3d visual grounding. In CVPR, pages 1391713926, 2024. 2, 3, 6 [35] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. In CVPR, pages 19231 19242, 2023. 1, 2, 3, 6, 7, 4 [36] Xiuwei Xu, Ziwei Wang, Jie Zhou, and Jiwen Lu. Binarizing sparse convolutional networks for efficient point cloud analysis. In CVPR, pages 53135322, 2023. 2 [37] Xiuwei Xu, Zhihao Sun, Ziwei Wang, Hongmin Liu, Jie Zhou, and Jiwen Lu. 3d small object detection with dynamic spatial pruning. In ECCV. Springer, 2024. 2 [38] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Sat: 2d semantics assisted training for 3d visual Luo. grounding. In ICCV, pages 18561866, 2021. 1, 6, 3 [39] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. In ICCV, pages 17911800, 2021. 1, 2, 6, [40] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. In ICCV, pages 29282937, 2021. 2 [41] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In ICCV, pages 29112921, 2023. 6, 3 Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding"
        },
        {
            "title": "Supplementary Material",
            "content": "We provide statistics and analysis for visual feature resolution (Sec. A), detailed comparisons of computational cost (Sec. B), detailed results on the ScanRefer dataset [3] (Sec. C), qualitative comparisons (Sec. D) and potential limitations (Sec. E) in the supplementary material. A. Visual Feature Resolution of Different Architectures To analyze the scene representation resolution of pointbased and sparse convolutional architectures, we compare the resolution changes during the visual feature extraction process for EDA [35] and TSP3D-B, as illustrated in Fig. 6. For thorough examination of the feature resolution of the sparse convolution architecture, we consider TSP3D-B without incorporating TGP and CBA. The voxel numbers for TSP3D-B are based on the average statistics from the ScanRefer validation set. In point-based architectures, the number of point features is fixed and does not vary with the scene size. In contrast, the number of voxel features in sparse convolutional architectures tends to increase as the scene size grows. This adaptive adjustment ensures that features do not become excessively sparse when processing larger scenes. As shown in Fig. 6, point-based architectures perform aggressive downsampling, with the first downsampling step reducing 50,000 points to just 2,048 points. Moreover, the final scene representation consists of only 1,024 points, leading to relatively coarse representation. By contrast, convolution-based architectures progressively downsample and refine the scene representation through multi-level structure. Overall, the sparse convolution architecture not only provides high-resolution scene representation but also achieves faster inference speed compared to point-based architectures. B. Detailed Computational Cost of Different"
        },
        {
            "title": "Architectures",
            "content": "We provide detailed comparison of the inference speed of specific components across different architectures, as shown in Tab. 6. Two-stage methods tend to have slower inference speed and are significantly impacted by the efficiency of the detection stage, which is not the primary focus of the 3DVG task. Therefore, we focus our analysis solely on the computational cost of single-stage methods. We divide the networks of existing methods and TSP3D into several components: text decoupling, visual backbone, text backbone, multi-modal fusion, and the head. The inference speed of each of these components is measured separately. Figure 6. Feature resolution progression of point-based EDA and sparse convolutional TSP3D-B. SA, FP, SpConv, and FU represent set abstraction, feature propagation, sparse convolution, and feature upsampling, respectively. For the point-based architecture, the downsampling process is aggressive, with the first downsampling reducing 50,000 points directly to 2,048 points. Furthermore, the final scene representation consists of only 1,024 points. In contrast, the sparse convolutional architecture performs progressive downsampling and refines the scene representation through multi-level structure. This approach not only provides high-resolution scene representation but also achieves faster inference speed compared to the point-based architecture. Backbone. Except for TSP3D, the visual backbone in other methods is PointNet++ [25], which has high computational cost. This is precisely why we introduce sparse convolution backbone, which achieves approximately three times the inference speed of PointNet++. As for the text backbone, both TSP3D and other methods use the pretrained RoBERTa [20], so the inference speed for this component is largely consistent across the methods. Multi-modal Fusion. The multi-modal feature fusion primarily involves the interaction between textual and visual features, with different methods employing different modules. For instance, the multi-modal fusion in SDSPS mainly includes the description-aware keypoint sampling (DKS) and target-oriented progressive mining (TPM) modules. And methods like BUTD-DETR, EDA, and MCLN rely on cross-modal encoders and decoders for their fuIn our TSP3D, the multi-modal fusion insion process. volves feature upsampling, text-guided pruning (TGP), and completion-based addition (CBA). Notably, even though Table 6. Detailed comparison of computational cost for different single-stage architectures on the ScanRefer dataset [3]. The numbers in the table represent frames per second (FPS). TSP3D demonstrates superior processing speed across all components compared to other methods, with the inference speed of the sparse convolution backbone being three times faster than that of the point-based backbone. Method Text Visual Decouple Backbone Backbone Text 3D-SPS [22] BUTD-DETR [16] EDA [35] MCLN [27] TSP3D (Ours) 126.58 126.58 126.58 10.88 10.60 10.89 10.52 31. 80.39 78.55 81.10 76.92 81.21 Multi-modal Fusion Head Overall 13.25 28.49 28.57 23.26 28.67 166.67 52.63 49.75 41.32 547.32 5.38 5.91 5.98 5.45 12. D. Qualitative Comparisons To qualitatively demonstrate the effectiveness of our proposed TSP3D, we visualize the 3DVG results of TSP3D alongside EDA [35] on the ScanRefer dataset [3]. As shown in Fig. 7, the ground truth boxes are marked in blue, with the predicted boxes for EDA and TSP3D displayed in red and green, respectively. EDA encounters challenges in locating relevant objects, identifying categories, and distinguishing appearance and attributes, as illustrated in Fig. 7 (a), (c), and (d). In contrast, our TSP3D gradually focuses attention on the target and relevant objects under textual guidance and enhances resolution through multi-level feature fusion, showcasing commendable grounding capabilities. Furthermore, Fig. 7 (b) illustrates that TSP3D performs better with small or narrow targets, as our proposed completion-based addition can adaptively complete the target shape based on high-resolution backbone feature maps. E. Limitations and Future Work Despite its leading accuracy and inference speed, TSP3D still has some limitations. First, the speed of TSP3D is slightly slower than that of TSP3D-B. While TSP3D leverages TGP to enable deep interaction between visual and text features in an efficient manner, it inevitably introduces additional computational overhead compared to naive concatenation. In future work, we aim to focus on designing new operations for multi-modal feature interaction to replace the heavy cross-attention mechanism. Second, the current input for 3DVG methods consists of reconstructed point clouds. We plan to extend this to an online setting using streaming RGB-D videos as input, which would support broader range of practical applications. TSP3D progressively increases the resolution of scene features and integrates them with fine-grained backbone features, it still achieves superior inference speed. This is primarily due to the text-guided pruning, which significantly reduces the number of voxels and computational cost. Head and Text Decouple. In the designs of methods such as BUTD-DETR, EDA, and MCLN, the input text needs to be decoupled into several semantic components. Additionally, their heads do not output prediction scores directly. Instead, they output embeddings for each candidate object, which must be compared with the embeddings of each word in the text to compute similarities and determine the final output. This can be considered additional preprocessing and post-processing steps, with the latter significantly impacting computational efficiency. In contrast, our TSP3D directly predicts the matching scores between the objects and the input text, making the head inference speed over ten times faster than these methods. C. Detailed Results on ScanRefer Due to page limitations, we report only the overall performances and inference speeds in the main text. To provide detailed results and analysis, we include the accuracies of TSP3D and other methods across various subsets on the ScanRefer dataset [3], as shown in Tab. 7. TSP3D achieves state-of-the-art accuracy, even when compared with twostage methods, leading by +1.13 in Acc@0.5. TSP3D also demonstrates level of efficiency that previous methods lack. In various subsets, TSP3D maintains comparable accuracy to both single-stage and two-stage state-of-the-art methods. Notably, the multi-object subset involves distinguishing the target object among numerous distractors of the same category within more complex 3D scene. In this setting, TSP3D achieves commendable performance of 42.37 in Acc@0.5, further demonstrating that TSP3D enhances attention to the target object in complex environments through text-guided pruning and completion-based addition, enabling accurate predictions of both the location and the shape of the target. Table 7. Detailed comparison of methods on the ScanRefer dataset [3] evaluated at IoU thresholds of 0.25 and 0.5. TSP3D achieves stateof-the-art accuracy even compared with two-stage methods, with +1.13 lead on Acc@0.5. In various subsets, TSP3D achieves comparable accuracy to both single-stage and two-stage state-of-the-art methods. Additionally, TSP3D demonstrates level of efficiency that previous methods lack."
        },
        {
            "title": "Venue",
            "content": "Two-Stage Model ScanRefer [3] TGNN [14] InstanceRefer [39] SAT [38] FFL-3DOG [10] 3D-SPS [22] ECCV20 AAAI21 ICCV21 ICCV21 ICCV21 CVPR22 BUTD-DETR [16] ECCV22 CVPR23 ICCV23 CVPR24 CVPR24 ECCV24 EDA [35] 3D-VisTA [41] VPP-Net [31] G3-LQ [34] MCLN [27] Single-stage Model 3D-SPS [22] CVPR22 BUTD-DETR [16] ECCV22 CVPR23 CVPR24 ECCV24 EDA [35] G3-LQ [34] MCLN [27] TSP3D (Ours) Unique (19%) Multiple (81%) 0.25 0.25 0.5 0. Accuracy 0.5 0.25 Inference Speed (FPS) 76.33 68.61 77.45 73.21 78.80 84.12 82.88 85.76 77.40 86.05 88.09 86.89 81.63 81.47 86.40 88.59 84.43 87.25 53.51 56.80 66.83 50.83 67.94 66.72 64.98 68.57 70.90 67.09 72.73 72. 64.77 61.24 69.42 73.28 68.36 71.41 32.73 29.84 31.27 37.64 35.19 40.32 44.73 49.13 38.70 50.32 51.48 51.96 39.48 44.20 48.11 50.23 49.72 51.04 21.11 23.18 24.77 25.16 25.7 29.82 33.97 37.64 34.80 39.03 40.80 40.76 29.61 32.81 36.82 39.72 38.41 42.37 41.19 37.37 40.23 44.54 41.33 48.82 50.42 54.59 45.90 55.65 56.90 57. 47.65 50.22 53.83 55.95 54.30 56.45 6.72 3.19 2.33 4.34 27.40 29.70 30.15 30.14 34.01 Not released 36.98 38.60 42.26 41.50 43.29 Not released 45.58 Not released 45.53 3.17 3.33 3.34 2.03 3.17 5.38 5.91 5. 36.43 37.87 41.70 44.72 Not released 42.64 46.71 5.45 12.43 Figure 7. Qualitative results of EDA [35] and our TSP3D on the ScanRefer dataset [3]. In each description, the red annotations indicate the target object. The orange annotations in (a) refer to relevant objects, while the yellow annotations in (d) denote the appearance or attributes of the target. TSP3D demonstrates exceptional performance in locating relevant objects, narrow or small targets, identifying categories, and distinguishing appearance and attributes."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Tsinghua University"
    ]
}