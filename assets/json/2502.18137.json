{
    "paper_title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
    "authors": [
        "Jintao Zhang",
        "Chendong Xiang",
        "Haofeng Huang",
        "Jia Wei",
        "Haocheng Xi",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn."
        },
        {
            "title": "Start",
            "content": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Jintao Zhang * 1 Chendong Xiang * 1 Haofeng Huang * 1 Jia Wei 1 Haocheng Xi 2 Jun Zhu 1 Jianfei Chen 1 5 2 0 2 5 2 ] . [ 1 7 3 1 8 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, universal sparse and quantized attention for any model. Our method uses two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmaxaware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-toend metrics. The codes are available at https: //github.com/thu-ml/SpargeAttn. 1 1. Introduction As sequence lengths in large models become longer, such as 45K-128K in video generation and language models (Yang et al., 2025; Bao et al., 2024; Dubey et al., 2024), the *Equal contribution 1Department of Computer Science and Technology, Tsinghua University 2University of CaliforJianfei Chen <jiannia, Berkeley. feic@tsinghua.edu.cn>. Correspondence to: Preprint. 1All experiments in this paper used SpargeAttn based on SageAttention. An updated implementation based on SageAttention2, is available at https://github.com/thu-ml/ SpargeAttn. It further offers 30% speedup over the Attention in this paper. 1 Figure 1. SpargeAttn can achieve 1.83x speedup on Mochi on L40 GPU, with no video quality loss. time consuming of attention occupies significant portion of inference latency in large models (Zhang et al., 2025). Fortunately, the attention map = Softmax(QK /d) exhibits inherent sparsity, as the softmax operation often creates many values approaching zero (Deng et al., 2024). Sparse attention methods exploit such sparsity to accelerate attention by (1) constructing sparse mask, which indicates the important non-zero entries of the attention map that should be computed, and (2) computing attention only for the parts corresponding to the sparse mask. There are three distinct categories of sparse attention methods based on how the sparse mask is generated. pattern-based method (Zhang et al., 2023; Xiao et al., 2024a; Fu et al., 2024; Zhu et al., 2024; Xiao et al., 2025; 2024b) relies on specific sparsity patterns based on empirical observations, dynamic sparse attention (Ribar et al., 2024; Singhania et al., 2024; Jiang et al., 2024; FlexPrefill, 2025; Gao et al., 2024) computes the mask on-the-fly based on the inputs, and training-based method (Kitaev et al., 2020; Pagliardini et al., 2023) directly train models with native sparse attention. Limitation. (L1. Universality) Though existing sparse attention methods already demonstrate promising speedup on some tasks, their universality is still limited. Existing works are typically developed for specific tasks, such as language modeling, utilizing task-specific patterns such as sliding windows or attention sinks. However, the attention pattern varies significantly across tasks (see examples in Fig. 2), making these patterns hard to generalize. (L2. Usability) Moreover, it is difficult to implement both accurate and efficient sparse attention for any input. This is because accuracy demands precise prediction of the sparse regions in the attention map, while efficiency requires the overhead of this prediction to be minimal. However, current methods are difficult to effectively satisfy both of the requirements siSpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference 2. Related Work Depending on how the sparsity mask is constructed, sparse attention methods can be divided into three types: (1) Pattern required methods rely on some fixed patterns of the attention map, such as sliding windows or attention sinks (Xiao et al., 2024b). H2O (Zhang et al., 2023), InfLLM (Xiao et al., 2024a), and DUOAttention (Xiao et al., 2025) rely on sliding window pattern. SampleAttention (Zhu et al., 2024), MOA (Fu et al., 2024), and StreamingLLM (Xiao et al., 2024b) rely on sliding window and attention sink pattern. DitFastAttn (Yuan et al., 2024) relies on sliding window patterns and similarities between different attention maps. Moreover, DitFastAttn is restricted to simple diffusion transformers, showing incompatibility with language models and MMDiT models like Flux (Black Forest Labs, 2023), Stable Diffusion3 and 3.5 (Stability AI, 2023), and CogVideoX (Yang et al., 2025). As the pattern varies across models, these methods may not universally work for different models. (2) Dynamic sparse methods dynamically construct the sparse mask based on the input without the need of preset patterns, and are thus potentially more universal. Existing works can be further categorized into channel compression and token compression. Channel compression methods include SparQAttn (Ribar et al., 2024) and LokiAttn (Singhania et al., 2024). They construct the mask by carrying full attention with reduced dimensionality. However, as the dimension is already small, e.g., 64, 128, in commonly used attention, the speedup potential might be limited. Token compression methods include MInference (Jiang et al., 2024) and FlexPrefill (FlexPrefill, 2025). They construct the mask by compressing each block of tokens to single token and compute attention on this shorter sequence. However, this approximation is too aggressive: missing important blocks of is possible if they do not have large attention score on the compressed sequence. SeerAttention (Gao et al., 2024) requires training of additional parameters for attention, which is expensive to use. Moreover, they are all designed for language models, and their applicability to other model types, such as diffusion models, remains uncertain. (3) Training-based methods modify the attention computation logic, requiring retraining the entire model, such as Reformer (Kitaev et al., 2020) and FastAttention (Pagliardini et al., 2023). These methods are much more expensive to use than training-free methods. There are other ways to accelerate attention, such as optimizing the kernel implementation (Dao et al., 2022; Dao, 2024; Shah et al., 2024), quantization (Zhang et al., 2025), distributing the workload (Liu et al., 2024a), and designing linear time attention (Wang et al., 2020; Choromanski et al., 2021; Yu et al., 2022; Katharopoulos et al., 2020). They are orthogonal to our approach. Figure 2. Some sampled patterns of attention map in video, image, and language generation models. multaneously. For example, MInference (Jiang et al., 2024) requires large sequence length, such as 100K, to achieve noticeable speedup. Goal. We aim to design training-free sparse attention operator that accelerates all models without metrics loss. Our approach. In this work, we develop SpargeAttn, training-free sparse attention that can be adopted universally on various tasks, including language modeling and text-toimage/video, and various sequence lengths. We propose three main techniques to improve the universality, accuracy, and efficiency. First, we propose universal sparse mask prediction algorithm, which constructs the sparse mask by compressing each block of Q, to single token. Importantly, we compress selectively based on the similarity of tokens within the block, so the algorithm can accurately predict sparse masks universally across tasks. Second, we propose sparse online softmax algorithm at the GPU warp level, which further omits some products by leveraging the difference between global maximum values and local maximum values in online softmax. Third, we integrate this sparse approach into the 8-bit quantized SageAttention framework for further acceleration. Result. We evaluate SpargeAttn on variety of generative tasks, including language modeling and text-toimage/video, with comprehensive performance metrics on the model quality. SpargeAttn can robustly retain model end-to-end performance while existing sparse attention baselines incur degradation. Moreover, SpargeAttn is 2.5x to 5x faster than existing dense and sparse attention models. 2 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Figure 3. Workflow of SpargeAttn. 3. SpargeAttn SpargeAttn contains two-stage online filter to implement sparse FlashAttention. First, as shown in Step1 and Step2 in Fig. 3, we design fast and accurate method to predict the sparse block in the attention map, thereby skipping the corresponding products of QiK and PijVj. Second, as shown in Step3 in Fig. 3, we design sparse PijVj. online softmax method to further skip the products of (cid:101) (cid:101) (cid:80) d, while the matrix and is exp(Sik) is the softmax operation. Let exp(Sij)/ be the sequence length and be the dimensionality of each head; the matrices Q, K, and each have dimensions . FlashAttention proposes to tile Q, K, and from the token dimension into with block sizes bq, bk, bk, respecblocks tively. Then, it uses online softmax (Milakov & Gimelshein, 2018) to progressively compute each block of O, i.e., Oi: , Ki} { , Qi} { Vi} { Sij = QiK /d, (mij, lij = exp(mi,j Pij) = σ(mi,j 1, Sij), Pij), 1 + rowsum( 1 Oij = diag (exp(mi,j mij)li,j (cid:101) mij)) Oi,j 1 1 vectors, which are initialized and 0 respectively. The σ() is an operator similar Pi,j = mij). Finally, the output Oi can be computed by 1, rowmax(Sij) } where mij and lij are bq to to softmax.: mij = max exp(Sij Oi = diag(lij) PijVj (cid:101) 1Oij. mi,j 1 + (1) (cid:101) (cid:101) { , Figure 4. Exemplary patterns of the query and key in the attention of various models. 3.1. Sparse FlashAttention SpargeAttn adopts the tiling strategy of FlashAttention (Dao, 2024), and skip computing the blocks that are filtered out. Consider an attention operation = QK /d, = σ(S), = , where σ(S)ij = Implementing sparse FlashAttention is intuitive. By skipping certain block matrix multiplications of QiK and PijVj, we can accelerate the attention computation. We formulate sparse attention based on FlashAttention in the following definitions. (cid:101) Definition 1 (Block Masks). Let Mg and Mpv be binary , where each value masks of dimensions N/bq is either 0 or 1. These masks determine which computations are skipped in the sparse attention mechanism. N/bk Definition 2 (Sparse FlashAttention). The computation rules for sparse FlashAttention based on the masks are defined as follows: QiK , PijVj are skipped if Mg[i, j] = 0. (2) 3 (cid:101) SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference PijVj is skipped if Mpv[i, j] = 0. (3) def Top_Cdf(P[i], tau): (cid:101) 3.2. Selective Token Compression for Sparse Prediction Key idea. Although attention maps vary across models, we observe that various models exhibit common trait: Most closer tokens in the query and key matrices of the attention show high similarity (See Fig. 4). Consequently, for blocks composed of highly similar tokens, we can consolidate these tokens into single representative token for the block. Based on this observation, we propose pattern-free online prediction method for identifying sparse blocks in to skip some computation of QiK and PijVj during the FlashAttention process. Specifically, we first compress blocks exhibiting high self-similarity within and into tokens. Then, we swiftly compute compressed attention map ˆP using the compressed and K. Finally, we selectively compute for those pairs (i, j) where PijVj} ˆP [i, j] accumulates high score in the compressed atten- { } (cid:101) tion map. Importantly, compressing only the token blocks with high self-similarity is crucial, as omitting computations for non-self-similar blocks can result in the loss of critical information. This will be confirmed in Sec. 4 and A.2. QiK , (cid:101) { Prediction. As shown in Step1 in Fig. 3, we first compute mean cosine similarity across tokens for each block of and K. Next, we compress each block into single token by calculating mean across tokens. Then, we compute compressed QK using the compressed and K. Finally, to prevent interference from non-self-similar blocks, i.e., the block similarity less than hyper-parameter θ, we set the corresponding values in to , and then obtain compressed attention map through softmax. This algorithm can be expressed as: = = = qi} { kj} { mean(Qi, axis = 0) { } mean(Kj, axis = 0) } { = sqi = CosSim(Qi), skj = CosSim(Kj) ˆS[:, j] = ˆS[i] = qik; , If skj < θ ˆP [i] = Softmax( ˆS[i]) R1 Rbq Rbk d, qi d, Kj R1 XX max(XX) d, kj measures the cosinewhere Qi and CosSim(X) = similarity within block. For each row of ˆP , i.e., ˆP [i], we select the positions of ˆP [i], the top values whose cumulative sum reaches τ where τ is hyper-parameter. These positions are set to 1 in Mg[i, :], while all other positions are set to 0. (cid:80) Mg[i, :] =TopCdf( ˆP [i], τ ) (4) where the TopCdf( ˆP [i], τ ) can be formulated as follows. 4 sorted_P, idx = torch.sort(P[i], descending=True) cusum_P = torch.cumsum(sorted_P, dim=0) mask = cusum_P <= tau * P[i].sum() M_i = torch.zeros_like(mask) M_i[idx] = mask return M_i Finally, we need to ensure that calculations involving nonself-similar blocks of or are not omitted. Therefore, we set all values in the rows of Mg corresponding to not self-similar blocks of to 1, and all values in the columns of Mg corresponding to non-self-similar blocks of to 1. Mg[i, :] = 1, If sqi < θ; Mg[:, j] = 1, If skj < θ (5) 3.3. Masking of the First Stage Masking. The Mg can be applied in FlashAttention directly to saving some computation. In the inner loop of FlashAttention, i.e., during computing attention between Qi and when , we can skip Mg[i, j] = 0. QiK , { , Kj} Vj} { { PijVj} (cid:101) Skip QiK and PijVj, If Mg[i, j] = 0 (6) 3.4. Sparse Warp Online Softmax (cid:101) Key idea. We can further identify the small enough values in the attention map during the online softmax process. If all values in PijVj will be Pij are close enough to zero, the negligible and can be omitted. mi,j) (See Sec. 3.1) To identify which Pij = exp(Sij contains values small enough to be omitted, we note that in every inner loop of FlashAttention, the Oij will be scaled (cid:101) mij) and then plus the by exp(mi,j PijVj: (cid:101) (cid:101) 1 mi,j mlocal =rowmax(Sij), mij = max (cid:101) { mij)) Oi,j Oij =diag (exp(mi,j 1 + 1, mlocal } PijVj 1 1. Consequently, (cid:101) PijVj. Furthermore, if rowmax(Sij) If rowmax(Sij) < mij, then mij = mi,j Oij = Oi,j 1 + mij holds ture, then all values in (cid:101) close to 0. This results in all values in to 0. This condition implies that rowmax(Sij) is significantly smaller than mij: mij) are Pij = exp(Sij PijVj being close (cid:101) PijVj is negligible when (cid:101) (cid:101) if max (exp(Sij max(mlocal 0 mij)) 0 mij) < λ max (exp(Sij The above equivalence is satisfied when λ is small enough. Oij Oi,j 1, mij)) Therefore, based on the analysis above, we propose simple yet effective sparse method to further skip the PijVj computation. Specifically, in the inner loop of FlashAttention, the Sij will be split by cw GPU warps to : bq Sij[ iw (cid:101) cw { SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference if [i, j]! = 0 then Load ˆQi and δQ[i] into SM ; for in [1, Tn] do Algorithm 1 Implementation of SpargeAttn. 1: Input: Matrices Q(FP16), K(FP16), (FP16) RN d, block size bq, bkv, count of GPU Warps cw, hyper-parameters τ, θ, and λ. 2: Divide to Tm = N/bq blocks {Qi}; divide K, to Tn = N/bkv blocks {Ki} and {Vi}. 3: ˆQi, ˆKj, δQ, δK = Quant(Qi, Kj) ; // per-block quantization in SageAttention. 4: = {qi} = {mean(Qi, axis = 0)} ; = {kj} = {mean(Kj, axis = 0)} ; 5: ˆS = qk; sqi = CosSim(Qi); skj = CosSim(Kj); ˆS[:, j] = , If skj < θ; 6: ˆP [i] = Softmax( ˆS[i]) ; [i, :] = TopCdf( ˆP [i], τ ) ; [i, :] = 1, If sqi < θ ; [:, j] = 1, If skj < θ ; 7: for = 1 to Tm do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: Oi = diag(li,Tn )1Oi,Tn ; Write Oi ; 21: end for 22: return = {Oi} ; Load ˆKj, ˆVj, and δK [j] into the SM ; Sij = Matmul( ˆQi, ˆK mlocal = rowmax(Sij); mij = max(mi,j1, mlocal) ; (cid:101)Pij = exp(Sij mij) ; iw = range(cw) ; Iw = [ iw bq : (iw +1)bq cw cw if max(mlocal[Iw] mij[Iw]) < λ then Oij[Iw] = diag(emi,j1[Iw ]mij [Iw ])1Oi,j1[Iw]+ Matmul( (cid:101)Pij[Iw], Vj) ; // Paralleled by cw warps. ) δQ δK ; // dequantization of SageAttention. lij = emi,j1mij + rowsum( (cid:101)Pij) ; end if end for end if ] ; bq (iw+1) cw Iw = [ iw ]. If max(mlocal[Iw] cw λ, where λ is small enough, then Oij[Iw] and we will skip the computation of used to update Oij[Iw]. , where iw is the index of the GPU warp. Let , :] } : (iw+1) bq mij[Iw]) < cw 1[Iw], Oi,j Pij[Iw]Vj which is bq 3.5. Combined with SageAttention (cid:101) To further accelerate our implementation of sparse attention, we integrate our method into SageAttention (Zhang et al., 2025), which proposes quantized method for accelerating attention. Since quantization operations and sparse operations are orthogonal, sparse computation can be directly applied to SageAttention. The complete algorithm is shown in Algorithm 1. Specifically, first, we need to add one judgment at the beginning of the inner loop of SageAttention (Line 10 in Algorithm 1) to decide whether to skip the whole inner loop once. Second, we add another judgment before the updating of Oij in the inner loop of SageAttention (Line 15 in Algorithm 1) to decide whether to skip the computation of PijVj. Moreover, to minimize the attention map prediction overhead, we implement the prediction using CUDA and adopt some kernel fusion techniques. (cid:101) 3.6. Hyper-parameters Determination for Model Layer Based on the method description in Sec. 3.2 and 3.4, our method incorporates three hyper-parameters: τ (0, 1), 1, 1), and λ < 0. The parameter determination proθ cess for each attention layer in any model is straightforward. We aim to identify set of hyperparameters that not only ( maximize attention sparsity but also constrain the attention error across five different model inputs. To evaluate attention accuracy, we employ strict error metric, the Relative . The L1 distance, defined as L1 = process begins by setting two L1 error thresholds l1 and l2, e.g., l1 = 0.05, l2 = 0.06. We first conduct grid search for τ and θ to identify the optimal pair that maximizes sparsity while ensuring L1 < l1. Subsequently, we perform another grid search for λ to find the optimal value that further maximizes sparsity while maintaining L1 < l2. (cid:80) (cid:80) / Figure 5. Illustration of different token permutation methods in 1 6 6 space, with block size of 4. 3.7. HilbertCurve Permutation Key idea. Improving sparsity while maintaining accuracy is key challenge in enhancing the performance of sparse attention. In our algorithm, increasing the self-similarity of key and query blocks can reduce the number of nonself-similar blocks. This allows more blocks to participate in TopCdf selection, thereby improving sparsity. Since attention is computationally invariant to token permutations, 5 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference the problem reduces to finding permutation that enhances the similarity of adjacent tokens. RT Image and video models benefit from strong priors: adjacent pixels are likely to be similar. To better leverage this prior, we propose the HilbertCurve permutation, given 3D visual d, We use the Hilbert Curve tokens Q, K, to fill the 3D space and then flatten tokens along the curve into shape RL . Fig. 5 illustrates an 6 visual tokens flatten by row-major example of 1 order and HilbertCurve. The Hilbert Curve preserves locality effectively, traversing the entire 3D space without crossing rows or columns, thereby increasing the similarity of adjacent tokens and the sparsity of attention. d, = 6 4. Experiment 4.1. Setup Models. We validate the effectiveness of SpargeAttn across diverse representative models from language, image, and video generation. Specifically, we conduct experiments on Llama3.1 (8B) (Dubey et al., 2024) for textto-text, CogvideoX (2B) and Mochi (Team, 2024) for text-to-video, Flux (Black Forest Labs, 2023)(.1-dev) and Stable-Diffusion3.5 (large) (Stability AI, 2023) for text-to-image. Datasets. The Text-to-text model is evaluated on four zeroshot tasks: WikiText (Merity et al., 2017) to assess the models prediction confidence, Longbench (Bai et al., 2024) and En.MC of InfiniteBench (Zhang et al., 2024b) for comprehensive assessment of long context understanding capabilities, and the Needle-in-A-Haystack task (Kamradt, 2023) to assess the models retrieval ability. Text-to-video models are evaluated using the open-sora (Zheng et al., 2024) prompt sets. Text-to-image models are assessed on COCO annotations (Lin et al., 2014). End-to-end metrics. For Llama3.1, we use perplexity (ppl.) (Jelinek et al., 1977) for WikiText, Longbench score (Bai et al., 2024), and retrival accuracy for the Needlein-A-Haystack task (Kamradt, 2023). For text-to-video models, following Zhao et al. (2025), we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIPTemp (CLIP-T) (Liu et al., 2024b) to measure the text-video alignment; VQA-a and VQA-t to assess the video aesthetic and technical quality, and Flow-score (FScore) for temporal consistency (Wu et al., 2023). For text-to-image models, generated images are compared with the images in the COCO dataset in three aspects: FID (Heusel et al., 2017) for fidelity evaluation, Clipscore (CLIP) (Hessel et al., 2021) for text-image alignment, and ImageReward (IR) (Xu et al., 2024) for human preference. Speed and sparsity metric. We use TOPS (tera operations per second) to evaluate the speed of sparse attention methods. Specifically, TOPS = O(attn)/t, where O(attn) represents the total number of operations in standard attention computation, and is the latency from given (Q, K, ) to the output of attention. Note that this speed metric is completely fair. This is because the O(attn) is fixed for set of inputs, and then the speed is determined by t, which includes the time spent predicting the sparse region of the attention map. We define Sparsity as the proportion of the Matmul of QiKj plus Vj that are skipped relative to the total number of QiKj plus Vj in full attention required. Implementation and Hyper-parameters. We implement our method using CUDA. As discussed in Sec. 3.6, we need to determine l1, l2 for models. We use (l1 = 0.08, l2 = 0.09) for Llama3.1, (l1 = 0.05, l2 = 0.06) for CogvideoX and Mochi, and (l1 = 0.07, l2 = 0.08) for Stable-Diffusion3.5 and Flux. Baselines. Currently, sparse attention methods applicable across different model types are limited. We choose blocksparse MInference (Jiang et al., 2024) and FlexPrefill (FlexPrefill, 2025) as our baselines. To vary the sparsity of these baselines, we use 30% and 70% for MInference, and use γ = 0.95 and 0.99 for FlexPrefill according to their paper. Figure 6. Comparison and examples Stable-Diffusion3.5. The sparsity of SpargeAttn, MInference and FlexPrefill is 0.38, 0.3, and 0.4 on Flux and 0.31, 0.3, and 0.35 on Stable-Diffusion3.5. on Flux 4.2. Quality and Efficiency Evaluation End-to-end metrics. We assess the end-to-end metrics of various models using SpargeAttn compared to using full attention and baselines. Table 1 shows the results. We can 6 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Table 1. End-to-end metrics across text, image, and video generation models. indicates an inability to generate results for evaluation. The speed and sparsity are the average for each layer in the model in real generation tasks described in Sec. 4.1. The speed and sparsity of Llama3.1 are measured in the NeedleInAHaystack task with 128K sequence length. Model (seq len) Attention (Sparsity) Speed (TOPS) WikiText (Ppl.) Longbench InfiniteBench NIAH Llama3.1 (128K) Full-Attention Minference (0.5) FlexPrefill (0.5) Minference (0.3) FlexPrefill (0.42) SpargeAttn (0.54) 156.9 140.1 240.6 115.7 206.9 708.1 6.013 10.631 6.476 6.705 6.067 6.020 38.682 28.860 38.334 34.074 38.334 39. 0.6594 0.5152 0.6460 0.6532 0.6581 0.6638 Model (seq len) Attention (Sparsity) CogvideoX (17K) Mochi (22K) Full-Attention Minference (0.5) FlexPrefill (0.6) Minference (0.3) FlexPrefill (0.45) SpargeAttn (0.46) Full-Attention Minference (0.5) FlexPrefill (0.48) Minference (0.3) FlexPrefill (0.4) SpargeAttn (0.47) Model (seq len) Attention (Sparsity) Flux (4.5K) StableDiffusion3.5 (4.5K) Full-Attention Minference (0.5) FlexPrefill (0.48) Minference (0.3) FlexPrefill (0.41) SpargeAttn (0.38) Full-Attention Minference (0.5) FlexPrefill (0.37) Minference (0.3) FlexPrefill (0.35) SpargeAttn (0.31) Speed (TOPS) CLIPSIM CLIP-T VQA-a VQA-t 75.946 62.410 4.5034 63.525 8.8426 74.846 67.663 50.839 0.0043 42.956 0.7413 67.219 0.9976 0.9959 0.9926 0.9964 0.9917 0.9974 0.9990 0.9891 0.9898 0.9889 0.9909 0.9990 0.1819 0.1728 0.1523 0.1754 0.1564 0.1798 0.1725 0.1629 0.1667 0.1682 0.1677 0. 80.384 70.486 1.5171 77.326 7.7259 78.276 56.472 6.668 0.582 14.541 2.941 54.179 166.0 264.6 175.3 196.9 142.0 507.9 164.2 202.4 191.3 147.7 171.7 582.4 Speed (TOPS) 158.2 151.8 47.7 118.9 40.9 280.3 164.2 186.4 23.1 150.3 22.7 293.0 FID 166.103 180.650 443.928 170.221 405.043 163.982 166.101 348.930 350.497 337.530 348.612 166.193 CLIP 31.217 30.235 18.3377 31.001 19.5591 31.448 32.007 18.3024 18.447 18.099 18.147 32.114 0.907 0.832 0.858 0.870 0.878 0. FScore 5.342 2.808 1.652 3.742 2.089 5.030 1.681 0.653 0.833 1.807 IR 0.8701 0.4084 -2.2657 0.7701 -2.2362 0.9207 0.9699 -2.2678 -2.2774 -2.2647 -2.2756 0.9727 observe that our method incurs almost no end-to-end metric loss across various models compared to Full-Attention and surpasses baselines with various sparsity levels in terms of end-to-end accuracy. Fig. 6 and 7 show some visible comparison examples on Flux, Stable-Diffusion3.5, and Mochi, showing that SpargeAttn incurs no performance loss and outperforms baselines. Attention speed. Table 1 shows that our method achieves faster speeds compared to Full-Attention and surpasses baselines with various sparsity levels in terms of attention speed. Fig. 9 illustrates the kernel speeds of various methods across different sparsity, highlighting the efficiency of our approach and its significant advantage over other methods. End-to-end speedup. Table 2 shows the end-to-end latency on CogvideoX, Mochi, and Llama3.1 using SpargeAttn. Notably, SpargeAttn achieves 1.83x speedup on Mochi. Table 2. End-to-end generation latency using SpargeAttn. Model GPU Original SageAttn SpargeAttn CogvideoX Mochi 87 1897 Llama3.1 (24K) RTX4090 4.01 Llama3.1 (128K) 52 RTX4090 L40 68 1544 3.53 42s 53 1037 2.6 29.98 Table 3. Overhead of sparse block prediction in SpargeAttn. Sequence Len Prediction (ms) Full Attention (ms) Overhead 8k 16k 32k 64k 128k 0.251 0.487 0.972 2.599 8.764 6.649 26.83 106.68 424.24 1696. 3.78% 1.82% 0.911% 0.612% 0.516% 4.3. Ablation Study and key Insights Overhead of sparse block prediction. Table 3 compares the overhead of dynamic sparse block prediction in 7 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Figure 9. Kernel speed comparison under varying sparsity on RTX4090. Input tensors have sequence length of 22K and head dimension of 128. SpargeAttn+FA2/Sage/Sage2 means deploying our method on FlashAttention2, SageAttention or SageAttention2 (Zhang et al., 2024a). with only marginal difference in accuracy. Please see Appendix A.1 for more analysis and details. Table 4. Effect of permutation on sparsity and accuracy. Sim-q and Sim-k are the average block self-similarity of the query and key. Method Sim-q Sim-k L1 Sparsity Random Rowmajor Timemajor HilbertCurve 0.321 0.551 0.514 0.572 0.019 0.390 0.367 0.479 0.0414 0.0307 0.0342 0.0389 0.048 0.363 0.338 0. Table 5. Abalation of self-similarity judge. Method VQA-a VQA-t FScore W/o. self-sim Judge With self-sim Judge 34.664 54.179 44.722 67.219 1.138 1.807 Table 6. Analysis of sparsity from Mg and Mpv. Strategy Sparsity only Mg only Mpv Mg +Mpv 51.2% 27.7% 54% Ablation of self-similarity judge We ablate the effect of the self-similarity judge on Mochi. As shown in Table 5, we find that self-similarity judge can guarantee end-to-end accuracy. Please see Appendix A.2 for more analysis. Analysis of sparsity from Mg and Mpv. Table 6 shows the sparsity when only using Mg, only using Mpv, and using Mg+Mpv on Llama3.1 in NeedleInAHaystack task with 128K sequence length. SpargeAttn enhance the LLM performance. From Table 1, Fig. 8 and 10, we observe that SpargeAttn enhances LLM performance in long-context tasks. This improvement may result from the fact that sparse attention helps the LLM focus on more relevant information. Sparsity increases with sequence length. As shown in Table 7, we find that on Llama3.1, sparsity increases with Figure 7. Comparison examples on Mochi. The sparsity of SpargeAttn, MInference and FlexPrefill is 0.47, 0.3, and 0.4. Figure 8. NeedleInAHaystack comparison example on Llama3.1. The sparsity of SpargeAttn, MInference, and FlexPrefill is 0.5, 0.5, and 0.54. SpargeAttn compared with attention execution latency. The results indicate that the prediction overhead is minimal compared to attention, particularly for longer sequences. Effect of Hilbert Curve permutation. We evaluate the impact of Hilbert Curve permutation on Mochi by comparing three metrics: average block similarity across blocks of query or key, L1 error defined in Sec. 3.6, and sparsity. Table 4 shows that the HilbertCurve permutation consistently achieves superior block self-similarity and sparsity, 8 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Table 7. Sparsity increases with sequence length under constant accuracy bound on Llama3.1. Sequence Len 8K 16K 24K 48K 128K Sparsity 6.8% 26.4% 35.7% 49.8% 54% sequence length. This suggests that the longer contexts, the higher speedup of SpargeAttn can achieve. 5. Conclusion In this paper, we propose SpargeAttn, universal sparse and quantized attention that executes attention efficiently and accurately for any input. Our method uses two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that SpargeAttn accelerates diverse models, including language, image, and video generation models, without sacrificing end-to-end metrics."
        },
        {
            "title": "References",
            "content": "Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, 2024. Bao, F., Xiang, C., Yue, G., He, G., Zhu, H., Zheng, K., Zhao, M., Liu, S., Wang, Y., and Zhu, J. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Deng, Y., Song, Z., and Yang, C. Attention is naturally sparse with gaussian distributed input. arXiv preprint arXiv:2404.02690, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. FlexPrefill. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. In International Conference on Learning Representations, 2025. Fu, T., Huang, H., Ning, X., Zhang, G., Chen, B., Wu, T., Wang, H., Huang, Z., Li, S., Yan, S., et al. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. Gao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and Choi, Y. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jelinek, F., Mercer, R. L., Bahl, L. R., and Baker, J. K. Perplexitya measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63S63, 1977. Jiang, H., LI, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Kamradt, G. Llmtest needle in haystack-pressure testing llms. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. 9 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The In International Conference on efficient transformer. Learning Representations, 2020. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Liu, H., Zaharia, M., and Abbeel, P. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024a. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22139 22149, 2024b. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. Milakov, M. and Gimelshein, N. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Pagliardini, M., Paliotta, D., Jaggi, M., and Fleuret, F. Fast attention over long sequences with dynamic sparse flash attention. Advances in Neural Information Processing Systems, 36:5980859831, 2023. Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D. Sparq attention: Bandwidthefficient LLM inference. In Forty-first International Conference on Machine Learning, 2024. Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., and Lin, W. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20144 20154, 2023. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. Infllm: Training-free longcontext extrapolation for llms with an efficient context In The Thirty-eighth Annual Conference on memory. Neural Information Processing Systems, 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024b. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In The International Conference on Learning Representations, 2025. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10819 10829, 2022. Singhania, P., Singh, S., He, S., Feizi, S., and Bhatele, A. Loki: Low-rank keys for efficient sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Stability AI. diffusion https://stability.ai/news/ 3.5. introducing-stable-diffusion-3-5, 2023. Introducing stable Team, G. Mochi 1. https://github.com/ genmoai/models, 2024. 10 Yuan, Z., Zhang, H., Pu, L., Ning, X., Zhang, L., Zhao, T., Yan, S., Dai, G., and Wang, Y. DiTFastattn: Attention compression for diffusion transformer models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization, 2024a. URL https://arxiv.org/abs/2411.10958. SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Zhang, J., Wei, J., Zhang, P., Chen, J., and Zhu, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations, 2025. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M., Han, X., Thai, Z., Wang, S., Liu, Z., and Sun, M. Bench: Extending long context evaluation beyond 100K tokens. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, 2024b. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhao, T., Fang, T., Huang, H., Liu, E., Wan, R., Soedarmadji, W., Li, S., Lin, Z., Dai, G., Yan, S., Yang, H., et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. In International Conference on Learning Representations, 2025. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Zhu, Q., Duan, J., Chen, C., Liu, S., Li, X., Feng, G., Lv, X., Cao, H., Chuanfu, X., Zhang, X., et al. Sampleattention: Near-lossless acceleration of long context llm inference with adaptive structured sparse attention. arXiv preprint arXiv:2406.15486, 2024. 11 SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference A. Appendix A.1. Detailed Explain and results of permutation ablation We use five distinct prompts and pre-searched hyperparameters with l1 = 0.05, l2 = 0.06 on both CogvideoX and Mochi models. The permutation are performed separately in attention operation for Q, K, after position embedding. To retain the original order of the input sequence, an inverse permutation is performed on the output of attention; for models using visual-language joint self-attention(e.g., CogvideoX), we only permute the visual tokens. When evaluating block self-similarity, we choose block size of 128 for query and 64 for key, which aligns with our kernel implementation. The precision metric(L1) is evaluated using FlashAttention2 output as ground truth. We choose different permutation methods to compare their impact on the performance of attention operations. Given 3D visual token tensor with shape d, where = W . The permutation methods and their detailed descriptions are shown in Table 8. d, the permutation finally results in tensor with shape H Method Detailed Description Random Rowmajor Columnmajor Timemajor HilbertCurve Random permutation of tokens, the order is recorded to perform inverse permutation. Permutation following row-major order. Tokens are continuous along the dimension. Permutation following column-major order. Tokens are continuous along the dimension. Permutation following time-major order. Tokens are continuous along the dimension. Permutation following Hilbert curve. Table 8. The detailed description of different permutation methods. Detailed results of permutation ablation for the CogvideoX and Mochi models are presented in Table 9. The HilbertCurve permutation consistently achieves superior block self-similarity and sparsity, with only marginal loss in precision. This suggests that the HilbertCurve permutation effectively enhances block self-similarity and sparsity. It is worth noting that the random permutation retains the precision metrics but sacrifices sparsity. This indicates that our algorithm has the property of dynamically adjusting and robust to complex token sequences. Method Random Rowmajor Columnmajor Timemajor HilbertCurve Sim-q Sim-k Precision(L1) Sparsity CogvideoX Mochi CogvideoX Mochi CogvideoX Mochi CogvideoX Mochi 0.502 0.676 0.633 0.692 0.709 0.321 0.551 0.547 0.514 0.572 0.025 0.435 0.335 0.479 0.523 0.019 0.390 0.394 0.367 0.479 0.0348 0.0265 0.0274 0.0294 0. 0.0414 0.0307 0.0342 0.0342 0.0389 0.027 0.242 0.198 0.238 0.265 0.048 0.363 0.366 0.338 0.392 Table 9. The impact of permutation on CogvideoX and Mochi models. Sim-q is the block self-similarity of the query, and Sim-k is the block self-similarity of the key. A.2. Ablation Study of Self-Similarity Judge To investigate the impact of the self-similarity judge on attention performance, we follow the experimental setting outlined in Sec. A.1 and conduct an ablation study by removing the self-similarity judge. In most cases, the presence of highly localized patterns results in minimal number of non-self-similar blocks, leading to only minor differences in precision and sparsity when averaging across all tensor cases. To obtain more meaningful and interpretable insights, we specifically analyze cases where the precision difference is statistically significant. judge (precision error with the self-similarity judge) and L1no To this end, we apply threshold-based selection criterion, retaining only those cases where the absolute difference between L1sim judge (precision error without the self-similarity judge) exceeds 0.05. This criterion results in approximately 2% of the tensor cases being retained for further analysis. We employ precision (L1 error) and sparsity as evaluation metrics to assess the influence of the self-similarity judge on the attention output. The results are summarized in Table 10. The findings demonstrate that the self-similarity judge effectively mitigates extreme precision loss while introducing only marginal reduction in sparsity. Furthermore, we observe that significant proportion of cases exhibiting notable differences SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Table 10. Impact of the self-similarity judge on the accuracy and sparsity of attention. w/ judge w/o judge filter w/ judge filter w/o judge CogvideoX Mochi CogvideoX Mochi CogvideoX Mochi CogvideoX Mochi 0.0316 0.199 0.0343 0.301 0.0325 0.203 0.0365 0.305 0.0843 0. 0.0555 0.371 0.214 0.275 0.154 0."
        },
        {
            "title": "Method",
            "content": "L1 error Sparsity originate from the Random permutation category in the CogvideoX model. This observation further highlights the role of the self-similarity judge in enhancing the models robustness to complex token sequences while maintaining high precision. Figure 10. NeedleInAHaystack comparison example on Llama3.1. The sparsity of SpargeAttn, MInference, and FlexPrefill is 0.36, 0.3, and 0.3. Table 11. End-to-end metrics on Llama3.1 in the NeedleInAHaystack task with 16-28K sequence lengths. Model (seq len) Llama3.1 (24K) Attention (Sparsity) Full-Attention Minference (0.5) FlexPrefill (0.6) Minference (0.3) FlexPrefill (0.3) SpargeAttn (0.36) Speed (TOPS) 156.9 122.5 179.6 102.3 117.6 443.6 NIAH 0.838 0.635 0.776 0.652 0.797 0.863 A.3. Additional Experiments In this section, we present additional experimental results further to evaluate the performance of SpargeAttn compared to baselines. Fig. 10 and 11 show the results on Llama3.1 in the NeedleInAHaystack task with 16-28K sequence length. Fig 11 shows visible comparison example on Mochi. SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference Figure 11. Comparison examples on Mochi. The sparsity of SpargeAttn, MInference and FlexPrefill is 0.47, 0.3, and 0.4."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University",
        "University of California, Berkeley"
    ]
}