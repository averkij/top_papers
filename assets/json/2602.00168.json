{
    "paper_title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation",
    "authors": [
        "Ranjan Sapkota",
        "Manoj Karkee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments."
        },
        {
            "title": "Start",
            "content": "YOLOE-26: INTEGRATING YOLO26 WITH YOLOE FOR REAL-TIME OPEN-VOCABULARY INSTANCE SEGMENTATION Cornell University, Biological & Environmental Engineering, Ithaca, NY 14850, USA rs2672@cornell.edu Ranjan Sapkota Manoj Karkee February 3,"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper presents YOLOE-26, unified framework that integrates the deployment-optimized YOLO26 architecture with the open-vocabulary learning paradigm of YOLOE for real-time openvocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracyefficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE26 provides practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments. Keywords YOLOE-26 YOLO26 YOLOv26 Open-Vocabulary Object Detection NMS-free Inference 6 2 0 2 9 2 ] . [ 1 8 6 1 0 0 . 2 0 6 2 : r Figure 1: Birds-eye diagram: simplified architecture of YOLOE-26 for open-vocabulary instance segmentation. YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE"
        },
        {
            "title": "INTRODUCTION",
            "content": "Object detection and instance segmentation constitute two of the most fundamental problems in computer vision, enabling machines to localize, recognize, and delineate objects within images and video streams. These capabilities form the core of numerous real-world applications, including autonomous driving, robotics, intelligent surveillance, medical image analysis, precision agriculture, and smart manufacturing [1, 2, 3, 4]. In such applications, real-time inference, low latency, and deployment efficiency are often as critical as recognition accuracy. The You Only Look Once (YOLO) family has fundamentally shaped modern real-time object detection by introducing unified, single-stage detection pipelines. YOLOv1 reframed detection as an end-to-end regression task, enabling unprecedented inference speed [5]. YOLOv2 and YOLOv3 extended this paradigm through anchor box clustering, multi-scale training, deeper Darknet backbones, and residual feature fusion, significantly improving robustness and small-object detection [6, 7]. Subsequent versions emphasized efficiency and stability, with YOLOv4 adopting CSPDarknet and Mish activation [8], YOLOv5 transitioning to PyTorch with modern training pipelines [9], YOLOv6 introducing EfficientRep and anchor-free heads [10], and YOLOv7 leveraging re-parameterized ELAN architectures [11]. Recent models reflect shift toward end-to-end and attention-aware designs, including anchor-free YOLOv8 [12, 13], PGI-enhanced YOLOv9 [14], NMS-free YOLOv10 [15], extended-task YOLO11, and attentionand graphbased YOLOv12 and YOLOv13 [16, 17]. The release of YOLOv26 in 2025 represents culmination of these trends. Rather than increasing architectural complexity, YOLOv26 adopts deployment-first philosophy centered on efficiency, robustness, and simplicity. Key innovations include native NMS-free end-to-end predictor, removal of Distribution Focal Loss (DFL) for faster inference, and the introduction of the MuSGD optimizer for stable and rapid convergence [18]. These design choices significantly reduce end-to-end latency and improve performance on low-power CPUs and edge devices, while supporting multiple vision tasks such as detection, instance segmentation, pose estimation, oriented detection, and classification. Despite the substantial gains in accuracy-latency efficiency demonstrated by successive YOLO generations in Fig. 2(a,b), including the end-to-end, NMS-free design of YOLOv26, these models remain inherently constrained by closedvocabulary formulation in which object categories are fixed at training time and cannot adapt to unseen concepts at inference. This limitation poses significant challenge in open-world scenarios, where object categories continuously evolve and retraining is impractical. Recent advances in foundation models and vision-language learning have given rise to open-vocabulary object detection and instance segmentation, enabling models to recognize unseen categories through text prompts, visual examples, or prompt-free inference [19, 20]. However, many existing open-vocabulary approaches rely on transformer-heavy architectures or large language models, resulting in high computational cost, slow inference, and limited deployability on edge hardware [20]. The YOLOE (You Only Look Once - Everything) paradigm addresses this gap by extending the YOLO framework with embedding-based classification and unified support for text prompts, visual prompts, and prompt-free operation, enabling seeing anything while preserving YOLOs hallmark efficiency [21]. By aligning visual features with semantic embeddings instead of fixed class logits, YOLOE introduces foundation-model-inspired open-vocabulary learning into real-time detection and segmentation. In this paper, we present comprehensive evaluation of YOLOE-26,which combines the NMS-free, end-to-end detection pipeline of YOLOv26 with the open-vocabulary learning mechanisms of YOLOE, enabling real-time instance segmentation across diverse prompting paradigms(Source Link). By combining the deployment-optimized design of YOLOv26 with the open-vocabulary capabilities of YOLOE, YOLOE-26 establishes unified and practical framework for real-time, open-world instance segmentation. This study systematically analyzes its performance across textprompted, visual-prompted, and prompt-free settings, highlighting its accuracyefficiency trade-offs and suitability for next-generation edge and open-world vision systems. 1.1 Background and Motivation Convolutional neural network (CNN)based object detection frameworks, particularly the YOLO family, have dominated real-time visual perception for nearly decade due to their unified architectures, high inference speed, and strong accuracyefficiency trade-offs [22, 23]. From YOLOv1 to YOLOv26, these models progressively transformed object detection from grid-based regression into highly optimized end-to-end pipelines. Early variants (YOLOv1YOLOv3) relied on dense grid predictions with fixed-category classification heads, which limited semantic flexibility and required careful anchor and scale tuning [5, 6, 7]. Later generations incorporated multi-scale feature pyramids, deeper backbones, and improved loss formulations to enhance robustness across object sizes, establishing YOLO as the de facto standard for real-time detection in resource-constrained environments [8]. 2 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 As YOLO matured, architectural focus shifted toward deployment robustness and pipeline simplification. YOLOv5YOLOv7 introduced PyTorch-based implementations, anchor-free detection heads, re-parameterizable convolutional blocks, and efficient feature aggregation mechanisms, significantly reducing training and inference complexity [9, 10, 11]. More recent versions, including YOLOv8 and YOLOv9, emphasized decoupled heads, task-aligned optimization, and multi-task perception, extending YOLOs applicability to instance segmentation, pose estimation, and panoptic understanding [12, 14]. This evolution culminated in YOLOv10 and YOLOv26, which eliminated heuristic post-processing such as non-maximum suppression (NMS), enabling fully end-to-end detection with reduced latency and improved determinism [15, 18]. As illustrated in Fig. 2(a), YOLOv26 achieves superior accuracylatency balance compared to earlier YOLO variants and other real-time detectors, while Fig. 2(b) highlights its advantage in end-to-end pipeline efficiency relative to transformer-based real-time baselines. Figure 2: Performance comparison of YOLO26 under TensorRT FP16 on an NVIDIA T4 GPU (Source Link). (a) COCO mAP(5095) versus inference latency (ms/image), comparing YOLO26 with earlier YOLO versions and other real-time detectors, highlighting its improved accuracyspeed trade-off. (b) COCO mAP(5095) versus end-to-end latency, comparing YOLO26 with YOLOv10 and RT-DETR variants, illustrating its advantage in overall pipeline efficiency. Despite these architectural advances, CNN-based YOLO detectors, including YOLOv26 remain fundamentally constrained by closed-set learning paradigm, where object categories are predefined during training and fixed at inference time [24, 25]. In real-world scenarios such as autonomous robotics, agricultural monitoring, and industrial inspection, object categories frequently evolve, making repeated data collection, retraining, and redeployment impractical. These limitations hinder adaptability in open-world environments and motivate the transition toward open-vocabulary vision systems. Open-vocabulary detection and instance segmentation methods attempt to overcome closed-set constraints by leveraging large-scale visionlanguage pretraining and semantic embeddings [26, 27]. Models such as GLIP, Grounding DINO, OWL-ViT, DINO-X, X-Decoder, OpenSeeD, and SEEM demonstrate strong zero-shot and open-set capabilities by aligning visual regions with textual or multimodal representations. However, these approaches typically rely on transformer-heavy backbones, dense cross-modal attention, and external language models, resulting in high computational cost, increased inference latency, and large memory footprints. Such characteristics severely limit real-time performance and edge deployability, particularly for safety-critical and low-power applications. 3 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE YOLOE represents critical step toward integrating open-vocabulary learning into efficient YOLO-style architectures [21]. By introducing embedding-based classification and unified support for text prompts, visual prompts, and promptfree inference, YOLOE enables open-vocabulary detection and segmentation within single model [28, 29, 30]. Nevertheless, early YOLOE designs still exhibit limitations related to prompt handling efficiency, scalability across deployment scenarios, and full exploitation of end-to-end detection pipelines. As reflected in Fig. 3, while YOLOE improves open-vocabulary performance compared to prior YOLO-World variants, challenges remain in balancing training cost, inference efficiency, and real-world deployability. Figure 3: Comparison of performance, training cost, and inference efficiency between YOLOE and advanced YOLOWorldv2 in terms of open text prompts. LVIS AP is evaluated on minival set and FPS w/ TensorRT and w/ CoreML is measured on T4 GPU and iPhone 12, respectively. The results highlight our superiority. (Source: YOLOE paper [21] The motivation behind YOLOE-26 is to systematically address these limitations by tightly integrating YOLOv26s NMS-free, end-to-end detection framework with the open-vocabulary learning mechanisms of YOLOE. By unifying deployment-efficient CNN-based detection with lightweight vision-language embedding strategies, YOLOE-26 enables text-prompted, visual-prompted, and prompt-free instance segmentation without sacrificing real-time performance. This design positions YOLOE-26 as practical solution for dynamic, open-world vision applications, including robotics, autonomous systems, surveillance, and precision agriculture, where both semantic flexibility and deployment efficiency are essential."
        },
        {
            "title": "2 YOLOE-26 Architecture Overview",
            "content": "2.1 Core YOLO26 Architectural Backbone and End-to-End Design YOLOE-26 is unified architecture that tightly integrates the deployment-efficient, NMS-free design of YOLOv26 with the open-vocabulary learning mechanisms introduced in YOLOE. As illustrated in Fig. 4, the model follows the canonical YOLO pipeline backbone, neck, and task-specific heads while replacing the conventional closed-set classification head with semantic embedding formulation that supports open-world instance segmentation. YOLOv26 Backbone and Feature Extraction: At its core, YOLOE-26 inherits the convolutional backbone of YOLOv26, which is designed for efficient multi-scale feature extraction across diverse hardware platforms. Given an input image R3HW , the backbone applies hierarchy of convolutional layers to extract feature maps at multiple resolutions. These features encode both low-level spatial details and high-level semantic context, which are essential for detecting objects of varying sizes. Compared to earlier YOLO variants, YOLOv26 emphasizes simplified convolutional blocks and optimized gradient flow, reducing computational overhead while preserving representational capacity. Neck: PAN/FPN-Style Feature Aggregation: The extracted backbone features are passed to PAN/FPN-style neck, which aggregates information across scales. Let {P3, P4, P5} denote feature maps at increasing semantic levels and decreasing spatial resolutions. The neck performs top-down and bottom-up fusion through upsampling, concatenation, and convolution operations, ensuring that each detection point has access to both fine-grained localization cues and global semantic information. This multi-scale aggregation is particularly important for instance segmentation, where precise object boundaries must be inferred alongside object identity. 4 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 Figure 4: Architectural diagram of YOLOE-26 for open-vocabulary instance segmentation. The upper part illustrates the core YOLOv26 end-to-end detection and segmentation pipeline, while the lower part depicts the YOLOE ([21]) components that enable text-prompted, visual-prompted, and prompt-free open-vocabulary learning. End-to-End Regression and Segmentation Heads: For each anchor point (or grid location) in the aggregated feature maps, YOLOE-26 employs multiple task-specific heads. The regression head predicts bounding box parameters, typically encoded as offsets relative to the anchor point, enabling precise object localization. In parallel, the instance segmentation head follows the prototype-based design common to modern YOLO segmentation models. It produces set of global mask prototypes and per-instance mask coefficients, which are linearly combined to generate instancespecific segmentation masks. This design decouples spatial mask representation from instance prediction, achieving high efficiency and scalability. 5 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE NMS-Free End-to-End Detection: defining characteristic of YOLOv26, inherited by YOLOE-26, is the removal of non-maximum suppression (NMS). Traditional YOLO pipelines rely on NMS as post-processing step to eliminate redundant detections, introducing additional latency and heuristic complexity. YOLOv26 instead adopts an end-to-end training formulation that enforces consistent assignment between predictions and ground truth, allowing the network to learn mutual exclusivity directly. As result, the final predictions are obtained in single forward pass, improving determinism, reducing latency, and simplifying deployment an advantage that becomes increasingly important in open-vocabulary settings with large and dynamic category spaces. Object Embedding Head for Open-Vocabulary Learning: The most critical architectural modification introduced by YOLOE-26 is the replacement of the closed-set classification head with an object embedding head. Instead of predicting logits over fixed set of class labels, the object embedding head outputs semantic embedding vector for each anchor point. Formally, let RN denote the object embeddings produced for anchor points, where is the embedding dimension. These embeddings represent visual object instances in shared semantic space, enabling flexible matching with arbitrary category representations. Prompt Embeddings and Similarity-Based Classification: YOLOE-26 supports three prompting modes text prompts, visual prompts, and prompt-free inference by encoding all prompts into common embedding space. Given set of prompts, their embeddings are denoted as RCD. Category prediction is then formulated as similarity operation between object embeddings and prompt embeddings: Label = RN C, (1) where each entry represents the affinity between an anchor point and prompt. This formulation replaces fixed-category classification with flexible retrieval-style mechanism, enabling zero-shot recognition of unseen categories. Re-Parameterizable RegionText Alignment (RepRTA): To improve visualtextual alignment without incurring inference overhead, YOLOE-26 employs RepRTA during training. Text prompts are first encoded using pretrained text encoder, producing embeddings P. lightweight auxiliary network fθ refines these embeddings during training to better align with visual features. The refined embeddings interact with object embeddings via convolutional operations: Label = R(I K) (fθ(P))T , (2) where denotes intermediate feature maps and represents convolution kernels. After training, the auxiliary network is re-parameterized into the object embedding head, yielding new kernels : = R(fθ(P)) . (3) This transformation preserves the standard YOLO inference path, ensuring zero additional runtime cost. Semantic-Activated Visual Prompt Encoder (SAVPE): For visual prompting, YOLOE-26 introduces SAVPE, lightweight encoder that avoids transformer-heavy designs. SAVPE consists of semantic branch that extracts prompt-agnostic semantic features and an activation branch that generates prompt-aware weights from visual cues such as bounding boxes or masks. These components are aggregated to form visual prompt embeddings: = Concat(G1, . . . , GA), Gi = Wi:i+1 ST (i+1), i: (4) where denotes semantic features and represents activation weights. This grouped formulation achieves efficient prompt encoding with minimal computational overhead. Lazy RegionPrompt Contrast for Prompt-Free Inference: In the absence of explicit prompts, YOLOE-26 adopts Lazy RegionPrompt Contrast (LRPC) to identify and name objects efficiently. specialized prompt embedding Ps is trained to detect objectness, filtering relevant anchor points: Only the filtered set is matched against large built-in vocabulary, significantly reducing computational cost while maintaining accuracy. = {o > δ}. (5) 2.2 Unified Object Embedding Space As illustrated in Figure 5, central design principle of YOLOE-26 is the replacement of conventional closed-set classification with unified object embedding space that supports flexible, open-vocabulary reasoning. In traditional 6 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE Text Prompt Embedding Pt Image Features (Backbone + PAN) Object Embedding RD Visual Prompt Embedding Pv Similarity Matching Category + Mask Prompt-Free Vocabulary Ps Figure 5: Unified object embedding space in YOLOE-26. Visual features are encoded into object embeddings and matched with text prompts, visual prompts, or prompt-free vocabulary embeddings via similarity-based inference for open-vocabulary instance segmentation. YOLO detectors, each anchor point predicts probability distribution over fixed set of class labels using softmax or sigmoid-based classifier [31, 32, 33]. Such formulations tightly couple visual features to predefined categories, limiting generalization to unseen concepts. YOLOE-26 instead represents each detected instance through continuous semantic embedding, enabling category inference via similarity matching rather than explicit class prediction. Concretely, for each anchor point, the object embedding head outputs D-dimensional vector that encodes the visual appearance and semantic attributes of the underlying object [34, 35]. These object embeddings are learned jointly with the detection and segmentation tasks, ensuring alignment between spatial localization, mask prediction, and semantic representation. In parallel, category descriptions provided as text prompts, visual prompts, or prompt-free object descriptors are mapped into the same D-dimensional embedding space, producing set of prompt embeddings. Classification is then performed by computing the similarity typically via inner product or cosine similarity between object embeddings and prompt embeddings, yielding affinity scores that indicate the likelihood of each object belonging to given semantic category. This embedding-based formulation unifies multiple inference modes within single architecture. Text prompts allow users to specify object categories using natural language descriptions, visual prompts enable category specification through example regions or masks, and the prompt-free mode relies on learned objectness embeddings to retrieve category names from built-in vocabulary. Importantly, all three modes share the same object embedding head, eliminating the need for task-specific classification branches or external language models during inference. From an application perspective, the unified object embedding space enables zero-shot and open-world instance segmentation without retraining. New categories can be introduced at inference time by simply providing corresponding prompt embeddings, making YOLOE-26 well suited for dynamic real-world environments such as robotics, autonomous navigation, surveillance, and precision agriculture. By decoupling semantic reasoning from fixed label spaces while preserving the efficiency of YOLO-style detection, YOLOE-26 achieves practical balance between flexibility and real-time deployability. 2.3 Performance Evaluation of Text-/Visual-Prompted and Prompt-Free Open-Vocabulary Instance Segmentation This subsection presents comprehensive performance analysis of YOLOE-26 under two complementary openvocabulary inference paradigms: text/visual-prompted segmentation and prompt-free segmentation. Quantitative results are summarized in Table 1 and Table 2, respectively, both evaluated at resolution of 640 px using end-to-end (e2e) metrics on the minival benchmark. Together, these tables provide critical insights into the accuracyefficiency trade-offs of YOLOE-26 across model scales and prompting strategies, directly informing real-world deployment decisions. Table 1 reports performance when explicit semantic guidance is provided through text or visual prompts. clear and consistent scaling trend is observed across the YOLOE-26 model family, where increasing model capacity yields substantial gains in segmentation accuracy. In particular, YOLOE-26x-seg achieves the highest overall performance, reaching mAP5095 of 39.5 (text) and 36.2 (visual), alongside strong results across rare, common, and frequent category splits. This highlights the effectiveness of the unified object embedding space and prompt-aware alignment mechanisms in handling long-tailed category distributions, which are common in open-world scenarios. From an 7 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE application perspective, such performance is especially valuable in domains like robotics, surveillance, and precision agriculture, where rare object instances (e.g., unusual tools, uncommon vehicle types, or early-stage crop anomalies) must be segmented reliably based on semantic descriptions rather than fixed labels. Mid-scale models such as YOLOE-26m-seg and YOLOE-26l-seg offer compelling balance between accuracy and computational cost. For instance, YOLOE-26l-seg delivers mAP5095 values exceeding 36 with fewer than 90 FLOPs (B), making it well suited for edge GPUs and embedded accelerators used in autonomous drones, mobile robots, and industrial inspection systems. In contrast, the nano and small variants prioritize efficiency, achieving respectable segmentation accuracy with significantly lower parameter counts and FLOPs, which is critical for deployment on low-power devices and real-time video analytics pipelines. Table 1: YOLOE-26 open-vocabulary instance segmentation performance using text and visual prompts at 640 px resolution. Results are reported on the minival set with end-to-end (e2e) evaluation. Rare (r), common (c), and frequent (f) category splits follow standard open-vocabulary benchmarks. Models are trained on Objects365v1, GQA, and Flickr30k datasets. Model Size (px) Prompt Type mAP5095 (Text / Visual) mAP5095 (Text / Visual) mAPr / / (Text / Visual) YOLOE-26n-seg YOLOE-26s-seg YOLOE-26m-seg YOLOE-26l-seg YOLOE-26x-seg 640 Text / Visual 640 Text / Visual 640 Text / Visual 640 Text / Visual 640 Text / Visual 23.7 / 20.9 29.9 / 27.1 35.4 / 31.3 36.8 / 33.7 39.5 / 36. 24.7 / 21.9 30.8 / 28.6 35.4 / 33.9 37.8 / 36.3 40.6 / 38.5 20.5 / 17.6 23.9 / 25.1 31.1 / 33.4 35.1 / 37.6 37.4 / 35.3 24.1 / 22.3 29.6 / 27.8 34.7 / 34.0 37.6 / 36.2 40.9 / 38.8 26.1 / 22.4 33.0 / 29.9 36.9 / 33.8 38.5 / 36.1 41.0 / 38.8 Params FLOPs (M) 4.8 13.1 27.9 32.3 69.9 (B) 6.0 21.7 70.1 88.3 196.7 Table 2: YOLOE-26 prompt-free open-vocabulary instance segmentation performance at 640 px resolution. Results are reported on the minival set with end-to-end (e2e) evaluation. Prompt-free models use built-in vocabulary (e.g., RAM++ tag set) and require no user-provided text or visual prompts. Models are trained on Objects365v1, GQA, and Flickr30k datasets. Model Size mAP5095 mAP50 Params FLOPs (e2e) (px) (e2e) (M) (B) YOLOE-26n-seg-pf YOLOE-26s-seg-pf YOLOE-26m-seg-pf YOLOE-26l-seg-pf YOLOE-26x-seg-pf 640 640 640 640 640 16.6 21.4 25.7 27.2 29. 22.7 28.6 33.6 35.4 38.7 6.5 16.2 36.2 40.6 86.3 15.8 35.5 122.1 140.4 314.4 Table 2 evaluates the prompt-free setting, where no user-provided text or visual cues are available and the model relies on built-in vocabulary for object discovery. As expected, absolute performance is lower than in the prompted setting, reflecting the increased difficulty of unconstrained open-world segmentation. Nevertheless, YOLOE-26x-seg-pf again demonstrates the strongest performance, achieving mAP50-95 of 29.9 and mAP50 of 38.7, indicating robust object discovery capability without external prompts. This mode is particularly relevant for large-scale image or video understanding tasks such as autonomous exploration, content indexing, and scene parsing where manual prompt specification is impractical. Importantly, the prompt-free results exhibit the same monotonic scaling behavior as the prompted setting, confirming that YOLOE-26s architectural design generalizes across inference modes. Smaller prompt-free models, such as YOLOE-26n-seg-pf and YOLOE-26s-seg-pf, provide lightweight solutions for continuous background monitoring and edge-based perception, while larger variants enable richer semantic coverage in compute-rich environments. Overall, the combined analysis of Table 1 and Table 2 demonstrates that YOLOE-26 offers flexible and scalable framework for real-world open-vocabulary instance segmentation, supporting both guided and unguided perception with strong accuracyefficiency trade-offs suitable for diverse deployment scenarios."
        },
        {
            "title": "3 OPEN-VOCABULARY PROMPTING MECHANISMS",
            "content": "A defining capability of YOLOE-26 is its support for open-vocabulary instance segmentation through multiple prompting mechanisms [36, 37, 38]. Unlike conventional YOLO-based detectors that rely on fixed set of class logits learned during training, YOLOE-26 decouples semantic category reasoning from closed-set classification. Instead, object recognition is formulated as similarity matching problem between learned object embeddings and prompt embeddings derived from text, visual examples, or built-in vocabulary. This section describes the three complementary 8 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 prompting mechanisms supported by YOLOE-26 text-prompted, visual-prompted, and prompt-free inference and explains how they collectively enable flexible, real-time, and deployment-friendly open-world perception. 3.1 Text-Prompted Instance Segmentation Text prompting allows users to specify target objects using natural language descriptions such as person, bus, or red apple. In YOLOE-26, text prompts are encoded into semantic embeddings and aligned with object embeddings predicted at each anchor point. The resulting similarity scores determine both category assignment and instance mask generation. To achieve efficient visualtextual alignment, YOLOE-26 adopts the Re-parameterizable RegionText Alignment (RepRTA) strategy. During training, RepRTA introduces lightweight auxiliary network that refines pretrained text embeddings to better align with object embeddings. Importantly, this auxiliary network is re-parameterized into the object embedding head after training, yielding zero additional inference cost. This design preserves the speed and determinism of YOLOv26 while enabling open-vocabulary reasoning. Algorithm 1: Text-Prompted Instance Segmentation Input: Image I, text prompts {t1, . . . , tC } Output: Instance masks and categories 1. Extract multi-scale features using the YOLOv26 backbone and PAN. 2. Predict object embeddings RN D. 3. Encode text prompts as embeddings Pt RCD. 4. Compute similarity . 5. Assign categories and generate instance masks. Example usage with text prompts: from ultralytics import YOLO model = YOLO ( \" yoloe -26 - seg . pt \" ) names = [ \" person \" , \" bus \" ] model . set_classes ( names , model . get_text_pe ( names ) ) results = model . predict ( \" path / to / image . jpg \" ) results [0]. show () 3.2 Visual-Prompted Instance Segmentation Text descriptions are not always sufficient to specify objects precisely, especially in domains such as agriculture, medical imaging, or industrial inspection. YOLOE-26 therefore supports visual prompting, where users provide example bounding boxes or masks to define target objects. This capability is enabled by the Semantic-Activated Visual Prompt Encoder (SAVPE), which consists of two lightweight branches: (i) semantic branch that extracts prompt-agnostic visual features, and (ii) an activation branch that encodes visual cues into prompt-aware weights. These branches are aggregated to form compact visual prompt embedding aligned with object embeddings, enabling efficient matching with minimal computational overhead. Algorithm 2: Visual-Prompted Instance Segmentation Input: Image I, visual prompts (boxes or masks) Output: Instance masks of visually similar objects 1. Extract multi-scale image features. 2. Encode visual prompts using SAVPE. 3. Generate visual prompt embedding Pv. 4. Match Pv with object embeddings O. 5. Predict instance masks for matched objects. Example usage with visual prompts: 9 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 import numpy as np from ultralytics import YOLO from ultralytics . models . yolo . yoloe import YOLOEVPSegPredictor model = YOLO ( \" yoloe -26 - seg . pt \" ) visual_prompts = dict ( bboxes = np . array ([[221.5 , 405.8 , 344.9 , 857.5] , [120 , 425 , 160 , 445]]) , cls = np . array ([0 , 1]) ) results = model . predict ( \" path / to / image . jpg \" , visual_prompts = visual_prompts , predictor = YOL OEVP Seg Predic tor ) results [0]. show () 3.3 Prompt-Free Instance Segmentation YOLOE-26 further supports prompt-free inference mode for fully autonomous perception. These models operate using built-in vocabulary of 4,585 categories derived from RAM++ tags. Rather than employing generative language models, YOLOE-26 introduces the Lazy RegionPrompt Contrast (LRPC) strategy, which first identifies object regions and then selectively retrieves category names only for relevant regions. Algorithm 3: Prompt-Free Instance Segmentation Input: Image Output: Instance masks and category names 1. Predict objectness embeddings to locate candidate regions. 2. Filter anchor points using threshold. 3. Lazily match filtered regions with the built-in vocabulary. 4. Assign category labels and generate instance masks. Example usage for prompt-free inference: from ultralytics import YOLO model = YOLO ( \" yoloe -26 - seg - pf . pt \" ) results = model . predict ( \" path / to / image . jpg \" ) results [0]. show () Together, these three prompting mechanisms unify text-guided, example-guided, and autonomous perception within single architecture. This versatility makes YOLOE-26 well suited for real-world deployment in robotics, autonomous systems, surveillance, industrial inspection, and precision agriculture, where object categories are dynamic and continuously evolving."
        },
        {
            "title": "4 TRAINING STRATEGY AND IMPLEMENTATION",
            "content": "This section describes the training strategy and practical implementation details of YOLOE-26, with emphasis on how open-vocabulary learning is integrated into YOLOv26-style real-time instance segmentation pipeline without sacrificing deployability. YOLOE-26 inherits the end-to-end, NMS-free efficiency of YOLOv26 while adopting the promptable open-vocabulary learning paradigm introduced in YOLOE [29]. The key idea is that the detector is trained not only to localize and segment objects, but also to produce semantic object embeddings that can be matched against prompt embeddings derived from text, visual cues, or built-in vocabulary. Unlike transformer-heavy open-vocabulary models that perform dense cross-attention between image tokens and text tokens at inference time, YOLOE-26 aims to 10 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 push most of the cross-modal alignment complexity into training, then re-parameterize modules so inference remains YOLO-like. 4.1 Datasets, Annotation Sources, and Supervision Signals Training data sources: YOLOE-26 is trained using large-scale detection and grounding datasets that collectively provide diverse object categories, language grounding, and rich visual variability. In practice, three public sources are commonly used: Objects365 (object detection with bounding boxes), GQA (grounding-style annotations aligned with language), and Flickr30k Entities (phrase grounding). These datasets provide complementary supervision: Objects365 contributes broad object diversity and dense bounding boxes; GQA and Flickr30k connect text phrases to regions, which is crucial for learning prompt alignment. Segmentation supervision: Since not all large-scale grounding/detection datasets provide high-quality instance masks, YOLOE-26 training typically uses pseudo-mask generation to produce segmentation targets. common approach is to generate instance masks from the provided bounding boxes using strong segmentation model (e.g., SAMfamily variants) and then refine the masks to reduce label noise. In implementation, the refinement step can include removing fragmented regions, suppressing out-of-box leakage, filtering small spurious components, and enforcing mask smoothness. The goal is not to replace human-annotated masks, but to create sufficiently accurate supervisory signals that enable training segmentation head at scale. Multi-source supervision schema: Let training sample consist of an image with set of annotated regions {(bi, mi, yi)}M i=1, where bi is bounding box, mi is an instance mask (ground-truth or pseudo-mask), and yi is semantic label. In grounding datasets, yi may originate from phrase rather than canonical class name; YOLOE26 therefore maps language phrases to prompt embeddings, allowing supervision to remain consistent even when vocabulary differs across datasets. Prompt-conditioned training targets: For text prompting, each semantic label yi is represented by text prompt embedding p(yi) RD. For visual prompting, the training sample additionally includes set of reference visual cues (e.g., box or mask) used to construct visual prompt embedding. For prompt-free training, the objective is to learn objectness and vocabulary retrieval without explicit user prompts. This is typically done in two stages: (i) learn specialized objectness prompt that identifies anchors corresponding to objects, and (ii) retrieve names from built-in vocabulary by matching only those candidate anchors. 4.2 Objective Functions and Optimization YOLOE-26 optimizes multi-task loss that couples localization, segmentation, and semantic alignment. The model predicts, for each anchor point (or anchor-free location), bounding box, mask representation, and an object embedding. Let RN denote the predicted object embeddings for anchor points and embedding dimension D. Let prompt embeddings be RCD for prompts. The similarity scores used for category assignment can be written as: = RN C, (6) where Sn,c indicates how well anchor matches prompt c. These scores replace conventional fixed-class logits in closed-set detectors. (i) Embedding-based classification loss: YOLOE-26 typically applies binary cross-entropy style objective over similarity scores, treating correct anchorprompt pairs as positives. Let tn,c {0, 1} denote whether anchor matches prompt c. The classification term can be expressed as: Lcls = 1 (cid:88) (cid:88) n=1 c=1 BCE(cid:0)σ(Sn,c), tn,c (cid:1), (7) where σ() is the sigmoid function. In practice, label assignment can follow task-aligned matching strategy similar to modern YOLO training, ensuring stable gradients even under large prompt sets. (ii) Bounding box regression loss: For localization, YOLOE-26 uses IoU-family losses (e.g., CIoU/GIoU) computed between predicted boxes ˆb and ground-truth boxes b: Lbox = 1 (cid:88) i=1 (cid:16) (cid:17) 1 IoU(ˆbi, bi) , (8) optionally with additional geometric penalties for aspect ratio and center distance. This term enforces tight spatial alignment and is essential for accurate instance masks. 11 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 (iii) Localization refinement loss: In segmentation-capable YOLO variants, distribution-based regression objectives are sometimes used for sub-bin precision. When included, the refinement term can be written as distributional focal loss (DFL) style objective over discretized offsets. In YOLOv26-style deployments, regression objectives may be simplified for speed; therefore, YOLOE-26 implementations often expose this term as configurable depending on whether the goal is maximal accuracy or maximal efficiency. (iv) Mask segmentation loss: YOLOE-26 employs YOLACT/YOLO-Seg style mask representation that predicts set of prototypes and per-instance mask coefficients. Let Mi be the predicted mask for instance and mi the target mask. standard choice is pixel-wise BCE: Lmask ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 BCE(Mi, mi), optionally combined with dice loss for robustness to class imbalance in foreground/background pixels. (v) Total loss. The overall objective is weighted sum: = λclsLcls + λboxLbox + λmaskLmask + λref Lref , (9) (10) where λ weights tune the balance between semantic alignment, localization, and segmentation quality. Optimization and schedules: Training is commonly staged to reduce compute and stabilize optimization across prompt modes. practical schedule is: (1) Text-prompt pretraining: learn strong regiontext alignment with RepRTA and large-scale grounding data. (2) Visual-prompt adaptation: fine-tune SAVPE (often freezing most of the model) using box/mask-based visual cues, which reduces training cost because only small prompt encoder is updated. (3) Prompt-free specialization: train an objectness prompt and enable lazy retrieval from large vocabulary, emphasizing efficiency and coverage rather than prompt specificity. Because RepRTA can be re-parameterized into the embedding head, the final inference graph can remain compact. For visual prompting, freezing most layers and updating only SAVPE is particularly beneficial: it reduces VRAM requirements, shortens training time, and preserves the pretrained detector representation. In practice, AdamW is often used for prompt encoder fine-tuning, while SGD-style optimizers may be used for large-scale pretraining depending on stability and throughput requirements. 4. Implementation Pipeline, Trainers, and Ultralytics Integration Model variants and operating modes YOLOE-26 is distributed in multiple scales (N/S/M/L/X) and in two operating families: (i) text/visual-prompt models (e.g., yoloe-26l-seg.pt) and (ii) prompt-free models (e.g., yoloe-26l-seg-pf.pt). Both families support inference, validation, training (fine-tuning), and export within the Ultralytics ecosystem, enabling consistent workflows from research to deployment. Fine-tuning on custom datasets Fine-tuning YOLOE-26 on custom segmentation dataset closely follows standard YOLO training, but requires prompt-aware trainer to handle embedding-based classification and prompt construction. For instance segmentation fine-tuning, practitioners typically use segmentation-specific trainer (e.g., YOLOEPESegTrainer) that ensures the mask branch, embedding head, and prompt modules are optimized consistently. Importantly, fine-tuning can be conducted either in closed-set style (fixed dataset categories) or in open-vocabulary style (dataset categories represented as prompts), depending on the desired deployment behavior. Detection-only fine-tuning from segmentation checkpoints When training detection model rather than segmentation, practical approach is to initialize detection configuration (YAML), load weights from segmentation checkpoint of the same scale, and then train using detection-specific trainer (e.g., YOLOEPETrainer). This reuses learned embeddings and localization features while discarding mask-specific parameters when unnecessary. Visual-prompt training efficiency distinctive implementation detail of YOLOE-style models is that visual-prompt models can be obtained by fine-tuning from trained text-prompt models. Since SAVPE is the main module requiring adaptation, one can freeze the entire backbone, neck, and most heads, and update only SAVPE-related layers. This significantly reduces compute, enabling short training runs that specialize the model for visual prompting. typical engineering workflow is: 1. Start from well-trained text-prompt model checkpoint. 2. (Optional) convert segmentation to detection graph for cheaper training. 3. Freeze all layers except the SAVPE module. 4. Train for small number of epochs on visual-prompt supervision. 12 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 5. Merge SAVPE back into the segmentation checkpoint if conversion was used. This workflow is particularly suitable for rapid iteration when new visual prompting behaviors are needed (e.g., one-shot part segmentation or instance retrieval in industrial scenes). Validation and prompt extraction In validation, prompt embeddings must be constructed consistently with training. For example, visual prompt evaluation may require extracting visual embeddings for each dataset category from reference set. Ultralytics-style APIs commonly support this with flags that automatically compute and cache category embeddings (e.g., loading visual prompt embeddings from dataset). This design reduces user burden and standardizes evaluation protocols across datasets. Export and deployment major benefit of YOLOE-26 is that deployment follows familiar YOLO export pathways (ONNX, TensorRT, CoreML). For text-prompted export, practitioners configure the prompt set before exporting, so the exported model contains the folded prompt representation. This results in an inference graph compatible with edge runtimes, without requiring external text encoders at deployment. Prompt-free models export directly like standard YOLO models because they do not require runtime prompt inputs. Practical usage patterns In real applications, prompt modes can be composed to maximize usability. common pattern is prompt-free discovery followed by prompted refinement: the system first identifies broad set of objects using the built-in vocabulary, then user or downstream agent specifies smaller set of target concepts through text or visual prompts to obtain precise instance masks. This hybrid workflow is effective in robotics, agricultural monitoring, and large-scale image/video analytics, where the object set evolves and the cost of repeated retraining is prohibitive. Overall, YOLOE-26 training and implementation are designed around deployment-first philosophy: large-scale open-vocabulary learning is achieved through prompt alignment objectives and staged specialization, while the final inference system remains lightweight, end-to-end, and compatible with standard YOLO acceleration toolchains."
        },
        {
            "title": "5 Conclusion and Future Roadmap",
            "content": "This paper presented systematic evaluation of YOLOE-26, unified framework that integrates the deployment-oriented, NMS-free, end-to-end design of YOLOv26 with the open-vocabulary, promptable learning paradigm introduced by YOLOE as the foundational version. The resulting model advances open-vocabulary image segmentation by enabling real-time instance segmentation under three complementary modes: text-prompted, visual-prompted, and prompt-free inference. In contrast to transformer-heavy visionlanguage models that often incur substantial latency and memory overhead, YOLOE-26 preserves the speed and determinism of the YOLO family through embedding-based similarity matching and re-parameterizable prompting components. Overall, YOLOE-26 provides practical balance between open-world semantic flexibility and edge-ready real-time segmentation, making it well aligned with high-impact application domains such as robotics, autonomous systems, precision agriculture, intelligent surveillance, medical imaging, and industrial inspection. Despite these strengths, important limitations remain that motivate clear roadmap for future research and engineering. First, prompt-free performance remains consistently below textand visual-prompted settings, reflecting the intrinsic difficulty of unconstrained open-world object discovery in large-vocabulary environments. Second, large-scale training depends on multi-source supervision and pseudo-mask generation, which can introduce label noise and reduce boundary precision for thin structures, heavily occluded objects, and fine-grained categories. Third, open-vocabulary generalization is sensitive to prompt phrasing, dataset bias, and long-tail semantics, and the unified embedding space may not always fully separate visually similar categories without stronger alignment constraints. Fourth, while inference is efficient, practical deployment under extreme compute and energy constraints still poses challenges, including quantization robustness, memory overhead for large vocabularies, and reliable confidence calibration for safety-critical applications. Future advancements of YOLOE-26, as illustrated in Figure 6, should prioritize fully autonomous open-vocabulary segmentation through agentic AI and AI agents that continuously improve perception in real-world environments. natural direction is to integrate YOLOE-26 into an agentic perception loop that performs prompt-free object discovery, automatic prompt refinement using contextual cues, uncertainty-aware re-prompting when predictions are ambiguous, and self-verification through temporal or multi-view consistency checks. Such an agentic learning pipeline can enable closed-loop semantic adaptation without repeated full retraining, supporting long-term autonomy in dynamic and evolving scenes. In parallel, incorporating lightweight visionlanguage reasoning modules can help generate more discriminative prompts, such as attribute-based or compositional descriptions, improving robustness to domain shift and long-tail categories. 13 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE YOLOE-26 Future Roadmap Edge-Efficient Open-Vocab Seg Reliable & Robust Prompting Agentic AI & Autonomous Perception Quantization-aware training (INT8/FP8) Prompt robustness: synonyms & paraphrases Perception agent: discover refine prompts Prompt caching & kernel folding (RepRTA) Uncertainty-aware thresholding & abstention Active vision: reframe, zoom, multi-view consistency Vocabulary indexing (ANN retrieval for prompts) Long-tail calibration: rare/common/frequent Memory for prompts: episodic + vector cache Streaming video: temporal mask propagation Domain shift handling: weather, blur, low-light Self-training loop: selective pseudo-labeling Distillation to tiny YOLOE-26 (N/S) Noisy pseudo-mask robust training (SAM-based) Tool use & pipelines: auto-annotate & audit Hardware-aware export: ONNX/TensorRT/CoreML Safety checks: false-positive control & QA Continual learning: new classes without forgetting Goal: Fully Real-Time Open-Vocabulary Instance Segmentation on Edge Figure 6: Future roadmap for YOLOE-26 and beyond. Dashed boxes summarize prioritized research directions across three coupled thrusts: (i) Edge-Efficient Open-Vocabulary Segmentation (deployment, compression, fast retrieval), (ii) Reliable & Robust Prompting (calibration, domain shift, noisy supervision), and (iii) Agentic AI & Autonomous Perception (prompt refinement, memory, continual learning). Dotted links highlight dependencies among efficiency, reliability, and agentic autonomy. From learning and systems perspective, several roadmap directions can further strengthen YOLOE-26 as generalpurpose open-vocabulary segmentation framework. Continual and federated open-vocabulary learning can enable incremental category expansion and personalized adaptation while mitigating catastrophic forgetting and preserving data privacy. Self-supervised and weakly supervised mask refinement strategies, including iterative pseudo-label cleanup, boundary-aware losses, and temporal consistency in video, can reduce reliance on costly manual annotations and improve pixel-level accuracy. Hierarchical and compositional embedding spaces can explicitly encode categories, attributes, and object parts, improving separability for visually similar classes and supporting scalable prompt-free retrieval. Finally, edge-first deployment optimization, including quantization-aware training, model distillation, prompt caching, and efficient indexing for large vocabularies, will be critical to ensure real-time performance on CPUs, embedded GPUs, and mobile NPUs. 14 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 In summary, YOLOE-26 demonstrates that open-vocabulary image segmentation can be achieved with YOLO-level efficiency by tightly integrating end-to-end detection with lightweight visionlanguage embedding mechanisms. Looking ahead, the convergence of YOLOE-26 with AI agents and agentic learning paradigms offers compelling pathway toward fully autonomous, self-improving, and deployment-ready visionlanguage segmentation systems capable of discovering new objects, refining semantic understanding, adapting to evolving environments, and maintaining real-time performance across diverse real-world scenarios."
        },
        {
            "title": "References",
            "content": "[1] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: review. IEEE transactions on neural networks and learning systems, 30(11):32123232, 2019. [2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: survey. Proceedings of the IEEE, 111(3):257276, 2023. [3] Chhavi Rana et al. Artificial intelligence based object detection and traffic prediction by autonomous vehiclesa review. Expert Systems with Applications, 255:124664, 2024. [4] Zohaib Khan, Yue Shen, and Hui Liu. Objectdetection in agriculture: comprehensive review of methods, applications, challenges, and future directions. Agriculture, 15(13):1351, 2025. [5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. [6] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72637271, 2017. [7] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. [8] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. [9] Ranjan Sapkota and Manoj Karkee. Ultralytics yolo evolution: An overview of yolo26, yolo11, yolov8 and yolov5 object detectors for computer vision and pattern recognition. arXiv preprint arXiv:2510.09653, 2025. [10] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, et al. Yolov6: single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976, 2022. [11] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74647475, 2023. [12] Mupparaju Sohan, Thotakura Sai Ram, and Ch Venkata Rami Reddy. review on yolov8 and its advancements. In International Conference on Data Intelligence and Cognitive Informatics, pages 529545. Springer, 2024. [13] Gang Wang, Yanfei Chen, Pei An, Hanyu Hong, Jinghu Hu, and Tiange Huang. Uav-yolov8: small-objectdetection model based on improved yolov8 for uav aerial photography scenarios. Sensors, 23(16):7190, 2023. [14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. In European conference on computer vision, pages 121. Springer, 2024. [15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et al. Yolov10: Real-time end-to-end object detection. Advances in Neural Information Processing Systems, 37:107984108011, 2024. [16] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: Attention-centric real-time object detectors. arXiv preprint arXiv:2502.12524, 2025. [17] Mengqi Lei, Siqi Li, Yihong Wu, Han Hu, You Zhou, Xinhu Zheng, Guiguang Ding, Shaoyi Du, Zongze Wu, and Yue Gao. Yolov13: Real-time object detection with hypergraph-enhanced adaptive visual perception. arXiv preprint arXiv:2506.17733, 2025. [18] Ranjan Sapkota, Rahul Harsha Cheppally, Ajay Sharda, and Manoj Karkee. Yolo26: key architectural enhancements and performance benchmarking for real-time object detection. arXiv preprint arXiv:2509.25164, 2025. [19] Yuheng Shi, Minjing Dong, and Chang Xu. Harnessing vision foundation models for high-performance, trainingfree open vocabulary segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2348723497, 2025. 15 YOLOE-26: (Ultralytics YOLO26 Official Source Link) SAPKOTA & KARKEE 2026 [20] Ranjan Sapkota and Manoj Karkee. Object detection with multimodal large vision-language models: An in-depth review. Information Fusion, 126:103575, 2026. [21] Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yoloe: Real-time seeing anything. arXiv preprint arXiv:2503.07465, 2025. [22] Leo Thomas Ramos and Angel Sappa. decade of you only look once (yolo) for object detection: review. IEEE Access, 13:192747192794, 2025. [23] Tong Bai, Haoran Zhao, Lei Huang, Zhipeng Wang, Dong In Kim, and Arumugam Nallanathan. decade of video analytics at edge: Training, deployment, orchestration, and platforms. IEEE Communications Surveys & Tutorials, 2025. [24] Spyridon Loukovitis, Anastasios Arsenos, Vasileios Karampinis, and Athanasios Voulodimos. Model-agnostic open-set air-to-air visual object detection for reliable uav perception. arXiv preprint arXiv:2509.09297, 2025. [25] Zhifeng Wang, Longlong Li, Chunyan Zeng, Shi Dong, and Jianwen Sun. Slbdetection-net: Towards closed-set and open-set student learning behavior detection in smart classroom of k-12 education. Expert Systems with Applications, 260:125392, 2025. [26] Lisa Weijler, Sebastian Koch, Fabio Poiesi, Timo Ropinski, and Pedro Hermosilla. Openhype: Hyperbolic embeddings for hierarchical open-vocabulary radiance fields. arXiv preprint arXiv:2510.21441, 2025. [27] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 47(8):10871102, 2023. [28] Jisha Anu Jose and Renjith Thomas. Zero shot multistage hierarchical fish classification using yoloe and large language model. Computers and Electronics in Agriculture, 242:111288, 2026. [29] Qiang Liu, Xiaonuo Han, Xinpan Yuan, Liujie Hua, and Xianchao Zhou. Yoloe-based semantic segmentation framework with vam and dvcl for enhanced monitoring accuracy in highway collapse detection. In 2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C), pages 343351. IEEE, 2025. [30] Damodharan Palaniappan, Rituraj Jain, Premavathi, Kumar Parmar, Wade Ghribi, Abdelmoty Ahmed, and Naim Ahmad. Yolo in healthcare: comprehensive review of detection architectures, domain applications, and future innovations. IEEe Access, 2025. [31] Zheng Zhou, Bohang Lin, Yijun Li, Zongyong Cui, Yiming Pi, and Zongjie Cao. Projection-evidence collaborative optimization for cross-modal few-shot sar target detection. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2026. [32] Tianhao Qi, Hongtao Xie, Pandeng Li, Jiannan Ge, and Yongdong Zhang. Balanced classification: unified framework for long-tailed object detection. IEEE Transactions on Multimedia, 26:30883101, 2023. [33] Zhifeng Wang, Minghui Wang, Chunyan Zeng, and Longlong Li. Scb-detr: Multiscale deformable transformers for occlusion-resilient student learning behavior detection in smart classroom. IEEE Transactions on Computational Social Systems, 2025. [34] Nirat Saini, Khoi Pham, and Abhinav Shrivastava. Disentangling visual embeddings for attributes and objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1365813667, 2022. [35] Yunpeng Zhang, Wenzhao Zheng, Zheng Zhu, Guan Huang, Dalong Du, Jie Zhou, and Jiwen Lu. Dimension embeddings for monocular 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15891598, 2022. [36] Yao Xiao, Shuai Yuan, and Dexin Zhao. Open-vocabulary object detection via prompt learning and dual-branch classification. Multimedia Systems, 32(1):39, 2026. [37] Qiujie Ma, Shuqi Yang, Lijuan Zhang, Qing Lan, Dongdong Yang, Honghan Chen, and Ying Tan. Apovis: Automated pixel-level open-vocabulary instance segmentation through integration of pre-trained vision-language models and foundational segmentation models. Image and Vision Computing, 154:105384, 2025. [38] Zekun Zhang, Vu Quang Truong, and Minh Hoai. Low-rank prompt adaptation for open-vocabulary object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42044215, 2025."
        }
    ],
    "affiliations": [
        "Cornell University, Biological & Environmental Engineering, Ithaca, NY 14850, USA"
    ]
}