{
    "paper_title": "Agent Learning via Early Experience",
    "authors": [
        "Kai Zhang",
        "Xiangchao Chen",
        "Bo Liu",
        "Tianci Xue",
        "Zeyi Liao",
        "Zhihan Liu",
        "Xiyao Wang",
        "Yuting Ning",
        "Zhaorun Chen",
        "Xiaohan Fu",
        "Jian Xie",
        "Yuxuan Sun",
        "Boyu Gou",
        "Qi Qi",
        "Zihang Meng",
        "Jianwei Yang",
        "Ning Zhang",
        "Xian Li",
        "Ashish Shah",
        "Dat Huynh",
        "Hengduo Li",
        "Zi Yang",
        "Sara Cao",
        "Lawrence Jang",
        "Shuyan Zhou",
        "Jiacheng Zhu",
        "Huan Sun",
        "Jason Weston",
        "Yu Su",
        "Yifan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents."
        },
        {
            "title": "Start",
            "content": "Agent Learning via Early Experience Kai Zhang1,3,,, Xiangchao Chen3,, Bo Liu2,, Tianci Xue3,, Zeyi Liao3,, Zhihan Liu1,, Xiyao Wang1,, Yuting Ning3,, Zhaorun Chen1,, Xiaohan Fu1, Jian Xie3, Yuxuan Sun3, Boyu Gou3, Qi Qi1, Zihang Meng1, Jianwei Yang1, Ning Zhang1, Xian Li2, Ashish Shah1, Dat Huynh1, Hengduo Li1, Zi Yang1, Sara Cao1, Lawrence Jang1, Shuyan Zhou1,, Jiacheng Zhu1,, Huan Sun3,, Jason Weston2,, Yu Su3,, Yifan Wu1, 1Meta Superintelligence Labs, 2FAIR at Meta, 3The Ohio State University Core Contributors, Work done at Meta, Joint Last Author long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only narrow range of scenarios, and expose the agent to limited environment diversity. We address this limitation with middle-ground paradigm we call early experience: interaction data generated by the agents own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers strong foundation for subsequent reinforcement learning, positioning it as practical bridge between imitation learning and fully experience-driven agents. Date: October 10, 2025 Correspondence: Kai Zhang (zhang.13253@osu.edu) 5 2 0 2 9 ] A . [ 1 8 5 5 8 0 . 0 1 5 2 : r Figure 1 Progression of training paradigms for language agents. Left: The Era of Human Data relies on expert demonstrations, where supervision comes from human-/expert-curated actions; it is reward-free (i.e., does not require the environment to provide verifiable reward) but not data-scalable. Right: The envisioned Era of Experience builds upon environments with verifiable rewards, using them as the primary supervision for reinforcement learning; however, many environments either lack such rewards (Xue et al., 2025) or require inefficient long-horizon rollouts (Xie et al., 2024a). Center: Our Early Experience paradigm enables agents to propose actions and collect the resulting future states, using them as scalable and reward-free source of supervision."
        },
        {
            "title": "1 Introduction",
            "content": "Autonomous agents (Russell and Norvig, 1995; Franklin and Graesser, 1997) have long been central goal of artificial intelligence, aiming to perceive, act, and learn in complex environments to accomplish goals without human intervention. This vision is becoming increasingly realistic with the emergence of language agents (Su et al., 2024; Sumers et al., 2024), which are built on top of large language models (LLMs; OpenAI (2024)). Powered by knowledge obtained from large-scale pretraining and the flexibility of the language interface, language agents are now being applied across wide range of environments. They can navigate websites and mobile applications (Zheng et al., 2024a; Deng et al., 2023; Zhou et al., 2024; Trivedi et al., 2024), control diverse tools (Xie et al., 2024a; Gu et al., 2024), and assist in scientific research (Chen et al., 2025; Lou et al., 2025), showing strong potential as foundation for the next generation of intelligent systems. To build such language agents, one promising solution is reinforcement learning (RL), where agents are trained by optimizing for expected cumulative reward returned by the environment. This paradigm has enabled traditional agents such as AlphaGo (Silver et al., 2016) to achieve superhuman performance in domains with well-defined environments and reward structures, such as Atari games (Bellemare et al., 2013) and the game of Go, echoing the vision of an emerging era of experience (Silver and Sutton, 2025) for language agents. However, applying RL to real-world language agents remains highly challenging now. Many environments of interest lack verifiable or dense reward signals, especially in open-ended settings such as websites where platforms do not expose ground truth feedback. For example, form may appear to be submitted successfully, but the agent receives no indication of whether each piece of information was filled out correctly. In addition, tasks in multi-turn tool-use environments often involve long interaction sequences (Xie et al., 2024a; Jin et al., 2025) with delayed or ambiguous outcomes, making credit assignment and training inefficient and unstable. As workaround, most current language agents are instead trained on expert-curated data with supervised fine-tuning (SFT; Deng et al. (2023); Pahuja et al. (2025); Prabhakar et al. (2025)). This paradigm bypasses the need for reward signals by learning from human demonstrations, where agents map states to actions using static datasets. While SFT is straightforward and efficient to train, it has inherent limitations. The agent under this paradigm does not interact with the environment during training; it does not observe the outcomes of its own actions. This restricts its ability to learn from failure, refine its decision-making, or generalize to unseen situations (Chu et al., 2025). Furthermore, this approach assumes the data are expert or near-optimal, yet scaling high-quality human demonstrations is expensive and difficult to sustain. More critically, it locks the agent into passive role, bound by the imagination and coverage of its training data rather than actively learning from its own experience. Given these limitations and that reliable reward signals are often unavailable aforementioned, how can we train agents to grow from their own experience, without any external reward signals? Motivated by these limitations, we introduce the early experience paradigm, middle ground between imitation learning and reinforcement learning, as shown in Figure 1. In this setting, agents learn not only from human-curated data but also from future states driven by their own proposed actions in the environment. These future states are the agents own experience, and can be transformed into supervision signals that enable it to grow directly from the consequences of its actions without relying on external reward signals. We explore two strategies to transform these future states as supervision: (1) Implicit World Modeling: using the collected future states to help the agent build internal representations of environment dynamics, allowing it to better understand the environment by predicting the future states. (2) Self-Reflection: guiding the agent to compare its behavior with expert demonstrations, identify suboptimal decisions, and extract lessons to improve future decision-making. Both strategies share the same principle: in the absence of external rewards, the agents own actions and the resulting future states can still constitute experience that serves as direct source of supervision. By turning future states generated from its own actions into learning signals, the language agent can continually improve without relying on additional human data or external rewards. We comprehensively evaluate early experience across eight diverse environments, spanning embodied navigation, web navigation, multi-turn tool-use, long-horizon planning, and multi-domain API tasks, using multiple base architectures. Across all settings, both methods consistently outperform purely imitation learning baselines, with average absolute gains of +9.6 in success rate and +9.4 in out-of-domain generalization. Moreover, in environments where verifiable rewards are available, initializing RL with checkpoints trained with early 2 experience methods leads to substantially stronger performance compared to standard imitation-learning warm starts, improving final success rates by up to +6.4. This shows that the performance gain from early experience stage can carry over to the final models performance after RL. Beyond these empirical gains, our analysis shows that early experience enables capabilities unattainable through imitation learning alone. It scales effectively, achieving comparable or superior performance with only half or even less of the expert data. The paradigm applies seamlessly to larger models, preserving its effectiveness across scales. These results show that early experience is not merely an alternative to imitation learning, but practical and scalable bridge to reinforcement learning, delivering both immediate gains in effectiveness and long-term benefits for era of experience training regimes. Our contributions are summarized as follows: (1) We advocate and formalize the early experience paradigm as practical and scalable bridge between imitation learning and reinforcement learning for building autonomous language agents. It empowers agents to convert their own experience into learning signals without relying on external rewards and can be seamlessly integrated into existing training pipelines. (2) We propose and systematically study two training strategies under this paradigm: implicit world modeling, which enhances decision-making by modeling environment dynamics directly from collected experience, and self-reflection, which distills fine-grained lessons from the agents own actions. (3) We conduct comprehensive evaluation across eight diverse environments and multiple model families. Our methods consistently improve task effectiveness, out-of-domain generalization, and downstream reinforcement learning performance, achieving state-of-the-art results on several benchmarks and offering actionable insights through detailed analysis."
        },
        {
            "title": "2.1 Training Paradigms for Language Agents\nSupervised Fine Tuning (SFT). Most language agents (Yao et al., 2022; Deng et al., 2023; Hong et al., 2024;\nFuruta et al., 2024; Pahuja et al., 2025) are trained with SFT, also known as imitation learning or behavior\ncloning in the RL literature, on expert trajectories, especially in complex settings such as the web (Zhou\net al., 2024) or operating systems (Xie et al., 2024b). These trajectories may be human-annotated (Yao et al.,\n2022; Deng et al., 2023) or synthesized by stronger language models that follow carefully human-designed\nworkflows (Murty et al., 2024; Pahuja et al., 2025). Although synthetic demonstrations increase coverage,\nthey offer only incremental gains because the underlying supervision signal is still static. SFT thus provides\ndense, reward-free supervision signals but remains limited by the cost of high-quality demonstrations (Qi\net al., 2025) and leaves agents brittle when they confront novel states (Chu et al., 2025; Deng et al., 2023).",
            "content": "Reinforcement Learning (RL). RL trains agents through trial and error, optimizing for long-term rewards (Sutton et al., 1998). Although it has achieved impressive results in control, board games, and Atari (Mnih et al., 2013; Silver et al., 2016; Hafner et al., 2020; Schrittwieser et al., 2020), RL remains difficult to apply effectively in language agent settings (Wang et al., 2025; Qi et al., 2025; Wei et al., 2025a; Feng et al., 2025; Zhou et al., 2025b; Jin et al., 2025; Zhou et al., 2025a). Current studies are still exploratory: many rely on approximate rewards produced by larger teacher models (Qi et al., 2025; Zhou et al., 2025b), or on carefully curated reward functions (Qian et al., 2025) and hand-tuned training recipes (Jin et al., 2025) to maintain stability. The supporting infrastructure is also underdeveloped; most real-world language agent environments lack reliable simulators, standard reset mechanisms, and scalable evaluation platforms (Wang et al., 2025; Feng et al., 2025), making large-scale RL training for language agents costly and brittle. Together, these limitations suggest that scalable RL for language agents is not yet mature, motivating paradigm that bridges current imitation-based training and future fully experience-driven learning (RL)."
        },
        {
            "title": "2.2 Supervision from Exploration",
            "content": "Traditional explorationexploitation strategies in RL collect trajectories that are later refined through reward feedback. Methods like Hindsight Experience Replay (Andrychowicz et al., 2017) densify sparse rewards by retrofitting achieved outcomes as goals, but still require verifiable reward functions unavailable in many language agent environments. Our setting uses exploration differently: interaction traces become direct supervision signals, eliminating the need for rewards or manual relabeling entirely. 3 World Models. World models (Sutton, 1991; Ha and Schmidhuber, 2018; Hafner et al., 2020, 2021) are traditionally trained on observed state transitions to predict future states and rewards, allowing model-based RL to reduce sample complexity and support speculative planning. Recent work extends this idea to language agents by using LLMs as world models (Gu et al., 2025; Chae et al., 2025; Hao et al., 2023), which improves downstream performance through language-mediated simulations. Despite the different state representations of world models in different eras, most of these systems still treat the world model as separate simulator, echoing classical control pipelines. In contrast, we view the interaction trace itself as an auxiliary prediction task for the agent policy, similar in spirit to mid-training (Zhang et al., 2025). By training the policy to predict its own future states, the model internalizes coarse environment dynamics without standalone simulator. This implicit world model grounds the agent in its operating context, offers lightweight warm-up for faster adaptation, and avoids the planning overhead required by explicit simulators. Self-Reflection. Self-reflection (Shinn et al., 2023; Madaan et al., 2023) was initially introduced as prompting technique that allows LLMs to revise their answers through multi-turn self-dialogues (Snell et al., 2024) or curated prompt variants (Madaan et al., 2023), without updating model parameters. Subsequent work summarizes lessons over rewarded trajectories in the prompt (e.g., short-term episodic memory (Xie et al., 2025)) to guide future inference. However, later studies (Huang et al., 2024; Valmeekam et al., 2023) show that such inference-time methods often fail without access to external feedback (e.g., rewards). separate line uses LLMs to generate rationales for correct answers, treating these rationales as training targets to bootstrap reasoning (Zelikman et al., 2022; Huang et al., 2023). We extend this view of reflection to the agent setting where explicit rewards are absent. Our approach trains agents to reflect on their own suboptimal actions and the resulting trajectories, then uses the reflected rationales as training signals to improve decision-making."
        },
        {
            "title": "3 Preliminaries",
            "content": "We formalize the language agent decision-making problem as Markov Decision Process (MDP; Bellman (1957)), which provides the mathematical foundation for our early experience paradigm. We consider an MDP defined by the tuple = (S, A, T, R, γ, ρ0), where denotes the state space and represents the action space. The transition function : (S) governs state dynamics, where (S) denotes the probability simplex over S. The reward function : provides feedback signals when available, though in many real-world settings this function may be unknown or unverifiable during training. γ [0, 1] is the discount factor, and ρ0 (S) specifies the initial state distribution. In language agent environments, states encode the environment configuration accessible to the agent, such as webpage contents, tool outputs, or textual environment descriptions. Actions correspond to discrete choices such as clicking elements, invoking tools, or generating text responses. The agent maintains policy πθ : (A), parameterized by θ, which maps states to action distributions (Williams, 1992)."
        },
        {
            "title": "3.1 Learning without Rewards",
            "content": "A key challenge in real-world language agent environments is the absence of reliable reward signals. Many environments either lack verifiable rewards entirely or provide only sparse, delayed feedback after long interaction sequences. This motivates learning from alternative supervision sources. Given dataset of expert demonstrations Dexpert = {(si, ai)}N , where ai denotes the expert action at state si, imitation learning (Pomerleau, 1991; Schaal, 1996; Hussein et al., 2017) aims to minimize the supervised learning loss: i=1 LIL(θ) = (cid:88) i= log πθ(ai si). (1) However, this approach suffers from distribution shift and lacks awareness of action consequences. Distribution shift occurs because the agents learned policy πθ inevitably deviates from the expert policy during deployment, leading to states not covered in training data where errors compound (Ross et al., 2011). The agent lacks awareness of action consequences because it never observes what happens when it takes non-expert actions; it 4 Figure 2 Overview of the two early experience approaches. Implicit world modeling (left) augments expert trajectories with alternative actions and predicted next states, training the policy to internalize transition dynamics before deployment. Self-reflection (right) augments expert actions with self-generated explanations c1, training the policy to reason about and revise its own decisions. Both methods use alternative actions proposed by the initial policy (LLM). The number of alternatives (K) is hyperparameter; for brevity, only one is illustrated. only sees expert state-action pairs without experiencing the outcomes of alternative choices. This limits its ability to recover from errors or reason about why certain actions fail (Ross and Bagnell, 2010)."
        },
        {
            "title": "4 Early Experience",
            "content": "We introduce the early experience paradigm, where language agents improve through interaction with the environment using reward-free but informative future states. To build intuition, consider language agent learning to book flights on the web. In traditional imitation learning, it only sees expert demonstrations of successful bookings. With early experience, the agent also explores what happens when it clicks different buttons or fills in forms incorrectly, observing error messages, page changes, and other outcomes. These observations become learning signals without explicit rewards. Starting from expert trajectories, the agent proposes its own actions at each visited state to collect additional environment feedback through exploration (Thrun, 1992)."
        },
        {
            "title": "4.1 Notation for Early Experience",
            "content": "i , a2 , . . . , aK , we define candidate action set Ai = }, where we sample alternative actions from the initial policy πθ( si). We also include the For each expert state si in the dataset Dexpert = {(si, ai)}N {a1 expert action ai in our analysis. For the expert action ai, executing it leads to the next state si+1. For each alternative action aj executing it in the environment leads to next state sj next states capture the immediate consequences of taking action aj environment such as updated DOM structures, new tool outputs, error messages, or task progression. Ai, ). These at state si, reflecting changes in the sampled from the transition function (si, aj i=1 We collect these interactions into rollout dataset: Drollout = {(si, aj , sj ) [N ], [K]}, (2) where each triple represents state, an alternative action taken at that state, and the resulting next state. All actions aj differ from the expert action ai, allowing the agent to experience diverse state transitions from its own proposed actions. This rollout dataset Drollout provides rich supervision signals without requiring explicit 5 rewards. The next states {sj responses, enabling the agent to learn from the consequences of both expert and non-expert behaviors. [K]} encode implicit feedback about action quality through environment and the rollout dataset Building on the notation from 3, we leverage the expert dataset Dexpert = {(si, ai)}N Drollout = {(si, aj ) [N ], [K]} to develop two different training approaches under the same early experience principle. The key insight is that the next states sj resulting from non-expert actions provide valuable supervision signals without explicit rewards. We now describe how this dataset is leveraged by our two early experience methods. , sj i="
        },
        {
            "title": "4.2 Implicit World Modeling",
            "content": "We formulate world modeling as an auxiliary prediction task that helps the agent internalize environment dynamics from its own early experience. In our setting, states are represented entirely in natural language, allowing us to model next-state prediction as standard next-token prediction objective. Inspired by prior work on training LLMs as world models (Gu et al., 2025), we use next states from the rollout set Drollout as direct training signals for the language agents policy πθ. For example, when booking flights on the web, the model may predict the page state after entering an invalid date, learning from the textual error message as natural-language representation of the next state. This design removes the need for separate module and fits naturally within the LLM fine-tuning paradigm. For each rollout triple (si, aj action pair (si, aj as next-token prediction loss: , sj ) as input and learns to predict the resulting next state sj ) Drollout, we construct prediction task where the model takes the state- . We define the training objective LIWM = (cid:88) log pθ(sj si, aj ), (3) )Drollout where pθ denotes the language models output distribution. Note that we use the same model parameters θ for both state prediction (during world modeling) and action prediction (during policy execution), allowing the policy to internalize environment dynamics directly. (si,aj ,sj This training objective encourages the model to capture regularities in environment behavior, including common transitions, side effects, and invalid action outcomes. Unlike inference-time world models used for planning, our implicit formulation integrates predictive signals directly into policy learning, serving as lightweight warm-up before supervised learning or downstream optimization. It exposes the agent to diverse, non-expert behaviors, improving robustness to distribution shifts and reducing dependence on brittle expert trajectories. In practice, the rollout data are often an order of magnitude larger than Dexpert. We adopt two-stage pipeline: first train with LIWM to internalize coarse dynamics, then fine-tune on Dexpert (i.e., LIL)."
        },
        {
            "title": "4.3 Self-Reflection",
            "content": "We formulate self-reflection as mechanism for agents to learn from their own exploratory outcomes. Rather than relying solely on expert stateaction pairs, the agent compares the expert action at each state with alternatives sampled from its policy, using the resulting next states to generate natural language explanations of why the expert choice is better. These explanations provide richer, transferable supervision than expert actions alone, leveraging the LLMs strength in processing language to internalize decision principles that generalize across tasks. Specifically, for each expert state si, we first execute the expert action ai to obtain the expert next state si+1. For each alternative action aj . We then prompt language model to generate chain-of-thought cj explaining why the expert action ai is preferable to the alternative aj . This prompt is designed to elicit natural language reasoning that highlights potential limitations or inefficiencies in aj , grounded in the actual state transitions observed. (where {1, ..., K}), we obtain the corresponding next state sj based on the differences between their resulting states si+1 and sj The resulting triplets (si, aj ) are collected into dataset Drefl. We then train the agent to jointly predict the chain-of-thought and the expert action conditioned on the state si, using next-token prediction loss over the concatenated target sequence cj , cj ai: 6 LSR = (cid:88) log pθ(cj , ai si), (4) ,cj )Drefl (si,aj where pθ denotes the language models output distribution, aligned with the agents policy πθ. In practice, we mix the self-reflection data Drefl with the expert dataset Dexpert and train the model using standard next-token prediction loss. Chain-of-thought reasoning is generated only for the self-reflection training data, and we retain the original chain-of-thought reasoning in Dexpert whenever provided by the expert trajectories, for all models trained with Dexpert. This joint training setup balances grounded decision-making from demonstrations with contrastive insights from exploratory outcomes. Learning from both sources encourages the model to move beyond rote imitation and develop more generalizable decision criteria. For example, in WebShop, when the expert action is \"click on the $15 blue shirt,\" an alternative might be \"click on the $30 red shirt.\" The generated reflection could be: \"While the red shirt matches the color preference, it exceeds the $20 budget constraint specified in the query. The blue shirt satisfies both the style requirement and budget limit.\" This teaches the model to prioritize constraints, lesson that generalizes beyond this specific item. We show the prompt used across environments below. Self-Reflection Prompt Template You will be presented with situation where you need to choose between multiple possible actions. Your task is to analyze the situation and provide reasoning about why we decide to take the expert action. Situation Description (si): {Situation Description} Expert Action (ai): {Expert Action} Expected Outcome (si+1): {Future State of Expert Action} Alternative Actions: 1. Action a1 2. Action a2 3. . . . : {Alt Action 1}, resulting state s1 : {Alt Action 2}, resulting state s2 : {State 1} : {State 2} Provide detailed self-reflection as an internal monologue that demonstrates your reasoning process for the current situation. Your monologue should: 1. Analyze the situation and the goal. 2. Compare the possible actions, explaining why each may be less optimal. 3. Justify why the expert action is most suitable, grounded in the expected outcome. 4. Highlight any relevant clues, constraints, or consequences from the situation. Guidelines: Stay strictly within the provided information. Avoid meta-commentary about being an AI. Use natural, step-by-step reasoning. Focus on logical decision-making. Output: Directly write the self-reflection monologue, no extra headings, disclaimers, or external notes. Both implicit world modeling and self-reflection follow the same principle of turning the agents own actions and resulting future states into scalable supervision, enabling more generalizable language agent policies."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate the early experience paradigm via the proposed two methods under this paradigm across diverse suite of language-agent environments, testing its effectiveness (5.2), out-of-domain generalization (5.3), and compatibility with post-hoc reinforcement learning (5.4). 7 Table 1 Benchmarks used across three major types of domains and tasks. # Traj. is the number of expert trajectories we collected/used; # Dexpert is the resulting count of stateaction (SA) pairs for imitation learning. Dashes indicate the value was not specified in our data construction text. Environment Description MISC (Embodied and Scientific Simulation, and Travel Planning) ALFWorld (Shridhar et al., 2021) ScienceWorld (Wang et al., 2022) TravelPlanner (Xie et al., 2024a) Embodied instruction-following tasks in simulated household, combining textual descriptions with high-level symbolic actions. We follow the setting of Feng et al. (2025). An interactive science lab simulator rendered in natural language, where agents perform multi-step experiments using tools and materials. We implement the gym (Brockman et al., 2016) for this environment. Long-horizon travel planning tasks that require generating and refining multi-day itineraries using various tools and databases. We focus on the sole-planning mode and implement the gym for such an environment. # Traj. # Dexpert 3,553 21,031 1,000 14,506 45 1, Multi-Turn Tool-Use SearchQA (Jin et al., 2025) BFCLv3 (Patil et al., 2025) Tau-Bench (Yao et al., 2025) Multi-hop question answering in open-domain settings, where agents issue search queries and reason over retrieved snippets to answer complex questions. We follow Search-R1 (Jin et al., 2025) settings and treat Musique as the in-domain dataset and HotpotQA, 2WikiMultiHopQA, and Bamboogle as out-of-domain datasets. Multi-turn tool-use tasks from the Berkeley Function Call Leaderboard v3, where agents interact with Python-based API environment that simulates functional programs. We focus on the multi-turn tool use. Realistic customer-service scenarios requiring agents to interact with LM-simulated users, perform multi-turn tool use via APIs, and adhere to domain-specific policy documents. We focus on the Retail subset. 2, 7,691 125 1,264 452 5,239 Web Navigation WebShop (Yao et al., 2022) WebArena-Lite (Zhou et al., 2024) (Liu et al., 2025) Shopping tasks in simulated e-commerce site, where agents must navigate, filter, and select the correct product based on natural language queries. We follow the setting of Feng et al. (2025). Web navigation tasks across domains like e-commerce, forums, and content management. We follow Koh et al. (2024) to evaluate results with accessibility tree as observation space. 1,571 15,464 7,"
        },
        {
            "title": "5.1 Experiment Setup\nEnvironments. We conduct experiments on eight language-agent environments covering a wide range of\ndomains and task formats including multi-turn tool use (Jin et al., 2025; Patil et al., 2025; Yao et al., 2025),\nweb navigation (Yao et al., 2022; Zhou et al., 2024), embodied simulation (Shridhar et al., 2021), scientific\nsimulation (Wang et al., 2022), and long-horizon planning (Xie et al., 2024a). The details of these benchmarks\nare listed in Table 1 and more details can be found in Appendix B.",
            "content": "Models and Expert Trajectories. We evaluate early experience using three instruction-tuned models from two model family: Llama-3.1-8B. Each model is trained on fixed number of expert demonstrations, with or without early experience augmentation. These demonstrations are drawn from diverse sources across environments. More details are provided in Appendix B. Qwen-2.5-7B, and Llama-3.2-3B, Training and Evaluation. We use consistent prompt formatting and decoding strategies across all settings. Because environments differ in data size and horizon, we first explore the number of optimization steps for the Imitation Learning baseline in each environment and select the checkpoint with the lowest training loss as well as the performance on the validation set. We then fix this step budget and use it unchanged for our methods to ensure fair comparison. For Implicit World Modeling, we begin with one epoch of the WM objective and then continue supervised updates so that the total updates equal the imitation budget without extra steps. For Self-Reflection, we train for the same number of epochs as imitation. All experiments use at 8 most 8 H100 GPUs for training and evaluation. In terms of evaluation, we report each benchmarks main native metric and follow its official validators. For full evaluation results, please refer to Appendix B."
        },
        {
            "title": "5.2 Effectiveness",
            "content": "Table 2 Results on eight benchmarks. All values are success rates (%) unless otherwise noted. Improvements over imitation learning are shown in green. Prompt indicates the performance of the instruction-tuned model. IWM and SR denote Implicit World Modeling and Self-Reflection, respectively. Appendix shows complete results. Benchmark Model"
        },
        {
            "title": "Imitation Learning",
            "content": "Ours-IWM Ours-SR Embodied and Scientific Simulation, and Travel Planning"
        },
        {
            "title": "ScienceWorld",
            "content": "TravelPlanner BFCLv3 Tau-Bench SearchQA (F1) WebShop WebArena -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B 8.6 20.3 25.0 2.3 3.9 3.1 0.0 0.0 0.0 78.1 78.1 80. 51.6 53.9 54.7 19.4 16.7 17.2 Multi-Turn Tool-Use 21.3 26.7 16.0 24.3 33.9 35.9 38.0 39.9 41. Web Navigation 1.3 10.6 6.7 5.2 20.0 6.0 13.3 19.3 21.0 0.0 0.8 0.0 1.2 1.8 0. 83.6 (+5.5) 82.8 (+4.7) 85.9 (+5.4) 55.5 (+3.9) 59.4 (+5.5) 57.0 (+2.3) 28.3 (+8.9) 22.2 (+5.5) 25.0 (+7.8) 25.3 (+4.0) 29.3 (+2.6) 20.0 (+4.0) 26.1 (+1.8) 38.7 (+4.8) 40.8 (+4.9) 39.0 (+1.0) 40.8 (+0.9) 44.3 (+3.3) 85.9 (+7.8) 82.0 (+3.9) 85.2 (+4.7) 56.2 (+4.6) 57.8 (+3.9) 68.0 (+13.3) 32.2 (+12.8) 31.7 (+15.0) 32.2 (+15.0) 29.3 (+8.0) 32.0 (+5.3) 20.0 (+4.0) 28.7 (+4.4) 39.5 (+5.6) 41.7 (+5.8) 38.6 (+0.6) 42.0 (+2.1) 41.8 (+0.8) 41.8 51.6 47.3 6.1 4.2 4.9 60.2 (+18.4) 56.2 (+4.6) 58.6 (+11.3) 52.7 (+10.9) 62.2 (+10.6) 58.2 (+10.9) 8.5 (+2.4) 7.3 (+3.1) 8.5 (+3.6) 7.3 (+1.2) 6.1 (+1.9) 8.5 (+3.6) We evaluate across eight environments spanning multi-turn tool use, web navigation, and more  (Table 2)  . All models are trained with the same prompt format and decoding strategy for each environment. Overall Gains. Early experience improves over imitation learning in nearly all settings and with both model sizes. Implicit World Modeling (IWM) yields steady gains in structured simulators and transactional sites (ALFWorld/ScienceWorld +2.3 to +5.5; WebShop +11.3 to +18.4). Self-Reflection (SR) delivers the largest jumps when tasks require multi-step reasoning and constraint satisfaction (TravelPlanner +12.8 to +15.0; ScienceWorld +13.3; BFCLv3 +8.0 on the 3B model). Even on the most challenging settings, the gains are consistent though smaller in absolute terms (WebArena +1.2 to +3.6; SearchQA +0.6 to +3.3). Action-Space Perspective. Across our eight environments, the action spaces fall into three regimes. Closed and finite action sets (e.g., ALFWorld for embodied navigation, ScienceWorld for scientific procedures, and TravelPlanner for itinerary planning) present small, fixed list of admissible actions from the start. Here, IWM helps the policy internalize transition regularities, while SR adds targeted corrections for long-horizon plans (e.g., large SR gains on TravelPlanner). Structured but large action sets (e.g., BFCLv3 for terminal tasks and Tau-Bench for multi-domain APIs) require selecting from many typed tools with arguments and 9 sequencing them correctly. In this setting, early experience reduces tool misuse and improves ordering; SR often helps more when policy errors are primarily logical. Open action sets (e.g., SearchQA with free-form search queries, WebArena with fine-grained web element interactions) allow vast number of possible actions, often combinatorial in nature. These are the hardest regimes; nevertheless, early experience still yields reliable gains by turning exploratory rollouts into dense training signals without requiring rewards. Observation-Space Perspective. Our benchmarks span wide range of observation complexities. At the low end, ALFWorld provides short, clean textual descriptions of the scene, while ScienceWorld produces procedural readouts of ongoing experiments. Mid-range settings like BFCLv3 and Tau-Bench return structured API schemas and tool outputs that must be parsed and sequenced correctly. At the high end, WebArena presents noisy, fine-grained web states as accessibility trees, requiring reasoning over hundreds of DOM-like elements. We provide examples of each environment in Appendix B. In settings where state transitions are consistent and predictable (e.g., WebShop), IWM excels by helping the agent internalize environment dynamics and improve next-state predictions. When failures stem primarily from reasoning errors or the need to repair long-horizon plans (e.g., TravelPlanner, ScienceWorld), SR delivers larger gains by explicitly comparing actions to expert trajectories. Overall, regardless of how simple or complex the environments observations are, early experience methods consistently turn the agents own actions and resulting states into effective supervision signals that improve policy learning without rewards. Takeaway. Early experience reliably converts an agents own actions and resulting states into scalable supervision beyond expert demonstrations. Both methods under this paradigm strengthen policies across environments that differ substantially in both action spaces and observation complexity. These effects hold across three model sizes and three environment families, demonstrating strong generalizable feasibility of our early experience paradigm. Table 3 Out-of-domain evaluation results (%). Improvements over imitation learning are shown in green. Prompt means the instruct models performance. IWM and SR refer to Implicit World Modeling and Self-Reflection, respectively. Prompt Imitation Learning -3.2-3B 5.5 74.2 AlfWorld -2.5-7B 4.7 64.1 -3.1-8B 18.8 63. -3.2-3B 1.3 5.3 BFCLv3 -2.5-7B 7.1 7.6 SearchQA (F1) -3.1-8B 6.2 6.7 -3.2-3B 24.6 40. -2.5-7B 33.1 47.0 -3.1-8B 37.0 47.4 Ours-IWM Ours-SR 77.3 (+3.1) 77.3 (+3.1) 70.3 (+6.2) 71.1 (+7.0) 78.1 (+14.8) 72.7 (+9.4) 8.9 (+3.6) 13.8 (+8.5) 12.9 (+5.3) 8.3 (+0.7) 7.6 (+0.9) 8.0 (+1.3) 45.4 (+4.9) 44.0 (+3.5) 49.5 (+2.4) 51.2 (+4.2) 49.6 (+2.2) 50.7 (+3.3)"
        },
        {
            "title": "5.3 Out-Of-Domain Generalization",
            "content": "To evaluate the robustness of trained policies beyond in-domain performance, we explore early experience in environments with out-of-domain (OOD) splits, using the same checkpoints evaluated in Section 5.2. To set up, for ALFWorld and SearchQA we follow the OOD splits defined in their original work. For BFCLv3 the in-domain setting is multi-turn base; OOD settings are averaged over multi-turn missing function, missing argument, and long context. The results of our trained models are shown in Table 3, from which we can make the following observations. OOD scores drop relative to in-domain across all tasks, yet early experience consistently recovers substantial portion of the gap. In several cases, the relative gains are larger than in-domain (e.g., SearchQA), indicating that converting ones own rollouts into supervision prepares the policy for states not covered by demonstrations. The method-wise pattern mirrors in-domain trends: IWM helps most where dynamics are stable (e.g., ALFWorld); SR is strongest when distribution shifts alter tool availability or arguments (e.g., BFCLv3); both IWM and SR help under retrieval shifts (e.g., SearchQA), for both model sizes. Takeaway. Early experience improves robustness under diverse OOD regimes: IWM excels when dynamics are stable, SR when shifts affect tool availability, arguments, or retrieval distributions. In several benchmarks (e.g., ALFWorld, SearchQA), OOD gains meet or exceed in-domain gains, reinforcing that an agents own experience provides supervision that generalizes beyond expert demonstrations. 10 Imitation Learning +GRPO Implicit World Modeling +GRPO Self-Reflection +GRPO 100 ) % ( R c 60 40 92.2 89.8 91.4 89. 91.4 89.8 82.0 84.4 80.5 60. 52.7 51.6 62.2 56.2 58.6 58. 47.3 41.8 100 99.2 97.7 97. 97.7 96.9 98.5 50.0 50 51. 51.1 48.6 48.4 51.0 49.8 48. 48.0 ) % ( R c 92. 91.4 93.8 90 85.9 83.6 78.1 78.1 82.8 82.0 80.5 85. 85.2 ) % ( 1 46.3 46.8 47.1 45.4 44.8 44. 43.4 42.3 39.8 40 -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B -3.2-3B -2.5-7B -3.1-8B (a) WebShop (b) AlfWorld (c) SearchQA Figure 3 Reinforcement learning (GRPO) starting from checkpoints trained with different methods on three infraready environments. Bars show performance before (deeper shade) and after RL (lighter shade) for three methods. Checkpoints from early-experience methods (IWM, SR) consistently lead to higher post-RL ceilings than imitation-only starts, with advantages often maintained or amplified after RL."
        },
        {
            "title": "5.4 Reinforcement Learning Following Early Experience",
            "content": "To evaluate the impact of early experience once environments provide verifiable rewards (the defining condition of the era of experience), we append reinforcement learning stage to models trained in Section 5.2. We focus on three reward-available benchmarks: WebShop, ALFWorld, and SearchQA, and adopt the widely used GRPO algorithm (Shao et al., 2024) with identical hyperparameters and training steps as established recipes (Feng et al., 2025; Jin et al., 2025). The only factor that changes across runs is the initialization: Imitation Learning (IL), Implicit World Modeling (IWM), or Self-Reflection (SR). Results in Figure 3 show clear pattern: starting from early experience consistently yields higher post-RL ceilings. In some cases, the performance gap grows during RL training (e.g., ALFWorld); in others, it narrows but never reverses. Even when reward optimization is applied for the same number of steps, IL starts rarely match the final performance of early-experience starts. For completeness, we also run GRPO directly from the raw pretrained model without any supervised stage. This performs worst across all tasks and shows unstable training dynamics, highlighting the necessity of strong initialization. The full results with detailed metrics can be found in Appendix B. Takeaway. Early experience acts as mid-training bridge between the era of human data and the era of experience. It produces policies that already perform strongly without rewards and that amplify the benefits of subsequent RL. Under identical RL recipes, early-experience starts achieve higher final performance. These results suggest that once RL infrastructure becomes available in new environments, early experience can immediately unlock further gains without retraining from scratch."
        },
        {
            "title": "6.1 Comparison to Baselines\nWe compare early experience to two alternatives that inject extra supervision or reasoning signals without\nexecuting alternative actions or observing their resulting states. This allows us to test whether our gains can\nbe matched by simply extending reasoning at inference or by adding ungrounded rationales during training.",
            "content": "(1) Long CoT (test-time scaling). Inspired by test-time scaling (Snell et al., 2024), we aim to help instructiontuned and imitation-only models trained on expert trajectories, where rationales are often absent, reason more extensively at inference. The prompt baseline uses the off-the-shelf instruction-tuned model with the official prompts from prior work, which typically produce short chain-of-thought (Wei et al., 2022). Our Long CoT variant forces longer reasoning before action generation by performing heavier prompt search on the training split and, when delimiter token marking the end of reasoning exists (e.g., </think>), truncating it to encourage continued generation. We report the best results on each environment. 11 (2) STaR-style data (reasoning without alternative actions or resulting states). Following STaR (Zelikman et al., 2022), we have the model generate rationale for the expert action at each state and retain only cases where the predicted action matches the expert. We then fine-tune on (state, rationale, action) tuples, as in Equation 4. Since alternative actions and their resulting states are not used, these rationales remain ungrounded in actual outcomes. We search over prompt variants for rationale synthesis and keep the strongest configuration. The number of optimization steps is matched to our self-reflection method. Table 4 Comparison of early experience with three representative baselines. All results are based on Llama-3.1-8B-Instruct."
        },
        {
            "title": "Prompt",
            "content": "+Long CoT"
        },
        {
            "title": "Imitation Learning",
            "content": "+Long CoT +STaR WebShop ALFWorld 0.0 1.6 (+1.6) 47.3 0.0 (-47.3) 25.0 (-22.3) 25.0 28.4 (+3.4) 80.5 25.8 (-54.7) 74.2 (-6.3) Ours-IWM Ours-SR 58.6 (+11.3) 58.2 (+10.9) 85.9 (+5.4) 85.2 (+4.7) Table 4 shows that both early experience methods achieve the largest gains across tasks and model sizes. For Long CoT, heavier prompt search and reasoning-length control can modestly improve the imitation-trained prompt baseline, but the gains vanish quickly in harder settings. Once fine-tuned only on expert trajectories lacking inherent rationales, models lose the ability to sustain coherent long-form reasoning, so extended chains often drift or collapse into invalid/off-policy actions despite truncation at the thoughtaction boundary. For STaR-style data, the match rate between generated and expert actions is low, leaving little usable training data. The retained rationales are ungrounded, having never been tested in the environment, and frequently hallucinate tools or facts, so fine-tuning on them can even degrade performance. In contrast, early experience directly converts the policys own off-expert rollouts into grounded supervision from observed next states, producing robust improvements that these alternatives fail to match."
        },
        {
            "title": "6.2 Impact of Amount of Human Data",
            "content": "Imitation Learning Implicit World Modeling Self-Reflection R c 60 46.9 40 38.3 30 25.8 54. 43.0 33.6 55.5 51.6 44.6 59. 58.6 80 45.3 60 56.2 20 42.9 24.2 1/8 82.0 77. 85.2 80.5 67.2 46.1 1/4 1/2 ALFWorld 60 55 50 45 50.8 47. 45.3 1 59.4 56.2 57.0 54. 2 4 Webshop 86 84 82 80 58.6 49.6 8 83.6 80.5 78. 1 84.4 81.3 83.6 82.0 2 4 ALFWorld 85.2 79.7 8 1/8 1/4 1/ 1 Webshop (a) % Expert Trajectories (b) Branching Factor Figure 4 Effect of demonstration budget and branching factor. (a): success rate vs. fraction of expert trajectories; (b): success rate vs. branching factor (number of alternative actions per state in Dexpert). Results are shown for WebShop and ALFWorld using Llama-3.1-8B-Instruct. To examine how performance scales with the amount of expert supervision, we vary the number of demonstrations used to seed early experience while keeping the total training budget fixed. Figure 4 (a) shows that early experience maintains consistent lead over imitation learning at every data level. On WebShop, just 1/8 of the demonstrations already surpasses imitation learning trained on the full dataset; on ALFWorld, the same holds with 1/2 of the demonstrations. Both IWM and SR improve with more expert data, yet the margin over imitation learning remains large, underscoring that early experience provides additional supervision signals beyond what demonstrations alone can supply."
        },
        {
            "title": "6.3 Impact of Branching Factor\nTo investigate the impact of branching factor for our methods, we also ablate the branching factor K, the\nnumber of alternative actions rolled out per expert state when generating early experience. Figure 4 (b)\nshows that IWM improves steadily as K increases, consistent with learning richer transition regularities. SR\nimproves at small to moderate K and can be non-monotonic at very large K: comparing many alternatives\noccasionally includes other success-leading actions, reducing contrast with the expert, and current models have",
            "content": "12 limited capacity to reason over many alternatives and outcomes in single context. Overall, both variants improve most of the time, with IWM favoring larger and SR working best with modest (e.g., 24)."
        },
        {
            "title": "6.4 Model Scaling",
            "content": "-3.2-3B, -3.1-8B, and We study whether the benefits of early experience persist as models scale. On WebArena we compare -3.3-70B. Due to limited compute, fine-tuning for 70B models uses parameter-efficient LoRA (Hu et al., 2022) for all methods with the same rank and update steps; for IWM, the same adapters are continued in the second stage so that total tunable parameters and compute match imitation learning. Raw (Instruct) Implicit World Modeling Imitation Learning Self-Reflection 10 5 ) % ( R c S Figure 5 shows that early experience outperforms imitation learning at every scale, with the gap persisting even for the 70B model. Absolute performance rises with scale, and early-experience checkpoints consistently occupy the top curve, indicating that the supervision it provides complements model size rather than substituting for it. Even with LoRA-only updates, both IWM and SR deliver steady gains, demonstrating that the approach remains effective under constrained compute budgets. We observe similar trends on Figure 5 Performance of Llama with different model sizes trained with imitation learning and methods under early experience on the WebArena benchmark. Qwen models in Table 10 in Appendix B. -3.3-70B -3.2-3B -3.1-8B"
        },
        {
            "title": "7 Conclusion",
            "content": "We advocate and present early experience as scalable, reward-free paradigm that advances language agents before reinforcement learning environments are fully ready. By converting an agents own actions and resulting states into supervision, without external reward signals, we achieve consistent gains across eight diverse environments, spanning embodied navigation, scientific experimentation, long-horizon planning, multi-turn tool use, and web navigation. The proposed two methods under this paradigm, implicit world modeling and self-reflection, improve both in-domain effectiveness and out-of-domain robustness, and retain their advantage when used to warm-start reinforcement learning, positioning early experience as practical and general foundation for building more capable language agents in the upcoming era of experience."
        },
        {
            "title": "Limitations and Future Work",
            "content": "While early experience improves performance across diverse environments, several limitations remain. Our current approaches, implicit world modeling and self-reflection, focus on short-horizon traces; extending them to address long-horizon credit assignment without explicit rewards remains an open challenge. Future work will explore combining early experience with richer self-supervised objectives, leveraging crossenvironment transfer, and integrating it with reward-based fine-tuning in continual learning setting. Another direction is to investigate other instances of early experience beyond the two approaches proposed in this paper. We also hope to extend the paradigm to large-scale, real-world deployments, where interaction data is collected organically and can drive continual policy improvement."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Boyuan Zheng and other members of the OSU NLP group for insightful discussions during the early stages of this project. We also thank other Meta interns including Zeyi Huang, Xuehai He, and many others for useful discussions and feedback. We are grateful to the Meta Agents team, particularly Jing Yu Koh for tremendous help and fruitful discussions, Yi Pan for infrastructure and evaluation support, and Ruslan Salakhutdinov for generous support."
        },
        {
            "title": "References",
            "content": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. JAIR, 47:253279, 2013. Richard Bellman. markovian decision process. Journal of mathematics and mechanics, pages 679684, 1957. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint, abs/1606.01540, 2016. Hyungjoo Chae, Namyoung Kim, Kai Tzu iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. In ICLR, 2025. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. In ICLR, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In ICML, 2025. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: learning environment for text-based games. arXiv preprint arXiv:1806.11532, 2019. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In NeurIPS, 2023. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Stan Franklin and Art Graesser. Is it an agent, or just program?: taxonomy for autonomous agents. In Jörg P. Müller, Michael J. Wooldridge, and Nicholas R. Jennings, editors, Intelligent Agents III Agent Theories, Architectures, and Languages, 1997. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. In ICLR, 2024. Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher Manning. Synthetic data generation & multi-step rl for reasoning & tool use. arXiv preprint arXiv:2504.04736, 2025. Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su. Middleware for LLMs: Tools are instrumental for language agents in complex environments. In EMNLP, 2024. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2025. David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In NeurIPS, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In ICLR, 2020. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In ICLR, 2021. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In EMNLP, 2023. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In ACL, 2020. 14 Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents. In CVPR, 2024. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In EMNLP, 2023. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In ICLR, 2024. Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: survey of learning methods. ACM Computing Surveys (CSUR), 50(2):135, 2017. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. ArXiv preprint, abs/2407.01476, 2024. Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024. Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Song XiXuan, Yifan Xu, Shudan Zhang, Hanyu Lai, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, and Jie Tang. Visualagentbench: Towards large multimodal models as visual foundation agents. In ICLR, 2025. Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, and Wenpeng Yin. AAAR-1.0: Assessing ais potential to assist research. In ICML, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In NeurIPS, 2023. Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Offer Akrabi, Aviad Sela, Asaf Adi, and Nir Mashkif. Towards enterprise-ready computer using generalist agent. arXiv preprint arXiv:2503.01861, 2025. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv: 1312.5602, 2013. Shikhar Murty, Christopher D. Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: bootstrapping agents by guiding exploration with language. In ICML, 2024. OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2024-09-28. Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, and Ahmed Awadallah. Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. In Findings of ACL, 2025. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In ICML, 2025. Dean Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural computation, 3 (1):8897, 1991. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, and Caiming Xiong. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. 15 Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of EMNLP, 2022. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Jiadai Sun, Xinyue Yang, Yu Yang, Shuntian Yao, Wei Xu, Jie Tang, and Yuxiao Dong. WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning. In ICLR, 2025. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Stéphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661668. JMLR Workshop and Conference Proceedings, 2010. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. Stuart J. Russell and Peter Norvig. Artificial Intelligence: Modern Approach. Prentice Hall, 1 edition, 1995. ISBN 0-13-103805-2. Stefan Schaal. Learning from demonstration. NeurIPS, 1996. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow data. arXiv preprint arXiv:2411.15004, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In EuroSys, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In NeurIPS, 2023. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. {ALFW}orld: Aligning text and embodied environments for interactive learning. In ICLR, 2021. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 2025. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 2016. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö Arık. Learn-by-interact: data-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. Yu Su, Diyi Yang, Shunyu Yao, and Tao Yu. Language agents: Foundations, prospects, and risks. In EMNLP: Tutorial, 2024. Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. Cognitive architectures for language agents. TMLR, 2024. Richard Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4): 160163, 1991. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Sebastian Thrun. Efficient exploration in reinforcement learning. Carnegie Mellon University, 1992. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. TACL, 2022. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. AppWorld: controllable world of apps and people for benchmarking interactive coding agents. In ACL, 2024. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. ScienceWorld: Is your agent smarter than 5th grader? In EMNLP, 2022. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025a. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025b. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, and Yue Zhang. Cofca: step-wise counterfactual multi-hop qa benchmark. arXiv preprint arXiv:2402.11924, 2024. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agentgym: Evolving large language model-based agents across diverse environments, 2024. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. In ICML, 2024a. Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, and Yanghua Xiao. Revealing the barriers of language agents in planning. In NAACL, 2025. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In NeurIPS, 2024b. Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, and Yu Su. An illusion of progress? assessing the current state of web agents. In COLM, 2025. Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: simple yet strong baseline for llm-based web agents. arXiv preprint arXiv:2410.13825, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In NeurIPS, 2022. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. In ICLR, 2025. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022. Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, and Junxian He. Breaking the data barrier building gui agents through task generalization. arXiv preprint arXiv:2504.10127, 2025. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. In ICML, 2024a. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In ACL: System Demonstrations, 2024b. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In ICLR, 2024. Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716, 2025a. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and Li Erran Li. Proposer-agent-evaluator (PAE): Autonomous skill discovery for foundation model internet agents. In ICML, 2025b."
        },
        {
            "title": "A Contribution Statement",
            "content": "Kai Zhang led the project. He initiated the work on efficient scaling of web agent data with Yu Su prior to his stay at Meta, and during the stay expanded its scope into the early experience paradigm. Kai Zhang designed both two early experience methods, implemented the core training pipeline, established the general training recipes, and validated the methods on ALFWorld and WebArena, while also guiding collaborators on experiments in other environments. He also designed most of the experimental setups, created the figures, and drafted the manuscript. Xiangchao Chen conducted all experiments on the WebShop environment, including the main experiments, reinforcement learning experiments, and follow-up discussions. He actively explored alternative state representations on this environment and identified the most effective setup. His careful analysis played an important role in refining the methodology and ensuring the robustness of results on WebShop. Bo Liu led the experiments on TravelPlanner. He implemented gym environment to standardize the official benchmark, ensuring consistent training and evaluation. His insights from the RL community contributed meaningfully in shaping this works direction. He also wrote 3 and part of 4. Tianci Xue conducted all experiments on the SearchQA environment, including the main experiments and follow-up discussions. He actively explored different variants of state representations together with Huan Sun on this environment. He also extended the scope to Gemma models. Zeyi Liao conducted all experiments on BFCLv3. He actively iterated on the method design and the data synthesis pipeline for this environment and set up the supporting infrastructure. He carefully analyzed the models in use and proposed techniques to encourage the diversity of alternative actions. Zhihan Liu conducted all experiments on ScienceWorld. He contributed valuable insights from the RL community throughout the project and suggested the model scaling experiments. The above authors contributed substantially to this work and played indispensable roles in bringing the early experience to its final form. As result, they own the right to list themselves as second author. Xiyao Wang led the experiments on Tau-Bench, with the help from Xiaohan Fu and synthesized self-reflection data from Kai Zhang. Yuting Ning, Boyu Gou, Qi Qi, and Yuxuan Sun explored the application of early experience on additional environments. Zhaorun Chen helped collect and process the WebArena training data while serving the WebArena service for Kai doing experiments on it. Jian Xie and Lawrence Jiang carefully review the paper and provide valuable suggestions. Shuyan Zhou, Jiacheng Zhu, Huan Sun, and Jason Weston provided valuable and concrete suggestions that greatly improved the quality of this work. Shuyan Zhou offered practical insights on computer use agents and web agents. Jiacheng Zhu in particular helped shape the project scope by intensive early discussions with Kai. Jason Weston and Huan Sun carefully reviewed the manuscript and offered significant feedback on methodology, experiments, and presentation, ensuring the papers clarity and rigor. Yu Su helped conceive the project with Kai Zhang. He guided the scientific direction, provided resources to the team, refined the scope of the work, and contributed substantially to shaping the manuscript through constructive feedback and mentorship. Yifan Wu coordinated internally to secure resources such as compute, data, and APIs and provided essential logistical support. She played key administrative role by resolving organizational challenges and facilitating smooth collaboration across teams. All other co-authors reviewed the manuscript or presentation and contributed through feedback."
        },
        {
            "title": "B Implementation Details",
            "content": "In this section, we provide implementation details for each environment. For each one, we present tables containing all available metrics. Also, we show concrete training examples synthesized (e.g., for self-reflection) by Llama-3.1-8B. B.1 ALFWorld We follow the default split of ALFWorld (Shridhar et al., 2021) with the TextWorld (Côté et al., 2019) setup under the Verl-Agent (Feng et al., 2025) framework. From the expert trajectories in ALFWorld, we extract 21,031 stateaction pairs to form Dexpert. These expert trajectories are optimal given the completeness of task solvability in the dataset. For implicit world modeling, we augment Dexpert with Drollout. At each state, we sample 8 non-expert actions uniformly without replacement from the admissible action list (excluding the expert action) and include the expert action, yielding 21,031 9 = 189,279 triplets for implicit world modeling. For self-reflection, we construct data by prompting the model to explain its own decisions. For each state, we use the same policy model with temperature 1.0 to propose up to 3 alternative actions. We canonicalize proposed actions and keep only unique ones. If proposed action is not in the admissible action space for that state, we discard it and instead sample uniformly at random from the remaining unselected admissible actions. The final prompt asks the model to justify why the expert action is preferable to the sampled alternatives given the current state and available tools. During training, we use batch size of 16 and learning rate of 1e5, and train with LlamaFactory (Zheng et al., 2024b) for 2 epochs. For RL training, we adopt the default hyperparameters in Verl-Agent and evaluate on the same split reported in their paper. For the evaluation, we set the maximum prompt length to be 4096, the maximum response length to be 1024, and the temperature to be 0.4. Below is data example illustrating the inputs for different methods given the same task and state (we list three examples for implicit world modeling). Full results are in Table 5. Training Example ALFWorld Task: Current State: Expert Action: Implicit World Modeling: put two book in bed. You have taken the action 1: go to desk 1 You are now at step 2 and your current observation is: You arrive at desk 1. On the desk 1, you see book 3, book 2, book 1, bowl 1, cd 1, cellphone 2, creditcard 2, mug 1, statue 2, and statue 1. take book 3 from the desk 1 examine desk 1 Nothing happens. examine drawer 1 Nothing happens. inventory You are not carrying anything. Self-Reflection: Let me think through the best course of action. The goal is to put two books in the bed. have just arrived at desk 1 and can see three books. Examining the desk or drawers does not surface new items or constraints, so those actions are unhelpful at this point. The most direct way to make progress is to start collecting the required objects. Taking book 3 from desk 1 immediately moves me toward the objective without extra navigation or redundant examinations. After securing the first book, can take second one from the same location, then navigate to the bed and place both books there. This sequence minimizes detours and aligns tightly with the task. 20 Table 5 Performance on ALFWorld. Results of closed-source models are adopted from Feng et al. (2025)."
        },
        {
            "title": "Method",
            "content": "Base: Closed-Source Model Prompting Prompting GPT-4o Gemini-2.5-Pro Base: Prompting Llama-3.2-3B-Instruct +RL (GRPO) Imitation Learning Behavior Cloning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Implicit World Modeling Self-Reflection Base: Prompting Qwen2.5-7B-Instruct +RL (GRPO) Imitation Learning Behavior Cloning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Implicit World Modeling Self-Reflection Base: Prompting Llama-3.1-8B-Instruct +RL (GRPO) Imitation Learning Behavior Cloning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO)"
        },
        {
            "title": "Implicit World Modeling",
            "content": "Self-Reflection"
        },
        {
            "title": "Cool",
            "content": "Pick"
        },
        {
            "title": "All",
            "content": "ALFWorld 48.0 60.3 8.6 78.9 78.1 92.2 83.6 97.7 85.9 99.2 14.8 77.6 78.1 91.4 82.8 96.9 82.0 97.7 25.0 83.6 80.5 93.8 85.9 97.7 85.2 98.5 75.3 92. 60.8 63.3 31.2 62.1 56.7 69.0 21.6 26.6 49.8 58.7 0.0 60.0 71.4 77.8 85.7 100.0 85.7 100. 21.6 66.1 85.7 76.9 42.9 100.0 71.4 90.9 28.6 80.0 85.7 88.9 57.1 100.0 71.4 100.0 3.7 94.7 85.2 87.5 85.2 100.0 81.5 95.0 19.3 89.3 77.8 90.9 85.2 94.7 77.8 100.0 25.9 90.0 85.2 100.0 88.9 100.0 85.2 95.0 0.0 82.6 82.4 100.0 88.2 93.3 88.2 100. 6.9 74.7 88.2 100.0 88.2 86.7 88.2 100.0 11.8 85.7 82.4 100.0 82.4 92.9 82.4 100.0 0.0 78.3 89.5 88.9 89.5 95.0 89.5 100.0 2.8 72.5 78.9 90.9 84.2 94.1 89.5 100.0 21.1 88.2 89.5 100.0 94.7 100.0 94.7 100.0 7.7 52.2 61.5 91.3 69.2 95.5 80.8 100. 3.2 64.7 69.2 96.2 76.9 100.0 65.4 91.3 19.2 56.0 53.8 95.5 84.6 92.0 80.8 95.5 25.0 93.3 78.1 97.4 87.5 100.0 90.6 100.0 33.4 90.8 78.1 90.3 90.6 100.0 93.8 100.0 37.5 97.3 90.6 95.0 87.5 100.0 87.5 100.0 B.2 WebShop From the official human demonstrations released by WebShop (Yao et al., 2022), we extract 1,571 human trajectories and convert them into the Verl-Agent (Feng et al., 2025) format, resulting in 15,464 stateaction pairs that constitute Dexpert for imitation learning. For implicit world modeling, the data has two components. The first is directly derived from Dexpert by reformatting each step into the world-modeling format, where the input contains the historical context and the action taken at the current step, and the target is an offline textual summary of the next state after executing that action (avg. length 345 characters). The second component is obtained by augmenting each expert state with non-expert actions: we let the same policy propose actions at temperatures {0.5, 0.8, 0.9} and additionally sample up to five admissible actions uniformly at random per state. We then convert the augmented samples into the same world-modeling format as the first component: for each non-expert action, we execute it in the WebShop environment to obtain the subsequent observation and derive an offline textual summary of the next state. All candidates are canonicalized and deduplicated. Merging these with the expert action yields 122,954 triplets for implicit world modeling. For self-reflection, we construct prompts that include the expert action together with 3 alternative actions and ask the model to justify why the expert action is preferable given the current state and the admissible actions. Because some actions in the raw expert trajectories are suboptimal, we apply simple quality filter that retains only actions from trajectories whose tasks can be completed within fewer than 15 steps, resulting in 6,235 reflection examples. For each such state, the alternatives are drawn using the same policy as in world modeling by mixing model-proposed actions (at the temperatures above) with uniformly sampled admissible actions; after canonicalization and deduplication, we keep 3 distinct alternatives. We intentionally retain diverse set of alternatives, including admissible but unhelpful actions, empty responses, and occasional invalid actions, to help the model learn sharper decision boundaries. During training, we use batch size of 4 and learning rate of 1e5, and train with LlamaFactory (Zheng et al., 2024b). For RL training, we adopt the default hyperparameters in Verl-Agent and evaluate on the same split reported in their paper. Below is data example illustrating the inputs for different methods given the same task and state (we list 3 examples for implicit world modeling). Full results are in Table 6. 22 Training Example WebShop Task: Current State: Expert Action: Implicit World Modeling: Self-Reflection: blue wireless bluetooth headphones, and price lower than 130.00 dollars. you have already taken the action 1: search[bluetooth blue headphones] and action 2: click[b082hmmbh1]. You are now at step 3 and your current observation is: Back to Search, < Prev, color, black red, pink, purple, black orange, blue with ears, n-red, nonears blue, non-ears pink, pink teal, purple teal, Bluetooth Headphones for Kids, PowerLocus Wireless Bluetooth Headphones, Kid Headphones Over-Ear with LED, Foldable Headset with Mic,Volume Limited, Wireless/Wired Headphone for Phones,Tablets,Laptop, Price: $24.99, Rating: N.A., Description, Features, Reviews, Buy Now. click[non-ears blue] click[non-ears blue] After clicking on color option, this page is product-details page. The page displays various product attributes such as color options, size options, and package quantity selections, along with sections for product description, features, reviews, and \"Buy Now\" button. Navigation controls include \"Back to Search\" link and \"< Prev\" button to return to the previous page. click[< prev] After clicking the < Prev button, this page is search-results page displaying multiple product listings for Bluetooth headphones and related accessories. The page includes navigation controls such as Back to Search and Next >, along with list of product items showing titles, prices, and product IDs. Each listing is accompanied by clickable option to view more details. click[buy now] After clicking \"Buy Now\" on the product details page, this page is checkout confirmation or thank-you page. It contains message thanking the user for their purchase, section for entering or pasting code, and displays the purchased items ASIN along with selected options such as color. The page also includes reward score and details, though the specifics of the reward are not elaborated. Navigation options are limited to search function at this stage. will now proceed with step-by-step reasoning. The task constraints are: select blue wireless Bluetooth headphones priced lower than $130.00. The current situation is that you are on product details page for specific item, which includes color selection menu with multiple options, product title, price, and rating information, along with tabs for viewing the product description, features, and reviews, as well as Buy Now button. The navigation options include returning to the search or going back to the previous page. Click[non-ears blue] wins because it directly addresses the color requirement and allows for further evaluation of the product details. Other actions fail because they either do not address the color requirement or may lead to irrelevant results, such as selecting different color, returning to the search results page without making color selection, or selecting blue color option with ears which may not be the desired style. Therefore, click[non-ears blue] is optimal. 23 Table 6 Performance on WebShop. Results of closed-source models are adopted from Feng et al. (2025). WebShop score succ. 31.8 42.5 1.3 17.2 55.1 89.4 71.9 97.9 67.2 93.8 4.4 78.8 62.1 92.1 69.5 95.1 77.0 93.6 0 2.1 66.8 90.9 72.7 96.0 72.5 94. 23.7 35.9 0 3.9 41.8 82.0 60.2 92.2 52.7 89.8 0.8 64.8 51.6 84.4 56.2 91.4 62.2 89.8 0 0.8 47.3 80.5 58.6 91.4 58.2 89."
        },
        {
            "title": "Method",
            "content": "Base: Closed-Source Model"
        },
        {
            "title": "Prompting\nPrompting",
            "content": "Base: Llama-3.2-3B-Instruct Prompting +RL (GRPO) Imitation Learning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Base: Qwen-2.5-7B-Instruct Prompting +RL (GRPO) Imitation Learning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Base: Llama-3.1-8B-Instruct Prompting +RL (GRPO) Imitation Learning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) GPT-4o Gemini-2.5-Pro Behavior Cloning Implicit World Modeling Self-Reflection Behavior Cloning Implicit World Modeling Self-Reflection Behavior Cloning"
        },
        {
            "title": "Implicit World Modeling",
            "content": "Self-Reflection 24 B.3 BFCLv3 We follow the default multi-turn function call split of the BFCLv3 (Patil et al., 2025) benchmark, which categorizes tasks into Base, Long-Context, Miss Function, and Miss Parameters. Base contains foundational yet diverse multi-turn interactions, where all necessary information, including the user request, execution results from previous turns, and exploratory function outputs, is available to complete the task without ambiguity. Long-Context evaluates the models ability to maintain accuracy in lengthy, information-dense settings by introducing large amounts of extraneous data (e.g., hundreds of files or thousands of records), thereby testing its capacity to extract essential details under cognitive load. Miss Function assesses whether the model can identify when no available function can fulfill the user request; once this limitation is recognized, the missing functions are provided in later turns, requiring the model to adapt to newly available capabilities. Miss Parameters examines whether the model can detect when essential parameters are absent from the user request and cannot be inferred from the system state, prompting it to request clarification rather than making unwarranted assumptions. As the default BFCLv3 benchmark does not provide training split, for constructing the training set, we exclusively use samples from the Base category. We randomly select 75% of them (125 trajectories) as the expert trajectories Dexpert for imitation learning. Each trajectory consists of multiple steps and interactions, which we further split into individual steps to improve training efficacy. For implicit world modeling, the data has two components. The first is directly derived from Dexpert by reformatting each trajectory into the world-modeling format, where given the historical context and the action at the previous step, the model predicts the next state. This yields 1,264 training examples. The second is generated through augmentation: for each state in the expert trajectory, we let target model sample ten alternative actions in addition to the expert action, producing 11,904 samples, following the same process as in ALFWorld. For self-reflection, we construct training data by prompting the model to explain its decisions, emphasizing why the expert action is preferable to other available actions in the current state, including the set of tools defined earlier. After filtering small number of low-quality samples where the concluded action did not match the expert action, we obtain 1,200 training examples. We train with batch size of 16, learning rate of 1e5, using LlamaFactory (Zheng et al., 2024b). For inference, we adopt the vLLM infrastructure for efficiency. data example showing inputs for different methods under the same task and state is given below. Full results are in Table 7. 25 Training Example BFCLv3 Task: Go to the workspace directory and move one of the log.txt files into new directory archive. Current State: Located at the workspace directory with access to the following tools: cat, cd, cp, diff, du, echo, find, grep, ls, mkdir, mv, pwd, rm, rmdir, sort, tail, touch, wc. Expert Action: Move the log.txt file to the archive directory. Implicit World Modeling: ls Find that the archive folder existed. mv mv tool can move the targeted file to the targeted folder cat, cd, cp, diff, du, echo, find, grep, mkdir, pwd, rm, rmdir, sort, tail, touch, wc. Cannot help fulfill the users task. Self-Reflection: Given the users request to move the log.txt file into new directory called archive, the most appropriate action is to use the mv command. This action directly achieves the goal without any unnecessary steps. The rm command would remove the file instead of moving it, which is not what the user wants. Creating directory using mkdir is redundant since archive already exists. Sorting the file or listing its contents wont help in archiving it. Therefore, moving the file to the archive directory is the best course of action. Table 7 Performance on BFCLv3 (Patil et al., 2025) multi-turn scenarios."
        },
        {
            "title": "Method",
            "content": "I.D."
        },
        {
            "title": "OOD",
            "content": "Base Long Cont. Miss Func. Miss Para. Llama-3.2-3B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Qwen-2.5-7B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Llama-3.1-8B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection 1.3 9.3 13.3 21.3 5.3 8.0 13.3 12. 6.7 8.0 8.0 17.3 1.3 0.0 1.3 5.3 4.0 5.3 10.7 2.7 8.0 1.3 4.0 0.0 1.3 6.7 12.0 14.7 12.0 9.3 14.7 10. 4.0 10.7 10.7 6.66 1.3 21.3 25.3 29.3 10.6 26.7 29.3 32.0 6.7 16.0 20.0 20.0 26 Avg. 1.3 9.3 13.0 17.7 8.0 12.0 16.0 14.0 6.8 9.0 10.7 11.0 B.4 Tau-Bench We conduct experiments using the retail task from Tau-Bench. In Tau-Bench, the retail task is divided into training set and an evaluation set, comprising 495 and 115 tasks, respectively. We employ high-performing instruction-tuned LLaMA-family model to collect expert trajectories on the training set. For each task, the inference temperature is set to 1.0, and four trajectories are generated. The trajectory with final reward of 1 is selected as the expert trajectory; if multiple such trajectories exist, one is chosen at random, and if none achieves reward of 1, the task is discarded. This process yields expert trajectories for 452 tasks, resulting in total of 5,239 observation, action pairs. For the world model data, we use the target model to propose five action candidates for each observation in the expert trajectories. To avoid repetitive tool calls and promote exploration, we remove the tool used in the expert action from the corresponding tool set of each expert observation, allowing the model to select from the remaining tools. The selected action is then executed in the environment to obtain the next observation. Each resulting expert observation, action, next observation triplet is included in the training dataset for the world model. For the self-reflection data, for each expert observation, expert action pair, we selecte three alternative actions from the five corresponding world model datapoints and present them to the model itself for reflection, prompting it to explain the rationale behind the expert action choice. We filter out small number of low-quality reflection samples, resulting in total of 5,233 training instances. We adopt LLamaFactory (Zheng et al., 2024b) as the training codebase. For imitation learning, we train for 6 epochs with learning rate of 1e-5. For implicit world model learning, we train for 1 epoch with learning rate of 5e-6. For self-reflection, we conduct 6 epochs of SFT with learning rate of 1e-5. In all training configurations, the batch size is fixed at 16. We report the full table in Table 2 as Tau-Bench does not contain more fine-grained metrics. Training Example Tau-Bench Task: Current State: Expert Action: Implicit World Modeling: Act as retail agent to assist customers in modifying their orders. Sure! My name is Sophia Nguyen, and my zip code is 20171. \"function_call\": \"Sophia\", \"last_name\": \"Nguyen\", \"zip\": \"20171\" \"find_user_id_by_name_zip\", \"arguments\": \"first_name\": Action: \"find_user_id_by_name_and_zip\" Next observation: \"Unknown action find_user_id_by_name_and_zip\" Self-Reflection: The customer has provided their name, Sophia Nguyen, and their zip code, 20171. The goal here is to identify the user based on these details so we can provide them with the appropriate service or information theyre seeking. The candidate action, find_user_id_by_name_zip, seems to be the right approach as it directly uses the name and zip code provided by the customer to locate their user ID. However, the alternative actions listed all have the same name, find_user_id_by_name_and_zip, but they result in an unknown action outcome, indicating that this specific function or command is not recognized within the system. This suggests that these alternatives are not valid options for the current system and cannot be used effectively. Therefore, would proceed with the candidate action, find_user_id_by_name_zip, because its the only viable option that matches the customers provided information and has produced valid user ID outcome, sophia_nguyen_2370. This action aligns perfectly with the objective of identifying the customer based on their name and zip code. The action is: {arguments: {\"first_name\":\"Sophia\",\"last_name\":\"Nguyen\",\"zip\":\"20171\"}, name: find_user_id_by_name_zip} 27 B.5 SearchQA Expert Trajectories Collection. We selected all 3-hop and 4-hop tasks from the MuSiQue training dataset, along with 1,438 randomly sampled 2-hop tasks, to fit within scenarios that require multi-step reasoning for solving complex problems. Finally, we have 7,000 tasks in total. Since the training data lacks fine-grained reasoning traces like the thinkingsearchanswer structure as Jin et al. (2025), we used the Search-R1 model to generate expert data. Specifically, we set the temperature to 1.0 and generated 5 trajectories for each task, retaining only those whose final answers match the ground truth. To reduce redundancy, we keep at most 2 correct trajectories per task. This process yields 2,082 trajectories containing total of 7,691 stateaction pairs for imitation learning. World Modeling Data Construction Consistent with the observations of Jin et al. (2025), we find that directly predicting the content of retrieved documents yields suboptimal performance, as many tokens are not directly relevant to the search query. To address this, we first instruct the model to summarize the retrieved documents, and then let the model predict these summaries rather than the full text. For each state in the expert trajectory, we let the model generate 30 alternative actions with temperature of 1.0, enabling it to internalize the environment dynamics from its own early experiences substantially. If generated action is invalid, i.e., the query is not enclosed within the <search></search> tags, we return the feedback: Format error! You must enclose the search query within the <search></search> tags if external knowledge is required. Self-Reflection Data Construction To construct the self-reflection training dataset, we randomly sample 2 alternative actions for each state. For each instance, the model is prompted to generate fine-grained reasoning explaining why the expert action is superior to the alternatives, based on the current state, the expert action, the alternative actions, and the retrieval documents associated with these actions. This procedure resulted in 7,691 training data containing detailed reasoning processes. Training Details We adopt LLamaFactory (Zheng et al., 2024b) as the codebase and perform full-parameter tuning on 4 H100 GPUs using ZeRO-3. Specifically, for imitation learning and self-reflection, we train for 3 epochs with learning rate of 1 105, context window of 8192 tokens, and batch size of 2 per GPU, while setting the gradient accumulation steps to 16. For implicit world model learning, we utilize the world model data from expert trajectories to achieve 1:1 ratio with the imitation learning dataset and train under the same settings. For Reinforcement Learning (RL), we employ the Search-R1 codebase and train on 8 H100 GPUs. All settings are kept consistent with Jin et al. (2025), except that we use the F1 score as the reward, set the maximum number of retrieval interactions to 6, configure the context window to 12,280 tokens, and specify maximum output length of 2,048 tokens. For the training data, we use all training tasks from the MuSiQue dataset. An example illustrating the inputs for different methods is provided below, with the full results presented in Table 8. Since our infrastructure also supports Gemma in the SearchQA setting, we further demonstrate that our method generalizes well across different models and model sizes. In particular, experiments on Gemma-4B-Instruct show that our approach significantly outperforms behavior cloning in both in-distribution and out-of-distribution scenarios. Since the early version of verl (Sheng et al., 2025) used by SearchQA does not yet support the Gemma family of models, we are unable to provide RL results of Gemma. For evaluation, we follow the setting of Goldie et al. (2025); Wu et al. (2024), where 500 examples are randomly sampled from the test set to accelerate the evaluation process. 28 Training Example SearchQA Task: Current State: Expert Action: Implicit World Modeling: Who was the ninth governor-general in the country where Boridi is located? <think>I need to find the ninth governor-general in the country where Boridi is located. Ill search for it.</think><search>Boridi</search><information>Doc 1(Title: Boridi) Boridi Boridi is village in the Owen Stanley Range in Central Province, Papua New Guinea. Boridi is Koiari village ......</information> <think>I found out that Boridi is located in Papua New Guinea. Now need to find the ninth governor-general in Papua New Guinea.</think><search>ninth governorgeneral in Papua New Guinea</search> <search>ninth governor-general in Papua New Guinea</search> The ninth Governor-General of Papua New Guinea was Sir Michael Ogio ...... <search>governor-general of Papua New Guinea history</search> The first Governor-General of Papua New Guinea was Sir John Guise, Papua New Guinean who was vocal supporter of independence ...... <search>Papua New Guinea governor-general list</search> The Governor-General of Papua New Guinea is the representative of the Papua New Guinean monarch, currently Queen Elizabeth II, and is known in Tok Pisin as Missis Kwin...... Self-Reflection: Okay, Ive found that Boridi is located in Papua New Guinea. Now, need to find the ninth governor-general of Papua New Guinea. Im going to search for ninth governorgeneral in Papua New Guinea. This query is specific and should return the information need. Im hoping to find document that mentions the ninth governor-general by name and provides some information about their background and term in office. Im not sure what the document will look like, but Im hoping it will be straightforward answer to my question. Im also considering searching for governor-general of Papua New Guinea history or Papua New Guinea governor-general list, but think the first query is more likely to return the information need. Ill see what the search results bring up and go from there. 29 Table 8 Performance on SearchQA (Jin et al., 2025) where we include four multi-hop question answering benchmarks: Musique (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020) and Bamboogle (Press et al., 2022)."
        },
        {
            "title": "Method",
            "content": "Base: Prompting Llama-3.2-3B-Instruct +RL (GRPO)"
        },
        {
            "title": "Imitation Learning Behavior Cloning",
            "content": "+ RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Implicit World Modeling Self-Reflection Base: Prompting Gemma3-4B-Instruct +RL (GRPO) Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Base: Prompting Qwen2.5-7B-Instruct +RL (GRPO) Imitation Learning Behavior Cloning + RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO) Implicit World Modeling Self-Reflection Base: Prompting Llama-3.1-8B-Instruct +RL (GRPO)"
        },
        {
            "title": "Imitation Learning Behavior Cloning",
            "content": "+ RL (GRPO) Early Experience + RL (GRPO) Early Experience + RL (GRPO)"
        },
        {
            "title": "Implicit World Modeling",
            "content": "Self-Reflection Musique HotpotQA 2WikiMultiHopQA Bamboogle"
        },
        {
            "title": "All",
            "content": "SearchQA 22.2 27.8 30.6 38.4 37.8 49.8 37.0 42.0 32.3 - 38.9 41.9 42.3 29.8 50.9 39.2 42.7 45.7 52.9 48.1 51.2 31.3 37.9 42.6 40.2 43.9 45.6 46.4 50.2 35.1 35.4 55.2 63.2 59.1 58.6 58.4 60. 35.8 - 53.5 55.6 56.9 40.3 55.6 57.7 61.6 57.7 54.6 52.9 55.4 49.4 54.4 58.8 59.7 58.6 59.7 58.3 59.4 21.1 29.7 39.8 44.8 43.4 50.0 42.3 46.3 23.4 - 41.0 42.8 44.3 28.8 48.0 44.8 48.6 46.8 51.1 48.4 51. 32.1 38.4 45.4 47.1 48.0 49.8 48.0 51.0 24.3 35.0 46.8 47.8 49.5 53.4 47.3 50.2 24.4 - 45.4 46.3 46.8 34.5 51.2 52.1 52.0 51.3 53.5 53.9 55.7 39.5 40.1 49.3 51.0 53.1 50.8 53.1 52.9 13.3 25.0 38.0 43.6 39.0 44.6 38.6 43. 10.3 - 35.5 37.0 40.5 19.3 40.1 39.9 47.7 40.8 45.9 42.0 46.5 21.0 33.1 41.0 47.0 44.3 50.6 41.8 47.7 30 B.6 ScienceWorld We follow the default split of ScienceWorld (Wang et al., 2022) with the AgentGym (Xi et al., 2024) setup under the Verl-Agent (Feng et al., 2025) framework. From the expert trajectories in ScienceWorld, we extract 14,506 stateaction pairs to form Dexpert. These expert trajectories are optimal given the completeness of task solvability in the dataset. For implicit world modeling, we augment Dexpert with Drollout. At each state, we sample 3 non-expert actions uniformly without replacement from the admissible action list (excluding the expert action) and include the expert action for implicit world modeling. For self-reflection, we construct data by prompting the model to explain its own decisions. For each state, we use the same policy model with temperature 1.0 to propose up to 3 alternative actions (2 alternative actions for Llama-3.1-8B-Instruct ). We canonicalize proposed actions and keep only unique ones. If proposed action is not in the admissible action space for that state, we discard it and instead sample uniformly at random from the remaining unselected admissible actions. The final prompt asks the model to justify why the expert action is preferable to the sampled alternatives given the current state and available tools. For all the training and evaluations, we use one-shot example, which is shown above. During training, we use batch size of 32 and learning rate of 5e6, and train with LlamaFactory (Zheng et al., 2024b) for 1 epoch. For the evaluation, we set the maximum prompt length to be 4096, the maximum response length to be 1024, and the temperature to be 0.4. Below is data example illustrating the inputs for different methods given the same task and state (we list three examples for implicit world modeling). We report the full table in Table 2 as ScienceWorld does not contain more fine-grained metrics. 31 Training Example ScienceWorld Task: Current State: Expert Action: Implicit World Modeling: Self-Reflection: Your task is to determine if aluminum foil is electrically conductive. The aluminum foil is located around the living room. First, focus on the aluminum foil. If it is electrically conductive, place it in the green box. If it is electrically nonconductive, place it in the blue box. This room is called the workshop. In it, you see: the agent substance called air table. On the table is: battery, black wire, green light bulb, which is off, orange light bulb, which is off, orange wire, red light bulb, which is off, red wire, switch, which is off. ultra low temperature freezer. The ultra low temperature freezer door is closed. You also see: door to the hallway (that is closed). connect battery cathode to yellow wire terminal connect inventory to orange terminal 1 on inventory is now connected to terminal 1 on orange. connect black wire terminal 2 to anode in green light bulb terminal 2 on black wire is now connected to anode on green light bulb. connect cathode in orange light bulb to freezer cathode on orange light bulb is now connected to terminal 1 on ultra low temperature freezer Im currently in the workshop, and my task is to determine if the aluminum foil is electrically conductive. To do this, need to place it in either the green box or the blue box, depending on its conductivity. Ive recently moved the aluminum foil to the workshop, and now see it sitting here, along with various other objects like the electric motor, the ultra low temperature freezer, and bunch of wires and light bulbs. Looking at my recent actions, connected the anode of the battery to terminal 1 on the green wire. This was deliberate step to set up circuit, but Im not sure what the ultimate goal is yet. also see that have the option to connect the cathode of the battery to terminal 1 on the yellow wire, which would complete the circuit. However, Im not sure if this is relevant to determining the conductivity of the aluminum foil. also notice that have the option to connect the door to the ultra low temperature freezer, which would result in the door being connected to the ultra low temperature freezer. This seems like strange action, and Im not sure what the purpose of this would be. Its possible that this action is related to the workshops equipment, but it doesnt seem directly relevant to my task. Another option have is to move the yellow light bulb to the blue box. This would simply move the light bulb to different location, but it doesnt seem to have any bearing on my task. Considering my goal of determining the conductivity of the aluminum foil, need to think about how can use the available equipment to test its conductivity. Im not sure what the properties of the aluminum foil are, but know that if its conductive, it should allow electricity to flow through it. Given this, think the most suitable action would be to connect the cathode of the battery to terminal 1 on the yellow wire. This would complete the circuit and allow me to test the conductivity of the aluminum foil. If the aluminum foil is conductive, it should allow electricity to flow through it, and can place it in the green box. If its not conductive, it wont allow electricity to flow, and can place it in the blue box. This action seems like the most logical choice because it directly addresses my task and allows me to test the conductivity of the aluminum foil. The other actions have available dont seem to be directly relevant to my task, and they might even be distracting or confusing. By connecting the cathode of the battery to terminal 1 on the yellow wire, can focus on my task and make progress towards determining the conductivity of the aluminum foil. 32 B.7 TravelPlanner We adapt the TravelPlanner (Xie et al., 2024a) benchmark into gym-based environment for training language agents. The original benchmark contains 1,225 queries divided into training (45 queries), validation (180 queries), and test sets. We use the 45 training trajectories that cover diverse planning scenarios across different difficulty levels based on trip duration (3, 5, or 7 days) and constraint complexity (Easy, Medium, Hard). Easy queries are primarily budget-constrained for single person. Medium queries introduce additional constraints such as cuisine type, room type, or room rules, with the number of travelers varying between 2 and 8. Hard queries include transportation preferences along with all medium-level constraints, containing three randomly selected hard constraints. We evaluate on the validation set of 180 queries. Environment Implementation. We implement TravelPlanner as gym environment with discrete action spaces and dictionary observation spaces. The state representation includes the current planning progress formatted in structured text format: query description, budget tracking (initial/spent/remaining), and the current plan status for each day showing transportation, meals, attractions, and accommodation fields. Actions are JSON objects with fields for action type (e.g., SET_TRANSPORTATION, SET_MEAL, SET_ACCOMMODATION), day number, field name, selected value, and cost. The action space dynamically generates all valid actions based on available data from reference information, including flights between cities, restaurants with cuisine types and prices, attractions, and accommodations with room rules and minimum night requirements. The environment tracks budget spending, validates constraints in real-time, and maintains planning progress through state machine that advances through each field sequentially. Expert Trajectory Collection. We use 45 annotated trajectories from the training set as expert demonstrations Dexpert. Each trajectory contains complete multi-day travel plan with ground-truth actions for transportation, accommodation, dining, and attractions. We decompose these trajectories into 1,395 individual state-action pairs using the SFTConverter, which maps expert plan entries to valid gym actions while handling city name variations and validating against environment constraints. Implicit World Modeling. For world modeling data, we generate two types of training examples. First, we reformat the expert trajectories into state-transition format where the model learns to predict next states given current state and action. Second, we perform exhaustive augmentation by executing ALL available valid actions at each state in the expert trajectories (not just sampling), collecting comprehensive state transitions to maximize coverage of environment dynamics. This process generates over 70,000 state-transition samples, providing rich supervision for learning environment dynamics including budget updates, constraint evaluations, and plan progression. Self-Reflection. We construct self-reflection data by prompting Llama-3.1-8B-Instruct to generate chainof-thought reasoning explaining why expert actions are preferable to alternatives. For each of the 1,395 state-action pairs, we explore up to 30 alternative valid actions and generate reasoning that considers multiple constraints: budget limits, minimum night stays for accommodations, restaurant diversity requirements, and round-trip completion. The reasoning generation uses temperature 0.9 with 8-way tensor parallelism to produce natural explanations while maintaining logical consistency. We do not apply additional filtering as the reasoning generation already validates constraint satisfaction. Training Details. We train models using LlamaFactory with full fine-tuning on 8 H100 GPUs using DeepSpeed ZeRO-3. For imitation learning and implicit world modeling, we train for 5 epochs with learning rate of 1e5 and cosine scheduler. For self-reflection, we extend the maximum generation length to 8K tokens to accommodate detailed reasoning. All models use 32K context windows with batch size 16 per GPU. For evaluation, we use vLLM with tensor parallelism across 8 GPUs and greedy decoding to ensure reproducibility. data example showing inputs for different methods under the same task and state is given below. Full results are in Table 9. 33 Training Example TravelPlanner Task: Current State: Expert Action: Implicit World Modeling: Self-Reflection: Please help me plan trip from St. Petersburg to Rockford spanning 3 days from March 16th to March 18th, 2022. The travel should be planned for single person with budget of $1,700. Total Days: 3, Initial Budget: $1700, Spent: $0, Remaining: $1700 Day 1: transportation: PENDING, breakfast: PENDING, attraction: PENDING, lunch: PENDING, dinner: PENDING, accommodation: PENDING Next action required: Plan day 1 transportation Available: Flight F3573659: $474, Departure 15:40, Arrival 17:04 {\"action_type\": \"SET_TRANSPORTATION\", \"day\": 1, \"field\": \"transportation\", \"value\": \"Flight Number: F3573659, from St. Petersburg to Rockford, Departure Time: 15:40, Arrival Time: 17:04\", \"cost\": 474} SET_TRANSPORTATION (F3573659) After this action, youve spent $474, leaving $1226 from your $1700 budget (28% used). Good start! Youre traveling to your destination. SKIP_TRANSPORTATION After this action, youve spent $0, leaving $1700 from your $1700 budget (0% used). Important: Consider all constraints. Flight F3573659 is valid option at $474. It satisfies all constraints and gets me to my destination efficiently. The only alternative is skipping transportation, which prevents reaching the destination. Given all constraints and optimization factors, Flight F3573659 is indeed the best choice as its the most cost-effective option that enables the trip. Therefore, Flight F3573659 is optimal. Table 9 Performance on TravelPlanner with different training approaches. Model Method Commonsense Constraint Hard Constraint Micro Macro Micro Macro Final Pass Rate Llama-3.1-8B Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Qwen-2.5-7B Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Llama-3.2-3B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection 0.0 25.0 38.9 42.2 0.0 23.9 30.6 43.9 0.0 27.8 44.4 46.1 61.0 54.8 56.9 61.0 63.3 48.1 58.1 63. 58.1 57.1 56.7 61.9 46.7 46.7 42.2 51.1 51.7 41.7 45.6 46.7 48.9 46.1 42.8 52.2 0.0 17.2 25.0 32.2 0.0 16.7 22.2 31. 0.0 19.4 28.3 32.2 36.9 82.6 84.0 84.7 35.0 77.2 82.2 86.9 34.2 81.7 84.7 87.0 34 B.8 WebArena Given that the full evaluation set in WebArena (Zhou et al., 2024) is lengthy and includes many similar tasks, we follow prior work (Qi et al., 2024; Wei et al., 2025b) and evaluate our trained agents on WebArena-Lite (Liu et al., 2024), more efficient and balanced subset of 165 high-quality, challenging tasks, hand-selected from the original 812. Therefore, the remaining 647 tasks in WebArena, excluding those in the evaluation set, are used for agent training. To obtain expert demonstrations in WebArena, we extract successful trajectories from the highest-performing agents on the public WebArena leaderboard.1 Specifically, we select those that include accessibility tree information in their observations, such as IBM CUGA (Marreed et al., 2025), ScribeAgent (Shen et al., 2024), Learn-by-Interact (Su et al., 2025), and AgentOccam (Yang et al., 2024). After filtering out the unsuccessful trajectories, we obtain 554 successful ones and 7,044 state-action pairs, forming Dexpert. To branch out from the expert trajectories for implicit world modeling, we augment Dexpert to form Drollout. For each state in Dexpert, we let the target model (to be trained) propose 5 non-expert actions using free-form generation, excluding any that are identical to the expert action. For each resulting next state, we apply an additional processing step: using the same model, we generate concise summary of the next-state observation conditioned on the task, replacing the raw observation to reduce noise and emphasize task-relevant information. We then include the expert action together with the sampled ones to create triplets of the form (current state, action, summarized next state), resulting in 7,044 6 = 42,264 triplets in total for each model. For self-reflection, we construct DSR by prompting the model to explain why the expert action is preferable to the sampled alternatives in the current state. We use the same 5 alternatives from Drollout, canonicalize action strings to avoid duplicates, and replace any invalid actions (e.g., referring to non-existent UI elements) with randomly sampled valid ones. The final prompt includes the current state, the admissible actions, and the expert action, and asks the model to justify the optimality of the expert choice in terms of task progress, constraint satisfaction, and efficiency. We filter out low-quality generations where the explanation incorrectly supports non-expert action, leaving 3,190 high-quality self-reflection examples. All models are trained for 2 epochs with learning rate of 1e5 and cosine scheduler. We report our full numbers on WebArena-Lite in Table 10. 1https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit?usp=sharing 35 Training Example WebArena Task: Current State: Expert Action: Implicit World Modeling: SelfReflection: What are the top-3 best-selling products in January 2023? [1281] RootWebArea Products / Inventory / Catalog / Magento Admin [1334] link Magento Admin Panel [1343] img Magento Admin Panel [1286] menubar orientation: horizontal [1288] link ue604 DASHBOARD [1292] link ue60b SALES [1296] link ue608 CATALOG [1300] link ue603 CUSTOMERS [1304] link ue609 MARKETING [1308] link ue602 CONTENT [1312] link ue60a REPORTS . . . [2146] button ue605Filters . . . Click [1312]. click [1312] The user clicked on the REPORTS link in the horizontal menubar, which changed the focus to this link. new vertical menu expanded below it, showing categories such as Marketing, Reviews, Sales, Customers, Products, Statistics, and Business Intelligence, each with sub-links. The main product table remains visible, but the navigation context has shifted to the reports section, enabling access to detailed reporting options. click [2146] Clicking the Filters button shifted focus to the Search by keyword textbox and likely opened the filter/search area. The product list updated to show different products such as Sol Active Short-32-Green and Sol Active Short-33-Purple, indicating that the action updated the product list via filtering. go_back Navigates from the Products / Inventory / Catalog page to the Dashboard / Magento Admin page. The view changes from product table to dashboard with summary information such as revenue, tax, shipping, and tabs like Bestsellers and Most Viewed Products. This navigation may reset the page state. The goal is to find the top-3 best-selling products in January 2023. The current Products / Inventory / Catalog page lists inventory details but not sales data or best-seller rankings. Clicking on REPORTS is the most logical choice, as it typically contains sales metrics, customer insights, and product performance reports. From there, it should be possible to select sales reports, filter by January 2023, and retrieve the ranked list of best sellers. Alternatives such as filtering or searching products on the catalog page will not reveal sales statistics, and going back to the dashboard may only provide limited summary widgets without detailed filtering options. Therefore, selecting REPORTS directly aligns with the task objective and is the optimal next step to access the necessary data. 36 Table 10 Performance on WebArena-Lite. Results of closed-source models are borrowed from Qi et al. (2025)."
        },
        {
            "title": "Method",
            "content": "Base: Closed-Source Model Prompting Prompting GPT-4-Turbo GPT-4o Llama-3.2-3B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Qwen-2.5-7B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Llama-3.1-8B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Llama-3.3-70B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection Qwen-2.5-72B-Instruct Base: Prompting Imitation Learning Behavior Cloning Early Experience Early Experience Implicit World Modeling Self-Reflection WebArena-Lite Reddit Gitlab CMS Map OSS Avg. SR 10.5 10.5 0.0 0.0 0.0 11.1 0.0 12.5 0.0 0.0 0.0 0.0 11.1 0.0 11.1 0.0 8.3 0.0 12.5 0.0 25.0 0. 16.7 10.0 0.0 7.7 15.4 15.4 7.7 0.0 15.4 7.7 0.0 0.0 0.0 15.4 15.4 8.3 16.7 16.7 15.4 7.7 15.4 15. 14.3 20.0 36.7 20.0 13.3 11.1 17.6 13.9 0.0 10.3 2.4 11.9 2.4 7.4 7.1 11. 7.1 0.0 7.3 11.9 17.1 14.3 16.7 23.8 12.2 19.0 26.2 23.8 1.7 0.0 13.8 3.5 1.8 4.0 8.6 5.2 0.0 11.9 8.6 10. 7.1 17.2 19.0 14.0 6.9 12.3 12.3 14.0 3.2 3.2 9.7 6.5 0.0 9.1 6.5 6.5 0.0 8.0 16.1 3.2 3.2 16.1 19.4 19. 6.7 16.7 19.4 20.0 1.2 6.1 8.5 7.3 1.8 4.2 7.3 6.1 0.6 4.9 8.5 8.5 9.1 13.3 16.4 15.2 8.5 12.7 17.6 15."
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "Meta Superintelligence Labs",
        "The Ohio State University"
    ]
}