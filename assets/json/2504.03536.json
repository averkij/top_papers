{
    "paper_title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration",
    "authors": [
        "Boyuan Wang",
        "Runqi Ouyang",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Guosheng Zhao",
        "Chaojun Ni",
        "Guan Huang",
        "Lihong Liu",
        "Xingang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \\textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models."
        },
        {
            "title": "Start",
            "content": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration Boyuan Wang1, 2*, Runqi Ouyang1, 2*, Xiaofeng Wang1, 2*, Zheng Zhu1* , Guosheng Zhao1, 2, Chaojun Ni1, 3, Guan Huang1, Lihong Liu2, Xingang Wang2 1GigaAI 2Institute of Automation, Chinese Academy of Sciences 3Peking University 5 2 0 2 ] . [ 1 6 3 5 3 0 . 4 0 5 2 : r Illustration of HumanDreamer-X. The pipeline initiates with single-image reconstruction to generate coarse 3D avatar, Figure 1. providing essential geometric and appearance priority for the restoration process. This approach facilitates the attainment of higherquality 3D avatar, suitable for subsequent animation tasks."
        },
        {
            "title": "Abstract",
            "content": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, novel framework that integrates multi-view human generation and reconstruction into unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving PSNR of up to 25.62 dB, while also showing generalization capabilities on inthe-wild data and applicability to various human reconstruction backbone models. 1. Introduction *These authors contributed equally to this work. Corresponding authors. zhengzhu@ieee.org, xingang.wang@ia.ac.cn. Project Page: https://humandreamer-x.github.io/ Creating 3D human avatars is gaining increasing significance across various domains, including virtual reality, gaming, and film production. Among the numerous methods, creating from single image represents common, practical, and user-friendly approach. Nevertheless, constructing versatile human avatar with diverse shapes, appearances, and clothing from just single image remains substantial challenge. Traditional reconstruction methods based on Neural Radiance Fields (NeRF) [36] and 3D Gaussian Splatting (3DGS) [24] enable multi-view reconstruction but are incapable of achieving single-image reconstruction [18, 20, 23, 26, 28, 53]. Despite these advancements, single-image reconstruction remains particularly challenging. To address these limitations, current approaches for single-view human reconstruction often integrate techniques from image or video generation [12, 13, 27, 31, 34, 41, 43, 63, 68]. common strategy, as seen in methods like PSHuman [27] and AniGS [43], involves first generating multi-view images using generative models [17, 39, 45, 50, 62, 66] and then performing subsequent reconstruction with meshbased or Gaussian-based techniques. However, this decoupled paradigmwhere generation precedes reconstructionheavily depends on the geometric and appearance consistency of the generative model. Any inconsistency in these aspects can lead to severe artifacts in the reconstruction stage, such as fragmented or distorted limbs. To alleviate the aforementioned issues, we introduce HumanDreamer-X, novel framework that integrates multiview human generation and reconstruction into unified pipeline. This integration facilitates mutual enhancement between the two processes, offering supplementary information on geometry and appearance. Consequently, this significantly improves the geometric consistency and visual fidelity of the reconstructed 3D models, effectively alleviating problems related to fragmented and blurred limbs in the generated models. As illustrated in Fig. 1, our framework first utilizes 3DGS to reconstruct the human body from single image and then renders multi-view images. Leveraging the geometric representation capability of 3DGS, these rendered multi-view images provide strong geometric and appearance priority for the subsequent generative process. Building upon this, HumanFixer is introduced to refine the renderings, producing photorealistic images. Finally, these restored images are further utilized to guide 3DGS in reconstructing high-quality human model. Compared to decoupled approaches, our unified framework effectively bridges the gap between reconstruction and generation, producing higher-quality avatars suitable for diverse downstream applications. Moreover, in the context of multiview human generation, directly generating such videos often leads to blurriness because of temporal inconsistencies. To tackle this issue, we explore the inherent challenges associated with attention mechanisms within attention layers and propose modulation strategy. This strategy effectively enhances geometric detail and identity consistency across multiple views, thus overcoming the limitations of traditional approaches. Notably, our experiments demonstrate that our approach significantly improves the generation metrics by 16.45% in PSNR and the reconstruction metrics by 12.65% in PSNR. And also demonstrating its generalization capabilities on inthe-wild data and its adaptability to various human reconstruction backbone models. The main contributions of this paper are summarized as follows: We propose novel framework, HumanDreamer-X, for generating animatable avatars from single-view image by coupling 3D reconstruction with video restoration. This unified approach significantly enhances the quality and consistency of reconstructed avatars compared to existing decoupled methods. We identify and address attention-related deficiencies in the generation of multi-view videos, which often lead to inconsistencies and blurriness. To mitigate these issues, we introduce an attention correction module that refines the temporal attention mechanism, thereby improving the quality of restored videos and ensuring better geometric and identity preservation. Extensive experimental results demonstrate the superior performance of HumanDreamer-X across multiple datasets. Specifically, our method achieves higher fidelity in avatar reconstruction, showcasing its practical applicability and efficiency. It also demonstrates generalization capabilities on in-the-wild data and is applicable to various human reconstruction backbone models. 2. Related Work 2.1. Single-image Human Recosntruction With the advancement of 3D representation methods such as mesh [7, 21], NeRF [13, 36], and 3DGS [24, 30, 56, 60], human reconstruction has seen significant improvements [9, 14, 16, 22, 37, 40, 42, 65]. However, these methods typically require large number of multi-view images or videos for reconstruction, which limits their applicability. In contrast, Single-image human reconstruction is more flexible task, aiming to reconstruct 3D human model from just one image [12, 13, 27, 34, 41, 43, 63, 68]. However, it is inherently an ill-posed problem, presenting considerable challenges. Current single-image human reconstruction methods integrate techniques from generative approaches [4, 44, 57, 61, 67]. These methods first generate unseen viewpoints [13] or multi-view images [12, 27, 34, 41, 43], followed by employing 3D representations to reconstruct the human. To ensure geometric consistency, methods such as SMPL [27, 43], masks [13], segmentation maps [12], 2 and pose estimation [41] are utilized to drive the generation process. However, these generate-then-reconstruct approaches heavily rely on the geometric and appearance consistency of the generated images, and inconsistencies can lead to issues like fragmented limbs and blurriness in the reconstructed models. Recently, there have been attempts [68] to infer 3DGS parameters directly from single image using feed-forward networks, but this requires extensive datasets for training. The approach most closely related to ours is SIFU [63], which refines the coarse texture of reconstructed models using generative models. However, SIFU uses frozen image generation model for frame-by-frame refinement, making it difficult to maintain inter-frame consistency. 2.2. Controllable Human Generation With the rapid advancements in image and video generation [46, 32, 38, 51, 55, 57, 61, 64], diffusion-based human generation models [17, 19, 33, 35, 45, 47, 48, 52, 58, 62, 66] have gained increasing attention. Among these, Animate Anyone [17] stands out as representative work, which constructs 3D U-Net upon Stable Diffusion [44] for modeling temporal information, and incorporates skeleton information as driving signals for controlling motion generation. To further enhance temporal consistency, MimicMotion [62] leverages pre-trained video generation model SVD [4] to improve the coherence of generated sequences. In contrast, UniAnimate [50] employs MAMBA-based [10] techniques for enhanced temporal modeling. Additionally, several approaches explore different motion control signals. For instance, DisCo [49] and MagicAnimate [54] utilize DensePose [11] as representation of the human body [49, 54], while Champ [66] integrates multi-modal information including depth, normal, and semantic signals derived from the 3D parametric human model SMPL [29]. Despite their ability to generate photorealistic frames, these models often struggle to fully ensure geometric and appearance consistency across frames. This inconsistency poses significant challenges for subsequent reconstruction tasks. 3. Method 3.1. Preliminary 3D Gaussian Splatting. 3DGS represents voxelbased rendering technique for scene representation, where the core concept involves modeling the scene using an optimized set of 3D Gaussian distributions = {N (xi, Σi)}N i=0. Each Gaussian distribution is parameterized by its position xi R3, covariance matrix Σi (which defines the ellipsoidal shape), and radiance attributes such as color ci and opacity αi. During rendering, these Gaussian distributions are projected onto the image plane. The contribution of each pixel wici/ (cid:80) 2 (u ui)Σ1 is computed via radial basis functions defined as wi(u) = (u ui)(cid:1), where the weight wi αi exp (cid:0) 1 decays with distance from the center of the Gaussian. This process is achieved through differentiable rasterization, which integrates depth sorting and weighted blending ((cid:80) wi), thereby facilitating gradient-based optimization for high-quality scene reconstruction. Video Diffusion Model. Video Diffusion Models (VDM) are generative frameworks that extend diffusion-based image synthesis to dynamic scenes by modeling sequential data in latent space. The core principle involves forward diffusion process that gradually corrupts video data x0 into 1 βtxt1, βtI), noise via steps: q(xtxt1) = (xt; where βt controls noise injection. VDM enhances temporal coherence by integrating spatiotemporal layers into pretrained latent space, enabling high-fidelity video generation from images or text while leveraging efficient VAE-based compression. As reverse denoising network it learns to approximate the posterior pθ(xt1xt), typically parameterized as xt1 = µθ(xt, t) + σtϵ (ϵ (0, I)), reconstructing coherent video frames through iterative refinement. 3.2. Overall Framework of HumanDreamer-X Traditional human reconstruction methodologies [18, 20, 23, 26, 28, 53] encounter substantial challenges due to their reliance on multi-view images. Recent advances [12, 13, 27, 31, 34, 41, 43, 63, 68] have endeavored to address this limitation by leveraging generative priors to compensate for the absence of invisible views. Nevertheless, the decoupling of generation from reconstruction has resulted in deficiency of geometric consistency among the generated multi-view images. Our proposed framework, HumanDreamer-X, integrates reconstruction and generation into unified pipeline. In this pipeline, the reconstruction step provides initial geometry and appearance priorities, and the generative model then refines the reconstructions by restoring coarse renderings. The overall framework is illustrated in Fig. 2. Specifically, we first use the 3DGS model [18, 28] to reconstruct an avatar Ac from single reference image IR: Ac = 3DGS(IR, θSMPL), (1) Following previous avatar reconstruction models, the initial point cloud for 3DGS is derived from the estimated Skinned Multi-Person Linear (SMPL) θSMPL. This ensures that even single image can reconstruct coarse human geometry and appearance, providing basic priority for subsequent refinement. Then, multi-view video Vc of the avatar is rendered using the trained 3DGS avatar: Vc = {Ac(di) = 1, 2, . . . , n}, (2) where di represents specific horizontal angle, and Ac(di) renders the avatar at that angle. Due to the limited avail3 Figure 2. Overall framework of the proposed HumanDreamer-X. The process begins by initializing coarse 3DGS avatar using reference image. rendered video serves as guide, providing geometric and appearance priors. Subsequently, HumanFixer performs video restoration, wherein an attention modulation is employed to enhance video consistency. Throughout this process, the restored video is used to continuously update the 3DGS model, ultimately resulting in refined 3DGS avatar. ability of only one viewpoint, the resulting model Ac provides only basic geometric and appearance priority of the avatar, leaving issues such as blurriness and artifacts in unseen views unresolved. To address these issues, we introduce HumanFixer, video generation model designed to restore details in the initial GS renderings. HumanFixer is built upon the pretrained video diffusion model [4], utilizing the coarse video Vc and the reference image IR as conditions to generate multi-view refined videos (see Sec. 3.3 for more details). The refined videos Vr capture the textures and geometry present in the reference image IR, making them suitable for modeling refined human avatar Ar. This approach leverages the geometry and appearance priority provided by 3DGS and exploits the temporal consistency inherent in video generation models, ensuring enhanced multi-view consistency in the repaired avatar. Additionally, to address blurriness in multi-view video generation, we investigate attention mechanisms and propose an attention modulation strategy (see Sec. 3.4 for more details). The refined video, enhanced through this strategy, is then utilized to optimize the human avatar. 3.3. Training and Inference of HumanFixer Reconstructed avatars derived directly from single-image inputs exhibit issues such as blurring at invisible views. To address these problems, we introduce HumanFixer for refining coarse avatars. This section will present the methodologies for the training and inference of HumanFixer. Figure 3. The creation of the dataset for training HumanFixer. First, we use Blender to render scans and obtain the ground truth video. Next, we employ the frontal image and its corresponding SMPL prior to reconstruct coarse 3DGS model, followed by rendering multi-view videos. This process yields paired video data for training. Training. HumanFixer hinges on constructing coarseIn this section, we propose conrefined pair dataset. 4 crete pipeline for dataset collection, as illustrated in Fig. 3. The steps are as follows: Initially, we utilize Blender to render multi-view video Vgt from each 3D scan, serving as the ground truth video. Subsequently, we leverage the GS model to reconstruct human avatar from single image, and use Eq. 2 to render multi-view coarse images Vc, paired with their corresponding ground truth videos Vgt, form the repair dataset. This dataset is designed to refine avatar reconstructions by providing examples of both lowquality and high-quality renderings, allowing the HumanFixer model to learn how to enhance the coarse images into refined, detailed videos. The architecture of HumanFixer is illustrated in Fig. 2. It employs SVD [4] as the backbone and leverages the lowquality video to guide the generation process. Additionally, it integrates an IP-Adapter [58] to inject person ID information through cross-attention mechanisms. During the training of HumanFixer, we first feed the coarse video Vc into Variational Autoencoder (VAE) [25] encoder to obtain the latent feature zc = E(Vc). Then, zc is used as condition, providing both geometric and apIt is concatenated with zgt = E(Vgt), pearance priority. serving as the input to the model. To maintain identity consistency across multiple views, we utilize reference image IR as source of identity information. The face embedding of the reference image Mf RntC is extracted using an existing facial embedding extraction model from IR. nt denotes number of tokens and means dimension of crossattention. The models output is: ϵtarget = hθ(zt gt, zc, Mf ), (3) Where hθ denotes the HumanFixer module with parameters θ, zt gt represents the latents of the ground truth video Vgt at the noising time step t, and zc signifies the latents of the coarse video Vc. As described in [4], we use predicted target loss [4] for optimizing the HumanFixer model. Inference. After training HumanFixer, the coarse video Vc to be refined and the reference image IR are used as inputs to obtain the refined video Vr. Vr = EDM(hθ(zT , zc, Mf )), (4) where zT is the initial latent noise, and we use EDM scheduler [4] for denoising. 3.4. Attention Mechanism Analysis Generating multi-view videos differs from generating standard videos due to the distinct temporal relationships involved. In typical videos, adjacent frames are strongly correlated, with correlation decreasing as the distance between frames increases. However, in multi-view videos, which encompass full circle of views, the final frames have strong relationship with the initial frames. Given that our model is Figure 4. Attention weights visualization. The left and right sides show the head 0 attention weights at the temporal self-attention stage for training on cyclic videos without and with an attention modulation, respectively. Brighter colors indicate higher weights. fine-tuned on SVD, which is pretrained on standard videos, the pretrained parameters align more closely with the assumption of strong correlations between adjacent frames. For multi-view video with total of views, we define non-cyclic video as one where each frame corresponds sequentially to views 0, 1, ..., 1. Conversely, cyclic video involves frames corresponding to views 0, 1, ..., 1, 0, effectively looping back to the initial view. Compared to non-cyclic videos, cyclic videos append the first frame to the end, making the training data more suitable for models pretrained on the assumption of strong correlations between adjacent frames. This should theoretically enhance overall consistency. However, during training, we observed that directly training on non-cyclic videos allows view 0 to develop stronger associations with views 1, 2, and 3. Nevertheless, significant discontinuity was noted between view 1 and view 0, whereas the difference between view 1 and view 0 was relatively minor. To address this issue, we further analyzed the temporal self-attention mechanism within the model. As illustrated in the left panel of Fig. 4, the attention weights for the starting and ending frames are significantly higher than those for intermediate frames. We hypothesize that this occurs because, in cyclic videos, the first and last frames are identical, leading to naturally higher attention weights compared to those calculated between different frames. This phenomenon suppresses information flow among intermediate frames. In summary, while cyclic videos aim to improve consistency by leveraging adjacency assumptions, they introduce challenges related to discontinuities and uneven attention weight distribution, necessitating further refinement of the temporal self-attention mechanism. To mitigate this issue, we apply an attention modulation, as shown in Fig. 2, which ensures that the first and last frames do not receive attention from the model, either from themselves or from each other. Formally, let the attention mask R(N +1)(N +1) be defined as: (i, j) = (cid:40) , 0, if (i, j) = (0, 0), (0, ), (N, 0), (N, ) otherwise. (5) Table 1. Multi-view video generation comparison of other SOTA method. Bold indicate the best result. Method Testset PSNR SSIM LPIPS FID PSHuman [27] Champ [66] HumanFixer (Ours) Champ [66] HumanFixer (Ours) CustomHumans THuman2. 21.998 20.228 25.618 17.547 23.741 0.826 0.889 0.882 0.859 0.889 0.1945 0.2438 0.0687 0.2701 0. 103.808 115.031 87.149 129.629 94.570 Table 2. 3D reconstruction comparison. * denotes the metric is from PSHuman[27], which choose 60 samples from THuman2.1. Gen Method Recon Method Testset PSNR SSIM LPIPS FID PSHuman PSHuman GaussianAvatar Champ HumanDreamer-X GaussianAvatar Animatable gaussians Champ HumanDreamer-X Animatable gaussians SiTH SiTH PSHuman PSHuman GaussianAvatar Champ HumanDreamer-X GaussianAvatar Champ Animatable gaussians HumanDreamer-X Animatable gaussians CustomHumans THuman2. 20.089 19.673 23.639 16.853 22.631 18.458 20.855 18.264 19.328 18.908 21.091 0.8439 0.8789 0.9100 0.9157 0.9458 0.8200 0.8636 0.8842 0.8945 0.9328 0.9403 0.1770 0.2643 0.2427 0.1251 0.0729 0.1004 0.0764 0.2639 0.2578 0.1278 0. 87.816 164.554 114.804 122.752 71.250 - - 129.413 132.200 176.836 78.174 Then, this attention mask is added to every temporal selfattention module in the model: Attn(Q, K, V, M) = softmax (cid:18) QK dk (cid:19) + V, (6) where for query, for key, for value, dk the dimension for scaling down the dot product results, denotes the attention mask on temporal self attention. This suppresses formulation attention effectively weights for the first and last frames, preventing the model from being unduly influenced by these boundary frames during video generation. Fig. 4 illustrates that, after training on cyclic videos with the attention modulation, the attention weights shift from being predominantly focused between the 0th and th frames to resulting in more evenly distributed attention pattern. 4. Experiments This section outlines our experimental framework, including the datasets, implementation specifics, and evaluation criteria. We then provide both quantitative and qualitative results to highlight the outstanding performance of the proposed HumanDreamer-X. 4.1. Experiment Setup Dataset. Our experiments are conducted on variety of datasets. To ensure that the human subject occupies sig6 nificant portion of the frame during training, we filter out instances where the arms are excessively spread, which would otherwise result in an overabundance of background content. The final training set comprises 388 scans from CustomHumans [15] and 1929 scans from THuman2.1 [59]. For testing, we use 39 samples from [15], 32 samples from [59]. All training data is standardized to resolution of 960 640, with cyclic video sequences consisting of 19 frames (18 multi-view frames plus one repeated frame 0). To ensure the reconstruction of complete avatar, it is stipulated that the input to HumanFixer must include facial information as identity cues. For the THuman2.1 dataset [59], facial detection is performed using InsightFace [8], and the image with the largest facial area is designated as the reference image. In contrast, for the CustomHumans dataset [15], the first frame (frame 0) is directly selected as the reference image. Baselines. In multi-view video generation, we utilize PSHuman [27] and Champ [66] as baselines. For 3D avatar reconstruction, in addition to comparing with PSHuman, we also validate the effectiveness of our framework across different baselines by experimenting with Champ and HumanDreamer-X on two distinct 3DGS backbones: Animatable Gaussians [28] and GaussianAvatar [18]. Additionally, we select LGM [46] for visual comparison. Figure 5. Comparison of generation with SOTA methods. Note that PSHumans training dataset contains all of the CustomHumans. Best viewed with zoom-in. 4.2. Main Results Quantity comparison results on multi-view generation. We compare the multi-view generation performance of HumanFixer with several SOTA methods, and the results are presented in Tab. 1. The findings indicate that HumanFixer achieves superior video consistency. Fig. 5 visually illustrates the enhanced multi-view consistency produced by our method. Quantity comparison results on 3D reconstruction. To evaluate the 3D reconstruction quality, we render the reconstructed models from 18 different views and calculate the PSNR, LPIPS, SSIM, and FID metrics. We compare our approach against various baselines to demonstrate the broad applicability of our proposed framework. As shown in Tab. 2, our framework supports the integration of more advanced 3DGS backbones for enhanced reconstruction performance, thereby achieving superior results. We also conducted visualization comparison experiment, as shown in Fig. 6. Our method surpasses other state-of-the-art (SOTA) methods in terms of detail fidelity. Specifically, compared to the second-best method, PSHuman, our approach shows improvements in PSNR, SSIM, LPIPS and FID by 12.65%, 12.07%, 58.81% and 18.86% on CustomHumans. Table 3. Ablation study of attention modulation on CustomHumans subset about generation. Method PSNR SSIM LPIPS FID w/o attention mask (non-cyclic) w/o attention mask (cyclic) attention mask (cyclic) 25.514 25.667 25.618 0.885 0.800 0.882 0.0704 0.1453 0.0687 95.065 106.705 87.149 4.3. Ablation Study Table 4. Ablation study of attention modulation on CustomHumans subset about reconstruction using Animatable Gaussians [28]. Method PSNR SSIM LPIPS FID w/o attention mask (non-cyclic) w/o attention mask (cyclic) attention mask (cyclic) 21.309 20.867 22.631 0.9398 0.9351 0.9458 0.0867 0.0955 0.0729 112.812 139.693 71.250 7 Figure 6. Comparison of 3D reconstruction with SOTA methods. Best viewed with zoom-in. We train HumanFixer on the CustomHumans [15] dataset, comparing non-cyclic videos (18frames), cyclic videos (19 frames) with and without attention modulation in both multi-view video generation (see Tab. 3) and 3D reconstruction (see Tab. 4) settings. Experimental results demonstrate that while video quality decreases when transitioning from non-cyclic to cyclic videos, the inclusion of the attention modulation significantly enhances performance, achieving optimal results. This validates our analysis of the attention mechanism and confirms the effectiveness of the proposed module in improving the fidelity and coherence of the generated avatar videos. 5. Discussion and Conclusion Single-image human reconstruction is crucial for digital human modeling applications but remains challenging task due to geometric inconsistencies and visual fidelity issues in current approaches. To address these limitations, we introduce HumanDreamer-X, novel framework that integrates multi-view human generation and reconstruction into unified pipeline. This framework leverages 3D GS to provide initial geometry and appearance priority, ensuring robust starting points for subsequent refinements. Building on this foundation, we train HumanFixer to restore 3DGS renderings, and facilitate photorealistic reconstructions. Additionally, we propose an attention modulation strategy to enhance geometric detail and identity consistency across multiple views, overcoming inherent challenges in multi-view human generation. Experimental results demonstrate the efficacy of our approach, which markedly improves generation and reconstruction quality. Furthermore, our method shows strong generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 2 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased gridbased neural radiance fields. In ICCV, 2023. 2 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 4, 5 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. [6] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. [7] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. 2 [8] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 6 [9] Zijian Dong, Xu Chen, Jinlong Yang, Michael Black, Otmar Hilliges, and Andreas Geiger. Ag3d: Learning to generate 3d avatars from 2d image collections. In ICCV, 2023. 2 [10] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 3 [11] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In CVPR, 2018. [12] Xu He, Xiaoyu Li, Di Kang, Jiangnan Ye, Chaopeng Zhang, Liyang Chen, Xiangjun Gao, Han Zhang, Zhiyong Wu, and Haolin Zhuang. Magicman: Generative novel view synthesis of humans with 3d-aware diffusion and iterative refinement. arXiv preprint arXiv:2408.14211, 2024. 2, 3 [13] Ho, Jie Song, Otmar Hilliges, et al. Sith: Single-view textured human reconstruction with image-conditioned diffusion. In CVPR, 2024. 2, 3 [14] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. Eva3d: Compositional 3d human generation from arXiv preprint arXiv:2210.04888, 2d image collections. 2022. 2 [15] Jie Song Hsuan-I Ho, Lixin Xue and Otmar Hilliges. Learning locally editable virtual humans. In CVPR, 2023. 6, 8 [16] Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang, et al. Expressive gaussian human avatars from monocular rgb video. NeurIPS, 2025. 2 [17] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, 2024. 2, 3 [18] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. In CVPR, 2024. 2, 3, [19] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image arXiv preprint animation with environment affordance. arXiv:2502.06145, 2025. 3 [20] Shoukang Hu, Tao Hu, and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In CVPR, 2024. 2, 3 [21] Tao Hu, Liwei Wang, Xiaogang Xu, Shu Liu, and Jiaya Jia. Self-supervised 3d mesh reconstruction from single images. In CVPR, 2021. 2 [22] Mustafa Isık, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nießner. Humanrf: High-fidelity neural radiance fields for humans in motion. ACM ToG, 2023. 2 [23] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Instantavatar: Learning avatars from monocular video in 60 seconds. In CVPR, 2023. 2, 3 [24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM ToG, 2023. [25] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 5 [26] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models. In CVPR, 2024. 2, 3 [27] Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, et al. Pshuman: Photorealistic single-view human reconstruction using cross-scale diffusion. arXiv preprint arXiv:2409.10141, 2024. 2, 3, 6 [28] Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. In CVPR, 2024. 2, 3, 6, 7 [29] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers. 2023. [30] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In CVPR, 2024. 2 [31] Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, and Fernando De la Torre. Gas: Generative avatar synthesis from single image, 2025. 2, 3 9 [32] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [33] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia, 2024. 3 [34] Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, and Xuansong Xie. En3d: An enhanced generative model for sculpting 3d humans from 2d synthetic data. In CVPR, 2024. 2, [35] Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. 3 [36] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. 2 [37] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, and Eduardo Perez-Pellitero. Human gaussian In splatting: Real-time rendering of animatable avatars. CVPR, 2024. 2 [38] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, et al. Recondreamer: Crafting world models for driving scene reconstruction via online restoration. arXiv preprint arXiv:2411.19548, 2024. 3 [39] Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, and Wenjun Mei. Wonderturbo: Generating arXiv preprint interactive 3d world in 0.72 seconds. arXiv:2504.02261, 2025. 2 [40] Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, and Yebin Liu. Humansplat: Generalizable single-image human gaussian splatting with structure priors. NeurIPS, 2024. 2 [41] Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. Charactergen: Efficient 3d character generation from single images with multi-view pose canonicalization. ACM ToG, 2024. 2, [42] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting. In CVPR, 2024. 2 [43] Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, et al. Anigs: Animatable gaussian avatar from single image with inconsistent gaussian reconstruction. arXiv preprint arXiv:2412.02684, 2024. 2, 3 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [45] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. 2, 3 [46] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2024. 6 [47] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In ECCV, pages 244260. Springer, 2024. 3 [48] Boyuan Wang, Xiaofeng Wang, Chaojun Ni, Guosheng Zhao, Zhiqin Yang, Zheng Zhu, Muyang Zhang, Yukun Zhou, Xinze Chen, Guan Huang, Lihong Liu, and Xingang Wang. Humandreamer: Generating controllable humanarXiv preprint motion videos via decoupled generation. arXiv:2503.24026, 2025. [49] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, ChungChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. arXiv preprint arXiv:2307.00040, 2023. 3 [50] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. 2, 3 [51] Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu. Worlddreamer: Towards general world models for video generation via predicting masked tokens. arXiv preprint arXiv:2401.09985, 2024. 3 [52] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 3 [53] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, 2022. 2, 3 [54] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. [55] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3 [56] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In CVPR, 2024. 2 [57] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [58] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 5 10 [59] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In CVPR, 2021. [60] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In CVPR, 2024. 2 [61] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2, 3 [62] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 2, 3 [63] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Sideview conditioned implicit function for real-world usable clothed human reconstruction. In CVPR, 2024. 2, 3 [64] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. 3 [65] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, and Yebin Liu. Avatarrex: Real-time expressive fullbody avatars. ACM ToG, 2023. [66] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV, 2024. 2, 3, 6 [67] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. 2 [68] Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, and Wei Liu. Idol: Instant photorealistic 3d human creation from single image. arXiv preprint arXiv:2412.14963, 2024. 2,"
        }
    ],
    "affiliations": [
        "GigaAI",
        "Institute of Automation, Chinese Academy of Sciences",
        "Peking University"
    ]
}