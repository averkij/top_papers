{
    "paper_title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge",
    "authors": [
        "Hao Liang",
        "Ruitao Wu",
        "Bohan Zeng",
        "Junbo Niu",
        "Wentao Zhang",
        "Bin Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner."
        },
        {
            "title": "Start",
            "content": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge 5 2 0 2 7 ] . [ 1 9 7 0 6 0 . 9 0 5 2 : r Hao Liang * 1 2 Ruitao Wu * 3 2 Bohan Zeng 1 Junbo Niu 1 Wentao Zhang 1 2 Bin Dong 1 Abstract Multimodal reasoning remains fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop & Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github. com/OpenDCAI/SciReasoner. 1. Introduction With the rapid advancement of Large Language Models (LLMs), models such as GPT-O3 and agent systems built upon them have demonstrated remarkable deductive abilities on purely textual tasks (Achiam et al., 2023; Guo et al., 2025; Huang & Yang, 2025; Chai et al., 2025). Although Multimodal LLMs (MLLMs) have achieved encouraging results (Bai et al., 2025; Yao et al., 2024), multimodal reasoningintegrating visual and textual information to derive coherent conclusionsremains core challenge in artificial intelligence. Recent benchmarks, such as SeePhys, further reveal that even state-of-the-art systems frequently struggle to extract and integrate visual information effectively, signaling that true cross-modal reasoning remains elusive (Xiang et al., 2025), especially in contrast to the impressive progress observed in text-only reasoning, where *Equal contribution 1Peking University 2Zhongguancun Academy 3Beihang University. Correspondence to: Wentao Zhang <wentao.zhang@pku.edu.cn>, Bin Dong <dongbin@bicmr.pku.edu.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 LLMs demonstrate far greater stability and accuracy. The stark contrast between the remarkable textual reasoning performance of models such as GPT-o3 and their substantial limitations in multimodal reasoning raises an important question: why does the same model demonstrate substantial performance gap between multimodal and text-only settings? Is this discrepancy caused by imperfect visual perception (e.g., difficulty in reading diagrams) or by inherent limitations in multimodal reasoning capabilities? Inspired by (He et al., 2025), we propose novel captionassisted reasoning framework, which leverages automatically generated or human-provided captions to bridge the gap between visual inputs and structured textual reasoning. By anchoring reasoning in caption-derived semantics, our framework enhances cross-modal alignment and enables more robust inference. Our framework directly addresses three key limitations in current multimodal reasoning: (1) unstable integration between visual perception and logical inference; (2) restricted generalization across diverse domainsparticularly in physics problems with varying complexity; and (3) dependence on costly fine-tuning or annotated chain-of-thought data. By contrast, lightweight caption-based approach offers semantically rich and interpretable bridge, improving both reasoning depth and transparency. Our contributions are summarized as follows: We introduce the concept of caption-assisted reasoning, demonstrating that for images with relatively low information density, generating concise captions can effectively mitigate the difficulty multimodal reasoning models face in capturing key visual details. Remarkably, we find that reasoning solely on captions without direct visual inputcan already yield highly competitive results. We validate this idea on both the SeePhys and geometric problem settings. In particular, our approach was extensively evaluated in the ICML 2025 SeePhys competition, benchmark built on the SeePhys multimodal dataset and hosted on CodaBench (cod, 2025), where our method achieved 1st place, demonstrating Submission and Formatting Instructions for ICML 2025 Figure 1. Grounding of images from MathVerse and SeePhy benchmark. its effectiveness and versatility across wide range of physics reasoning tasks. Furthermore, we show that our method generalizes well to the MathVerse benchmark, confirming its broader applicability. 2. Method 2.1. Motivation In tasks involving relatively simple figures, such as basic geometric diagrams or schematic physical systems, captionassisted reasoning can be much more concise and efficient than directly processing the raw image. This is because the visual information is often sparse and the structural relations are simple (e.g., point is 30, BD is the diameter). short textual caption can explicitly convey these relations with only few tokens, whereas multimodal encoders typically expand the image into hundreds of tokens, most of which may be redundant. Moreover, captions naturally filter out distracting visual elements and highlight only the information needed for reasoning. Thus, for lowvisual-complexity tasks, captioning not only reduces token consumption but also leads to clearer logical chains and more stable reasoning performance. 2.2. Methods Inspired by prior works on reasoning, we consider scenarios where figures contain rich textual information, such as physics diagrams that require extensive textual quantification. To address these challenges, we explore the following approaches: 1. Rephrasing: Reformulating the original question for clarity. The prompt is shown in Section A.1. 2. Default Captioning: Generating descriptive caption without additional constraints. The prompt is shown in Section A.2. 3. Grounding: Producing captions with explicit grounding to entities and relations, thereby reducing ambiguities that often arise in Default Captioning when describing physics problems. The prompt is shown in Section A.3. 4. Structured Captioning: Generating captions in structured format to improve consistency and facilitate downstream reasoning. The prompt is shown in Section A.4. 5. Image Reintegration (Img): Reinserting the original image alongside the generated caption to provide complementary visual context. 6. Adaptive Answer Routing (AAR): Dynamically choosing between caption-based reasoning and direct image-based reasoning depending on the physics domain of the question (e.g., astrophysics, electromagnetism, quantum mechanics). Using the publicly released SeePhys-Dev answers, we evaluated performance under both settings across categories. For categories where direct image input outperformed caption2 Submission and Formatting Instructions for ICML 2025 which serves as the basis for our analysis. All model outputs are generated via API calls using default parameters. The specific model versions used are gpt-4o-2024-08-06, o3-2025-04-16, and gemini-2.5-pro. For evaluation, the o3 model is employed as judge to compare the model-generated answer against the ground truth. The scoring is binary, where only fully correct answers are counted as correct, and any partially correct answers are considered incorrect. 3.2. Performance of Foundational Methodologies Direct Multimodal Reasoning (w/o caption). The baseline approach, which utilizes direct multimodal input without an explicit captioning step, establishes strong performance reference. The G2.5P model achieves total accuracy of 58.0%, confirming the robust native capability of advanced models to process and integrate concurrent textual and visual information for problem-solving. The Rephrasing Method. The Rephrasing method, which instructs the model to generate comprehensive textual summary of the problem before solving it, shows marked decrease in performance, with maximum accuracy of 50.5%. This is significant reduction from the 58.0% direct multimodal baseline. primary reason for this performance degradation may be that the nature of the rephrasing task itself is suboptimal for visual information extraction. By prompting the model to rehearse, the task may cause the model to focus on the act of repetition and textual synthesis, potentially leading it to neglect thorough analysis of the visual information. Compared to direct instruction to describe the image, the rephrasing prompt might not effectively guide the models attention to capture all critical visual details. 3.3. The Impact of Captioning Quality From Default to Structured Captioning. The experiments reveal that better grounding strategies can enhance reasoning. We experiment three captioning methods, i.e. Default, Grounding, and Structured methods. The Default method, using simple prompt for image description, achieves 58.5% accuracy with well-matched model pair (o3 o3). The Grounding method, which adds explicit instructions to identify coordinates and component relationships, improves the total accuracy to 59.0% (G2.5P o3). This indicates that directing the models attention to specific geometric and relational details yields more effective textual representation. The Structured method, by enforcing strict, domainspecific template for the description, proves to be the most Figure 2. Our final reasoning pipeline. based reasoning (including quantum mechanics, projectile motion, electromagnetic fields, charge distribution, circuit diagrams, spring force, and atomic physics), we consistently selected the image input; for other categories, we relied on caption-based reasoning. 7. Format Optimization (FO): Enforcing standardized answer formats to reduce ambiguity and improve parsing. The prompt is shown in Section A.6. 8. Critical Review (CR): Leveraging second powerful model to re-evaluate and refine the initial answer. The prompt is shown in Section A.7. We begin by evaluating direct reasoning performance and, as baseline enhancement, rephrasing the problem statement for clarity (Section 3.2). We then improve caption quality through Structured Captioning (Section 3.3). Finally, we incorporate additional enhancement techniques, including Image Reintegration, Adaptive Captioning, Format Optimization, and Critical Review (Section 3.4), to further boost reasoning accuracy. Our final reasoning pipeline integrates the most effective components: Structured Captioning, Image Reintegration, Format Optimization, and Critical Review, as shown in Figure 2. 3. Experiments 3.1. Dataset and Implementation Details Dataset. The experiments use SeePhys-mini, subset comprising 200 questions randomly selected from the full 2,000-question SeePhys benchmark. The distribution of questions across the eight knowledge levels is as follows: Middle School (8), High School (18), Beginner Olympiad (13), Advanced Olympiad (46), Undergraduate (42), Senior Undergraduate (19), Masters (7), and PhD (47). Implementation and Evaluation. The experiments systematically explore different strategies for generating and utilizing image captions, employing various state-of-theart models such as OpenAIs GPT-o3 and GPT-4o (Hurst et al., 2024), and Googles Gemini-2.5-Pro (G2.5P) (Comanici et al., 2025). The results are presented in Table 1, 3 Submission and Formatting Instructions for ICML 2025 Method AAR Img FO CR Describe Answer Mid High BO AO UG SUG MA PhD Total Table 1. Accuracy (%) on the SeePhys-mini subset. w/o caption Rephrasing Default Captioning Grounding Structured o3 G2.5P G2.5P 4o o3 o3 o3 G2.5P o3 o3 o3 o3 o3 o3 o3 o3 87.5 75.0 87.5 87.5 37.5 50.0 87.5 75. 50.0 75.0 75.0 75.0 87.5 87.5 87.5 87.5 87.5 44.4 50.0 38.9 44.4 50.0 33.3 50.0 55.6 16.7 44.4 61.1 61.1 66. 72.2 66.7 61.1 72.2 69.2 61.5 53.8 53.8 30.8 38.5 61.5 76.9 38.5 69.2 69.2 61.5 76.9 69.2 76.9 76.9 69. 47.8 52.2 47.8 47.8 8.7 26.1 52.2 56.5 47.8 41.3 58.7 56.5 47.8 41.3 54.3 56.5 54.3 66.7 73. 54.8 61.9 26.2 38.1 66.7 66.7 51.7 66.7 71.4 66.7 78.6 78.6 81.0 78.6 81.0 57.9 63.2 36.8 36. 21.1 31.6 63.2 68.4 36.8 57.9 42.1 47.4 57.9 57.9 63.2 63.2 63.2 71.4 85.7 42.9 57.1 14.3 57.1 85.7 57. 42.9 71.4 71.4 71.4 57.1 71.4 57.1 71.4 71.4 44.7 42.6 29.8 42.6 12.8 23.4 42.6 44.7 31.9 27.7 46.8 48.9 46. 55.3 48.9 57.4 57.4 55.5 58.0 45.0 50.5 21.0 32.0 57.0 59.0 41.5 49.5 59.0 58.0 60.5 61.5 63.5 65.5 66. o3 o3 4o 4o o3 o3 o3 o3 G2.5P G2.5P G2.5P G2.5P G2.5P G2.5P G2.5P effective foundational strategy. It achieves an accuracy of 61.5%, demonstrating that providing the reasoning model with predictable, easily parsable format minimizes ambiguity and maximizes the utility of the extracted information. 3.4. Stepwise Enhancement Strategies Image Reintegration. Providing the original image alongside the generated caption consistently improves performance. For example, in the best Grounding configuration, adding the image increases accuracy from 59.0% to 60.5%. This indicates that the image provides complementary information to the caption, with each modality capturing aspects of the problem that the other may miss. Adaptive Answer Routing. This method adaptively selects between the caption-based answer and the direct multimodal answer. Its effectiveness depends on the relative strength of the two pipelines. In the Grounding setting, the caption-based approach (G2.5P o3) achieves 59.0% accuracy, already surpassing the direct multimodal baseline at 58.0%. When AAR is applied, some of these stronger caption-based answers are replaced by weaker baseline answers, reducing total accuracy to 58.0%. This reveals key insight: once the captioning pipeline is sufficiently strong (e.g., Grounding or Structured Captioning), it can outperform direct end-to-end multimodal reasoning, rendering AAR less beneficial. Answer Refinement: Format Optimization and Critical Review. These methods serve as the final refinement layers in the pipeline. Applying strict output formatting rules to the Structured method raises accuracy from 61.5% to 63.5%, demonstrating that standardization reduces ambiguity and improves reliable answer parsing. Adding Critical Review stage, where second powerful model reevaluates the initial answer, further boosts accuracy. For instance, combining Structured + Img achieves 65.5%, while the addition of CR raises it to the overall maximum of 66.0%. Although the 0.5 percentage point gain is modest, it represents the final quality check necessary for SOTA performance, correcting subtle reasoning errors that earlier stages may miss. 3.5. Analysis of SeePhys Results Foundational and Curriculum-Based Levels At foundational levels such as Middle School (Mid) and Undergraduate (UG), high accuracy can be achieved through relatively straightforward strategies. The best results are consistently obtained with the Structured method, particularly when combined with enhancements like Format Optimization (FO). For instance, the UG level reaches its peak accuracy of 81.0% under the Structured+FO configuration. This suggests that for problems closely tied to well-defined curricula, clarity, unambiguity, and structural rigor in the models outputfacilitated by FOare key determinants of performance. At the High School (High) level, the best accuracy of 72.2% is attained using the baseline Structured method alone, highlighting the central role of clear and systematically organized problem representation at this stage. Advanced and Abstract Reasoning Levels For more challenging levels that demand complex, non-standard, or abstract reasoning, stronger gains require combination of advanced enhancements. At the Beginner Olympiad 4 Submission and Formatting Instructions for ICML 2025 Table 2. Performance of our method on the MathVerse benchmark."
        },
        {
            "title": "Captioning Model",
            "content": "w image w/o image"
        },
        {
            "title": "MLLM",
            "content": "Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Claude-Opus-4-20250514 Gemini-2.5-pro-preview-05-06 GPT-o3 DeepSeek-R Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct w/o GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct w/o GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct w/o GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct w/o GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct w/o GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct GPT-4o GPT-o3 Qwen2.5-VL-72B-Instruct 7.1 51.5 59.7 49.4 8.6 59.0 66.4 56. 44.8 49.1 73.7 61.4 62.0 56.6 73.7 60.5 67.6 67.8 75.6 64."
        },
        {
            "title": "LLM",
            "content": "43.3 60.8 60.3 70.2 58.0 66.9 72.6 71.6 60.2 76.7 85.5 64.0 54.3 78.0 85.4 59.0 64.5 81.3 86.3 60.0 45.9 61.9 52.7 51.1 66.1 57.0 51.8 59.4 60.9 57.1 60.3 62. 54.6 60.3 60.3 57.1 68.2 62.3 47.1 61.5 53.4 50.6 66.1 54.8 61.5 63.2 67.4 66.5 74.9 70. 75.0 79.3 74.1 76.1 83.0 77.9 76.3 82.5 76.3 73.9 71.9 75.3 60.2 63.5 68.3 65.6 73.2 69. (BO) level, the top score of 76.9% is achieved when Structured descriptions are augmented with Format Optimization (FO) or Adaptive Answer Routing (AAR). For Advanced Olympiad (AO) problems, which require deep synthesis across multiple reasoning steps, the highest accuracy of 58.7% is obtained with the Grounding method. This underscores the importance of detailed, component-level grounding in handling visually intricate problems. At the Senior Undergraduate (SUG) level, the best performance (68.4%) emerges uniquely from the Default captioning method combined with AAR, suggesting that flexible routing strategies complement simpler descriptive styles at this stage. At the Masters level, the highest score of 85.7% is achieved with the direct multimodal baseline G2.5P, showing that strong pretrained multimodal capabilities remain highly effective. For PhD-level tasks, however, only multi-faceted strategy yields the best results. The peak score of 57.4% is achieved through configurations incorporating Structured description, AAR, Image Reintroduction (Img), and either FO or Critical Review (CR). These findings indicate that for the most abstract and demanding problems, success hinges on integrating multiple enhancements: structured representations, iterative refinement (CR), and adaptive routing (AAR), working in concert to maximize reasoning depth and robustness. 4. Generalization to Other Domains To further assess the generalization of our method, we evaluate it on the MathVerse benchmark (Zhang et al., 2024), modifying only the captioning prompt to adopt more geometry-oriented formulation, as illustrated in Figure 10. Submission and Formatting Instructions for ICML 2025 We observe that incorporating Captioning Model consistently yields superior performance across all tested MLLMs. For instance, in the case of Claude-Opus-4 (Anthropic, 2025), the combination with GPT-o3 captioning improves accuracy from 60.2% (w/o) to 85.5% (w image, Vision Intensive), achieving significant margin over the direct multimodal baseline. Similar trends are observed for Qwen2.5VL and Gemini-2.5, where captioning substantially boosts performance on both Vision Only and Vision Intensive tasks. This confirms that strong captioning pipeline can effectively enhance reasoning in complex, text-heavy mathematical diagrams. We further evaluate the text-only setting, where images are not available and models rely solely on captions for reasoning. Interestingly, even under this constraint, large LLMs such as DeepSeek-R1 and Qwen2.5-72B-Instruct demonstrate remarkable capability. For example, DeepSeek-R1 combined with GPT-o3 captioning achieves 68.2% (Vision Only), surpassing the performance of GPT-o3 itself under the same text-only setting. Similarly, Qwen2.5-72B-Instruct and the paired Qwen2.57B-Instruct with Qwen2.5-VL-72B-Instruct captioning achieve 69.7% and 68.3% on Vision Intensive tasks, respectively, surpassing GPT-o3s 64.5% performance without captioning. These results demonstrate that strong reasoningoriented LLMs, when equipped with high-quality captions, can not only bridge the gap but even outperform multimodal baselines. Overall, these results highlight two key findings: (1) captioning consistently improves multimodal reasoning across different models, and (2) when combined with sufficiently advanced LLMs, the caption-only paradigm can rival or even outperform end-to-end multimodal reasoning. 5. Future Work While our caption-assisted reasoning framework demonstrates strong performance and generalization ability, several directions remain open for future exploration. First, we plan to integrate more advanced reasoning strategies, such as program-of-thought and tool-augmented reasoning, to further strengthen complex problem-solving beyond physics and mathematics. Second, extending the framework to other scientific domains (e.g., chemistry, biology, and engineering) will allow us to test its robustness in handling diverse multimodal information. Third, an important direction is the development of adaptive captioning strategies that can dynamically adjust the granularity and structure of captions according to task complexity and modality requirements. Finally, exploring the incorporation of human-in-the-loop feedback and interactive reasoning may lead to systems that are both more interpretable and more aligned with scientific discovery processes. Together, these efforts will advance our framework toward becoming general-purpose scientific reasoning system. 6. Conclusion In this work, we addressed long-standing challenge in multimodal reasoning by introducing caption-assisted reasoning framework. Our approach leverages automatically generated or human-provided captions to bridge visual and textual modalities, thereby enhancing cross-modal alignment and enabling more robust inference. Through extensive evaluation on the SeePhys benchmark, our method achieved 1st place in the ICML 2025 AI for Math Workshop & Challenge 2: SeePhys, and further demonstrated strong generalization on the MathVerse benchmark. Our experiments highlight that (1) high-quality captions consistently improve multimodal reasoning performance, and (2) strong LLMs combined with caption-only reasoning can rival or even outperform direct multimodal pipelines. We believe this work takes significant step toward closing the gap between visual perception and textual reasoning, paving the way for more transparent, efficient, and generalizable multimodal reasoning systems."
        },
        {
            "title": "References",
            "content": "for math workshop & challenge on Icml 2025 ai https://www.codabench.org/ seephys. competitions/7925/, 2025. Competition submission platform via CodaBench. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Transparency hub: Model report. https: //www.anthropic.com/transparency/ model-report, 2025. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Chai, J., Tang, S., Ye, R., Du, Y., Zhu, X., Zhou, M., Wang, Y., Zhang, Y., Zhang, L., Chen, S., et al. Scimaster: Towards general-purpose scientific ai agents, part i. xmaster as foundation: Can we lead on humanitys last exam? arXiv preprint arXiv:2507.05241, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, 6 Submission and Formatting Instructions for ICML 2025 and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, H., Ye, M., Zhang, J., Cai, X., Liu, J., Du, B., and Tao, D. Reasoning-ocr: Can large multimodal models solve complex logical reasoning problems from ocr cues? arXiv preprint arXiv:2505.12766, 2025. Huang, Y. and Yang, L. F. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Xiang, K., Li, H., Zhang, T. J., Huang, Y., Liu, Z., Qu, P., He, J., Chen, J., Yuan, Y.-J., Han, J., Xu, H., Li, H., Sachan, M., and Liang, X. Seephys: Does seeing help thinking? benchmarking vision-based physics reasoning. arXiv preprint arXiv:2505.19099, 2025. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Submission and Formatting Instructions for ICML 2025 A. Prompts A.1. Prompt for Rephrasing The prompt for Rephrasing is shown in Figure 3. A.2. Prompt for Default Captioning The prompt for Default Captioning is shown in Figure 4. A.3. Prompt for Grounding The prompt for Grounding is shown in Figure 5. A.4. Prompt for Structured Captioning The prompt for Structured Captioning is shown in Figure 6. A.5. Prompt for Answer The prompt for Answer is shown in Figure 7. A.6. Prompt for Format Optimization The prompt for Format Optimization is shown in Figure 8. A.7. Prompt for Critical Review The prompt for Critical Review is shown in Figure 9. B. Examples B.1. Example of Rephrasing The example of Rephrasing is shown in Figure 11. B.2. Example of Default Captioning The example of Default Captioning is shown in Figure 12. B.3. Example of Grounding The example of Grounding is shown in Figure 13. B.4. Example of Structured Captioning The example of Structured Captioning is shown in Figure 14. B.5. Example of Critical Review The example of Critical Review is shown in Figure 15. 8 Submission and Formatting Instructions for ICML 2025 Figure 3. Prompt for Rephrasing. Figure 4. Prompt for Default Captioning. 9 Submission and Formatting Instructions for ICML 2025 Figure 5. Prompt for Grounding. 10 Submission and Formatting Instructions for ICML Figure 6. Prompt for Structured Captioning. 11 Submission and Formatting Instructions for ICML 2025 Figure 7. Prompt for Answer. 12 Submission and Formatting Instructions for ICML Figure 8. Prompt for Format Optimization. 13 Submission and Formatting Instructions for ICML 2025 Figure 9. Prompt for Critical Review. 14 Submission and Formatting Instructions for ICML Figure 10. Prompt for MathVerse benchmark captioning. Figure 11. Example of Rephrasing. 15 Submission and Formatting Instructions for ICML 2025 Figure 12. Example of Default Captioning. Submission and Formatting Instructions for ICML 2025 Figure 13. Example of Grounding. 17 Submission and Formatting Instructions for ICML 2025 Figure 14. Example of Structured Captioning. Submission and Formatting Instructions for ICML 2025 Figure 15. Example of Critical Review."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Peking University",
        "Zhongguancun Academy"
    ]
}