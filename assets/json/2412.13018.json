{
    "paper_title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
    "authors": [
        "Shuting Wang",
        "Jiejun Tan",
        "Zhicheng Dou",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47\\% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in \\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 8 1 0 3 1 . 2 1 4 2 : r OmniEval: An Omnidirectional and Automatic RAG Evaluation"
        },
        {
            "title": "Benchmark in Financial Domain",
            "content": "Shuting Wang1,, Jiejun Tan1,, Zhicheng Dou1, and Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China {wangshuting, zstanjj, dou}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "As typical and practical application of Large Language Models (LLMs), RetrievalAugmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domainspecific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multidimensional evaluation framework, including (1) matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to structured assessment of diverse query scenarios; (2) multi-dimensional evaluation data generation approach, which combines GPT-4based automatic generation and human annotation, achieving an 87.47% acceptance ratio in human evaluations on generated instances; (3) multi-stage evaluation system that evaluates both retrieval and generation performance, result in comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in https://github.com/RUCNLPIR/OmniEval."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG) techniques have been one of the most widespread and *Corresponding author. Equal contribution author. 1 practical applications of Large Language Models (LLMs). Especially for various vertical domains, where LLMs usually lack in-domain expert knowledge, RAG models can incorporate the advantages of external domain corpus and LLMs internal knowledge to enhance the overall quality of generative AI systems. However, how to automatically build high-quality omnidirectional benchmarks to evaluate the performance of RAG models on vertical domains is still an open problem. In this study, we propose an automatic and omnidirectional benchmark, OmniEval, to evaluate RAG systems on widely adopted vertical domain, finance. Our proposed benchmark illustrates its versatility and automaticity from the following angels: (1) Matrix-based RAG scenario evaluation. On the one hand, the queries in an RAG system are usually multiform and require the system to be equipped with various response abilities. For example, some queries aim to search for factual information that can be extracted from web pages, while others request RAG systems to do financial computation. Therefore, to evaluate the abilities of RAG systems, we classified the RAG scenarios into five task classes: extractive question answering, multi-hop reasoning, long-form question answering, contrast question answering, and conversational question answering. On the other hand, topic systems for specialist areas (such as finance) are often clearly compartmentalized, and users information needs are often specific to different topic areas. As result, we can further distinguish RAG scenarios based on the topical categories of queries. Specifically, we evaluate financial RAG system in 16 different sub-categories. These two classification dimensions are orthometric, leading to matrix-based RAG scenario evaluation system. The visualization of the corresponding subset statistic is shown in Figure 8, 9, and 10. According to this matrix-based evaluation, we can implement more specific and fine-grained assessment of RAG modFigure 1: The visualization of OmniEvals generation pipeline of evaluation data. els abilities, resulting in comprehensive evaluation profiles for RAG systems. (2) Multi-dimensional evaluation data generation. To build an extensible and high-quality evaluation dataset, we combine the GPT-4-based automated generation and human annotation data generation approaches together. The former enables the data generation pipeline to be flexible to expand or adapt to other domains, and the latter guarantees the quality of the constructed dataset. We also conduct human evaluation on automatically generated instances and the acceptable ratio is 87.47%, further confirming the effectiveness of our automated data generation pipeline. (3) Multi-stage evaluation. For evaluating the entire pipeline of RAG systems, not only the performance of the final responses is important, but the retrieval quality is also critical for the RAG systems. It is more crucial for vertical domains since the open-domain retrievers may lack expert knowledge, potentially leading to worse final responses. Therefore, OmniEval evaluate both retriever and generator performance to provide comprehensive assessing dimensions and comparison results. (4) Multi-dimensional evaluation metrics. To accurately and reliably evaluate RAG systems, we build our evaluation metrics from two directions: (a) Rule-based evaluation metrics and (b) LLM-based evaluation metrics. The former embodies some widely-used evaluation metrics, such as MAP and Rouge to provide convincing evaluation results. The latter is built by prompting LLMs to generate high-level evaluation angles beyond exact or term-level matching, such as hallucination detection, comprehensiveness evaluation, and numerical accuracy. To ensure the reliability of the LLM-based metrics, we manually annotated some evaluation results based on the instructions of our proposed LLM-based metrics and supervised fine-tuning Qwen2.5-7B-Instruct (Team, 2024) to serve as our LLM-based evaluator. Our comparison results show that our proposed evaluator significantly surpasses zero-shot-based LLMs, such as Qwen2.5-72B-Instruct (Team, 2024) and Llama3.1-70B-Instruct (Dubey et al., 2024), and demonstrates 74.4% accuracy with human evaluation. Experimental results confirm the effectiveness and soundness of our LLM-based evaluator. As result, our proposed benchmark, OmniEval, contains 11.4k automatically generated test examples and 1.7k human-annotated test examples. We further split out 3k automatically generated examples as training set for future potential finetuning.1 We conducted our experiments on various retrievers, including BGE-M3 (Chen et al., 2024b), BGE-large-zh (Xiao et al., 2023a), GTEQwen2-1.5b (Li et al., 2023), and jina-zh (Günther et al., 2023), and diverse open-resource LLMs, i.e., Qwen2.5-72B-Instruct (Team, 2024), Llama3.170B-Instruct (Dubey et al., 2024), Deepseek-v2chat (DeepSeek-AI, 2024), and Yi15-34B (Young et al., 2024). Our comprehensiveness experiments reveal that the performance of RAG systems varied between different topics and tasks. Moreover, there is still large space to be improved for RAG systems on vertical domains. 1Note that the automatically generated examples are extensible by prompting GPT-4, we currently provide this amount of examples due to the limited budgets. 2 Benchmark Evaluation Scenarios Data Generation Evaluation Metrics Evaluation Models Task-spe. Topic-spe. Manual Auto. Rule Model Human Retriever Generator PIXIU (Xie et al., 2023) DISC-FinLLM (Chen et al., 2023) FinanceBench (Islam et al., 2023) AlphaFin (Li et al., 2024) FinBen (Xie et al., 2024) FinTextQA (Chen et al., 2024a) OmniEval Table 1: The comparison between our proposed benchmark with existing financial benchmarks. Auto. is short for Automated-generated, Spe. is short for Specific."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Benchmark of Retrieval-augmented Generation Due to the rapid development of RAG investigation and the limitation of existing datasets and evaluation metrics, various researchers (Chen et al., 2024c; Liu et al., 2023; Xiong et al., 2024; SaadFalcon et al., 2024; Yu et al., 2024; Lyu et al., 2024; Wang et al., 2024a) pay more attention to building comprehensive and reliable RAG benchmarks. For example, the early study, RGB (Chen et al., 2024c), proposed to focus on more advanced abilities of RAG models, such as noise robustness, information integration, etc., and build new open-domain RAG dataset by extracting information from news articles. ARES (Saad-Falcon et al., 2024) automatically builds RAG benchmark with the support of LLMs, including automatically generating data instances and automatically judging responses. Beyond open-domain question-answer scenarios, some studies (Xiong et al., 2024; Wang et al., 2024a) also constructed domain-specific RAG benchmarks to evaluate the abilities of the RAG system in vertical domain scenarios."
        },
        {
            "title": "2.2 LLM Evaluation in Financial Domains",
            "content": "In practice, finance is one of the most widely used vertical domains, which also contain rich professional knowledge. Therefore, the evaluation of RAG or LLM systems in the financial domain is critical to assess their capabilities on domain-specific scenarios, which are more practical application situations for Generative AI models. Considering that there are various financial QA datasets (Thakur et al., 2021; Sinha and Khandait, 2020; Salinas Alvarado et al., 2015; Chen et al., 2021, 2022; Soun et al., 2022), which support various investigations of traditional machine learningbased financial models, some studies (Shah et al., 2022; Xie et al., 2023, 2024; Li et al., 2024; Chen et al., 2023) propose to collect existing financial QA datasets to build benchmarks to assess the grasping of financial knowledge by LLMs. Recently, Xie et al. (2023) further builds instructiontuning financial benchmarks by writing corresponding instructions for various financial tasks, hence evaluating LLMs abilities on these expert tasks. Beyond assessing LLMs alone, AlphaFin(Li et al., 2024) also introduces RAG tasks to judge RAG models on financial tasks. However, it only evaluates the quality of final responses, neglecting the retrieval performance. Different from existing studies, our work constructs an omnidirectional and automatic RAG evaluation benchmark, which could automatically generate the evaluation dataset from the user-provided knowledge corpus, posing excellent flexibility and expandability. Moreover, it also proposes an omnidirectional evaluation system, including matrix-based RAG scenario evaluation, multi-dimensional evaluation data generation, multi-stage evaluation system, and multidimensional evaluation metrics. Concretely, we compare our benchmark to existing financial LLM benchmarks to demonstrate our advantages, which is shown in Table 1."
        },
        {
            "title": "3 Construction Pipeline of OmniEval",
            "content": "The construction pipeline of our benchmark consists of the following steps: (1) Construction of knowledge corpus (Section 3.1). First, we collect massive finance-related documents from diverse data resources and build our knowledge corpus based on these documents. (2) Generation of evaluation instances (Section 3.2). Then, we generate diverse evaluation question-answer (QA) samples from the knowledge corpus automatically by requesting GPT-4. To implement matrixbased RAG scenario evaluation system, we utilize 3 overlap as 256. The statistical information of our data resources is shown in Table 2, where we call built LlamaIndex nodes as documents. 3.2 Generation of Evaluation Instances Given the knowledge corpus with abundant domainspecific information, we set up our automatic data generation pipeline by GPT-4-based multiagent method. The visualization of our generation pipeline is illustrated in Figure 1. We demonstrate its processing steps as below. 3.2.1 RAG Scenario Recognition It is worth noting that we propose to conduct matrix-based RAG scenarios evaluation to implement the mimic of the real and diverse application scenarios of RAG systems. Specifically, we define two evaluation perspectives from the domain topics and RAG tasks. On the one hand, different domain topics related to user questions could split RAG scenarios from the topic perspective, such as the stock market, investment bank, property insurance, and so on. On the other hand, different question types also pose different QA tasks for RAG models. For example, some questions focus on comparing two similar financial products, which is treated as contrast task for RAG models, while some questions request RAG models to reason out the investment income over period, which is the reasoning task. These two perspectives are orthotropic and their cartesian product leads to RAG scenario matrix, where each element represents specific topic-task scenario. In our study, we build our topic system by prompting GPT-4 and pruning it according to the topic frequency. Furthermore, we set five common and important RAG tasks, including extractive QA task, multi-hop reasoning QA task, contrast QA task, conversational QA task, and long-form QA task. The final topic and task systems used in our benchmark are shown in Figure 2. With the pre-defined topic-task matrix (T2M), we devise topic classification agent by GPT-4 that receives sampled document from the knowledge corpus and then classifies which domain topic is the most related to this document. This process could locate specific row of our T2M. The sampled document, the chosen topic, and all tasks are then used to generate associated evaluation instances in the subsequent steps. Figure 2: Topic & task systems used for building our benchmark. the multi-agent technique to build multi-topic and multi-tasks evaluation datasets. (3) Evaluation of RAG models (Section 3.3). Finally, we leverage various retrievers and LLMs to build comprehensive experiment baselines and evaluate their performance on our diverse RAG evaluation scenarios. We further design some model-based metrics, such as comprehensiveness, numerical accuracy, hallucination, etc., to evaluate the high-level quality of RAG responses beyond rule-based term-matching. The details are demonstrated in the following sections."
        },
        {
            "title": "3.1 Construction of Knowledge Corpus",
            "content": "To build wide coverage and diverse financial document corpus to support the construction of matrixbased RAG evaluation scenarios, we collect our knowledge corpus from various data sources, including, two open-source financial challenges, BS Challenge Financial (BSCF for short) and FinGLM, finance-related web pages from wikipediazh, open-source financial pretraining dataset, BAAI IndustryCorpus Finance (BAAI-Fin for short), and crawled financial web pages from the official agency websites. Considering that these external documents have diverse formats, such as PDF, text, and SQLite, we leverage LlamaIndex2, which is compatible with multiple data formats, to build our retrieval corpus. Specifically, we first transfer SQLite data to the JSON format, then utilize the LlamaIndex toolkit to split all documents into passages with the length being set as 2048 and the 2https://www.llamaindex.ai/ 4 Data Source Data Type Doc. Count Length Sum. Avg. Length per Doc. BSCF-DB BSCF-PDF FinGLM Wiki-Fin BAAI-Fin Official Websits DB - JSON PDF - TXT PDF - TXT JSON JSON JSON 193,774 3,082 55,595 3,367 48,124 58,616 23,631,875 10,587,648 97,296,690 5,679,758 70,014,858 45,837,298 122 3,435 1,750 1,687 1,455 782 Table 2: Statistical information of our diverse data sources. Doc., Sum., and Avg. are short for Document, summation, and Average. 3.2.2 Data Generation 3.2.4 Manual Quality Inspection and The previous step first specifies document and its related domain topic. We then traverse all predefined RAG tasks to generate associated data instances for each RAG scenario belonging to T2M element. Specifically, given document, its related domain topic, and an RAG task description, we build data generation agent that views this information as input and generates question-answer pair, satisfying the requirement of the provided task and related to the provided topic. We directly treat the input document as the relevant document of this QA pair. In addition, considering the long content of the input document, which may contain some irrelevant information, we further prompt the data generation agent to identify the most relevant passage within the document to the generated QA pair, improving the accuracy of the marked relevant passage. As result, data instance contains the following elements: user question, its answer, the relevant document, and relevant passage."
        },
        {
            "title": "3.2.3 Data Quality Inspection",
            "content": "To ensure the quality of generated data instances, we further introduce quality inspection agent to filter the low-quality generated data examples. The basis of this idea is that judging the quality of provided data instance is an easier task than directly generating high-quality data examples, hence introducing an inspection process may potentially improve the precision and quality of the filtered dataset. This agent treats the generated data instance as the input and predicts whether the instance contains meaningful information and satisfies the description of the target task. We only retain the instances that the quality inspection agent identifies as high-quality ones. Correction In addition to agent-based quality inspection, we further introduce human annotators to conduct data quality inspection and correction, leading to highquality evaluation dataset to confirm the reliability of our benchmark. Specifically, we sampled subset of all generated instances for each T2M element. Annotators are requested to check the following items: (1) Whether the generated question satisfies the requirements of the corresponding task description; (2) Is the generated question related to the corresponding topic; (3) Whether the semantics of the question is complete; (4) Whether the generated answer is correct and complete; (5) Whether the extracted relevant passages are accurate, complete, and precise. The annotation is five-scale from one to five, where 1 and 2 means the data quality is pretty low, and we should drop this instance, 3 means there are some defects but can be corrected manually, and finally 4 or 5 means the data quality is pretty good or excellent. The number of labeled data instances is 910, we statistic the inspection results and present them in Figure 3. The results reveal that the acceptance rate of our automated-generated cases is 87. 47%, which potentially confirms the effectiveness and usability of our multi-agent-based data generation pipeline. According to the above inspection and correction steps, we can obtain high-quality humanannotated dataset, which greatly enhances the soundness and reliability of our benchmark. As result, we construct two evaluation datasets where one is automated-generated and the other one is human-annotated. We further split the automatedgenerated ones into train and test datasets to support the further related investigation based on our benchmark. The data amounts of these three Setting Base Model κ Accuracy Zero-shot Llama3.1-8B-Inst Zero-shot Llama3.1-70B-Inst Zero-shot Qwen2.5-7B-Inst Zero-shot Qwen2.5-32B-Inst Zero-shot Qwen2.5-72B-Inst Lora Lora Llama3.1-8B-Inst Qwen2.5-7B-Inst 39.70 54.14 48.05 61.44 55.38 48.63 64.86 55.60 66.40 62.00 71.60 67.20 62.80 74.40 Table 3: Experimental results of model-based evaluator. Consequently, we propose completeness metric that measures whether the response can satisfy all aspects of the ground truth. It is 4-scale metric, where 1 means the response hits no aspects, 2 means the response partially satisfies all aspects, and 3 means the response covers all aspects comprehensively. Moreover, -1 means the case does not need to measure the completeness for some scenarios with short answers. Hallucination (HAL). It focuses on measuring whether the LLM-generated responses contain the hallucination, which is defined as below: If the response is totally right, the HAL is 0, if the response is wrong, but is derived from the retrieved documents, is also 0. If the answer is wrong, and the contents do not stem from the retrieved documents, the hallucination is 1. Therefore, the lower the HAL, the better the response is. Utilization (UTL). The utilization mainly assesses whether the LLM could make good use of the retrieved documents. Thus, it focuses on whether the answer could be traced from the retrieved documents. It is still 3-scale metric, which is similar to ACC. Numerical accuracy (NAC). Such metric primarily pays attention to the scenarios related to financial computation, where the answers are usually in the form of numbers. It is 2-scale metric, where 1 means right and 0 means wrong. Note that all metrics will be normalized into [0,1] to ensure the same scale. To ensure the reliability of our LLM-based evaluator, we conducted human annotation on subset of RAGs generated responses for these five metrics, leading to labeled dataset for training stable LLM-based evaluator. Specifically, we randomly sampled 127 cases and produced 635 examples by summing up all five metrics. We split it into training, validation, and test sets by the ratio 5:1:4. We have performed various Lora-based fineFigure 3: Statistical information of manual inspection. datasets are shown in Figure 8, 9, and 10. 3.3 Evaluation of RAG Models Given the automated-generated and humanannotated evaluation datasets, we utilize two types of metrics, rule-based metrics, and model-based metrics, to evaluate various RAG baselines comprehensively and accurately. Rule-based Metrics Considering the wide usage and stability of rule-based metrics, we leverage Rouge-L to provide basic evaluation of experimented RAG systems. Besides, we further incorporate ranking metrics, MAP and MRR, to evaluate the performance of retrievers in RAG systems, leading to comprehensive evaluation of the entire RAG pipeline. Model-based Metrics However, due to the flexibility and diversity of AI chatbots responses, it is challenge for rule-based metrics to provide an accurate and semantical evaluation. Therefore, we further propose five high-level evaluation metrics, which are implemented by prompting LLMs to conduct evaluation. Accuracy (ACC). In fact, LLMs often generate responses that are correct in content but poorly matched in words. Therefore, the semantic matching between the LLM responses and the golden answers is critical for reliably evaluating RAG systems. Therefore, we propose model-based accuracy metric to measure semantic consistency. It is 3-scale metric with 1 means poor, 2 means average, and 3 means good. Completeness (COM). Long-form QA is also common situation, where users tend to ask questions that are broad and general, hoping LLMs could return comprehensive answer that covers various aspects of the question (Wang et al., 2024b). 6 Models MAP MRR Rouge-L F1 ACC HAL COM UTL NAC Automated-generated evaluation set Jina-zh BGE-large-zh BGE-M3 GTE-Qwen2-1.5b 0.3395 0.3777 0.3961 0.4370 0.3469 0.3865 0.4057 0.4491 0.1662 0.1693 0.1746 0.1778 0.2553 0.2541 0.2593 0. 0.3908 0.4080 0.4091 0.4326 0.0794 0.0597 0.0634 0.0467 0.5981 0.6048 0.6092 0.6256 0.5078 0.5194 0.5203 0.5613 0.2837 0.3124 0.3060 0.3293 Human-annotated evaluation set Jina-zh BGE-large-zh BGE-M3 GTE-Qwen2-1.5b 0.3458 0.4153 0.4152 0.4443 0.3533 0.4252 0.4236 0.4574 0.2341 0.2435 0.2517 0.2528 0.3821 0.3870 0.3913 0.3919 0.4089 0.4325 0.4450 0. 0.0886 0.0718 0.0709 0.0618 0.5930 0.6224 0.6208 0.6190 0.5163 0.5367 0.5410 0.5576 0.3073 0.3545 0.3472 0.3595 Table 4: The overall results of retrieval models with the generator being set as Qwen2.5-72B. Retriever Generator Rouge-L F1 ACC HAL COM UTL NAC Automated-generated evaluation set Yi15-34B Deepseek-v2-chat Qwen2.5-72B Llama3-70B-Instruct CLOSE CLOSE CLOSE CLOSE GTE-Qwen2-1.5B Yi15-34B GTE-Qwen2-1.5B Deepseek-v2-chat GTE-Qwen2-1.5B Qwen2.5-72B GTE-Qwen2-1.5B Llama3-70B-Instruct 0.0326 0.1861 0.1607 0.1993 0.0593 0.2279 0.1778 0.3235 0.0673 0.3709 0.3222 0.3989 0.0958 0.3300 0.2563 0.4810 0.1573 0.3587 0.3788 0.3238 0.3402 0.4099 0.4326 0.4398 Human-annotated evaluation set Yi15-34B Deepseek-v2-chat Qwen2.5-72B Llama3-70B-Instruct CLOSE CLOSE CLOSE CLOSE GTE-Qwen2-1.5B Yi15-34B GTE-Qwen2-1.5B Deepseek-v2-chat GTE-Qwen2-1.5B Qwen2.5-72B GTE-Qwen2-1.5B Llama3-70B-Instruct 0.0497 0.2250 0.2082 0.2195 0.0887 0.2916 0.2528 0.3390 0.1161 0.4353 0.4191 0.4183 0.1583 0.4353 0.3919 0.5042 0.1461 0.3306 0.3405 0.2859 0.3366 0.4234 0.4476 0.4433 - - - - 0.0597 0.0634 0.0467 0.0792 - - - - 0.0648 0.0750 0.0618 0. 0.5063 0.5755 0.6017 0.5284 0.5778 0.6072 0.6256 0.5926 0.4987 0.5541 0.5754 0.5133 0.5821 0.6006 0.6190 0.5745 - - - - 0.4229 0.5197 0.5613 0.4754 - - - - 0.4234 0.5160 0.5576 0.4764 0.0693 0.1121 0.1256 0.0677 0.1682 0.3175 0.3293 0.3088 0.0749 0.1153 0.1241 0.0659 0.1856 0.3213 0.3595 0. Table 5: The overall evaluation results on final responses of RAG models. Figure 4: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Yi15-34B on Rouge-L. 7 Figure 5: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Deepseek-v2-chat on Rouge-L. Figure 6: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Qwen2.5-72B on Rouge-L. Figure 7: Visualization of matrix-based evaluation of GTE-Qwen2.5-1.5B+Llama3.1-70B on Rouge-L. tuning or zero-shot settings on Qwen2.5 (Team, 2024) and Llama3.1 (Dubey et al., 2024) with varied model sizes. The comparison experiments are shown in Table 3. The evaluation metrics are accuracy and κ value between the predicted results and ground truths. Finally, we build our evaluator by 8 fine-tuning Qwen-2.5-7B-Instruct, which has the highest accuracy, 74.4%. These experimental results also validate the effectiveness and credibility of our LLM-based evaluator and evaluation results."
        },
        {
            "title": "4 Experiment",
            "content": "We conduct our experiments on various openSpecifically resource retrievers and LLMs. ,for retrievers, we select GTE-Qwen2-1.5B (Li et al., 2023), BGE-large-zh (Xiao et al., 2023b), BGE-M3 (Xiao et al., 2023b), and Jinazh (Mohr et al., 2024). For LLMs, we choose Qwen2.5-72B-Instruct (Team, 2024), Deepseekv2-chat (DeepSeek-AI, 2024), Yi15-34b (Young et al., 2024), and Llama3.1-70B-Instruct (Dubey et al., 2024). 4.1 Overall Experimental Results Our experiments aim to evaluate the entire pipeline of RAG systems, including retrievers and generators (LLMs). First, we conducted the overall experiments on our two evaluation datasets, the autogenerated set and the human-annotated set. The main results are shown in Table 4 and 5, where the CLOSE setting means we only generate responses by LLMs themselves. Note that HAL and UTL metrics are required to be evaluated based on the retrieved results. Therefore, there are no results for CLOSE settings. According to the shown results, we conclude the following points: (1) GTE-Qwen2-1.5b outperforms all compared retrievers on almost all metrics. We analyze the reason because GTE-Qwen2-1.5b is fine-tuned from existing LLMs, which can potentially utilize the world knowledge contained in LLMs parameters, leading to better retriever quality. Therefore, when comparing subsequent RAG systems, we choose GTE-Qwen2-1.5b as our basic retrievers and compare the response quality of existing popular LLMs. (2) RAG systems generally surpass close-book LLMs on our evaluation datasets. We notice that after equipping the retrievers, LLMs usually perform better results than close-book settings. These results prove that for domain-specific scenarios, it is necessary for LLM to retrieve external expert knowledge to enhance reliable responses. (3) There is still large space for existing retrievers and LLMs to enhance the RAG abilities in financial domains. We find that even RAG systems still perform poorly for all retriever and LLM settings. This phenomenon implies the difficulty of our evaluation datasets, involving some expert and hard reasoning financial tasks. It also confirms that our benchmark introduces new challenges for existing RAG systems, which may potentially promote the investigation of RAG models in domain-specific scenarios. 4.2 Experiments on Topic-specific Subsets As we mentioned before, we build topic tree to build several subsets to evaluate the RAG systems performance on different scenarios with diverse query topics. We further demonstrate RAG models performance on topic-specific subsets to distinctly illustrate their abilities to handle diverse topics. The comparison results are shown in Figure 11 and Figure 12. We notice that the same RAG model will perform differently on diverse topic scenarios, which reveals that their capabilities are still imbalanced across different query topics. It may be because those different topics have different popularity among the pre-trained corpus of LLMs, leading to imbalanced RAG abilities. Therefore, how to balance the RAG models capabilities among diverse topics with different popularities may also be an important investigation direction."
        },
        {
            "title": "4.3 Experiments on Task-specific Subsets\nAccording to our T2M-based evaluation subsets,\nwe further compare experimental RAG models on\ndifferent task evaluation sets, evaluating their abil-\nities on diverse query tasks. The experimental re-\nsults are illustrated in Figure 13 and Figure 14.",
            "content": "It is obvious that for different query tasks, the RAG system also performs diversely. This phenomenon may stem from the different hardness of these tasks. For example, most RAG models perform worst on multi-hop reasoning and conversational QA tasks, since these two tasks require RAG models to be equipped with strong reasoning or context-understanding abilities, which introduces more difficulty in generating accurate responses. As result, the investigation of enhancing RAG systems abilities on these hard but more practical tasks is also promising and important direction. As we aforementioned, our matrix-based evaluation approach could provide more fine-grained assessment of RAG models, which distinctly reveals the models performance on specific topics and tasks, leading to comprehensive profile of evaluated models capabilities. Therefore, we also provide matrix-based visualization of our four eval9 uated RAG models, which are shown in Figure 4, 5, 6, and 7."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we propose an automatic and omnidirectional RAG benchmark in vertical and expert domain, finance i.e.. We propose to first identify diverse query scenarios by matrix-based method, which considers two orthotropic perspectives, topics, and tasks. By this means, we can comprehensively and fine-grained evaluate RAG systems by simulating diverse practical query scenarios, leading to more detailed evaluation results. We utilize multi-agent technique to automatically build our evaluation datasets. With sufficient model-based and manual quality inspections, we derive three datasets, an automated-generated training set, an automated-generated test set, and human-annotated test set to build our dataset. The high human-annotated data acceptance confirms the quality and reliability of our evaluation dataset. Our experimental results show that there still exists significant improvement space for existing RAG models. In addition, RAG systems usually perform differently in diverse query scenarios, which also suggests several challenges and investigation directions for RAG studies."
        },
        {
            "title": "References",
            "content": "Jian Chen, Peilin Zhou, Yining Hua, Loh Xin, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. 2024a. Fintextqa: dataset for longform financial question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 60256047. Association for Computational Linguistics. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024b. BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216. Jiawei Chen, Hongyu Lin, Xianpei Han, and Benchmarking large lanLe Sun. 2024c. guage models in retrieval-augmented generation. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1775417762. AAAI Press. Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, and Zhongyu Wei. 2023. Disc-finllm: chinese financial large language model based on multiple experts fine-tuning. CoRR, abs/2310.15205. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. Finqa: dataset of numerical reasoning over financial data. Proceedings of EMNLP 2021. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. Proceedings of EMNLP 2022. DeepSeek-AI. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. CoRR, abs/2310.19923. 10 Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: new benchmark for financial question answering. CoRR, abs/2311.11944. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, and Jun Huang. 2024. Alphafin: Benchmarking financial analysis with retrievalaugmented stock-chain framework. In Proceedings International Conference on of Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 773783. ELRA and ICCL. the 2024 Joint Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281. Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. RECALL: benchmark for llms robustness against external counterfactual knowledge. CoRR, abs/2311.08147. Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024. CRUD-RAG: comprehensive chinese benchmark for retrievalaugmented generation of large language models. CoRR, abs/2401.17043. Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, et al. 2024. Multi-task contrastive learning for 8192token bilingual text embeddings. arXiv preprint arXiv:2402.17016. Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024. ARES: an automated evaluation framework for retrieval-augmented generation systems. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 338354. Association for Computational Linguistics. Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of named entity recognition to support credit risk assessment. In Proceedings of the Australasian Language Technology Association Workshop 2015, pages 84 90, Parramatta, Australia. Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. WHEN FLUE MEETS FLANG: benchmarks and large pre-trained language model for financial domain. CoRR, abs/2211.00083. Ankur Sinha and Tanmay Khandait. 2020. Impact of news on the commodity market: Dataset and results. CoRR, abs/2009.04202. Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and Kang. 2022. Accurate stock movement prediction with self-supervised learning from sparse noisy tweets. In 2022 IEEE International Conference on Big Data (Big Data), pages 16911700. IEEE Computer Society. Qwen Team. 2024. Qwen2.5: party of foundation models. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: heterogeneous benchmark for zeroshot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu, and Zhicheng Dou. 2024a. Domainrag: chinese benchmark for evaluating domain-specific retrievalaugmented generation. CoRR, abs/2406.05654. Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. 2024b. Richrag: Crafting rich responses for multi-faceted queries in retrieval-augmented generation. CoRR, abs/2406.12566. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023a. C-pack: Packaged resources to advance general chinese embedding. CoRR, abs/2309.07597. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023b. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, and Jimin Huang. 2024. The finben: An holistic financial benchmark for large language models. CoRR, abs/2402.12659. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: large language model, instruction data and evaluation benchmark for finance. CoRR, abs/2306.05443. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented In Findings of the generation for medicine. 11 Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 62336251. Association for Computational Linguistics. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. CoRR, abs/2403.04652. Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, and Jianfeng Gao. 2024. Reeval: Automatic hallucination evaluation for retrieval-augmented large language models via transferable adversarial attacks. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 13331351. Association for Computational Linguistics."
        },
        {
            "title": "A Some visualization results of OmniEval",
            "content": "12 Figure 8: Statistical information of the automated-generated training set. Figure 9: Statistical information of the automated-generated test set. Figure 10: Statistical information of the human-annotated test set. 13 Figure 11: The topic-specific evaluation results on the auto-generated set. Figure 12: The topic-specific evaluation results on the human-annotated set. 14 Figure 13: The task-specific evaluation results on the auto-generated set. Figure 14: The task-specific evaluation results on the human-annotated set."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}