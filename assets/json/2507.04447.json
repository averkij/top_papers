{
    "paper_title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge",
    "authors": [
        "Wenyao Zhang",
        "Hongsi Liu",
        "Zekun Qi",
        "Yunnan Wang",
        "XinQiang Yu",
        "Jiazhao Zhang",
        "Runpei Dong",
        "Jiawei He",
        "He Wang",
        "Zhizheng Zhang",
        "Li Yi",
        "Wenjun Zeng",
        "Xin Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 4 4 4 0 . 7 0 5 2 : r DreamVLA: Vision-Language-Action Model Dreamed with Comprehensive World Knowledge Wenyao Zhang124 Xinqiang Yu4 Hongsi Liu27 Jiazhao Zhang45 Zekun Qi34 Runpei Dong6 Yunnan Wang12 Jiawei He4 He Wang45 1SJTU Zhizheng Zhang4 Li Yi3 2EIT 3THU 4Galbot Wenjun Zeng2 6UIUC 5PKU Xin Jin2 7USTC"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing perceptionprediction-action loop for manipulation tasks. Specifically, DreamVLA introduces dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks."
        },
        {
            "title": "Introduction",
            "content": "The evolution of robot learning has demonstrated impressive progress [111] in training policies capable of performing diverse tasks across various environments [1225]. One promising direction is Vision-Language-Action (VLA) models, which leverage the rich understanding capabilities of pre-trained Multimodal Large Language Models (MMLMs) [2629] to directly map natural language instructions and visual observations to robot actions [15, 1, 12]. Although these approaches [30 32, 13, 1, 3338] have achieved impressive results, their direct mapping from observations to actions lacks the closed-loop forecasting capability that humans typically possess when understanding and reasoning about future knowledge of environments. To incorporate future knowledge prediction into VLA, most existing methods [39, 5, 4051] leverage copilot generation model to generate future frames/keypoints, then predict action sequences conditioned on goal images. Several methods [5257] integrate pixel-level image forecasting with the Equal contribution. Corresponding author. Preprint. Figure 1: (a) Vanilla VLA directly maps visual observations and language instructions to actions. (b) Models leveraging separate image/video generation or copilot models to generate future frames or trajectories, subsequently guiding an action head. (c) VLA variants explicitly predict subgoal image as an intermediate visual reasoning step prior to action generation. (d) Our proposed DreamVLA, which explicitly predicts dynamic regions, depth map, semantics (DINOv2 and SAM) knowledge, significantly enhances the models action reasoning and generalization. action prediction in single framework, which exploits the synergy of prediction and planning and regards the prediction as an intermediate reasoning step [54] akin to those used in large language models (LLMs) [58]. Despite early success in incorporating dense visual forecasting, these methods naturally exhibit limitations: (1) Redundant pixel information: There exists significant overlap between forecasted images and current observations, making the prediction less efficient and effective. (2) Lack of spatial information: Absence of explicit 3D knowledge of environments [5962, 22]. (3) Lack of high-level knowledge forecasting: Missing high-level understanding of future states, e.g., semantics information. Therefore, we argue that existing methods (Figure 1 (a-c)) are insufficient to forecast future states for more comprehensive prediction-action loop in the context of world-level future knowledge. To address these issues, we propose DreamVLA, novel framework that incorporates comprehensive world knowledge forecasting into the vision-language-action models, thereby establishing perception-prediction-action loop for the manipulation task. As shown in Figure 1 (d), instead of directly generating entire future frames, our proposed method introduces world embedding to predict comprehensive world knowledge, which is highly relevant to robot execution, such as dynamic area, depth, and high-level semantic features. This approach aligns with the way humans interact with the world, emphasizing relevant changes and world knowledge. By dreaming/forecasting these targeted aspects of the environment, we aim to provide the model with concise and relevant intermediate representations that facilitate more effective action planning. To obtain comprehensive world knowledge, our approach incorporates three key features: (1) Dynamic region-based forecasting. We leverage an off-the-shelf optical flow prediction model [63, 64] to identify dynamic regions within the scene, enabling the model to concentrate on areas of motion that are critical for task execution instead of redundant frame reconstruction. (2) Depth-aware forecasting. We employ depth estimation techniques [59] to generate per-frame depth maps, providing valuable spatial context that aids in understanding the three-dimensional structure of the environment. (3) High-level foundation features. We incorporate semantic features aligned with visual foundation models such as DINOv2 [65] and SAM [66]. In this way, DreamVLA offers more comprehensive and effective pathway for the model to plan and execute. Furthermore, we adopt block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Since the world and action embeddings occupy the same latent space and share similar statistics, naive MLP head cannot disentangle modality-specific information or exploit their cross-modal correlations. We employ diffusion-based transformer that disentangles action representations from shared latent features to reason actions. Through extensive experiments on public benchmarks, we find that incorporating world knowledge prediction leads to significant performance improvements. Our method achieves state-of-the-art performance on the CALVIN benchmark (4.44 average length), and we analyze the influence of the ingredients of our world knowledge and find that they have improvements in different aspects. Specifically, comprehensive ablation shows that predicting dynamic regions alone delivers the greatest gains, while depth and semantic cues offer smaller, roughly equal benefits. Worse, when depth or semantic prediction is used in isolation, it not only fails to help but can actually degrade performance. Extensive experiments on both simulation and real-world demonstrate the effectiveness of our method. 2 The key contributions of our work are summarized as follows: We recast the visionlanguageaction model as perceptionpredictionaction model and make the model explicitly predict compact set of dynamic, spatial and high-level semantic information, supplying concise yet comprehensive look-ahead cues for planning. We introduce block-wise structured-attention mechanism, coupled with diffusion-transformer decoder, to suppress representation noise from cross-type knowledge leakage and thus enable coherent multi-step action reasoning. DreamVLA sets new state of the art on the CALVIN ABC-D benchmark (4.44 average task length), outperforming prior methods by up to 3.5% on the simulation platform, and boosts real-world success to 76.7%. Ablation studies confirm each components contribution."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 VisionLanguageAction Models The earliest VLA [16, 67, 2, 6870] lay the foundation by combining pretrained vision-language representations with task-conditioned policies for manipulation and control. Inspired by the recent advances of Large Language Models [7174] and multimodal large language models [28, 26, 75, 61, 76] and the emergence of large-scale robot datasets [12, 7779], VLA has become trend in robot learning. RT series [2, 80, 81] is the pioneer attempt to fine-tune the MLLM on robot demonstration datasets, resulting in strong accuracy and generalization. Building on this foundation, many advanced techniques [30, 32, 13, 1, 33, 34, 69, 3537, 8284, 38, 85] are developed to boost the performance. Meanwhile, considering the advantage of the diffusion model in modeling multi-peak, some researchers [8690] employ different architectures to sample action from noise conditioned on observation, task instruction, and robot prior knowledge. Given on this manner which directly maps observation and instruction to action lacks reasoning step like LLM [58], most existing methods [39, 5, 4045] leverage copilot image/video generation model to generate future frames then predict action sequences conditioned on goal images. However, the above methods still need an extra generation model, which introduces inference time and computation load. Therefore, several methods [5257] integrate pixel-level image forecasting with the action prediction in single framework, which exploits the synergy of prediction and planning. Despite success, these methods naturally exhibit limitations in redundant reconstruction [91], suffer from lack of spatial and semantic information. 2.2 Knowledge Forecasting for Robotics Learning future world knowledge for robot training has increasingly become popular to enable policies for achieving an action-forecasting loop. Early attempts [45, 19, 14, 39, 47, 46, 92] to implement this based on off-the-shelf world models [86, 93, 49] and feed the forecasting of images or states into the robot policy model to conduct inverse dynamics. This two-stage training strategy is easy to implement but is limited by the performance of world models. More advanced solutions [54, 52, 84, 94] incorporate forecasting and action planning in an end-to-end manner, requiring the policy to predict actions along with future states. This line of work demonstrates better performance and generalization. However, the future states are limited to redundant visual information [59, 60, 95, 65, 96, 62] In contrast to previous work, DreamVLA proposes to predict or monotonous states [21, 44]. future knowledge in an efficient (dynamic region) and effective (comprehensive knowledge) way, demonstrating strong performance and generalization."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Problem Definition and Notation We aim to improve robot execution by leveraging rich world knowledge as guiding principle. In this context, we formulate visionlanguageaction reasoning as an inverse dynamics problem [97, 52, 45], which regards the future world knowledge prediction as the intermediate reasoning for robot control, fully unleashing the synergy of prediction and execution. At each time step t, the robot receives three heterogeneous signals: natural language instruction l, raw visual frame ot, and its proprioceptive 3 Figure 2: Framework Overview. Given the current robot state st, observation ot, and language instruction, DreamVLA encodes multimodal inputs via frozen text, visual encoders and tunable state encoder. These tokens, together with learnable set of <dream> queries, are processed by large language model to produce world embedding. Three lightweight decoders then project each corresponding element of this embedding into the dynamics region ˆft+n, monocular depth ˆdt+n and high-level semantics ˆct+n. separate <action> query draws latent action embedding, which conditions diffusion transformer that refines Gaussian noise into an n-step action sequence ˆat:t+n1. The dashed box highlights prediction heads that are used only during training, inference skips these heads and operates directly on the world embedding. state st. To inject look-ahead reasoning, we define set of special tokens called <dream> queries [75], and concatenate all inputs into sequence. unified model maps these inputs into compact latent representation, which we call the world embedding: wt+n = (l, ot, st<dream>) . (1) Next, the world embedding predicts the comprehensive world knowledge that combines motion cues, spatial details and high-level semantics. Specifically, set of predictor extrapolates steps ahead, ˆpt+n = P(cid:0)wt+n (cid:1) = (cid:2) ˆft+n, ˆdt+n, ˆct+n (cid:3), (2) where ˆft+n marks dynamic regions, ˆdt+n encodes monocular depth, and ˆct+n optionally stores high-level semantic feature (e.g. DINOv2 [65], SAM [66]). Given world embedding wt+n, the <action> query is assigned to the latent action embedding by the unified model to aggregate the correlated action information. denoising-diffusion Transformer formulates an n-step action based on the latent feature: ˆat:t+n1 = D(M(cid:0)l, ot, st, <dream><action>)), thus completing perceptionpredictionaction loop that is identical during training and inference. The remainder of this chapter details the system componentsencoders, world-knowledge predictor, and diffusion-based action generatorthat instantiate the above formulation. (3) 3.2 Model Architecture As illustrated in Figure 2, our DreamVLA framework comprises three core modules operating within unified Transformer architecture. Firstly, heterogeneous inputsincluding natural language l, visual observations ot, and proprioceptive states stare individually processed by modality-specific encoders. We encode language instructions using CLIP [95] text embeddings, visual frames through Masked Autoencoder [98] to obtain spatiotemporal patch representations, and proprioceptive signals via several convolutional and fully-connected layers. Following encoding, set of learnable queries designated as <dream> and <action> are appended to these multimodal embeddings, where <dream> contains three subqueries (dynamic, depth and semantics), which could be used for the prediction of specific knowledge. Subsequently, we leverage large language model based on GPT4 Figure 3: Visualization of dynamic regions over time. We show the static camera (left) and wrist-mounted camera (right) observations alongside the corresponding dynamic masks generated by our method at multiple time steps. The masks highlight dynamic regions by leveraging optical flow trajectories extracted via CoTracker [64, 63]. Compared to the original observations, our method effectively suppresses irrelevant background and focuses on interaction-relevant areas (e.g., moving objects and end-effector), enabling more structured and efficient action reasoning. [99] to integrate and attend across modalities and queries using carefully structured causal and non-causal attention mechanisms (Figure 4). This effectively fuses low-level perceptual signals into compact, semantically coherent representations of the world state. Finally, specialized light-weight output heads comprising by shallow convolutional layers decode world embedding into explicit predictions: reconstruct anticipated dynamic region, monocular depth, and semantic features. During inference, DreamVLA skips the decoder entirely, saving substantial Instead, the model outputs an world embedding that encapsulates predictions of computation. future dynamics, depth, and semantics without pixel-level reconstruction, thereby retaining the accuracy gains from future-state reasoning while maintaining low latency. In parallel, we employ Denoising Diffusion Transformer [86] to decode latent action embedding into executable robot action sequences. Collectively, these components enable DreamVLA to perform robust, predictive visionlanguageaction reasoning in an end-to-end manner. 3.3 Comprehensive World Knowledge Prediction Predicting what will matter next is more valuable than merely reproducing the raw future frame. DreamVLA explicitly forecasts future world knowledge that is most relevant for manipulation, including (i) motioncentric dynamic region, (ii) 3D depth geometry, and (iii) high-level semantics. These complementary signals provide compact, structured surrogate for raw pixels and supply the policy with look-ahead context for inverse dynamics planning. Motion-centric dynamic-region reconstruction. Predicting dynamic regions tells the robot what parts of the scene are about to move, allowing the model to capture the statistical link between the current scene, the language instruction, and the actions needed to realize the predicted motion. As shown in Figure 3, DreamVLA neither predicts dense optical flow nor synthesizes an entire future frame. Instead, we first apply CoTracker [63, 64] to extract dynamic regions, namely pixels that move with the robot end-effector or other movable objects, and then train DreamVLA to reconstruct only these regions. Furthermore, generating reconstruction targets with an asymmetrical tokenizer can further enhance performance [98]. From the perspective of discrete variational autoencoder (dVAE) [100103], the overall optimization is to maximize the evidence lower bound (ELBO) [104 106, 62] of the log-likelihood P(xixi). Let denote the original image, the masked motion region, and the reconstruction target. The generative modeling can be described as: (cid:88) log P(xixi) (cid:88) (cid:16) EziQϕ(zxi) (cid:2) log Pψ(xizi)(cid:3) DKL (cid:2)z, Pθ(z ˆzi)(cid:3)(cid:17) , (4) (zi,zi)D (xi,xi)D where Pψ(xz) is the tokenizer decoder to recover origin data, ˆzi = Qϕ(zxi) denotes the masked motion region tokens from masked data and Pθ(z ˆzi) reconstructs masked tokens in an autoencoding fashion. Here, the Pθ(z ˆzi) is zero, and the dynamic region prediction loss can be formulated as: Ldyn = 1 (cid:88) xi EzQϕ(zxi) (cid:104) log Pψ (cid:0)(xi)M z(cid:1)(cid:105) . (5) 5 Depth prediction. Predicting how the depth field will evolve tells the robot where it should move next, steering it toward free space and away from impending obstacles. If depth sensors are available, we supervise the DreamVLA with ground-truth maps; on low-cost platforms without depth sensing, we instead hallucinate future geometry from single RGB stream. To do so, we treat Depth-Anything [59, 60] predictions as self-supervised teacher and train dedicated depth query to regress the aligned future map ˆdt+n. The objective is scale-normalized mean-squared error, (cid:88) Ldepth = 1 HW (cid:0) ˆd(i,j) t+n α d(i,j) t+n (cid:1)2 , (6) i,j α = (cid:80) i,j (cid:80) t+n d(i,j) ˆd(i,j) i,j d(i,j) 2 t+n t+n , (7) where α removes the global scale ambiguity that monocular methods cannot resolve. In practice, this simple loss is sufficient: the teacher provides metrically plausible depth, and the scale-normalization term encourages the model to preserve ordinal depth relationships, property that is crucial for grasp synthesis and collision checking, while ignoring any arbitrary global scale shift. Contrastive semantic forecasting. Predicting future semantics teaches the robot which objects or regions will matter for the task, providing high-level context (for example, object identity and affordances) that guides the selection of goals and grasp choice. To learn these semantics, DreamVLA predicts future DINOv2 [65] and SAM [66] feature ˆct+n using an InfoNCE loss [107, 62]: the ground-truth feature is the positive sample, whereas spatially shifted features act as negatives. This encourages discriminative anticipation that the model must pick the correct object semantics among plausible but wrong futures: Lsem = log exp(cid:0)ˆc exp(cid:0)ˆc (cid:80) t+nct+n/τ (cid:1) t+nck/τ (cid:1) , (8) where represents the number of tokens in spatial, and τ denotes the temperature. Structured attention for cross-type knowledge disentanglement. To preserve clear cross-type knowledge boundaries, <dream> is decomposed into three sub-queries (dynamic, depth and semantics). If these sub-queries could freely attend to one another, high-frequency flow details would contaminate depth reasoning, and semantic cues might bleed into motion features, producing noisy mixed representations. We therefore mask their mutual attention: each sub-query attends only to the shared visual, language, and state tokens, while direct links among the three are disabled, keeping their latent features disentangled and free of crosstalk. As shown in Figure 4, both <dream> and <action> queries also employ causal attention restricted to past context, which preserves temporal causality. This organized pattern mirrors the specialist routing used in Mixture-ofExperts (MoE) networks [108]. By avoiding cross-modal leakage, the structured attention supplies clean future world knowledge for action prediction, improves robustness, and maintains temporal consistency. Figure 4: Block-wise structured attention. 3.4 Inverse Dynamics via Denoising Diffusion Transformer Given two ordered observations ot and ot+1, classical inverse dynamics infers the intermediate action ˆat. We extend this formulation by predicting full action sequence ˆat:t+n1 conditioned on the current observation ot and future latent world embeddings wt+n. Specifically, DreamVLA first aggregates this latent embedding, already enriched with predicted future dynamics, depth, and 6 Table 1: CALVIN ABC-D results. We present the average success computed over 1000 rollouts for each task and the average number of completed tasks to solve 5 instructions consecutively (Avg. Len.). DreamVLA shows significant superiority over baselines. The best results are bolded. Method Roboflamingo [30] Susie [112] GR-1 [14] 3D Diffusor Actor [89] OpenVLA [1] RoboDual [113] CLOVER [114] UNIVLA [115] UP-VLA [53] Robovlm [37] Seer [52] VPP [45] DreamVLA 1 82.4 87.0 85.4 92.2 91.3 94.4 96.0 95.5 92.8 98.0 96.3 95.7 98.2 Task completed in row 3 46.6 49.0 59.6 63.9 62.0 72.1 70.8 75.4 81.5 85.4 86.1 86.3 89. 4 33.1 38.0 49.7 51.2 52.1 62.4 57.5 66.9 76.9 77.8 80.3 81.0 83.4 5 23.5 26.0 40.1 41.2 43.5 54.4 45.4 56.5 69.9 70.4 74.0 75.0 78.1 Avg. Len. 2.47 2.69 3.06 3.27 3.27 3.66 3.53 3.80 4.08 4.25 4.28 4.29 4. 2 61.9 69.0 71.2 78.7 77.8 82.7 83.5 85.8 86.5 93.6 91.6 91.2 94.6 semantics, into compact action embedding via dedicated action query and the models causal attention. Since the world and action embeddings occupy the same latent space and share similar statistics, naive MLP head cannot disentangle modality-specific information or exploit their crossmodal correlations. We therefore employ Denoising Diffusion Transformer (DiT) [86, 109] as the action head. Conditioned on the action embedding, DiT employs iterative self-attention and denoising to fuse perceptual forecasts with control priors and to transform Gaussian noise into an n-step trajectory at:t+n1, yielding coherent, diverse, and physically grounded action sequences. The loss of action prediction can be formulated as: (cid:13) LDiT = Eτ,ε (cid:13)ε εθ ατ at:t+n1 + where εθ is the DiT denoiser, ε (0, I), ατ follows cosine noise schedule and is the latent action embedding obtained from large language model. Inference is performed by drawing Gaussian sample and running the learned reverse diffusion, yielding diverse yet physically plausible trajectories that close the perceptionpredictionaction loop. 1 ατ ε, τ, c(cid:1)(cid:13) 2 2, (cid:13) (cid:0) (9)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details All models are implemented in PyTorch and trained on NVIDIA 8 A800 GPUs. We use an AdamW [110] optimizer with initial learning rate 103, weight decay 1e 4, and cosine learningrate schedule with 5% linear warm-up. Batch size is set to 8, we set the query length of each modality 9 and diffusion steps in DiT to 10. We weight the dynamic region, depth and segmentation prediction losses as λdyn=0.1, λdepth=0.001, λsem=0.1, and the action loss as λDiT=1, respectively. We first pre-train DreamVLA on the language-free split of the CALVIN [111] and on the full DROID dataset [78]. The model predicts entire frames instead of comprehensive knowledge, keeping storage and computation requirements manageable. We then fine-tune DreamVLA on each target dataset using the comprehensive world knowledge forecasting objective. All models are trained for 20 epochs, and we select the checkpoint with the highest validation success rate (SR) for final evaluation. 4.2 Simulation Benchmark Experiments Simulation setup. We evaluate DreamVLA on CALVIN [111], simulated benchmark designed for learning long-horizon, language-conditioned robot manipulation policies. It comprises four distinct manipulation environments and over six hours of teleoperated play data per environment, captured from multiple sensors including static and gripper-mounted RGB-D cameras, tactile images, and proprioceptive readings. We report the success rate of every track and the average length of 5 tasks. Results. As shown in Table 1, DreamVLA achieves the highest performance on ABC-D tasks, indicating that our method has better multi-task learning and generalization capabilities in simulation tasks. More detailed experimental results are shown in the supplementary materials. 7 Table 2: Real-world evaluation with the Franka Robot across three tasks. Method Pick Place Drawer Task (All) Bottle Doll Avg. Banana Chili Avg. Open Close Avg. Diffusion Policy [13] Octo-Base [13] OpenVLA [1] DreamVLA 50.0 50.0 50.0 85.0 70.0 60.0 40.0 80.0 60.0 55.0 45.0 82. 65.0 40.0 20.0 80.0 45.0 50.0 30.0 80.0 55.0 45.0 25.0 80.0 15.0 20.0 40.0 70.0 60.0 50.0 30.0 65.0 37.5 35.0 35.0 67. Avg. 50.8 45.0 35.0 76.7 4.3 Real World Experiments To evaluate the effectiveness of our method in the real-world, we use the Franka Panda arm to conduct real-world experiments on gripper grasping. In our setups, two RealSense D415 cameras capture RGB images. One is in third-person view, and the other is at the end of the robotic arm, as shown in Figure 5. We collect four categories of objects for two tasks: pick and place. Additionally, we conduct experiments on drawer opening and closing tasks, as shown in the supplementary. Follow [52], we pretrain DreamVLA on the DROID [78] contains large-scale trajectories of Franka robots in varied scenes. For fair comparison, we fine-tune Diffusion Policy [86], Octo-Base [13], OpenVLA [1] and DreamVLA on collected demonstration datasets containing 100 trajectories for each task. In the experimental setup, each trial permits maximum of 20 consecutive attempts. For the grasping experiments, objects are randomly positioned on the table surface. trial is deemed successful if the robotic arm successfully grasps the target object within the predefined attempt limit. In the placement experiments, the robot is required to transfer the grasped object into designated basket. Success is recorded only if both the grasping and placement operations are completed within the allowed attempts. For the drawer manipulation tasks, the drawer is placed randomly in front of the robotic arm. The experiment is considered successful if the drawer displacement exceeds 10 centimeters, indicating effective interaction. The results, presented in Table 2, demonstrate that our method performs better than other methods. More real-world experiment visualizations are shown in the supplementary section. 4.4 Ablation Study Figure 5: Real-world experiment setup. In this section, we design the experiments to investigate the following questions. Q1: What is the contribution of each modal characteristic? The core motivation of DreamVLA is to enable the model to predict comprehensive visual knowledge of the future to enhance action reasoning. However, not all types of knowledge contribute equally to subsequent execution, we consider four types of predictive knowledge: dynamic region, depth, and semantic segmentation features derived from SAM and DINO. As shown in Figure 6, we first train the model with each knowledge forecasting independently. The green dashed line denotes the performance of the Vanilla VLA baseline, which uses no knowledge prediction. Among all, predicting dynamic regions proves to be the most beneficial, because these masks explicitly flag the pixels that are about to change and therefore align almost perfectly with the policys action semantics. By contrast, supervising the network with depth map, DINO or SAM features alone not only fails to help but often degrades performance. We analyze that this gap stems from how closely each auxiliary target matches the downstream objective: dynamic-region labels supply gradients that reinforce the action head, whereas depth regression and high-dimensional feature matching (DINO/SAM) inject large, noisy losses that dominate optimization. With the limited model attention budget, these competing gradients dilute the task-relevant features and push the backbone toward suboptimal optima, producing the observed drop below the dashed baseline. 8 Table 3: Performance comparison between predicting the optical flow and dynamic region. Notably, the * denotes that this result is from [52]. Method Vanilla VLA* + dynamic region + depth + segment Task completed in row 1 93.0 97.6 98.3 98.2 82.4 92.6 94.3 94.6 3 72.3 87.5 88.5 89.5 4 62.6 80.4 82.0 83.4 53.3 73.7 77.2 78.1 Avg. Len. 3.64 4.32 4.40 4.44 Figure 6: CALVIN ABC-D performance with respect to different combinations of knowledge prediction. All=all of five models, and All-X=taking out of All. Next, we train the model with all five knowledge heads simultaneously (All) and perform an ablation study (All-X), where we remove one knowledge signal at time to evaluate its contribution. Removing leads to the most significant performance drop, confirming its essential role. Interestingly, removing DINO results in similar or even better performance, suggesting that not all semantic signals are equally helpful or stable in predicting outcomes, so we only use semantic features from SAM in the subsequent ablations. Table 3 reveals clear and decreasing return pattern in all ablations. Q2: Auxiliary Tasks vs. Future Knowledge Prediction: which drives improvement? Table 4 contrasts two training regimes: predicting complete world knowledge and performing auxiliary reconstructions, showing that the former is decisively superior. In our ablation, every prediction strategy is individually replaced by its reconstruction counterpart, yet each substitution consistently lowers performance: VLA trained only to redraw the current RGB, depth, semantics, or DINOv2 features can handle the first few actions but soon loses coherence, whereas network trained to forecast the next dynamic region, depth map, and semantics preserves accuracy throughout the trajectory and carries tasks much farther before failure. The reason is that prediction provides richer, action-oriented signal, directing learning toward the pixels that will drive the upcoming decision, while reconstruction merely revisits background detail that the control policy never actually needs. Q3: Why do we use the optical flow as the mask instead of directly forecasting it? To justify our choice of employing motion-centric dynamic regions over direct flow forecasting, we implement both variants under identical settings  (Table 5)  . In the optical flow setup, the model must predict the full future flow field along with the subgoal image, which significantly increases the training complexity. This extra burden manifests in markedly lower multi-step success rates. By contrast, our dynamic region approach merely employs the pretrained flow model to obtain binary mask, focusing the model on where relevant motion occurs, bringing significant improvement. Q4: The effectiveness of structured attention in DreamVLA. To demonstrate the effectiveness of our proposed structure attention mechanism in Figure 4, we swap it for vanilla causal mask while keeping everything else fixed. In this setting, every <dream> query, including the one meant to capture semantics, can also read the flow and depth tokens produced in the same step; the extra cross-peek mixes unrelated signals, adds gradient noise, and quickly degrades long-horizon control. Our mask removes all query-to-query edges, so <action> query consults only past language, state and multimodal predictions, never their siblings. Table 6 shows the payoff: the causal variant brings marginal improvement for Vanilla VLA, whereas the block-sparse version keeps success high throughout, confirming that blocking intra-step leakage is important. Q5: Can we use the shared query to predict the comprehensive world knowledge? 9 Table 4: Performance comparison between cotraining with auxiliary tasks and predicting the comprehensive world knowledge. Table 5: Performance comparison between predicting the optical flow and dynamic region. Method Task completed in row 2 3 4 5 Avg. Len. Method Task completed in row 1 2 3 4 Avg. Len. Auxiliary Prediction 97.7 98.2 92.3 94.6 85.6 89.5 79.5 83. 74.2 78.1 4.14 4.44 Optical 97.6 Dynamic 98.2 92.4 94.6 86.8 89.5 81.7 83. 75.4 78.1 4.23 4.44 Table 6: Performance comparison between vanilla causal and our structured attention. Table 7: Performance comparison between shared and seprated queries. Method Task completed in row Method Task completed in row 1 2 3 5 Avg. Len. 1 2 3 5 Avg. Len. Causal Structure 94.2 98.2 86.5 94.6 78.4 89. 71.3 83.4 62.7 78.1 3.75 4.44 Shared Separated 95.5 98.2 90.1 94. 83.8 89.5 76.9 83.4 70.4 78.1 4.17 4.44 Instead of assigning separate queries to dynamic region, depth, and semantics features, one might let single set of shared queries predict all signals. To test this idea, we split each world-embedding vector into four equal sub-spaces, with each quarter intended to carry different modality. Table 7 shows that the shared-query design hurts action performance: mixing modalities in the same query introduces cross-talk, so the diffusion head receives noisy features. In contrast, giving each modality its query keeps the representations disentangled and yields clear performance gain. Q6: Effect of the query count per modality inside <dream> queries. Table 8: Performance comparison between different numbers of <dream> queries. Each <dream> query contains three groups of elements: dynamic, depth, and semantics, each assigned queries. We vary {4, 9, 16} to examine its influence. When = 4, the limited capacity prevents the model from encoding fine-grained motion, geometry, and semantics, so accuracy drops even though memory usage is lowest. With = 9, each modality has sufficient bandwidth without overloading the backbone, yielding the best success rate and the longest uninterrupted task execution. Increasing to = 16 introduces redundant tokens that compete for attention and raise GPU memory, bringing no extra gain and slightly lower generalization. Task completed in row 75.1 78.1 73.9 80.7 83.4 81.0 86.4 89.5 86. 92.6 94.6 93.0 4.32 4.44 4.33 97.2 98.2 98.1 Avg. Len. Number 4 9 1 2 3"
        },
        {
            "title": "5 Limitation & Future Works",
            "content": "While DreamVLA demonstrates solid vision-language-action and achieves state-of-the-art performance on CALVIN [111], its current scope is still narrow: it practises mainly parallel-gripper manipulation, relies on RGB-centric data, and is trained on scenes with limited geometric and material diversity. We therefore plan to (i) add dexterous-hand demonstrations with rich contact annotations [116, 117], (ii) introduce 3D point clouds [118, 119, 96, 62, 120, 121, 61, 122] and spatial information [22, 123], tactileand fuse them into volumetric world states, and (iii) extend data collection and on-policy fine-tuning to bolster generalization and long-horizon robustness."
        },
        {
            "title": "6 Conclusion",
            "content": "We present DreamVLA, novel Visual-Language-Action framework that enables inverse dynamics modeling through comprehensive world knowledge prediction, supporting the perception-predictionaction loop for manipulation tasks. DreamVLA leverages dynamic-region-guided knowledge forecasting, combining spatial and semantic cues to generate compact and informative representations for action planning. We introduce block-wise structured-attention mechanism, coupled with diffusion-transformer decoder, to suppress representation noise from cross-type knowledge leakage and thus enable coherent multi-step action reasoning. Extensive experiments in both real and simulated environments demonstrate the effectiveness of DreamVLA, achieving 76.7% success rate on real-world robot tasks and outperforming prior methods on the CALVIN ABC-D benchmark."
        },
        {
            "title": "References",
            "content": "[1] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1, 3, 7, 8, 26 [2] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for real-world control at scale. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. 3, 26 [3] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. [4] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024. [5] Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, and Levine Sergey. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. 1, 3 [6] Kaifeng Zhang, Zhao-Heng Yin, Weirui Ye, and Yang Gao. Learning manipulation skills through robot chain-of-thought with sparse failure guidance. arXiv preprint arXiv:2405.13573, 2024. [7] Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, and Ping Luo. Robotwin: Dual-arm robot benchmark with generative digital twins (early version). In European Conference on Computer Vision, pages 264273. Springer, 2025. [8] Jiangran Lyu, Yuxing Chen, Tao Du, Feng Zhu, Huiquan Liu, Yizhou Wang, and He Wang. Scissorbot: Learning generalizable scissor skill for paper cutting via simulation, imitation, and sim2real. arXiv preprint arXiv:2409.13966, 2024. [9] Wenbo Cui, Chengyang Zhao, Songlin Wei, Jiazhao Zhang, Haoran Geng, Yaran Chen, Haoran Li, and He Wang. Gapartmanip: large-scale part-centric dataset for material-agnostic articulated object manipulation. arXiv preprint arXiv:2411.18276, 2024. [10] Jinghuan Shang, Karl Schmeckpeper, Brandon May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, and Laura Herlant. Theia: Distilling diverse vision foundation models for robot learning. arXiv preprint arXiv:2407.20179, 2024. [11] Jiawei He, Danshi Li, Xinqiang Yu, Zekun Qi, Wenyao Zhang, Jiayi Chen, Zhaoxiang Zhang, Zhizheng Zhang, Li Yi, and He Wang. Dexvlg: Dexterous vision-language-grasp model at scale. arXiv preprint arXiv:2507.02747, 2025. 1 [12] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 1, 3, [13] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 1, 3, 8 11 [14] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In The Twelfth International Conference on Learning Representations. 3, 7, 24 [15] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [16] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on robot learning, pages 894906. PMLR, 2022. 3 [17] Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint arXiv:2410.18647, 2024. [18] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [19] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024. 3 [20] Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. In Conference on Robot Learning (CoRL), 2024. [21] Jiangran Lyu, Ziming Li, Xuesong Shi, Chaoyi Xu, Yizhou Wang, and He Wang. Dywa: Dynamics-adaptive world action model for generalizable non-prehensile manipulation. arXiv preprint arXiv:2503.16806, 2025. 3 [22] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, et al. Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. arXiv preprint arXiv:2502.13143, 2025. 2, 10, 26 [23] Xialin He, Runpei Dong, Zixuan Chen, and Saurabh Gupta. Learning getting-up policies for real-world humanoid robots. arXiv preprint arXiv:2502.12152, 2025. [24] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. [25] Jiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, and He Wang. Gamma: Graspability-aware mobile manipulation policy learning based on online grasping pose fusion. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 13991405. IEEE, 2024. 1 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, [27] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint arXiv:2402.07865, 2024. [28] OpenAI. Gpt-4v(ision) system card, 2023. URL https://openai.com/research/ gpt-4v-system-card. 3 [29] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 1 12 [30] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. In The Twelfth International Conference on Learning Representations. 1, 3, 7, [31] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. In 8th Annual Conference on Robot Learning, 2024. [32] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 3 [33] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. 1, 3 [34] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. 3 [35] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025. 3 [36] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025. URL https://arxiv.org/abs/2501. 15830. [37] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. 3, 7, 27 [38] Moritz Reuss, Hongyi Zhou, Marcel Rühle, Ömer Erdinç Yagmurlu, Fabian Otto, and Rudolf Lioutikov. Flower: Democratizing generalist robot policies with efficient vision-languageaction flow policies. In 7th Robot Learning Workshop: Towards Robots with Human-Level Abilities. 1, 3 [39] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [40] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. 1, 3, 26 [41] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. In International Conference on Machine Learning, pages 3732137341. PMLR, 2024. [42] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, et al. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. In The Twelfth International Conference on Learning Representations. [43] Kaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, and Xiaodan Liang. Pivot-r: Primitive-driven waypoint-aware world model for robotic manipulation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [44] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. 3 13 [45] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. 3, 7 [46] Dongxiu Liu, Haoyi Niu, Zhihao Wang, Jinliang Zheng, Yinan Zheng, Zhonghong Ou, Jianming Hu, Jianxiong Li, and Xianyuan Zhan. Efficient robotic policy learning via latent space backward planning. arXiv preprint arXiv:2505.06861, 2025. [47] Kanchana Ranasinghe, Xiang Li, Cristina Mata, Jongwoo Park, and Michael Ryoo. Pixel motion as universal representation for robot control. arXiv preprint arXiv:2505.07817, 2025. 3 [48] Wenyan Yang, Ahmet Tikna, Yi Zhao, Yuying Zhang, Luigi Palopoli, Marco Roveri, and Joni Pajarinen. Symbolically-guided visual plan inference from uncurated video data. arXiv preprint arXiv:2505.08444, 2025. [49] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through neural trajectories. arXiv preprint arXiv:2505.12705, 2025. 3 [50] Yuhang Huang, JIazhao Zhang, Shilong Zou, XInwang Liu, Ruizhen Hu, and Kai Xu. Ladiwm: latent diffusion-based world model for predictive manipulation. arXiv preprint arXiv:2505.11528, 2025. [51] Jiange Yang, Haoyi Zhu, Yating Wang, Gangshan Wu, Tong He, and Limin Wang. Tra-moe: Learning trajectory prediction model from multiple domains for adaptive policy conditioning. ArXiv, abs/2411.14519, 2024. 1 [52] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. Int. Conf. Learn. Represent. (ICLR), 2024. 1, 3, 7, 8, 9, [53] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025. 7, 24 [54] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. arXiv preprint arXiv:2503.22020, 2025. 2, 3 [55] Yuyin Yang, Zetao Cai, Yang Tian, Jia Zeng, and Jiangmiao Pang. Gripper keypose and object pointflow as interfaces for bimanual robotic manipulation. arXiv preprint arXiv:2504.17784, 2025. [56] Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. arXiv preprint arXiv:2504.02792, 2025. [57] Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, and Donglin Wang. Reinbot: Amplifying robot visual-language manipulation with reinforcement learning. arXiv preprint arXiv:2505.07395, 2025. 1, 3 [58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022. 2, [59] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 2, 3, 6 [60] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37: 2187521911, 2024. 3, 6, 23 14 [61] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLIII, volume 15101 of Lecture Notes in Computer Science, pages 214238. Springer, 2024. 3, 10 [62] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In Int. Conf. Mach. Learn. (ICML), 2023. 2, 3, 5, 6, 10 [63] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. 2, 5, [64] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. 2, 5 [65] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024, 2024. 2, 3, 4, 6, 21, 23 [66] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 39924003. IEEE, 2023. 2, 4, 6, 21, 23 [67] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. 3 [68] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 3, 21, 26 [69] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based vlm plans the next step for vision-and-language navigation. Robotics: Science and Systems, 2024. [70] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: video-based vision-language-action model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224, 2024. 3 [71] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [72] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901, 2020. [73] Konstantinos I. Roumeliotis and Nikolaos D. Tselikas. Chatgpt and open-ai models: preliminary review. Future Internet, 15(6):192, 2023. [74] OpenAI. Openai o3 and o4-mini system card, 2025. URL https://openai.com/research/ o3-o4-mini-system-card. 3 [75] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In Int. Conf. Learn. Represent. (ICLR), 2024. 3, 4 15 [76] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. CoRR, abs/2406.16855, 2024. 3 [77] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. 3 [78] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. 7, 8, [79] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, et al. Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data. arXiv preprint arXiv:2505.03233, 2025. 3 [80] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer In Conference on Robot Learning, CoRL 2023, 6-9 web knowledge to robotic control. November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 21652183. PMLR, 2023. 3, 26 [81] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. RT-H: action hierarchies using language. CoRR, abs/2403.01823, 2024. 3, 26 [82] Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, et al. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631, 2025. 3 [83] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi0.5: visionlanguage-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [84] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [85] Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, et al. Hume: Introducing system-2 thinking in visual-languageaction model. arXiv preprint arXiv:2505.21432, 2025. 3 [86] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 3, 5, 7, 8 [87] Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist vision-language-action policy. arXiv preprint arXiv:2503.19757, 2025. [88] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 16 [89] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024. 7 [90] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy. arXiv e-prints, pages arXiv2403, 2024. 3 [91] Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, et al. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025. 3 [92] Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Yecheng Shao, and Renjing Xu. Reinforcement learning with generalizable gaussian splatting. 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 435441, 2024. URL https://api.semanticscholar.org/CorpusID:269042854. 3 [93] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [94] Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, and Xihui Liu. Moto: Latent motion token as the bridging language for robot manipulation. arXiv preprint arXiv:2412.04445, 2024. 3 [95] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 4, 21 [96] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In Int. Conf. Learn. Represent. (ICLR), 2023. 3, 10 [97] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 24552467, 2018. 3 [98] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. 4, 5, 21, 25, 26 [99] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 5, [100] Alex Graves. Practical variational inference for neural networks. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, Adv. Neural Inform. Process. Syst. (NIPS), 2011. 5 [101] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Int. Conf. Learn. Represent. (ICLR), 2014. [102] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Int. Conf. Mach. Learn. (ICML), 2021. [103] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In Int. Conf. Learn. Represent. (ICLR). OpenReview.net, 2022. [104] Jorma Rissanen. Modeling by shortest data description. Autom., 14(5):465471, 1978. 5 [105] Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In ACM Conf. Comput. Learn. Theory (COLT), 1993. 17 [106] Runpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, and Kaisheng Ma. Finding the task-optimal low-bit sub-distribution in deep neural networks. In Int. Conf. Mach. Learn. (ICML), 2022. 5 [107] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. [108] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 6 [109] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 7, 22 [110] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn. Represent. (ICLR), 2019. 7 [111] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. 7, 10, 23, 24 [112] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint arXiv:2310.10639, 2023. 7, [113] Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, and Yu Qiao. Towards synergistic, generalized, and efficient dual-system for robotic manipulation. arXiv preprint arXiv:2410.08001, 2024. 7 [114] Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, and Hongyang Li. Closed-loop visuomotor control with generative expectation for robotic manipulation. arXiv preprint arXiv:2409.09016, 2024. 7, 24 [115] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2505.06111, 2025. 7 [116] Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, and He Wang. Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 10 [117] Weikang Wan, Haoran Geng, Yun Liu, Zikang Shan, Yaodong Yang, Li Yi, and He Wang. Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning. In Int. Conf. Comput. Vis. (ICCV), 2023. 10 [118] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pages 7785, 2017. [119] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. In Adv. Neural Inform. Process. Syst. (NIPS), pages 50995108, 2017. 10 [120] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma. VPP: efficient conditional 3d generation via voxel-point progressive representation. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 10 [121] Shaochen Zhang, Zekun Qi, Runpei Dong, Xiuxiu Bai, and Xing Wei. Positional prompt tuning for efficient 3d representation learning. CoRR, abs/2408.11567, 2024. 10 18 [122] Guofan Fan, Zekun Qi, Wenkai Shi, and Kaisheng Ma. Point-gcc: Universal self-supervised 3d scene pre-training via geometry-color contrast. In Jianfei Cai, Mohan S. Kankanhalli, Balakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian, Liang Zheng, Vivek K. Singh, Pablo César, Lexing Xie, and Dong Xu, editors, Proceedings of the 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024, pages 47094718. ACM, 2024. [123] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. 10 [124] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. 21 [125] Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision-language models. CoRR, abs/2210.13431, 2022. 26 [126] Markus Grotz, Mohit Shridhar, Tamim Asfour, and Dieter Fox. Peract2: Benchmarking and learning for robotic bimanual manipulation tasks. CoRR, abs/2407.00278, 2024. [127] Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Selfsupervised skill abstractions for learning continuous control. CoRR, abs/2407.15840, 2024. 26 [128] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Manon Devin, Alex X. Lee, Maria Bauzá Villalonga, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Fernandes Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Zolna, Scott E. Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Thomas Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin A. Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and Nicolas Heess. Robocat: self-improving generalist agent for robotic manipulation. Trans. Mach. Learn. Res., 2024, 2024. 26 [129] Shizhe Chen, Ricardo Garcia Pinel, Cordelia Schmid, and Ivan Laptev. Polarnet: 3d point clouds for language-guided robotic manipulation. In Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 17611781. PMLR, 2023. 26 [130] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. CoRR, abs/2406.10721, 2024. 26 [131] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as can, not as say: Grounding language in robotic affordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287318. PMLR, 2022. 26 [132] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Inner monologue: Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Embodied reasoning through planning with language models. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 17691782. PMLR, 2022. 19 [133] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 94939500. IEEE, 2023. [134] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Annu. Conf. Robot. Learn. (CoRL), 2023. [135] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, and Brian Ichter. Grounded decoding: Guiding text generation with grounded models for embodied agents. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [136] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. CoRR, abs/2403.08248, 2024. [137] Kuan Fang, Fangchen Liu, Pieter Abbeel, and Sergey Levine. MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. [138] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Abdeslam Boularias, Peng Gao, and Hongsheng Li. A3VLM: actionable articulation-aware vision language model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 16751690. PMLR, 2024. 26 [139] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1806118070. IEEE, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 DreamVLA Architecture Text Encoder. We use the CLIP ViT-B/32 text encoder [95] to process natural language task instructions. The encoder transforms each instruction into fixed-length embedding that captures semantic intent. These embeddings are then projected into the shared latent space and used to condition the subsequent modules, enabling effective grounding of language into perception and action. Visual Encoder. We employ an MAE-pretrained ViT-B [98] as the vision encoder. At each timestep, images are captured from two views: eye-on-hand and eye-on-base. Each image is processed by the vision encoder to produce 196 latent vectors, which represent local patch information, along with [CLS] token that encodes the global representation of the image. Directly inputting all 197 tokens into the transformer backbone would create significant computational burden, particularly when processing long histories. Moreover, many image details are redundant for accomplishing manipulation tasks. To address this, we utilize the Perceiver Resampler [124] to condense the image representations and extract task-relevant features. The Perceiver Resampler employs learnable latent vectors with shape of (num latents, dim), where num latents is significantly smaller than the number of image tokens. Through Perceiver Attention, these latent vectors condense the input image features, along with the [CLS] token, to form the final image tokens. Robot State. The robot state consists of the arm and gripper state. The arm state includes the end-effector position and its rotation in Euler angles, resulting in six-dimensional representation. The gripper state is binary value indicating whether the gripper is open or closed. We tokenize the robot state using an MLP. Specifically, the gripper state is first converted into one-hot encoding. The one-hot encoding of the gripper state and the arm state are then each passed through separate linear layers. The outputs are concatenated and passed through final linear layer to produce the state token. Learnable Queries. We introduce two sets of learnable query tokens, denoted as <dream> and <action>, to extract and integrate information from multimodal inputs for joint prediction. The <dream> queries provide structured supervision through comprehensive knowledge prediction tasks and consist of 64 tokens in total, organized as 9 queries for each of the three modalities: dynamic motion, depth estimation and semantic features. These queries guide the model in reconstructing rich visual representations, enhancing the quality of the learned latent space. The <action> query is dedicated to action sequence prediction. Their length is determined by the temporal prediction horizon, as defined in the action chunking strategy from [68]. Large Language Model. We adopt GPT-2 Medium [99] as our language backbone. GPT-2 Medium is 24-layer, 16-head Transformer decoder with hidden size of 1,024 and total of approximately 345 million parameters. It was pretrained on the WebText corpus (8 million documents, 40 GB of text) using autoregressive language modeling to predict the next token with byte-pair encoding vocabulary of 50,257 tokens. Output Heads. To decode the world embedding into comprehensive world knowledge, we incorporate multiple task-specific output heads that predict dynamic motion regions, depth maps, and high-level semantics, including DINOv2 [65] and SAM-style segmentation features [66]. Each prediction head is implemented using lightweight Vision Transformer (ViT) decoder, which operates on two types of tokens produced by the multimodal backbone: the latent embeddings associated with specific modality and set of learnable mask tokens used for reconstruction. To retain spatial correspondence, we inject fixed sinecosine positional encodings into the token embeddings. These tokens are then processed through several Transformer encoder layers, followed by modality-specific linear projection head that maps each patch token to its output space, such as per-pixel depth values or semantic logitsthereby reconstructing the expected visual signals of future observations. Concrete details of each module are shown in Table 9. Table 9: The parameters of the each module in DreamVLA."
        },
        {
            "title": "Number of heads",
            "content": "image encoder perceiver resampler LLM image decoder depth decoder DINO decoder segment decoder 768 768 1024 1024 1024 1024 1024 12 3 24 2 2 2 2 12 8 16 16 16 16 16 Action Prediction with Diffusion Transformer To generate future actions conditioned on latent action embeddigns, we adopt diffusion-based Transformer architecture, DiT-B [109], as our action decoder. DiT enables flexible modeling of complex action distributions by progressively denoising sequence of latent action tokens through series of Transformer layers, allowing the model to capture multimodal uncertainty in robot control. We configure the DiT model with the base variant (DiT-B), using an action token embedding size equal to the hidden dimension of the fusion Transformer. The model predicts future actions, where each action is 7-dimensional vector that encodes the displacement of the pose and gripper state of the end effector. In our experiments, we set = 2, corresponding to 3-frame prediction window (current + 2 future steps). The model does not utilize past action context during generation (i.e., past window size is 0), focusing solely on predictive synthesis. During training, Gaussian noise is added to the future action trajectories, and the model learns to reverse this corruption process step by step. This module operates on top of the aggregated representation via <action> query, enabling temporally coherent and semantically grounded action generation. The concrete detail of DiT is shown in Table 10. Table 10: Configuration of the DiT-B model used for action prediction. Parameter Value Model type Token size Action prediction window Past context steps Number of Transformer layers Number of attention heads Positional encoding Diffusion timesteps (Train) Diffusion timesteps (Inference) Noise schedule Loss function Precision DiT-B 1024 2 future steps (3-frame chunk) 0 12 12 Learned (1D for time) 8 10 Linear Denoising Score Matching (L2 loss) float32 A.2 Feature Extraction To facilitate dynamic region prediction, we adopt motion-based heuristic to generate coarse binary masks that highlight regions of interest. Given sequence of consecutive RGB frames of resolution , we uniformly sample one keypoint every 8 pixels in both spatial dimensions, resulting in = H/8 W/8 sampled locations per frame. For each sampled location, we compute interframe displacements (x, y) by tracking its position across adjacent frames using CoTracker [63]. The magnitude of displacement is converted into scalar speed value: sij = (cid:113) (xij)2 + (yij)2, 22 where (i, j) denotes the spatial coordinates of each sampled patch. We then apply speed threshold τ (e.g., τ = 1 pixel/frame) to obtain binary motion mask. To account for small motions and ensure spatial connectivity, we perform single-pixel morphological dilation, expanding each positive location to its eight-connected neighbors. The resulting mask is flattened and reshaped into the form (B, 1, L), where = H/8 W/8 and is the batch size. We apply this binary mask element-wise to both predicted patch embeddings {ˆpi} and their corresponding ground-truth embeddings {pi} during loss computation, encouraging accurate representation in dynamic regions. For depth supervision, we use the ground-truth depth maps provided by datasets when available. In cases where depth annotations are not providedsuch as in certain real-world robot datasetswe use monocular depth estimators, specifically Depth-Anything v2 [60], to generate pseudo-ground-truth depth labels. In addition to depth and dynamic signals, we include high-level feature supervision. For DINOv2 [65], we extract features from the final transformer layer, capturing global semantic and structural representations. For SAM [66], we utilize the output of its image encoder as dense segmentation-aware features. These diverse modalities collectively provide comprehensive supervision signals to improve the quality and generalizability of our learned visual representations. A.3 Training Detail The total loss can be formulated as: = λdynLdyn + λdepthLdepth + λsemLsem + λDiTLDiT (10) where λdyn = 0.1, λdepth = 0.001, λsem = 0.1, λDiT = 1. We train DreamVLA on 8 NVIDIA A800 GPUs. The main bottleneck is the memory bandwidth to load large spatial feature tensors, for example, of 2566464 for SAM. We pre-compute the features from off-the-shelf models instead of conducting inference on the fly. This approach requires extra storage space to save all the features extracted from the above foundation models, but significantly saves on training time and avoids loading models with high GPU memory usage during training. All training configurations are listed in Table 11. Table 11: DreamVLA Training Configuration Hyperparameters Value # GPUs Batch size Learning rate (LR) LR Schedule Weight decay Optimizer Betas Epochs Warm-up epochs Warm-up LR schedule Linear (1e-2 * LR) 8 8 / GPU (64 effective) 1e-3 Constant 0.01 AdamW [0.9, 0.999]"
        },
        {
            "title": "B Experiments",
            "content": "B.1 Simulation Benchmark and Settings We evaluate DreamVLA on the CALVIN benchmark [111], simulated robotic manipulation suite designed for studying long-horizon, language-conditioned tasks. CALVIN aims to facilitate the development of agents that operate solely based on onboard sensor inputs and free-form human instructions, without access to privileged information or external supervision. The tasks in CALVIN require agents to execute long sequences of low-level control commands in response to complex language goals, reflecting realistic robotic interaction scenarios. 23 The benchmark includes four structurally similar but visually distinct environments, referred to as Env A, B, C, and D. Each environment features Franka Emika Panda arm with parallel gripper and tabletop workspace containing manipulable elements such as sliding door, drawer, and light button. The textures, object placements, and scene layouts vary across environments to encourage generalization and robustness. Observations consist of RGB images from both fixed and gripper-mounted cameras (resized to 224224), as well as low-dimensional robot state inputs that include the end-effectors position, orientation, and gripper status. The agent outputs 7-dimensional continuous action vector: 6 dimensions control the spatial displacement of the gripper, and the final dimension governs the open/close state of the gripper. The dataset contains approximately 2.4 million interaction steps and 40 million short-horizon action windows. Environments A, B, and provide language-free demonstrations for large-scale pretraining, while annotated instructions are available in subset of the data for downstream policy learning. We hold out Env for evaluation to assess zero-shot generalization to unseen combinations of instructions and environment variations. Following standard protocol [111, 52], we evaluate performance on set of 34 diverse tasks that include object pushing, placing, rotating, and other dexterous operations. In contrast to prior work, DreamVLA not only predicts actions conditioned on visual-language observations but also simultaneously learns to infer comprehensive future world knowledge, including depth maps, dynamic saliency regions, DINOv2 features, and SAM-based segmentation maps. This multi-task supervision enables richer scene understanding and improves policy generalization. We report success rate (SR) as our primary evaluation metric, measuring whether the instructed task was completed correctly based on the final state of the environment. B.2 Simulation Results We evaluate our approach on the CALVIN ABC-D benchmark, where training is conducted on environments A, B, and C, and testing is performed exclusively in Environment D. This evaluation setting poses strong challenge for generalization, as Environment features novel textures, object arrangements, and visual configurations not seen during training. As reported in Table 1 in the main manuscript, DreamVLA achieves superior performance across all tasks, substantially outperforming previous state-of-the-art methods. In particular, our model significantly outperforms two-stage inverse dynamics approaches such as Susie [112], demonstrating the effectiveness of our end-to-end architecture that unifies multimodal prediction and action generation. Compared to CLOVER [114], UP-VLA [53], Seer [52], which also incorporates visual foresight, DreamVLA benefits from more integrated design and joint optimization, resulting in consistently stronger execution accuracy. Furthermore, our method surpasses video generation-based pretraining approaches like GR-1 [14], highlighting the advantage of coupling visual world modeling with action planning in single framework. Notably, DreamVLA, achieves an average episode length of 4.44 on the ABC-D split, establishing new state-of-the-art on the CALVIN benchmark and validating the benefits of predicting future knowledge. The qualitative results as shown in Figure 7. B.3 Visualization As shown in Figure 8 and Figure 9, we visualize the models predictions of dynamic regions and depth maps. Although supervision is applied only to dynamic regions, DreamVLA is able to reconstruct semantically meaningful representations of the entire scene. This surprising generalization ability can be attributed to two factors. First, in long-horizon manipulation sequences, the robot arm is in constant motion and frequently interacts with various objects, causing most task-relevant regions to become dynamic at some point in time. This ensures that large portion of the scene is eventually observed under dynamic supervision. Second, although static regions are not explicitly supervised, the input frames inherently contain global visual contextincluding background structures, object appearances, and spatial layoutwhich the model can leverage to hallucinate and complete missing details. As result, DreamVLA implicitly learns to integrate temporal dynamics with static priors, leading to coherent and accurate predictions beyond the explicitly labeled regions. 24 Figure 7: Qualitative results of the CALVIN long horizon task. Although the predicted depth maps are relatively coarse due to the patch-level reconstruction inherent in MAE-style decoders [98], they still provide valuable guidance for downstream tasks. In particular, the model benefits from anticipating future depth, which helps refine action decisions and improves spatial awareness. 25 Figure 8: Visualization results of the dynamic region predictions. B.4 Real-world Settings In our real-world training setup, we use history length of 7, with the model jointly predicting the next 3 future visual representations and action steps. The visual backbone is initialized with ViT-B model pre-trained using MAE [98], and inference is accelerated using bfloat16 mixed-precision without any observed degradation in task performance. This configuration strikes balance between computational efficiency and policy stability in manipulation tasks. For pretraining, we leverage large-scale dataset such as DROID [78], which contains approximately 76,000 successful robot trajectories collected in diverse settings. For downstream adaptation, we fine-tune the model using 100 task-specific demonstrations for each task collected with SoFar [22]. As shown in Figure 10, we present the qualitative results of real-world experiments."
        },
        {
            "title": "C Additional Related Works",
            "content": "C.1 Language-Grounded Robot Manipulation Language-grounded robot Manipulation adopts the human language as general instruction interface. Existing works can be categorized into two groups: i) End-to-end models like RT-series [2, 80, 81] built upon unified cross-modal Transformers with tokenized actions [68, 125127, 30, 128], large vision-language-action (VLA) models built from VLMs [1], or 3D representations [129, 40, 130]. Training on robot data such as Open X-Embodiment [12] and DROID [78], remarkable process has been made. However, the data scale is still limited compared to in-the-wild data for training VLMs. ii) Decoupled high-level reasoning and low-level actions in large vision-language models and small off-the-shelf policy models, primitives [131137, 22], or articulated priors [138, 139]. 26 Figure 9: Visualization results of the depth maps."
        },
        {
            "title": "D Additional Discussions and Future Work",
            "content": "i. Scaling Laws. promising direction for future exploration involves investigating scaling behavior in DreamVLA. In particular, we plan to study how increasing the capacity of key componentssuch as the backbone visual encoder or the size of the language modelaffects model performance. This includes replacing the current text encoder with larger-scale language models (e.g., LLaMA-2 or GPT variants) to assess the impact of richer linguistic understanding on multimodal reasoning and action generation. ii. Integration with Additional Baselines. We also aim to evaluate DreamVLA in conjunction with more recent and diverse baselines. For example, RoboVLMs [37] incorporate wide range of visionlanguage backbones and offer unified framework for robotic policy learning. Combining DreamVLA with these baselines can help standardize performance comparisons and reveal architectural synergies between pretrained vision-language models and action-centric transformers. iii. Contribution of Multi-View Observations. Our current framework leverages both fixed and egocentric camera views. In future work, we plan to conduct detailed ablation study to quantify the contribution of each view modality to task performance. This analysis will provide insights into how multi-view information improves spatial reasoning and robustness, especially in occluded or ambiguous scenarios. iv. Extension to More Complex and Long-Horizon Tasks. While DreamVLA demonstrates strong performance on the CALVIN benchmark, we are interested in extending the framework to more complex, long-horizon tasks that involve extended temporal dependencies, delayed rewards, and multi-stage subgoals. This includes evaluating on benchmarks that require sustained interaction, sequential tool use, or high-level planning. Addressing these challenges will require not only more 27 Figure 10: Qualitative results of real world language-grounded manipulation. powerful temporal modeling but also better integration of memory, goal abstraction, and hierarchical reasoning mechanisms. v. Application to Robotic Navigation and Humanoid. Beyond tabletop manipulation, DreamVLA could be adapted to robot navigation tasks in indoor or semi-structured environments. By learning to predict dynamic regions, obstacles, and semantic scene components, the model could support instruction-driven navigation and path planning under multimodal supervision, especially in settings where map-based planning is infeasible. Furthermore, another compelling extension is applying DreamVLA to humanoid robots, which require reasoning over whole-body motion, balance, and physically grounded interactions. The modularity of our framework allows for integration with additional proprioceptive inputs and more complex action spaces. This line of work would explore how multimodal predictive learning can scale to full-body motor control and human-like task execution."
        },
        {
            "title": "E Broader Impacts",
            "content": "DreamVLA proposes new training paradigm for vision-language-action (VLA) modeling, going beyond the conventional mapping from visual observations and language to actions. Instead of directly predicting actions from high-dimensional input, our framework first encourages the model to predict comprehensive world knowledge, including depth, dynamic motion, segmentation, and semantic features, before generating actions. This intermediate representation improves action grounding and generalization. 28 key strength of DreamVLA lies in its simplicity and efficiency: by adding only lightweight decoder and set of learnable queries, we significantly enhance the performance of existing VLA backbones with minimal parameter overhead. This makes the method both scalable and compatible with current VLM-based architectures, paving the way for more robust and transferable policies. Practically, this design can benefit the development of assistive robots navigation and humanoid robots, where it is essential for agents to generalize across novel environments and language goals. Furthermore, since our method leverages unlabeled perceptual signals during training, it reduces reliance on curated language-instruction datasets, which are often expensive and domain-specific. Overall, DreamVLA offers practical, extensible, and training-efficient framework for improving VLA systems, and we hope it inspires further research into multimodal abstraction and low-cost robot learning."
        }
    ],
    "affiliations": [
        "EIT",
        "Galbot",
        "PKU",
        "SJTU",
        "THU",
        "UIUC",
        "USTC"
    ]
}