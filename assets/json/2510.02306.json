{
    "paper_title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation",
    "authors": [
        "Raphael Tang",
        "Crystina Zhang",
        "Wenyan Li",
        "Carmen Lai",
        "Pontus Stenetorp",
        "Yao Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the \"battle\" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 0 3 2 0 . 0 1 5 2 : r Drawing Conclusions from Draws: Rethinking Draw Semantics in Arena-Style LLM Evaluation Raphael Tang1 Crystina Zhang2 Wenyan Li3 Carmen Lai4 Pontus Stenetorp1,5 Yao Lu1 1Centre for Artificial Intelligence, University College London 2University of Waterloo 3University of Copenhagen 4Independent Researcher 5Research and Development Center for Large Language Models, National Institute of Informatics"
        },
        {
            "title": "Abstract",
            "content": "In arena-style evaluation of large language models (LLMs), two LLMs respond to user query, and the user chooses the winning response or deems the battle draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields 13% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates."
        },
        {
            "title": "Introduction",
            "content": "In arena-style evaluation, as popularized by Chatbot Arena (Chiang et al., 2024), users issue arbitrary queries to two large language models (LLMs) and judge their responses, either choosing the winner or declaring the battle draw. The battles are treated as two-player zero-sum games like chess, where wins, losses, and draws respectively indicate outperformance, underperformance, and equal ability. Naturally, most applications model these rating dynamics using the Elo rating system (Elo, 1978) and its numerous derivatives (Glickman and Jones, 2024): wins increase models rating at the expense of the opposing model, and draws equalize the ratings of the two models. In this paper, we critically examine this twoplayer game paradigm, specifically questioning whether draws genuinely mean skill parity and thus whether ratings should be equalized. Instead, we conjecture that draws mostly predict query difficulty and subjectivity. If the query is too easy, both models are more likely to succeed equally. Likewise, if the query is objective as opposed to subjective, the likelihood for both models to arrive at the same answer increases as well. In short, we hypothesize that draws relate more strongly to query properties rather than equality of model ability. We validate our hypothesis with two main experiments: First, we evaluate the effectiveness of established rating systems when rating updates If draws are uninformafor draws are ignored. tive, there should be no difference in rating quality. Across four rating systems and three real-world datasets, we find that ignoring draw updates increases battle prediction accuracy by relative 1 3%, despite the evaluation still including draws. The improvement was consistently present for 11 of the 12 datasetrating system combinations. Second, we investigate how query difficulty, subjectivity, and model ratings relate to the probability of observing draw. We show that queries with difficulty and subjectivity ratings of 0 out of 5 are associated with 3537% increased relative risk of observing draw, whereas rating proximity has no substantial connection to draw probability. In summary, our main contributions are (1) to our knowledge, we are the first to demonstrate that draws largely do not indicate model parity in arenastyle evaluation, and (2) we provide insights into draw semantics, finding that query difficulty and subjectivity are better predictors of draw likelihood than model rating closeness. Our work suggests the reconsideration of draw semantics in arena-style evaluation and the inclusion of query properties in rating updates. We release our codebase at https: //github.com/daemon/lmarena-draws."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "Arena-style evaluation comprises two stages: user judgement elicitation and model rating updates. First, users interact with pair of anonymous LLMs and provide judgements, either picking the better response or declaring them to be the samesee Figure 4 in Appendix for an example user interface. Next, the system updates the two models ratings based on the judgement, with the winning model receiving points at the expense of the losing model. If the battle is draw, then the rating system equalizes the two ratings, subtracting from the higher rating and adding to the lower one. Formally, let be finite set of models and be the set {1, 0, 1 2 } denoting win, loss, and draw, respectively. Then let (cid:0)(mai, mbi, yi)(cid:1)n i=1 be the time-ordered sequence of battles, where mai, mbi denote the two models and yi is the judgement of mai relative to mbi, e.g., 1 means mai won against mbi. The rating system initializes each model indexed by with rating r(j) 1 R, which is updated after each battle by the systems update rule (cid:0)r(ai) i+1 , r(bi) i+1 (cid:1) := (cid:16) r(ai) , r(bi) , yi (cid:17) , (1) where : (cid:55) takes two model ratings at timestep and the battle outcome yi to produce two updated ratings for the next timestep. At timestep + 1 for all r(j) where is neither ai nor bi, the rating is unchanged, i.e., r(j) . i+1 := r(j) i"
        },
        {
            "title": "2.2 Rating Systems",
            "content": "Online score-based rating systems primarily vary in their update rules . In this paper, we consider four established rating systems: Elo (Elo, 1978), popular in competitive chess; Glicko-2 (Glickman, 2012), an alternative model for chess; online BradleyTerry, as implemented by Chatbot Arena (Chiang et al., 2023); and TrueSkill (Herbrich et al., 2006), Bayesian system originally designed for matchmaking on Xbox Live. Elo. Elo proposes logistic model for the expected probabilities Eai of mai or mbi winning: (mbi (mai ) ) (cid:17) (cid:16) Eai := 1/ 1 + 10 , Ebi := 1 Eai , (2) 400 then uses an update rule with learning rate (the K-factor) given the actual observed outcome: (mai ) r(ai) i+1 := (mbi + K(yi Eai ), + K((1 yi) Ebi ). r(bi) i+1 := (4) (3) ) Glicko-2. Glickman (2012) is another logistic model that additionally tracks the rating deviation (RD) ϕ(j) . Let the weighting function be g(ϕ(j) expected win probability and volatility σ(j) ) := (1 + 3ϕ(j) /π2) 1 2 and the 2 i Eai := 1/(cid:0)1 + exp(g(ϕ(bi) Let the variance vi := (cid:0)g(ϕ(bi) and the delta be := vig(ϕ(bi) update rule for σ(ai) i ))(cid:1). )(r(ai) r(bi) (5) )2Eai(1 Eai)(cid:1)1 )(yi Eai). The i+1 is then the root of vi ex) ex(2 ϕ(ai) 2(ϕ(ai) + vi + ex)2 2 ln σ(ai) τ 2 , (6) h(x) := 2 + where τ > 0 is constant for volatility change. := (cid:0)1/(ϕ(ai) The RD is updated as ϕ(ai) i+1 i+1 ) + 1/v(cid:1) 1 σ(ai) i+1 := r(ai) 2 and the rating as r(ai) + ϕ(ai) g(ϕ(bi) )(yi Eai). The update rules for mbi i+1 proceed analogously with yi (cid:55) 1 yi. Intuitively, Glicko-2 scales the size of its updates with the level of uncertainty and volatility. BradleyTerry. Chatbot Arena adopts an online BradleyTerry model (Bradley and Terry, 1952) over Elo due to its greater stability (Chiang et al., 2023). The update rule for yi {0, 1} is i+1 := r(ai) r(ai) + η(yi Eai), (7) i r(ai) where η > 0 is the learning rate and Eai := 1/(cid:0)1 + exp(r(bi) )(cid:1) is the probability of mai winning against mbi. The other model rating r(bi) is updated similarly with yi flipped. For draws, i.e., yi = 1/2, Chatbot Arena (Chiang et al., 2023) performs simultaneous win and loss update, effectively reducing the gap between the two ratings. TrueSkill. Herbrich et al. (2006) introduce Bayesian rating system that treats ratings as Gaus2 sian priors (r(j) ) in factor graph. Each (s(j) battle draws performance p(j) , β2), where s(j) ), and the probability of mai winning against mbi is modeled as the truncated Gaussian over the performance difference: (r(j) , σ(j) , σ(j) 2 (cid:32) Eai := 1 Φ (cid:113) ε (r(ai) r(bi) ) 2β2 + σ(ai) 2 + σ(bi) (cid:33) , 2 (8) where Φ is the Gaussian CDF and ε > 0 is the draw margin. The hard evidence (likelihood) is the outcome yi, and the new rating posterior (r(ai) i+1 , σ(ai) ) is computed using full Bayesian update i+1 with message passing over the factor graph; see Herbrich et al. (2006) for closed-form equations. 2 Method LMArena SearchArena VisionArena Acc. WL-Acc. Acc. WL-Acc. Acc. WL-Acc. % Elo random omission 36.88 (+0.2%) 57.30 (+0.3%) 43.77 (-0.3%) 60.59 (0.0%) -0.1% w/o draw updates 38.15(+3.7%) 58.32(+2.1%) 45.03(+2.5%) 61.85(+1.9%) 45.07(+5.3%) 67.18(+2.5%) +3.0% 42.80 42.73 (-0.2%) 65.24 (-0.5%) 65.57 60. 57.12 36.79 43.94 Glicko-2 40.45 random omission 40.41 (0.0%) w/o draw updates 40.87(+1.0%) 61.85(+0.9%) 47.91(+2.0%) 65.37 (0.0%) 61.26 61.35 (+0.1%) 46.93 (0.0%) 46.95 46.88 65.34 65.17 (-0.3%) 46.86 (0.0%) -0.1% 47.03 (+0.3%) 69.74 (+0.1%) +0.7% 69.61 69.47 (-0.2%) BradleyTerry random omission 40.44 (0.0%) -0.1% w/o draw updates 40.98(+1.3%) 61.30(+0.7%) 47.29(+2.2%) 65.06(+0.6%) 47.46(+1.1%) 69.88(+0.5%) +1.1% 60.85 60.58 (-0.4%) 46.23 (-0.1%) 64.62 (0.0%) 46.96 46.95 (0.0%) 69.53 69.58 (0.0%) 64. 40.44 46.28 TrueSkill 61.52 random omission 40.83 (0.0%) 61.61 (+0.1%) 46.85 (0.0%) w/o draw updates 41.04(+0.6%) 62.01(+0.7%) 46.90 (0.0%) 40.81 46. 47.17 64.69 65.07 (+0.6%) 47.20 (0.0%) 0.0% 65.37(+1.1%) 47.45 (+0.6%) 69.74 (-0.3%) +0.5% 69.95 69.60 (-0.5%) Table 1: Prequential battle outcome prediction accuracy under various experimental treatments, where Acc. denotes the overall accuracy and WL-Acc. the winloss accuracy if we disallow draws. The relative changes of each ablation with respect to the baselines are in parentheses, and % is the global average. Best results are bolded. One-sided statistical significance at the 95% level (p < 0.05) according to McNemars test (McNemar, 1947)."
        },
        {
            "title": "3.1 Draw Update Ablation Study",
            "content": "We selected three open datasets of real-world LLM battles curated from Chatbot Arena: LMArena, SearchArena, and VisionArena. LMArena (Tang et al., 2025) consists of 106K battles from users chatting with 55 text-only LLMs, ranging from LLaMA 3.1-405B (Grattafiori et al., 2024) to GPT4o. SearchArena (Miroyan et al., 2025) comprises 24K battles of 13 LLM-driven agents for information access, such as GPT-4o-search. VisionArena (Chou et al., 2025) has 30K public battles among 17 visionlanguage models (VLMs), e.g., LLaVA (Liu et al., 2023). Roughly 3040% of each dataset are draws, with the remainder split evenly between wins and losses. To evaluate rating systems, we followed Herbrich et al. (2006) and measured prequential battle prediction accuracy: we iterated through the battles chronologically, predicting the outcome from the current ratings before updating them. Concretely, (cid:80)n I(ˆyi = yi), where the prewe computed 1 diction ˆyi depends only on r(ai) (and any state at i). TrueSkill predicts draws naturally using the draw margin ε; for Elo, Glicko-2, and BradleyTerry, we introduced draw margin ε in the decision rule: and r(bi) i=1 ˆy = 0 1 2 1 if Ebi Eai > ε, if Eai Ebi ε, if Eai Ebi > ε, (9) which can be tuned like any other hyperparameter. Setup. We first evaluated the impact of omitting rating updates for draws. For each dataset, we set aside the first 5% as the calibration set and the remaining 95% as the validation set. We tuned the draw margin ε on the calibration set, sweeping it in the interval [0.05, 0.45] with step size of 0.05. We then used the best ε from including draw updates for all experiments within each methoddataset combination, as separately tuning it for ignoring draw updates may lead to unfair bias in favor of ignoring updates. As baseline, to remove fewer updates as potential confounder, we also omitted both draws and winloss updates randomly at rate equal to the number of draws in each dataset. Results. As shown in Table 1, ignoring draw updates improves outcome prediction accuracy by relative 0.53.0% on average for all four rating systems, with median overall and winloss accuracy improvements of 1.2% and 0.7%, respectively. These gains are statistically significant in 18 of 23 cases. The effect on Elo is most prominent (+3.0%), followed by BradleyTerry (+1.1%), Glicko-2 (+0.7%) and TrueSkill (+0.5%), possibly because Elo does not model uncertainty. With its net-zero change, the random omission ablations also demonstrate that the effect cannot be explained by merely using less data. Performancewise, Glicko-2, BradleyTerry, and TrueSkill are evenly matched, with median overall accuracy range of 0.42 absolute points (see VisionArenas 47.0347.46). This contrasts with Elo, which lags behind the other systems by median 3.6 points. Figure 1: The risk ratio of observing draw compared to win or loss, binned by difficulty and subjectivity. Error bars denote 95% confidence intervals. Figure 2: The risk ratio of observing draw as function of the absolute difference in model ratings, binned by percentile range. foreseeable concern is that disregarding draw updates may benefit winloss accuracy but hurt draw prediction accuracy, while still increasing aggregate accuracy. To address this, we sweep ε and plot the resulting winloss-to-draw accuracy curves. As Figure 3 in Appendix confirms, ignoring draws improves draw prediction accuracy at all operating points, i.e., it is Pareto-better."
        },
        {
            "title": "3.2 Draw Semantics Study",
            "content": "Setup. To assess the effect of query difficulty and subjectivity on draws, we sampled 3,000 battles from LMArena and labeled the querys difficulty and subjectivity on scale from 05 using GPT-4.1. Then, we binned all the outcomes by rating and computed the risk ratio (RR) of observing draw versus win or loss. RRs above 1.0 represent higher likelihood of draws and below 1.0 the opposite. For all 106K battles, we also collected the absolute difference in the model ratings and whether draw occurred, then binned them by the difference percentile and computed the RR. Results. Figure 1 shows that draws are indeed more likely for queries with difficulty and subjectivity ratings of 0, which reach respective RRs of 1.37 and 1.35. This likely follows from very easy queries being broadly answerable by any LLM and highly objective queries having an exact match. Other ratings are not significantly different from an RR of 1.0, except for highly subjective queries rated as 5 more likely to result in win or loss, possibly due to creative tasks eliciting stronger feedback. Next, Figure 2 presents the draw RR as binned function of the rating difference. If lower percentiles have high RRs, then that suggests rating closeness is predictive of draws. This was not the case, since all RRs are close to 1.0 until the 90100th percentiles, which only slightly differs at RRs of 0.890.96, further affirming our central hypotheses."
        },
        {
            "title": "4 Related Work",
            "content": "Although Chatbot Arena popularized anonymous head-to-head battles for LLMs (Chiang et al., 2024), ordinal comparisons of LLM responses originated with InstructGPT (Ouyang et al., 2022), the direct predecessor to ChatGPT. Recent work has probed pitfalls of pairwise judging, such as position bias (Shi et al., 2024), test contamination (White et al., 2024), and misalignment with real-world utility (Miller and Tang, 2025). Large-scale benchmarks such as BIG-Bench similarly emphasize broad coverage and systematic evaluation across diverse array of tasks (Srivastava et al., 2023). Other studies critically analyze the robustness of arena-style evaluation, focusing on the Elo rating system. Boubdir et al. (2023) show that Elo can violate desirable axioms such as reliability and transitivity; Liu et al. (2025) address these issues with am-ELO, maximum-likelihood reformulation that jointly models annotator reliability. Wu and Aji (2025) find that single Elo scores overweight stylistic fluency over factual correctness, motivating the Multi-Elo Rating System (MERS). Our work complements this body of literature by questioning draw semantics in arena evaluation."
        },
        {
            "title": "5 Conclusions",
            "content": "In this work, we questioned the standard assumption that draws in arena-style LLM evaluation indicate model parity. Across three real-world datasets, ignoring draw updates improved outcome prediction accuracy by 13%, despite draws comprising 3040% of battles. Our analysis further showed that draws are disproportionately associated with very easy and highly objective queries (risk ratios of 1.37 and 1.35), suggesting they stem more from query properties. For future rating systems, we recommend reconsidering what draws mean and to explicitly account for query properties."
        },
        {
            "title": "References",
            "content": "Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. 2023. Elo uncovered: Robustness and best practices in language model evaluation. In Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM). Ralph A. Bradley and Milton E. Terry. 1952. Rank analysis of incomplete block designs. Biometrika. Wei-Lin Chiang, Tim Li, Joseph E. Gonzalez, and Ion Stoica. 2023. Chatbot Arena: New models & Elo system update. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, et al. 2024. Chatbot Arena: An open platform for evaluating LLMs by human preference. In ICML. Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, Ion Stoica, Joseph E. Gonzalez, and Wei-Lin Chiang. 2025. VisionArena: 230K real world user-VLM conversations with preference labels. In CVPR. Arpad E. Elo. 1978. The Rating of Chessplayers, Past and Present. Mark E. Glickman. 2012. Example of the Glicko-2 system. Boston University. 2025. am-ELO: stable framework for arena-based LLM evaluation. In ICML. Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika. Justin K. Miller and Wenjia Tang. 2025. Evaluating LLM metrics through real-world capabilities. arXiv:2505.08253. Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, et al. 2025. Search Arena: Analyzing search-augmented LLMs. arXiv:2506.05334. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. NeurIPS. Lin Shi, Chiyu Ma, Wenhua Liang, Xingjian Diao, Weicheng Ma, and Soroush Vosoughi. 2024. Judging the judges: systematic study of position bias in LLM-as-a-judge. arXiv:2406.07791. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. TMLR. Mark E. Glickman and Albyn C. Jones. 2024. Models and rating systems for head-to-head competition. Annual Review of Statistics and Its Application. Kelly Tang, Wei-Lin Chiang, and Anastasios N. Angelopoulos. 2025. Arena Explorer: topic modeling pipeline for LLM evals & analytics. Andrea Grattafiori, Joshua Ainslie, Shruti Bhosale, and et al. 2024. The LLaMA 3 herd of models. arXiv:2407.21783. Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill: Bayesian skill rating system. NeurIPS. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. NeurIPS. Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, and Shijin Wang. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, et al. 2024. LiveBench: challenging, contamination-limited LLM benchmark. arXiv:2406.19314. Minghao Wu and Alham Fikri Aji. 2025. Style over substance: Evaluation biases for large language models. In COLING. Figure 3: Trade-off curves between draw and win/loss prediction accuracy as we vary the draw margin. Larger draw margins result in better draw prediction accuracy at the expense of win/loss accuracy, and vice versa. Curves with higher maxima and AUC are better. Figure 4: An example user interface from https://lmarena.ai."
        },
        {
            "title": "B Example User Interface",
            "content": "In Figure 3, we vary the draw threshold ε and plot the trade-off curves between draw and winloss prediction accuracy. Ignoring draw updates (orange line) attains higher AUC and is Pareto-better than including everything. We present an example user interface of arena-style evaluation in Figure 4. Users can choose left is better, right is better, or draw."
        }
    ],
    "affiliations": [
        "Centre for Artificial Intelligence, University College London",
        "Independent Researcher",
        "Research and Development Center for Large Language Models, National Institute of Informatics",
        "University of Copenhagen",
        "University of Waterloo"
    ]
}