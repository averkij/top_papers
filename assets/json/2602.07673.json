{
    "paper_title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation",
    "authors": [
        "Jiangnan Fang",
        "Cheng-Tse Liu",
        "Hanieh Deilamsalehy",
        "Nesreen K. Ahmed",
        "Puneet Mathur",
        "Nedim Lipka",
        "Franck Dernoncourt",
        "Ryan A. Rossi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 ] . [ 1 3 7 6 7 0 . 2 0 6 2 : r Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation Jiangnan Fang1, Cheng-Tse Liu2, Hanieh Deilamsalehy3, Nesreen K. Ahmed4, Puneet Mathur5, Nedim Lipka6, Franck Dernoncourt7, Ryan A. Rossi8 1,2Independent Researchers, 3,5,6,7,8Adobe Research, 4Cisco Research {1jfang53, 2cliu282}@ucsc.edu Abstract Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at more granular level in relation to well-defined overlap metric. In this work we provide an LLM judge bias analysis as function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond simple comparison. Keywords: LLMs, summarization As large language models (LLMs) continue to improve in their capabilities, LLM-as-a-judge has emerged as method of automating evaluation. Compared to traditional overlap-based metrics, LLM judges better capture semantic content of texts and are more robust to paraphrasing. As models can leverage reasoning capabilities gained from training, they also enable evaluation that dont rely on reference texts that can be expensive to obtain (Freitag et al., 2024). However, bias remains an issue in the LLM-as-a-judge paradigm. Biases of judge LLMs reveal particular tendencies learned through LLMs training, and these tendencies displayed for one task (position bias, for example) extend to other areas and input domains (Tian et al., 2025) especially for zero-shot tasks as they rely on the internal knowledge of model. Understanding these bias patterns is crucial in evaluating LLM performance and informing future training or design practices. While previous work has assessed how well LM decisions correlate with human judgments (Goyal et al., 2023; Zhu et al., 2025), including at different levels of quality as rated by humans (Shen et al., 2023), the present work aims to look at correlations in much more granular level in terms of levels of overlap as measured by n-gram metrics. In this work, we study the following research questions: (1) How does similarity as measured by n-gram overlap metrics (e.g. ROUGE, BLEU) correlate with LLM judgments, and is such judgment vs. similarity pattern observed for models of different sizes and types? (2) How does presentation order (position bias) interact with the judgment vs. similarity pattern? After extensive experimentation, we make the following contributions and findings: We collect benchmark dataset containing 6,744 LLM summaries of filtered subset of WikiSum and CNN_DailyMail datasets and over 94,000 LLM judgments between human and machine-generated summaries. LLM judges preference toward their own answers is (1) more prominent when generated summaries have fewer n-gram overlaps with the human-written summaries, (2) exists even towards summaries by small models, and (3) large difference is needed for such preference to show. (See Figure 4 where the proportion of generated choices grows towards the left of individual results). Position bias is more prominent when the generated summaries are more like human-written summaries. In addition, models with more parameters tend to prefer the last-presented summaries while models with fewer parameters prefer the first-presented summaries. We note that the bias for generated summaries as described above persists for variously sized models we have tested, regardless of types of position bias. (See Figure 5 for further breakdowns for each evaluator and generator model.) We conduct large-scale systematic study consisting of 9 LLMs, with parameter counts ranging from 1 billion to 12 billion, of judge LLM bias as function vs. the degree of overlap between machine and human summaries. Read the following section of long document and write concise summary that captures its main points and key details in around 100 words. only and nothing else. Output the summary text 1. Related Works [original text] Recent studies in LLM judges include various evaluation datasets, frameworks, and prompting methods. Zhu et al. (2025) built dataset for fine-tuning LLM judges and proposes scalable framework for open-ended LLM judging tasks. Kumar et al. (2024) developed benchmark dataset for evaluating personalized long text generation. Hashemi et al. (2024) develops weighing framework for combining LLM responses to items in rubric, and G-Eval in Liu et al. (2023) uses combination of chain-of-thought prompting and form-filling to improve judge LLMs alignment with human preferences in summarization and text generation. LLM judges exhibit content-level biases, i.e. they are biased towards textual content not related to the assigned evaluation task, or they ignore textual content that are related to evaluation. Chen et al. (2024) shows that LLM judges may overlook factual errors, and show gender, authority, and beauty biases. Fu et al. (2023) has similarly found that LLMs may struggle with evaluating factual information in summarizations. Judge LLMs can even be swayed by short phrases like informative, or solution: injected into evaluated texts (Raina et al., 2024; Zhao et al., 2025). Patterns of biases towards other aspects of the texts, e.g. authorship and length, are also found in both judging outputs of different language models and between human and model outputs. Laurito et al. (2024) found that LLMs often favor outputs of other LLMs over human outputs, and between different AI output texts. Hu et al. (2024) found that longer generated responses are preferred by LLM judges than shorter responses. Panickssery et al. (2024) and Wataoka et al. (2024) found that LLMs also recognize and favor texts produced by the same model over other models. (i.e. GPT 4o-mini favor outputs of GPT 4o-mini over that of other LMs). Self-favoritism and position bias are also reported by Zheng et al. (2023) and Li et al. (2024), with the former suggesting several mitigation strategies for position bias including swapping operation and few-shot prompting and the latter proposing split-and-merge approach to align semantically similar sections of evaluated texts. Large model evaluators bias towards other models extends to other domains like images as well (Taesiri et al., 2025). Figure 1: Prompt used for the LLMs to generate initial summaries to be evaluated Given the original text below, along with indexed summaries, please evaluate the summaries and output the name of the best summary. Output the exact name only and nothing else. original text: [original text] summary_1 [first summary text] summary_2 [second summary text] Figure 2: Prompt used for the LLMs to generate initial summaries to be evaluated. 2. Methodology 2.1. Experimental Setup We use test sets from WikiSum (Cohen et al., 2021) and CNN_DailyMail (See et al., 2017; Hermann et al., 2015), covering diverse range of topics for summarization. Both datasets contain 2,000 articles and their human written summaries in the test set. We use Phi-4-mini-instruct (Microsoft, 2025), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), GPT-4o mini (OpenAI, 2024), and variants of Gemma (Gemma Team, 2025) and LLaMA (Grattafiori et al., 2024), covering parameter counts from 1 billion to 12 billion. The models are decoderonly transformers. summary of tested models can be found in Table 1. During experiments, model temperatures are left at 0.7. For an LLM-generated summary, we use the average of BLEU-1, BLEU4, ROUGE-1, and ROUGE-2 to score its similarity to the human summary. These four metrics cover recalland precision-oriented scores and range of n-gram lengths to capture keyword and short phrases matches. We first obtain LLM summaries (generated summaries) using the prompt in Figure 1, where original text is the article in each of the datasets. We then prompt an evaluator model to judge pairs Model name # of parameters Context length Summarizer Evaluator google/gemma-3-1B-it google/gemma-3-4B-it google/gemma-3-12B-it meta-llama/Llama-3.2-3B-Instruct meta-llama/Meta-Llama-3-8B-Instruct microsoft/Phi-4-mini-instruct mistralai/Mistral-7B-Instruct-v0.3 Qwen/Qwen3-8B GPT-4o mini 1B 4.3B 12.2B 3.21B 8.03B 3.84B 7.25B 8.19B 8B? 32k 128k 128k 128k 8k 128k 8k 32k 128k Table 1: Models used in the current work. Model names are Huggingface repo_ids (excluding GPT-4o mini). Summarizer means the model was used to produce generated summaries; Evaluator means the model was used to evaluate summaries. GPT-4o minis parameter count is unknown; however, Abacha et al. (2025) estimates 8 billion, citing Zeff (2024). We couldnt confirm this figure. of summaries given the original texts using the prompt format in Figure 2. Here, LLM-generated and human-generated ground truth summaries are assigned to either [first summary text] or [second summary text] as described later in Section 2.2. We keep no conversation history with LLMs so that they are not influenced by past summarizations or evaluations. While our instruction to the judge LLMs is to only respond with the name of the summary, models occasionally return answers that quote their choices. We perform string matching to recover some judgments from these texts. 2.2. Controlling for Length and Order"
        },
        {
            "title": "Bias",
            "content": "ROUGE and BLEU scores are affected by the length difference between reference and input texts, as longer input texts can simply include duplicates of segments in the reference text to inflate the scores. At the same time, LLM judges can be biased towards longer texts as discussed in Section 1. To reduce the effect of length bias, we control for summary length by filtering each dataset so that the reference (human-generated) summaries are between 95 and 105 space-delimited words long. Words are counted with space delimiters instead of using particular tokenization algorithm because each model may tokenize texts differently. In the prompt in Figure 1, we instruct LLMs to output 100 words to match lengths of ground truth summaries. After filtering, the CNN_DailyMail dataset contains 286 articles and the WikiSum dataset contains 276 articles. To reduce the effects of ordering bias and selfpreferential bias as mentioned in Section 1, we perform evaluations with summaries presented in both orders. The evaluator LLM choices are accepted if they are consistent when summaries are presented in both orders and mark choices as tied if they are different depending on summary order. Tied choices are further broken down by their ordering preference, i.e. if model chooses the first summary for both orders, the tie is marked as tiedchose-first, and if it chooses the last summary for both orders, the tie is marked as tied-chose-last. We provide visual representation of these categories in Figure 3. Figure 3: Visual representation of evaluator choice labels. GT means the evaluator chooses the ground truth summary in both orders; Generated means the evaluator chooses the LLM-generated summary in both orders. Tied-chose-first means the evaluator chooses the first presented summary in both orders, and Tied-chose-last means the evaluator chooses the last presented summary in both orders. 2.3. Extending the Range of Similarity"
        },
        {
            "title": "Scores",
            "content": "After obtaining summaries and scoring their similarity with human summaries, we observed that the generated summaries had limited range for the averaged score, namely below 0.55. To get fuller picture with wider range of similarity scores, we obtain additional LLM summaries by submitting ground truth summaries and prompting models to rephrase and reorganize them, keeping longer expressions and segments intact (see Figure 6 for prompt). Since summaries with more long phrase overlaps would score higher for ROUGE and BLEU, the additional summaries extend the range of scores for similarity metrics. These summaries are treated as input summaries and passed to evaluator LLMs as described in Section 2.1, i.e. the evaluator prompt does not reveal that these summaries are rephrased from ground-truth summaries. Rephrase and reorganize this text in your own style, but retain as many long phrases in it as possible. Keep to the same length. Output your rephrased text and nothing else. [ground truth summary] Figure 6: Prompt used for the LLMs to generate additional, higher-scoring summaries. 3. Results & Discussion Since model choices are discrete and only consist of 4 categories (excluding other), we present the results in histograms to better visualize model preferences. For each graph in Figures 4 and 5, we show the proportion of model choices for each bin of values on the x-axis. RQ1: How do LLM judgments correlate with ngram-based similarity metrics, and is that consistent across models of different sizes? We first note that for all models, the human summary is rarely chosen as the better summary regardless of similarity to human summaries. For all models excluding variants and the smallest Gemma3-1B-it we observe pattern that AI-AI bias (i.e. LLM tending to choose generated summaries over human summaries) is more prominent when generated summaries are less like the human-written summaries, i.e. fewer n-gram overlaps; see Figure 4 where the proportion of generated choices is larger towards the left of the subfigures. We see this tendency for LLM judges even when judging summaries produced by smaller models. For example, the preference patterns for the 12B-parameter Gemma 3 judge and the 8B Mistral judge are similar for summaries produced by the 1B Gemma 3 (see Figure 5), even though the position biases of the judges are different, namely that Gemma 3 (12B) tend to choose last-presented summary while Mistral (7B) prefer the first-presented summary. It is worth noting that for most of the models tested the bias towards generated summaries diminish well before the average scores approaches 1. For example, Mistrals (8B) preference frequency for LLM summaries drops below 25% for mean scores above 0.5. In other words, significant non-overlap is required for the bias towards generated summaries to show. RQ2: How does position bias interact with the judgment vs. similarity pattern? We observe that position bias is more prominent when the generated summaries are more like human-written summaries. In Figure 4 this is represented by larger proportion of tied summaries towards the right of each subfigure. Models with more parameters tend to prefer the last-presented summaries while models with fewer parameters prefer the first-presented summaries. We note that for variously sized models we have tested, even though models may have different patterns of position bias, the pattern for the preference of generated summaries persists. We find that LLM preferences towards generated summaries are similar for models with the same architecture above certain parameter count, as exemplified in the two very different sizes of LLaMA models and the two biggest Gemma models tested. 4. Conclusion In this work we have investigated the relationship between the human-machine text similarities and judge LLMs preference, specifically for the summarization task. We find that LLMs prefer generated summaries, and this preference is more prominent when generated and human summaries are less similar as measured by overlap metrics. This preference extends to summaries generated by smaller models, e.g. 1 billion parameters. We also find that this preference exists regardless of the type of position bias (preferring the first or last summary in both orders) present in that model. LLM bias towards not just their own but other LLMs over human texts points to possible stylistic marker in LLM output text that is present even with varied training techniques and training data, which could be useful in contexts like LLM detection but unproductive in scenarios like LLM automatic evaluations. At the same time, judging bias displayed by this diverse set of models also could reveal areas for improvement in future LLM training and LLM-as-a-judge frameworks. 5. Limitations The main limitation of this study is the scope for the independent variables for LLM bias. We have Figure 4: Proportion of documents where the evaluator chooses ground truth (GT), generated summaries, and when the evaluator chose first, chose last regardless of order, plotted against the score of the non-ground truth summaries. See Figure 3 for visual representation of evaluator choices. The score is the mean of ROUGE-1, ROUGE-2, BLEU-1, and BLEU-4. Here the generators (summarizers) are Gemma 3, Phi 4 mini, Mistral, Llama 3, and GPT-4o mini (i.e. no Qwen 3). For Llama 3 8B and Mistral, additional summaries are generated with different prompts to ascertain possible patterns in higher-scored summaries. Further breakdowns for the variants Llama and Gemma can be found in Figure 7. Figure 5: Alternative version of Figure 4, where each row of the grid are the results with the same summarizer instead. exclusively focused on judge LLM bias vs. degree of short phrases overlap metrics as crude approximation for similarity with human-generated texts. Future studies could benefit from investigating many more potential metrics as the independent variable. Additionally, while we have used 9 models to generate summaries for evaluation, only one reference text was used to test judge LLM bias and to calculate overlap metrics. Future research may obtain multiple diverse human-written summaries for more robust results for LLM bias patterns. In controlling for summary length, we have limited the findings to the subset of the datasets where human summaries are between 95 and 105 spacedelimited words, and further work can expand on this range for both human and machine generated summaries. Finally, We have not considered adversarial examples, which may disrupt bias patterns. Figure 7: Alternative version of Figure 4, showing breakdowns of variants of Llama and Gemma. 6. References Gemma Team. 2025. Gemma 3 technical report. Asma Ben Abacha, Wen wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and Thomas Lin. 2025. Medec: benchmark for medical error detection and correction in clinical notes. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024. Humans or LLMs as the judge? study on judgement In Proceedings of the 2024 Conference bias. on Empirical Methods in Natural Language Processing, pages 83018327, Miami, Florida, USA. Association for Computational Linguistics. Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and Alessandro Moschitti. 2021. WikiSum: Coherent summarization dataset for efficient humanevaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 212219, Online. Association for Computational Linguistics. Markus Freitag, Nitika Mathur, Daniel Deutsch, ChiKiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and Alon Lavie. 2024. Are LLMs breaking MT metrics? results of the WMT24 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 4781, Miami, Florida, USA. Association for Computational Linguistics. Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, and Shashi Bhushan Tn. 2023. Are large language models reliable judges? study on the factuality evaluation capabilities of LLMs. In Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 310316, Singapore. Association for Computational Linguistics. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, and Alex Vaughan et al. 2024. The llama 3 herd of models. Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. 2024. LLMrubric: multidimensional, calibrated approach to automated evaluation of natural language texts. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1380613834, Bangkok, Thailand. Association for Computational Linguistics. Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NIPS, pages 16931701. Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Tianfu Wang, Zhengyu Chen, Nicholas Jing Yuan, Jianxun Lian, Kaize Ding, and Hui Xiong. 2024. Explaining length bias in llm-based preference evaluations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. 2024. Longlamp: benchmark for personalized long-form text generation. Bui, and Anh Totti Nguyen. 2025. Understanding generative ai capabilities in everyday image editing tasks. Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. 2025. Identifying and mitigating position bias of multi-image vision-language models. Koki Wataoka, Tsubasa Takahashi, and Ryokan Ri. 2024. Self-preference bias in llm-as-a-judge. Maxwell Zeff. 2024. Openai unveils gpt-4o mini, smaller and cheaper ai model. Accessed: 202506-10. Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, and Dong Yu. 2025. One token to fool llm-asa-judge. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mtbench and chatbot arena. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2025. Judgelm: Fine-tuned large language models are scalable judges. Walter Laurito, Benjamin Davis, Peli Grietzer, Tomáš Gavenčiak, Ada Böhm, and Jan Kulveit. 2024. Ai ai bias: Large language models favor their own generated content. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2024. Split and merge: Aligning position biases in LLM-based evaluators. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1108411108, Miami, Florida, USA. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Microsoft. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. OpenAI. 2024. Gpt-4 technical report. Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. Vyas Raina, Adian Liusie, and Mark Gales. 2024. Is LLM-as-a-judge robust? investigating universal adversarial attacks on zero-shot LLM assessment. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 74997517, Miami, Florida, USA. Association for Computational Linguistics. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computational Linguistics. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023. Large language models are not yet human-level evaluators for abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 42154233, Singapore. Association for Computational Linguistics. Mohammad Reza Taesiri, Brandon Collins, Logan Bolton, Viet Dac Lai, Franck Dernoncourt, Trung"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Cisco Research",
        "Independent Researchers"
    ]
}