{
    "paper_title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "authors": [
        "Shresth Grover",
        "Priyank Pathak",
        "Akash Kumar",
        "Vibhav Vineet",
        "Yogesh S Rawat"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA."
        },
        {
            "title": "Start",
            "content": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates Shresth Grover1, Priyank Pathak2, Akash Kumar3, Vibhav Vineet4, Yogesh Rawat5 1University of California San Diego, 2,3,5University of Central Florida, 4Microsoft Research 1shgrover@ucsd.edu, {2ppriyank, 3akash.kumar, 5yogesh}@ucf.edu, 4vibhav.vineet@microsoft.com 5 2 0 2 1 1 ] . [ 1 2 4 3 0 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, visionbased sequential planning tasks across 4 domains: maze navigation, block re-arrangement, image reconstruction, and object re-organization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an avg. performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as PlanBench and VQA. Code and dataset will be made public at https://github.com/shroglck/CosPlan. 1. Introduction Vision-Language Models (VLMs) [1, 6] demonstrate strong zero-shot generalization across diverse tasks, increasingly integrating into complex workflows [15, 37]. This raises key question: How well can VLMs handle practical decision-making? e.g. robotics, autonomous navigation etc. particularly challenging scenario is Sequential Planning [23, 24, 38], where models must execute series of actions to reach goal. Realistically, as the number of instrucFigure 1. Corrective Sequential Planning: Given the initial and final states, with already performed actions w/ some errors (initial context), model identifies errors in the provided context, and picks the optimal action steps to reach the final goal, correcting the error. tions increases, errors become more likely  (Fig. 1)  . Hence, detecting errors and course-correcting towards the goal is essential for assessing VLMs robustness against errors. in erroneous Existing work on sequential planning has largely focused on the text domain [26, 37], with limited exploration in the vision domain [32]. Moreover, these works assume ideal conditions, with perfect instructions [30], limiting their applicability in the physical world. However, handling of errors in such sequential planning tasks is vastly unexplored. Motivated by this, we introduce CoSPlan (Corrective Sequence Planning), benchmark designed to study VLMs planning capabilities scenarios. CoSPlan focuses on 2D spatial vision tasks guided by text-based instructions, requiring models to plan sequence of actions toward goal (temporal), while detecting and correcting an erroneous action. CoSPlan introduces error to simulate realistic challenges and evaluates VLMs on two key abilities: Error Detection (identify the error in the initial context (already performed actions)) and Step Completion (reach the final goal while correcting the error). CoSPlan includes four diverse tasks: 1) Maze-E: Navigation in 2D maze 2) Blocks-World-E: Re-arranging colored blocks 3) Shuffle-E: Reconstructing shuffled image tiles 4) Robo-VQA-E: Re-organizing real-world objects. 1 We evaluate five leading VLMs on our benchmark, namely GPT-4o [1], CoG-VLM [42], InternVLM-26B [7], Janus-pro-7B [6], and Qwen2 VL-8B [41]. On average, the zero-shot performance of these models is close to random guessing. Hence we opt for two popular techniques, known for complex visual reasoning: (i) Chain-of-Thought (CoT) [43], which guides VLM through intermediate reasoning steps, and (ii) Scene Graph [5], which provides structured representations of objects by modeling their attributes and relationships. This setup enables us to assess our benchmarks complexity and validate its effectiveness. While Scene Graph structured representations perform well in error-free sequence planning tasks, our evaluation shows that they struggle in error-prone ones. Specifically, they fail to internalize the sequence of steps and miss contextual cues needed to reach the goal. This is likely because solving corrective sequential planning in single step, from the initial state to the final goal, is inherently difficult. VLMs lack representations of intermediate states and struggle with tracking the evolution of scenes across multiple actions. Addressing this, we propose SGI (Scene Graph Incremental updates), novel training-free method that refines Scene Graphs step-by-step for each action, generating intermediate states. SGI significantly enhances VLMs capability to i) handle long instruction sequences, ii) track evolving scenes, iii) detect and correct errors to reach the final goal, making corrective sequence planning more robust. In summary, we make the following contributions: i) CoSPlan (Corrective Sequence Planning) is the first to reveal limitations of VLMs in handling error-prone sequence planning, with temporal sequences of actions in vision + language domain. CoSPlan includes four diverse planning tasks to test the abilities of Error Detection and Step Completion to reach desired goal. ii) We benchmark VLMs like GPT-4o, exposing their vulnerabilities in handling errors, sequence planning in the vision domain, and lack of context understanding, among other insights. iii) We propose SGI, Scene Graph Incremental update technique, that refines structured representations step-by-step for every action, enhancing robustness in CoSPlan and traditional datasets like VQA [40], and Plancbench [37]. 2. Related work Reasoning in Foundation Models Enhancing Reasoning without fine-tuning [8] is of great interest in LLM / VLMs. Wei et al. introduced the Chain-of-Thought (COT) via series of intermediate reasoning steps to guide LLMs toward the final answer. CoT has been shown to greatly improve performance in reasoning tasks like mathematical problems [34], but in visual reasoning, it does not account for spatial relationships. [28] explores planning using GPT. Chen et al. proposed using structured representations (Scene Graphs) of objects, scenes, and relationships, to improve VLMs Table 1. Previous Reasoning Benchmarks: Existing works are mostly text only. For modality: is vision, is Text. Temporal means change in scene, and synthetic means source of tasks. Benchmark Modality Temporal Synthetic Error ALFWorld [33] (2020) PlanBench [37] (2022) WebArena [50] (2023) SpatialEval [40] (2024) CoSPlan (Ours) V+T + complex reasoning abilities on tasks like VQA [11], visual grounding [5], image generation [19], spatial reasoning [21], etc. However, SG faces challenges in erroneous sequential planning and multi-step reasoning. Our Scene Graph Incremental updates (SGI) approach relies on pair of images, and an instruction set to update the scene graphs, unlike [27], which uses ground-truth 3D scene graphs. Sequential Planning Valmeekam et al. proposed variations of Sequence Planning, including completing steps based on partial context. Most sequential planning datasets [2, 22, 30, 31, 45] assume ideal instructions, which may not hold outside the lab environment. Many rely on human / videobased supervision [4, 10, 47], limiting scalability. Most benchmarks focus on textual planning [3, 26, 44, 49], with limited exploration in the vision domain [9, 32, 36] (Tab. 1). SpatialEval [40] only evaluates static images. Recent works have also examined VLMs for planning [14, 16, 20, 36, 46]. CoSPlan is the first benchmark to evaluate VLMs on sequential planning under vision-language, and temporal domain, under erroneous instructions. 3. CoSPlan Benchmark Corrective Sequence Planning (CoSPlan) mimics general decision-making by evaluating the models ability to navigate complex challenge of detecting and correcting non-optimal (error) steps in sequential planning task. In this setup, Model progresses from an initial state I0 to goal state Ig through sequence of actions: (A1, A2, ...AN ). We introduce an intentional non-optimal (error) action AE within the initial context (already performed actions) (A1, A2, ..AE ..Ak<N ). The model must detect this erroneous action AE and course-correct to complete the remaining actions steps (Ak+1, Ak+2, ...AN ) towards the final goal. Mathematically, it can be shown as CoSPlan M(A1,..E,..k; I0; Ig) Initial State: I0 Goal: Ig Performed actions: A1,..E,..k Error Detection IdentifyAE . Step Complete Ak+1,k+2,..N (1) This setup is used to solve diverse scenarios, such as reconstructing correct image from shuffled image tiles, re2 Table 2. CoSPlan Dataset Details: Initial Context Length is the avg. number of actions already performed, and Remaining Step Length are avg. additional steps required to reach the goal after the initial context. The Source is where images (or text) are taken from. Dataset Task Type Navigation Maze-E Blocks-World-E Re-arrangement Re-construction Shuffle-E Re-organization Robo-VQA-E Path Planning Blocks Puzzle Real-world # Test Samples (Images & Text) 5000 5000 1000 350 Initial Context Length (Avg.) 2.0 2.0 3.7 5. Remaining Step Length (Avg.) 4.6 3.8 7.1 4.1 Source Synthetic Synthetic ImageNet (2009) ROM (2024) (a) Maze-E (Navigating): denotes movement. (b) Blocks-World-E (Re-arrangement): from (a) (b) indicates move box # from column (a) to column (b) (c) Shuffle-E (Re-construction): indicates patch swap (d) Robo-VQA (Re-organization): Real World Figure 2. Overview of CoSPlan Benchmark: Given initial (Is) and final state (Ie) and initial set of instructions (orange), the model needs to perform two tasks: Step completion, choosing right set of future paths (green) to complete the task, and Error detection the sub-optimal / erroneous action in past actions (initial context). Shown coordinates (row, column) are 0-indexed. Initial steps visualized as black arrows, and safeguard against cheating (Section 3.2) highlighted in pink for Blocks-World-E. Shuffle-E sub-optimal errors are computationally infeasible, hence ignored (Sec. 3.1). Rest all errors (red) violate the rules of the environment or unnecessary (sub-optimal). arranging & re-organizing the objects / blocks into coherent order (obeying physics), and navigating through maze. Success relies on addressing and resolving non-optimal (errors) steps encountered along the way  (Fig. 2)  . Sec. 3.1 describes each dataset with proposed planning tasks. Whats an Error? We loosely define error as plausible but suboptimal action that deviates from the optimal path to the goal, potentially resulting in longer sequences. Error can also be purely wrong action that makes it impossible to reach the goal without correction, e.g. referencing nonexistent objects, violating task/physics constraints etc. 3.1. Benchmark Datasets We introduce four sequential planning datasets, each featuring diverse tasks with intentional sub-optimal errors (except Shuffle-E) posing unique challenges in corrective sequence planning. The datasets are structured as multiplechoice questions that test: i) Error Detection: Identifying the non-optimal erroneous action from initial context (already performed actions), or selecting none of the above. ii) Step Completion: Selecting the correct answer among 5 options that would correct the mistake and lead to the final goal. The use of synthetic datasets [25, 40] has been shown to test reasoning vulnerabilities in VLMs. overview is provided in Tab. 2 & Fig. 2, respectively. Maze-E (Fig. 2a): The goal is to solve maze while navigating from the start cell to the goal cell. Inputs is maze layout with starting position (green, I0), destination position (blue Ig), and sequence of initial moves with error like moving into red dead-end cell, detours, diagonal move, moving out of maze etc. requiring backtraction. Dataset is constructed via randomly sampled grid of size 3 Figure 3. Error Correction Initial context (orange arrows) with error move AE (diagonal (1, 0) (2, 1)) to cell. Step completion to Ig w/ error correction (yellow) is correct while w/o (pink) is not. [33, 88], and up to 5 obstacles,susing OpenCV [17]1. Blocks-World-E (Fig. 2b): The goal is to stack blocks in specific (target) configuration. Inputs is the initial block arrangement (I0) and the final arrangement (Ig), initial sequence for stacking with an erroneous (sub-optimal) step to mislead the model. Suboptimal errors involve inefficient stacking, placing blocks in impossible positions (e.g. air), moving inner blocks (not on top), etc. The dataset is generated using OpenCV by rendering start and end configurations with 38 boxes randomly placed across columns. Shuffle-E (Fig. 2c): The goal is to restore shuffled scene to its original order, via swapping image patches (tiles). Input is starting shuffled image (I0), the final restored image (Ig), and the initial sequence of image patch swaps. Step completion is correct sequence of swaps to generate the restored image. Error Detection is skipped here, as one initial erroneous swap will cascade into subsequent incorrect swaps, breaking the one-error assumption applied to other datasets (limitations Sec. 5). ImageNet [12] sampled uniformly from each class totaling 1000 images were used. Robo-VQA-E (Fig. 2d): The goal is to manipulate / reorganize the objects via robot, collected in the real-word scenarios. The dataset (restructured ROM [31]) consists of 350 image pairs, curated by us (humans). Inputs include starting (I0) image, final image (Ig), and sequence of initial actions for object placements. suboptimal error might involve unnecessarily picking of objects, arbitrarily placements, manipulate an out-of-scene object, invalid action (e.g. open already open door), etc. Goal is the remaining sequence of object placements to get final organization. 3.2. Safeguard against Cheating Revealing the final image (Ig) to the model can help cheat the planning task by simply picking the option that best describes Ig, without looking at intial context or identifying the error. To prevent this leakage, we include an incorrect option that is identical to the correct one but omits the error correction step. For example, in Fig. 2b Step Completion, the green (2) and pink options (3) both match Ig, but only 1Black & white pattern helps distinguish cells/navigate. Figure 4. CoT for Maze-E: Detailed description in Sec. 3.5.1 the green option (2) has the error correction step of moving block 4 from 2nd column to 1st column. Similarly, in Fig. 3, the Step Completion that reverts the error (#2 yellow arrow) is the solution compared to the other option (#1 pink arrow) that reaches the target but doesnt correct the error. 3.3. Sequence Completion Design CoSPlan design choice for error correction within step completion mimics general scenarios where agents must detect and recover from errors in ongoing sequences, while completing the task. Alternatives like separating tasks into (i) explicit error correction and (ii) continuation from valid state assume the error-free steps for reaching the goal, which may not reflect practical decision-making. Instead, our correct option may begin from the erroneous state but proposes recovery sequence that leads to the goal, without additional errors. Similarly, incorrect options may perpetuate the error or introduce new ones. 3.4. Evaluation CoSPlan is evaluated using multiple-choice question (MCQ) framework (shown in Fig. 2), with Top-1(%) accuracy as the evaluation metric. We independently evaluate Step Completion: the model chooses one correct option among 5 options (random accuracy 1 5 ), and Error Detection: MCQ setup presents initial context actions as choices, with an additional option of none of the above denoting no error present (random accuracy E[ 1 Initial context length+1 ]). 3.5. Models & Techniques We follow the OpenVLM leaderboard (Huggingface) for selecting Vision-Language Models (VLMs) for our corrective sequential planning tasks. We incorporate both closed (GPT-4o [1]) and open-source (e.g. CoG-VLM [42], InternVLM-26B [7], Qwen2 VL-8B [41], Janus-pro7B [6]). Since GPT-4o is not open-source and requires payment per use, we have used it judiciously for selected experiments. More details about each model in Supplementary. We next describe how Chain-of-Thought (CoT) and Scene Graphs (SG) are used to further improve these VLMs. 4 Table 3. CoSPlan benchmark: Evaluation described in Section 3.4. Higher number () implies better performance. is Vanilla VLM (no CoT and SG modification), CoT is Chain-of-Thought, and SG is Scene Graphs (both initial and final states are input). indicates GPT-4o vanilla wasnt evaluated because of its inferior performance compared CoT and SG in other VLMs, and its paid (budget constraint). Step Completion (% ) VLM Robo-VQA-E CoT SG 20 Shuffle-E CoT SG Random 20 Qwen2 VL-8B 17.1 17.6 18.9 24.1 24.9 25.1 26.5 27.9 28.3 18.1 18.6 18.8 13.1 12.5 21.5 23.1 27.1 23.7 25.1 25.9 26.5 25.5 25.2 26.7 CoG-VLM 14.1 14.7 21.3 23.2 23.1 23.5 20.4 20.2 21.7 24.2 23.1 25.1 Janus-pro-7B 22.1 23.5 25.1 20.1 23.2 23.4 21.6 35.8 41.2 18.3 21.2 18.9 Intern-VLM GPT-4o 49.7 54. Blocks-World-E CoT SG 20 Maze-E CoT SG 20 48.2 52.2 27.6 30.1 45.6 46.1 - - - - Error Detection (% ) Maze-E CoT SG 26.1 Blocks-World-E CoT SG 26.1 Robo-VQA-E CoT SG 25.4 20.5 20.8 20.7 32.3 30.6 35.2 9.2 9.6 9.1 32.1 33.4 35.3 13.3 41.3 43.1 44.5 6.4 8.4 17.5 18.1 26.1 20.5 19.1 21.0 29.3 31.0 27.6 24.3 25.2 26.1 32.8 33.1 33.4 36.5 37.9 37.3 35.1 42. 45.3 44.2 40.3 35.3 - - - lations) via unified prompts, rejecting invalid formats. To ensure fairness across models, identical SG schemas and prompts were enforced across models, with strict JSON validation for outputs. Cross-model comparisons thus focus on task performance under consistent structures, despite inherent differences, e.g. GPT-4o vs. Qwen2 VL-8B verbosity. 3.6. Results & Analysis Vanilla vs CoT vs SG: Table 3 compares VLMs on CoSPlan benchmark, via Vanilla method (raw image-text input) and via enhanced reasoning (CoT and Scene Graph (SG)). CoT improves performance on vanilla models, and SG provides additional gains (with few exceptions), underscoring the value of structured representations for corrective sequence planning. GPT-4o makes relatively informed reasoning decisions, while Janus-pro-7B, CoG-VLM, and Qwen2 VL-8B perform near or below random chance, indicating the difficulty of the task. Task difficulty follows: Shuffle-E > Maze-E > Robo-VQA-E >Blocks-World-E. Accuracy less than random can partially be explained by overwhelmingly picking certain options [48] (Janus selects option 94% times) or options without error correction (cheating, Sec. 3.2). Exploring VLMs pattern for answering MCQ is left as future work. Error Detection provably is substantially harder than Step Completion, likely because step completion can leak answers while error detection requires deeper understanding of context and task. Higher performance on Blocks-World-E error detection might be due to data leakage, since variants of Blocks-World are widely used in pretraining these VLMs. Complexity of Errors: Figure 6a shows VLMs perform relatively well in error-free settings (GPT-4o near-perfect accuracy) but struggle when errors are introduced, revealing VLMs dependency on ideal, error-free settings. These highlight the need for challenging, error-aware benchmarks like CoSPlan to highlight the vulnerable gap between training and practical error-prone scenarios (CoG-VLM and Janus-pro-7B predict randomly under errors). Effect of Context: CoSPlan includes random mix of two types of errors: i) In-Context Suboptimal erroneous step Figure 5. Scene Graph for Robo-VQA-E SG generated via GPT4o, with objects as nodes, location as edge, state as attributes. 3.5.1. Baseline Reasoning Chain-of-Thought (CoT [43]) We adapt CoT for our CoSPlan datasets by i) Identify: Providing models with detailed description of the problem and constraints; ii) Context: Step-by-step description of each action in the initial context; iii) Verify: Ask model to plan path to reach goal while verifying it follows all constraints. An example CoT for the Maze-E is shown in Fig. 4. This approach is modelagnostic, and the same set of instructions is provided to all models. Detailed examples in supplementary. Scene Graphs (SG [43]) In our work, we use the Scene Graph (SG) in addition to CoT as added context to aid VLM reasoning abilities. Given an initial state (I0) and goal state (Ig), we QUERY (prompt) VLM to Construct scene graphs for the initial and goal states, capturing key objects, attributes, spatial relationships, and target configurations. The VLMs then generate state graph consisting of three key components: a) Nodes: Objects present in the scene, b) Edges: Relationships (e.g. spatial position) between objects c) Attributes: Object properties and interactions. An example SG for the Robo-VQA-E is shown in Fig. 5 (detailed examples in supplementary). Comparison Different VLM have different structural representations, hence SGs are very model-specific. We have standardized attributes (e.g. nodes for objects, edges for re5 (a) Effect of error in context (b) Out vs In-context errors (c) Multi-modal vs Text-only Figure 6. (a) VLMs excel in error-free settings, highlighting the complexity of error-prone ones. (b) Errors from within context (scene) are harder than random ones (out-context). (c) VLMs struggle on visual reasoning; however perform exceptionally well on text-only domain. Figure 7. All tasks (a,b,c) shown for Step completion, with (a) using Scene Graph (SG), and (b,c) using Chain-of-Though (CoT). a) Effect of # MCQ options As the number of options , Intern-VLM accuracy starts to drop. b) Information from MCQ options With 9 remaining steps, VLMs take next steps toward the goal. Constant accuracy indicates VLMs ignore additional context from MCQ. c) Length of Initial Context VLM accuracy shows positive correlation with context length, i.e. as # of already performed steps , accuracy . that involves objects present in the scene, ii) Out-Context Error uses random objects not in scene. Figure 6b shows lower performance on In-Context errors, suggesting VLMs struggle more when erroneous suboptimal actions involve plausible objects from within the scene while they can handle the out-of-context errors with relative ease. Necessity of Vision modality: Figure 6c shows that transforming multi-modal tasks (vision + text) into text-only formats significantly boosts model reasoning, validating recent works [18, 29]. Qwen2s near-random prediction in BlocksWorld-E reveals its limitations for CoSPlan, while other tasks like Robo-VQA-E and Shuffle-E cannot be faithfully represented as text-only without visual aid. This exposes the VLMs vulnerability in visual reasoning, and the need for sequence planning benchmark in the visual domain. MCQ Options: Fig. 7(a) shows that increasing the number of options (the default is 5 for step completion), exposes the randomness in picking options, as the accuracy drops with the number of options (first reported by [13]). This reveals the added complexity of MCQ in CoSPlan. Alternative to MCQ (plan generation) is left as future work. Ignoring additional context: We input constant context of length 2 (1 initial step and 1 error), and evaluate step completion, where the models need to take 9 steps to reach the goal Ig (inclusive of 1 error correction). Fig. 7(b) evaluates step completion where only the next steps towards Ig are available, e.g. = 2 means MCQ options will show only the next 2 steps (wont reach Ig). = 9 would have options where Ig is reached. This helps us to measure if models use additional available context ( k) in MCQ options to reach goal Ig. All models (including GPT-4o) maintain stable accuracy regardless of k, indicating additional information via MCQ options (# of the remaining steps or k) does not influence the reasoning to reach the goal. This can be explained by models preference towards certain options (e.g. Janus selects option 94% times) and models strong dependency on initial context (only two steps provided here) to make reasonable predictions. Initial Context importance Fig. 7(c) shows that as the length of the initial context goes up, model accuracy goes up, signifying the importance of already performed actions in sequence planning. We hypothesize that models may not understand context (Fig. 7(b)), but seeing more steps likely help weed out erroneous step. We use this as the motivation for our novel SGI (Scene Graph Incremental update) technique to maximize information gain from this context. 4. Scene Graph Incremental update (SGI) Tab. 3 shows that Scene Graphs (and CoT) enhance VLMs performance on sequential planning tasks. However, with only initial and final states, the model is forced to internally interpolate intermediate steps. This is because SG and CoT both attempt to encapsulate the entire task transformation within single-step graph, making it challenging to coherently capture the intermediate states (and transitions) across sequential actions. This places heavy burden on the models ability to simulate long action sequences, something VLMs struggle with. Addressing this, we propose Figure 8. SGI 1) Initial and Goal Scene Graphs (SG) are generated. 2) Incremental Scene Update sequentially modifies SG for each action Ai 3) Similarity Comparison matches the resultant SG with Goal graph for searching for the best-aligned sequence. dynamic approach that adaptively represents evolving scenes, incrementally updating the Scene Graph as actions unfold. Simulating each action generates explicit representations of intermediate states, breaking down sequences into smaller transitions. Explicit intermediate states bridge the gap between the initial and final states, improving VLMs corrective sequence planning and error detection. 4.1. Algorithm An overview  (Fig. 8)  and pseudo code for step completion is shown in Algorithm 1, with detailed steps below. More description and SGI for error detection in supplementary. 1) Vanilla Scene Graphs (SG): We QUERY VLMs (feed the states to the model to generate Scene Graphs, described in Section 3.5.1) to generate the Scene Graphs for the initial state I0 as S0 and the final goal Ig as Sg. We have already evaluated the performance of these vanilla Scene Graphs on the baselines in Tab. 3 as SG = [S0, Sg] 2) Incremental Scene Update: Starting from the initial SG (S0), we feed textual description of each action (A1,..E,..k) to VLM and ask it SIMULATE the action on the SG producing intermediate SG (Sc). The SIMULATE prompt to VLM: Simulate the given action sequence from the initial state, incrementally updating the scene graph. modifies nodes, attributes, and edges of SG. We then use intital context SG SC to SIMULATE each MCQ option independently producing for final scene graph Sm for the m-th option encompassing Ak+1,k+2..,..N . Note, SG SC after all the intial context steps is same for all MCQ options. Initial state I0, Goal state Ig, Initial Context actions Algorithm 1 SGI for Step Completion (Sec. 4) Input: A1, A2, ..AE ..Ak<N Objective: Pick the best option from MCQ for Step Completion Ak+1, Ak+2...AN Require: VLM M, Step Completion CQ options. ## 1) Vanilla Scene Graph, (ref Section 3.5.1) 1: S0 QUERY(cid:2)M(I0)(cid:3) 2: Sg QUERY(cid:2)M(Ig)(cid:3) // Obtain initial Scene Graph // Obtain goal Scene Graph ## 2) Incremental Scene Update (S0 Sc Sm) 3: Sc S0 4: for Ai in [A1, A2, ..AE ..Ak<N ] do Sc SIMULATE(cid:2)M(Sc, Ai)(cid:3) 5: // simulates i-th action Ai to incrementally update intermediate context Scene Graph Sc 6: end for k+2...Am k+1, Am Sm Sc for Am 7: for each option CQ do 8: Am 9: 10: 11: // actions from option // make copy for option k+1, Am ] do Sm SIMULATE(cid:2)M(Sm, Am // Simulate test actions to reach goal for the mth option k+2...Am in [Am )(cid:3) end for 12: 13: end for ## 3) Similarity Comparison 14: arg maxmM CQ SIMILARITY(cid:2)M(Sm, Sg)(cid:3) // Ask VLM to determine the similarity between all MCQs derived Scene Graphs and goal Scene Graph Sg 15: Output: 4.2. Difference between SGI vs SG & CoT 3) Similarity Comparison: After simulating each action, VLM is asked to compare SIMILARITY between resultant SG Sm and goal SG Sg. Prompt used for this: Compare the resulting scene graph with the goal scene graph to identify incorrect relationships, misplaced objects, or unmet constraints and score them between 0-100. compares mismatches in SGs. The option with best similarity score between the option Sm and Sg is chosen as prediction. Chain-of-Thought (CoT) represents the most basic form, where VLMs break complex tasks into sequence of stepby-step reasoning steps. Scene Graph (SG) builds on CoT via structured representation of the scene, enabling more coherent tracking and reasoning. Both CoT and SG focus on reasoning within single scene and interpolating decisions from that. Our Scene Graph Incremental update (SGI) extends this framework by adding temporal, 7 Table 4. Scene Graph Incremental update (SGI): SGI improvement relative to vanilla SG, same naming convention as Tab. 3. Method Robo-VQA-E SG SGI Step Completion (% ) Maze-E SGI Shuffle-E SGI SG SG Blocks-World-E SG SGI Robo-VQA-E SG SGI Error Detection (% ) Maze-E SGI SG Blocks-World-E SG SGI Intern-VLM 25.1 32.1 (+7.0) 23.4 25.2 (+1.8) 41.2 43.2 (+2.0) GPT-4o 18.9 29.2 (+10.3) 52.2 56.4 (+4.2) 30.1 37.0 (+6.9) 46.1 56.1 (+10.0) 54.3 55.3 (+1.0) 33.4 34.8 (+1.4) 37.3 42.9 (+5.6) 26.1 31.5 (+5.4) 44.2 57.4 (+13.2) 35.3 41.1 (+5.8) 42.1 50.7 (+8.6) Table 5. SGI on VQA dataset [40] Format same as Tab. 3. Step Completion (% ) Maze-Nav CoT SG SGI CoT SG SGI CoT SG SGI Spatial-Grid Spatial-Map 25.1 36.7 35.8 32.3 32.4 31.2 30.1 34.3 38.2 42.4 47.4 47.8 20.8 27.3 29.3 34.4 35.8 36.3 36.3 41.3 44.3 28.6 40.5 42.1 33.3 33.8 35.1 Method CoG -VLM Janus -pro-7B Intern -VLM Table 6. SGI on Planbench [37] PlanBench score (Plan completion) Method Variant Task 8 Score () Qwen2 VL-8B (our) Vanilla CoT SG SGI 13.8 14.1 13.9 14.7 Figure 9. Error Free Step Completion 3D component, where scene graphs not only to represent the current scene but also to derive next-time-frame scene graphs. Effectively, SGI interpolates CoT and SG reasoning across sequential scenes, allowing VLMs to reason through evolving scenes rather than interpolating scene-level decisions. In terms of reasoning hierarchy, CoT SG SGI. 4.3. Results CoSPlan Comparison Table 4 shows that SGI significantly outperforms the vanilla Scene Graph (SG) approach across all benchmark tasks/datasets. For Step Completion, SGI achieves 1.8%10.3% improvement for Intern-VLM and 1%10% for GPT-4o. For Error Detection, SGI improves performance by 1.4%5.6% for Intern-VLM and 5.8%13.2% for GPT-4o. Gemini-2.5-pro [35] observes 67% for CoT, 70% with SG and 71.5% for SGI on BlocksWorld-E for Step Completion. Compute-wise, SGI makes 1 VLM call/step, i.e. length of initial context + # of MCQ options Avg. # of steps per option. The added compute is justified by up to 13% boost in error detection, with similarly consistent improvements across all tasks, proving the effectiveness of incorporating intermediate scene representations in robust sequential planning. External Dataset Unlike Sequence Planning, which involves final state and sequence of intermediate transitions, Visual Question Answering (VQA) [40] lacks temporal structure. VQA typically presents static scene accompanied by MCQs. In our formulation, we treat this figure as both the initial and the final state, and ask VLM to iteratively simulate all MCQ options (checking for feasibility). In contrast to Scene Graph or CoT, SGI independently and iteratively evaluates each MCQ option. This extra emphasis on options can enables more informed decision-making, as highlighted in the superior performance of SGI in Tab. 5. Description of each task in Supplementary. We also evaluate our SGI algorithm on text-only PlanBench [37], plan completion with blockworld (task 8). We use Qwen2 VL-8B, with SGI applied to textual SG, yielding the best PlanBench score (Tab. 6). Note, other tasks of PlanBench are plan generation and not plan completion (outside the scope of the work). Error-Free Scenario: Figure 9 shows SGI not only outperforms SG on error-prone corrective sequence planning but also in the error-free ideal scenario, making our appraoch generic for all types of sequence planning tasks, further validating the robustness of our approach in enhancing VLMs for structured sequential reasoning and decision-making. 5. Limitation CoSPlan reveals near-random predictions of VLMs in error-prone sequential planning with just one error. Expanding our analysis to multiple error cases requires automation currently limited by VLMs (e.g. GPT-4o) not being able to handle even one error. Additionally, CoSPlan adds temporal dimension to sequence planning, but extending it to videos requires VLMs processing multiple videos (start and goal states), left as future work. 6. Conclusion In this work, we introduce CoSPlan, benchmark designed to evaluate the decision-making capabilities of VLMs in error-prone, sequential planning tasks that simulate practical scenarios. CoSPlan challenges VLMs to solve 2D spatial vision tasks with text-based instructions, requiring temporal reasoning over previously executed actions. Our empirical analysis reveals key limitations in current VLMs: i) they often make random predictions, ignoring contextual understanding, ii) struggle with in-context errors, and iii) exhibit bias toward text-based reasoning over multimodal decision-making. Notably, even advanced models like GPT-4o fail to leverage contextual cues effectively 8 to reach goals. To address these challenges, we propose SGI (Scene Graph Incremental update), technique that refines scene graphs step-by-step with each action, generating intermediate steps. SGI substantially boosts performance across error-prone, error-free, and VQA settings compared to vanilla scene graphs, highlighting its robustness and ability to enhance corrective sequence planning in VLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 4, 11 [2] Masataro Asai. Photo-realistic blocksworld dataset, 2018. 2 [3] Masataro Asai, Hiroshi Kajino, Alex Fukunaga, and Christian Muise. Classical planning in deep latent space, 2022. 2 [4] Smail Ait Bouhsain, Rachid Alami, and Thierry Simeon. Learning to predict action feasibility for task and motion In 2023 IEEE International planning in 3d environments. Conference on Robotics and Automation (ICRA), pages 37363742. IEEE, 2023. 2 [5] Ming Chen, Yuan Li, et al. Measuring and improving chain-of-thought reasoning in vision-language models. arXiv preprint arXiv:2309.04461, 2023. 2 [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1, 2, 4 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2, 4, 11 [8] Anjie Cheng et al. Spatialrgpt: Grounded spatial reasoning in vision language model. OpenReview, 2024. 2 [9] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. 2 [10] Breanne Crockett, Carl Mueller, and Bradley Hayes. Human demonstrations enable efficient solutions to sequential In Proceedings of the 2025 manifold planning problems. ACM/IEEE International Conference on Human-Robot Interaction, pages 800809, 2025. 2 [11] Vinay Damodaran, Sharanya Chakravarthy, Akshay Kumar, Anjana Umapathy, Teruko Mitamura, Yuta Nakashima, Noa Garcia, and Chenhui Chu. Understanding the role of scene graphs in visual question answering, 2021. 2 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248255. IEEE, 2009. 3, 4 [13] Shresth Grover, Vibhav Vineet, and Yogesh Rawat. Navigating hallucinations for reasoning of unintentional activities, 2024. [14] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with arXiv language model preprint arXiv:2305.14992, 2023. 2 is planning with world model. [15] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2024. 1 [16] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716, 2024. 2 [17] Itseez. Open source computer vision library. https:// github.com/itseez/opencv, 2015. [18] Anubhooti Jain, Mayank Vatsa, and Richa Singh. Words over pixels? rethinking vision in multimodal large language In Proceedings of the Thirty-Fourth International models. Joint Conference on Artificial Intelligence, IJCAI-25, pages 1048110489. International Joint Conferences on Artificial Intelligence Organization, 2025. Survey Track. 6 [19] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs, 2018. 2 [20] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, and Anil Murthy. Position: Llms cant plan, but can help planning in llm-modulo frameworks. In Forty-first International Conference on Machine Learning, 2024. 2 [21] Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun. Embodied semantic scene graph generation. In 5th Annual Conference on Robot Learning, 2021. 2 [22] Kartik Nagpal and Negar Mehr. Optimal robotic assembly sequence planning: sequential decision-making approach, 2025. 2 [23] Siddharth Nayak, Adelmo Morrison Orozco, Marina Ten Have, Vittal Thirumalai, Jackson Zhang, Darren Chen, Aditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, Brian Ichter, James Harrison, Anuj Mahajan, and Hamsa Balakrishnan. MAP-THOR: Benchmarking long-horizon multi-agent planning frameworks in partially observable enIn Multi-modal Foundation Model meets Emvironments. bodied AI Workshop @ ICML2024, 2024. [24] Swarna Kamal Paul. Sequential planning in large partially observable environments guided by llms, 2023. 1 [25] Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. Capture: Evaluating spatial reasoning in vision language models via occluded object counting. arXiv preprint arXiv:2504.15485, 2025. 3 [26] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cogarXiv preprint nition emerge in frontier models? arXiv:2410.06468, 2024. 1, 2 [27] Krishan Rana, Jesse Haviland, Sourav Garg, Jad AbouChakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. In 7th Annual Conference on Robot Learning, 2023. 2 [28] Nicholas Rossetti, Massimiliano Tummolo, Alfonso Emilio Gerevini, Luca Putelli, Ivan Serina, Mattia Chiari, and Matteo Olivato. Learning general policies for planning through gpt models. Proceedings of the International Conference on Automated Planning and Scheduling, 34(1):500508, 2024. 2 [29] Luca Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, 7(1):96106, 2025. 6 [30] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: large-scale multi-view video dataset for understanding procedural activities. CVPR 2022, 2022. 1, 2 [31] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for In 2024 IEEE International Conference on robotics. Robotics and Automation (ICRA), pages 645652. IEEE, 2024. 2, 3, 4 [32] Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. 1, 2 [33] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CË†ote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. [34] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. 2 [35] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 8 [36] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models - critical investigation. In Advances in Neural Information Processing Systems, pages 75993 76005. Curran Associates, Inc., 2023. 2 [37] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:7599376005, 2023. 1, 2, 8, 13 [38] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning, 2024. 1 [39] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thou10 sand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37:7539275421, 2024. [40] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models, 2024. 2, 3, 8 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 4, 11 [42] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 2, 4, 11 [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, 5, 13 [44] Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, and Yongbin Li. FlowBench: Revisiting and benchmarking workflow-guided planning for LLM-based agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 10883 10900, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2 [45] Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, and Ge Zhang. Ing-vp: Mllms cannot play easy vision-based games yet, 2024. 2 [46] Jiatao Zhang, Lanling Tang, Yufan Song, Qiwei Meng, Haofu Qian, Jun Shao, Wei Song, Shiqiang Zhu, and Jason Gu. Fltrnn: Faithful long-horizon task planning for In 2024 IEEE Interrobotics with large language models. national Conference on Robotics and Automation (ICRA), pages 66806686. IEEE, 2024. [47] He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos G. Derpanis, Richard P. Wildes, and Allan D. Jepson. P3iv: Probabilistic procedure planning from instructional videos with weak supervision, 2022. 2 [48] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. arXiv preprint arXiv:2309.03882, 2023. 5 [49] Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, HengTze Cheng, Quoc V. Le, Ed H. Chi, and Denny Zhou. Natural plan: Benchmarking llms on natural language planning, 2024. 2 [50] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. 2 CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates Supplementary Material both proprietary and open-source solutions. These models exhibit diverse architectural characteristics for multimodal understanding. 8.1. GPT-4o GPT-4o [1] is general-purpose VLM that uses textual inputs and outputs. It integrates modality processing within single model architecture for text, images, and audio processing. 8.2. CoG-VLM CoG-VLM [42] (Cognitive Vision-Language Model) is designed for visual reasoning tasks with enhanced spatial understanding capabilities.Vision Backbone: Utilizes EVA2CLIP-E as the ViT encoder with the final aggregation layer removed to preserve spatial information.Language Model: Built on Vicuna1.5-7B, with causal masking for attention operations. 8.3. InternVL2-26B InternVL2-26B [7] is multimodal model optimized for visual understanding and reasoning tasks.Architecture: Combines InternViT-300M-448px for vision processing with internlm2 5-7b-chat for language tasks. 8.4. Qwen2-VL Qwen2-VL [41] is multimodal model from the Qwen family, utilizing 7B parameter variant.Language Backbone: Based on Qwen2-7B large language models. 8.5. Janus Janus is vision-language model designed for understanding, generation tasks, and specially multi modal reasoning represented by the Janus-Pro variant.Vision Understanding: Employs SigLIP as vision encoder for semantic feature extraction.Language Processing: Implements transformer-based language model. 8.6. Experimental Setup Input Preparation: Raw images and text prompts are converted into model-specific input formats, with concatenated initial and target images. Query Formation: Structured as {information about the env} -> Task to optimize reasoning capabilities. Output Processing: Model responses are parsed into structured formats for evaluation metrics. Figure 10. CoSPlan overview: The input context comprises executed actions and both the initial and final states. The model predicts the optimal action steps to reach the goal (green) and identifies errors in the provided context (red). The Main Submission also showed an example in Figure 1. 7. Clarification on Step Completion Design Current VLMs struggle with real-world deployment because their self-supervised training rarely includes the suboptimal steps or execution errors common in autonomous navigation and robotics. CoSPlan mimics this setting to test recovery capabilities. key design choice is the integration of error correction into step completion: rather than explicit instruction, the agent must autonomously recognize the need to correct the course if it deems their is suboptimal step. The correct option recovers from the error state to the goal, while incorrect options perpetuate the failure. Keeping this integrated design, we evaluate Error Detection and Step Completion separately to distinguish between error detection failures and step completion failure. This granular analysis prevents confounding variables such as models ability to spot an error versus its ability to fix it from masking specific weaknesses.(e.g. high detection but low completion scores in GPT-4o). 8. Vision-Language Models Overview We employ suite of state-of-the-art vision-language models (VLMs) to address visual reasoning tasks, including 11 9. Scene Graph Incremental update Details Initial state I0, Goal state Ig, Initial Context actions Algorithm 2 SGI (Error Detection) Input: A1, A2, ..AE ..Ak<N Require: VLM M, Initial Context as CQ options. ## 1) Vanilla Scene Graph 1: S0 QUERY(cid:2)M(I0)(cid:3) 2: Sg QUERY(cid:2)M(Ig)(cid:3) // Obtain initial Scene Graph // Obtain final Scene Graph ## 2) Incremental Scene Update (S0 Sc) 3: Sc S0 4: for Ai in [A1, A2, ..AE ..Ak<N ] do Sc SIMULATE(cid:2)M(Sc, Ai)(cid:3) 5: // simulates i-th action Ai to incrementally update intermediate context Scene Graph Sc simi SIMILARITY(cid:2)M(Sc, Sg)(cid:3) // we compute similarity of context action Sc with goal Sg 6: 7: end for ##) Error Detection Similarity 8: simi arg miniSi [simi] Find the least similar context action, measures deviation. 9: if simi > 0.75 then 10: return None of the above // if the least similarity simi > 0.75 (hyperparameter), deviation from goal is not large enough for error. 11: else 12: return Ai // Ai produces scene graph with similarity < 0.75 and has maximum deviation. 13: end if The Scene Graph Incremental update (SGI) framework enhances the decision-making of VLMs in sequential instruction-following, particularly when handling incomplete plans or embedded errors (AE ). Unlike conventional Chain-of-Thought (CoT) approaches that infer the transformation from I0 to Ig in single step, SGI decomposes reasoning into structured, interpretable updates. While the main paper details SGI for Step Completion, here we present the adaptation for Error Detection in Algorithm 2. Formally, given the initial state I0 and goal state Ig, we derive structured scene graphs S0 and Sg, capturing entities and spatial relations. The VLM processes context sequence A1...E...k<N containing an error AE . SGI operates via Incremental Simulation and Similarity-Based Selection. Incremental Scene Update. We task the VLM to SIMULATE the textual actions, acting as state tracker to modify nodes and relational edges. The simulation prompt is: Simulate the given action sequence from the initial state, incrementally updating the scene graph. The model propagates the context actions (A1, . . . , AE , . . . , Ak) over the initial graph S0 to generate an intermediate graph Sc, representing the environment state after the context actions. Subsequently, for each available MCQ candidate option m, the model applies the proposed action to Sc, generating hypothetical resultant graph Sm. Since VLMs simulate all the steps, the simulation depth is fixed for all the models. Additionally, we do not supervise SG generation, i.e. nodes, and edges can sometimes be noisy. Similarity Comparison. To identify the optimal continuation, SGI compares the hypothetical graph Sm against the ground truth goal graph Sg. The VLM is prompted: Compare the resulting scene graph with the goal scene graph to identify incorrect relationships, misplaced objects, or unmet constraints. Select the best-aligned plan. The system selects the option that maximizes structural alignment with Sg, ensuring decisions are grounded in verifiable physical constraints rather than superficial text probabilities. The similarity scores are based on VLM judgment, which is not guided by us, since Scene-graphs are model-dependent and one universal similarity metric may not be applicable on all kind of scene graphs. 10. Results 10.1. External Dataset Description VQA External Dataset Details Wang et al. proposed series of visual question-answering tasks to test VLMs ability on visual reasoning (different from our sequence planning tasks, as they involves no intial context). i) Spatial-Map: Tests spatial relationships between objects with unique location names (e.g. Unicorn Umbrellas). Objects have pairwise relationships like is Southeast of B. Questions ask about spatial relationships and counting objects meetii) Maze-Nav: Evaluates navigation ing spatial criteria. through mazes from the starting point (S) to the exit (E). Uses colored blocks (green=start, red=exit, black=walls, white=paths, blue=solution path) or ASCII representation. Questions count turns and determine spatial relationships between and E. iii) Spatial-Grid: Tests spatial reasoning in structured 5 5 grids containing animals (cat, dog, elephant, giraffe, rabbit). Questions involve counting specific animals and identifying animals at specific grid coordinates. These datasets collectively focus on evaluating and advancing spatial reasoning capabilities and are publicly available. PlanBench (Algorithmic Generalization) In this evaluation setting, we test the models ability to perform inductive reasoning over sequential actions. We define planning problem as tuple consisting of an initial state and goal configuration, and plan as the sequence of actions required to transition from the start to the goal. Unlike standard instruction following, the prompt here consists of few-shot example traces generated by fixed underlying program, e.g. script containing latent control flows such as loops Table 7. All performance on CoSPlan benchmark: Same naming convention as that of Table 2 from main submission. SGI results same as Table 3. VLM Method Step Completion (% ) Error Detection (% ) Robo-VQA-E Shuffle-E Maze-E Blocks-World-E Robo-VQA-E Maze-E Blocks-World-E Random Qwen2 VL-8B CoG-VLM (Our) Janus-pro-7B (Our) Intern-VLM (Our) GPT-4o (Our) Vanilla CoT SG SGI Vanilla CoT SG SGI Vanilla CoT SG SGI Vanilla CoT SG SGI CoT SG SGI 20 17.1 17.6 18.9 19.1 13.1 12.5 21.5 22.1 14.1 14.7 21.3 21.1 22.1 23.5 25.1 32.1 48.2 52.2 56. 20 24.1 24.9 25.1 25.0 23.1 27.1 23.7 26.9 23.2 23.1 23.5 26.1 20.1 23.2 23.4 25.2 27.6 30.1 37.0 20 26.5 27.9 28.3 28.5 25.1 25.9 26.5 29.3 20.4 20.2 21.7 23.2 21.6 35.8 41.2 43.2 45.6 46.1 56.1 or conditionals (e.g. an algorithm to unstack all blocks). The model is tasked with generating plan for new problem instance that follows this same structural logic but differs in complexity (e.g. larger number of objects requiring more iterations). We evaluate performance by verifying if the generated sequence is valid: it must be executable within the domain constraints and successfully satisfy the specified goal conditions. SGI for planning [37] uses an off the shelf planner to evaluate LLM plans for known tasks like blockworld, indicating that these planners are able to achieve very high accuracy on these tasks. In our experiments, the text only version also shows almost perfect performance especially for GPT-4o indicating this is with how VLM anticipates the sequence of actions. 10.2. Experiments Baseline CoT step-by-step prompting have been shown to significantly boost performance on arithmetic and logical tasks [43]. This makes it our preferred baseline approach. SG, on the other hand, captures objects, attributes, and relationships, providing structured representations that enhance VLMs ability to reason about complex scenes. 20 18.1 18.6 18.8 18.5 25.5 25.2 26.7 26.4 24.2 23.1 25.1 26.3 18.3 21.2 18.9 29.2 49.7 54.3 55. 15.4 9.2 9.1 9.6 10.1 32.1 33.4 35.3 38.7 17.5 18.1 26.1 27.6 24.3 25.2 26.1 31.5 45.3 44.2 57.4 33.3 20.5 20.8 20.7 21.3 6.4 8.4 13.3 11.0 20.5 19.1 21.0 21.6 32.8 33.1 33.4 34.8 40.3 35.3 41.1 33.3 32.3 30.6 35.2 35.7 41.3 43.1 44.5 46.1 29.3 31.0 27.6 33.2 36.5 37.9 37.3 42.9 35.1 42.1 50.7 Figure 11. As the number of obstacles increases, the accuracy remains pretty much constant for all VLMs. CoT technique. tion and Error Detection. GPT-4o achieves the highest performance across all tasks, indicating the synergy between strong underlying language and reasoning about sequence of actions as expected. Intern-VLM also demonstrates significant gains over its counterparts showing strong performance on Maze-E and Robo-VQA as compared to other models. Table 7 presents detailed results for the models providing comperehensive coverage of models and zero shot adapting techniques. Table 2 results Comparing different VLMs, from Table 2, we observe that InternVLM and GPT-4o methods consistently outperform prior baselines on both Step CompleEffect on number of Obstacles Figure 11 shows that the number of obstacles (red box) in Maze-E doesnt seem to impact performance. There are 2 likely explanations: 1) In13 Figure 12. Models have strong bias towards picking option regardless of the goal and context, partially explaining the reason for random accuracy prediction. CoT technique. crease in the number of obstacles reduces the possibilities of paths towards goal, easing the difficulty. ii) Constant accuracy may indicate the model not understanding/ignoring the boxes/obstacles. Bias towards selective options For Blocks-World-E, Janus predicts 100% of the time option A, and strong bias towards the prediction of option  (Fig. 12)  . This partially explains the accuracy near random, as the correct solution appears at option with uniform probability among all 5 options (A, B, C, D, and E). Bias towards Cheating the answers Figure 13 shows Figure 14. Accuracy of taking steps towards the goal Ig. Figure 15. Same convention as fig 7a) in the main submission. Model accuracy goes down with the number of MCQ options. servations remains consistent here as well, i.e. models dont seem to be using the additional available context ( k) in MCQ options to reach goal Ig. All models maintain stable accuracy regardless of k, MCQ Options: Figure 15 generalizes the observation of Fig 7a) in the main submission for Janus-pro7B, and CoGVLM, showing that increasing the number of MCQ options drops the performance. Figure 13. No of times Intern-VLM (CoT) cheat (pick the option describing final state without error correction. 11. Hyperparameters (Reproducibility) when Intern-VLM cheats (pick the option describing the final state, without error correction), that too with bias towards picking option A. The random probability of picking an option is 20%, which implies its conscious decision by the model to pick certain option, whether bias towards certain option or cheating. Ignoring additional context: Figure 14 generalizes the observation of how models ignores additional context (Fig 7b in main submission). The setup remains the same, as the main submission, where for constant context of length 2 (1 initial step and 1 error), performance was evaluated for step completion, where the models need to take 9 steps to reach the goal Ig (inclusive of 1 error correction). Main submission showed the constant accuracy for Blocks-World-E, while here we show it for Shuffle-E and Maze-E. The obWe set threshold of 0.75 for similarity in error detection (Algorithm 2). The batch size was set 1. Number of GPUs used was 1, 48Gb on NVIDIA RTX A6000 GPU. We would additionally release our code base for task generation and evaluation, along with our SGI algorithm."
        },
        {
            "title": "Ethical Statement",
            "content": "The CoSPlan benchmark includes both synthetic and realworld task settings. All real-world images are either synthetic or sourced under permissible licenses without depicting identifiable individuals or private information. While CoSPlan highlights the limitations of VLMs in sequential reasoning, it is not intended for deployment in safetycritical applications. Additionally, models evaluated may exhibit biases inherited from pretraining data. The dataset 14 and code will be released for research purposes only, and we advise responsible use. 12. Future work Testing the robustness of the SGI algorithm to the noises in faulty Scene graph generation (node, state edges) can better assess the practical value of our algorithm. However, all VLMs struggle with visual + text based sequence planning tasks, further complicated by the addition of just one basic error. In such scenarios, diving deeper into the model scene representation will be slightly out of scope for this work, left as future scope. Since our algorithm is based on the idea of simulating each step/action in sequence, in its core, its not really dependent on the scene graph. Future work will look into the extension of step-by-step simulation to other forms of reasoning algorithms. 13. Task-Specific CoT Examples 13.1. Maze-E 15 Maze-E CoT prompt Please analyze this grid-based image following these specific instructions along with the provided context: You are looking at grid pattern image that contains: grid structure (e.g., 3x3 or 4x4 etc) designated start point (e.g., marked by green circle) designated end point (e.g., marked by blue circle) Obstacles (e.g., represented as red rectangular blocks) Determine path from the start point to the end point, avoiding obstacles. Provide your answer in two formats: Path Coordinates: Start at the designated start point and list all coordinates as [(row,col)], where (0,0) is the top-left cell. Directional Instructions: Starting from the designated start point, provide step-by-step directions. For example, if path from top-left to bottom-right around an obstacle is observed, the response should be structured as: Path Coordinates: [(0,0), (0,1), (0,2), (1,2), (2,2)] Please verify that: The path originates from the start point. The path terminates at the end point. The path circumvents all obstacles. Only horizontal and vertical movements are utilized. Identify the path present in the image. The path has already started with the initial step: [0, 0] -> [-1, 0] You will be presented with set of options, each representing potential path. Select the option that correctly guides Path Description, ...}. from the start to the end point. Options are provided as: {Option Label: 13.2. Blocks-World-E Blocks-World-E CoT prompt Given an image depicting Block Manipulation transformation, identify the correct sequence of moves to transition from the initial state (left) to the final state (right). In this Block Manipulation scenario: Blocks are uniquely identified (e.g., by numbers and colors). Columns are labelled from 0 - 4 from left to right Blocks can only be moved onto the top of other blocks or into empty columns. block can only be moved if it is clear of blocks above it. Each move is described as: Move block [Identifier] from column [Source Column] to column [Destination Column]. {Option Label: Sequence of Moves, ...} The following steps have already been performed: Preceding Steps: ... (e.g., 0. Move block 0 from column 3 to column 4, 1. Move block 2 from column 0 to column 1) From the current state, select the correct option from the provided choices. Options: block 0 from column 3 to column 4, 2. 1, ...) Analyze the image and determine the correct option. Consider: 1. The initial configuration of blocks. 2. Which blocks are currently movable (unobstructed). 3. The feasibility of each move. 4. Whether the sequence of moves results in the depicted final state. Provide only the label of the correct option (e.g., A, B, C, or D) without any explanation. Move block 2 from column 0 to column Option A: 1. Move (e.g., 16 13.3. Shuffle-E Shuffle-E CoT Prompt Given information about image patches, determine the subsequent steps required to rearrange shuffled image into target image. The image is composed of patches, and each patch is identified by its row and column index, starting from (0,0) for the top-left patch. The following rearrangement steps have already been executed: Preceding Steps: at position (2, 1), Step 2: Swap patch at position (0, 1) with patch at position (0, 0), ...) Select the correct option that outlines the subsequent steps to correctly rearrange the patches from the current shuffled state to the target image. Options: Swap patch at position (2, 1) with patch at position (0, 2), Step 2: patch at position (1, 0) with patch at position (0, 2), ...) Provide only the option label as the answer, without any explanation. {Option Label: Sequence of Swap Operations, ...} (e.g., A. Step 1: Swap patch at position (0, 0) with patch Step 1: Swap (e.g., 13.4. Robo-VQA-E Robo-VQA-E CoT Prompt Review the provided composite image, which illustrates two states: the initial state (left) and the completed state (right). robotic arm has performed sequence of actions to transition between these states. Your task is to identify any errors within the executed actions that would prevent reaching the completed state, or to detect any unnecessary steps. It is assumed that all listed actions have been completed, and any subsequent actions must build upon this sequence. The sequence of actions already performed is given as: Actions Taken: (e.g., 1Put glass upside down on the table, 2Transfer plastic cups from table to dustbin.) From the provided options, select the step(s) that best contribute to achieving the completed state. Possible Steps: {Option Label: upside down on the table, B: Transfer plastic cups from table to dustbin.) The answer should be in the form of the option label (e.g., A, B, C, ...) representing the correct choice(s), without any explanation. Action Description, ...} (e.g., A: Put glass 14. Scene Graph Examples 17 Figure 16. Initial and Final states for Example"
        },
        {
            "title": "SGI",
            "content": "Input Prompt - Step 1 Examine composite image showing two stages: the left side depicts the initial stage of task, and the right side illustrates the completed task. Analyze the provided initial and goal state images to extract structured scene graph representation. The scene graph should include objects, their attributes (such as color, size, and material), their spatial relationships (such as on, inside, near), and any interactions between objects. Ensure that each objects role and placement are clearly defined. 18 Output - Step 1: Scene Graph Analysis Initial Scene Graph: Objects = bowl1 bowl2 apples oranges dry fruit container pen holder napkin dispenser (empty, table left) (empty, table right) (on table, near bowls) (on table, near bowls) (closed, back) (back of table) (back of table) Goal Scene Graph: Objects = bowl1 bowl2 dry fruit container pen holder napkin dispenser (filled with apples, left) (filled with oranges, right) (closed, back) (back of table) (back of table) Input Prompt - Step 2 Examine composite image showing two stages: the left side depicts the initial stage of task, and the right side illustrates the completed task. Here is sequence of actions that have already been executed: 1. Two bowls are placed on the table, spaced apart to accommodate the fruit 2. Tissue or decorative paper is placed inside each bowl to act as lining 3. Open the containers containing dry fruits 4. Put some dry fruits in each bowl 5. Place another tissue over the dry fruits in both bowls Given the starting scene graph, simulate the sequential execution of the provided steps and update the scene graph at each stage. Ensure that object states and positions are modified based on their interactions. Output - Step 2: Intermediate State Intermediate Scene Graph: Objects = bowl1 bowl2 apples oranges dry fruit container pen holder napkin dispenser (contains dry fruits with tissue, left) (contains dry fruits with tissue, right) (on table, near bowls) (on table, near bowls) (open, back) (back of table) (back of table) 19 Figure 17. Initial and Goal State Images Alternative Paths - Input Prompts and Outputs Option Prompt: Simulate the following actions: (1) Remove tissue from both bowls, (2) Transfer dry fruits out of the bowls, (3) Place apples inside the left bowl, (4) Place oranges inside the right bowl. Output (Match Score: 100%): Actions = {Remove tissue, Transfer dry fruits, Place apples (left), Place oranges (right)} Option Prompt: Simulate the following actions: (1) Remove tissue from both bowls, (2) Place pens inside the holder, (3) Remove the dry fruit container, (4) Place oranges inside the right bowl. Output (Match Score: 50%): Actions = {Remove tissue, Place pens, Remove dry fruit container, Place oranges} Option Prompt: Simulate the following actions: (1) Transfer dry fruits from bowls, (2) Place pens inside the holder, (3) Remove the dry fruit container, (4) Place snacks inside the drawer. Output (Match Score: 30%): Actions = {Transfer dry fruits, Place pens, Remove container, Place snacks in drawer} Input Prompt - Step 1 Analyze the provided grid-based scene to extract the initial and goal state scene graphs. The scene graph should include the agent, the goal, and any obstacles, with their positions and properties. 20 Output - Step 1: Scene Graph Analysis Initial Scene Graph: { \"initial_scene_graph\": { \"objects\": [ {\"name\": \"agent\", \"type\": \"green_circle\", \"state\": \"active\", \"position\": [5, 0], \"properties\": {}}, {\"name\": \"goal\", \"type\": \"blue_circle\", \"state\": \"target\", \"position\": [2, 4], \"properties\": {}}, {\"name\": \"obstacle_1\", \"type\": \"red_rectangle\", \"state\": \"blocking\", \"position\": [3, 2], \"properties\": {}}, {\"name\": \"obstacle_2\", \"type\": \"red_rectangle\", \"state\": \"blocking\", \"position\": [2, 2], \"properties\": {}} ], \"grid\": {\"dimensions\": [6, 5], \"coordinates\": \"(0,0) is top-left\"} } } Target Scene Graph: { \"target_scene_graph\": { \"objects\": [ {\"name\": \"agent\", \"type\": \"green_circle\", \"state\": \"active\", \"position\": [2, 4] , \"properties\": {}}, {\"name\": \"goal\", \"type\": \"blue_circle\", \"state\": \"target\", \"position\": [2, 4] , \"properties\": {}}, {\"name\": \"obstacle_1\", \"type\": \"red_rectangle\" , \"state\": \"blocking\", \"position\": [3, 2] , \"properties\": {}}, {\"name\": \"obstacle_2\", \"type\": \"red_rectangle\", \"state\": \"blocking\", \"position\": [2, 2] , \"properties\": {}} ], \"grid\": {\"dimensions\": [6, 5], \"coordinates\": \"(0,0) is top-left\"} } } 21 Input Prompt - Step 2 Given the initial scene graph, simulate sequence of actions for the agent to move towards the goal. Update the scene graph to reflect an intermediate state after executing the following actions: 1. Move up from [5, 0] to [4, 0] 2. Move right from [4, 0] to [4, 1] 3. Move right from [4, 1] to [4, 2] Ensure the agents position is updated while maintaining the positions of the goal and obstacles. Output - Step 2: Intermediate Scene Graph Intermediate Scene Graph: { \"intermediate_scene_graph\": { \"objects\": [ {\"name\": \"agent\", \"type\": \"green_circle\", \"state\": \"active\", \"position\": [4, 2], \"properties\": {}}, {\"name\": \"goal\", \"type\": \"blue_circle\" , \"state\": \"target\", \"position\": [2, 4], \"properties\": {}}, {\"name\": \"obstacle_1\", \"type\": \"red_rectangle\" , \"state\": \"blocking\", \"position\": [3, 2], \"properties\": {}}, {\"name\": \"obstacle_2\", \"type\": \"red_rectangle\" , \"state\": \"blocking\", \"position\": [2, 2] , \"properties\": {}} ], \"grid\": {\"dimensions\": [6, 5] , \"coordinates\": \"(0,0) is top-left\"} } } Input Prompt - Step 3 Evaluate the following proposed paths for the agent to reach the goal from the initial position [5, 0], checking if they avoid obstacles, use legal moves, and reach the target position [2, 4]. Provide step-by-step analysis and corrected score for each option: Option A: [5, 0] [4, 0] - [4, 1] - [4, 2] - [4, 3] - [4, 4] - [3, 4] - [2, 4] Option B: [5, 0] - [4, 0] - [4, 1] - [3, 1] - [2, 1] - [3, 1] Option C: [5, 0] - [4, 0] - [4, 1] - [3, 1] - [4, 1] - [4, 0] Output - Step 3: Path Evaluations Option A: Correct Path Path: { \"path\": [ [5, 0], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [3, 4], [2, 4] ] } Step-by-Step Analysis:"
        },
        {
            "title": "Step Action",
            "content": "1 2 3 4 5 6 7 move up move right move right move right move right move up move up From [5,0] [4,0] [4,1] [4,2] [4,3] [4,4] [3,4] To [4,0] [4,1] [4,2] [4,3] [4,4] [3,4] [2,4]"
        },
        {
            "title": "Valid\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue",
            "content": "Metrics: Similarity Score: 100 Reaches Goal: true Avoids Obstacles: true Uses Legal Moves: true Path Length: 7 Option B: Incorrect Path Path: { \"path\": [ [5, 0], [4, 0], [4, 1], [3, 1], [2, 1], [3, 1] ] } Step-by-Step Analysis: Step Action 1 2 3 4 5 move up move right move up move up move down From [5,0] [4,0] [4,1] [3,1] [2,1] To [4,0] [4,1] [3,1] [2,1] [3,1] Valid true true true true true Metrics: Similarity Score: 40 Reaches Goal: false Avoids Obstacles: true Uses Legal Moves: true Path Length: 5 Option C: Incorrect Path Path: { \"path\": [ 23 [5, 0], [4, 0], [4, 1], [3, 1], [4, 1], [4, 0] ] }"
        },
        {
            "title": "Step Action",
            "content": "1 2 3 4 5 move up move right move up move down move left From [5,0] [4,0] [4,1] [3,1] [4,1] To [4,0] [4,1] [3,1] [4,1] [4,0]"
        },
        {
            "title": "Valid\ntrue\ntrue\ntrue\ntrue\ntrue",
            "content": "Step-by-Step Analysis: Metrics: Similarity Score: 20 Reaches Goal: false Avoids Obstacles: true Uses Legal Moves: true Path Length: 5 starting position. Option C: Incorrect Path Path: { \"path\": [ [5, 0], [4, 0], [3, 0], [2, 0], [1, 0], [0, 0] ] } Step Action 1 2 3 4 move up move up move up move up move up From [5,0] [4,0] [3,0] [2,0] [1,0] To [4,0] [3,0] [2,0] [1,0] [0,0] Valid true true true true true Step-by-Step Analysis: Metrics: Similarity Score: 50 Reaches Goal: false Avoids Obstacles: true Uses Legal Moves: true Path Length: Input Prompt - Step 1 Examine composite image showing two stages: the left side depicts the initial stage of task, and the right side illustrates the completed task. Analyze the provided initial and goal state images to extract structured scene graph representations for BlockWorld problem. The scene graphs should include objects (blocks), their attributes (color), states (resting or stacked), positions (column and level), and relationships (on top, beside). 24 Output - Step 1: Scene Graph Analysis Initial Scene Graph: { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_1, level_1\", \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_2, level_1\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_2, level_2\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_2\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_3\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_1\", \"properties\": {\"color\": \"light_pink\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_3\", \"properties\": {\"color\": \"red\"}} ], \"relationships\": [ {\"relationship\": \"on_top\", \"subject\": \"block_7\", \"object\": \"block_0\"}, {\"relationship\": \"on_top\", \"subject\": \"block_6\", \"object\": \"block_1\"}, {\"relationship\": \"on_top\", \"subject\": \"block_5\", \"object\": \"block_6\"}, {\"relationship\": \"beside\", \"subject\": \"block_2\", \"object\": \"block_0\"}, {\"relationship\": \"beside\", \"subject\": \"block_3\", \"object\": \"block_4\"} ], \"environment\": { \"global_constraints\": {\"max_height\": 3, \"columns\": 5}, \"valid_actions\": [\"move\", \"stack\", \"unstack\"], \"boundary_conditions\": [\"blocks must remain within 5 columns\", \"only one block can be moved at time\"] } } Target Scene Graph: { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_1, level_1\", \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_2\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_3\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_4\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_5\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"red\"}}, {\"name\": \"block_3\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_1\", \"properties\": {\"color\": \"light_pink\"}} ], \"relationships\": [ {\"relationship\": \"on_top\", \"subject\": \"block_0\", \"object\": \"block_2\"}, {\"relationship\": \"on_top\", \"subject\": \"block_7\", \"object\": \"block_0\"}, {\"relationship\": \"on_top\", \"subject\": \"block_1\", 26 \"object\": \"block_7\"}, {\"relationship\": \"on_top\", \"subject\": \"block_6\", \"object\": \"block_1\"}, {\"relationship\": \"beside\", \"subject\": \"block_3\", \"object\": \"block_4\"}, {\"relationship\": \"beside\", \"subject\": \"block_5\", \"object\": \"block_3\"} ], \"environment\": { \"global_constraints\": {\"max_height\": 5, \"columns\": 5}, \"valid_actions\": [\"move\", \"stack\", \"unstack\"], \"boundary_conditions\": [\"blocks must remain within 5 columns\", \"only one block can be moved at time\"] } } Input Prompt - Step 2 Given the initial scene graph, simulate the following prerequisite actions and update the scene graph accordingly: 1. Move block 0 from column 2 to column 5. 2. Move block 2 from column 1 to column 2. Ensure to track state changes and validate each move. Output - Step 2: Intermediate Scene Graphs After Action 0: Move block 0 from column 2 to column 5 { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_1, level_1\", \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_1\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_2, level_1\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_2\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_3\", 27 \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_2\", \"properties\": {\"color\": \"light_pink\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_3\", \"properties\": {\"color\": \"red\"}} ], \"relationships\": [ {\"relationship\": \"on_top\", \"subject\": \"block_6\", \"object\": \"block_1\"}, {\"relationship\": \"on_top\", \"subject\": \"block_5\", \"object\": \"block_6\"}, {\"relationship\": \"beside\", \"subject\": \"block_2\", \"object\": \"block_7\"}, {\"relationship\": \"beside\", \"subject\": \"block_0\", \"object\": \"block_3\"}, {\"relationship\": \"beside\", \"subject\": \"block_3\", \"object\": \"block_4\"} ], \"action_history\": [ {\"step\": 0, \"action\": \"move_block\", \"affected_entities\": [\"block_0\"], \"state_changes\": [{\"entity\": \"block_0\", \"property\": \"position\", \"old_value\": \"column_2, level_1\", \"new_value\": \"column_5, level_1\"}, {\"entity\": \"block_7\", \"property\": \"position\", \"old_value\": \"column_2, level_2\", \"new_value\": \"column_2, level_1\"}, {\"entity\": \"block_7\", \"property\": \"state\", \"old_value\": \"stacked\", \"new_value\": \"resting\"}], \"validity\": true} ] } After Action 1: Move block 2 from column 1 to column 2 { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_2, level_1\", 28 \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\",\"state\": \"resting\", \"position\": \"column_5, level_1\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_2, level_2\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_2\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_3\", \"type\": \"block\" , \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_2\", \"properties\": {\"color\": \"light_pink\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_3, level_3\", \"properties\": {\"color\": \"red\"}} ], \"relationships\": [ {\"relationship\": \"on_top\", \"subject\": \"block_7\", \"object\": \"block_2\"}, {\"relationship\": \"on_top\", \"subject\": \"block_6\", \"object\": \"block_1\"}, {\"relationship\": \"on_top\", \"subject\": \"block_5\", \"object\": \"block_6\"}, {\"relationship\": \"beside\", \"subject\": \"block_2\", \"object\": \"block_1\"}, {\"relationship\": \"beside\", \"subject\": \"block_0\", \"object\": \"block_3\"}, {\"relationship\": \"beside\", \"subject\": \"block_3\", \"object\": \"block_4\"} ], \"action_history\": [ {\"step\": 0, \"action\": \"move_block\", 29 \"affected_entities\": [\"block_0\"], \"state_changes\": [{\"entity\": \"block_0\", \"property\": \"position\", \"old_value\": \"column_2, level_1\", \"new_value\": \"column_5, level_1\"}, {\"entity\": \"block_7\", \"property\": \"position\", \"old_value\": \"column_2, level_2\", \"new_value\": \"column_2, level_1\"}, {\"entity\": \"block_7\", \"property\": \"state\", \"old_value\": \"stacked\", \"new_value\": \"resting\"}], \"validity\": true}, {\"step\": 1, \"action\": \"move_block\", \"affected_entities\": [\"block_2\"], \"state_changes\": [{\"entity\": \"block_2\", \"property\": \"position\", \"old_value\": \"column_1, level_1\", \"new_value\": \"column_2, level_1\"}, {\"entity\": \"block_7\", \"property\": \"position\", \"old_value\": \"column_2, level_1\", \"new_value\": \"column_2, level_2\"}, {\"entity\": \"block_7\", \"property\": \"state\", \"old_value\": \"resting\", \"new_value\": \"stacked\"}], \"validity\": true} ] } Input Prompt - Step 3 Simulate the following action sequences starting from the intermediate scene graph after Step 2 and evaluate them against the target scene graph: Option A: 1. Move block 2 from column 2 to column 1. 2. Move block 5 from column 3 to column 0. 3. Move block 5 from column 0 to column 1. Option B: 1. Move block 6 from column 3 to column 2. 2. Move block 2 from column 2 to column 1. 3. Move block 5 from column 3 to column 0. 4. Move block 5 from column 0 to column 1. For each option, provide the final scene graph and compute the similarity score based on goal state achievement, constraint satisfaction, solution efficiency, and resource optimization. Output - Step 3: Option Evaluations Option A: { 30 \"scene_graph\": { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_1, level_1\", \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\" , \"state\": \"stacked\", \"position\": \"column_1, level_2\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_3\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_4\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_5\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"red\"}}, {\"name\": \"block_3\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_5, level_1\", \"properties\": {\"color\": \"light_pink\"}} ] }, \"metrics\": { \"similarity_with_target_scene_graph\": 100, \"goal_state_achievement\": 100, \"constraint_satisfaction\": 100, \"solution_efficiency\": 100, \"resource_optimization\": 100 } } Option B: { \"scene_graph\": { \"objects\": [ {\"name\": \"block_2\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_1, level_1\", \"properties\": {\"color\": \"green\"}}, {\"name\": \"block_0\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_2\", \"properties\": {\"color\": \"purple\"}}, {\"name\": \"block_1\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_3\", \"properties\": {\"color\": \"orange\"}}, {\"name\": \"block_5\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_1, level_4\", \"properties\": {\"color\": \"red\"}}, {\"name\": \"block_6\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_2, level_1\", \"properties\": {\"color\": \"blue\"}}, {\"name\": \"block_7\", \"type\": \"block\", \"state\": \"stacked\", \"position\": \"column_2, level_2\", \"properties\": {\"color\": \"yellow\"}}, {\"name\": \"block_3\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_3, level_1\", \"properties\": {\"color\": \"pink\"}}, {\"name\": \"block_4\", \"type\": \"block\", \"state\": \"resting\", \"position\": \"column_4, level_1\", \"properties\": {\"color\": \"light_pink\"}} ] }, \"metrics\": { \"similarity_with_target_scene_graph\": 92, \"goal_state_achievement\": 90, \"constraint_satisfaction\": 95, \"solution_efficiency\": 85, \"resource_optimization\": 88 } }"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of California San Diego",
        "University of Central Florida"
    ]
}