{
    "paper_title": "R1-RE: Cross-Domain Relationship Extraction with RLVR",
    "authors": [
        "Runpeng Dai",
        "Tong Zheng",
        "Run Yang",
        "Hongtu Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE."
        },
        {
            "title": "Start",
            "content": "R1-RE: Cross-Domain Relationship Extraction with RLVR Runpeng Dai1 Tong Zheng2 Run Yang3 Hongtu Zhu1: 1University of North Carolina at Chapel Hill 2University of Maryland, College Park 3BiliBili {runpeng, htzhu}@email.unc.edu tzheng24@umd.edu yangrun@bilibili.com 5 2 0 J 7 ] . [ 1 2 4 6 4 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Relationship extraction (RE) is core task in natural language processing. Traditional approaches typically frame RE as supervised learning problem, directly mapping context to labelsan approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and private MDKG dataset. The R1RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE."
        },
        {
            "title": "Introduction",
            "content": "Relationship extraction (RE) (Zhao et al., 2024) is fundamental task in natural language processing (NLP) that involves either classifying the relationships between pairs of entities (relationship classification) or extracting (subject, relation, object) triples (triplet extraction) from context. RE serves as foundation for numerous downstream applications (Nayak et al., 2021), most notably in knowledge graph (KG) construction (Zhong et al., 2023). KGs have demonstrated broad utility across domains such as biomedical research (Yang et al., 2024) and e-commerce (Li et al., 2020), and are increasingly recognized for their potential to enhance LLMsfor example, through approaches like GraphRAG (Edge et al., 2024) and graph-based reasoning (Luo et al., 2023). Consequently, develFigure 1: Testing accuracy on the MDKG dataset for R1RE-7B trained on the Sem-2010 dataset, compared with other models. Detailed results are provided in Table 3. oping robust and effective RE methods is of growing importance. Standard approaches to RE typically adopt pretraining-and-finetuning paradigm, employing mid-sized pre-trained models such as BART (Lewis et al., 2019) and BERT (Devlin et al., 2019). While these models are lightweight, they require substantial fine-tuning to generalize to out-of-domain scenarios. Recently, there has been growing interest in leveraging LLMs for RE, capitalizing on their rich language understanding (Zeng et al., 2023) and impressive in-context learning capabilities (Brown et al., 2020). In this context, some studies have explored few-shot prompting strategies with API models1 (Wadhwa et al., 2023; Xu et al., 2023), while others have investigated supervised fine-tuning (SFT) methods (Ettaleb et al., 2025) using lighter-weight, open-source LLMs for RE tasks. 1API models refer to proprietary, closed-source models that are typically accessible only via external APIs. As illustrated in Figure 2(a), few-shot learning yields only marginal improvements over the original model. Furthermore, Figure 2(b) demonstrates that the naive SFT approach can boost in-domain performance but provides limited generalization to out-of-domain scenarios, suggesting that SFT primarily promotes memorization rather than genuine annotation ability (Chu et al., 2025). This observation raises critical question: How to enhance the RE capability of small LLMs for robust cross-domain performance? domains such as mathematics and code generation (Guo et al., 2025). We evaluate our proposed method on both public dataset (SemEval-2010 Task 8) and private dataset (MDKG). Our results show that R1-RE improves the OOD performance of Qwen-7B-Instruct by approximately 30%, achieving results comparable to those of leading proprietary models. Further analysis demonstrates that incorporating additional training data can further enhance performance."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Task Definition Let denote the input sentence, and let and represent the predefined sets of entity types and relationship types, respectively. Let denote the annotation guideline for the task, which provides the definitions of and as well as other annotation instructions. We consider two relationship extraction (RE) tasks: Relation Classification (RC) and Triplet Extraction (TE). In this work, we primarily focus on RC tasks; the definition of the TE task is provided in Appendix B. Relationship Classification(RC): In this task, the subject-object pair (esub, eobj) has already been identified from S. The goal is to assign relationship type Y, where is the predefined set of relationship types, based on the sentence and the annotation guideline K. Each sentence contains exactly one entity pair. For instance, Sentence <e1>Counseling interventions</e1> can be effective in preventing <e2>perinatal depression</e2>. Label treatment-for(e1, e2)"
        },
        {
            "title": "2.2 Group Relative Policy Optimization",
            "content": "(GRPO) In this work, we formulate the language generation process of LLMs as sequential Decision Process and optimize the LLM with the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024). Specifically, let πθ denote the LLM with parameters θ. At each training step, given prompt sampled from the dataset D, we use πθ to generate group of candidate outputs, denoted as o1, o2, . . . , oG. For each candidate output, we compute the corresponding reward r1, r2, . . . , rG by comparing the output with the gold standard. The advantage at the t-th token of the i-th output is then Figure 2: (a) Test accuracy on two RE datasets with varying numbers of few-shot examples. (b) In-domain and out-of-domain test accuracy of the SFT model using naive prompt (Figure 3) versus prompt incorporating the annotation guide (Figure 4). To this end, we examined the workflow of human annotators (see Figure 3). Annotators are typically provided with detailed annotation guidelines and iteratively compare the target sentence against these guidelinesformulating hypotheses, verifying them, and ultimately reaching conclusion. While entities and relationship types may vary across domains, the reasoning skills developed through annotation guideline usage are broadly generalizable. Motivated by this intuition, we reconceptualize RE as reasoning task grounded in annotation guidelines. To this end, we modify the standard SFT paradigm by explicitly incorporating the annotation guide into the prompt. As shown in Figure 2, this results in an approximately 10% improvement in out-of-domain RE performance. However, substantial gap remains when compared to proprietary API models. To this end, we propose R1-RE to enhance the reasoning abilities of LLMs for RE tasks. Our framework is inspired by the recent success of reinforcement learning with verifiable reward (RLVR), which has proven effective in promoting reasoning skills in smaller models on challenging calculated as Ai,t Ai ri meanpr1, r2, . . . , rGq stdpr1, r2, . . . , rGq . Let πθold denote the model from the previous training step, and πref denote the original model prior to training. GRPO maximizes the following objective function to optimize πθ: qD,toiuG i1πθold 1 Gÿ min πi,t θ πi,t θold ` β DKL πθ Ai, clipp πref , i1 πi,t θ πi,t θold ȷ 1 oi oiÿ , 1 ε, 1 ` εqAi where πi,t θ πθpoi,t q, oi,1 . . . , oi,t1q, and similarly for πi,t θold. The hyperparameters ε and β control the ratio clipping threshold and the weight of the KullbackLeibler (KL) divergence penalty, respectively. Specifically, ratio clipping mitigates the risk of large, destabilizing policy updates, while the KL penalty constrains the updated policy from deviating excessively from the reference model πref. For additional details on the sequential decision formulation and the GRPO algorithm, please refer to (Yuan et al., 2025; Yu et al., 2025; Yue et al., 2025)."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first introduce new paradigm for relationship extraction inspired by the human annotation process (Section 3.1). We then present the R1-RE framework (Section 3.2) along with its associated reward design (Section 3.3)."
        },
        {
            "title": "3.1 Human-Inspired RE paradigm",
            "content": "As illustrated in Figure 3, existing relationship extraction methods always focus on learning mappings between sentences and golden labels, for example, golden relationships or gold standard annotations. Despite achieving strong in-domain performance, these direct mapping strategies always suffer from poor generalization to out-of-domain scenarios. In contrast, human annotation is inherently multi-step reasoning process. As shown in Figure 3 (right), annotators consult an annotation guidea manual that precisely defines each relation and entity type and provides detailed instructions for edge cases (see Table 2). During annotation, they repeatedly compare the target sentence with these definitions, formulate hypotheses, and iteratively refine their judgments through several inference steps before reaching final decision. While concrete definitions may vary across tasks, this stepby-step reasoning paradigm is universal, making it widely applicable to diverse RE settings. Motivated by this insight, we introduce novel RE framework that explicitly embeds the step-by-step human annotation process into LLM-based relation extraction, thereby bridging the gap between human reasoning and automated learning. Specifically, we introduce the following prompt, designed to guide LLMs toward human-style reasoning in relation extraction. The user provides sentence containing two entities: one enclosed in <e1> </e1> tags and the other in <e2> </e2> tags. Please identify the relationship type and determine the direction of the relationship between these two entities. Below are the definitions and restrictions for all relationship types: {Annotation guide} Please thinks about the reasoning process in the mind and then provides the user with the final answer. The reasoning process and final class are enclosed within <think> </think> and <answer> </answer> tags, respectively. The answer should aligh with format: For example, <answer> Product-Producer(e1,e2) </answer> means e1 is product of e2 while <answer> ProductProducer(e2,e1) </answer> means e2 is product of e1. Always use \"e1\" and \"e2\" in place of the actual entity names. Below is the sentence: {Sentence} Figure 4: The prompt for RC tasks."
        },
        {
            "title": "3.2 R1-RE",
            "content": "Our goal is to elicit human-like annotation behaviour in the LLM-based RE pipeline. Achieving this, however, is far from trivial. Preliminary experiments show that simply prompting an LLM to emulate the annotation procedure yields unsatisfactory results. As illustrated in Figure 6, the model drifts toward shallow, linear chain of thought instead of the richer, multi-step reasoning observed in human annotators. To this end, we propose R1-RE, R1-style reinforcement learning training framework that aligns an LLMs reasoning with the multi-step workflow of human annotators. This is motivated by the recent evidence that reinforcement learning with verifiable reward as discussed in Section 5, can successfully equip LLM with the capability of performing human-like reasoning on wide range of Figure 3: comparison of existing RE training paradigm and the annotation process of human annotators. tasks including math(Shao et al., 2024), coding and QA(Lai et al., 2025). Given the LLM (policy) π, our objective is to incorrect ones. In this subsection, we discuss the reward design for the RC task; details for the TE task are provided in the Appendix. maximize the following expectation: pq,yqD, pyπp qq rppy, yq (1) where denotes the dataset of promptgold label pairs, py represents the output generated by the LLM given the prompt q. denotes the golden standard RE results and rppy, yq is reward function (described in the next section) that measures the quality of the generated answer. We solve this optimization problem using the GRPO algorithm (Shao et al., 2024) as detailed in Subsection 2.2."
        },
        {
            "title": "3.3 Multi-stage Reward design",
            "content": "Reward design is critical aspect of reinforcement learning, as it encodes feedback from the environment to guide the learning process. In this work, we adopt rule-based reward scheme similar to those used in DeepSeek-R1-Zero (Guo et al., 2025) and Logic R1 (Xie et al., 2025). Our reward consists of two components. The first is format reward, which checks whether the models response adheres to specified structure. Since the primary purpose of this component is to facilitate evaluation, we enforce only minimal requirements on the response format. The second component is the accuracy reward. For relation classification, we employ binary rewardassigning positive feedback for correct predictions and negative feedback for Format Reward: The structured prompt template for RC is illustrated in Figure 4. We use regular expression extraction to enforce standardized response format. The model is permitted to reason freely, without restrictions on the number or placement of <think></think> tags. Correctness is determined solely based on the content within the final pair of <answer></answer> tags. Furthermore, we require the answer to be in the form ype1, e2q or ype2, e1q, where denotes one of the predefined relation types. The format reward score rformat is computed as: # rformat 1, 3, if format is correct if format is incorrect Metric Reward: If the response passes the format evaluation, we assign an additional reward based on its RE accuracy. Specifically, we employ rule-based scheme in which the model receives positive reward for correct classifications and negative reward for incorrect ones: # rRC if ytrue ypred 2, 1.5, otherwise This reward design is consistent with standard practices in rule-based reinforcement learning, incentivizing accurate predictions while penalizing errors. Final Reward: The final reward combines format reward (rformat) and task-specific metric reward (rmetric), and is defined as follows: Sentence: <e1>hypothalamus</e1> may The as key brain region involve in the <e2>inflammatory related depressive-like behaviors</e2>. # rformat 2, rformat ` rmetric, if rformat 1 if rformat Relationship: hyponym-of (e1, e2) We omit metric-based evaluation for outputs that do not satisfy the required format (rformat 1), thereby enforcing strict adherence to the format before assessing task-specific performance. Definition of hyponym-of: This relationship can indicate hierarchical link, with being subordinate or specific instance of . . . \"X is hyponym of Y\" has following types: (a) Direct Categorization: . . . (b) Appositive Formulation: . . . (c) Indicators of Inclusion or Example: . . . (d) Alternative Naming: . . . (e) Usage of Colon for Definition: . . ."
        },
        {
            "title": "4 Experiment on Entity Classification",
            "content": "4.1 Dataset In this paper, we consider two relation classification datasets. The first is the public SemEval-2010 Task 8 dataset (Sem2010) (Hendrickx et al., 2019), widely used benchmark for multi-class relation classification. The second is human-annotated proprietary corpus constructed for the Mental Disease Knowledge Graph (MDKG), which is also designed for multi-class relation classification. Key statistics of these two datasets are summarized in Table 1. Sem2010 MDKG Relationships ComponentWhole, InstrumentAgency, etc. 19 8,353 / # of classes Train/Test Hyponym of, Located in, Risk factor of, Treatment for, etc. 19 10,033 / 500 Table 1: Relationship types and train/test split of Sem2010 and MDKG datasets. Number of classes considers relation directions. Each dataset consists of sentencerelationship pairs, along with an annotation guide defining each relationship type. Table 2 presents an example sentencerelationship pair and the corresponding relationship definition."
        },
        {
            "title": "4.2 Main results",
            "content": "The main results are presented in Table 3. Here, R1-RE-7B refers to our proposed method, with all evaluations conducted in zero-shot setting using the prompt shown in Figure 4. All results are reported in terms of Avg@4 accuracy, which denotes the average Pass@1 accuracy across four samples. The key observations are as follows: Table 2: An example sentencegold standard pair from the MDKG dataset, along with the definition of the corresponding relationship from the annotation guide, is shown below. Full relationship definitions for both datasets are provided in Appendix A.1. The reinforcement learning process significantly enhances the relationship extraction capabilities of the base model. R1RE-7B substantially outperforms its backbone (Qwen-2.5-7B-Instruct) in both indomain (`52.9, 53.2) and out-of-domain (`27.9, 30.6) accuracy. Compared to its SFT counterpart, R1-RE demonstrates much stronger out-of-domain generalization, surpassing it by `16.0 and `15.2 points. Both proprietary and open-source models achieve substantially higher accuracy on the Sem-2010 (public) dataset than on the MDKG (private) dataset, suggesting that the public benchmark may suffer from data leakage. R1-RE-7B demonstrates OOD performance on the private MDKG dataset that is comparable to state-of-the-art proprietary models such as GPT-4o and GPT-4.1-mini. The relatively lower performance observed on the Sem-2010 dataset is likely attributable to data leakage, which artificially inflates accuracy of proprietary models."
        },
        {
            "title": "4.3 How R1-RE boosts RE Performance?",
            "content": "In this section, we further investigate key question: How R1-RE improves RE performance? To answer this question, we visualize the training dynamics of R1-RE and conduct human analysis of Model MDKG (Private) Avg@4 Pass@4 Sem-2010 (Open) Avg@4 Pass@ Claude 3.5 Sonnet GPT-4o GPT-4.1-mini Qwen-2.5-7B-Instruct Llama-3-8B-Instruct Qwen-2.5-14B-Instruct Qwen-2.5-32B-Instruct Qwen-2.5-72B-Instruct Proprietary Models 99.2 71.1 0.1 99.4 65.9 0.1 99.6 70.2 0.1 Open Source Models 48.4 35.2 1.2 53.8 22.7 0.0 56.2 41.3 0.8 64.8 49.2 2.1 73.6 55.2 1.5 81.3 0.1 79.8 0.0 81.0 0.2 38.6 1.2 25.6 0.1 54.6 1.4 66.4 1.2 70.2 0. 100 100 99.8 59.8 56.2 74.2 83.8 85.4 RL from Instruct Models Qwen-2.5-7B-Instruct ë R1-RE-7B (MDKG) ë R1-RE-7B (Sem) 88.1 0.3 65.8 0.1 90.8 76. 66.5 0.2 91.8 0.0 84.2 100 SFT from Instruct Models Qwen-2.5-7B-Instruct ë SFT-7B (MDKG) ë SFT-7B (Sem) 90.4 0.2 49.8 0.1 99 65. 51.3 0.4 92.4 0.0 69.0 100 Table 3: Zero-shot relationship classification accuracy of different models on the MDKG and Sem-2010 datasets. R1-RE-7B (MDKG) denotes the model trained on the MDKG dataset; the same naming convention applies to the SFT models. Avg@4 indicates the average Pass@1 accuracy across 4 samples. For fair comparison, all the models use the Template in 4. Out-of-domain accuracies are highlighted in bold. its outputs. Figures 5 track response length, training reward, and both in-domain and out-of-domain (OOD) accuracy over the course of training, while Figure 6 compares outputs from the Qwen-2.5-7BInstruct and R1-RE. We observe two key findings: Key Finding 1: Performance is correlated with both training rewards and response length. Training rewards, response length, indomain accuracy, and out-of-domain (OOD) accuracy all increase simultaneously throughout training. Specifically, compared with around 200 tokens of output at the beginning of the training process, the response length increases to 500/1000 tokens on two datasets, indicating the emergence of long COT and aligns well with the phenomena in the existing literature (Xie et al., 2025). Key Finding 2: R1-RE elicits genuinely human-like annotation behaviour. Unlike the baselines terse, pattern-matching answers, R1-RE-7B first identifies the entities in the context, then systematically compares each candidate relation with the definitions in the annotation guide, following hypothesisvalidation procedure; it finally draws conclusion and outputs the answer. This stepby-step reasoning process closely mirrors the human annotation workflow depicted in Figure 3(b). This also indicates that the increased length shown in Figure 5 is indeed result of learning good annotation paradigms instead of simple overthinking (Sui et al., 2025). Notably, this reasoning patter emerges naturally during the RL training process, without the need for explicit distillation or supervised finetuning (Zheng et al., 2025)."
        },
        {
            "title": "4.4 Further Analysis",
            "content": "Finding: The R1-RE training process preserves the models performance on other tasks. Although R1-RE has acquired strong RC skills through reinforcement learning, its performance on other tasks remains unclear. Specifically, we are interested in whether the RC training process comes at cost, such as reduced generalization or capability in unrelated domains. As highlighted (a) (b) Figure 5: (a) Training dynamics of R1-RE (Sem-2010), with the left y-axis representing the training reward and the right y-axis showing both in-domain and out-of-domain test accuracy. (b) Response length of R1-RE-7B models during training. Figure 6: Case study comparing the chain-of-thought (COT) reasoning of R1-RE-7B and Qwen2.5-7B-Instruct. Due to space constraints, some COT outputs are omitted; the complete COT reasoning process for R1-RE-7B is provided in Appendix A.2. in recent studies (Zhao et al.; Kotha et al., 2023; Dai et al., 2025), when LLMs are fine-tuned to excel at particular skill, they may struggle to maintain performance in other areas. To investigate this trade-off, we benchmark R1-RE on three widely used datasets that evaluate distinct aspects of LLM ability: MATH-500 (mathematical reasoning) (Hendrycks et al., 2021), IFEval (instruction following) (Zhou et al., 2023), and GPQA (factual knowledge) (Rein et al., 2024). The results in Table 4 reveal surprising yet reasonable trend. The RL process not only failed to suppress but actually improved the performance of R1-RE across all three benchmarks. In contrast, supervised fine-tuning (SFT) led to noticeable decline in generalization performance. This finding is consistent with recent studies indicating that SFT tends to promote memorization of training data, thereby impairing out-of-domain generalization, whereas RL-based methods can facilitate better skill transfer and generalization across diverse tasks (Chu et al., 2025). Model Avg@4 (MDKG) Avg@4 (Sem-2010) Qwen-2.5-7B-Instruct ë R1-RE (MDKG) Sem-2018 ë R1-RE (Sem-2010) Sem-2018 35.2 65.8 69.7 38.6 66.5 70.8 Model MATH IFEval GPQA Qwen-2.5-7B R1-RE-7B SFT 73.6 75.4 69.8 72.3 73.0 71.9 29.2 29.6 27.2 Table 4: Performance of Qwen-2.5-7B-Instruct, R1-RE, and SFT models on three benchmarks. The evaluation is conducted with the default setting of the Evalscope framework. Finding: Incorporating additional training data can further improve the OOD performance of R1-RE. Given the strong out-of-domain generalization exhibited by R1-RE on RC tasks, natural question arises: how can we further improve its performance to match or even surpass that of proprietary models? Leveraging the broad availability of public RC datasets, we investigate the effect of incorporating additional data into training. Specifically, we include the SemEval-2018 Task 7 dataset (Buscaldi et al., 2017) (denoted as Sem-2018), training R1RE on the combined dataset and re-evaluating its out-of-domain (OOD) accuracy. Remarkably, the inclusion of Sem-2018 leads to substantial 4% improvement in OOD performance on both the MDKG and Sem-2010 datasets. This result demonstrates the significant potential of our approach to further enhance generalization by integrating diverse and complementary datasets."
        },
        {
            "title": "LLM reasoning",
            "content": "LLM reasoning has garnered significant attention, as it demonstrates the potential of LLMs to generalize to complex real-world problems through human-like reasoning. Early efforts primarily focused on prompting methods, such as chain of thought(Wei et al., 2022) and tree of thought(Yao et al., 2023), to elicit step-by-step Table 5: Out-of-domain (OOD) performance of R1-RE7B after incorporating the additional Sem-2018 dataset. reasoning. More recently, research has shifted toward explicitly training LLMs to master reasoning processes. Initial approaches often relied on reward modelssuch as outcome-based (ORM) or process-based (PRM) reward models (Uesato et al., 2022)but these methods can suffer from issues like reward hacking. Recent advances, such as DeepSeek-R1 (Guo et al., 2025) and Vision-R1 (Huang et al., 2025), have demonstrated that applying Reinforcement Learning with Verifiable Reward (RLVR) can effectively guide LLMs toward self-emergent reasoning without requiring trained reward functions or step-level human annotation. RLVR has been explored across variety of domains, including logic games (Xie et al., 2025), search (Jin et al., 2025), and machine translation (Feng et al., 2025). However, its application to knowledge extraction or relationship extraction tasks remains underexplored. Relationship extraction Early approaches to relationship extraction predominantly adopt supervised classification framework. Pipeline-based methods follow two-step procedure: first applying named entity recognition (NER), then performing relation classification on the identified entity pairs (Cai et al., 2016). Alternatively, span-based methods frame RE as token-level classification task (Eberts and Ulges, 2020). With the advent of LLMs, recent work has begun to leverage their in-context learning capabilities through few-shot learning (Borchert et al., 2023; Xu et al., 2023). Other approaches seek to enhance performance using retrieval-augmented generation (RAG) methods (Wan et al., 2023). In addition, several studies aim to further improve accuracy via supervised finetuning (Wadhwa et al., 2023; Ettaleb et al., 2025; Shi and Luo, 2024)."
        },
        {
            "title": "Limitations",
            "content": "In this work, we primarily focus on relationship classification (RC) tasks and leave the exploration of more complex triplet extraction (TE) tasks for future research. Additionally, our experiments are limited to 7B-parameter models due to computational constraints. Evaluating R1-RE with larger models represents an important direction for future work."
        },
        {
            "title": "References",
            "content": "Philipp Borchert, Jochen De Weerdt, Kristof Coussement, Arno De Caigny, and Marie-Francine Moens. 2023. Core: few-shot company relation classification dataset for robust domain adaptation. arXiv preprint arXiv:2310.12024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Davide Buscaldi, Anne-Kathrin Schumann, Behrang Qasemizadeh, Haïfa Zargayouna, and Thierry Charnois. 2017. Semeval-2018 task 7: Semantic relation extraction and classification in scientific papers. In International Workshop on Semantic Evaluation (SemEval-2018), pages 679688. Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 756765. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Runpeng Dai, Run Yang, Fan Zhou, and Hongtu Zhu. 2025. Breach in the shield: Unveiling the vulnerabilities of large language models. arXiv preprint arXiv:2504.03714. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Mohamed Ettaleb, Véronique Moriceau, Mouna Kamel, and Nathalie Aussenac-Gilles. 2025. The contribution of llms to relation extraction in the economic In The Joint Workshop of the 9th Finanfield. cial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal). Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. 2025. Mt-r1-zero: Advancing llm-based machine translation via r1zero-like reinforcement learning. arXiv preprint arXiv:2504.10160. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, and 1 others. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2019. Semeval-2010 task 8: Multiway classification of semantic relations between pairs of nominals. arXiv preprint arXiv:1911.10422. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Markus Eberts and Adrian Ulges. 2020. Span-based joint entity and relation extraction with transformer pre-training. In ECAI 2020, pages 20062013. IOS Press. Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2023. Understanding catastrophic forgetting in language models via implicit inference. arXiv preprint arXiv:2309.10105. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. 2025. Med-r1: Reinforcement learning for generalizable medical reasoning in visionlanguage models. arXiv preprint arXiv:2503.13939. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. Feng-Lin Li, Hehong Chen, Guohai Xu, Tian Qiu, Feng Ji, Ji Zhang, and Haiqing Chen. 2020. Alimekg: Domain knowledge graph construction and application in e-commerce. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 25812588. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061. Tapas Nayak, Navonil Majumder, Pawan Goyal, and Soujanya Poria. 2021. Deep neural approaches to relation triplets extraction: comprehensive survey. Cognitive computation, 13(5):12151232. Andrew Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 278287. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Somin Wadhwa, Silvio Amir, and Byron Wallace. 2023. Revisiting relation extraction in the era of large language models. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2023, page 15566. Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. Gpt-re: In-context learning for relation extraction using large language models. arXiv preprint arXiv:2305.02105. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Xin Xu, Yuqi Zhu, Xiaohan Wang, and Ningyu Zhang. 2023. How to unleash the power of large language arXiv models for few-shot relation extraction? preprint arXiv:2305.01555. Yue Yang, Kaixian Yu, Shan Gao, Sheng Yu, Di Xiong, Chuanyang Qin, Huiyuan Chen, Jiarui Tang, Niansheng Tang, and Hongtu Zhu. 2024. Alzheimers disease knowledge graph enhances knowledge discovery and disease prediction. bioRxiv. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Yongliang Shen, Xinyin Ma, Yechun Tang, and Weiming Lu. 2021. trigger-sense memory flow framework for joint entity and relation extraction. In Proceedings of the web conference 2021, pages 1704 1715. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Zhengpeng Shi and Haoran Luo. 2024. Cre-llm: domain-specific chinese relation extraction framework with fine-tuned large language model. arXiv preprint arXiv:2404.18085. Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. 2025. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, and 1 others. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641. Yuexiang Zhai, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, and Yi Ma. 2022. Computational benefits of intermediate rewards for goal-reaching policy learning. Journal of Artificial Intelligence Research, 73:847896. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. survey of large language models. Xiaoyan Zhao, Yang Deng, Min Yang, Lingzhi Wang, Rui Zhang, Hong Cheng, Wai Lam, Ying Shen, and Ruifeng Xu. 2024. comprehensive survey on relation extraction: Recent advances and new frontiers. ACM Computing Surveys, 56(11):139. Tong Zheng, Lichang Chen, Simeng Han, Thomas McCoy, and Heng Huang. 2025. Learning to reason via mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817. Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. 2023. comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):162. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Additional Materials on RC task",
            "content": "A.1 Annotation Guide The annotation guide for the MDKG dataset is provided in Table 6, while the guidelines for the SemEval datasets are available on their official website. A.2 Complete reasoning output from R1-RE-7B and Qwen-2.5-7B-Instruct The COT reasoning outputs for an example prompt from R1-RE-7B and Qwen-2.5-7B-Instruct are shown in Table 7 and Table 8, respectively."
        },
        {
            "title": "B Additional Materials on TE task",
            "content": "Triplet Extraction(TE): In this task, the goal is to extract all valid triplets (esub: tsub, y, eobj: tobj) from the sentence S, where tsub, tobj denote the entity types of the subject and object, respectively. sentence may contain multiple triplets. For instance Sentence Olanzapine was also associated with more frequent reports of weight gain and significantly greater VA costs . . . Label [[Olanzapine:drug, risk-factor-of, weight gain:symptom]] B.1 Reward Design Format Reward: For the TE task, the answer should be list of triplets, each in the format pesub:tsub, y, eobj:tobjq, where is valid relationship and tsub, tobj are the corresponding entity types. Metric Reward: The reward design for the TE task is inherently more complex, as it incorporates aspects of the named entity recognition (NER) task, requiring an evaluation of the extracted entities against gold standards. Prior to the emergence of large language models (LLMs), traditional approaches typically adopted strict criterion: an entity was considered correct only if the predicted span exactly matched the ground truth (Eberts and Ulges, 2020). However, such evaluation protocols can be overly rigid when applied to LLMs. As noted by Wadhwa et al. (2023), this strictness often fails to account for the more flexible and nuanced outputs of LLMs. To address this limitation, Wadhwa et al. (2023) employed human judges to provide more accurate and fair assessment of model predictions. In this work, we allow the extracted entity to deviate by one token, either in the front or back, from the gold standard. We propose that more flexible rule can be adopted, such as using LLM as the judge (Gu et al., 2024). We propose two-stage, F1 score-based as follows: rTE went F1ent ` wtri F1tri. Specifically, F1ent denotes the F1 score computed over the extracted entities. To compute F1ent, we extract all unique (entity, entity type) pairs from both the ground truth and the models output, and calculate the precision and recall. F1tri represents the F1 score over the extracted triplets. For F1tri, we directly compare the predicted and goldstandard triplets. We make two key observations regarding our reward design. First, the RL framework enables us to directly employ the F1 score as the reward, thereby eliminating the trainingevaluation gap that arises in conventional approaches which rely on surrogate loss functions (Eberts and Ulges, 2020; Shen et al., 2021). Second, our reward comprises two levels: the entity-level reward acts as an intermediate signal, providing early-stage feedback when the model is not yet capable of extracting complete triplets. This approach is supported both in theory and practice, as intermediate rewards are known to accelerate learning and enhance exploration in RL settings (Ng et al., 1999; Zhai et al., 2022). The triplet-level F1 score, on the other hand, reflects the final objective of the task. To ensure that learning remains focused on the end goal, we assign higher weight to the triplet-level reward (went 1, wtri 3), thereby mitigating the risk of divergence from the main objective."
        },
        {
            "title": "C Prompt Design",
            "content": "The user gives sentence. The Assistant need to extract triplet from the sentence. Only consider the following entity types: {Annotation guide - Entity} Only consider the following relationship types: {Annotation guide - Relationship} The assistant first thinks out load and then provides the user with the final answer, make sure the final answer is enclosed within <answer> </answer> and only appear once. i.e., reasoning process here <answer> answer here </answer>. The answer should be list of triplets, with each triplet have form [object:type, relationship, subject:type]. User: {Sentence} Figure 7: The prompt for TE tasks. risk-factor-of: The relationship descried in entails that the presence of hightens the risk or possibility of Y. \"X is risk-factor-of Y\" has following types: (a) Direct Risk Factor: The statement \"X is risk factor for Y\" indicates that directly increases the risk of occurring. (b) Cause and Effect: Statements like \"X is leading cause for Y\" or \"X is caused/induced by Y\" emphasize Xs causative role in leading to Y. Relevant keywords include \"contribute to\" and \"result in\". (c) Susceptibility and Prevalence: Phrases like \"X is prone to developing Y\", \"X is more prevalent in Y\", \"X predisposes to Y\", \"X is at high risk for Y\", \"higher likelihood of X\", \"increased odds of X\" suggest that increases the likelihood of occurring. Additionally, statements like \"X (a specific characteristic group) has an increased Y\" fall under this category. (d) Predictive Relationship: The phrase \"The happening of is predictor of Y\" indicates that the occurrence of can be used to predict the likelihood of happening in the future. (e) Adverse Effects: is treatment or intervention and is an adverse effect or complication of that treatment. help-diagnose: This relationship type encompasses methods used for diagnosing diseases as well as for observing specific signs, genes, or health factors, and may include diagnostic markers. \"X help-diagnose Y\" has following types: (a) Methods of Measurement and Detection: Tag sentences like \"Use to estimate/measure/detect/evaluate/assess Y\". Here, is used as tool or method to quantify, identify, or assess Y. Other important keywords include based on, according to, diagnosed by, classify, screening, score. (b) Diagnostic Criteria: Phrases such as \"meeting for Y\" suggest that acts as standard or criterion for diagnosing or defining Y. (c) Aiding in Identification and Differentiation: Phrases indicating that can help identify, distinguish, differentiate, discriminate, or classify (a disease), pointing to Xs usefulness in differentiating from other conditions or in making definitive diagnosis of Y. characteristic-of: This relationships describe the symptoms, clinical manifestations, or distinct features of disease. \"X is characteristic-of Y\" has following types: (a) Characteristic Identification: Sentences where is described with phrases like characterized by, symptom of, clinical expression, hallmark, signifying as characteristic or symptom of X. For instance, \"X is characterized by Y\". (b) Manifestation Descriptions: Sentences where X(disease) is noted to present, show, exhibit, report, indicate, demonstrate, or have symptom, namely Y. For example, \"X presents/shows/exhibits Y\". (c) Marker Identification: Sentences where is identified by Y, or is mentioned as marker or biomarker for X, or is more sensitive to Y. For instance, \"X is identified by Y\" or \"Y is marker/biomarker for X\". (d) Accompaniment Patterns: Sentences where is usually accompanied by Y. For example, \"X with/accompanied by Y\". (e) Quantitative Changes: Sentences indicating that is higher, decreased, or increased in (disease). For instance, \"Y is higher/decreased/increased in X\". (f) Observation in Disease: Sentences where was observed, found, or occurs in X. For example, \"Y was observed/found/occur in X\". associated-with: This relationship describes an association or connection between and Y, where changes in may correspond with changes or changes in correspond with X. The associated with relationship doesnt have direction such that associated-with(e1,e2) and associated-with(e2,e1) are equivalent. It not necessarily imply cause-and-effect relationship. treatment-for: This relationship type highlights the link between X(treatments or interventions) and their impact on Y(a specific disease or condition). It includes range of treatment methods like medication, psychotherapy, lifestyle modifications, surgical procedures, and others. \"X is treatment-for Y\" has following types: 1. Application in Treatment: Phrases like \"Use to treat Y\", \"X is treatment/intervention for Y\" or \"Y treated/medicated with X\" indicate the use of in the treatment of Y. 2. Signs of Improvement: Phrases where improves, alleviates, suppresses, shows efficacy in, benefits, prevents, or reverses Y, suggesting beneficial impact of on Y. hyponym-of: This relationship can indicate hierarchical link, with being subordinate or specific instance of Y, or constitutive link, showing that is component or part of Y. \"X is hyponym-of Y\" has following types: (a) Direct Categorization: Sentences where \"X is Y\" are used, signifying that belongs to the category or class of Y. (b) Appositive Formulation: Sentences in the form of \"X, Y\", clearly identifying as type or instance of Y. (c) Indicators of Inclusion or Example: Phrases using include, involve, such as, like, e.g., to demonstrate that is an example or part of the broader category Y. (d) Alternative Naming: The use of \"also called\" to connect with Y, indicating that is another name or term for Y. (e) Usage of Colon for Definition: Sentences that use colon (\":\") to lead into description, often employed to list or define hyponyms or specific instances under broader category. abbreviation-for: This relationship type refers to the use of X(abbreviations or acronyms) to represent Y(longer terms or phrases), especially in medical or technical fields. For example, \"AIDS\" is an abbreviation for \"Acquired Immune Deficiency Syndrome\", and \"MRI\" stands for \"Magnetic Resonance Imaging\". occurs-in: The occurs in relationship applies to scenarios where X(an event, condition, or phenomenon) is observed in Y(groups or settings). Example: \"Condition occurs in populations with attributes Y\". located-in: \"X is located-in Y\" occurs in following three scenarios: (a) Gene or Region Specificity: This relationship applies when discussing the location of genes or specific regions. For instance, \"Gene/region is located in gene/region Y\". (b) Disease, Symptom, or Physiological Activity Location: highlights the physical or anatomical location of medical conditions or biological processes. For example, \"Disease/symptom/physiological activity occurs in region Y\". (c) Localization Indicators for Signs: This relationship also applies to sentences that indicate where signs of particular condition (like disease, gene, region, etc.) are found. For example, \"X of (region/gene/. . . )\". Table 6: The Annotation guide of the MDKG dataset. <think> Lets analyze the given sentence: \"Monitoring prolactinemia and addressing hyperprolactinemia is paramount in women with <e1>schizophrenia</e1> being treated with <e2>prolactin-increasing antipsychotics</e2>.\" We need to determine the relationship between <e1>schizophrenia</e1> and <e2>prolactin-increasing antipsychotics</e2>. Lets break this down step by step. 1. **Identify the entities**: - <e1>schizophrenia</e1> is the disease or condition. - <e2>prolactin-increasing antipsychotics</e2> is type of medication or treatment. 2. **Analyze the relationship**: - The sentence states that schizophrenia is associated with specific type of antipsychotics (prolactin-increasing antipsychotics) being used in the treatment or management of schizophrenia. - We need to determine if this relationship is that schizophrenia leads to or is treated by these antipsychotics, or if its some other relationship. 3. **Apply the definitions**: - **treatment-for**: This relationship would apply if the antipsychotics are used to treat or manage schizophrenia. The sentence mentions that these antipsychotics are used specifically in the context of women with schizophrenia, implying therapeutic use. - **risk-factor-of**: This would apply if the antipsychotics increased the risk of schizophrenia, but here its the other way around; the antipsychotics are being used because of schizophrenia, not the other way. - **associated-with**: While its possible that there could be an association, the sentence is more about the use of these antipsychotics as therapeutic intervention rather than just an association. - **hyponym-of**: This would apply if prolatcin-increasing antipsychotics were subtype or specific example of larger category, but here it seems to be specific type of antipsychotic used for particular condition. - **causes**: This wouldnt apply because the sentence suggests the antipsychotics are being used to treat, not cause, schizophrenia. - **characteristic-of**: This would describe characteristic of the condition, but here we are describing the treatment rather than characteristic of schizophrenia. 4. **Conclusion**: - The sentence clearly states that these antipsychotics are being used in the context of women with schizophrenia. This implies that the antipsychotics are form of treatment specifically for schizophrenia. The key phrase is \"being treated with,\" which indicates that the antipsychotics are used to manage or treat the condition of schizophrenia. Given that the sentence directly states that antipsychotics are used in the treatment of schizophrenia, this fits squarely into the definition of treatment relationship. The antipsychotics are specifically used to address or manage schizophrenia, which is the essence of the treatment-for relationship. Therefore, the relationship here is that the antipsychotics are used to treat schizophrenia. </think> <answer>treatment-for(e2,e1)</answer> Table 7: An example of chain-of-thought (COT) reasoning output from R1-RE-7B <think> The sentence describes an interplay between DISC1 (intrinsic signaling pathway) and GABA signaling (extrinsic signaling pathway) in the context of neurogenesis, which is affected by susceptibility genes. The susceptibility genes here are DISC1 and GABA. The relationship is about how these pathways influence neurogenesis and their impact on neuronal development and risk for mental disorders. The main focus is on how the presence of DISC1 and GABA signaling, which are considered susceptibility pathways, influence neurogenesis. Since susceptibility pathways are risk factors for certain conditions (in this case, potentially mental disorders), we can infer that DISC1 is risk-factor-of neurogenesis in the context of developmental processes and experiences. However, the sentence does not directly indicate that DISC1 is treatment or method for neurogenesis. Therefore, the relationship \"risk-factor-of\" seems most appropriate here, as it captures the role of DISC1 in influencing the risk or likelihood of neurogenesis being affected by susceptibility genes. The direction of the relationship is from DISC1 to neurogenesis, indicating that the presence of DISC1 (as susceptibility pathway) impacts neurogenesis. </think> <answer> risk-factor-of(e1,e2) </answer> Table 8: An example of chain-of-thought (COT) reasoning output from Qwen-2.5-7B-Instruct"
        }
    ],
    "affiliations": [
        "BiliBili",
        "University of Maryland, College Park",
        "University of North Carolina at Chapel Hill"
    ]
}