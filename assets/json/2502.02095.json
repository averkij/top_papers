{
    "paper_title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information",
    "authors": [
        "Bowen Ping",
        "Jiali Zeng",
        "Fandong Meng",
        "Shuo Wang",
        "Jie Zhou",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones."
        },
        {
            "title": "Start",
            "content": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Bowen Ping 1 Jiali Zeng 2 Fandong Meng 2 Shuo Wang 3 Jie Zhou 2 Shanghang Zhang 1 5 2 0 2 4 ] . [ 1 5 9 0 2 0 . 2 0 5 2 : r Abstract Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply steplevel DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones. 1. Introduction in large advancements Recent language models (LLMs) (Zhou et al., 2024; Xiao et al., 2024b;a; Ping et al., 2024) have significantly enhanced their ability to process long text sequences leading models like GPT-4o (OpenAI et al., 2024) can handle context of up to 128K. Despite these advancements, there has been less emphasis on the models ability to generate high-quality long-form text outputs. The capability to produce long-form content is essential for various real-world applications, including writing academic papers, novels, and scripts in literature, generating legal contracts in law, and producing repository-level code 1Peking University, China 2Pattern Recognition Center, WeChat AI, Tencent Inc, China 3Tsinghua University, China. Correspondence to: Shanghang Zhang <shanghang@pku.edu.cn>. 1 Figure 1. The above is outcome supervision in long-form generation tasks. Below is LongDPO uses process supervision with global memory to maintain factual consistency, and external critiques to refine low-reward chosen candidates. in technology (Bai et al., 2024b; Wang et al., 2024c). However, many LLMs still struggle to generate content exceeding 2,000 words, highlighting the need for further advancements in this area. Previous research has investigated methods to extend the output window by constructing long-form training data and utilizing preference learning. For example, Suri (Pham et al., 2024) creates various instructions for the same response and introduces instructional ORPO. LongWriter (Bai et al., 2024b) employs an agent-based pipeline that decomposes ultra-long generation tasks into subtasks to build long-form dataset, followed by supervised fine-tuning and DPO. These approaches primarily rely on outcome supervision (Lightman et al., 2024) during DPO, which provides feedback on the final result, for long-form generation tasks. Nevertheless, outcome supervision is not well-suited for long-form generation due to several key challenges. Specifically, outcome supervision means that intermediate steps are not adequately guided, potentially leading to loss of coherence and inconsistency in the final output (Zhang et al., 2024b). As result, this coarse supervision can produce conLongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information tent that does not fully meet query requirements, leading to problems like length deviations or poor quality (Zhang et al., 2024b; Bai et al., 2024b). The absence of fine-grained feedback prevents the model from learning to improve particular aspects of the output. In this paper, we introduce novel approach called LongDPO for long-form generation using step-level supervision. Our method is divided into two main components: constructing preference data with stepwise supervision signals and implementing stepwise DPO. Specifically, we employ Monte Carlo Tree Search (MCTS) (Browne et al., 2012) with large language model (LLM) acting as judge during the evaluation phase to collect stepwise preference pairs. To maintain consistency in the generated text, we incorporate global memory pool to store factual content from selected nodes during the MCTS search. However, not all chosen candidates in the collected preference pairs are of high quality. To address this issue, we utilize the judge model from the MCTS evaluation phase to provide critiques, followed by critique-augmented generation to refine the chosen candidates. Compared to external critiques, searching for additional solutions is inefficient and may yield only limited performance improvements (Qi et al., 2024). Additionally, relying solely on self-critique, which depends on the models inherent capabilities, can result in unstable performance gains (Qi et al., 2024; Zhang et al., 2024c). Given the high-quality stepwise preference pair data, we propose using stepwise DPO for training to enhance the learning process. As shown in Figure 1, vanilla DPO applies sample-wise supervision directly. Previous work has shown that this approach may result in less distinct reward margin, making learning more difficult (Lai et al., 2024). In contrast, LongDPO uses fine-grained learning at each step, which may yield better results. We evaluate long-form generation capabilities with specific length requirements and the ability to follow complex longform instructions using LongBench-Write-en and LongGenBench (Bai et al., 2024b; Wu et al., 2024c), as well as several general benchmarks such as TruthfulQA (Lin et al., 2022) to assess general task performance. Our method, built on Llamaand Qwen-based backbones, outperforms their DPO versions in long-form generation tasks while maintaining near-lossless performance on general tasks. Our contributions can be summarized as follows: We propose LongDPO, which differs from traditional outcome supervision by adopting process supervision. LongDPO enables step-wise, more fine-grained learning for long-form text generation. To implement process supervision, we introduce MCTS to construct step-level preference data. Specifically, we utilize global memory pool to maintain 2 factual consistency and incorporate external critiques to collect better preference pairs. We validate that our method enhances performance in long-form generation tasks while maintaining nearlossless results for general tasks. Meanwhile, we conduct an in-depth analysis to show that the generated responses align more closely with human preferences. 2. Related Work Long Context LLMs Some studies explore training-free methods to extend the input context window, such as modifications to the model architecture (Peng et al., 2024; Xiao et al., 2024c), position encoding (Ding et al., 2024; Xiao et al., 2024b), and others. Another approach focuses on training-based techniques (Bai et al., 2024a; Munkhdalai et al., 2024; Fu et al., 2024). Many LLMs can support input context windows of 32K or even over 128K words. However, far fewer are capable of generating outputs exceeding 2K words in length. Recent studies (Pham et al., 2024; Bai et al., 2024b) have employed outcome supervision to extend the output window. Most recently, Zhang et al. (2024b) proposed LongReward, which is orthogonal to our work. However, in addition to the instruction and response, it requires an additional reference long document as input, which limits its applicability in both outcome and process supervision. Process Supervision in Preference Learning Recently, scaling inference-time compute has become increasingly popular (Chen et al., 2024b; Setlur et al., 2024; Snell et al., 2024). Process supervision with MCTS can further enhance models reasoning abilities (Tian et al., 2024; Zhang et al., 2024d;a). Recent studies (Wang et al., 2024b; Xu et al., 2024) use MCTS in both math and code tasks. In addition to MCTS, Zhao et al. (2024) also incorporate self-reflection. Cheng et al. (2024) employ tree search and train refiner for iterative optimization. In this work, we primarily focus on exploring the potential of process supervision with MCTS in long-form generation. Use LLM to Critic The LLM-generated critiques are able to provide additional information and have been widely applied (Madaan et al., 2023; Yuan et al., 2024). CriticGPT (McAleese et al., 2024), trained using reinforcement learning, can generate critiques that surpass those produced by humans. Recent studies (Ankner et al., 2024; Ye et al., 2024) use self-generated critiques for each piece of preference data, which are used to train reward models. Yu et al. (2024) further uses an instance-level critiques filter to reduce conflicts. LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Figure 2. The pipeline of LongDPO. LongDPO incorporates process supervision and MCTS to collect stepwise preference data, where the preference data share the same prefix and only one pair is collected at each layer. During the selection phase, LongDPO uses the global memory pool to filter out candidates that may result in inconsistency, then selects the highest-scoring one as the chosen candidate, with another randomly selected as the rejected candidate. During tree expansion, some chosen candidates may have low rewards, LongDPO uses external knowledge to provide critiques for refinement. Then the collected preference pairs are used for step-level DPO training. 3. LongDPO Our method consists of two main parts: 1) collecting stepwise preference data, and 2) using the collected preference data for DPO training. 3.1. Stepwise Preference Data Construction Currently, MCTS has demonstrated its potential in reasoning tasks which employs an additional reward model to better preference data at each reasoning step (Chen et al., 2024a; Xie et al., 2024), enabling 7B models to achieve performance comparable to GPT-o1 (Guan et al., 2025). Intuitively, long-form generation may also be learned by collecting stepwise preference data. We will elaborate on collecting preference data in the following. 3.1.1. OVERVIEW that balances exploration and exploitation. vi denotes the value of the node, and we use the average reward provided by reward model. Expansion: For each node to be expanded, we generate several child nodes using sampling-based algorithm (Holtzman et al.). Evaluation: In terms of evaluating each node, we assess each node using the value provided by reward model, as previous work has demonstrated its effectiveness (Wang et al., 2024b;a). We consider seven principles to evaluate each node. Each principle is rated between 1 and 5, as detailed in Appendix A.1. Back-propagation: We update the parent node using the value of the leaf nodes and also update the parent nodes visit count. MCTS executes four procedures: selection, expansion, evaluation, and back-propagation. To be specific, our tree is executed according to the following: Specifically, given query q, during the expansion phase, the node in layer is represented as st. The newly node st+1 is generated using the Equation 2: Selection: We select the node to be expanded using Equation 1 with global memory pool to filter out inconsistent nodes. (cid:115) CBi = α 2 ln (cid:19) (cid:18) Ni ni + vi, (1) st+1 = πθ(q s1 s2 st), (2) where πθ is the generator, and represents the concatenation operation. In each evaluation phase, its corresponding value is evaluated as: rst+1 = Θ(q s1 s2 st, st+1), (3) where ni and Ni represent the visit count and the parent visit count of the node, respectively. α is scalar where rst+1 is the average reward of the seven principles, Θ is the reward model used to evaluate the reward of st+ 3 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information as the suffix. When reaching each leaf node, the backpropagation phase is executed. At each selection phase, we use Equation 1 along with global memory pool to make selections, as detailed in the next subsection. 3.1.2. PREFERENCE PAIR EXTRACTION To maintain factual consistency as much as possible, we use global memory pool to store relevant factual context. After selecting each node s, the memory pool is also updated accordingly. To be specific, after selecting the node st, we extract the factual content of st using the model Θ. The extracted factual content is denoted as {m1, m2, . . . , mk}. After extracting, we first employ Θ to check the extracted factual contents to ensure they are factually correct as much as possible. We retain only the factual content {m1, m2, . . . , mk} that does not conflict with the internal knowledge of Θ, using the template provided in Appendix A.3. Then, we update the memory correspondingly Mt = Mt1 {m1, m2, . . . , mk}. Next, we will use Mt to select the node st+1. Specifically, after the expansion phase, we visit the nodes in descending order of their average rewards derived using Equation 3. We break the currently visited node scur into chunks of 128 words, resulting in sequence of chunks {chunk1, chunk2, . . . , chunkj} and calculate the similarity score using each mk in Mt as query. simkj = E(mk) E(chunkj)T , (4) where simkj is the similarity score, E(x) represents get the embedding of x, we use gte-Qwen2-1.5B-instruct1 as embedding model. Then, we use the similarity score to filter irrelevant factual content for each mk. Ak = {chunkj simkj δ}, (5) where δ the similarity threshold is set to 0.8. Finally, we use each mk and its corresponding supported factual content Ak to check for any inconsistencies using model Θ using templates in Appendix A.3. Finally, if no inconsistencies are found, we select scur as st+1. Otherwise, we will visit the next candidate node without expanding the current one further. For each layer of the tree, we select one pair for preference learning: the node with the highest average reward and no consistency errors is selected as the chosen candidate swin, while another node is randomly selected as the rejected candidate slose. Different from Zhang et al. (2024b), we focus on global information by using the previously selected node as query, rather than solely focusing on the current local text. On the other hand, we use more powerful model for doublecheck to ensure that the extracted facts are not factually incorrect as much as possible. Figure 3. Reward analysis of the selected candidates, we focus solely on the chosen candidate in each preference pair. On the x-axis, 0-3.0 represents the proportion of candidates with an average reward < 3.0, while 3.0-3.5 represents the proportion of candidates with an average reward 3.0 but < 3.5. Detailed reward distribution can be found in Appendix 6. 3.2. Chosen Candidates Refinement using Critiques After collecting preference pairs for long-form generation, we then randomly select 1,000 pairs and only analyze the average reward of the chosen candidate in each pair, as shown in Figure 3. On the one hand, many of the chosen candidates in each preference pair have low rewards which may lead to suboptimal performance. On the other hand, the large reward discrepancies between different samples could result in unstable training (Wu et al., 2024a). One way to improve performance is by expanding the search space. On the one hand, this is inefficient, especially in the context of long-form generation. On the other hand, recent studies (Brown et al., 2024; Qi et al., 2024) have shown that the gains from this approach are limited. Therefore, we propose leveraging external critiques to guide the generator in text generation, as self-critique relies on the models inherent capabilities. Recent studies have highlighted its instability in driving improvement (Qi et al., 2024; Zhang et al., 2024c). To be specific, we collect the chosen candidates in each preference pair with average rewards below the threshold η for refinement, as shown in Equation 6. Srefinement = {swin rswin η}, (6) 1https://huggingface.co/Alibaba-NLP/ gte-Qwen2-1.5B-instruct where swin and rswin represent the chosen candidate of the collected preference pair and the corresponding average 4 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information reward. We only refine the chosen candidates, set η = 2.5, and have conducted an ablation study. Collect Data for Critiques Generation Srefinement contains the chosen candidates that need to be refined. Next, we prepare the data for the generation of critiques. Specifically, each data is triplet (principleu, ssib, swin), where principleu is used in the evaluation phase in MCTS to assess the reward of each node, swin is the chosen candidate to be refined, and ssib is the sibling node of swin, which serves as an example of refinement as illustrated in Figure 2. Detailed principles are given in Appendix A.1. We construct each pair as the following: for each principleu and swin, if there exists ssib whose reward is greater than swin under principleu, the tuple (principleu, ssib, swin) forms pair to generate critiques. Figure 4. Main body of generated critiques which have detailed in Appedix A. Generate critiques Next, we use the reward model Θ to generate critiques for each triplet using template in Appendix A.2. Figure 4 has shown the main body of the critiques. Analysis,\" Justification,\" and Relevant Text\" are used to enhance the accuracy of the analysis, while the Confidence Score\" helps assess the models confidence in the accuracy of its analysis. Writing Suggestion\" provides recommendations for improvement. Critique-augmented Generation For each swin, we utilize its corresponding critiques {z1, z2, . . . , zλ}, sorted in descending order by Confidence Score,\" to perform critiqueaugmented generation. Specifically, if swin is selected in layer + 1, we rewrite Equation 2 as follows: swin_new = πθ(q s1 s2 st z1 zλ ), (7) where we use each Writing Suggestion\" from zλ , with maximum of three. Then, we use the refined data for DPO training. 5 3.3. LongDPO Training Objective Previous work on outcome supervision in long-form generation directly utilizes the complete chosen and rejected responses for training (Pham et al., 2024; Bai et al., 2024b). LDP = E(q,yw,yl)D (cid:104) log σ(cid:0) β log πθ(ywq) πref (ywq) β log πθ(ylq) πref (ylq) (cid:1)(cid:105) , (8) where yw and yl is the chosen and rejected response, respectively and πref is the reference model. is the pair-wise preference dataset, σ is the sigmoid function, and β controls the degree of deviation from the reference model. In LongDPO, the response is decomposed into = s1 s2 st, where si represents the i-th intermediate result. LongDPO conducts learning at each step. Specifically, for the (i + 1)-th step, swin is the chosen step, slose is the rejected step, and s1i = s1 si has already been learned. LongDPO aims to maximize the probability of swin and minimize the probability of slose. LLongDP = E(qs1i,swin,slose)D (cid:104) log σ(cid:0) β log πθ(swinq s1i) πref (swinq s1i) β log πθ(sloseq s1i) πref (sloseq s1i) (cid:1)(cid:105) . (9) 4. Experimental Results 4.1. Setting Up Setting on Collecting Stepwise Pair We conduct our experiments using LongWriter-llama3.1-8b 2 and LongWriterQwen2.5-7B-Instruct 3. To evaluate text rewards and generate critiques for Eq 7, we utilize Llama-3.1-70B-Instruct 4. For the MCTS tree configuration, we set the maximum depth to 4, with each node generating 4 child nodes during expansion. Each node can contain up to 2048 tokens, and we use decoding temperature of 0.7, along with fixed random seed for reproducibility. Training Setting We randomly sample 2.5k instructions from WildChat (Zhao et al.) to collect stepwise preference pairs, which we then combine with Ultrafeedback (Cui et al., 2024) for DPO training. The learning rate is set to 1e-6, and we employ cosine learning rate scheduler. The maximum sequence length is configured to 32,768 through packing, 2https://huggingface.co/THUDM/ LongWriter-llama3.1-8b 3https://www.modelscope.cn/models/swift/ MS-LongWriter-Qwen2.5-7B-Instruct 4https://huggingface.co/meta-llama/ Llama-3.1-70B-Instruct LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Table 1. Evaluation results on LongBench-Write-en. LongWriter-Llama and LongWriter-Qwen represent LongWriter-llama-8B and LongWriter-Qwen2.5-7B, respectively. Our method improves performance across four output length ranges, particularly for outputs exceeding 4K. We have set random seed to ensure reproducibility. Models [0, 500) [500, 2k) [2k, 4k) [4k, 20k) Average Sl Sq Sl Sq Sl Sq Sl Sq Sl Sq LongWriter-Llama w/ DPO w/ LongDPO LongWriter-Qwen w/ DPO w/ LongDPO 88.10 90.93 90.68 90.80 86.32 88. 86.00 85.78 86.27 87.99 88.23 91.91 74.50 76.67 77.23 84.37 88.71 85.47 86.90 85.46 91.25 89.37 89.16 91. 89.10 90.01 93.35 84.21 89.28 88.63 88.30 90.53 90.53 84.84 84.09 85.60 80.80 81.07 88.25 58.69 60.89 71. 79.20 80.90 85.06 78.13 78.82 85.41 83.12 85.55 87.38 79.51 81.30 83.54 85.10 85.66 88.28 85.08 85.07 88. with random seed set to 42, and training is conducted for 250 steps. We utilize UltraEval (He et al., 2024) and lm-evaluationharness (Gao et al., 2024) for evaluation. Evaluation We evaluate long-form generation capabilities using the following benchmark: LongBench-Write employs two metrics: the length score Sl, which assesses how closely the models generated length matches the required length, and the quality score Sq, which evaluates the quality of the models output using GPT-4o (Bai et al., 2024b). Our evaluation is performed using the English version. LongGenBench (Wu et al., 2024c) evaluates whether models can maintain writing coherence and follow instructions which proposes three metrics to evaluate. Completion Rate (CR) assesses the degree to which all designated subtasks are successfully completed. STIC1 evaluates the models adherence to specific task instructions. STIC-2 provides more granular evaluations, measuring the overall completion of specific task instructions. We use the official scripts for evaluation 5 6. Additionally, we assess the models general abilities using the following: TruthfulQA (Lin et al., 2022) to evaluate the helpfulness of the models response. MMLU (Hendrycks et al., 2021) to evaluate the models multitask processing. We use 5-shot evaluation in our assessment following (Grattafiori et al., 2024) setting. GSM8K (Cobbe et al., 2021) to evaluate the reasoning ability of LLM. We use an 8-shot evaluation following (Grattafiori et al., 2024) setting. Baselines The LongWriter-(.) w/ DPO baseline models are versions of LongWriter-(.) that have been trained using DPO. For each instruction from WildChat (Zhao et al.), we generate four responses. The response with the highest reward is selected as the chosen candidate, while one of the remaining responses is randomly selected as the rejected candidate. Then combine Ultrafeedback for training. 4.2. Main Results The main results are presented in Table 1. Our method significantly outperforms baselines across both the Llama and Qwen series models. Consistent with the results of Bai et al. (2024b), the use of DPO alone did not lead to substantial performance improvement. This could be due to the challenge of maintaining response quality when directly sampling long responses generated by DPO (Cheng et al., 2024). In contrast, our method demonstrates performance gains, likely because fine-grained supervision facilitates the acquisition of high-quality data. To be specific, regarding the length score, LongWriterLlama w/ LongDPO consistently shows improvements across various lengths, generating text that more accurately meets the length requirements. Notably, for outputs exceeding 4,000 words, performance improved by approximately 8%. The quality score results are detailed in Table 6. When comparing LongWriter-Llama and LongWriterLlama w/ DPO, the primary factors contributing to the improved scores of our generated texts are enhancements in Clarity,\" Breadth and Depth,\" and Reading Experience.\" 4.3. Generalization on more long-form and general benchmarks 5https://github.com/THUDM/LongWriter 6https://github.com/mozhu621/LongGenBench Table 2 displays the results of various methods on LongGenBench. For both the Llama and Qwen series models, their 6 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Table 2. Performance comparison across more long-form and general benchmarks. LongGenBench can be used to evaluate output lengths up to 32k. The other benchmarks assess the models performance on more general tasks like helpfulness and reasoning. For TruthfulQA, we report partition MC1\" and MC2\". For each task, all three methods use the same decoding settings, and we have set random seed to ensure reproducibility. Models LongGenBench (16k) LongGenBench (32k) TruthfulQA MMLU GSM8k CR STC1 STC2 CR STC STC2 ACC ACC LongWriter-Llama w/ DPO w/ LongDPO LongWriter-Qwen w/ DPO w/ LongDPO 46.00 64.99 69.38 98.94 95.95 98.51 22.60 25.99 27. 31.39 31.18 33.07 9.80 16.29 18.45 31.02 29.83 32.52 34.50 65.24 66.96 58.67 82.23 84.95 33.60 32.47 32. 33.58 29.02 29.86 10.00 20.39 20.83 18.93 22.33 24.32 38.43 38.17 40.76 45.29 39.29 44.92 56.07 55.68 58. 61.78 57.67 62.75 ACC 63.24 63.30 63.67 74.16 63.67 74.25 ACC 57.70 59.20 61. 83.78 83.85 84.08 Table 3. Ablation on refinement methods and w/o critique\" stands for without critiques meaning MCTS is applied alone. Self-critique\" refers to critiques generated by the model itself. To verify generalization, we set different values of η and report the average result. Methods [0, 500) [500, 2k) [2k, 4k) [4k, 20k) Average Sl Sq Sl Sq Sl Sq Sl Sq Sl Sq LongWriter-Llama w/o critique w/ self-critique w/ LongDPO LongWriter-Qwen w/o critique w/ self-critique w/ LongDPO 88.10 89.69 92.51 90.74 90.80 89.59 90.67 89.36 86.00 87.00 88.15 89.14 87.99 86.99 90.68 91. 75.40 75.46 74.40 76.61 84.37 85.35 83.60 85.48 86.90 89.58 89.81 90.70 89.37 89.01 93.26 92.10 89.10 92.72 90.15 93.46 84.21 88.14 87.46 89. 88.30 89.01 88.48 91.10 84.84 84.31 86.61 87.16 80.80 83.93 83.62 87.77 58.69 63.98 65.20 67.66 79.20 79.51 81.38 81.94 78.13 80.20 78.24 83. 83.12 85.45 85.17 87.14 79.51 81.77 81.73 83.03 85.30 86.27 86.96 88.22 85.08 85.12 87.20 88.40 performance on LongGenBench shows significant improvement. Notably, in terms of CR, this suggests that the model can better follow instructions after being trained with LongDPO. Additionally, using LongDPO results in better performance than DPO. For other tasks, similar trend can be observed: directly applying DPO fails to deliver significant performance improvements and, in some cases, even leads to notable declines. This is particularly evident in the MMLU task, where the performance of LongWriter-Qwen significantly deteriorates after applying DPO. In contrast, our method results in virtually no degradation of the models other capabilities and even leads to slight improvements. This illustrates the generalizability of our approach to tasks beyond long-form generation. et al., 2024; Zhang et al., 2024c). To further verify whether self-generated critiques can effectively collect better preference pairs, we compare self-generated critiques with external critiques in Table 3. We have ensured that the only difference lies in the critic model used between self-critique and LongDPO. To enable more thorough comparison, we set multiple values for η in Equation 6. Specifically, we set η to {2.0, 2.5, 3.0} and report the average performance in Table 3. We detailed the results in Table 7 and 8. Self-critique exhibits performance fluctuations which may be because the generators internal knowledge is insufficient, making it difficult to distinguish high-quality steps in long-form generation. 5. Analysis 4.4. Comparision with Different Critic Methods 5.1. Reliability of Evaluation Self-critique is widely used (Ankner et al., 2024; Ye et al., 2024) to leverage models internal knowledge to provide feedback to provide better solution. However, recent studies have emphasized that relying solely on models internal knowledge can result in unstable performance gains (Qi Reliability on Quality Score We evaluate the consistency of GPT-4o in LongBench-Write based on three evaluation runs and report the variance following (Bai et al., 2024c). Table 9 presents the results of the average quality score, which may indicate that GPT-4o demonstrates good consistency. 7 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Figure 5. case is randomly sampled from LongGenBench. The instruction primarily requires visiting the farmers market starting from week 10 and then every 5 weeks thereafter. On the left, LongWriter-Llama fulfills the requirement in week 10 but fails in week 15. On the right, after applying LongDPO, LongWriter-Llama is able to consistently meet the demands. Table 4. Human evaluation with win rates under three criteria: Diversity, Consistency, and Informativeness"
        },
        {
            "title": "Win\nTie\nLose",
            "content": "65.0 8.30 26.7 61.7 16.7 21.6 61.7 6.70 31.6 to farmers market.\" However, after applying LongDPO, it successfully does so. We find that small number of attention heads have attended to farmers market,\" with over 1% of attention heads scoring above 0.5. However, the LongWriter model does not exhibit similar pattern. This behavior may be linked to retrieval heads (Wu et al., 2024b). We also provide examples in Figure 7 and 8 to show factual correctness after applying LongDPO. Human Evaluation In addition to utilizing GPT-4o, we conduct human evaluation to assess the generated text in terms of diversity, consistency, and informative detailed guidelines can be seen in A.4. We compare the responses generated by LongWriter-Llama and LongWriter-Qwen with those produced by the same models trained using LongDPO. Three independent annotators, who are undergraduate and graduate students, are tasked with comparing the response pairs and evaluating them as win, tie, or lose. The results, present in Table 4, indicate that our responses are rated as superior by the human judges. Additionally, Table 5 shows the agreement among the three judges, demonstrating high level of consistency in their evaluations. 5.2. Case Study Figure 5 presents case sampled from LongGenBench. The instruction primarily requires visiting the farmers market starting from week 10 and then every 5 weeks thereafter. LongWriter-Llama fulfills the requirement in week 10 but fails in week 15. However, after applying LongDPO, it is able to consistently meet the demands. We analyze the attention distribution across models and observe that, in week 15, LongWriter-Llama fails to attend Table 5. Human agreement between different annotators. Judge-1, Judge-2, and Judge-3 are three human judges."
        },
        {
            "title": "Judge",
            "content": "Judge-1 Judge-2 Judge-3 Judge-1 Judge-2 Judge-3 - 61.7 65.0 61.7 - 58. 63.4 61.7 - 6. Conclusion In this paper, we propose LongDPO to enhance long-form generation abilities for LLMs. First, we construct stepwise preference pairs by leveraging process supervision with MCTS, incorporating global memory pool to maintain factual consistency, and utilizing external critiques to refine the chosen candidates which are of low-rewards in the collected preference pairs. Then, LongDPO utilizes step-level preference data for DPO training. This approach enhances performance in long-form generation tasks while maintaining near-lossless performance on several general tasks. Furthermore, we conduct an in-depth analysis to show models trained by LongDPO can generate responses that align more closely with humans. 8 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information"
        },
        {
            "title": "Impact Statement",
            "content": "This work proposes methods for generating high-quality long-form text, which may help make human content creation easier. We have introduced approaches aimed at improving the factual accuracy of the generated content. We believe that long-form generation is promising direction worth exploring which may have an impact on legal, code content generation."
        },
        {
            "title": "References",
            "content": "Ankner, Z., Paul, M., Cui, B., Chang, J. D., and Ammanabrolu, P. Critique-out-loud reward models, 2024. URL https://arxiv.org/abs/2408.11791. Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong, Y., and Li, J. LongAlign: recipe for long In Alcontext alignment of large language models. Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 13761395, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-emnlp.74. Bai, Y., Zhang, J., Lv, X., Zheng, L., Zhu, S., Hou, L., Dong, Y., Tang, J., and Li, J. Longwriter: Unleashing 10,000+ word generation from long context llms, 2024b. URL https://arxiv.org/abs/2408.07055. Bai, Y., Zhang, J., Lv, X., Zheng, L., Zhu, S., Hou, L., Dong, Y., Tang, J., and Li, J. Longwriter: Unleashing 10,000+ word generation from long context https://openreview.net/forum?id= llms. kQ5s9Yh0WI, 2024c. OpenReview submission. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):143, 2012. doi: 10.1109/TCIAIG.2012.2186810. Chen, G., Liao, M., Li, C., and Fan, K. Alphamath almost zero: Process supervision without process, 2024a. URL https://arxiv.org/abs/2405.03553. Chen, W., Yuan, J., Qian, C., Yang, C., Liu, Z., and Sun, M. Optima: Optimizing effectiveness and efficiency for llm-based multi-agent system, 2024b. URL https:// arxiv.org/abs/2410.08115. Cheng, J., Liu, X., Wang, C., Gu, X., Lu, Y., Zhang, D., Dong, Y., Tang, J., Wang, H., and Huang, M. Spar: Selfplay with tree-search refinement to improve instructionfollowing in large language models, 2024. URL https: //arxiv.org/abs/2412.11605. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Cui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y., Xie, G., Xie, R., Lin, Y., Liu, Z., and Sun, M. ULTRAFEEDBACK: boosting language models with scaled AI feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=BOorDpKHiJ. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. Longrope: Extending LLM context window beyond 2 million tokens. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=ONOtpXLqqw. Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y., and Peng, H. Data engineering for scaling language models to 128k context. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=TaAqeo7lUh. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, 9 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information E., Smith, E. M., Radenovic, F., Guzmán, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., Çelebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, 10 D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519. Munkhdalai, T., Faruqui, M., and Gopal, S. Leave no context behind: Efficient infinite context transformers with infini-attention, 2024. URL https://arxiv.org/ abs/2404.07143. He, C., Luo, R., Hu, S., Zhao, R., Zhou, J., Wu, H., Zhang, J., Han, X., Liu, Z., and Sun, M. UltraEval: lightweight platform for flexible and comprehensive evaluation for LLMs. In Cao, Y., Feng, Y., and Xiong, D. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 247257, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.23. URL https: //aclanthology.org/2024.acl-demos.23. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In International Conference on Learning Representations. Lai, X., Tian, Z., Chen, Y., Yang, S., Peng, X., and Jia, J. Step-dpo: Step-wise preference optimization for longchain reasoning of llms, 2024. URL https://arxiv. org/abs/2406.18629. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=v8L0pN6EOi. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 32143252. Association for Computational Linguistics, 2022. doi: 10.18653/ V1/2022.ACL-LONG.229. URL https://doi.org/ 10.18653/v1/2022.acl-long.229. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback, 2023. URL https:// arxiv.org/abs/2303.17651. McAleese, N., Pokorny, R. M., Uribe, J. F. C., Nitishinskaya, E., Trebacz, M., and Leike, J. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. OpenAI, :, Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., adry, A., Baker-Whitcomb, A., Beutel, A., Borzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A., Paino, A., Renzin, A., Passos, A. T., Kirillov, A., Christakis, A., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A., Crookes, A., Tootoochian, A., Tootoonchian, A., Kumar, A., Vallone, A., Karpathy, A., Braunstein, A., Cann, A., Codispoti, A., Galu, A., Kondrich, A., Tulloch, A., Mishchenko, A., Baek, A., Jiang, A., Pelisse, A., Woodford, A., Gosalia, A., Dhar, A., Pantuliano, A., Nayak, A., Oliver, A., Zoph, B., Ghorbani, B., Leimberger, B., Rossen, B., Sokolowsky, B., Wang, B., Zweig, B., Hoover, B., Samic, B., McGrew, B., Spero, B., Giertler, B., Cheng, B., Lightcap, B., Walkin, B., Quinn, B., Guarraci, B., Hsu, B., Kellogg, B., Eastman, B., Lugaresi, C., Wainwright, C., Bassin, C., Hudson, C., Chu, C., Nelson, C., Li, C., Shern, C. J., Conger, C., Barette, C., Voss, C., Ding, C., Lu, C., Zhang, C., Beaumont, C., Hallacy, C., Koch, C., Gibson, C., Kim, C., Choi, C., McLeavey, C., Hesse, C., Fischer, C., Winter, C., Czarnecki, C., Jarvis, C., Wei, C., Koumouzelis, C., Sherburn, D., Kappler, D., Levin, D., Levy, D., Carr, D., Farhi, D., Mely, D., Robinson, D., Sasaki, D., Jin, D., Valladares, D., Tsipras, D., Li, D., Nguyen, D. P., Findlay, D., Oiwoh, E., Wong, E., Asdar, E., Proehl, E., Yang, E., Antonow, E., Kramer, E., Peterson, E., Sigler, E., Wallace, E., Brevdo, E., Mays, E., Khorasani, F., Such, F. P., Raso, F., Zhang, F., von Lohmann, F., Sulit, F., Goh, G., Oden, G., Salmon, G., Starace, G., Brockman, G., Salman, H., Bao, H., Hu, H., Wong, H., Wang, H., Schmidt, H., Whitney, H., Jun, H., Kirchner, H., de Oliveira Pinto, H. P., Ren, H., Chang, H., Chung, H. W., Kivlichan, I., OConnell, I., OConnell, I., Osband, I., Silber, I., Sohl, I., Okuyucu, I., Lan, I., Kostrikov, I., Sutskever, I., Kanitscheider, I., Gulrajani, I., Coxon, J., Menick, J., Pachocki, J., Aung, J., Betker, J., Crooks, J., Lennon, J., Kiros, J., Leike, J., Park, J., Kwon, J., Phang, J., Teplitz, J., Wei, J., Wolfe, J., Chen, J., Harris, J., Varavva, J., Lee, J. G., Shieh, J., Lin, J., Yu, J., Weng, J., Tang, J., Yu, J., Jang, J., Candela, J. Q., Beutler, J., Landers, J., Parish, J., Heidecke, J., Schulman, J., Lachman, J., McKay, J., Uesato, J., Ward, J., Kim, J. W., Huizinga, J., Sitkin, J., Kraaijeveld, J., Gross, J., Kaplan, J., Snyder, J., Achiam, J., Jiao, J., Lee, J., Zhuang, J., Harriman, J., Fricke, K., Hayashi, K., Singhal, K., Shi, K., Karthik, K., Wood, K., Rimbach, K., Hsu, K., Nguyen, K., Gu-Lemberg, K., Button, K., Liu, K., Howe, K., Muthukumar, K., Luther, K., Ahmad, L., Kai, L., Itow, L., Workman, L., Pathak, L., Chen, L., Jing, L., Guy, L., 11 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Fedus, L., Zhou, L., Mamitsuka, L., Weng, L., McCallum, L., Held, L., Ouyang, L., Feuvrier, L., Zhang, L., Kondraciuk, L., Kaiser, L., Hewitt, L., Metz, L., Doshi, L., Aflak, M., Simens, M., Boyd, M., Thompson, M., Dukhan, M., Chen, M., Gray, M., Hudnall, M., Zhang, M., Aljubeh, M., Litwin, M., Zeng, M., Johnson, M., Shetty, M., Gupta, M., Shah, M., Yatbaz, M., Yang, M. J., Zhong, M., Glaese, M., Chen, M., Janner, M., Lampe, M., Petrov, M., Wu, M., Wang, M., Fradin, M., Pokrass, M., Castro, M., de Castro, M. O. T., Pavlov, M., Brundage, M., Wang, M., Khan, M., Murati, M., Bavarian, M., Lin, M., Yesildal, M., Soto, N., Gimelshein, N., Cone, N., Staudacher, N., Summers, N., LaFontaine, N., Chowdhury, N., Ryder, N., Stathas, N., Turley, N., Tezak, N., Felix, N., Kudige, N., Keskar, N., Deutsch, N., Bundick, N., Puckett, N., Nachum, O., Okelola, O., Boiko, O., Murk, O., Jaffe, O., Watkins, O., Godement, O., Campbell-Moore, O., Chao, P., McMillan, P., Belov, P., Su, P., Bak, P., Bakkum, P., Deng, P., Dolan, P., Hoeschele, P., Welinder, P., Tillet, P., Pronin, P., Tillet, P., Dhariwal, P., Yuan, Q., Dias, R., Lim, R., Arora, R., Troll, R., Lin, R., Lopes, R. G., Puri, R., Miyara, R., Leike, R., Gaubert, R., Zamani, R., Wang, R., Donnelly, R., Honsby, R., Smith, R., Sahai, R., Ramchandani, R., Huet, R., Carmichael, R., Zellers, R., Chen, R., Chen, R., Nigmatullin, R., Cheu, R., Jain, S., Altman, S., Schoenholz, S., Toizer, S., Miserendino, S., Agarwal, S., Culver, S., Ethersmith, S., Gray, S., Grove, S., Metzger, S., Hermani, S., Jain, S., Zhao, S., Wu, S., Jomoto, S., Wu, S., Shuaiqi, Xia, Phene, S., Papay, S., Narayanan, S., Coffey, S., Lee, S., Hall, S., Balaji, S., Broda, T., Stramer, T., Xu, T., Gogineni, T., Christianson, T., Sanders, T., Patwardhan, T., Cunninghman, T., Degry, T., Dimson, T., Raoux, T., Shadwell, T., Zheng, T., Underwood, T., Markov, T., Sherbakov, T., Rubin, T., Stasi, T., Kaftan, T., Heywood, T., Peterson, T., Walters, T., Eloundou, T., Qi, V., Moeller, V., Monaco, V., Kuo, V., Fomenko, V., Chang, W., Zheng, W., Zhou, W., Manassra, W., Sheu, W., Zaremba, W., Patil, Y., Qian, Y., Kim, Y., Cheng, Y., Zhang, Y., He, Y., Zhang, Y., Jin, Y., Dai, Y., and Malkov, Y. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language In The Twelfth International Conference on models. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=wHBfxhZu1u. Pham, C., Sun, S., and Iyyer, M. Suri: Multi-constraint instruction following in long-form text generation. In AlOnaizan, Y., Bansal, M., and Chen, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 17221753. Association for Computational Linguistics, 12 2024. URL https://aclanthology.org/2024. findings-emnlp.94. Ping, B., Wang, S., Wang, H., Han, X., Xu, Y., Yan, Y., Chen, Y., Chang, B., Liu, Z., and Sun, M. Delta-come: Training-free delta-compression with mixed-precision for large language models, 2024. URL https://arxiv. org/abs/2406.08903. Qi, Z., Ma, M., Xu, J., Zhang, L. L., Yang, F., and Yang, M. Mutual reasoning makes smaller llms stronger problemsolvers, 2024. URL https://arxiv.org/abs/ 2408.06195. Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers for llm reasoning, 2024. URL https://arxiv.org/ abs/2410.08146. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. Tian, Y., Peng, B., Song, L., Jin, L., Yu, D., Mi, H., and Yu, D. Toward self-improvement of llms via imagination, searching, and criticizing, 2024. URL https: //arxiv.org/abs/2404.12253. Wang, A., Song, L., Tian, Y., Peng, B., Yu, D., Mi, H., Su, J., and Yu, D. Litesearch: Efficacious tree search for llm, 2024a. URL https://arxiv.org/abs/ 2407.00320. Wang, X., Song, L., Tian, Y., Yu, D., Peng, B., Mi, H., Huang, F., and Yu, D. Towards self-improvement of llms via mcts: Leveraging stepwise knowledge with curriculum preference learning, 2024b. URL https: //arxiv.org/abs/2410.06508. Wang, Y., Guo, Q., Yao, W., Zhang, H., Zhang, X., Wu, Z., Zhang, M., Dai, X., Zhang, M., Wen, Q., Ye, W., Zhang, S., and Zhang, Y. Autosurvey: Large language models can automatically write surveys, 2024c. URL https://arxiv.org/abs/2406.10252. Wu, J., Xie, Y., Yang, Z., Wu, J., Gao, J., Ding, B., Wang, X., and He, X. β-dpo: Direct preference optimization with dynamic β, 2024a. URL https://arxiv.org/ abs/2407.08639. Wu, W., Wang, Y., Xiao, G., Peng, H., and Fu, Y. Retrieval head mechanistically explains long-context factuality, 2024b. URL https://arxiv.org/abs/2404. 15574. LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Wu, Y., Hee, M. S., Hu, Z., and Lee, R. K.-W. Spinning the golden thread: Benchmarking long-form generation in language models. arXiv preprint arXiv:2409.02076, 2024c. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. Infllm: Training-free longcontext extrapolation for llms with an efficient context memory, 2024a. URL https://arxiv.org/abs/ 2402.04617. Zhang, J., Hou, Z., Lv, X., Cao, S., Hou, Z., Niu, Y., Hou, L., Dong, Y., Feng, L., and Li, J. Longreward: Improving long-context large language models with ai feedback, 2024b. URL https://arxiv.org/abs/ 2410.21252. Zhang, Q., Qiu, H., Wang, D., Qian, H., Li, Y., Zhang, T., and Huang, M. Understanding the dark side of llms intrinsic self-correction, 2024c. URL https: //arxiv.org/abs/2412.14959. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads, 2024b. URL https://arxiv.org/abs/2410.10819. Zhang, X., Du, C., Pang, T., Liu, Q., Gao, W., and Lin, M. Chain of preference optimization: Improving chainof-thought reasoning in llms, 2024d. URL https:// arxiv.org/abs/2406.09136. Zhao, W., Ren, X., Hessel, J., Cardie, C., Choi, Y., and Deng, Y. Wildchat: 1m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations. Zhao, Y., Yin, H., Zeng, B., Wang, H., Shi, T., Lyu, C., Wang, L., Luo, W., and Zhang, K. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. URL https://arxiv.org/abs/2411.14405. Zhou, Z., Li, C., Chen, X., Wang, S., Chao, Y., Li, Z., Wang, H., An, R., Shi, Q., Tan, Z., Han, X., Shi, X., Liu, Z., and Sun, M. Llmmapreduce: Simplified long-sequence processing using large language models, 2024. URL https://arxiv.org/abs/2410.09342. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 711, 2024. OpenReview.net, 2024c. URL https:// openreview.net/forum?id=NG7sS51zVF. Xie, Y., Goyal, A., Zheng, W., Kan, M.-Y., Lillicrap, T. P., Kawaguchi, K., and Shieh, M. Monte carlo tree search boosts reasoning via iterative preference learning, 2024. URL https://arxiv.org/abs/2405.00451. Xu, B., Lin, Y., Li, Y., and Gao, Y. Sra-mcts: Self-driven reasoning augmentation with monte carlo tree search for code generation, 2024. URL https://arxiv.org/ abs/2411.11053. Ye, Z., Greenlee-Scott, F., Bartolo, M., Blunsom, P., Campos, J. A., and Gallé, M. Improving reward models with synthetic critiques, 2024. URL https://arxiv. org/abs/2405.20850. Yu, Y., Chen, Z., Zhang, A., Tan, L., Zhu, C., Pang, R. Y., Qian, Y., Wang, X., Gururangan, S., Zhang, C., Kambadur, M., Mahajan, D., and Hou, R. Self-generated critiques boost reward modeling for language models, 2024. URL https://arxiv.org/abs/2411.16646. Yuan, W., Liu, P., and Gallé, M. LLMCrit: Teaching large language models to use criteria. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 7929 7960, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.472. URL https://aclanthology. org/2024.findings-acl.472/. Zhang, D., Huang, X., Zhou, D., Li, Y., and Ouyang, W. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b, 2024a. URL https://arxiv.org/abs/2406.07394. 13 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information A. Appendix A.1. Reward Evaluation Templates Reward Evaluation Template You are an expert at evaluating the quality of text. As an impartial evaluator, please assess the assistants response to users requirements. Now, you will receive specific principles that provide the criteria for evaluating the response. Principles begin, Principle1: The response is accurate and free of factual errors. Principle2: The response meets the users purpose and needs. Principle3: The response is non-toxic and safe. Principle4: The response meets the users formatting requirements and maintains logical consistency. Principle5: The response contains diverse and comprehensive information with minimal repetition. Principle6: The response provides an excellent reading experience. Principle7: The response is insightful and provides the user with additional avenues for thought. Principles end. In the next, you will receive detailed guidelines to help you rate the response according to each principle. Now, guidelines begin 5: perfect response with no improvement needed. The content is comprehensive, accurate, clear, and wellstructured. The response fully addresses all aspects of the question or need without any omissions or errors. 4: very good response with minor issues. It is almost perfect but may have slight areas that could be improved, such as minor details that are unclear or small omission. Overall, it still meets the need effectively. 3: An acceptable response that generally meets the question or need but has noticeable shortcomings. The content might be incomplete or unclear, or there may be minor grammar or logical errors. It needs improvement but is still functional. 2: response with significant issues that requires substantial improvement. The content is incomplete, unclear, or contains major errors, omissions, or misunderstandings. It does not fully satisfy the request. 1: completely inadequate response that fails to meet the question or need. It contains serious errors or misunderstandings and cannot provide useful help. Guidelines end. Now, you will receive the user request and the assistants response to evaluate. <User Request> $INST$ </User Request> <Response> $RESPONSE$ </Response> Your task is to evaluate the quality of the response and assign rating with distinguishable differentiation for each principle. When rating, please carefully read the guidelines and ensure your ratings fully adhere to them. You must first provide brief analysis of its quality, then determine the weights for each Principle, for example {\"Principle1\": [0.2,0.2,0.2,0.2,0.2]} represents the final score is 0.2 * 1 + 0.2 * 2 + 0.2 * 3 + 0.2 * 4 + 0.2 * 5 = 3. The output must strictly follow the JSON format: {\"Analysis\":..., \"Principle1\":[..,..,..,..,..], \"Principle2\":[..,..,..,..,..], \"Principle3\":[..,..,..,..,..], \"Principle4\":[..,..,..,..,..], \"Principle5\":[..,..,..,..,..], \"Principle6\":[..,..,..,..,..], \"Principle7\":[..,..,..,..,..]}. You do not need to consider whether the response meets the users length requirements in your evaluation. Ensure that only one integer or float is output for each principle. 14 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information A.2. Templates for Generate Critiques Templates for Generate Critiques You are an expert at evaluating the quality of text. In the following, you will revice user request, one principle and two candidates: <User Request> $INST$ </User Request> <Principle> $PRINCIPLE$ </Principle> <Candidate1> $CANDIDATE1$ </Candidate1> <Candidate2> $CANDIDATE2$ </Candidate2> Now, your task is 1. Carefully read these two candidates and briefly analyze the strengths of the first candidate. 2. Provide \"Justification\" explaining why it scores higher. 3. Assign \"Confidence Score\" on scale of 1 to 5, where 1 indicates you are quite uncertain, and 5 indicates you are very confident. 4. Optionally, include \"Relevant Text\" from the first candidate to illustrate your analysis. 5. Summarize briefly in 1-2 sentences with \"Writing Suggestion\" based on the evaluation. The output must strictly follow the JSON format: {\"Analysis\":..., \"Justification\":..., \"Writing Suggestion\":..., \"Confidence Score\":...,\"Relevant Text\":...}. Ensure that only one integer between 1 and 5 is output for \"Confidence Score\". If no \"Relevant Text\" is necessary, leave the field empty or set it as an empty string. A.3. Templates for Check Consistency Template for Finding Fact Youre an expert in natural language processing and information retrieval. You will receive response. Your task is to extract factual statements from the response provided. Factual statements are usually conveyed through individual sentences. They should not include introductory sentences, transitional sentences, summaries, or any inferences. If factual statement is missing subject or contains pronouns like \"he/she/it/these/those,\" the subject must be explicitly added, or the pronoun must be clarified based on the context. Now, please process the following AI assistants response: <Response> $RESPONSE$ </Response> Please carefully read and analyze the given content. Then, breaking the factual content. After extracting each factual information, you must first determine the \"Validity\" whether it contradicts your internal knowledge, where \"True\" indicates contradiction, \"False\" indicates no contradiction, and \"Unsure\" means uncertain. Provide the relevant \"Evidence\" accordingly. Then, output the result in the following format: {\"Analysis\":..., \"Fact1\":{\"Content\":...,\"Validity\":...,\"Evidence\":...}, \"Fact2\":{\"Content\":...,\"Validity\":...,\"Evidence\":...},...}. Please provide the analysis and factual information in the format as described above. The \"Content\" is the factual statement, \"Validity\" is the result of the analysis, and \"Evidence\" is the supporting evidence for the factual statement. 15 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Template for Judge Inconsistency You are an expert at evaluating text. You will receive factual statements along with related response. Your task is to carefully evaluate whether the response contradicts the factual statement. Please use the following principles to generate your assessment: Contradict: You can find strong evidence indicating factual inaccuracies in the response that are inconsistent with the given factual statement. Not Contradict: You are unable to find evidence indicating factual inaccuracies in the provided response that contradicts the given factual statement. Ensure that you do not use any information or knowledge beyond the response provided, and only check whether the statement is supported by the response. Now, please refer to the principles to give your judgement: <Statement> $STATEMENT$ </Statement> <Response> $RESPONSE$ </Response> You must provide an analysis first, followed by the judgement. The output must strictly follow the JSON format: {\"Analysis\":..., \"Judgement\":...,\"Evidence\":...}. A.4. Guidelines for Human Annotation Guidelines for Human Annotation 1. Diversity: Which text is more diverse in content? This can be evaluated holistically, considering factors such as the lexical variety, the richness of semantics, the complexity of writing style, and the diversity in article structure. 2. Consistency: Which text demonstrates higher degree of consistency? This can be assessed holistically, considering factors such as thematic coherence, ensuring the central theme remains clear; logical coherence, reflected in the natural flow of ideas; and factual consistency, verified through accurate and reliable information. 3. Informative: Which text is more informative in content? This can be evaluated holistically, considering factors such as the accuracy of the information presented, the comprehensiveness in covering all relevant aspects, the clarity of explanations, and the ease of readability and understanding. A.5. More Evaluation Results Table 6. Detailed quality score for length exceeding 4000 in LongBench-Write-en. LongWriter-Llama +DPO +LongDPO LongWriter-Qwen +DPO +LongDPO Sq 79.20 80.90 85.06 78.13 78.81 85.41 Relevance Accuracy Coherence Clarity Breadth and Depth Reading Experience 90.90 93.75 93.75 83.33 85.41 91.67 87.50 83.33 85. 81.25 81.25 91.67 84.48 77.08 85.42 83.33 83.33 83.33 81.89 77.08 81.25 77.08 81.25 83.33 59.48 83.33 87. 68.75 85.41 83.33 71.55 70.83 77.08 75.00 70.83 79.16 16 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Table 7. Results on changing η using llama-based backbones LongWriter-Llama [0, 500) [500, 2k) [2k, 4k) [4k, 20k) Average Sl Sq Sl Sq Sl Sq Sl Sq Sl Sq Self-critique +η 2.0 +η 2.5 +η 3. LongDPO +η 2.0 +η 2.5 +η 3.0 94.07 93.08 90.38 92.01 90.68 89.51 88.97 88.48 87.01 92.91 86.27 88.23 72.39 76.43 74. 72.55 77.23 80.04 87.99 91.04 90.41 91.45 91.25 89.39 86.86 91.66 91.94 93.35 93.35 93.68 89.39 88.54 87. 93.75 90.53 89.01 82.72 84.63 83.50 88.86 88.25 86.19 80.55 82.35 81.25 80.20 85.06 80.55 84.01 86.45 85. 86.69 87.38 86.47 86.72 87.60 86.54 89.57 88.19 86.80 Table 8. Results on changing η using Qwen-based backbones LongWriter-Qwen [0, 500) [500, 2k) [2k, 4k) [4k, 20k) Average Sl Sq Sl Sq Sl Sq Sl Sq Sl Sq Self-critique +η 2.0 +η 2.5 +η 3.0 LongDPO +η 2.0 +η 2.5 +η 3.0 88.71 91.96 91. 87.84 88.93 91.32 88.23 91.66 92.15 91.45 91.91 90.19 84.45 83.16 83.20 86.21 85.47 84.75 93.54 92.91 93. 92.15 91.25 92.91 86.37 88.94 87.06 91.35 88.63 88.82 84.46 86.36 89.01 86.86 85.60 89.01 64.88 67.69 63. 66.85 71.14 64.99 78.47 79.16 77.08 82.59 85.41 81.51 81.10 82.93 81.16 83.06 83.54 82.47 86.17 87.52 87. 88.26 88.54 88.51 Table 9. Evaluated Models and the average Sq Scores. We evaluate LongWriter-Llama + LongDPO and LongWriter-Qwen + LongDPO, while Bai et al. (2024c) report the remaining results. Evaluated Models Sq 87.7 0.5 Claude 3.5 Sonnet 86.6 0.4 GPT-4 Turbo 90.3 0.3 GPT-4o mini 91.8 0.5 GPT-4o 85.5 0.4 GLM-4-9B-chat 70.6 0.3 Llama-3.1-8B-Instruct 80.3 0.3 Llama-3.1-70B-Instruct 88.3 0.4 Mistral-Large-Instruct 53.5 0.5 Suri-I-ORPO 82.2 0.4 LongWriter-Llama LongWriter-Llama + LongDPO 88.2 0.5 LongWriter-Qwen + LongDPO 88.6 0.5 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Figure 6. Detailed reward analysis of the chosen candidates. Figure 7. The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly. 18 LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information Figure 8. The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "Peking University, China",
        "Tsinghua University, China"
    ]
}