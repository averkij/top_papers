{
    "paper_title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
    "authors": [
        "Zichen Jeff Cui",
        "Hengkai Pan",
        "Aadhithya Iyer",
        "Siddhant Haldar",
        "Lerrel Pinto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 2 2 9 1 2 1 . 9 0 4 2 : r DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control Zichen Jeff Cui Hengkai Pan Aadhithya Iyer Siddhant Haldar Lerrel Pinto"
        },
        {
            "title": "Abstract",
            "content": "Imitation learning has proven to be powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through behavior In this work, we present DynaMo, new in-domain, selfcloning objective. supervised method for learning visual representations. Given set of expert demonstrations, we jointly learn latent inverse dynamics model and forward dynamics model over sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io."
        },
        {
            "title": "Introduction",
            "content": "Learning visuo-motor policies from human demonstrations is an exciting approach for training difficult control tasks in the real world [15]. However, key challenge in such learning paradigm is to efficiently learn policy with fewer expert demonstrations. To address this, prior works have focused on learning better visual representations, often by pretraining on large Internet-scale video datasets [611]. However, as shown in Dasari et al. [12], these out-of-domain representations may not transfer to downstream tasks with very different embodiments and viewpoints from the pretraining dataset. An alternative to using Internet-pretrained models is to train the visual representations in-domain on the demonstration data collected to solve the task [13, 4]. However, in-domain datasets are much smaller than Internet-scale data. This has resulted in the use of domain-specific augmentations [13] to induce representational invariances with self-supervision or to collect larger amounts of demonstrations [2, 14]. The reliance of existing methods on large datasets might suggest that in-domain self-supervised pretraining is ineffective for visuo-motor control, and we might be better with simply training end-to-end. In this work, we argue the contrary in-domain self-supervision can be effective with better training objective that extracts more information from small datasets. Corresponding author. Email: jeff.cui@nyu.edu 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Figure 1: (a) We present DynaMo, new self-supervised method for learning visual representations for visuomotor control. DynaMo exploits the causal structure in demonstrations by jointly learning the encoder with inverse and forward dynamics models. DynaMo requires no augmentations, contrastive sampling, or access to ground truth actions. This enables downstream policy learning using limited in-domain data across simulated and real-world robotics tasks. For each environment, we pretrain the visual representation in-domain with DynaMo and learn policy on the pretrained embeddings. (b) We provide real-world rollouts of policies learned with DynaMo representation on our multi-task xArm Kitchen and Allegro Manipulation environments. Prevalent approaches for using self-supervision in downstream control often make bag-of-frames assumption, using contrastive methods [15, 16] or masked autoencoding [11, 8] on individual frames for self-supervision. Most of these approaches ignore rich supervision signal: action-based causality. Future observations are dependent on past observations, and unobserved latent actions. Can we obtain good visual representation for control by simply learning the dynamics? In fact, this idea is well-established in neuroscience: animals are thought to possess internal models of the motor apparatus and the environment that facilitate motor control and planning [1724]. In this work, we present Dynamics Pretraining for Visuo-Motor Control (DynaMo), new selfsupervised method for pretraining visual representations for visuomotor control from limited indomain data. DynaMo jointly learns the encoder with inverse and forward dynamics models, without access to ground truth actions [25, 26]. To demonstrate the effectiveness of DynaMo, we evaluate our representation on four simulation suites - Franka Kitchen [27], Block Pushing [28], Push-T [3], and LIBERO [29], as well as eight robotic manipulation tasks on two real-world environments. Our main findings are summarized below: 1. DynaMo exhibits an overall 39% improvement in downstream policy performance over prior state-of-the-art pretrained and self-supervised representations, especially on the harder closed-loop control tasks in Block Pushing and Push-T  (Table 1)  , and on real robot experiments  (Table 2)  . 2. DynaMo is compatible with various policy classes, can be used to fine-tune pretrained weights, and works in the low-data regime with limited demonstrations on real-world Allegro hand (Tables 4, 5, and 2 respectively). 3. Through an ablation analysis, we study the impact of each component in DynaMo on downstream policy performance (4.6). 2 All of our datasets, and training and evaluation code will be made publicly available. Videos of our trained policies can be seen here: https://dynamo-ssl.github.io."
        },
        {
            "title": "2 Background",
            "content": "2.1 Visual imitation learning Our work follows the general framework for visual imitation learning. Given demonstration data = {(ot, at)}t, where ot are raw visual observations and at are the corresponding ground-truth actions, we first employ visual encoder fθ : ot st to map the raw visual inputs to lowerdimensional embeddings st. We then learn policy π(atst) to predict the appropriate actions. For rollouts, we model the environment as Markov Decision Process (MDP), where each subsequent observation ot+1 depends on the previous observation-action pair (ot, at). We assume the actionconditioned transition distribution p(ot+1ot, at) to be unimodal for our manipulation tasks. 2.2 Visual pretraining for policy learning Our goal is to pretrain the visual encoder fθ using dataset of sequential raw visual observations = {ot}t to support downstream policy learning. During pretraining, we do not assume access to the ground-truth actions {at}t. Prior work has shown that pretraining encoders on large out-of-domain datasets can improve downstream policy performance [611]. However, such pretraining may not transfer well to tasks with different robot embodiments [12]. Alternatively, we can directly pretrain the encoder in-domain using self-supervised methods. One approach is contrastive learning with data augmentation priors, randomly augmenting an image twice and pushing their embeddings closer. Another approach is denoising methods, predicting the original image from noise-degraded sample (e.g. by masking [11, 8, 30]). third approach is contrastive learning with temporal proximity as supervision, pushing temporally close frames to have similar embeddings [31, 32]."
        },
        {
            "title": "3 DynaMo",
            "content": "Limitations of prior self-supervised techniques: Prior self-supervised techniques can learn to fixate on visually salient features and ignore fine-grained features important for control. We illustrate this limitation using the Block Pushing environment from Florence et al. [28]. In this task, the goal is to push block into target square. While the robot arm occupies much of the raw pixel space, the blocks are central to the task despite being smaller in the visual field. Figure 2 visualizes random frame from the demonstration data and its 20 nearest neighbors in the embedding space learned by several self-supervised techniques. We observe that prior self-supervised methods (details in 4.2) focus on the visually dominant robot, matching the whole robot arm extremely accurately. However, they fail to capture the block positions, which are important to the task despite being much less salient visually. Can we learn visual encoder that extracts task-specific features better? We know that the demonstrations are sequential: each observation is dependent on the previous observation, and an action (unobserved in this setting). Prior self-supervised methods ignore this sequential structure. Contrastive augmentations [16, 33] and autoencoding objectives [30, 8, 11] assume that the demonstration video is bag of frames, discarding temporal information altogether. Temporal contrast [32, 31] uses temporal proximity but discards the sequential information in the observations: the contrastive objectives are usually symmetric in time, disregarding past/future order. Instead of contrastive or denoising objective, we propose dynamics prediction objective that explicitly exploits the sequential structure of demonstration observations. Overview of DynaMo: The key insight of our method is that we can learn good visual representation for control by modeling the dynamics on demonstration observations, without requiring augmentations, contrastive sampling, or access to the ground truth actions. Given sequence of raw Figure 2: Embedding nearest neighbor matches for DynaMo, BYOL, MoCo, and TCN on the Block Pushing environment. (Top) The nearest neighbor matches visualized in pixel space. (Bottom) Matches visualized in top-down view. We see that the DynaMo representation captures task-relevant features (end effector, block, and target locations in this case), whereas prior work fixates on the large robot arm. Figure 3: Architecture of DynaMo. DynaMo jointly learns an image encoder, an inverse dynamics model, and forward dynamics model with forward dynamics prediction loss. visual observations (o1, . . . , oT ), we jointly train the encoder fθ : ot st, latent inverse dynamics model q(zt:t+h1st:t+h), and forward dynamics model p(ˆst+1:t+hst:t+h1, zt:t+h1). We model the actions as unobserved latents, and train all models end-to-end with consistency loss on the forward dynamics prediction. For our experiments, we use ResNet18 [34] encoder, and causally masked transformer encoders [35] for the inverse and forward dynamics models. The architecture is illustrated in Figure 3. 3.1 Dynamics as visual self-supervised learning objective First, we sample an observation sequence ot:t+h of length and compute its representation st:t+h = fθ(ot:t+h). For convenience, we will write st:t+h as s:h, and st+1:t+h as s1:h below. At any given step, the distribution of possible actions can be multimodal [5]. Therefore, the forward dynamics transition p(s1:hs:h1) can also have multiple modes. To address this, we first model the inverse dynamics q(z:h1s:h), where zt is the latent transition between frames. We assume zt to be welldetermined and unimodal given consecutive frames {st, st+1}. We have Rm, Rd, such that the latent cannot trivially memorize the next frame embedding. Finally, we concatenate (st, zt) and predict the one-step forward dynamics p(ˆs1:hs:h1, z:h1). We compute dynamics loss Ldyn(ˆs, s) on the one-step forward predictions ˆst+1:t+h, where t+1:t+h are the target next-frame embeddings; and covariance regularization loss Lcov from Bardes et al. [36] on minibatch of observation embeddings S: Ldyn(ˆst, ) = 1 ˆst, ˆst2 2 Lcov(S) = (cid:88) [Cov(S)]2 i,j (1) 1 i=j = Ldyn + λLcov For environments with multiple views, we compute loss over each view separately and take the mean. We choose λ = 0.04 following Bardes et al. [36] for the total loss L. We find that covariance regularization slightly improves downstream task performance. Naively, this objective admits constant embedding solution. To prevent representation collapse, for Ldyn(ˆs, s), we follow SimSiam [37] and set the target embedding := sg(st), where sg is the stop gradient operator. Alternatively, our objective is also compatible with target from momentum encoder fθ [33, 16], We train all three models end-to-end with the objective in Eq. 1, and use the encoder for downstream control tasks. := st = fθ(ot), where θ is an exponential moving average of θ."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our dynamics-pretrained visual representation on suite of simulated and real benchmarks. We compare DynaMo representations with pretrained representations for vision and control, as well as other self-supervised learning methods. Our experiments are designed to answer the following questions: (a) Does DynaMo improve downstream policy performance? (b) Do representations trained with DynaMo work on real robotic tasks? (c) Is DynaMo compatible with different policy classes? (d) Can pretrained weights be fine-tuned in domain with DynaMo? (e) How important is each component in DynaMo? 4.1 Environments and datasets We evaluate DynaMo on four simulated benchmarks and two real robot environments (depicted in Figure 4). We provide brief description below with more details included in Appendix A. (a) Franka Kitchen [27]: The Franka Kitchen environment consists of seven simulated kitchen appliance manipulation tasks with 9-dimensional action space Franka arm and gripper. The dataset has 566 demonstration trajectories, each completing three or four tasks. The observation space is RGB images of size (224, 224) from fixed viewpoint. We evaluate for 100 rollouts and report the mean number of completed tasks (maximum 4). (b) Block Pushing [28]: The simulated Block Pushing environment has two blocks, two target areas, and robot pusher with 2-dimensional action space (end-effector translation). Both the blocks and targets are colored red and green. The task is to push the blocks into either sameor opposite-colored targets. The dataset has 1 000 demonstration trajectories. The observation is RGB images of size (224, 224) from two fixed viewpoints. We evaluate for 100 rollouts and report the mean number of blocks in targets (maximum 2). (c) Push-T [3]: The environment consists of pusher with 2-dimensional action space, T-shaped rigid block, and target area in green. The task is to push the block to cover the target area. The dataset has 206 demonstration trajectories. The observation space is top-down view of the environment, rendered as RGB images of size (224, 224). We evaluate for 100 rollouts and report the final coverage of the target area (maximum 1). (d) LIBERO Goal [29]: The environment consists of 10 manipulation tasks with 7dimensional action space simulated Franka arm and gripper. The dataset has 500 demonstration trajectories in total, 50 per task goal. The observation space is RGB images of size (224, 224) from fixed external camera, and wrist-mounted camera. We evaluate 5 Figure 4: We evaluate DynaMo on four simulated benchmarks - Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal, and two real-world environments - Allegro Manipulation, and xArm Kitchen. goal-conditioned policy for 100 rollouts in total, 10 per task goal, and report the average success rate (maximum 1). (e) Allegro Manipulation: real-world environment with an Allegro Hand attached to Franka arm. We evaluate on three tasks: picking up sponge (6 demonstrations), picking up teabag (7 demonstrations), and opening microwave (6 demonstrations). The observation space is RGB images of size (224, 224) from fixed external camera. The action space is 23-dimensional, consisting of the Franka pose (7), and Allegro hand joint positions (16). (f) xArm Kitchen: real-world multi-task kitchen environment with an xArm robot arm and gripper. The environment consists of five manipulation tasks. The dataset includes 65 demonstrations across five tasks. The observation space is RGB images of size (128, 128) from three fixed external cameras, and an egocentric camera attached to the gripper. The action space is 7-dimensional with the robot end effector pose and the gripper state. 4.2 Does DynaMo improve downstream policy performance? We evaluate each representation by training an imitation policy head on the frozen embeddings, and reporting the downstream task performance on the simulated environments. We use VectorQuantized Behavior Transformer (VQ-BeT) [1] for the policy head. For xArm Kitchen, we use goal-conditioned BAKU [38] with VQ-BeT action head. MAE-style baselines (VC-1, MVP, MAE) use ViT-B backbone. All other baselines and DynaMo use ResNet18 backbone. For environments with multiple views, we concatenate the embeddings from all views for the downstream policy. Further training details are in Appendix B. Table 1 provides comparisons of DynaMo pretrained representations with other self-supervised learning methods, and pretrained weights for vision and robotic manipulation: Random, ImageNet, R3M: ResNet18 with random, ImageNet-1K, and R3M [9] weights. VC-1: Pretrained weights from Majumdar et al. [11]. MVP: Pretrained weights from Xiao et al. [8]. BYOL: BYOL [16] pretraining on demonstration data. BYOL-T: BYOL + temporal contrast [32]. Adjacent frames ot, ot+1 are sampled as positive pairs, in addition to augmentations. MoCo-v3: MoCo [33] pretraining on demonstration data. RPT: RPT [39] trained on observation tokens. TCN: Time-contrastive network [31] pretraining on demonstrations. MV: multi-view objective; SV: single view objective. MAE: Masked autoencoder [30] pretraining on demonstrations. DynaMo: DynaMo pretraining on demonstrations. The best pretrained representation is underlined and the best self-supervised representation is bolded. We find that our method matches prior state-of-the-art visual representations on Franka Kitchen, and outperforms all other visual representations on Block Pushing, Push-T, and LIBERO Goal. 6 Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks."
        },
        {
            "title": "Pretrained\nrepresentations",
            "content": "Self-supervised methods"
        },
        {
            "title": "Method",
            "content": "Random ImageNet R3M VC-1 MVP BYOL BYOL-T MoCo-v3 RPT TCN-MV TCN-SV MAE DynaMo Franka Kitchen ( /4 ) Block Pushing ( /2 ) Push-T ( /1 ) LIBERO Goal ( /1 ) 3.32 3.01 2.84 2.63 2.31 3.75 3.33 3.28 3.54 2.41 2.70 3.64 0.07 0.12 0.11 0.05 0.00 0.09 0.16 0.03 0.52 0.07 0.07 0.00 0.65 0.07 0.41 0.49 0.38 0.20 0.23 0.34 0.57 0.56 0.07 0.07 0. 0.80 0.93 0.89 0.91 0.88 0.28 0.28 0.70 0.17 0.69 0.76 0.59 0.93 Table 2: We evaluate DynaMo on eight tasks across two real-world environments: Allegro Manipulation, and xArm Kitchen. Results are presented as (successes/total). We observe that DynaMo significantly outperforms prior representation learning methods on real tasks. Allegro xArm Kitchen Task BYOL BYOL-T MoCo-v3 DynaMo Sponge Tea Microwave Put yogurt Get yogurt Put ketchup Get tea Get water 2/10 1/10 2/10 4/5 0/5 5/5 2/5 0/5 4/10 0/10 3/ 4/5 4/5 3/5 2/5 0/5 5/10 2/10 1/10 2/5 4/5 5/5 3/5 3/5 7/10 5/10 9/10 5/5 5/5 4/5 5/5 3/5 4.3 Do representations trained with DynaMo work on real robotic tasks? We evaluate the representations pre-trained with DynaMo on two real-world robot environments: the Allegro Manipulation environment, and the multi-task xArm Kitchen environment. For the Allegro environment, we use k-nearest neighbors policy [40] and initialize with ImageNet-1K features for all pretraining methods, as the dataset is relatively small with around 1 000 frames per task. In the xArm Kitchen environment, we use the BAKU [38] architecture for goal-conditioned rollouts across five tasks. For our real-robot evaluations, we compare DynaMo against the strongest performing baselines from our simulated experiments (see Table 1). The results are reported in Table 2. We observe that DynaMo outperforms the best baseline by 43% on the single-task Allegro hand and by 20% on the multi-task xArm Kitchen environment. Additionally, as shown in Table 3, DynaMo exceeds the performance of pretrained representations by 50% on the Allegro hand. These results demonstrate that DynaMo is capable of learning effective robot representations in both single-task and multi-task settings. 4.4 Is DynaMo compatible with different policy classes? On the Push-T environment [3], we compare all pretrained representations across four policy classes: VQ-BeT [1], Diffusion Policy [3], MLP (with action chunking [2]), and k-nearest neighbors with locally weighted regression [40]. We present the results in Table 4. We find that DynaMo representa7 Table 4: We evaluate the compatibility of DynaMo with different policy classes for downstream policy learning on the Push-T simulated benchmark. We report the final target coverage achieved (maximum 1) and demonstrate that DynaMo significantly outperforms prior representation learning methods across all policy classes."
        },
        {
            "title": "Method",
            "content": "VQ-BeT Diffusion MLP (chunking) kNN"
        },
        {
            "title": "Pretrained\nrepresentations",
            "content": "Self-supervised methods Random ImageNet R3M VC-1 MVP BYOL BYOL-T MoCo v3 RPT TCN-SV MAE DynaMo 0.07 0.41 0.49 0.38 0.20 0.23 0.34 0.57 0.56 0.07 0.07 0.66 0.04 0.73 0.63 0.63 0. 0.40 0.50 0.67 0.62 0.14 0.06 0.73 0.07 0.24 0.27 0.22 0.11 0.11 0.16 0.30 0.30 0.07 0.07 0.35 0.01 0.09 0.08 0.07 0.08 0.04 0.04 0.07 0.07 0.01 0.02 0.12 Table 5: We evaluate the ability of DynaMo to finetune an ImageNet-pretrained ResNet-18 encoder across 4 benchmarks. We demonstrate that using pretrained encoder can further improve the performance of DynaMo. Representation Franka Kitchen ( /4 ) Block Pushing ( /2 ) Push-T ( /1 ) LIBERO Goal ( /1 ) ImageNet DynaMo (random init) DynaMo (ImageNet fine-tuned) 3.01 3.64 3.82 0.12 0.65 0.67 0.41 0.66 0.50 0.93 0.93 0.90 tions improve downstream policy performance across policy classes compared to prior state-of-the-art representations. We also note that our representation works on the robot hand in 4.3 with nearest neighbor policy. 4.5 Can pretrained weights be fine-tuned in domain with DynaMo? Table 3: Pretrained baselines on Allegro We fine-tune an ImageNet-1K-pretrained ResNet18 with DynaMo for each simulated environment, and evaluate with downstream policy performance on the frozen representation as described in 4.2. The results are shown in Table 5. We find that DynaMo is compatible with ImageNet initialization, and can be used to fine-tune out-of-domain pretrained weights to further improve in-domain task performance. We also note that our method works in the low-data regime with ImageNet initialization on the real Allegro hand in Table 2. ImageNet R3M DynaMo 4/10 1/10 7/10 1/10 1/10 5/10 Method Sponge 0/10 5/10 9/10 Tea Microwave 4.6 How important is each component in DynaMo? In Table 6, we ablate each component in DynaMo and measure its impact on downstream policy performance on our simulated benchmarks. Forward dynamics prediction: We replace the one-step forward prediction target 1:h with the same-step target given st, we replace the forward dynamics input (s:h1, z:h1) with only z:h1. The ablated objective is essentially variant of autoencoding st. We observe that removing forward dynamics prediction degrades performance across environments. :h1. To prevent the model from trivially predicting 8 Table 6: Ablation analysis of downstream performance relative to the full architecture (100%)"
        },
        {
            "title": "Kitchen Block",
            "content": "Push-T LIBERO No forward No inverse No bottleneck No cov. reg. No stop grad. Short context 34% 72% 92% 94% 1% 100% 8% 35% 22% 62% 5% 75% 44% 97% 9% 85% 9% 88% 33% 41% 75% 59% 0% 89% Table 7: Variants with ground truth actions, downstream performance relative to the base model (100%)"
        },
        {
            "title": "Kitchen Block",
            "content": "Push-T LIBERO Inverse dynamics only DynaMo + action labels 100% 97% 54% 29% 70% 94% 11% 86% Inverse dynamics to transition latent: As described in 3.1, the forward dynamics loss assumes that the transition is unimodal and requires an inferred transition latent. We observed that removing the latent from the forward dynamics input results in significant performance drop. Bottleneck on the transition latent dimension: For the transition latent and the observation embedding s, we find that having dim dim stabilizes training. Here we set dim := dim s, and find that our model can still learn reasonable representation in some environments, but training can destabilize, leading to high variance in downstream performance. Covariance regularization: We find that covariance regularization from Bardes et al. [36] improves performance across environments. Training still converges without it, but the downstream performance is slightly worse. Stop gradient on target embeddings: We observe that removing techniques like momentum encoder [33, 16] and stop gradient [37] leads to representation collapse [41, 16, 36]. Observation context: The dynamics objective requires at least 2 frames of observation context. For Franka Kitchen, we find that context of 2 frames works best. For the other environments, longer observation context (5 frames) improves downstream policy performance. Details of hyperparameters used for DynaMo visual pretraining can be found in Appendix B.1. 4.7 Variants with access to ground truth actions In Table 7, we compare with two variants of DynaMo where we assume access to ground truth action labels during visual encoder training. Only inverse dynamics to ground truth actions: as proposed in Brandfonbrener et al. [26], we train the visual encoder by learning an inverse dynamics model to ground truth actions, with covariance regularization, and without forward dynamics. Full model + inverse dynamics to ground truth actions: we train the full DynaMo model plus an MLP head to predict the ground truth actions given the transition latents inferred by the inverse dynamics model. We observe that in both cases, having access to ground truth actions during visual pretraining does not seem to improve downstream policy performance. We hypothesize that this is because the downstream policy already has access to the same actions for imitation learning."
        },
        {
            "title": "5 Related works",
            "content": "This work builds on large body of research on self-supervised visual representations, learning from human demonstrations, neuroscientific basis for learning dynamics for control, predictive models for decision making, learning from videos for control, and visual pretraining for control. 9 Self-supervised visual representations: Self-supervised visual representations have been widely studied since the inception of deep learning. There are several common approaches to self-supervised visual representation learning. One approach is to recover the ground truth from noise-degraded samples using techniques like denoising autoencoders [42, 43] and masked modeling [44, 45, 30]. Another approach is contrastive learning, which leverages data augmentation priors [41, 16, 33, 36, 37] or temporal proximity [31, 46] to produce contrastive sample pairs. third self-supervised method is generative modeling [4749], which learns to sequentially generate the ground truth data. More recently, self-supervision in the latent space rather than the raw pixel space has proven effective, as seen in methods that predict representations in latent space [50, 51]. Learning from demonstrations: Learning from human demonstrations is well-established idea in robotics [5255]. With the advances in deep learning, recent works such as [3, 2, 5, 4, 1, 56] show that imitation learning from human demonstrations has become viable approach for training robotic policies in simulated and real-world settings. Neural basis for learning dynamics: It is widely believed that animals possess internal dynamics models that facilitate motor control. These models learn representations that are predictive of sensory inputs for decision making and motor control [5760]. Early works such as [1720] propose that there exists an internal model of the motor apparatus in the cerebellum for motor control and planning. [21, 22] propose that the central nervous system uses forward models that predict motor command outcomes and model the environment. Learning forward and inverse dynamics models also helps with generalization to diverse task conditions [23, 24]. Predictive models for decision making: Predictive model learning for decision making is wellestablished in machine learning. Learning generative models that can predict sequential inputs has achieved success across many domains, such as natural language processing [61], reinforcement learning [6264], and representation learning [46, 65]. Incorporating the prediction of future states as an intrinsic reward has also been shown to improve reinforcement learning performance [6668]. Moreover, recent work demonstrates that world models trained to predict environment dynamics can enable planning in complex tasks and environments [6973]. Learning from video for control: Videos provide rich spatiotemporal information that can be leveraged for self-supervised representation learning [7479]. These methods have been extended to decision-making through effective downstream policy learning [711, 6]. Further, recent work also enables learning robotic policies directly from in-domain human demonstration videos by incorporating some additional priors [8084], as well as learning behavioral priors from actionless demonstration data [8587]. Visual representation for control: Visual representation learning for control has been an active area of research. Prior work has shown that data augmentation improves the robustness of learned representations and policy performance in reinforcement learning domains [88, 89]. Additionally, pretraining visual representations on large out-of-domain datasets before fine-tuning for control tasks has been shown to outperform training policies from scratch [10, 12, 9, 11, 90, 8, 91]. More recent work has shown that in-domain self-supervised pretraining improves policy performance [9295] and enables non-parametric downstream policies [40]."
        },
        {
            "title": "6 Discussion and Limitations",
            "content": "In this work, we have presented DynaMo, self-supervised algorithm for robot representation learning that leverages the sequential nature of demonstration data. DynaMo incorporates predictive dynamics modeling to learn visual features that capture the sequential structure of demonstration observations. During pretraining, DynaMo jointly optimizes the visual encoder with dynamics models to extract task-specific features. These learned representations can then be used for downstream control tasks, leading to more efficient policy learning compared to prior approaches. We believe that training DynaMo on larger unlabeled datasets could potentially improve generalization. Additionally, while promising for control tasks, more research is needed to evaluate DynaMos effectiveness on robotic manipulation outside of lab settings."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Ademi Adeniji, Alex Wang, Gaoyue Zhou, Haritheja Etukuru, Irmak Güzey, Mahi Shafiullah, Nikhil Bhattasali, Raunaq Bhirangi, Seungjae (Jay) Lee, and Ulyana Piterbarg for their valuable feedback and discussions. This work was supported by grants from Honda, Google, NSF award 2339096 and ONR awards N00014-21-1-2758 and N00014-22-1-2773. LP is supported by the Packard Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. arXiv preprint arXiv:2403.03181, 2024. 1, 6, 7, 10, 21 [2] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 1, 7, 10 [3] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. 2, 5, 7, 10, 17 [4] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022. 1, 10 [5] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning modes with one stone. Advances in neural information processing systems, 35:2295522968, 2022. 1, 4, 10 [6] Annie Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions from\" in-the-wild\" human videos. arXiv preprint arXiv:2103.16817, 2021. 1, 3, 10 [7] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. 10 [8] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. 2, 3, 6, 10 [9] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. 6, 10 [10] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. In international conference on machine learning, pages 1735917371. PMLR, 2022. [11] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 3, 6, 10 [12] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In Conference on Robot Learning, pages 11831198. PMLR, 2023. 1, 3, 10 [13] Sridhar Pandian Arunachalam, Irmak Güzey, Soumith Chintala, and Lerrel Pinto. Holo-dex: Teaching dexterity with immersive mixed reality. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 59625969. IEEE, 2023. 1 [14] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 1 11 [15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. [16] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 2, 3, 5, 6, 9, 10 [17] Daniel Wolpert, Zoubin Ghahramani, and Michael Jordan. An internal model for sensorimotor integration. Science, 269(5232):18801882, 1995. 2, 10 [18] Daniel Wolpert, Chris Miall, and Mitsuo Kawato. Internal models in the cerebellum. Trends in cognitive sciences, 2(9):338347, 1998. [19] Shidara, Kawano, Gomi, and Kawato. Inverse-dynamics model eye movement control by purkinje cells in the cerebellum. Nature, 365(6441):5052, 1993. [20] Shigeru Kitazawa, Tatsuya Kimura, and Ping-Bo Yin. Cerebellar complex spikes encode both destinations and errors in arm movements. Nature, 392(6675):494497, 1998. 10 [21] Chris Miall and Daniel Wolpert. Forward models for physiological motor control. Neural networks, 9(8):12651279, 1996. 10 [22] Michael Jordan and David Rumelhart. Forward models: Supervised learning with distal teacher. Cognitive Science, 16(3):307354, 1992. 10 [23] Randall Flanagan and Alan Wing. The role of internal models in motion planning and control: evidence from grip force adjustments during movements of hand-held loads. Journal of Neuroscience, 17(4):15191528, 1997. 10 [24] Masahiko Haruno, Daniel Wolpert, and Mitsuo Kawato. Multiple paired forward-inverse models for human motor learning and control. Advances in neural information processing systems, 11, 1998. 2, 10 [25] William Whitney, Rajat Agarwal, Kyunghyun Cho, and Abhinav Gupta. Dynamics-aware embeddings. arXiv preprint arXiv:1908.09357, 2019. 2 [26] David Brandfonbrener, Ofir Nachum, and Joan Bruna. Inverse dynamics pretraining learns good representations for multitask imitation. Advances in Neural Information Processing Systems, 36, 2024. 2, [27] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 2, 5, 17 [28] Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158168. PMLR, 2022. 2, 3, 5, 17 [29] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36, 2024. 2, 5, 17 [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. 3, 6, 10 [31] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics and automation (ICRA), pages 11341141. IEEE, 2018. 3, 6, 10 [32] Sarah Young, Jyothish Pari, Pieter Abbeel, and Lerrel Pinto. Playful interactions for representation learning. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 992999. IEEE, 2022. 3, 6 [33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 3, 5, 6, 9, 10 [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [36] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021. 5, 9, [37] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1575015758, 2021. 5, 9, 10 [38] Siddhant Haldar, Zhuoran Peng, and Lerrel Pinto. Baku: An efficient transformer for multi-task policy learning. arXiv preprint arXiv:2406.07539, 2024. 6, 7 [39] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. In Conference on Robot Learning, pages Robot learning with sensorimotor pre-training. 683693. PMLR, 2023. 6 [40] Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021. 7, [41] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. 9, 10 [42] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1580215812, 2023. 10 [43] Vladimiros Sterzentsenko, Leonidas Saroglou, Anargyros Chatzitofis, Spyridon Thermos, Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios Zarpalas, and Petros Daras. Selfsupervised deep depth denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12421251, 2019. 10 [44] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 10 [45] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. [46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 10 [47] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. 10 [48] Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. 13 [49] Trieu Trinh, Minh-Thang Luong, and Quoc Le. Selfie: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019. 10 [50] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with jointembedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 10 [51] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. 2023. 10 [52] Nathan Delson and Harry West. Robot programming by human demonstration: Adaptation and inconsistency in constrained motion. In Proceedings of IEEE International conference on Robotics and Automation, volume 1, pages 3036. IEEE, 1996. 10 [53] Michael Kaiser and Rüdiger Dillmann. Building elementary robot skills from human demonstration. In Proceedings of IEEE International Conference on Robotics and Automation, volume 3, pages 27002705. IEEE, 1996. [54] Sheng Liu and Haruhiko Asada. Teaching and learning of deburring robots using neural networks. In [1993] Proceedings IEEE International Conference on Robotics and Automation, pages 339345. IEEE, 1993. [55] Haruhiko Asada and Boo-Ho Yang. Skill acquisition from human experts through pattern processing of teaching data. Journal of The Robotics Society of Japan, 8(1):1724, 1990. 10 [56] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation learning using score-based diffusion policies. arXiv preprint arXiv:2304.02532, 2023. 10 [57] Richard Sutton and Andrew Barto. Toward modern theory of adaptive networks: expectation and prediction. Psychological review, 88(2):135, 1981. 10 [58] Hermann Von Helmholtz. Handbuch der physiologischen Optik, volume 9. Voss, 1867. [59] Andre Bastos, Martin Usrey, Rick Adams, George Mangun, Pascal Fries, and Karl Friston. Canonical microcircuits for predictive coding. Neuron, 76(4):695711, 2012. [60] Lisa Feldman Barrett and Kyle Simmons. Interoceptive predictions in the brain. Nature reviews neuroscience, 16(7):419429, 2015. 10 [61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [62] Younggyo Seo, Kimin Lee, Stephen James, and Pieter Abbeel. Reinforcement learning with action-free pre-training from videos. In International Conference on Machine Learning, pages 1956119579. PMLR, 2022. 10 [63] Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. arXiv preprint arXiv:2007.05929, 2020. [64] Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bachman, and Aaron Courville. Pretraining representations for data-efficient reinforcement learning. Advances in Neural Information Processing Systems, 34: 1268612699, 2021. 10 [65] Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen Tian, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Learning predictive models from observation and interaction. In European Conference on Computer Vision, pages 708725. Springer, 2020. 10 [66] Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778 2787. PMLR, 2017. 10 [67] Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016. [68] Zhaohan Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent Altché, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. Advances in neural information processing systems, 35:3185531870, 2022. 10 [69] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. 10 [70] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 25552565. PMLR, 2019. [71] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. [72] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. [73] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 10 [74] Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised In Proceedings of the IEEE international learning of spatiotemporally coherent metrics. conference on computer vision, pages 40864093, 2015. [75] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. [76] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32993309, 2021. [77] Xiaolong Wang, Allan Jabri, and Alexei Efros. Learning correspondence from the cycleconsistency of time. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 25662576, 2019. [78] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycle-consistency learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18011810, 2019. [79] Sören Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, and Pierre Sermanet. Online object representations with contrastive learning. arXiv preprint arXiv:1906.04312, 2019. [80] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. arXiv preprint arXiv:2207.09450, 2022. 10 [81] Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made easy (mime): Large scale demonstrations data for imitation. In Conference on robot learning, pages 906915. PMLR, 2018. [82] Boyuan Chen, Pieter Abbeel, and Deepak Pathak. Unsupervised learning of visual 3d keypoints for control. In International Conference on Machine Learning, pages 15391549. PMLR, 2021. [83] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In European Conference on Computer Vision, pages 570587. Springer, 2022. [84] Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. Robotic telekinesis: Learning robotic hand imitator by watching humans on youtube. arXiv preprint arXiv:2202.10448, 2022. 10 [85] Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent policies from observation. In International conference on machine learning, pages 17551763. PMLR, 2019. 10 [86] Dominik Schmidt and Minqi Jiang. Learning to act without actions. arXiv preprint arXiv:2312.10812, 2023. [87] Weirui Ye, Yunsheng Zhang, Pieter Abbeel, and Yang Gao. Become proficient player with In The Eleventh International Conference on limited data through watching pure videos. Learning Representations, 2022. 10 [88] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. 10 [89] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33:1988419895, 2020. 10 [90] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In Conference on Robot Learning, pages 416426. PMLR, 2023. [91] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 10 [92] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and Lerrel Pinto. On bringing robots home. arXiv preprint arXiv:2311.16098, 2023. 10 [93] Gaoyue Zhou, Victoria Dean, Mohan Kumar Srirama, Aravind Rajeswaran, Jyothish Pari, Kyle Hatch, Aryan Jain, Tianhe Yu, Pieter Abbeel, Lerrel Pinto, et al. Train offline, test online: real robot learning benchmark. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 91979203. IEEE, 2023. [94] Irmak Guzey, Ben Evans, Soumith Chintala, and Lerrel Pinto. Dexterity from touch: arXiv preprint Self-supervised pre-training of tactile representations with robotic play. arXiv:2303.12076, 2023. [95] Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daumé III, Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Shankar Basu, and Furong Huang. Premier-taco: Pretraining multitask representation via temporal action-driven contrastive loss. arXiv preprint arXiv:2402.06187, 2024. [96] Aadhithya Iyer, Zhuoran Peng, Yinlong Dai, Irmak Guzey, Siddhant Haldar, Soumith Chintala, and Lerrel Pinto. Open teach: versatile teleoperation system for robotic manipulation, 2024. 17, 18 [97] Andrej Karpathy. nanogpt. https://github.com/karpathy/nanoGPT, 2023. Accessed: 2024-05-20."
        },
        {
            "title": "A Environment and dataset details",
            "content": "A.1 Franka Kitchen The Franka Kitchen environment introdued by Gupta et al. [27] consists of Franka arm with 9-dimensional action space. This environment includes seven tasks and dataset of 566 humancollected demonstrations. While the original environment is state-based, we created an image-based variant by rendering the states to 224 224 RGB images. A.2 Block Pushing In the Block Pushing environment introduced by Florence et al. [28], the objective is for the robot to push two colored blocks (red and green) into two target squares (also red and green). The training dataset consists of 1 000 trajectories, evenly distributed among the four possible combinations of block target and push order. These trajectories were collected by scripted expert controller. A.3 Push-T In the Push-T environment introduced by Chi et al. [3], the goal is to push T-shaped block to designated target position on table. The dataset for this environment contains 206 demonstrations collected by human operators. The action space in this environment is two-dimensional end-effector position control. Similar to the Franka Kitchen environment, we have created an image-based variant by rendering demonstrations to 224 224 RGB images. A.4 LIBERO Goal In the LIBERO Goal environment introduced by Liu et al. [29], there are 10 manipulation tasks, each with 50 teleoperated demonstrations for goal-conditioned policy benchmarking. The environment has 7-dimensional action space and an observation space of 224 224 RGB images from two cameras (fixed external view, and wrist-mounted egocentric view). A.5 Allegro Manipulation The environment consists of an Allegro hand attached to Franka arm, and fixed camera for image observations. The observation space is 224 224 RGB images. The action space is 23-dimensional, consisting of Cartesian position and orientation of the Franka robot arm (7 DoF), and 16 joint positions of the Allegro Robot Hand. The demonstrations are collected at 50Hz for Franka, and 60Hz for the Allegro hand. The learned policies are rolled out at 4Hz. We evaluate on three contact-rich dexterous manipulation tasks that require precise multi-finger control and arm movement, described in detail below. Sponge picking: This task requires the hand to reach to the position of the sponge, grasp the sponge, and lift the sponge from the table. We collect 6 demonstrations via OpenTeach [96] for the task, starting from different positions, with 543 frames in total. The task is considered successful if the robot hand can grasp the sponge from the table within 120 seconds. Teabag picking: This task is similar to the previous task, but more difficult with smaller task object. We collect 7 demonstrations via OpenTeach with 1 034 frames in total. In this task, the robot needs reach the teabag, grasp the teabag with two fingers, then pick it up. The task is considered successful if the robot hand can grasp the teabag from the table within 240 seconds. Microwave opening: This task requires the hand to reach the microwave door handle, grasp the handle, and pull down the door. We collect 6 demonstrations via OpenTeach with 735 frames in total. The task is considered successful if the robot hand can open the door within 240 seconds. A.6 xArm Kitchen This is real-world multi-task kitchen environment comprising Ufactory xArm 7 robot with an xArm Gripper. The policies are trained on RGB images of size 128 128 obtained from four different camera views, including an egocentric camera attached to the robot gripper. The action space comprises the robot end effector pose and the gripper state. We collect total of 65 demonstrations across 5 tasks, depicted in Figure 5. The demonstrations were collected using OpenTeach [96] at 30Hz. The learned policies are deployed at 10Hz. Figure 5 shows real-world task rollouts for the multitask policy learned for all 5 tasks. Figure 5: xArm Kitchen environment tasks"
        },
        {
            "title": "B Hyperparameters and implementation details",
            "content": "B.1 Visual encoder training We present the DynaMo hyperparameters below. Table 8: Environment-dependent hyperparameters for DynaMo pretraining, random init Obs. context EMA β"
        },
        {
            "title": "Forward dynamics dropout Transition latent dim",
            "content": "Franka Kitchen Block Pushing Push-T LIBERO Goal xArm Kitchen 2 5 5 5 5 SimSiam 0.99 SimSiam SimSiam 0.99 0 0.3 0 0 0 64 16 8 32 64 Table 9: Shared hyperparameters for DynaMo pretraining, random init Name Optimizer Learning rate Weight decay Betas Gradient clip norm Covariance reg. coefficient Epochs Batch size Value AdamW 104 0.0 (0.9, 0.999) 0.1 0.04 40 64 Table 10: Environment-dependent hyperparameters for DynaMo fine-tuning from ImageNet weights Obs. context EMA β Transition latent dim Franka Kitchen Block Pushing Push-T LIBERO Goal Allegro 2 5 5 5 5 SimSiam 0.99 SimSiam 0.99 SimSiam 64 16 8 32 Table 11: Shared hyperparameters for DynaMo fine-tuning Name Optimizer Learning rate Forward dynamics dropout Weight decay Betas Gradient clip norm Covariance reg. coefficient Epochs Batch size Value AdamW 105 0.0 0.0 (0.9, 0.999) 0.1 0.04 40 64 For Block Pushing and xArm kitchen, we use an EMA encoder with the beta schedule from the MoCo-v3 official repo. For DynaMo training, we use constant learning rate schedule for LIBERO 19 Goal, and cosine learning rate decay schedule with 5 warmup epochs on all other environments. For DynaMo fine-tuning, we use cosine learning rate decay schedule with 5 warmup epochs on all environments. We use the following official implementation repos: MoCo-v3: https://github.com/facebookresearch/moco-v3 BYOL: https://github.com/lucidrains/byol-pytorch MAE: https://github.com/facebookresearch/mae R3M: https://github.com/facebookresearch/r3m/ MVP: https://github.com/ir413/mvp VC-1: https://github.com/facebookresearch/eai-vc We base our transformer encoder implementation on nanoGPT [97] at https://github.com/ karpathy/nanoGPT. For the Allegro Manipulation environment, we fine-tune MoCo and BYOL from ImageNet-1K weights for 1 000 epochs. For all other environments, we train MoCo and BYOL for 200 epochs, MAE for 400 epochs, all from random initialization. The hyperparameters used for training these models are detailed in Table 12. Compute used for training DynaMo: Franka Kitchen: 3 hours on 1x NVIDIA A100. Block Pushing: 7 hours on 1x NVIDIA A100. Push-T: 1 hour on 1x NVIDIA A100. LIBERO Goal: 2 hours on 1x NVIDIA H100. Allegro Manipulation: 3 minutes on 1x NVIDIA RTX A6000 for the sponge task, 4 minutes for the teabag task, and 3 minutes for the microwave task. xArm kitchen: 4 hours on 1x NVIDIA RTX A6000. Table 12: SSL Hyperparameters (a) MoCo Hyperparameters (b) BYOL Hyperparameters Name Optimizer Batch size Learning rate Momentum Weight decay Value LARS 1024 0.6 0.9 106 Name Value Optimizer Batch size Learning rate Momentum Weight decay LARS 512 0.2 0.9 1.5 10 (c) MAE Hyperparameters Name Value Optimizer Batch size Learning rate Weight decay AdamW 64 2.5 105 0.05 B.2 Downstream policy training Table 13, 14 and 15 detail the downstream policy hyperparameters for VQ-BeT, Diffusion Policy and MLP training for the simulated environments. 20 For VQ-BeT, we use the implementation from the original paper [1] with the recommended hyperparameters. For Diffusion Policy, we use the implementation at https://github.com/ real-stanford/diffusion_policy with transformer-based noise prediction network with the recommended hyperparameters. We use AdamW as optimizer for the three policy heads. Compute used for downstream policy training: Franka Kitchen VQ-BeT: 8.5 hours on 1x NVIDIA A4000. Block Pushing VQ-BeT: 4 hours on 1x NVIDIA A100. Push-T VQ-BeT: 7 hours on 1x NVIDIA A100. Push-T Diffusion Policy: 8 hours on 1x NVIDIA A100. Push-T MLP: 2 hours on 1x NVIDIA A100. LIBERO Goal VQ-BeT: 5 hours on 1x NVIDIA A4000. xArm Kitchen VQ-BeT: 6 hours on 1x NVIDIA A4000. Table 13: Hyperparameters for VQ-BeT training Parameter Franka Kitchen Block Pushing Push-T LIBERO Goal Batch size Epochs Window size Prediction window size Learning rate Weight decay 2048 1000 10 1 5.5 105 2 10 64 300 3 1 104 0 512 5000 5 5 5.5 105 2 104 64 50 10 1 5.5 105 2 104 Table 14: Hyperparameters for Diffusion Policy Training Parameter Push-T Batch size Epochs Learning rate Weight decay Observation horizon Prediction horizon Action horizon 256 2000 104 0 2 10 8 Table 15: Hyperparameters for MLP Training Parameter Push-T Batch size Epochs Learning rate Weight decay Hidden dim Hidden depth Observation context Prediction context 256 2000 104 0 256 8 5"
        },
        {
            "title": "C Real robot environment rollouts",
            "content": "Figure 6: Rollouts on Allegro Manipulation with our DynaMo-pretrained encoder. 22 Figure 7: Rollouts on xArm Kitchen with our DynaMo-pretrained encoder."
        }
    ],
    "affiliations": []
}