{
    "paper_title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future",
    "authors": [
        "Yidong Wang",
        "Xin Wang",
        "Cunxiang Wang",
        "Junfeng Fang",
        "Qiufeng Wang",
        "Jianing Chu",
        "Xuran Meng",
        "Shuxun Yang",
        "Libo Qin",
        "Yue Zhang",
        "Wei Ye",
        "Shikun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \\textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \\textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data."
        },
        {
            "title": "Start",
            "content": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future Yidong Wang1*, Xin Wang1*, Cunxiang Wang2*, Junfeng Fang3, Qiufeng Wang4, Jianing Chu5, Xuran Meng6, Shuxun Yang7, Libo Qin8, Yue Zhang9, Wei Ye1, Shikun Zhang1 1Peking University, 2Tsinghua University, 3National University of Singapore, 4Southeast University, 5North Carolina State University, 6University of Michigan, 7Beijing Institute of Technology, 8Central South University 9Westlake University, 5 2 0 2 8 ] . [ 1 6 2 0 6 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial models outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using nextgeneration model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to SelfRewarding using same computation resources. For example, Llama3.1-8B reaches 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data. Introduction Large language models (LLMs) have attracted increasing attention in the field of artificial intelligence (OpenAI 2023; Google 2023; Zeng et al. 2022; Brown et al. 2020; Chowdhery et al. 2022; Anil et al. 2023; Zhang et al. 2023), with post-training (Stiennon et al. 2020; Diamantidis and Chatzoglou 2014; Ouyang et al. 2022; Bai et al. 2022; Li et al. 2025a,b; Zhang et al. 2025)techniques proving particularly effective in enhancing model capabilities. Among *These authors contributed equally. Correspondence to: wangcunxiang303@gmail.com, wye@pku.edu.cn, zhangsk@pku.edu.cn various research methodologies, self-improvement(Huang et al. 2022, 2024; Yu et al. 2023; Qu et al. 2024; Wang et al. 2023) paradigms have emerged as promising direction for autonomous model refinement. Recent advances in Self-Rewarding(Yuan et al. 2024) language models demonstrate an alternative paradigm to self-improvement, where language models serve dual roles as both response generators and evaluators (Yuan et al. 2024; Wu et al. 2024). Specifically, the Self-Rewarding paradigm builds upon the Supervised Fine-Tuned (SFT) model through an iterative optimization cycle that: (1) generating candidate responses to given prompts, (2) using the same LLM to evaluate these responses via LLM-as-a-Judge prompting (Zheng et al. 2023; Li et al. 2023a; Wang et al. 2024a), and (3) selecting preference pairs from the highest and lowest scoring responses for DPO training (Rafailov et al. 2023). Most existing work has focused on enhancing the models judging capabilities to improve the effectiveness of the Self-Rewarding paradigm. For example, meta-rewarding approaches refine judgment skills through self-evaluation (Wu et al. 2024), while other methods include consistency regularization of reward models (Wang et al. 2024b), self-consistency mechanisms for internal rewards (Zhou et al. 2025), and process-based evaluation for mathematical reasoning (Zhang et al. 2025). Unlike traditional approaches that rely on static reward models or fixed preference datasets, these methods allow for the continuous co-evolution of generation and evaluation quality. Despite the success of Self-Rewarding language models on benchmarks like AlpacaEval (Li et al. 2023c) and ArenaHard (Li et al. 2024), our theoretical analysis reveals critical limitation: when the representational similarity between chosen and rejected responses increases, the DPO gradient vanishes, causing the training process to collapse. This theoretical prediction is empirically validated by our findings - as quantified in Fig. 1, the representations of chosen and rejected responses in the Self-Rewarding paradigm become progressively similar, with the score gap between them shrinking by 9 times during the same period (all responses evaluated by GPT-4o-mini to ensure consistent scoring and eliminate potential bias from varying judge capabilities across LLMs). This representational convergence diFigure 1: Comparison of response preference dynamics between Self-Rewarding and Temporal Self-Rewarding (Temporal SR) frameworks across iterations. We track: (1) the score difference (chosen - rejected) evaluated by GPT-4o-mini (the scoring prompt is the same as that used in Temporal Self-Rewarding, detailed in Appendix A) and (2) similarity between chosen and rejected responses(cosin similarity calculated through the last layers features). With similar computational budgets (4 vs. 2 iterations), Self-Rewarding shows rapid degradation with score gap shrinking 9 times and rapid similarity improvement between chosen and rejected responses of Llama3.1-8B, indicating progressive narrowing of the quality gap between chosen and rejected samples. Our Temporal approach effectively mitigates this quality convergence. rectly leads to diminishing quality differences between generated answers, which in turn weakens or eliminates the learning signal for preference optimization. We attribute this convergence to reduced response diversity after reinforcement learning (Zhang et al. 2024; Kirk et al. 2023), which conflicts with the fundamental assumption of preference learning that requires clear quality differences between positive and negative samples for effective optimization (Lanchantin et al. 2025; Razin et al. 2025). The resulting vanishing gradient problem creates vicious cycle where decreasing answer distinctness makes it harder to produce high-quality preference data, further exacerbating the learning signal deterioration. To address the above issues, we propose Temporal SelfRewarding Language Models that strategically coordinates past, present, and future model generations to maintain effective preference learning signals. Our approach consists of two key components: (1) Anchored Rejection that fixes rejected responses using outputs from the initial SFT model (past generation) to prevent quality inflation in negative samples, and (2) Future-Guided Chosen that selects high-quality positive samples by incorporating predictions from future model version. The future model is obtained by first performing DPO training on the current model using the anchored rejection pairs, creating temporary model that represents the next generations capabilities. This future model then helps produce superior responses that would otherwise be unavailable to the current model. By decoupling the chosen and rejected responses through this temporal approach, our method maintains clear differences between good and bad examples during training, as shown in Figure 1. Note that our method consumes the same computational resources with traditional Self-Rewarding approach because we adopts half the training iterations in the whole paper."
        },
        {
            "title": "Extensive experimental",
            "content": "three model families (Llama, Qwen, Mistral) and different model sizes(Llama3B/8B/70B) evaluated on multiple benchmarks results across (AlpacaEval 2.0, Arena-Hard-v0.1, MT-Bench) demonstrate the superior performance of our Temporal Self-Rewarding approach. On AlpacaEval 2.0, our method achieves 29.44% win rate with Llama3.1-8B, outperforming the SelfRewarding baseline (19.69%) by 9.75%. Similar improvements are observed on Arena-Hard-v0.1, where Qwen2.57B scores 34.4 with our method, exceeding the SelfRewarding baseline (21.5) by 12.9%. The effectiveness of our temporal coordination strategy is further validated by the sustained quality gap between chosen and rejected responses throughout training iterations. Notably, these gains are achieved with fewer iterations (2 vs. 4) and generalize consistently across model sizes and model structures. Our method also demonstrates strong generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, with Temporal SR Iter1 achieving 54.43% accuracy on TruthfulQA - 2.66% higher than the best Self-Rewarding. In conclusion, to the best of our knowledge, Temporal Self-Rewarding represents the first systematic approach to address the diminishing preference signal problem in Self-Rewarding language models through temporal coordination of model generations. Our method establishes new paradigm for iterative self-improvement that maintains effective learning signals by strategically leveraging past, present, and future model capabilities. The proposed framework not only outperforms existing Self-Rewarding approaches but also provides insights into the dynamics of preference learning in iterative optimization settings. By decoupling the chosen and rejected samples, we enable more stable and effective model alignment while preserving the computational efficiency of the Self-Rewarding paradigm."
        },
        {
            "title": "Methodology",
            "content": "In this section, we first present our theoretical analysis of the gradient collapse problem in Self-Rewarding models, then introduce our two-phase Temporal Self-Rewarding. Theoretical Analysis We define process where for prompt x, chosen response yw and rejected response yl are generated from latent representations hw πh θ (x) and hl πh θ (x), respectively. Critically, the designation of these responses as chosen or rejected is performed by the same model, πθ, that generated them. Our approach builds upon DPO, which directly optimizes policy model πy θ using preference data. We use ˆr to represent the reward difference between the policy model πθ and reference model πref . The key components are the implicit reward: ˆr = β (cid:18) (cid:0) log πy θ (ywx) log πy θ (ylx)(cid:1) (cid:0) log πy ref (ywx) log πy ref (ylx)(cid:1) (1) (cid:19) , and the gradient of the DPO loss LDPO, which takes the explicit form: Rejection and Future-Guided Chosen. This prevents the DPO gradient from vanishing and ensures stable iterative optimization. The details of our proof is in Appendix B. We now present the technical details of our approach and the pseudo-code is provided in Algorithm 1. SFT Model Initialization The initialization process begins with pretrained foundation model Mb, which we enhance through supervised fine-tuning to develop dual capabilities in response generation and quality assessment. Following Self-Rewarding (Yuan et al. 2024), we fine-tune Mb on two complementary datasets: (1) instruction following fine-tuning (IFT) to improve response generation, and (2) evaluation fine-tuning (EFT) to develop quality assessment capabilities. The resulting model M0 serves as the foundation for subsequent optimization rounds and provides the anchored rejection responses required by our framework. This initialization process is formally defined as: M0 = SFT(Mb, IFT EFT). (4) θLDPO = β (1 σ(ˆr)) (cid:124) (cid:123)(cid:122) (cid:125) Adaptive Weighting (cid:0)θ log πy (cid:124) θ (ywx) θ log πy (cid:123)(cid:122) Directional Guidance θ (ylx)(cid:1) (cid:125) (2) This gradient reveals two crucial mechanisms: Adaptive Weighting, which scales updates based on confidence, and Directional Guidance, which pushes the policy toward preferred responses. Theorem 1 (Bound on Directional Guidance). Let πθ be model that generates response via latent representation h. Assume the gradient of the log-likelihood, θ log πh θ (hx) is continuously differentiable with respect to the latent representation h. Then, for any chosen-rejected pair (yw, yl) generated from (hw, hl), the norm of the DPO directional guidance term is bounded as follows: Iterative Optimization Process For each iteration from 0 to , our framework executes two key phases using the optimization dataset = {p0, . . . , pN } containing (Q = 5k) queries per prompt set pi. . 0, . . . , rK , . . . , rK } and r0 = {r , . . . , sK 0 } for M0s outputs. Phase 1: Anchored Rejection For each prompt pi, we generate responses (K = 7) from both the current model Mi and the initial model M0, denoted as ri = {r1 0 } respectively. The current model Mi scores all responses, producing si = {s1 } for its own generations and s0 = 0, . . . , sK {s1 The chosen response is selected as rarg max si (highestscoring from Mi), while the rejected response is determined by comparing minima: rarg min s0 if min(s0) < min(si), otherwise rarg min si . Valid preference pairs (chosen, rejected) where the chosen score exceeds the rejected score are added to dataset D1. 0 θ (ywx) θ log πy θ log πy = θ log πh Chw,hl hw hl. θ (hwx) θ log πh θ (hlx) θ (ylx)"
        },
        {
            "title": "We then train a temporary future model Mf using these",
            "content": "anchored rejection pairs: Mf = DPO(Mi, D1). (5) (3) Here, Chw,hl < is constant related to hw and hl."
        },
        {
            "title": "We provide the formal proof in Appendix B and offer the",
            "content": "following interpretations and insights: (i) As quantified in Figure 1, representations of chosen hw and rejected hl responses progressively converge in the selfrewarding paradigm (hw hl 0). Our theory proves this representational collapse directly causes the gradient difference between the pair to vanish: θ log πy θ (ywx) θ log πy θ (ylx) 0. (ii) Consequently, in the DPO gradient formulation (Eq. 2), the Directional Guidance term approaches zero. Since the Adaptive Weighting term is bounded in (0, 1), the overall DPO gradient vanishes (θLDPO 0), leading to the collapse of the self-rewarding training process. (iii) By contrast, our framework preserves clear representational difference via two sequential phasesAnchored , . . . , rK Phase 2: Future-Guided Chosen For each prompt pi, we generate responses rf = {r1 } using Mf and score them with Mi to obtain sf = {s1 , . . . , sK }. The chosen response is upgraded to rarg max sf if max(sf ) > max(si), otherwise retaining rarg max si . These are paired with the same rejected responses from Phase 1 of prompt p, with valid pairs added to D2. The final model for the next iteration is obtained by: Mi+1 = DPO(Mi, D2). (6) This two-phase process maintains clear quality gap between chosen and rejected responses by anchoring negatives to past model capabilities while proactively incorporating superior generations from future model predictions. To ensure computationally fair comparisons with Self-Rewarding approaches (which typically run for five iterations), we limit our optimization to three iterations - each requiring training of an additional future model Mf . Despite running for fewer iterations (2 vs. 4 in Self-Rewarding), our temporal approach achieves superior performance improvements through more effective preference learning. Algorithm 1: Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future. 1: Input: Instruction Fine-Tuning data (IFT), Evaluation Fine-Tuning data (EFT), base model Mb, Iteration Data = {p0, . . . , pN } (each pi contains queries). 2: Output: Aligned Model Mi after each iteration (0 <= <= ). 3: M0 SFT(Mb, IFT + EFT) {Supervised Tuning} 4: for = 0 to do 5: D1, D2 . {Preference datasets for DPO} 6: 7: 8: Phase 1: Decoupling Rejected Responses via M0 for pi do 0, . . . , rK ) and M0 (r0 = Generate responses each from Mi (ri = r1 , . . . , rK 0 ). Then, score all responses using Mi, yielding si = s1 , . . . , sK (for ri) and s0 = s1 chosen rarg max si rejected rarg min s0 rarg min si {Lowest from M0 and Mi} If schosen > srejected, add (chosen, rejected) to D1 if min(s0) < min(si) else {Highest from Mi} 0, . . . , sK 0 (for r0) end for 11: 12: 13: Mf DPO(Mi, D1) {Train future model} 14: 15: 16: Phase 2: Decoupling Chosen Responses via Mf for pi do , . . . , rK } using , . . . , sK } Generate responses rf = {r1 Mf and Score responses: sf = {s1 (judged by Mi) chosen rarg max sf rarg max si Use same rejected from Phase 1 for this If schosen > srejected, add (chosen, rejected) to {Highest from Mi and Mf } if max(sf ) > max(si) else 9: 10: 17: 18: 19: 20: 21: Mi+1 DPO(Mi, D2) 22: end for end for Experiments We conduct extensive experiments using multiple models from the LLaMA (Touvron et al. 2023), Qwen (Yang et al. 2024), and Mistral (Jiang et al. 2023) families as our base models. vLLM (Kwon et al. 2023) is used for inference and deepspeed (Aminabadi et al. 2022) for SFT and DPO. The inference and training details are shown in Appendix and Appendix D. Data Preparation Following Self-Rewarding, our study processes two primary datasets for model development: the Open Assistant dataset (Kopf et al. 2023) containing question-answer pairs with human judgments (rank 0-4), and the UltraFeedback dataset (Cui et al. 2024) with scored responses. Both datasets provide questions, answers, and rankings but lack scoring explanations. We create three specialized subsets through the following pipeline. Instruction Fine-Tuning (IFT) Seed Data Following (Li et al. 2023b), we construct the IFT seed dataset by sampling high-quality initial conversational turns in English. The preparation process consists of three steps: First, we extract all English samples from the Open Assistant dataset(oasst1 and oasst2), resulting in 5,000 samples after removing null-score entries. Next, we apply twostage selection: (1) identifying highest-ranked responses in initial conversation rounds, and (2) combining these with the top-scoring 25,000 samples from UltraFeedback (excluding the 2,000 most variable entries). The final IFT dataset comprises 5,000 randomly selected question-answer pairs from this merged collection after thorough shuffling. Each IFT sample contains both the question and its corresponding answer. Evaluation Fine-Tuning (EFT) Seed Data For EFT data preparation, we begin with the 2,000 most variable examples from the UltraFeedback dataset. Each samples four responses are evaluated by GPT-4o, retaining only those where the models scoring order matches the original human ratings. This quality control process yields 1,871 validated samples. Importantly, the EFT dataset not only contains questions and answers, but also includes our carefully constructed judge explanations that justify the rankings. Iteration Optimization Data After excluding the 5,000 IFT samples from the mixed dataset, we divide the remaining 20,000 items into five equal parts for iterative optimization. All baseline methods use only questions, except for SPIN, which additionally requires answers as chosen. Following prior Self-Rewarding works (typically 2-3 iterations), our Temporal Self-Rewarding conducts 2 iterations, while standard SR is extended to 4 iterations to ensure fair computational comparison. Baseline Methods We conduct comprehensive comparisons of all the following baseline approaches, all of which follow an iterative optimization paradigm. Specifically, Rejection-Sampling SFT performs supervised fine-tuning (SFT) in each round, whereas the other baselines apply direct preference optimization (DPO) iteratively. Their primary differences lie in the strategies for constructing training data. Self-Rewarding: Using both chosen/rejected samples from current model. Temporal Self-Rewarding (Ours): Decoupling Chosen-Rejected via past-future models. Rejection-Sampling SFT (Liu et al. 2023): Instead of DPO, Fine-tuning with the highest self-rated responses. SPIN (Chen et al. 2024): Using labels as chosen, current models responses as rejected. SPIN-Fair: Serving as variant of SPIN to ensure fair comparison with other baselines which retains the label answers as chosen, but selects the lowest-scored responses from model-generated candidates as rejected."
        },
        {
            "title": "SPIN",
            "content": "SPIN-Fair Self-Rewarding (SR) Temporal SR (Ours) - 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 AlpacaEval 2.0 LC Win(%) Win(%) Length 1324 1385 1406 1451 1487 1712 1150 1705 1404 1555 1239 1736 1398 1567 1789 1865 1882 1820 2063 5.96 6.71 7.33 6.96 5.59 5.28 4.72 4.72 6.83 6.40 5.59 5.47 7.20 10.99 15.71 17.08 19.69 19.07 29.44 8.73 9.04 9.58 8.82 6.63 5.63 7.09 4.97 8.93 7.37 7.82 5.83 9.82 13.29 17.00 17.54 19.92 20.48 27.94 Arena-Hard-v0.1 95% CI (-1.0, 1.0) (-0.9, 0.9) (-1.0, 0.9) (-0.8, 1.1) (-1.0, 0.8) (-0.8, 0.7) (-0.8, 1.0) (-0.8, 0.7) (-0.7, 0.8) (-0.7, 0.8) (-0.7, 0.7) (-0.7, 0.8) (-0.9, 1.0) (-1.2, 1.0) (-1.4, 1.7) (-1.3, 1.4) (-1.3, 1.3) (-1.8, 1.3) (-1.7, 1.7) Score(%) 6.3 5.4 6.6 4.6 4.5 3.7 3.8 3.5 4.7 4.3 3.4 2.9 4.7 6.7 7.7 9.4 8.8 11.3 14.6 Length 652 656 638 606 610 823 573 962 666 779 601 1000 622 578 592 592 613 605 698 MT-Bench 2nd 3.79 4.04 4.04 4.14 3.29 3.75 4.24 3.89 4.26 4.08 4.22 3.85 4.29 4.46 4.70 4.89 4.66 4.98 4.94 Avg 4.81 5.08 5.01 5.06 4.48 4.68 5.01 4.76 5.11 4.84 5.09 4.75 5.09 5.33 5.60 5.74 5.66 5.79 5.89 1st 5.84 6.11 6.00 5.99 5.68 5.61 5.78 5.64 5.96 5.60 6.02 5.65 5.89 6.19 6.50 6.60 6.66 6.61 6.84 Table 1: Main results of all baselines of Llama3.1-8B on AlpacaEval 2.0, Arena-Hard-v0.1 and MT-Bench. The best results of each baseline are in bold. The marker represents the best results of all baselines. Evaluation Metrics We evaluate all methods using three widely adopted benchmarks: AlpacaEval 2.0, Arena-Hard-v0.1, and MT-Bench. We adopt GPT-4o as the judge model across all benchmarks for its faster inference and lower cost(OpenAI 2023). MT-Bench evaluates models multi-turn dialogue abilities through direct scoring, where the judge assigns numerical ratings to responses for each turn. In contrast, both AlpacaEval 2.0 and Arena-Hard-v0.1 use pairwise comparison to evaluate models. GPT-4o acts as judge, comparing each models responses against the baseline. The baselines differ between benchmarks: AlpacaEval 2.0 uses responses from GPT-4 Preview, while Arena-Hard-v0.1 uses GPT-4-0314. AlpacaEval 2.0 employs two primary metrics: win rate and length-controlled win rate. Arena-Hard-v0.1 utilizes score as its evaluation metric to measure win rate. MT-Bench provides first-turn, second-turn and average score. Main Results Our experimental evaluation demonstrates significant improvements achieved by Temporal Self-Rewarding across three major benchmarks (AlpacaEval 2.0, Arena-Hard-v0.1, and MT-Bench) compared to existing approaches. Table 1 presents comprehensive results on Llama3.1-8B, comparing our method against four baselines: Self-Rewarding (SR), Rejection-Sampling SFT, SPIN, and SPIN-Fair. from two key factors: (1) Unlike SPINs fixed chosen responses (from human data), Self-Rewarding adopts both chosen and rejected samples to improve, enabling higher performance ceilings. (2) Compared to RejectionSampling which only uses positive examples, SelfRewardings preference learning provides more effective optimization signals. Temporal SR Outperforms Self-Rewarding: Our method achieves consistent improvements over standard Self-Rewarding across all benchmarks, with particularly notable gains on AlpacaEval 2.0 (29.44% vs 19.69% win rate) and Arena-Hard-v0.1 (14.6% vs 9.4% score). This enhancement results from our temporal coordination strategy which maintains clear quality gaps between chosen and rejected responses - preserving effective learning signals that would otherwise diminish in standard Self-Rewarding. Iteration Dynamics and Practical Implications: We observe varying optimal iteration points across methods and benchmarks. While Self-Rewarding requires 4 iterations to peak, our Temporal SR achieves best performance at iteration 1 on Arena-Hard-v0.1 and iteration 0 on MT-Bench. This variation suggests practitioners should carefully monitor performance across iterations to avoid overfitting (as seen in Rejection-Samplings performance decline after iteration 1). Superiority of Self-Rewarding Paradigm: The SelfRewarding approach outperforms traditional methods, with the best iteration achieving 19.69% win rate on AlpacaEval 2.0 compared to 7.20% for SPIN-Fair and 7.33% for Rejection-Sampling. This advantage stems The results validate our key insight: decoupling chosen and rejected responses through temporal coordination (past anchoring and future guidance) sustains effective preference learning signals. Additional ablation studies in Section further analyze the contributions of each component. Ablation Studies We conduct comprehensive ablation studies to examine the effectiveness of our Temporal Self-Rewarding mechanism and its key components. Past-Future Model Ablation To investigate the impact of the Past and Future models on our method, we conduct an ablation study by removing the Future component to optimize model solely using the Past model. This approach allows us to directly compare the optimization effects of the Past component on rejected examples and the Future model on chosen examples. As shown in Table 2, the Past model achieves significant improvements over the Self-Rewarding baseline, with substantial gains across all metrics. While the Future model has relatively less pronounced effect, it still contributes to further optimization of the model, highlighting its complementary role in enhancing overall performance. Additionally, its no strange to understand why the Past model plays an more important role than the Future model as the model improves through iterative optimization, its generated responses tend to achieve consistently high scores, so the refinement on rejected examples using Past model can accentuates the contrast between chosen and rejected samples. Method SFT Temporal SR w/o Future&Past (Self-Rewarding) Temporal SR w/o Future Temporal SR Iter AlpacaEval 2.0 ArenaHard MT-Bench LC Win Win Score Average - 0 1 2 3 0 1 2 0 1 8.73 5.96 13.29 17.00 17.54 19.92 14.35 20.96 24.75 25.73 24.08 27. 10.99 15.71 17.08 19.69 11.61 19.69 27.20 29.06 19.07 29.44 6.3 6.7 7.7 9.4 8.8 8.1 10.2 11.4 13. 11.3 14.6 4.81 5.33 5.60 5.74 5.66 5.39 5.76 5.86 5.88 5.79 5.89 Table 2: Ablation study of Temporal Self-Rewarding (Temporal SR) components. Metrics include Length Control Win Rate (LC Win), Win Rate (Win), ArenaHard Score, and MTBench Average. Base model is Llama3.1-8B. Judge Model Ablation Self-Rewarding paradigm involves the model generating and scoring its own outputs. Considering that the data used in the DPO process primarily aim to enhance the models generation capabilities, we conduct an ablation study on the judge model component. Specifically, we employ an off-the-shelf external model AutoJ (in 6B and 13B variants), to score the responses throughout the entire Self-Rewarding and Temporal Self-Rewarding workflows, enabling comparison of the model improvement achieved by each method. Additionally, we assess whether our method could still outperform Self-Rewarding under different Judge model variants. As shown in Figure 2, when using AutoJ-6B as the judge, the models performance under both Self-Rewarding and Temporal Self-Rewarding baselines is overall compaFigure 2: The performance of Self-Rewarding and Temporal Self-Rewarding(Temporal SR) using different Judge, evaluated by AlpacaEval 2.0 Win Rate and Arena-Hard-v0.1 Score. Base model is Llama3.1-8B. This figure illustrates the best model of all iterations in each baseline, detailed results of all iterations can be seen in Appendix E. rable to that achieved with the Self-Judge approach. However, when AutoJ-13B is used as the judge, both baselines show significant improvements over the Self-Judge method. This suggests that the self-judgment mechanism in SelfRewarding may lack advantages when faced with stronger Reward Model. Additionally, it is clear that regardless of whether Self-Judge or AutoJ-Judge is used, Temporal SelfRewarding consistently outperforms Self-Rewarding, emphasizing the effectiveness of our method regardless of the choice of judge model. Model Llama3B Llama8B Llama70B Qwen7B Mistral7B Method AlpacaEval 2.0 ArenaHard MT-Bench LC Win Win Score Average SFT SR TSR SFT SR TSR SFT SR TSR SFT SR TSR SFT SR TSR 2.99 3.37 3.41 8.73 19.92 27.94 19.96 35.57 38.70 11.45 21.53 34.01 12.72 25.48 30. 2.86 3.42 8.20 5.96 19.69 29.44 12.80 32.91 33.66 7.70 18.14 35.90 8.45 27.58 35.16 1.2 2.3 2. 6.3 8.8 14.6 13.0 38.9 40.1 12.7 21.5 34.4 6.3 12.8 15.7 4.06 4.03 4.32 4.81 5.66 5. 6.06 6.93 6.98 5.51 6.09 6.29 5.28 5.68 5.49 Table 3: Comparison of different models and their variants using AlpacaEval 2.0 Length Control Win Rate (LC Win), Win Rate (Win), ArenaHard Score, and MT-Bench Average metrics. SR stands for Self-Rewarding and TSR stands for Temporal Self-Rewarding. This table illustrate the best model of all iterations in each baseline, detailed results of all iterations can be seen in Appendix F. Generalization Experiment on Model Families To evaluate the generalization capability of our approach across different model architectures, we test it on Qwen2.5-7B, Llama3.1-8B and Mistral-7B, which differ significantly in design and training methodologies. As shown in Table 3, Temporal Self-Rewarding consistently outperforms SelfRewarding and the fine-tuning baseline (SFT) across all models tested. To demonstrate the models performance across different tasks, we also evaluated Qwen and Mistral on specific scores for each category in the AlpacaEval 2.0 benchmark. Detailed results can be found in Appendix G. Generalization Experiment on Model Sizes To further explore the scalability of our approach across different model sizes, we compare Temporal Self-Rewarding and Self-Rewarding on models ranging from small-scale architectures like Llama3.2-3B to mid-scale ones like Llama3.18B, and finally to large-scale models such as Llama3.1-70B. As shown in Table 3, Temporal Self-Rewarding consistently delivers superior performance across all model sizes. These results highlight the robustness of Temporal SelfRewarding not only across diverse model structures but also across different model size. Method ARC GSM8K TruthfulQA HumanEval SFT 0.531 SR Iter0 0.538 SR Iter1 0.541 SR Iter2 0.539 0.538 SR Iter3 TSR Iter0 0.545 TSR Iter1 0.549 0.530 0.532 0.546 0.549 0.550 0.559 0.563 0.505 0.516 0.518 0.519 0.518 0.537 0. 0.220 0.232 0.238 0.238 0.238 0.244 0.262 Table 4: Evaluation results on some NLP Benchmarks. The base model of all baselines is Llama3.1-8B. SR and TSR stand for Self-Rewarding and Temporal Self-Rewarding. Out-of-distribution Analysis While Self-Rewarding and Temporal Self-Rewarding primarily focus on improving instruct-following performance, we also evaluat them on common NLP tasks such as scientific reasoning task(ARC-Challenge), mathematical reasoning problems(GSM8K), factual question answering benchmark(TruthfulQA), and code generation task(HumanEval). Surprisingly, our method achieved impressive results even on these out-of-distribution tasks as illustrated in Table 4. For example, on the reasoning-heavy GSM8K dataset, Temporal Self-Rewarding (iter1) significantly improved accuracy from 0.530 under SFT to 0.563, outperforming SelfRewarding by substantial margin. The same improvement can also been vitnessed on HumanEval. Related Work Self-Rewarding Language Models Recent advances in Self-Rewarding language models have demonstrated promising alternatives to traditional human-supervised approaches. The foundational work by Yuan et al. (Yuan et al. 2024) first proposed the concept of models serving dual roles as both generators and evaluators, establishing an iterative optimization cycle that combines generation and selfassessment. Subsequent research has focused on enhancing the judging capabilities within this paradigm, with MetaRewarding (Wu et al. 2024) introducing self-evaluation mechanisms to refine judgment skills. The field has also seen innovations in consistency regularization for reward models (Wang et al. 2024b) and self-consistency mechanisms for internal rewards (Zhou et al. 2025), particularly in specialized domains like mathematical reasoning (Zhang et al. 2025). These developments collectively represent shift from static reward models to dynamic, co-evolving generation and evaluation frameworks, though they share the common limitation of synchronized quality improvement between chosen and rejected samples that our work addresses. Preference Learning and Response Diversity The relationship between reinforcement learning and response diversity has been extensively studied in language model alignment. Zhang et al. (Zhang et al. 2024) and Kirk et al. (Kirk et al. 2023) documented the phenomenon of reduced diversity post-reinforcement learning, while Razin et al. (Razin et al. 2025) analyzed the theoretical foundations of effective preference learning. Direct Preference Optimization (DPO) (Rafailov et al. 2023) emerged as significant advancement over traditional reinforcement learning from human feedback (RLHF), with subsequent variants like DVPO (Lanchantin et al. 2025) exploring mechanisms to maintain meaningful quality gaps between samples. Our work builds upon these insights by introducing temporal decoupling of sample quality levels, addressing diminishing contrast between chosen and rejected responses that has been observed in iterative optimization processes. Limitations While our Temporal Self-Rewarding approach demonstrates significant improvements, two key limitations merit discussion. First, our approach remains effective as long as Self-Rewarding yields any model improvement, however marginal, since our framework leverages the synergy between Future and Past components to amplify the enhancement. However, the method would become inoperative should Self-Rewarding completely fail to optimize the model. Second, while our framework is theoretically compatible with judge optimization techniques in SelfRewarding paradigm such as meta-rewarding, we are unable to explore this integration due to limitations in time and research resources. We believe this represents promising direction for future work that could improve the systems performance. Conclusion In this paper, we introduced Temporal SelfRewarding Language Models, novel framework that addresses the critical limitation of diminishing preference signals in conventional Self-Rewarding approaches. Our method strategically coordinates past, present, and future model generations through two key innovations: Anchored Rejection that fixes negative samples using past model outputs, and FutureGuided Chosen that incorporates next-generation model predictions to maintain quality differentiation. Extensive experiments across three model families (Llama, Qwen, Mistral) and multiple benchmarks (AlpacaEval 2.0, Arena-Hard, MT-Bench) demonstrate that our approach significantly outperforms standard Self-Rewarding methods while requiring fewer iterations. The success of Temporal Self-Rewarding establishes new paradigm for iterative self-improvement that preserves effective learning signals through temporal decoupling of sample quality levels, offering both theoretical insights and practical advancements in LLM alignment."
        },
        {
            "title": "Appendix",
            "content": "A. Judging prompt As illustrated in Figure 3, the user prompt is strategically constructed following the self-rewarding paradigm. Additionally, we construct system prompt carefully as Figure 4. The DPO Objective The DPO loss is defined as: LDPO(πy θ ; πref ) = E(x,yw,yl)D[log σ(ˆr)], where ˆr represents the reward difference between the policy model πθ and reference model πref : (cid:18) ˆr = β log πy θ (ywx) πref (ywx) log πy θ (ylx) πref (ylx) (cid:19) . (7) Here: πy θ : The current policy model being trained (e.g., Mi+1 in our analysis). πref : fixed reference model (e.g., Mi). yw: The preferred or chosen response. yl: The dispreferred or rejected response. β: temperature parameter scaling the reward. By rearranging the terms of ˆr in (7), the optimization goal could be given more clearly: ˆr = β[ (log πy (cid:124) θ (ywx) log πy (cid:123)(cid:122) Implicit reward of policy model (log πref (ywx) log πref (ylx)) (cid:125) (cid:123)(cid:122) Implicit reward of reference model θ (ylx)) (cid:125) (cid:124) ] (8) Inference: The objective of DPO is to maximize the gap between the policy models and the reference models implicit rewards for the preference pair (yw, yl). In other words, it encourages the policy πθ to be better at discriminating between chosen and rejected samples than the reference model πref . Gradient Derivation To understand how the model parameters θ are updated, we derive the gradient of the loss for single sample, = log σ(ˆr). Applying the chain rule, and knowing that the derivative of the sigmoid function σ(z) = σ(z)(1 σ(z)), we get: θL = = 1 σ(ˆr) 1 σ(ˆr) θσ(ˆr) σ(ˆr)(1 σ(ˆr)) θ ˆr = (1 σ(ˆr)) θ ˆr. (9) The gradient of ˆr with respect to θ only depends on the terms involving πθ: θ ˆr = β(θ log πy θ (ywx) θ log πy θ (ylx)). (10) Combining these, we arrive at the final gradient expression: θLDPO (1 σ(ˆr)) β(cid:0)θ log πy θ log πy θ (ywx) θ (ylx))(cid:1). (11) Inference: The gradient update consists of two key components: Figure 3: User Prompt of Judging in Self-Rewarding paradigm. Figure 4: System Prompt of Judging in Self-Rewarding paradigm. B. DPO Loss Function and Gradient Derivation The foundation of our analysis is the DPO loss function, which aims to directly optimize policy model on static dataset of preferences. Weighting Term (1 σ(ˆr)): This term modulates the magnitude of the update. When the model is uncertain or incorrect (i.e., ˆr is small or negative), this weight approaches 1, leading to large update. When the model is confident and correct (ˆr is large), the weight approaches 0, reducing the update. Direction Term (θ log πy θ (yl)): This term dictates the update direction, effectively increasing the log-probability of the chosen response yw and decreasing the log-probability of the rejected response yl. θ (yw) θ log πy Applying the Gradient Analysis We now use this gradient derivation to analyze the dynamics of both standard and temporal self-rewarding methods. In this iterative context, we are training model Mi+1 with Mi as the reference. Theoretical Analysis of Gradient Similarity. While deduplication ensures yw = yl at the string level, it does not prevent them from being semantically equivalent paraphrases, which highly capable model Mi is adept at generating. We argue that this semantic proximity is the root cause of the vanishing gradient direction, even when the loglikelihoods are not identical. θ (yx) be the log-likelihood function for sequence y. By the definition of the high-dimensional internal representation Rd, there exists bijection mapping : Rd between and such that hw = h(yw) and hl = h(yl). core property of well-trained neural networks is that semantically similar inputs are mapped to nearby points in the representation space. Thus, for paraphrases yw and yl, we have Let πy hw hl 0. Note that under the change of variables, the density transforms as: πy θ (yx) = πh θ (h(y)x) we have (cid:12) (cid:12) (cid:12) (cid:12) h(y) (cid:12) (cid:12) , (cid:12) (cid:12) θ log πy θ (ywx) θ log πy θ (ylx) θ (hwx) θ log πh = θ log πh θ (hlx). By the Mean Value Theorem for vector-valued functions, we can bound the difference between the gradients at hw and hl: θ log πh sup θ (hwx) θ log πh θ (hlx) Jh(g)(λhw + (1 λ)hl) hw hl, 0λ1 (12) where Jh(g) is the Jacobian of the gradient function with respect to h, which corresponds to the mixed Hessian hθ log πh θ (hx). We denote this supremum by (cid:13) (cid:13)Jh(g)(cid:0)λhw + (1 λ)hl (cid:1)(cid:13) (cid:13). Chw,hl = sup 0λ1 The boundedness of Chw,hl follows from the fact that the Jacobian Jh(g) is continuous and the line segment {λhw + (1 λ)hl : 0 λ 1} is compact, so the continuous function attains its maximum on this set. The continuous property then guarantees that (cid:13) (cid:13)Jh(g)(cid:0)λhw + (1 λ)hl (cid:1)(cid:13) (cid:13) < . sup 0λ"
        },
        {
            "title": "This indicates that",
            "content": "θ log πh θ (hwx) θ log πh Chw,hl hw hl 0. θ (hlx) We hence complete the proof of Theorem 1. This demonstrates rigorously that even if the log-likelihoods are not identical, their corresponding gradient vectors converge towards each other as their underlying semantic representations become increasingly similar. This leads to the cancellation of the directional signal in the DPO update, causing learning to stagnate. Scenario A: Standard Self-Rewarding (The Problem) Setup: Policy model πθ = Mi+1 Reference model πref = Mi Both yw and yl are generated by the current model, Mi. Analysis: As the iteration progresses, the capability of Mi improves. Consequently, the intrinsic quality of both its best outputs (yw) and its worst outputs (yl) rises. The qualitative gap between them narrows. This has direct impact on ˆr. As Mi+1 begins its training (where Mi+1 Mi), the two main terms in Equation 8 become nearly equal: (log Mi+1(yw) log Mi+1(yl)) (log Mi(yw) log Mi(yl)) This leads to ˆr 0. As ˆr approaches zero, the weighting term (1 σ(ˆr)) approaches constant 0.5. More critically, the underlying signalthe distinguishability between yw and ylis diminished. The optimization landscape flattens, providing weak and ambiguous signal for the model, thus impeding further learning. Scenario B: Temporal Self-Rewarding (The Solution) This method decouples the data generation to counteract signal decay. Setup: Policy model πθ = Mi+1 Reference model πref = Mi **Anchored Rejection**: yl is sampled from an early, fixed, and weaker model, M0. **Future-Guided Chosen**: yw is sampled from stronger, temporary future model, Mf . Analysis: 1. Effect of Anchored Rejection: By anchoring yl to consistently low-quality source (M0), the method prevents the quality of negative samples from inflating. Both the reference model Mi and the policy model Mi+1 can easily assign very low log-probability to mization for memory-efficient training. The SFT models are trained for 3 epochs with learning rate of 2.0 106 and global batch size of 32 (4 per device * 8 GPUs), using cosine learning rate scheduling with 10% warmup ratio. For DPO training, the models are trained for 1 epoch with learning rate of 5.0 107, β = 0.1 and global batch size of 32 (4 per device * 8 GPUs). The training use cosine learning rate scheduling with 10% warmup ratio, maintaining the maximum sequence length of 2048 tokens and with maximum prompt length of 512 tokens. E. Details of Judge Model Ablation We employ an off-the-shelf external model Au toJ (in 6B and 13B variants), to score the responses throughout the entire Self-Rewarding and Temporal Self-Rewarding workflows, enabling comparison of the model improvement achieved by each method. We demonstrate our detailed results of all iterations in Table 5. F. Details of Generalization Experiment To evaluate the generalization capability of our approach across different model architectures and scales, we test it on Qwen2.5-7B, Llama3.1-8B, Mistral-7B, Llama3.2-3B and Llama3.1-70B. We demonstrate our detailed results of all iterations in Table 6. G. AlpacaEval win rate across categories. To assess model capabilities across diverse task domains, we conduct comprehensive evaluation of Qwen2.5-7B and Mistral-7B across different categories from the AlpacaEval 2.0 benchmark. Retaining the optimal iteration models from both Self-Rewarding and Temporal Self-Rewarding methodologies, our analysis reveals that Temporal Self-Rewarding achieves consistent performance improvements over both baseline Self-Rewarding and the initial SFT model in virtually all categories. this poor sample. This makes the θ log πθ(yl) part of the gradients direction term consistently large and provides clear avoid this signal. 2. Effect of Future-Guided Chosen: The chosen sample yw comes from model Mf that is more capable than the current reference model Mi. The reference model Mi will likely assign modest log-probability, log Mi(yw), to this advanced sample. However, the policy model Mi+1 is explicitly trained to learn to generate such higher-quality outputs. Its objective is to make log Mi+1(yw) very high. Overall Impact on the Gradient: This past-future decoupling artificially preserves and widens the quality gap between yw and yl. It systematically forces large difference between the policy models implicit reward and the reference models implicit reward, ensuring that the reward signal ˆr remains large and positive. Consequently, the gradient (Equation 11) remains strong, clear, and stable throughout the training process, effectively resolving the signal decay problem inherent in the standard selfrewarding approach. C. Inference details We use batched inference of vllm to accelerate the generation and judging process. The generation parameters are temperature=1.0, top p=1.0 and max token=1024. Iter AlpacaEval 2.0 ArenaHard MT-Bench LC Win Win Score Average Method SFT SR (iterJudge) TSR (iterJudge) SR (autoj-6b) TSR (autoj-6b) - 0 1 2 0 1 0 1 2 3 0 1 SR (autoj-13b) 0 1 2 3 TSR (autoj-13b) 0 1 14.59 19.42 21.43 19.59 26.61 19. 8.73 13.29 17.00 17.54 19.92 5.96 10.99 15.71 17.08 19.69 19.07 20.48 27.94 29.44 14.84 16.28 19.13 18.15 20.50 19.64 20.95 22.36 22.67 22.30 24.47 28.57 14.31 22.80 28.07 28. 31.30 35.40 6.3 6.7 7.7 9.4 8.8 11.3 14.6 8.6 8.0 10.2 10.0 10.7 15.2 9.5 9.8 12.0 13.8 14.7 22.4 4.81 5.33 5.60 5.74 5.66 5.79 5.89 5.46 5.52 5.54 5.45 5.74 5.83 5.33 5.55 5.62 5.58 5.78 5.69 Table 5: Detailed results of all iterations of Self-Rewarding and Temporal Self-Rewarding(SR and TSR) using different models as judge(iterJudge/autoj-6b/autoj-13b). The best results of each baseline are in bold. The marker represents the best results of all baselines. D. Training details We conduct supervised fine-tuning (SFT) and direct preference optimization (DPO) with DeepSpeed ZeRO-3 optiFigure 5: AlpacaEval win rate breakdown for instruction categories of Qwen2.5-7B and Mistral-7B. Temporal SelfRewarding models give gains across nearly all topics than Self-Rewarding and SFT intial. Method Iter AlpacaEval 2.0 ArenaHard MT-Bench LC Win Win Score Average Model Llama3B Llama8B SFT SR TSR SFT SR TSR SFT Llama70B SR Qwen7B TSR SFT SR TSR SFT Mistral7B SR TSR - 0 1 2 3 0 1 - 0 1 2 3 0 1 - 0 1 2 3 0 1 - 0 1 2 3 0 - 0 1 2 3 0 1 2.99 3.93 2.74 2.57 3.37 4.79 3.41 2.86 3.85 2.86 2.73 3.42 5.71 8.20 5.96 8.73 10.99 13.29 15.71 17.00 17.08 17.54 19.69 19.92 19.07 20.48 27.94 29.44 12.80 19.96 22.92 29.42 28.20 33.51 29.88 33.14 32.91 35.57 23.11 30.33 38.70 33.66 7.70 11.45 12.92 19.82 15.53 21.66 15.65 20.24 18.14 21.53 24.78 27.85 34.01 35.90 8.45 12.72 15.09 17.98 17.70 18.97 26.15 24.10 27.58 25.48 32.11 32.05 35.16 30.58 1.2 1.8 1.5 2.2 2.3 2.0 2.9 6.3 6.7 7.7 9.4 8.8 11.3 14.6 13.0 26.2 29.2 34.8 38.9 30.7 40.1 12.7 18.4 19.5 22.0 21.5 27.2 34.4 6.3 9.1 9.8 9.8 12.8 14.0 15.7 4.06 3.91 3.87 4.08 4.03 4.25 4.32 4.81 5.33 5.60 5.74 5.66 5.79 5.89 6.06 6.66 6.86 6.98 6.93 6.75 6.98 5.51 5.93 6.12 6.00 6.09 6.25 6.29 5.28 5.55 5.35 5.49 5.68 5.76 5.49 Table 6: Detailed results of all iterations of Self-Rewarding and Temporal Self-Rewarding(SR and TSR) across models of different families and scales. The best results of each baseline are in bold. The marker represents the best results of all baselines. References Aminabadi, R. Y.; Rajbhandari, S.; Awan, A. A.; Li, C.; Li, D.; Zheng, E.; Ruwase, O.; Smith, S.; Zhang, M.; Rasley, J.; et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, 115. IEEE. Anil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403. Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; et al. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Chen, Z.; Deng, Y.; Yuan, H.; Ji, K.; and Gu, Q. 2024. Selfplay fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cui, G.; Yuan, L.; Ding, N.; Yao, G.; He, B.; Zhu, W.; Ni, Y.; Xie, G.; Xie, R.; Lin, Y.; Liu, Z.; and Sun, M. 2024. UltraFeedback: Boosting Language Models with Scaled AI Feedback. arXiv:2310.01377. Diamantidis, A. D.; and Chatzoglou, P. D. 2014. Employee post-training behaviour and performance: evaluating the results of the training process. International Journal of Training and Development, 18(3): 149170. Google. 2023. Bard. Huang, A.; Block, A.; Foster, D. J.; Rohatgi, D.; Zhang, C.; Simchowitz, M.; Ash, J. T.; and Krishnamurthy, A. 2024. Self-Improvement in Language Models: The Sharpening Mechanism. arXiv preprint arXiv:2412.01951. Huang, J.; Gu, S. S.; Hou, L.; Wu, Y.; Wang, X.; Yu, H.; and Han, J. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. arXiv:2310.06825. Kirk, R.; Mediratta, I.; Nalmpantis, C.; Luketina, J.; Hambro, E.; Grefenstette, E.; and Raileanu, R. 2023. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452. Kopf, A.; Kilcher, Y.; Von Rutte, D.; Anagnostidis, S.; Tam, Z. R.; Stevens, K.; Barhoum, A.; Nguyen, D.; Stanley, O.; Nagyfi, R.; et al. 2023. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36: 4766947681. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Lanchantin, J.; Chen, A.; Dhuliawala, S.; Yu, P.; Weston, J.; Sukhbaatar, S.; and Kulikov, I. 2025. Diverse Preference Optimization. arXiv preprint arXiv:2501.18101. Li, J.; Sun, S.; Yuan, W.; Fan, R.-Z.; Zhao, H.; and Liu, P. 2023a. Generative Judge for Evaluating Alignment. arXiv preprint arXiv:2310.05470. Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu, B.; Gonzalez, J. E.; and Stoica, I. 2024. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv preprint arXiv:2406.11939. Li, X.; Yu, P.; Zhou, C.; Schick, T.; Levy, O.; Zettlemoyer, L.; Weston, J.; and Lewis, M. 2023b. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259. Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023c. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca eval. Li, Y.; Hu, X.; Qu, X.; Li, L.; and Cheng, Y. 2025a. TestTime Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback. arXiv e-prints, arXiv:2501.12895. Li, Y.; Wang, Z.; Fu, T.; Cui, G.; Yang, S.; and Cheng, From Drafts to Answers: Unlocking LLM Y. 2025b. arXiv e-prints, Potential via Aggregation Fine-Tuning. arXiv:2501.11877. Liu, T.; Zhao, Y.; Joshi, R.; Khalman, M.; Saleh, M.; Statistical rejection samLiu, P. J.; and Liu, J. 2023. arXiv preprint pling improves preference optimization. arXiv:2309.06657. OpenAI. 2023. Gpt-4 technical report. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744. Qu, Y.; Zhang, T.; Garg, N.; and Kumar, A. 2024. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems, 37: 5524955285. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36: 5372853741. Razin, N.; Wang, Z.; Strauss, H.; Wei, S.; Lee, J. D.; and Arora, S. 2025. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477. Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. F. 2020. Learning to summarize with human feedback. Advances in neural information processing systems, 33: 3008 3021. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang, Y. 2024a. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. Wang, Z.; He, W.; Liang, Z.; Zhang, X.; Bansal, C.; Wei, Y.; Zhang, W.; and Yao, H. 2024b. Cream: Consistency regularized self-rewarding language models. arXiv preprint arXiv:2410.12735. Wang, Z.; Hou, L.; Lu, T.; Wu, Y.; Li, Y.; Yu, H.; and Ji, H. 2023. Enabling Language Models to Implicitly Learn SelfImprovement. arXiv preprint arXiv:2310.00898. Wu, T.; Yuan, W.; Golovneva, O.; Xu, J.; Tian, Y.; Jiao, J.; Weston, J.; and Sukhbaatar, S. 2024. Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. arXiv preprint arXiv:2407.19594. Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yu, X.; Peng, B.; Galley, M.; Gao, J.; and Yu, Z. 2023. Teaching language models to self-improve through interactive demonstrations. arXiv preprint arXiv:2310.13522. Yuan, W.; Pang, R. Y.; Cho, K.; Li, X.; Sukhbaatar, S.; Xu, J.; and Weston, J. 2024. Self-Rewarding Language Models. arXiv e-prints, arXiv:2401.10020. Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414. Zhang, H.; Cui, H.; Bao, G.; Yang, L.; Wang, J.; and Zhang, Y. 2025. Direct Value Optimization: Improving Chain-ofThought Reasoning in LLMs with Refined Values. arXiv e-prints, arXiv:2502.13723. Zhang, S.; Liu, X.; Zhang, X.; Liu, J.; Luo, Z.; Huang, S.; and Gong, Y. 2025. Process-based self-rewarding language models. arXiv preprint arXiv:2503.03746. Zhang, X.; Li, Z.; Zhang, Y.; Long, D.; Xie, P.; Zhang, M.; and Zhang, M. 2023. Language Models are Universal Embedders. arXiv preprint arXiv:2310.08232. Zhang, Y.; Schwarzschild, A.; Carlini, N.; Kolter, Z.; and Ippolito, D. 2024. Forcing diffuse distributions out of language models. arXiv preprint arXiv:2404.10859. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623. Zhou, X.; Guo, Y.; Ma, R.; Gui, T.; Zhang, Q.; and Huang, X. 2025. Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models. arXiv preprint arXiv:2502.08922."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Central South University",
        "National University of Singapore",
        "North Carolina State University",
        "Peking University",
        "Southeast University",
        "Tsinghua University",
        "University of Michigan",
        "Westlake University"
    ]
}