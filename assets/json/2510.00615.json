{
    "paper_title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
    "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement."
        },
        {
            "title": "Start",
            "content": "ACON: OPTIMIZING CONTEXT COMPRESSION FOR LONG-HORIZON LLM AGENTS Minki Kang1 Wei-Ning Chen2 Dongge Han2 Huseyin A. Inan2 Lukas Wutschitz2 Yanzhi Chen2, 3 Robert Sim2 1KAIST 3University of Cambridge 2Microsoft Saravan Rajmohan2 5 2 0 O 1 ] . [ 1 5 1 6 0 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as longhorizon agents with up to 46% performance improvement."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have become the backbone of AI agents across diverse real-world tasks, leveraging language knowledge and reasoning to plan, act, and adapt in dynamic environments (Yao et al., 2023). These tasks often unfold over extended horizons: the agent must gather information, invoke tools, and revise plans based on feedback. In such settings, context is not auxiliary but foundational, as tool APIs and data formats lack standardization (Anthropic, 2024a), and even personalized environments differ in file structures or account data. Agents must therefore accumulate and maintain records of prior actions, observations, and world state. Losing this information, such as an email identifier, file version, or API format, can derail task success. Figure 1: AccuracyPeak tokens trade-off on AppWorld (Trivedi et al., 2024). We compare average accuracy versus peak input tokens in history compression. ACON (ours) reduces cost while preserving accuracy for the large model (gpt-4.1) relative to naive prompting baseline, and even improves accuracy on smaller models (gpt-4.1-mini and Qwen-14B). More results are in Section 4. Work done during the internship at Microsoft. Contact: minkikang@kaist.ac.kr 1 Figure 2: Motivation: Unbounded context in LLM agents. As LLM agents interact with environments, actions and observations continuously accumulate, leading to ever-growing contexts that incur high memory usage as in the red line on the right plot. This motivates Agent Context Optimization (ACON), which optimally compresses histories and observations into concise summaries, reducing peak tokens and memory as in the blue line on the right plot. The centrality of context, however, makes it bottleneck. As interactions accumulate, contexts grow unbounded as in Figure 2, creating two major challenges. First, the inference cost of transformers scales with the context length, which becomes prohibitive in long-horizon tasks. Second, excessively long contexts dilute relevant information, distracting the model with outdated or extraneous details (Shi et al., 2023). These issues make effective context management and compression indispensable. Existing compression approaches only partially address this need. Dialogue-oriented systems rely on session-level summarization or tiered memories suitable for conversational coherence but inadequate for multi-step carry-over (Packer et al., 2023). Document-centric methods in long-context QA or in-context learning (Li et al., 2023; Jiang et al., 2024) assume single-step reasoning where context can be discarded after producing an answer. While effective in their domains, these strategies fall short for complex multi-step tasks where success depends on retaining structured signals across many interactions. The challenge becomes particularly critical in productivity scenarios (e.g., email management, document processing, or workflow automation) where agents must coordinate across heterogeneous tools and maintain precise state information. Unlike simpler agent tasks, these environments demand preservation of diverse signal types: factual history, actionoutcome relationships, evolving environment states, success preconditions, and future decision cues. Naive strategies like token truncation or generic summarization easily lose critical details essential for long-horizon reasoning. Recent agent-focused compression methods (Deng et al., 2023; Lee et al., 2025; Smith, 2025) either specialize narrowly or rely on brittle heuristics, limiting their applicability across the full spectrum of multi-step agent tasks. We address these challenges with Agent Context Optimization (ACON), unified framework for systematic and adaptive context compression. Our study yields three key findings. First, taskand environment-specific guidelines enable consistent context compression across diverse agents without sacrificing performance. Second, optimized contexts not only reduce costs but also improve decision quality, allowing smaller LLMs to act more effectively. Third, high-quality compressors can be distilled into even smaller models, reducing overhead and improving deployability. ACON dynamically condenses environment observations and interaction histories into concise, informative representations. Rather than handcrafting prompts, we introduce guideline optimization pipeline that refines compressor prompts via failure analysis in natural language space (Pryzant et al., 2023; Khattab et al., 2024; Yüksekgönül et al., 2025; Han et al., 2025), ensuring that critical environment-specific and task-relevant information is retained. Importantly, ACON is gradient-free, requiring no parameter updates, making it directly usable with closed-source or production models. We further distill optimized compressors into smaller models for cost-efficient deployment. We validate ACON on three multi-step agent benchmarks: AppWorld (Trivedi et al., 2024), OfficeBench (Wang et al., 2024b), and Multi-objective QA (Kwiatkowski et al., 2019; Zhou et al., 2025), each requiring 15+ interaction steps. Our empirical results demonstrate clear advantages of ACON: (i) lowers memory usage by 2654% (peak tokens) while largely maintaining task perfor2 mance; (ii) enables effective distillation of the context compressor into smaller models, preserving 95% of the teachers accuracy across all benchmarks, thereby reducing the overhead of the additional module; (iii) allows small LMs to function more effectively as agents, improving performance by 32% on AppWorld, 20% on OfficeBench, and 46% on Multi-objective QA by mitigating the distraction of long contexts through ACON. Our result highlights on AppWorld benchmark are in Figure 1. To summarize, our work makes the following contributions: We propose Agent Context Optimization (ACON), framework for compressing both environment observations and interaction histories, tailored to multi-step, long-horizon agentic tasks. We develop failure-driven, task-aware compression guideline optimization. Our approach is entirely gradient-free and readily applicable to any LLMs, including API-based models. We provide cost-efficient solution by distilling optimized compressors into smaller models, preserving over 95% of the teachers performance while reducing module overhead. We validate ACON on AppWorld, OfficeBench, and Multi-objective QA, showing that it reduces memory usage by 26-54% (peak tokens) while preserving task success with large LLMs, and enabling small LMs to achieve 20-46% performance improvements."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Long-horizon LLM agents. Large language model (LLM) agents extend pretrained models beyond static single-step reasoning tasks (e.g., RAG-based QA, math problem solving, or code generation) to interactive decision-making in dynamic environments (Yao et al., 2023; Wang et al., 2024a; Team et al., 2025; OpenAI, 2025a). Unlike chatbots or solvers that return an answer in one pass, agents must iteratively observe their surroundings, select tools, and execute actions while revising their plans based on feedback (Shridhar et al., 2021; Jimenez et al., 2024; Zhou et al., 2024; Wei et al., 2025; Xie et al., 2024; Bonatti et al., 2024). Recent work highlights the importance of longhorizon LLM agents, which tackle tasks that unfold over dozens to hundreds of steps and require coordination across multiple applications and tools (Kwa et al., 2025; Trivedi et al., 2024; Wang et al., 2024b; 2025b). central challenge in these scenarios lies in managing the dynamic long context, where the agent must retain multi-step interaction histories and handle diverse observations produced by heterogeneous environments. Context compression for LLMs. Managing this ever-growing context has been longstanding challenge, and variety of approaches have been proposed to compress LLM inputs. Prior works on context compression can be broadly grouped into three directions: documentor retrieval-based compression (Seo et al., 2025; Li et al., 2023; Xu et al., 2024; Yoon et al., 2024; Zhou et al., 2025; Jiang et al., 2024; Shandilya et al., 2025), dialogue memory summarization (Xu et al., 2025; Maharana et al., 2024; Wang et al., 2025a), and low-level KV cache compression (Zhang et al., 2025). While each line of research has demonstrated benefits in its respective setting, they remain insufficient for the dynamic and heterogeneous contexts required by long-horizon agents. Beyond these directions, few recent studies have attempted to compress context specifically for LLM agents (Deng et al., 2023; Lee et al., 2025; Smith, 2025; Yang et al., 2024b). However, these approaches either rely on naive prompting or target narrow domains such as the accessibility tree in web browsing, limiting their generality. In contrast, our work introduces universal agent context optimization framework that is applicable to arbitrary agents. It supports both history and observation compression, and incorporates generalizable optimization methodology that is agnostic to the underlying model, making it applicable to both open-source and proprietary API-based LLMs."
        },
        {
            "title": "3 AGENT CONTEXT OPTIMIZATION (ACON)",
            "content": "We present Agent Context Optimization (ACON), unified framework for optimized history and observation compression in long-horizon LLM agents. We begin by formulating the agent task and defining context cost in Section 3.1. Next, in Section 3.2, we introduce generative compression with LLMs for both history and observation, and formalize the associated optimization objective and its challenges. We then propose our optimization method in Section 3.3, followed by distillation that enables smaller models for compressions to improve efficiency (Section 3.4)."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "Task. An agentic task is formulated as Partially Observable Markov Decision Process (POMDP) = S, A, O, , with state space S, action space A, and observation space O. The transition function (s, a) (s, o) is deterministic, and it is determined by the implementation of the environment. Specifically, it executes an action in the environment and returns the next state and observation. The reward function returns the reward given the terminal state sT . The terminal state is arrived when the transition function receives the special action (e.g., finish_task). An LLM agent interacts with the environment to get information for making decision to achieve given task o0 through multiple steps. For each step t, the LLM generates the action at followed by its reasoning at each step (Yao et al., 2023; Wang et al., 2024a) given the interaction history ht1 = (o0, a0, o1, a1, , ot1, at1) and the latest observation ot: M(ot, ht1; θ, Pagent) (cid:55) at, (1) where θ refers to the pre-trained parameters of the LLM and Pagent is the prompt that consists of general environment description, tools, output format, and few-shot examples in natural language. Cost function for context. We assume that the LLM agents parameters θ and the task and system prompt Pagent are fixed. We define cost function that measures the cost of encoding the dynamic context during action generation at each step such as O(n) computational cost of transformer for decoding given input tokens. The cost function takes the interaction history ht1, and the latest observation ot as input and returns the per-step cost: C(H) = (cid:88)T t=1 C(ht1, ot), (2) where is the total cost of completing the task, = {ht1, ot}T t=1 denotes the sequence of history and observation of each step. Typically, is proportional to the summation of token lengths of action and observations in each step, ht1 and ot. While the prompt cost is static and can be budgeted in advance, the costs from interaction histories are unbounded, leaving the user with only two options: terminate the task early or truncate the context heuristically to maximum length. This raises the central question: how can we compress context more effectively than such heuristics?"
        },
        {
            "title": "3.2 HISTORY & OBSERVATION COMPRESSION WITH LLMS",
            "content": "To address this challenge, we use an LLM (; ϕ, P), parameterized by pre-trained weights ϕ and compression guideline P, to minimize context cost defined in Equation 2 (e.g., summarize the given interaction history). As in Equation 1, the LLM receives two inputs at each step: the interaction history ht1 and the latest observation ot. This introduces two options for context compression: History compression. The interaction history accumulates both environment observations and agent actions. In long-horizon tasks, this history can grow excessively large. To manage its length, we apply history compression only when the history length exceeds predefined threshold Thist: = (ht; ϕ, Phist) if ht > Thist, ht otherwise. (3) The compressed history replaces the raw history in Equation 1. This selective compression ensures that the overhead of invoking the compressor is incurred only when necessary (Smith, 2025). Latest observation compression. Given an action a, the environment returns an observation according to the transition function (s, a) (s, o). We similarly apply observation compression only when the observation length exceeds threshold Tobs: = (ot, ht1; ϕ, Pobs) if ot > Tobs, ot otherwise. (4) This mechanism avoids unnecessary overhead when ot is already short, while still reducing redundant or distracting content in long observations (Xu et al., 2024; Deng et al., 2023; Lee et al., 2025). The compressed one replaces the raw one in Equation 1 and is stored in the interaction history h. In both cases, the compressor LLM selects information to preserve based on its learned prior knowledge of importance. However, there is no guarantee that the salient details required for successful Figure 3: Compression Guideline Optimization. Feedback is generated by contrasting successful trajectories (no compression) with failed ones (with compression). The collected feedback is then used by LLM to refine the compression guidelines. task completion are retained. The agent context effectively serves as world model of the environment, encompassing diverse forms of information such as causal relations (e.g., email leaves drafts), evolving states (e.g., account balance), preconditions (e.g., login required), and task-relevant decision cues (e.g., due dates). Effective context compression must therefore accommodate this heterogeneous and dynamic nature of agent context, ensuring that the most critical signals are preserved for long-horizon reasoning and task success. Optimization objective. We optimize the compressor parameters ψ (ϕ, P) to maximize task reward while minimizing context cost. At each step t, the compressor produces either compressed history = fobs(ot, ht1; ψ). Let the compressed context be = fhist(ht; ψ) or observation (ψ) = { t1, }T t=1, C(H (ψ)) = (cid:88)T t= C(h t1, t). (5) With the agent M(; θ, Pagent) fixed, the environment induces trajectory τ (ψ) and terminal state sT (ψ) when the agent conditions on (ψ). Our learning objective is E(cid:2) R(cid:0)sT (ψ)(cid:1) (cid:3) (cid:124) (cid:123)(cid:122) (cid:125) maximize λ E(cid:2) C(cid:0)H (ψ)(cid:1) (cid:3) , (cid:125) (cid:123)(cid:122) minimize max ψ λ 0, (6) (cid:124) where λ is multiplier and the expectations are over tasks. Challenges. The optimization objective in Equation 6 is difficult to optimize in practice because there is no gold supervision for compression, the reward is sparse and only revealed at the end of the trajectory, and the context cost is defined over discrete quantities, which precludes direct gradient computation. While these properties naturally motivate reinforcement learning (RL) (Sutton & Barto, 2018), applying RL to this setting introduces additional obstacles: (1) updating the parameters ϕ of LLM with RL can be computationally prohibitive, (2) environment roll-outs are extremely expensive since each reward requires multi-step executions of both agent and compressor LLMs, and (3) policy gradient estimates suffer from high variance because compression quality is only indirectly evaluated through eventual task success."
        },
        {
            "title": "3.3 OPTIMIZING COMPRESSION GUIDELINES",
            "content": "To overcome these challenges, we propose to optimize compression guidelines (natural language prompts) for context compression, rather than fine-tuning model parameters ϕ. Trajectories under compressed contexts provide dense signals about the quality of compression. For example, if the agent fails with compressed context while succeeding without compression, this indicates that the compressed context may have lost crucial information. Such trajectory-level comparisons yield richer feedback than scalar rewards (e.g., binary task success). We instantiate this idea as prompt optimization using an LLM as the optimizer, where the natural language prompt is refined via feedback expressed in natural language (Yang et al., 2024a; Yüksekgönül et al., 2025; Khattab et al., 2024). We introduce compression guideline optimization based on contrastive task feedback. On the training set Dtrain, we run the LLM agent both without and with context compression to obtain baseline context and compressed context . We collect tasks where the agent succeeds 5 with but fails with , forming contrastive subset Dcont. For each task in Dcont, we query optimizer LLM with the context before and after compression to obtain natural language feedback: Feedback = LLM(Feedback Instruction, H, ). (7) This feedback serves as natural language gradient (Yüksekgönül et al., 2025), indicating how the compression guideline should be refined. We then aggregate feedback from multiple trajectories: (1) = LLM(Update Instruction, (0), i=1Feedbacki), (8) where is concatenation of feedbacks from each task, which corresponds to batch optimization step in textual gradient descent (Yüksekgönül et al., 2025). We also generate multiple candidate prompts {P (1) }, evaluate them on Dcont, and select the best-performing one. We refer this process as utility maximization step UT as it primarily maximizes the first term (task reward) of Equation 6. However, optimizing only for reward may neglect the context cost (second term in Equation 6). To address this, motivated by alternating optimization, we perform second iteration that conditions only on successful task with compressed context, asking the LLM to generate feedback about which information was actually used during execution. This refines (1) (2), encouraging shorter yet sufficient contexts. We refer this additional process as compression maximization step CO as it minimizes the second term (context cost) of Equation 6. We illustrate overall process in Figure 3. Algorithm 1 and prompts are in Appendix B."
        },
        {
            "title": "3.4 DISTILLING CONTEXT COMPRESSION INTO SMALL MODELS",
            "content": "While compression guideline optimization enables effective compression, repeatedly invoking the large LLM for compression adds substantial overhead. To reduce this cost, we distill the compressor into smaller model. The teacher with optimized guideline (parameters ϕT) generates compressed outputs from input x, which supervise the student (parameters ϕS). We train the student with cross-entropy objective (Kim & Rush, 2016) with input-output pair (x, y), where (x, y) = (ht, t) for Equation 3 or (x, y) = ((ht1, ot), min ϕS (x,y)D+ train (cid:34) Ly (cid:88) n=1 t) for Equation 4: (cid:35) , log (yn x, y<n; ϕS, ) (9) where D+ train denotes tasks where the teacher succeeds with compressed context. Once trained, the student replaces the teacher during inference, decoupling decision making from compression. This two-stage pipeline, guideline optimization then distillation, achieves effective compression with much smaller model (ϕT ϕS): (; ϕT, P)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "prompt optimization (; ϕT, ) distillation (; ϕS, ). (10) We evaluate ACON on three challenging benchmarks that require multi-step interactions across diverse domains. Our experiments are designed to address the following key questions: How well does ACON improve token efficiency while preserving performance? (Section 4.2) Does distilling the compressor reduce its size while maintaining agent performance? (Section 4.3) Can ACON help small, distilled LM agents perform better under long contexts? (Section 4.4)"
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Benchmarks & Metrics. We focus on long-horizon agentic task benchmarks that require 10+ interaction steps on average: (1) AppWorld (Trivedi et al., 2024): Main benchmark with 9 simulated apps (e.g., Venmo, Spotify, SimpleNote) and 100 simulated users. Performance is measured by task completion score. (2) OfficeBench (Wang et al., 2024b): Productivity tasks across 6 apps (e.g., Word, Excel, Email), operating on simulated documents. Performance is measured by benchmarkdefined accuracy functions. (3) 8-objective QA (Kwiatkowski et al., 2019; Zhou et al., 2025): QA 6 Table 1: Results across different difficulty levels on Appworld benchmark (test-normal). Each block reports accuracy (task goal completion score), steps, peak input tokens (103), and dependency (106) for agents. Best results in each column are highlighted in bold. Rows in blue background indicate the results from ours . ACON consistently improves accuracy while reducing peak tokens and dependency, with ACON UTCO achieving the best overall performance. Method Average (168) Easy (57) Medium (48) Hard (63) Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Agent: gpt-4.1 / Compressor: gpt-4.1 No compression 56.0 16. 9.93 5.96 80.7 7.57 2.98 47. 10.10 5.36 39.7 11.95 9.11 History Compression FIFO Retrieval LLMLingua Prompting ACON UT ACON UTCO 45.8 27.4 39.3 43.5 51.2 56.5 Observation Compression LLMLingua Prompting ACON UT ACON UTCO 32.1 42.3 47.0 53.6 28.48 33.17 24.42 24.01 20.92 22.82 18.16 17.38 16.67 18.12 6.73 8.39 7.50 6.93 7.17 7. 8.17 6.58 7.62 7.43 5.69 6.68 6.37 5.29 4.49 4.69 6.01 4.09 5.08 4.93 84.2 61.4 66.7 66.7 77.2 86.0 54.4 64.9 70.2 82.5 5.85 7.40 6.38 6.36 6.45 7. 5.78 4.92 5.87 5.66 2.89 3.97 3.04 2.84 2.43 2.84 2.33 1.88 2.21 2.63 39.6 12.5 37.5 41.7 50.0 56.2 29.2 35.4 45.8 47.9 7.26 8.74 8.04 7.10 7.39 7. 8.24 6.96 7.79 7.30 6.24 7.72 7.39 5.36 4.47 4.43 5.23 4.11 5.00 4.43 15.9 7.9 15.9 23.8 28.6 30.2 14.3 27.0 27.0 31.8 7.14 9.02 8.09 7.31 7.65 7. 10.29 7.79 9.07 9.14 7.80 8.33 8.59 7.48 6.37 6.55 9.92 6.07 7.73 7.50 Table 2: Results on OfficeBench and 8-objective QA benchmarks. We report performance metrics (acc/EM/F1) along with steps, peak input tokens (103), and dependency (106). Best values are in bold. Rows in blue are ours . ACON consistently improves accuracy/efficiency trade-offs. Method (a) OfficeBench Steps Acc. Peak Dep. Method (b) 8-objective QA Steps F1 EM Peak Dep. Agent: gpt-4.1 / Compressor: gpt-4. Agent: gpt-4.1 / Compressor: gpt-4.1 No Compression 76.84 11.52 7.27 4. No compression 0.366 0.488 15.78 10.35 3. History Compression FIFO Retrieval LLMLingua Prompting ACON UT ACON UTCO 67.37 65.26 70.53 71.58 74.74 72.63 Observation Compression LLMLingua Prompting ACON UT ACON UTCO 71.58 55.79 73.68 72.63 12.26 16.20 10.89 10.13 13.13 11.54 11.89 12.24 10.83 10. 4.02 4.33 4.65 4.40 4.93 4.54 7.38 6.44 6.55 6.17 2.64 2.06 1.85 1.10 3.85 1.91 6.14 2.68 3.85 2.88 History Compression FIFO Retrieval LLMLingua Prompting ACON UT ACON UTCO 0.293 0.331 0.363 0.376 0.373 0. Observation Compression LLMLingua Prompting ACON UT ACON UTCO 0.320 0.288 0.364 0.336 0.388 0.438 0.481 0.478 0.494 0.458 0.414 0.397 0.475 0.461 19.26 20.06 17.68 18.70 17.14 17.79 14.23 11.64 16.33 14. 5.09 5.11 5.68 4.73 4.71 4.65 5.16 3.41 4.97 4.22 2.51 2.62 2.24 1.66 1.57 1.50 1.35 0.45 1.28 0.81 benchmark where agents interact with search tool to answer 8 questions and output consolidated answer set. Performance is the average of Exact Match (EM) and F1 scores across 8 questions. In addition to task-specific performance metrics, we report three token efficiency metrics following prior work (Zhang et al., 2025; Zhou et al., 2025): (1) Steps: The average number of interaction steps per task. (2) Peak Tokens: The maximum context length encountered across all steps. (3) Dependency: The cumulative dependency of each generated action on prior tokens, measuring how much generation relies on the context history. Full details are provided in the Appendix B. (1) No Compression: full uncompressed context. (2) FIFO: keep the most recent Baselines. interactions, discarding earlier ones (Yang et al., 2024b). (3) Retrieval: select past interactions most similar to the current query via embedding search (Xu et al., 2025). (4) LLMLingua: extractive compression with an encoder-only LM (Jiang et al., 2023; Pan et al., 2024). (5) Prompting: naive baseline using general compression instruction (Smith, 2025; Lee et al., 2025). Our Methods. We evaluate two versions of ACON. (1) ACON UT utilizes an optimized guideline for context compression after utility maximization step. (2) ACON UTCO applies compression maximization step CO after utility maximization UT, aiming for shorter but informative compression."
        },
        {
            "title": "4.2 OVERALL PERFORMANCE AND TOKEN EFFICIENCY ON LLMS",
            "content": "In Table 1 and Table 2, we first evaluate ACON using on gpt-4.1 (OpenAI, 2025b) for both agent and compressor, which already achieves strong results on three long-horizon benchmarks. 7 Figure 4: Results of distilled compressors on history compression with gpt-4.1 as the agent. Student models (Qwen3-14B, Qwen3-8B, Phi-4) are distilled from gpt-4.1 compressor using the optimized compression guideline after UT step, and evaluated across all benchmarks. We also include result with gpt-4.1-mini without distillation for comparison. Figure 5: Performance-efficiency trade-off of the Qwen3-14B agent distilled from gpt-4.1 trajectories. For distilled compressors, we use the same distillation setting as in Figure 4. Compared to the baseline without compression, our framework ACON provides compressed trajectories combined with distilled compressor, enabling the distilled agent to achieve consistently higher accuracy while requiring substantially fewer peak input tokens across all benchmarks. For history compression, as shown in Table 1, on AppWorld, ACON reduces peak tokens by over 25% while preserving the accuracy of the no compression upper bound, outperforming all baselines that suffer severe degradation on medium and hard tasks spanning longer steps. On OfficeBench (Table 2a), ACON lowers peak context size by nearly 30% while maintaining accuracy above 74%. On 8-objective QA (Table 2b), ACON even surpasses the no compression baseline in EM/F1 while reducing peak tokens and dependency by 54.5% and 61.5%, respectively. For observation compression, ACON consistently outperforms all baselines confirming that compression guideline optimization is effective for compressing not only history but also raw observations. Applying only the utility maximization step (UT) improves performance while reducing token cost across all benchmarks, whereas the compression maximization step (CO) further lowers token cost but may slightly hurt accuracy, except in AppWorld where it even yields additional gains."
        },
        {
            "title": "4.3 COMPRESSOR DISTILLATION",
            "content": "We distill the compressor with optimized guidelines after UT step into smaller models such as Qwen3-14B, Qwen3-8B (Yang et al., 2025), and Phi-4 (Abdin et al., 2024) using LoRA (Hu et al., 2022). As shown in Figure 4, distilled compressors retain over 95% of the performance of gpt-4.1 compressor while reducing overhead. We also find that gpt-4.1-mini, even without distillation, can serve as lightweight compressor in OfficeBench and QA. These results confirm that small models are sufficient for compression, enabling the expensive LLM to be only reserved for the agent."
        },
        {
            "title": "4.4 ACON FOR DISTILLED SMALL AGENTS",
            "content": "We examine whether ACON also benefits smaller LLM agents, which are particularly vulnerable to long-horizon inefficiency. Without compression, models such as Qwen3-14B often fail on medium and hard tasks due to distracting context. As shown in Figure 5, ACON substantially improves their performance: on AppWorld, Qwen3-14B improves from 26.8% to 33.9%, and on 8-objective QA from 0.158 to 0.197 EM. These results demonstrate that ACON acts as an equalizer, enabling smaller agents with concise but informative contexts to approach the performance of larger models. 8 Figure 6: Ablation studies on thresholds for compression on AppWorld with gpt-4.1. (1) the number of compressions (compression frequency) for each length of task trajectories (task steps). (2) the performance comparison for each threshold setting. Table 3: Ablation studies on the prompt optimizer in AppWorld, gpt-4.1 agent and history compressor. Default is o3 optimizer with task contrastive feedback. Optimizer model Task contrastive Average Acc. o3 o3 gpt-4.1 gpt51.2 50.6 (-0.6) 47.6 (-3.6) 50.6 (-0.6)"
        },
        {
            "title": "4.5 ANALYSIS",
            "content": "Figure 7: API cost comparison. denotes history and denotes observation compression. Compression threshold: moderate value yields the best trade-off. In Figure 6, we provide ablations on threshold for compression in Equation 3 and Equation 4. Results show that smaller thresholds reduce tokens but incur more frequent compression calls and degrade accuracy, while larger thresholds preserve accuracy with higher cost. Moderate values (4096 for history, 1024 for observation) provide the best trade-off, maintaining accuracy close to no compression while still reducing peak tokens substantially. Prompt optimizer: o3 + contrastive feedback works best. We analyze how the choice of optimizer and the use of contrastive feedback affect compression guideline quality. As shown in Table 3, the default o3 with contrastive feedback yields the best performance, while removing contrastive feedback (only using failed trajectories) or switching to other models results in lower accuracy. Limitation: Cost Analysis. ACON compresses context while maintaining performance, but one limitation remains: the compressor module can introduce extra cost, and history compression can increase costs in agentic tasks. We provide cost analysis, and use an API cost metric that accounts for cheaper cached input tokens to approximate computational overhead. As shown in Figure 7, observation compression reduces cost by compressing inputs, whereas history compression rarely helps due to KV-cache overhead. Compressor distillation partly alleviates this issue by replacing expensive LLM calls with smaller models. We include detailed discussions in Appendix A."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented Agent Context Optimization (ACON), unified framework that systematically compresses both interaction histories and environment observations for long-horizon LLM agents. Unlike prior work that relies on naive prompting or narrow domains, ACON introduces compression guideline optimization in natural language space, enabling adaptive and model-agnostic compression. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces peak tokens by 26-54% while maintaining or even improving task success. Beyond efficiency, we demonstrate that optimized compressors can be distilled into smaller models, substantially lowering overhead without sacrificing performance. Moreover, by supplying concise yet informative contexts, ACON allows small agents such as Qwen3-14B to approach the performance of much larger models. Overall, our findings highlight that ACON lays foundation for more general, cost-effective, and deployable long-horizon LLM agents."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work investigates optimized context compression framework for long-horizon LLM agents. It does not involve human subjects, user studies, or the collection of personally identifiable information. All experiments are conducted on publicly available benchmarks released under their respective licenses, which, to the best of our knowledge, do not contain sensitive personal data."
        },
        {
            "title": "REPRODUCIBLE STATEMENTS",
            "content": "We conduct all experiments using the Azure OpenAI endpoint with fixed model snapshots and seed, specifically gpt-4.1-2025-04-14 and gpt-4.1-mini-2025-04-14, to ensure reproducibility. Our implementation relies on PyTorch (Paszke et al., 2019), the Hugging Face Transformers library (Wolf et al., 2020), and the TRL library for training and vLLM (Kwon et al., 2023) for inference. Additional implementation details are provided in the Appendix B. We will also release our codebase to enable the research community to fully reproduce, verify, and extend our work on long-horizon LLM agents."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat S. Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report. arXiv, 2412.08905, 2024. URL https://doi.org/ 10.48550/arXiv.2412.08905. Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, 2024a. Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024b. URL https://api. semanticscholar.org/CorpusID:268232499. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, and Zack Hui. Windows agent arena: Evaluating multi-modal OS agents at scale. arXiv, 2409.08264, 2024. URL https: //doi.org/10.48550/arXiv.2409.08264. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, NanJiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaï Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ramé, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel, Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, BoJuen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv, 2507.06261, 2025. URL https://doi.org/10.48550/arXiv.2507.06261. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, 10 Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - URL http://papers.nips.cc/paper_files/paper/2023/ 16, 2023, 2023. hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_ Benchmarks.html. Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian del Carmen Hipolito Garcia, Jin Xu, Victor Rühle, and Saravan Rajmohan. Enhancing reasoning capabilities of small language models with blueprints and prompt template search. arXiv, 2506.08669, 2025. URL https://doi.org/10.48550/arXiv.2506.08669. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 1335813376. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/ 2023.emnlp-main.825. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 16581677. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.91. 11 Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. In The Narasimhan. Swe-bench: Can language models resolve real-world github issues? Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= VTF8yNQM66. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv, 2503.09516, 2025. URL https://doi.org/10.48550/arXiv.2503.09516. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=sY5N0zY5Od. Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, and Minsik Cho. Epicache: Episodic kv cache management for long conversational question answering, 2025. URL https://arxiv. org/abs/2509.17396. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 1317 1327. The Association for Computational Linguistics, 2016. URL https://doi.org/10. 18653/v1/d16-1139. Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, and Lawrence Chan. Measuring AI ability to complete long tasks. arXiv, 2503.14499, 2025. URL https: //doi.org/10.48550/arXiv.2503.14499. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466, 2019. URL https://doi.org/10.1162/tacl_a_00276. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace (eds.), Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pp. 611626. ACM, 2023. URL https: //doi.org/10.1145/3600006.3613165. Dongjun Lee, Juyong Lee, Kyuyoung Kim, Jihoon Tack, Jinwoo Shin, Yee Whye Teh, and Kimin Lee. Learning to contextualize web pages for enhanced decision making by LLM agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= 3Gzz7ZQLiz. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 63426353. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/2023.emnlp-main.391. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. 12 OpenAI. Introducing chatgpt agent: bridging research and action. https://openai.com/ index/introducing-chatgpt-agent/, 2025a. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025b. OpenAI. 2025c. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, OpenAI. Introducing openai o3 and o4-mini. introducing-o3-and-o4-mini/, 2025d. https://openai.com/index/ Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 963981. Association for Computational Linguistics, 2024. URL https://doi.org/10. 18653/v1/2024.findings-acl.57. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Neural Information Processing Systems (NeurIPS), 2019. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \"gradient descent\" and beam search. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 79577968. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/2023. emnlp-main.494. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Efficient long context language model retrieval with compression. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 1525115268. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.acl-long.740/. Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, Victor Rühle, and Saravan Rajmohan. TACO-RL: task aware prompt compression optimization with reinforcement learning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 15821597. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.findings-acl.81/. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 2329 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 3121031227. PMLR, 2023. URL https://proceedings.mlr.press/ v202/shi23a.html. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= 0IOX0YcCdTn. 13 Calvin Smith. Hands AI Blog, April openhands-context-condensensation-for-more-efficient-ai-agents. Openhands context condensensation for more efficient ai agents. All URL https://www.all-hands.dev/blog/ 2025. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: controllable world of apps and people for benchmarking interactive coding agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1602216076. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.850. Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, and Liang Ding. Recursively summarizing enables long-term dialogue memory in large language models. Neurocomputing, 639:130193, 2025a. Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor Rühle, and Saravan Rajmohan. Odysseybench: Evaluating llm agents on long-horizon complex office application workflows, 2025b. URL https://arxiv.org/abs/2508.09124. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum?id=jJ9BoXAfFa. Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, and Jingbo Shang. Officebench: Benchmarking language agents across multiple applications for office automation. arXiv, 2407.19056, 2024b. URL https://doi.org/10.48550/arXiv.2407.19056. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv, 2504.12516, 2025. URL https://doi. org/10.48550/arXiv.2504.12516. 14 Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, and Sung Ju Hwang. Training free exponential context extension via cascading KV cache. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=dSneEp59yX. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP 2020 - Demos, Online, November 16-20, 2020, pp. 3845, 2020. URL https://doi.org/10.18653/v1/2020.emnlp-demos.6. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_ Benchmarks_Track.html. Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: improving retrieval-augmented lms with In The Twelfth International Conference on context compression and selective augmentation. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=mlJLVigNHp. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-MEM: agentic memory for LLM agents. arXiv, 2502.12110, 2025. URL https://doi.org/10.48550/ arXiv.2502.12110. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv, 2505.09388, 2025. URL https://doi.org/10.48550/ arXiv.2505.09388. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum?id=Bb4VGOWELI. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Swe-agent: Agent-computer interfaces enable automated Narasimhan, and Ofir Press. software engineering. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html. 15 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 2142421439. Association for Computational Linguistics, 2024. URL https://doi.org/ 10.18653/v1/2024.emnlp-main.1194. Mert Yüksekgönül, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative AI by backpropagating language model feedback. Nature, 639(8055):609616, 2025. URL https://doi.org/10.1038/s41586-025-08661-4. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv, 2502.15589, 2025. URL https://doi.org/10.48550/arXiv.2502.15589. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. MEM1: learning to synergize memory and reasoning for efficient long-horizon agents. arXiv, 2506.15841, 2025. URL https://doi.org/10. 48550/arXiv.2506.15841. 16 LIMITATIONS & FUTURE WORK Our work addresses the context management problem in long-horizon LLM agents and proposes framework for optimized context compression. While the method effectively reduces token costs with minimal performance degradation, it also presents several limitations. primary limitation is computational overhead. As discussed in Section 4.5, history compression can in some cases increase total cost, since additional steps may be required to complete challenging tasks with the compressed history. Moreover, it breaks the KV-cache of transformer-based LLMs, which forces re-computation of compressed histories. This effect is particularly evident in the FIFO baseline. Observation compression alleviates some of this overhead, but the generative compression procedure itself introduces latency, slowing down agent response time (Lee et al., 2025). promising future direction is the development of KV-cachelevel compression or eviction strategies. Prior work has considered KV-cache compression for single-step reasoning (Zhang et al., 2025), long documents (Xiao et al., 2024; Willette et al., 2025), and long conversations (Kim et al., 2025). However, its role in multi-turn, long-horizon agents remains underexplored. Another limitation is model coverage. Our experiments primarily evaluate GPT models due to budgetary constraints. Although the framework is designed to be model-agnostic, its generalizability to other foundation models such as Gemini (Comanici et al., 2025) or Claude (Anthropic, 2024b) remains unverified. Similarly, we were unable to include large-scale open-source models such as DeepSeek-R1 (DeepSeek-AI et al., 2025) or Qwen3-235B (Yang et al., 2025) due to limited resources. Extending the analysis to these models would provide stronger evidence of robustness and broaden the applicability of our conclusions."
        },
        {
            "title": "B EXPERIMENTAL SETUP DETAILS",
            "content": "B.1 DATASETS AppWorld (Trivedi et al., 2024). AppWorld is our primary benchmark, providing high-quality execution environment that integrates nine everyday applications (Spotify, SimpleNote, Amazon, It also includes Venmo, Gmail, Splitwise, File system, Todoist, and Phone) through 457 APIs. realistic simulations of approximately 100 functional users. This benchmark is particularly suitable for evaluating long-horizon productivity agents, as its multi-step tasks require an average of 42.5 API calls per task. We follow the official split, using 90 training tasks for guideline optimization and distillation, and 168 test-normal tasks for evaluation. An example trajectory from AppWorld is provided in Example E.1. OfficeBench (Wang et al., 2024b). OfficeBench is benchmark for office automation using applications such as Word, Excel, PDF, Calendar, Email, Shell, and Calculator. It evaluates the ability of agents to coordinate across multiple apps to complete complex tasks, making it well suited for long-horizon scenarios. Tasks are categorized as 1-app, 2-app, or 3-app depending on the number of applications required. We restrict our experiments to text-related tasks, excluding those requiring OCR, as OCR quality could confound the evaluation. Since no official split is available, we randomly partition the tasks into training and test sets with 1:1 ratio, resulting in 92 training tasks and 95 test tasks. We additionally refine the dataset by removing ambiguous tasks and ensuring that synthetic files (testbeds) are not shared across splits. 8-Objective QA (Zhou et al., 2025). The 8-objective QA benchmark simulates deep researchstyle agentic tasks. Unlike conventional multi-hop QA, which requires answering single question using multiple pieces of evidence, this benchmark poses eight distinct questions within one task, and the agent must provide answers to all of them at the end. This design creates more challenging setting for long-horizon agents. Following Zhou et al. (2025), we construct each task by grouping eight questions together. Questions are drawn from NaturalQuestions (Kwiatkowski et al., 2019), resulting in 100 training tasks (from the train split) and 100 test tasks (from the test split). For retrieval, we use BM25 retriever over the 2018 Wikipedia knowledge base, following Jin et al. (2025). We include the example task of each benchmark in Table 4. Table 4: Example tasks across benchmarks. Benchmark / Difficulty Example Task"
        },
        {
            "title": "Hard",
            "content": "OfficeBench 1-app 2-app 3-app 8-objective QA Mark Taking solo backpacking trip in my Bucket List Simple Note as not done. Like all the Venmo transactions from today involving any of my roommates on my Venmo social feed. Start playing playlist on Spotify that has enough songs for my workout today. do not want to have to change the playlist in the middle of my workout. The workout plan is in Simple Note. Create new Word file called random_paragraph.docx and add the content in random_paragraph.txt to it. Analyze Excel data of students grade and generate teaching report in teaching.docx. Read company revenues and send an email with subject revenues, containing data to Bob for reporting, also write revenue.docx to summarize it. who wrote the song Oceans Where Feet May Fail?; who plays Eddie the Eagle in the movie?; when was the last time England were in the final of World Cup?; who plays Chelseas mom on Young and the Restless?; what is the largest coin in the US?; who sang Even the Bad Times Are Good?; who sings This Is My Town country song?; which of the Guianas is not an independent country? B.2 EVALUATION METRICS For efficiency evaluation, we adopt two metricspeak tokens and dependencyintroduced in LightThinker (Zhang et al., 2025) and MEM1 (Zhou et al., 2025). Peak tokens. Peak tokens are measured as the maximum number of tokens observed in any single sequence throughout the agents trajectory, excluding system prompts. This metric serves as proxy for inference-time memory requirements and corresponds to the maximum peak shown in Figure 2. Dependency. Dependency is defined as the area under the curve in Figure 2. At each step t, given the number of input tokens n(t) and output tokens n(t) , it is calculated as: (n(t) ) n(t) + 2n(t) 2 . (cid:88) t[T ] (11) This metric approximates the cumulative computational cost incurred by action generation across the trajectory. API Cost. For the cost analysis in Section 4.5, we use the official OpenAI pricing (as of September 2025) for gpt-4.1 and gpt-4.1-mini (OpenAI, 2025b). Specifically, gpt-4.1 is priced at $3.00 per 1M input tokens, $0.75 per 1M cached input tokens, and $12.00 per 1M output tokens. For gpt-4.1-mini, the costs are $0.80 per 1M input tokens, $0.20 per 1M cached input tokens, and $3.20 per 1M output tokens. For Qwen3-14B (Yang et al., 2025), since no official API pricing 18 is available, we approximate the cost using OpenRouter1 : $0.06 per 1M input tokens, $0.015 per 1M cached input tokens, and $0.24 per 1M output tokens. B. IMPLEMENTATION DETAILS & HYPERPARAMETERS API Inference. We set temperature 0.0 and fix the seed 42. Note that there is still non-determinism with fixing the seed and setting temperature as 0. To reduce the instability, we use the API snapshot form Azure OpenAI endpoint gpt-4.1-2025-04-14 and gpt-4.1-mini-2025-04-14. Compression. For history compression, we set Thist = 4096 for AppWorld and OfficeBench, and 2048 for 8-objective QA. We keep the last action, observation pair to preserve the latest information. This is the same for ACON and all baselines. For observation compression, we set Tobs = 1024 for AppWorld, 512 for OfficeBench, and 400 for 8-objective QA. Prompt Optimization. We use the OpenAI o3 model (OpenAI, 2025d) for both analysis and update of prompts. During the update stage, we sample 5 candidate prompts and select the one that performs best on subset of the training set. The prompts used in each stage and step are provided as follows: Analysis prompt for UT step: Prompt E. Update prompt for UT step: Prompt E.2 Analysis prompt for CO step: Prompt E.3 Update prompt for CO step: Prompt E.4 We also provide the detailed procedure in Algorithm 1. For the subset used in prompt selection during the UT step, we consider training tasks in D(r) cont where the agent succeeds without compression but fails with compression. For the CO step, we use training tasks in D(r) succ where the agent succeeds with compression. Baselines For FIFO, we keep last 5 interaction turns which fits in similar compression rate in average with ACON. For retrieval, we also retrieve 4 interaction turns and keep the last turn. We use OpenAI text-embedding-3-large for embedding. For LLMLingua, we set keep rate as 30% for both history and observation. For naive prompting, we use the similar prompt with Lee et al. (2025) and do some human prompt engineering to specialize each prompt to history or observation compression. Compressor & Agent Distillation Both compressor and agent distillation use LoRA (Hu et al., 2022) with rank 16, α = 32, learning rate 104, 3 epochs, batch size 4, and maximum sequence length 10,000. We adopt linear warmup (5% ratio), weight decay 0.01, and AdamW optimizer. No hyperparameter tuning was performed; the same setup is applied across all models and benchmarks. We sample single generation from the teacher for fine-tuning, leaving potential improvements from hyperparameter tuning or multi-sample training for future work. We use 1 A100 80GB GPU for both training and inference. For inference of fine-tuned models, we use greedy decoding (temperature 0.0)."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "We provide additional quantitative results to complement the main experiments in Section 4. OfficeBench difficulty breakdowns. We further analyze OfficeBench with gpt-4.1 by difficulty level. The detailed breakdown in Table 5 shows that ACON yields the largest gains on the most challenging tasks in Level 3. 1https://openrouter.ai/ 19 Experiments with gpt-4.1-mini. Results for the smaller variant gpt-4.1-mini (OpenAI, 2025b) across three benchmarks are reported in AppWorld  (Table 6)  , OfficeBench  (Table 7)  , and 8-objective QA  (Table 8)  . The trends of ACON are consistent with those for gpt-4.1 in Section 4. In particular, Table 6 shows that history compression improves the performance of gpt-4.1-mini compared to the baseline, complementing the findings in Section 4.4 that ACON enhances the effectiveness of smaller LM agents. These results highlight the robustness of our method under resource-constrained settings. Experiments with gpt-5-chat. We also evaluate on AppWorld using gpt-5-chat (OpenAI, 2025c), as reported in Table 9. The improvements follow the same trend as with gpt-4.1, demonstrating that ACON generalizes to the latest stronger proprietary models. Distilled optimizer. Additional results for the distilled optimizer in AppWorld are shown in Table 10. Beyond the analysis in Section 4.3, we also include experiments where the compressor is distilled using guidelines without optimization. The results confirm that optimized guidelines consistently yield stronger performance when distilled into smaller models. History and observation compression. In Table 11, we report ablations with gpt-4.1 using both history and observation compression. While combining the two compressions achieves larger reductions in peak token usage and dependency, it also leads to substantial performance degradation compared to applying either compression alone. Additional guideline optimization step. We investigate whether running an extra utility maximization step (UT) after the standard sequence of utility maximization and compression maximization (CO) is beneficial. As shown in Table 11, this additional iteration results in performance drop, indicating that single round of optimization is sufficient for effective guideline learning. Distilled compressor for observation. In addition to Section 4.3, we report results for observation compressor distillation in Figure 8. Similar to history compression, the performance is largely preserved after distillation, confirming that ACON enables effective transfer of optimized observation compressors to smaller models. Case study: history compression turns failure into success. notable case study illustrates how history compression enables smaller agent to succeed on tasks that would otherwise fail. In the uncompressed trajectory in Example E.2, the gpt-4.1-mini agent repeatedly attempted to use the file_system APIs without managing authentication, leading to persistent 401 Unauthorized errors. After compressing the history as in Example E.3, however, the compressed history retained only the essential reasoning steps: the need for both username and password, the importance of passing the returned access_token into subsequent calls, and the absence of proxy APIs in the supervisor app. This compressed context prevented redundant exploration and guided the agent directly to the correct sequencelogin with full credentials, capture the token, and provide it explicitly in show_directory and delete_file calls. As result, the agent was able to enumerate and remove all .pdf files in /downloads, task it had previously failed. This example highlights how compression does not merely shorten history but clarifies critical dependencies, turning failure trajectory into successful one."
        },
        {
            "title": "D QUALITATIVE EXAMPLES",
            "content": "We complement the quantitative results with qualitative illustrations. Compression guidelines. We present examples of compression guidelines before and after optimization in AppWorld. The history compression guideline before optimization is shown in Prompt E.5, the optimized version (UT) in Prompt E.6, and the optimized version (UTCO) in Prompt E.7. Similarly, observation compression guideline examples are provided in Prompt E.8 and Prompt E.9, and the optimized version (UTCO) in Prompt E.10. These comparisons demonstrate that optimization yields more targeted guidelines for compressors. 20 Compressed histories. Compression Example E.1 illustrates history segments before and after guideline optimization in AppWorld with gpt-4.1. The optimized guideline retains more detailed record of task progress, including variable states and guardrails for the environment. After the compression maximization step (CO), the histories become shorter while still preserving the essential information required for future decision-making. This qualitative evidence demonstrates how our framework improves both the efficiency and effectiveness of context compression, complementing the guideline optimization procedure described in Section 3.3. We also present Compression Example E.2 for 8-objective QA and Compression Example E.3 for OfficeBench, which confirm that the effects of guideline optimization are consistent across benchmarks. Compressed observations. Compression Example E.4 shows observations before and after guideline optimization in AppWorld. We illustrate the case of printing available APIs for the Spotify app, which produces lengthy observation. The optimized guideline yields more structured and faithful representation: whereas naive prompting loses the JSON format and omits the crucial play_music API, the optimized version preserves both structure and key functionality necessary to complete the task."
        },
        {
            "title": "E LLM USAGE",
            "content": "We used large language models (LLMs) solely as writing assistant, for improving grammar and clarity of the paper. No part of the research ideation, experimental design, or analysis relied on LLMs. 21 Algorithm 1 Alternating Guideline Optimization (UT CO) Input: Training set indices I; fixed agent M(; θ, Pagent); compressor (; ϕ, P); initial guideline (0); tradeoff λ 0; rounds R; candidates Output: Optimized guideline Notation. For each and guideline P: baseline (no compression): context sequence Hi with success rbase compressed: i(P) with success ri(P) {0, 1} and cost C(H {0, 1} i(P)) = (cid:80) C(h i,t1, i,t) // 0) Collect baseline contexts (no compression)"
        },
        {
            "title": "Run M without compression to obtain Hi and rbase",
            "content": "i 1: for all do 2: 3: end for 4: + { rbase 5: for = 0 to 2R 2 step 2 do = 1 } // Stage A: UT (reward-first update using vs ) for all do end for D(r) cont { (Hi, for all (H, ) D(r) i(P (r))) +, ri(P (r)) = 0 } cont do Feedback LLM(FeedbackInstr, H, ) Append to multiset Futil Run with compression (; ϕ, (r)) to obtain i(P (r)), ri(P (r)), C(H i(P (r))) contrastive feedback: what did miss vs H? indices where baseline succeeds end for {P (r+1) k=1 LLM(UpdateInstr, (r), Futil ) }K Select by reward: evaluate on held-out subset of + and pick // : concatenation util arg max SuccessRate(cid:0){ri(P (r+1) )}iI+ (cid:1) util (r+1) (r+1) util // Stage B: CO (cost-minimizing refinement using only ) , obtain i, ri, C(H i) for all do Using (r+1) util end for D(r) succ { for all D(r) ri = 1 } succ do CompFeedback LLM(CompressInstr, ) Append to multiset Fcomp find redundant spans within end for { (r+2) k=1 LLM(UpdateInstr_Compress, (r+1) }K Select by rewardcost: evaluate on held-out split of and pick , Fcomp ) util comp arg max (cid:16) SuccessRate({ri( (r+2) )}) λ NormCost({C(H i( (r+2) (cid:17) ))}) 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: (r+2) (r+2) if early-stop criterion satisfied then comp end if break 28: 29: 30: 31: end for 32: (r+2) 33: 34: return e.g., success/cost convergence or budget met 22 Table 5: Detailed results on OfficeBench benchmark. We report accuracy (%), and efficiency metrics: average steps, peak input tokens (103), and dependency (106) for Average and each difficulty level. Best values are in bold. Rows in blue background indicate the results from ours . Method Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Average (All) Level 1 (1-app, 42) Level 2 (2-app, 22) Level 3 (3-app, 31) Agent: gpt-4.1 / Compressor: gpt-4. No Compression 76.84 11.52 7.27 4.43 92. 6.23 4.05 77.27 6.14 1.81 54. 8.37 6.08 History Compression FIFO Retrieval LLMLingua Prompting ACON UT ACON UTCO 67.37 65.26 70.53 71.58 74.74 72.63 Observation Compression LLMLingua Prompting ACON UT ACON UTCO 71.58 55.79 73.68 72. 12.26 16.20 10.89 10.13 13.13 11.54 11.89 12.24 10.83 10.28 4.02 4.33 4.65 4.40 4.93 4.54 7.38 6.44 6.55 6.17 2.64 2.06 1.85 1.10 3.85 1.91 6.14 2.68 3.85 2. 83.33 85.71 83.33 85.71 85.71 88.10 80.95 78.57 90.48 88.10 4.19 4.35 4.17 4.18 4.71 3.92 7.35 4.51 6.57 4.75 0.72 0.84 0.67 0.81 6.89 0.76 12.40 0.98 8.02 0. 63.64 63.64 68.18 77.27 72.73 72.73 72.73 50.00 77.27 72.73 3.51 3.52 4.61 4.53 4.64 4.72 6.31 6.98 6.11 6.41 1.01 1.37 1.18 1.08 1.44 1.16 2.11 2.61 1.97 2. 48.39 38.71 54.84 48.39 61.29 51.61 58.06 29.03 48.39 51.61 4.23 4.78 4.88 4.42 5.19 4.71 7.99 6.98 6.80 6.65 4.39 2.99 2.74 1.23 3.89 2.84 5.70 3.46 3.10 4. Table 6: Results across different difficulty levels on Appworld benchmark (test-normal) with gpt4.1-mini. We adopt the same compression guidelines as those used in the gpt-4.1 experiments. Each block reports accuracy (task goal completion score), average steps, average peak input tokens (103), and average dependency (106) for agents. Best results in each column are highlighted in bold. Rows in blue background indicate the results from ours . Method Average (168) Easy (57) Medium (48) Hard (63) Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Agent: gpt-4.1-mini / Compressor: gpt-4.1-mini No compression 35.7 18.14 8.55 5. 56.1 6.45 3.72 31.2 8.31 4. 20.6 10.64 9.18 History Compression FIFO Retrieval LLMLingua Prompting ACON UT ACON UTCO 39.3 14.9 36.3 35.7 42.3 32.7 Observation Compression LLMLingua Prompting ACON UT ACON UTCO 25.6 33.9 33.9 27.4 30.39 40.18 28.41 24.98 22.46 24.27 20.75 16.71 16.78 17.89 6.18 7.49 7.24 6.56 6.51 6.99 8.04 6.04 6.86 6.37 5.24 5.95 6.65 4.95 5.48 4. 8.21 3.87 4.58 4.44 75.4 36.8 66.7 64.9 64.9 57.9 38.6 59.7 59.7 40.4 4.76 7.10 6.96 5.96 5.87 7.50 6.13 5.21 5.44 5.18 2.66 4.29 3.84 2.90 2.62 2. 3.03 3.41 2.93 2.40 35.4 8.3 33.3 27.1 37.5 33.3 27.1 33.3 33.3 35.4 5.33 7.44 7.05 6.65 7.18 8.45 8.74 5.99 7.13 6.84 4.81 6.80 7.60 5.35 5.22 4. 13.78 3.27 4.26 5.03 9.5 0.0 11.1 15.9 25.4 9.5 12.7 11.1 11.1 9.5 8.10 7.89 7.62 6.84 7.18 6.95 9.24 6.83 7.97 7.09 7.91 6.81 8.47 6.49 8.25 6. 8.65 4.74 6.38 5.82 Table 7: Detailed results on OfficeBench benchmark with gpt-4.1-mini. We adopt the same compression guidelines as those used in the gpt-4.1 experiments. We report accuracy (%), and efficiency metrics: average steps, peak input tokens (103), and dependency (106) for Average and each difficulty level. Rows in blue background indicate the results from ours . Method Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Average (All) Level 1 (1-app, 42) Level 2 (2-app, 22) Level 3 (3-app, 31) Agent: gpt-4.1-mini / Compressor: gpt-4.1-mini No Compression 72.63 11.96 7. 3.92 88.10 6.66 4.29 68.18 4. 1.01 54.84 9.02 5.40 History Compression FIFO Retrieval LLMLingua Prompting ACON 65.26 67.37 67.39 71.58 73. Observation Compression LLMLingua Prompting ACON 66.32 73.68 71.58 10.91 14.46 11.59 11.78 12.41 11.02 11.43 10.96 4.03 4.55 4.90 4.93 4.82 6.34 6.45 6. 1.46 2.74 2.18 3.10 1.96 2.40 2.62 2.19 83.33 85.71 87.18 85.71 88.10 78.57 88.10 88.10 0.78 5.86 3.87 4.75 0.83 2.12 1.44 1. 59.09 59.09 59.09 72.73 68.18 63.64 72.73 63.64 3.69 3.47 4.58 4.40 4.39 4.82 4.95 4.89 0.96 0.87 0.92 0.86 0.86 0.97 1.06 1. 45.16 48.39 48.39 51.61 58.06 51.61 54.84 54.84 4.19 4.59 5.34 5.32 5.37 7.30 8.01 7.30 2.03 2.45 2.17 3.06 3.07 3.34 4.01 3. 4.10 5.85 4.31 4.73 4.12 6.09 4.82 4.45 23 Table 8: Results on 8-objective QA benchmark with gpt-4.1-mini. We adopt the same compression guidelines as those used in the gpt-4.1 experiments. We report EM/F1 and efficiency metrics (Steps, Peak input tokens (103), and Dependency (106)). Method EM F1 Steps Peak Dep. Agent: gpt-4.1-mini / Compressor: gpt-4.1-mini No compression 0. 0.436 19.80 12.93 5.63 History Compression FIFO Retrieval LLMLingua Prompting ACON ACON (iter2) 0.024 0.143 0.140 0.149 0.238 0. Observation Compression LLMLingua Prompting ACON ACON (iter2) 0.316 0.282 0.323 0.316 0.031 0.190 0.194 0.207 0.325 0.353 0.430 0.402 0.434 0.443 28.45 26.90 25.24 25.27 21.05 19.18 15.96 11.71 14.42 11. 5.33 5.34 6.69 4.85 4.78 4.79 5.54 3.91 4.71 3.97 3.89 3.55 3.92 2.44 2.03 1.79 1.60 0.65 1.10 0.63 Table 9: Results across different difficulty levels on AppWorld benchmark (test-normal) with gpt5-chat. We adopt the same compression guidelines as those used in the gpt-4.1 experiments. Each block reports accuracy (task goal completion score), steps, peak input tokens (103), and dependency (106) for agents. Best results in each column are highlighted in bold. Rows in blue background indicate the results from ours . Method Average (168) Easy (57) Medium (48) Hard (63) Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Agent: gpt-5-chat / Compressor: gpt-5-chat No compression 66.7 16.45 9.67 4.78 89.5 7. 2.31 64.6 9.58 4.13 47.6 11. 7.51 History Compression FIFO (last-5) Prompting ACON UT ACON UTCO 46.4 58.9 58.3 62.5 Observation Compression Prompting ACON UT ACON UTCO 60.1 65.5 62.5 30.61 22.24 20.15 22. 17.39 17.16 18.21 History + Observation Compression ACON UT ACON UTCO 20.02 22.90 63.1 58.9 6.81 7.46 6.97 7.26 6.50 7.58 7. 5.89 5.83 4.85 4.02 3.74 3.85 3.72 3.96 4.24 3.63 4.07 79.0 82.5 80.7 86.0 80.7 84.2 80. 77.2 80.7 5.21 7.15 6.66 6.44 4.98 5.62 5.52 5.27 5.35 2.10 2.13 2.04 2.04 1.72 1.94 2. 1.92 1.94 43.8 66.7 66.7 72.9 68.8 68.8 70.8 77.1 77.1 6.82 7.19 7.08 6.98 6.40 7.49 7. 6.03 5.94 5.50 3.69 3.40 3.93 3.48 3.46 3.69 3.52 3.56 19.1 31.8 31.8 33.3 34.9 46.0 39. 39.7 25.4 8.24 7.93 7.16 8.20 7.96 9.41 8.76 6.35 6.17 6.84 5.97 5.54 5.42 5.70 6.16 6. 5.28 6.39 Table 10: Results across different difficulty levels on AppWorld with distilled compressors. We report accuracy (task goal completion score), average steps, peak input tokens (103), and dependency (106). For all compressors, we use the optimized compression guideline after the utilization maximization UT step. Fine-tune means that we fine-tune small models with outputs from naive prompt before compression guideline optimization. Method Average Easy Medium Hard Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Agent: gpt-4.1 / Compressor: gpt-4.1-mini or Distilled models (Qwen3, Phi-4) History Compression Prompting (gpt-4.1-mini) ACON (gpt-4.1-mini) Fine-tune (Qwen3-14B) ACON (Qwen3-14B) ACON (Qwen3-8B) ACON (Phi-4) Observation Compression Prompting (gpt-4.1-mini) ACON (gpt-4.1-mini) Fine-tune (Qwen3-14B) ACON (Qwen3-14B) ACON (Qwen3-8B) ACON (Phi-4) 39.3 47.6 44.6 50.0 47.0 44.6 44.0 48.2 40.5 56.5 48.2 50. 23.61 21.46 24.16 21.72 21.58 21.19 16.67 18.00 17.71 16.78 16.10 16.88 7.03 7.25 7.16 6.83 6.98 7.24 6.84 8.66 6.64 7.57 7.33 7.88 5.19 5.24 4.95 4.80 4.76 4.76 4.30 6.62 4.38 5.06 4.82 5. 6.64 6.75 6.79 6.42 6.64 7.33 5.08 6.05 4.91 5.69 5.49 5.85 3.17 2.84 2.88 2.54 2.93 2.75 2.19 2.60 1.97 2.20 2.03 2.88 35.4 35.4 43.8 50.0 37.5 39.6 35.4 37.5 31.2 54.2 50.0 52. 7.63 7.25 7.39 6.87 7.24 7.12 6.72 9.23 6.72 7.39 7.20 7.75 5.42 5.36 4.88 4.89 4.67 4.16 3.77 7.41 4.05 4.46 4.20 4.77 19.1 31.8 20.6 23.8 31.8 27.0 25.4 34.9 25.4 34.9 25.4 25. 6.93 7.70 7.33 7.17 7.09 7.26 8.53 10.60 8.16 9.40 9.10 9.83 6.84 7.32 6.88 6.79 6.48 7.04 6.61 9.65 6.81 8.10 7.82 8.18 64.9 75.4 71.9 79.0 71.9 68.4 71.9 71.9 64.9 82.5 71.9 77. 24 Table 11: Additional results for additional guideline optimization step and unified compression on Appworld benchmark (test-normal). Each block reports accuracy (task goal completion score), steps, peak input tokens (103), and dependency (106) for agents. Best results in each column are highlighted in bold. Rows in blue background indicate the results from ours . Method Average (168) Easy (57) Medium (48) Hard (63) Acc. Steps Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Acc. Peak Dep. Agent: gpt-4.1 / Compressor: gpt-4.1 History Compression ACON UTCOUT 47.0 22.28 7. 4.66 68.4 7.01 2.69 58.3 7. 4.39 19.1 7.45 6.65 History + Observation Compression Prompting ACON ACON UTCO 19.33 20.32 21. 36.3 45.8 44.6 5.38 5.85 5.90 3.44 4.26 4.98 71.9 75.4 77.2 4.87 5.29 5.50 1.80 2.07 2. 21.6 39.6 39.6 5.63 6.15 6.18 3.60 4.29 3.80 14.3 23.8 19.1 5.64 6.12 6.18 4.79 6.21 8. Figure 8: Results of distilled compressors on observation compression with gpt-4.1 as the agent. Student models (Qwen3-14B, Qwen3-8B, Phi-4) are distilled from gpt-4.1 compressor using the optimized compression guideline after UT step, and evaluated across all benchmarks. We also include result with gpt-4.1-mini without distillation for comparison. 25 Prompt E.1: Prompt for analysis before prompt optimization (utility step) You are an expert agent trajectory auditor. Analyze why the HISTORY-OPTIMIZED agent failed OR became significantly less efficient while the BASELINE succeeded. You are given: - task_name: {{ task_name }} - Baseline full history (single continuous session) - Optimized history split into multiple sessions where each new session starts with fresh system + user prompt and an injected <HISTORY_SUMMARY> summarizing earlier interactions. - baseline_success={{ baseline_success }} optimized_success={{ optimized_success }} - baseline_env_steps={{ baseline_env_steps default('null') }} optimized_env_steps={{ optimized_env_steps default('null') }} step_ratio={{ step_ratio default('null') }} performance_regression={{ performance_regression default(' false') }} Goals: 1. Determine whether summarization / session resetting removed, distorted, delayed, or bloated reasoning causing failure OR inflated step count (> threshold factor of baseline). 2. Identify the FIRST divergence point where the optimized trajectory meaningfully deviates from the successful & efficient baseline path. 3. Categorize root causes (e.g., Missing Critical Fact, Incorrect Summary, Lost Variable/State, Unnecessary Rediscovery, Instruction Drift, API Misuse, Premature Completion, Token Truncation, Inefficient Looping, Redundant API Calls, Over-Exploration, Other). 4. Extract concrete evidence snippets (quote exact lines) from baseline vs optimized showing: - Critical facts present in baseline but absent/altered in optimized (esp. after session boundary) - Summary inaccuracies (baseline ground truth vs summary text ) - Redundant or looping action patterns causing step inflation . 5. Suggest precise remediation strategies: summary style changes , retain variable/value tables, move session boundaries, guardrail prompts, caching, early-exit heuristics, loop detection, etc. 6. Provide reliability_score (0.0-1.0) reflecting confidence in your causal attribution. 7. If performance_regression==true, analyze efficiency degradation even if optimized_success==true. Output STRICTLY valid JSON object with keys: { \"task_name\": str, \"divergence_step_description\": str, \"root_cause_categories\": [str, ...], \"missing_or_distorted_facts\": [ {\"baseline\": str, \" optimized_context_absent_or_changed\": str, \"impact\": str} ], \"summary_inaccuracies\": [ {\"summary_excerpt\": str, \"issue_type \": str, \"correct_baseline_reference\": str, \"impact\": str} ], \"lost_state_variables\": [ {\"name_or_pattern\": str, \" baseline_evidence\": str, \"optimized_issue\": str} ], \"api_or_action_errors\": [ {\"optimized_step_excerpt\": str, \" error_type\": str, \"improvement\": str} ], 26 \"inefficiency_patterns\": [ {\"pattern\": str, \"evidence_excerpt \": str, \"excess_steps\": int, \"cause\": str, \"remediation\": str} ], \"timeline_of_divergence\": [ {\"phase\": str, \"optimized_excerpt \": str, \"baseline_contrast\": str, \"effect\": str} ], \"performance_regression\": bool, \"baseline_env_steps\": int null, \"optimized_env_steps\": int null, \"step_ratio\": float null, \"remediation_recommendations\": [ str, ... ], \"recovery_opportunities_missed\": [ {\"optimized_excerpt\": str, \"missed_fix_action\": str} ], \"reliability_score\": float, \"concise_failure_mechanism_summary\": str } If some sections have no data, use an empty list. For nonapplicable numeric fields use null. Do NOT include any extra commentary outside JSON. --- BASELINE_HISTORY_START {{ baseline_history }} BASELINE_HISTORY_END OPTIMIZED_MULTI_SESSION_HISTORY_START {{ optimized_history }} OPTIMIZED_MULTI_SESSION_HISTORY_END Failure or performance report / metadata (may be null): {{ failure_report }} Proceed with rigorous comparison. Prompt E.2: Prompt for prompt optimization after analysis (utility step) You are an expert prompt engineer tasked with refining HISTORY SUMMARIZATION prompt. Rewrite the ORIGINAL PROMPT to reduce length of the HISTORY SUMMARY while preserving factual continuity for the next session. Ground all changes in the PER-SAMPLE REDUCTION SIGNALS below. Do not aggregate across samples; use the patterns and rules as -is. Constraints: - Keep all Jinja placeholders, variable names, and structure intact where possible. - Add explicit, concrete rules that prevent verbosity and retain essential state. - Do not include literal values from prior content; refer to variable names only. - Output ONLY the improved prompt template (no extra commentary) . Context (samples below are the only ground truth signals to use) : - Average original summary size (chars) across sampled set: {{ avg_orig_chars }} {% for in samples %} 27 ===== SAMPLE {{ loop.index0 }} ===== - Task/Session: {{ s.task_label }} / {{ s.session or 'unknownsession' }} - Analysis Overview: {% if s.overview %} {% for k, in s.overview.items() %} {% endfor %} {% else %} {% endif %} - (none provided) - {{ }}: {{ }} - Removals (patterns -> action): {% for in s.removals %} - [{{ r.category default('unknown') }}] {{ r.pattern default('') }} -> {{ r.action default ('drop') }} {% endfor %} - KEEP examples (evidence-driven essentials): {% for in s.keeps %} - Reason: {{ k.reason default('') }} Evidence: {{ k.evidence_spans default([]) join('; ') }} {% endfor %} - Summary Rules: {% for rule in s.rules %} {% endfor %} - {{ rule }} {% endfor %} Original Prompt Template (verbatim between markers): <<<ORIGINAL_PROMPT>>> {{ original_prompt }} <<<ORIGINAL_PROMPT>>> Output only the improved prompt template text, ready to be used as Jinja template. Prompt E.3: Prompt for analysis before prompt optimization (compression step) You are an expert prompt engineer tasked with refining HISTORY SUMMARIZATION prompt. Rewrite the ORIGINAL PROMPT to reduce length of the HISTORY SUMMARY while preserving factual continuity for the next session. Ground all changes in the PER-SAMPLE REDUCTION SIGNALS below. Do not aggregate across samples; use the patterns and rules as -is. Constraints: - Keep all Jinja placeholders, variable names, and structure intact where possible. - Add explicit, concrete rules that prevent verbosity and retain essential state. - Do not include literal values from prior content; refer to variable names only. - Output ONLY the improved prompt template (no extra commentary) . Context (samples below are the only ground truth signals to use) : - Average original summary size (chars) across sampled set: {{ avg_orig_chars }} {% for in samples %} ===== SAMPLE {{ loop.index0 }} ===== - Task/Session: {{ s.task_label }} / {{ s.session or 'unknownsession' }} - Analysis Overview: {% if s.overview %} {% for k, in s.overview.items() %} {% endfor %} {% else %} {% endif %} - (none provided) - {{ }}: {{ }} - Removals (patterns -> action): {% for in s.removals %} - [{{ r.category default('unknown') }}] {{ r.pattern default('') }} -> {{ r.action default ('drop') }} {% endfor %} - KEEP examples (evidence-driven essentials): {% for in s.keeps %} - Reason: {{ k.reason default('') }} Evidence: {{ k.evidence_spans default([]) join('; ') }} {% endfor %} - Summary Rules: {% for rule in s.rules %} {% endfor %} - {{ rule }} {% endfor %} Original Prompt Template (verbatim between markers): <<<ORIGINAL_PROMPT>>> {{ original_prompt }} <<<ORIGINAL_PROMPT>>> Output only the improved prompt template text, ready to be used as Jinja template. Prompt E.4: Prompt for analysis before prompt optimization (compression step) You are an expert prompt engineer tasked with refining HISTORY SUMMARIZATION prompt. Rewrite the ORIGINAL PROMPT to reduce length of the HISTORY SUMMARY while preserving factual continuity for the next session. Ground all changes in the PER-SAMPLE REDUCTION SIGNALS below. Do not aggregate across samples; use the patterns and rules as -is. Constraints: - Keep all Jinja placeholders, variable names, and structure intact where possible. - Add explicit, concrete rules that prevent verbosity and retain essential state. - Do not include literal values from prior content; refer to variable names only. - Output ONLY the improved prompt template (no extra commentary) . Context (samples below are the only ground truth signals to use) : - Average original summary size (chars) across sampled set: {{ avg_orig_chars }} 29 {% for in samples %} ===== SAMPLE {{ loop.index0 }} ===== - Task/Session: {{ s.task_label }} / {{ s.session or 'unknownsession' }} - Analysis Overview: {% if s.overview %} {% for k, in s.overview.items() %} {% endfor %} {% else %} {% endif %} - (none provided) - {{ }}: {{ }} - Removals (patterns -> action): {% for in s.removals %} - [{{ r.category default('unknown') }}] {{ r.pattern default('') }} -> {{ r.action default ('drop') }} {% endfor %} - KEEP examples (evidence-driven essentials): {% for in s.keeps %} - Reason: {{ k.reason default('') }} Evidence: {{ k.evidence_spans default([]) join('; ') }} {% endfor %} - Summary Rules: {% for rule in s.rules %} {% endfor %} - {{ rule }} {% endfor %} Original Prompt Template (verbatim between markers): <<<ORIGINAL_PROMPT>>> {{ original_prompt }} <<<ORIGINAL_PROMPT>>> Output only the improved prompt template text, ready to be used as Jinja template. Prompt E.5: AppWorld Prompt for history compression before optimization You are maintaining structured context-aware summary for productivity agent. You will be given the user instruction for the agent, list of interactions corresponding to actions taken by the agent, and the most recent previous summary if one exists. Produce the following: ### REASONING Summarize key progress, decisions made, important observed outcomes, and rationale behind actions taken so far. Include how earlier steps influenced later ones and why certain data is retained in the summary. ### COMPLETED List completed subtasks or successful outcomes, with brief results if applicable. --- ## [Information Source] ### USER INSTRUCTION 30 {{ task }} ## [PREVIOUS SUMMARY] (if any) {{ prev_summary }} ## [HISTORY OF INTERACTIONS] {{ history }} --- ## PRIORITIZE 1. Keep all sections relevant and concise. 2. Use reusable structured formats when summarizing artifacts. 3. Ensure agent can resume task with no loss of information. 4. Include key info from errors or failed attempts to prevent repeated mistakes. 5. Preserve all essential artifacts and data needed to complete the task. --- ### [Output Format] Do **not** include the input or any additional explanation. Only return the formatted summary. Prompt E.6: AppWorld Prompt for history compression after optimization (UT) You maintain compact, state-preserving HISTORY_SUMMARY for multi-session agent. Input: [USER INSTRUCTION] {{ task }} [PREVIOUS SUMMARY] {{ prev_summary }} [HISTORY OF INTERACTIONS] {{ history }} Create the following sections-use the exact headings and order: <HISTORY_SUMMARY> 1. REASONING - Key progress, decisions, outcomes, and their rationale. - Note how earlier steps influence later ones. 2. VARS name value purpose ---------------------- Record every runtime value the next session must re-declare ( tokens, ids, lists, last page_index/page_limit, etc.). 3. TODO List pending actions with enough detail to execute directly. 4. COMPLETED Bullet list of finished subtasks with brief results. 5. GUARDRAILS Short reminders that prevent repeat errors, e.g. 31 - Memory resets; re-create VARS before use. - Paginate until empty page. - Validate API parameters against spec. - Avoid redundant logins or doc look-ups. Requirements: - Be concise-bullets and tables preferred; no extraneous prose. - Preserve all essential facts, parameters, and artifacts; omit nothing critical. - Include errors only if they inform future avoidance. - Do not output the input or any commentary-return only < HISTORY_SUMMARY>. Prompt E.7: AppWorld Prompt for history compression after optimization (UTCO) You maintain compact, state-preserving HISTORY_SUMMARY for multi-session agent. Input: [USER INSTRUCTION] {{ task }} [PREVIOUS SUMMARY] {{ prev_summary }} [HISTORY OF INTERACTIONS] {{ history }} Summary Compression Rules: - Collapse multi-bullet narratives into <=2 concise sentences. - Replace repetitive step logs with one summarizing phrase. - Truncate long token/credential strings to \"<token>\" unless verbatim reuse is required. - Remove unused/expired credentials, page_index/page_limit, verbose API dumps, and table borders. - Shrink GUARDRAILS to one bullet unless multiple items are still critical. - Delete tool/API log output, greetings, meta prose, and section headers that no longer contain content. - Keep only variables actively referenced in upcoming steps; list each once in VARS. - Reference removal categories [repetition], [tool-logs], [meta ], [formatting] to prune similar lines. - Preserve factual continuity; never invent or alter state variables. - Target summaries well under {{ max_chars default(1500) }} characters. Critical Essentials: Always keep evidence-driven items required next session (e.g., tokens, ids, emails, amounts, lists, paths, description strings, brief task status). Output EXACTLY the following structure---nothing more: <HISTORY_SUMMARY> 1. REASONING One brief paragraph on key progress and rationale. 2. VARS key=value pairs, comma-separated; only still-needed runtime values. 3. TODO 32 Bulleted next actions (<=5). 4. COMPLETED Bulleted finished subtasks (<=5). 5. GUARDRAILS Single concise bullet, or omit if none. Return only the <HISTORY_SUMMARY> block---no additional commentary or input echoes. Prompt E.8: AppWorld Prompt for observation compression before optimization Your task is to generate \"Reasoning\" and \"Refined Observation\" based on the inputs below. In the \"Reasoning\", analyze the user instruction and history to identify what information from the current observation is necessary to complete the remaining steps. Think about what parts can be summarized or transformed to reduce length, while ensuring that future actions can still be executed based on the refined observation alone. In the \"Refined Observation\", include only the information that is minimal but sufficient for the next steps. [Information source] # User Instruction {{ task }} # History of interactions {{ history }} # Observation at the current time step {{ observation }} [Output format] # Reasoning ... your reasoning for what matters and how to optimize it ... # Refined Observation ... reduced and actionable observation ... Prompt E.9: AppWorld Prompt for observation compression after optimization (UT) Your task: write two sections---\"Reasoning\" and \"Refined Observation\". 1. Reasoning - Examine task, history, and observation. - Decide exactly which parts of the observation must be kept so the next agent step can succeed. - Note any need to paginate (page_limit default = 5, page_index). - Justify any data you drop. 2. Refined Observation 33 - Contain only the minimal yet sufficient info for the next step. - Always preserve: - Every endpoint that may be called, plus its full parameter list and defaults (especially page_limit/ page_index, auth tokens). - Response-schema fields referenced or likely needed later (e.g., play_count, release_date, like_count, position, ids). - Raw data rows required for future comparisons or loops; if summarising, keep at least all positive-match examples. - Never: - Omit defaults that affect behaviour. - Declare parameters \"not critical\" without proof. - Hallucinate endpoints or fields. - Replace machine-readable data with vague prose. [Information source] # User Instruction {{ task }} # History of interactions {{ history }} # Observation at the current time step {{ observation }} [Output format] # Reasoning ...concise analysis explaining what is kept/removed... # Refined Observation ...trimmed yet complete observation ensuring future steps remain possible... Prompt E.10: AppWorld Prompt for observation compression after optimization (UTCO) Your task: create two sections---\"Reasoning\" and \"Refined Observation\". 1. Reasoning (<=40 words) - Briefly state what was kept and why; note dropped categories and any pagination needs. 2. Refined Observation (use ONLY the current observation) - Keep strictly necessary data for the next step. - Always preserve, when present: - access_token or other auth values - page_limit, page_index and other defaults that affect calls - every endpoint name; include parameters only if required; description <= 4 words - identifiers/fields needed for comparisons, loops, or API calls (ids, titles, counts, paths, etc.) - Minimise length: - Delete unused fields and narrative text. - Compress JSON/arrays (one object per line, no extra spaces). - Summarise long uniform lists with range/pattern when individual rows are not needed. - Never invent, alter, or omit required literal. - Exclude history, prior summaries, and meta comments. [Information source] # User Instruction {{ task }} # History of interactions {{ history }} # Observation at the current time step {{ observation }} [Output format] # Reasoning ... # Refined Observation ... 35 Example E.1: AppWorld task trajectory example Problem: Name the artist least recommended to me on Spotify. # First, let's check what APIs are available under the Spotify app to see if there is an API related to recommendations or artists. (cid:44) print(apis.api_docs.show_api_descriptions(app_name='spotify')) [{\"name\":\"login\",...},{\"name\":\"show_recommendations\",\"descriptio n\":\"Get personalized song recommendations for the user.\"}, (cid:44) ...] (cid:44) # There is an API called \"show_recommendations\" which provides (cid:44) personalized song recommendations. Let's check its detailed specification... (cid:44) print(apis.api_docs.show_api_doc(app_name='spotify', (cid:44) api_name='show_recommendations')) {\"path\":\"/recommendations\",\"method\":\"GET\",\"parameters\":[{\"name\": (cid:44) \"access_token\",\"required\":true}, {\"name\":\"page_index\",...},{\"name\":\"page_limit\",...}], ...} (cid:44) login spec. # To call `show_recommendations`, need an access token. Check the (cid:44) print(apis.api_docs.show_api_doc(app_name='spotify', (cid:44) api_name='login')) {\"path\":\"/auth/token\",\"method\":\"POST\",\"parameters\":[{\"name\":\"use (cid:44) rname\"},{\"name\":\"password\"}], ...} from supervisor. # need Timothy's Spotify account password to log in. Let's get it (cid:44) spotify_passwords = apis.supervisor.show_account_passwords() spotify_password = next((acc[\"password\"] for acc in spotify_passwords (cid:44) print(spotify_password) if acc[\"account_name\"]==\"spotify\"), None) <REDACTED> # Now log in to get the access token. spotify_login = apis.spotify.login(username=\"timothy.whit@gmail.com\", password=\"<REDACTED>\") access_token = spotify_login.get(\"access_token\") print(access_token) eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...<REDACTED>... # Now get the first page of personalized song recommendations. recommendations_page_0 = apis.spotify.show_recommendations( access_token=access_token, page_index=0, page_limit=20 ) print(recommendations_page_0[:4]) # show few items only [ You\",\"artists\":[{\"name\":\"Lucas Grey\"}]}, {\"song_id\":10,\"title\":\"The Curse of Loving (cid:44) {\"song_id\":11,\"title\":\"Lost in Moment's Grace\",\"artists\":[{\"name\":\"Apollo (cid:44) Serenade\"},{\"name\":\"Phoenix Rivers\"}]}, (cid:44) {\"song_id\":40,\"title\":\"Burning Like Distant (cid:44) ... Star\",\"artists\":[{\"name\":\"Aria Sterling\"}]}, ] # To find the artist least recommended to Timothy, will count how (cid:44) many times each artist appears in the recommendations. The artist with the fewest appearances is the least recommended. (cid:44) from collections import Counter r[\"artists\"]] artist_names = [a[\"name\"] for in recommendations_page_0 for in (cid:44) artist_counts = Counter(artist_names) least_recommended_artist = min(artist_counts, key=artist_counts.get) print(least_recommended_artist) Evelyn Rose # Task complete. will submit \"Evelyn Rose\" as the answer. apis.supervisor.complete_task(answer=\"Evelyn Rose\") Example E.2: AppWorld trajectory example - gpt-4.1-mini agent fails without history compression Problem: Delete all .pdf files from /downloads. # Understand available APIs in the file_system app. print(apis.api_docs.show_api_descriptions(app_name='file_system')) [ {\"name\":\"show_account\",\"description\":\"Show your account (cid:44) information. Unlike show_profile, this includes private information.\"}, last name in the account profile.\"}, (cid:44) {\"name\":\"signup\",\"description\":\"Sign up to create account.\"}, {\"name\":\"delete_account\",\"description\":\"Delete your account.\"}, {\"name\":\"update_account_name\",\"description\":\"Update your first or (cid:44) ... {\"name\":\"copy_directory\",\"description\":\"Copy directory to (cid:44) {\"name\":\"move_directory\",\"description\":\"Move directory to (cid:44) {\"name\":\"compress_directory\",\"description\":\"Compress (cid:44) {\"name\":\"decompress_file\",\"description\":\"Decompress compressed (cid:44) ] another location.\"}, another location.\"}, directory.\"}, file.\"} # List files in /downloads to find PDFs. downloads_path = '/downloads' 37 downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: Response status code is 401: {\"message\":\"You are either not authorized to access this (cid:44) file_system API endpoint or your access token is missing, invalid or expired.\"} (cid:44) # Need authentication. Check supervisor for stored credentials. print(apis.supervisor.show_account_passwords()) [ {\"account_name\":\"amazon\",\"password\":\"Gt$!_*W\"}, {\"account_name\":\"file_system\",\"password\":\"8nNw!jZ\"}, {\"account_name\":\"gmail\",\"password\":\"qu4Y7}s\"}, {\"account_name\":\"phone\",\"password\":\"C4n&I40\"}, {\"account_name\":\"simple_note\",\"password\":\"e+QwbmV\"}, {\"account_name\":\"splitwise\",\"password\":\"mSqG}QU\"}, {\"account_name\":\"spotify\",\"password\":\"%CCvl8v\"}, {\"account_name\":\"todoist\",\"password\":\"jHZ#RPM\"}, {\"account_name\":\"venmo\",\"password\":\"Wq8!RAU\"} ] # Try logging in with just the password (no username). apis.supervisor.show_account_passwords() file_system_password = [a[\"password\"] for in (cid:44) if a[\"account_name\"] == \"file_system\"][0] file_system_login_response = (cid:44) print(file_system_login_response) apis.file_system.login(password=file_system_password) Execution failed. Traceback: ... Exception: Response status code is 422: {\"message\":\"Validation error. Reason: nusername: Field (cid:44) required\"} # Need username. Check supervisor profile for email/username. print(apis.supervisor.show_profile()) @gmail.com](mailto:susanmiller@gmail.com)\", {\"first_name\":\"Susan\",\"last_name\":\"Burton\",\"email\":\"[susanmiller (cid:44) \"phone_number\":\"3296062648\",\"birthday\":\"1994-04-30\",\"sex\":\"femal (cid:44) e\"} # Attempt login with email as username. \"[susanmiller@gmail.com](mailto:susanmiller@gmail.com)\" file_system_username = (cid:44) file_system_password = [a[\"password\"] for in (cid:44) apis.supervisor.show_account_passwords() 38 if a[\"account_name\"] == \"file_system\"][0] file_system_login_response = apis.file_system.login( username=file_system_username, password=file_system_password) print(file_system_login_response) {\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...<REDACTE (cid:44) D>...\",\"token_type\":\"Bearer\"} # Try to set token manually (guessing an API), then list directory. apis.file_system.set_access_token(\"...<REDACTED>...\") downloads_path = '/downloads' downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: No API named 'set_access_token' found in the (cid:44) file_system app. # Call show_directory again, assuming session is persisted after (cid:44) login. downloads_path = '/downloads' downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: Response status code is 401: {\"message\":\"You are either not authorized to access this (cid:44) file_system API endpoint or your access token is missing, invalid or expired.\"} (cid:44) # Retry: fresh login, try immediate call. login_response = apis.file_system.login( username=file_system_username, password=file_system_password) access_token = login_response.get(\"access_token\") print(access_token) # for debugging eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...<REDACTED>... # Try show_directory once more after login. downloads_path = '/downloads' downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: Response status code is 401: {\"message\":\"You are either not authorized to access this (cid:44) file_system API endpoint or your access token is missing, invalid or expired.\"} (cid:44) 39 # Inspect file_system APIs again for any token/session helpers. print(apis.api_docs.show_api_descriptions(app_name='file_system')) [{\"name\":\"show_account\",\"description\":\"Show your account (cid:44) information. Unlike show_profile, this includes private information.\"}, last name in the account profile.\"}, (cid:44) {\"name\":\"signup\",\"description\":\"Sign up to create account.\"}, {\"name\":\"delete_account\",\"description\":\"Delete your account.\"}, {\"name\":\"update_account_name\",\"description\":\"Update your first or (cid:44) {\"name\":\"login\",\"description\":\"Login to your account.\"}, ... {\"name\":\"copy_directory\",\"description\":\"Copy directory to (cid:44) {\"name\":\"move_directory\",\"description\":\"Move directory to (cid:44) {\"name\":\"compress_directory\",\"description\":\"Compress (cid:44) {\"name\":\"decompress_file\",\"description\":\"Decompress compressed (cid:44) another location.\"}, another location.\"}, directory.\"}, file.\"}] # Try logout -> login -> call again. apis.file_system.logout() login_response = apis.file_system.login( username=file_system_username, password=file_system_password) downloads_path = '/downloads' downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: Response status code is 422: {\"message\":\"You are either not authorized to access this (cid:44) file_system API endpoint or your access token is missing, invalid or expired.\"} (cid:44) # Final retry without logout; still unauthorized. login_response = apis.file_system.login( username=file_system_username, password=file_system_password) downloads_path = '/downloads' downloads_files = apis.file_system.show_directory(path=downloads_path) print(downloads_files) Execution failed. Traceback: ... Exception: Response status code is 401: {\"message\":\"You are either not authorized to access this (cid:44) file_system API endpoint or your access token is missing, invalid or expired.\"} (cid:44) # Supervisor cannot manage files; mark task failed due to (cid:44) authorization issues. print(apis.api_docs.show_api_descriptions(app_name='supervisor')) 40 profile information.\"}, task as complete with the given answer.\"}, active task assigned to you by the supervisor.\"}, [ {\"name\":\"show_active_task\",\"description\":\"Show the currently (cid:44) {\"name\":\"complete_task\",\"description\":\"Mark the currently active (cid:44) {\"name\":\"show_profile\",\"description\":\"Show your supervisor's (cid:44) {\"name\":\"show_addresses\",\"description\":\"Show your supervisor's (cid:44) {\"name\":\"show_payment_cards\",\"description\":\"Show your (cid:44) {\"name\":\"show_account_passwords\",\"description\":\"Show your (cid:44) ] supervisor's app account passwords.\"} supervisor's payment_cards.\"}, addresses.\"}, apis.supervisor.complete_task(status=\"fail\") Example E.3: AppWorld trajectory example - gpt-4.1-mini agent successes with history compression Problem: Delete all .pdf files from /downloads. 41 (... previous raw history is omitted for brevity ...) <HISTORY_SUMMARY> 1. REASONING failed with 401 Unauthorized. - User requested deletion of all .pdf files in /downloads. - file_system APIs require authentication; initial calls (cid:44) (... omitted ...) - Confirmed login API returns access_token but no mechanism to (cid:44) - Need to find way to authorize file_system API calls with (cid:44) the access token or alternative approach. use it in subsequent calls found. 2. VARS value purpose name (cid:44) --------------------------- file_system_username \"susanmiller@gmail.com\" (cid:44) file_system_password \"8nNw!jZ\" (cid:44) file_system_token (cid:44) Password for file_system login Username for file_system login \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...sBA8nTOB...\" Access token from login (cid:44) downloads_path (cid:44) Target directory for file listing \"/downloads\" 3. TODO file_system API calls to authorize them. - Determine how to pass or set the access token for (cid:44) - Once authorized, list files in /downloads to identify .pdf (cid:44) - Delete all .pdf files found in /downloads. files. 4. COMPLETED password, obtained access token. - Retrieved file_system account password from supervisor app. - Successfully logged in to file_system app with username and (cid:44) - Confirmed file_system APIs require authorization; calls (cid:44) - Verified no supervisor app API to proxy file system (cid:44) - Confirmed login API returns access token but no visible (cid:44) method to use it in subsequent calls. without token fail. operations. 5. GUARDRAILS avoid 401 errors. environment does not auto-use them. - Always provide both username and password for login API. - Access tokens must be explicitly handled or passed if (cid:44) - Validate authorization before calling protected APIs to (cid:44) - Avoid redundant login attempts without token usage (cid:44) - Preserve tokens and credentials securely for session continuity. (cid:44) </HISTORY_SUMMARY> resolution. # The show_directory API requires the access_token as parameter. # List files in \"/downloads\" using the token. 42 downloads_path = \"/downloads\" file_system_access_token = (cid:44) \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...sBA8nTOB...\" print(apis.file_system.show_directory( access_token=file_system_access_token, directory_path=downloads_path, entry_type=\"files\", recursive=False)) [ \"/home/susan/downloads/aerobics_class.jpg\", \"/home/susan/downloads/art_inspiration_sketches.zip\", \"/home/susan/downloads/bakeware.pdf\", \"/home/susan/downloads/bowls_for_snacks.pdf\", \"/home/susan/downloads/candy.pdf\", \"/home/susan/downloads/charcoal_pencils.pdf\", ..., \"/home/susan/downloads/travel_insurance.pdf\", \"/home/susan/downloads/workout_clothes.pdf\", \"/home/susan/downloads/world_cuisine_recipe_book.epub\", \"/home/susan/downloads/world_landmarks_photo_album.zip\" ] # Filter to only \".pdf\" files. downloads_files = [ \"/home/susan/downloads/aerobics_class.jpg\", \"/home/susan/downloads/bakeware.pdf\", \"/home/susan/downloads/bowls_for_snacks.pdf\", \"/home/susan/downloads/candy.pdf\", \"/home/susan/downloads/charcoal_pencils.pdf\", ..., \"/home/susan/downloads/travel_insurance.pdf\", \"/home/susan/downloads/workout_clothes.pdf\" ] pdf_files = [p for in downloads_files if p.lower().endswith(\".pdf\")] print(pdf_files) [ \"/home/susan/downloads/bakeware.pdf\", \"/home/susan/downloads/bowls_for_snacks.pdf\", \"/home/susan/downloads/candy.pdf\", \"/home/susan/downloads/charcoal_pencils.pdf\", ..., \"/home/susan/downloads/travel_insurance.pdf\", \"/home/susan/downloads/workout_clothes.pdf\" ] # Confirm delete_file API parameters before deletion. print(apis.api_docs.show_api_doc(app_name=\"file_system\", (cid:44) api_name=\"delete_file\")) 43 { \"app_name\":\"file_system\", \"api_name\":\"delete_file\", \"path\":\"/file\", \"method\":\"DELETE\", \"description\":\"Delete file.\", \"parameters\":[ {\"name\":\"file_path\",\"type\":\"string\",\"required\":true, \"description\":\"Path of the file. Path can be absolute, starting (cid:44) with '/', or relative to the user's home directory, starting with '/'.\"}, login.\"} (cid:44) {\"name\":\"access_token\",\"type\":\"string\",\"required\":true, \"description\":\"Access token obtained from file_system app (cid:44) ], \"response_schemas\":{ \"success\":{\"message\":\"string\",\"file_path\":\"string\"}, \"failure\":{\"message\":\"string\"} } } # Delete each PDF using the access_token. delete_results = [] for pdf in pdf_files: res = apis.file_system.delete_file( file_path=pdf, access_token=file_system_access_token) delete_results.append(res) # Mark task complete. apis.supervisor.complete_task() 44 Compression E.1: AppWorld History Compression Example task: got some Splitwise group invitations over phone text messages yesterday. If their number is in my phone contact book, accept it, otherwise delete those messages. ---------------------------------------- History Compression (Prompting baseline): ### REASONING The agent began by identifying relevant apps for handling text messages and Splitwise invitations, then explored the phone app 's APIs to manage messages and contacts. To search for Splitwise group invitations received via text message, the agent determined it needed to log in to the phone app to obtain an access token. Using credentials from the supervisor app, the agent successfully logged in and retrieved the access token . The agent then calculated yesterday's date range and searched for text messages containing \"Splitwise,\" retrieving list of messages. The rationale for retaining message details (sender name, phone number, contact_id, message content, sent_at) is to enable the next step: checking if the sender's number is in the contact book, so the agent can accept or delete invitations as instructed. Earlier steps (API exploration, login, date calculation) directly enabled the message search and data extraction. ### COMPLETED - Identified relevant apps and APIs for the task. - Successfully logged in to the phone app and obtained access token . - Calculated yesterday's date range. - Searched and retrieved text messages mentioning \"Splitwise\" from yesterday and beyond, including sender details and message content. ---------------------------------------- History Compression (ACON (utility step)): 1. REASONING - Identified \"phone\" app for managing text messages and contacts . - Retrieved API docs to find endpoints for searching messages and contacts. - Determined yesterday's date (May 17, 2023) for filtering relevant messages. - Checked \"search_text_messages\" API spec for required parameters. - Used supervisor app to obtain phone app password. - Logged in to phone app to get access_token. - Searched for messages containing \"Splitwise\" with page_limit =20, sorted by -created_at. - Located Splitwise group invitation messages; sender contact_id used to determine if sender is in contact book. 2. VARS name value purpose ------------------------------------------------------------ 45 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. access_token eyJzdWIiOiJwaG9uZSs3OTM0ODE4MTkxIiwiZXhwIjoxNjg0NDEyMDk4fQ. ad3t3S_Xj7YhoDDFZeW4BlVL4dNniMdfaXC1143fbzw Auth for phone app APIs phone_number 7934818191 User's phone number phone_password X53y6aW search_query \"Splitwise\" Phone app password sort_by page_index page_limit Filter for relevant messages -created_at Get latest messages first 0 Current page index for pagination Number of results per page target_date 2023-05-17 invitation_msgs [IDs: 16794, 16800] Date to filter messages Splitwise invitations from 2023-05-17 sender_info :\"3707865891\"}, {id:null, name:\"Anita Burch\", phone :\"3643463570\"}] For contact book check [{id:1857, name:\"Alex White\", phone 3. TODO - For each Splitwise invitation message from 2023-05-17: - Check if sender's phone number exists in contact book ( search_contacts API). - If in contact book, accept invitation (Splitwise API, if needed). - If not in contact book, delete the message ( delete_text_message API). - Paginate search_text_messages if more relevant messages may exist. 4. COMPLETED - Identified relevant apps and APIs for task. - Retrieved phone app password from supervisor. - Logged in to phone app and obtained access_token. - Searched and filtered text messages for Splitwise invitations on target date. - Isolated relevant message IDs and sender info for next steps. 5. GUARDRAILS - Always re-declare VARS after memory reset. - Paginate message search until no more results. - Validate API parameters before each call. - Avoid redundant logins and API doc look-ups. - Only act on messages matching both query and target date. ---------------------------------------- History Compression (ACON (utility step + compression step)): 46 1. REASONING identified voice messages from the day before yesterday (2023-05-16) containing Splitwise group invitations. To follow instructions, need to check if the senders' phone numbers are in Sierra's contact book before accepting or deleting these messages. 2. VARS access_token=<token>, splitwise_invite_voice_ids =[17022,17036,17028,17034], sender_numbers =['9595488531','7375474944','2123782445','3808719492'] 3. TODO - Search Sierra's contact book for sender_numbers. - Accept Splitwise invitations from contacts found. - Delete voice messages from non-contacts. 4. COMPLETED - Logged in to phone app and obtained access_token. - Retrieved voice messages from 2023-05-16. - Identified Splitwise invitation messages and sender numbers. 5. GUARDRAILS Only process Splitwise invitations from contacts; delete others. Compression E.2: 8-objective QA History Compression Example task: when was the last time the dodgers played yankees in the world series?; where will be the next olympics be held?; who is tinker air force base named after?; who does the voice of cruz on cars 3?; who's the original singer of help me make it through the night?; where does the movie mothers day take place ?; what is the big gold dome in jerusalem?; when did russia join the world economic forum? ---------------------------------------- History Compression (Prompting baseline): ### REASONING The agent began by addressing the user's multi-part query, starting with \"when was the last time the Dodgers played Yankees in the World Series?\" Through Wikipedia searches, the agent identified 1981 as the most recent World Series meeting between the Dodgers and Yankees. This answer was retained due to clear evidence in the retrieved documents. The agent then attempted to answer \"where will be the next Olympics be held?\" but initial searches returned irrelevant or outdated results (e.g., Amsterdam 1928, Buenos Aires 2018). Recognizing the lack of direct answers, the agent refined its search queries multiple times, but still did not retrieve the current location for the next Olympics. The agent's iterative approach demonstrates commitment to accuracy and highlights the need to adjust search strategies when initial attempts fail. Retaining details of failed searches helps prevent repeated mistakes and guides future queries. ### COMPLETED - Identified the last time the Dodgers played the Yankees in the World Series: 1981. ---------------------------------------- History Compression (ACON (utility step)): ### REASONING - Used Wikipedia search for each question; refined Olympics query after initial results lacked direct info. - Collapsed repeated/irrelevant Olympic bid results; focused on confirmed host city. - For Tinker AFB, confirmed name origin from first document. ### COMPLETED - Last Dodgers vs Yankees World Series: 1955 (source: \"1955 World Series\") - Next Olympics location: Paris, 2024 (source: \"Budapest bid for the 2024 Summer Olympics\") - Tinker Air Force Base named after Major General Clarence L. Tinker (source: \"Tinker Air Force Base\") ### STATE LOG FACT CACHE: - When was the last time the Dodgers played Yankees in the World Series? -> 1955 (\"1955 World Series\") - Where will the next Olympics be held? -> Paris, 2024 (\"Budapest bid for the 2024 Summer Olympics\") - Who is Tinker Air Force Base named after? -> Major General Clarence L. Tinker (\"Tinker Air Force Base\") PENDING: - Who does the voice of Cruz on Cars 3? - Who is the original singer of \"Help Me Make It Through the Night \"? - Where does the movie \"Mother's Day\" take place? - What is the big gold dome in Jerusalem? - When did Russia join the World Economic Forum? ATTEMPTED QUERIES / LOOP GUARD: - \"Dodgers Yankees World Series history\" OK - \"next Olympics location\" FAIL (no direct info) - \"2024 Olympics location\" OK - \"Tinker Air Force Base name origin\" OK - \"next Olympics location\" tried twice (loop guard: must reformulate/abandon) ERROR NOTES: None (End of summary - no extra text) ---------------------------------------- History Compression (ACON (utility step + compression step)): ### FACT CACHE - When was the last time the Dodgers played Yankees in the World Series? -> 1956 ### PENDING where will the next olympics be held?, who is tinker air force base named after?, who does the voice of cruz on cars 3?, who is the original singer of help me make it through the night?, where does the movie mothers day take place?, what is the big gold dome in jerusalem?, when did russia join the world economic forum? 48 ### BLOCKED next Olympics location: 3x failed searches (loop-guard) ### REASONING Olympics location queries returned irrelevant results; alternate sources or query phrasing needed. Compression E.3: OfficeBench History Compression Example task: read company revenues, save the top 2 revenue years in top2. xlsx, generate reports.docx ---------------------------------------- History Compression (Prompting baseline): ### REASONING The agent began by identifying the relevant data file ( company_revenues.xlsx) containing company revenue information. After reading the file, the agent determined the top 2 revenue years: 2014 ($4,983,791) and 2015 ($4,611,210). The agent then created new Excel file (top2.xlsx) and started populating it with the required data, first adding headers (\"Year\", \"Revenue ($)\") and then entering the first top year (2014) in the second row, first column. The rationale for retaining row and column indices, file paths, and the top 2 years is to ensure accurate and complete transfer of data and to facilitate the next steps (completing the top2.xlsx file and generating reports.docx). ### COMPLETED - Located and read company_revenues.xlsx. - Identified top 2 revenue years: 2014 ($4,983,791), 2015 ($4 ,611,210). - Created top2.xlsx and added headers (\"Year\", \"Revenue ($)\"). - Entered \"2014\" in (2, 1) of top2.xlsx. ---------------------------------------- History Compression (ACON (utility step)): ### REASONING - Located company revenue data in company_revenues.xlsx. - Read all rows; identified top 2 revenue years: 2014 ($4,983,791) and 2015 ($4,611,210). - Created top2.xlsx and began writing header and first top year. - Each numeric value was copied verbatim from source. - No errors or mismatches so far; headers and first year written successfully. - Pending: finish writing revenue for 2014, add 2015 row, then generate reports.docx. Verify numeric values directly from source file before use. --- ### DATA STATE #### company_revenues.xlsx (source table) Row Year Revenue ($) ------------------------ 2004 4439044 2 49 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2005 2212203 2006 1193225 2007 2793265 2008 2080460 2009 1791112 2010 3098049 2011 3975966 2012 3251777 2013 2918667 2014 4983791 2015 4611210 2016 3294631 2017 1463500 2018 3945846 2019 3907271 2020 3651262 2021 2005820 2022 2170828 2023 2832545 #### top2.xlsx (current state) (row, col) Value --------------------------- (1, 1) (1, 2) (2, 1) (2, 2) Year Revenue ($) 2014 [Empty Cell] --- ### TODO / PENDING - Write 2014 revenue (4983791) to (2,2) in top2.xlsx. - Write 2015 year (3,1) and revenue (4611210) (3,2) in top2.xlsx. - Generate reports.docx summarizing top 2 revenue years. - Re-read source rows before writing each numeric value for consistency. --- ### COMPLETED - Located and read company_revenues.xlsx. - Created top2.xlsx. - Wrote headers and 2014 year to top2.xlsx. Compression E.4: AppWorld Observation Compression Example task: am going for 15-minute drive without internet. Play an album from my Spotify library that already has enough downloaded songs for it, so do not have to repeat. action: print(apis.api_docs.show_api_descriptions(app_name='spotify ')) ---------------------------------------- Raw Observation: [ { \"name\": \"show_account\", \"description\": \"Show your account information. Unlike show_profile, this includes private information.\" }, 50 { \"name\": \"signup\", \"description\": \"Sign up to create account.\" }, { \"name\": \"delete_account\", \"description\": \"Delete your account.\" }, { \"name\": \"update_account_name\", \"description\": \"Update your first or last name in the account profile.\" }, { \"name\": \"login\", \"description\": \"Login to your account.\" }, { \"name\": \"logout\", \"description\": \"Logout from your account.\" }, { \"name\": \"send_verification_code\", \"description\": \"Send account verification code to your email address.\" }, { \"name\": \"verify_account\", \"description\": \"Verify your account using the verification code sent to your email address.\" }, { \"name\": \"send_password_reset_code\", \"description\": \"Send password reset code to your email address.\" }, { \"name\": \"reset_password\", \"description\": \"Reset your password using the password reset code sent to your email address.\" }, { \"name\": \"show_profile\", \"description\": \"Show public profile information of user.\" }, { \"name\": \"show_genres\", \"description\": \"Show the list of all music genres.\" }, { \"name\": \"search_songs\", \"description\": \"Search for songs with query.\" }, { \"name\": \"show_song\", \"description\": \"Get details of specific song.\" }, { \"name\": \"show_song_privates\", \"description\": \"Show information about the song that is private to the user.\" }, { \"name\": \"like_song\", 51 \"description\": \"Like song.\" }, { \"name\": \"unlike_song\", \"description\": \"Unlike song.\" }, { \"name\": \"show_liked_songs\", \"description\": \"Get list of songs you have liked.\" }, { \"name\": \"search_albums\", \"description\": \"Search for albums with query.\" }, { \"name\": \"show_album\", \"description\": \"Get details of specific album.\" }, { \"name\": \"show_album_privates\", \"description\": \"Show information about the album that is private to the user.\" }, { \"name\": \"like_album\", \"description\": \"Like album.\" }, { \"name\": \"unlike_album\", \"description\": \"Unlike album.\" }, { \"name\": \"show_liked_albums\", \"description\": \"Get list of albums you have liked.\" }, { \"name\": \"show_playlist_library\", \"description\": \"Get list of playlists in the user's playlist library.\" }, { \"name\": \"search_playlists\", \"description\": \"Search for playlists with query. It will search over all public playlists and your own private playlists.\" }, { \"name\": \"create_playlist\", \"description\": \"Create new playlist.\" }, { \"name\": \"show_playlist\", \"description\": \"Get detailed information about specific playlist. You can view your own playlists or others' playlists if they are public.\" }, { \"name\": \"delete_playlist\", \"description\": \"Delete playlist.\" }, { \"name\": \"update_playlist\", \"description\": \"Update playlist title or privacy.\" }, { \"name\": \"show_playlist_privates\", \"description\": \"Show information about the playlist that is private to the user.\" }, { \"name\": \"like_playlist\", \"description\": \"Like playlist.\" }, { \"name\": \"unlike_playlist\", \"description\": \"Unlike playlist.\" }, { \"name\": \"show_liked_playlists\", \"description\": \"Get list of playlists you have liked.\" }, { \"name\": \"search_artists\", \"description\": \"Search for artists with query.\" }, { \"name\": \"show_artist\", \"description\": \"Get details of specific artist.\" }, { \"name\": \"show_artist_following\", \"description\": \"Show if the user is following the artist.\" }, { \"name\": \"show_song_library\", \"description\": \"Get list of songs in the user's song library.\" }, { \"name\": \"add_song_to_library\", \"description\": \"Add song to the user's song library.\" }, { \"name\": \"remove_song_from_library\", \"description\": \"Remove song from the user's song library.\" }, { \"name\": \"show_album_library\", \"description\": \"Get list of albums in the user's album library .\" }, { \"name\": \"add_album_to_library\", \"description\": \"Add an album to the user's album library.\" }, { \"name\": \"remove_album_from_library\", \"description\": \"Remove an album from the user's album library.\" }, { \"name\": \"add_song_to_playlist\", \"description\": \"Add song to playlist.\" }, { \"name\": \"remove_song_from_playlist\", \"description\": \"Remove song from playlist.\" }, { 53 \"name\": \"show_downloaded_songs\", \"description\": \"Get list of downloaded songs.\" }, { \"name\": \"download_song\", \"description\": \"Download song.\" }, { \"name\": \"remove_downloaded_song\", \"description\": \"Remove song from downloads.\" }, { \"name\": \"show_following_artists\", \"description\": \"Get list of artists the user is following.\" }, { \"name\": \"follow_artist\", \"description\": \"Follow an artist.\" }, { \"name\": \"unfollow_artist\", \"description\": \"Unfollow an artist.\" }, { \"name\": \"show_song_reviews\", \"description\": \"Get list of reviews for song.\" }, { \"name\": \"review_song\", \"description\": \"Rate or review song.\" }, { \"name\": \"show_song_review\", \"description\": \"Show song review.\" }, { \"name\": \"delete_song_review\", \"description\": \"Delete song review.\" }, { \"name\": \"update_song_review\", \"description\": \"Update song review.\" }, { \"name\": \"show_album_reviews\", \"description\": \"Get list of reviews for an album.\" }, { \"name\": \"review_album\", \"description\": \"Rate or review an album.\" }, { \"name\": \"show_album_review\", \"description\": \"Show an album review.\" }, { \"name\": \"delete_album_review\", \"description\": \"Delete an album review.\" }, { \"name\": \"update_album_review\", \"description\": \"Update an album review.\" }, 54 { \"name\": \"show_playlist_reviews\", \"description\": \"Show list of reviews for your playlist or others' public playlist.\" }, { \"name\": \"review_playlist\", \"description\": \"Rate or review playlist.\" }, { \"name\": \"show_playlist_review\", \"description\": \"Show playlist review.\" }, { \"name\": \"delete_playlist_review\", \"description\": \"Delete playlist review.\" }, { \"name\": \"update_playlist_review\", \"description\": \"Update playlist review.\" }, { \"name\": \"show_payment_cards\", \"description\": \"Get list of users payment cards.\" }, { \"name\": \"add_payment_card\", \"description\": \"Add new payment card.\" }, { \"name\": \"show_payment_card\", \"description\": \"Get details of payment card.\" }, { \"name\": \"delete_payment_card\", \"description\": \"Delete payment card information.\" }, { \"name\": \"update_payment_card\", \"description\": \"Update payment card information.\" }, { \"name\": \"show_current_song\", \"description\": \"Show details of the current song on the queue.\" }, { \"name\": \"play_music\", \"description\": \"Play music based on various criteria. You can pass, at most, any one of queue_position, song_id, album_id or playlist_id. If one of song_id, album_id or playlist_id is passed, that song, album or playlist will be added to the queue and played. Otherwise, the queue will remain unchanged. If queue_position is passed, the song at that position in the queue will be played. If none is passed, the current song in the queue will be played.\" }, { \"name\": \"pause_music\", \"description\": \"Pause the currently playing song.\" }, { \"name\": \"previous_song\", \"description\": \"Go to the previous song in the song queue.\" 55 }, { \"name\": \"next_song\", \"description\": \"Go to the next song in the song queue.\" }, { \"name\": \"move_song_in_queue\", \"description\": \"Move song in the queue to new position.\" }, { \"name\": \"seek_song\", \"description\": \"Seek the current song to the given number of seconds.\" }, { \"name\": \"loop_song\", \"description\": \"Set whether to loop the current song.\" }, { \"name\": \"shuffle_song_queue\", \"description\": \"Shuffle songs in the music player queue.\" }, { \"name\": \"show_song_queue\", \"description\": \"Get the music player song queue. Songs are played in the order of the queue in cycle.\" }, { \"name\": \"add_to_queue\", \"description\": \"Add song, album or playlist to the music player song queue.\" }, { \"name\": \"clear_song_queue\", \"description\": \"Clear the music player song queue.\" }, { \"name\": \"remove_song_from_queue\", \"description\": \"Remove song at the given position from the music player song queue.\" }, { \"name\": \"show_volume\", \"description\": \"Get the volume level of the music player.\" }, { \"name\": \"set_volume\", \"description\": \"Set the volume level of the music player.\" }, { \"name\": \"show_recommendations\", \"description\": \"Get personalized song recommendations for the user.\" }, { \"name\": \"show_premium_plans\", \"description\": \"Show information about premium plans available.\" }, { \"name\": \"show_premium_subscriptions\", \"description\": \"Show your premium subscription history.\" }, { 56 \"name\": \"subscribe_premium\", \"description\": \"Subscribe to premium membership.\" }, { \"name\": \"download_premium_subscription_receipt\", \"description\": \"Download the receipt for premium subscription.\" } ] ---------------------------------------- Obs. Compression (Prompting baseline): The Spotify API provides: - show_album_library: get user's album library. - show_downloaded_songs: get list of downloaded songs. - show_album: get details of specific album. ---------------------------------------- Obs. Compression (ACON (utility step)): [ { \"name\": \"show_album_library\", \"description\": \"Get list of albums in the user's album library.\" }, { }, { }, { \"name\": \"show_downloaded_songs\", \"description\": \"Get list of downloaded songs.\" \"name\": \"show_album\", \"description\": \"Get details of specific album.\" \"name\": \"play_music\", \"description\": \"Play music based on various criteria. You can pass, at most, any one of queue_position, song_id, album_id or playlist_id. If one of song_id, album_id or playlist_id is passed, that song, album or playlist will be added to the queue and played. Otherwise, the queue will remain unchanged. If queue_position is passed, the song at that position in the queue will be played. If none is passed, the current song in the queue will be played.\" } ] ---------------------------------------- History Compression (ACON (utility step + compression step)): [{\"name\":\"show_album_library\",\"description\":\"Get user's album library.\"},{\"name\":\"show_downloaded_songs\",\"description\":\"Get downloaded songs.\"},{\"name\":\"show_album_privates\",\"description \":\"Show album private info.\"},{\"name\":\"play_music\",\"description \":\"Play music; album_id allowed.\"}]"
        }
    ],
    "affiliations": [
        "KAIST",
        "Microsoft",
        "University of Cambridge"
    ]
}