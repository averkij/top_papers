{
    "paper_title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
    "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Huizhuo Yuan",
        "Yang Yuan",
        "Quanquan Gu",
        "Andrew C Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 0 5 7 1 . 5 0 5 2 : r On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning Yifan Zhang* 1 Yifeng Liu* 1 Huizhuo Yuan1 Yang Yuan2,3 Quanquan Gu1 Andrew Yao2,3 1University of California, Los Angeles 2IIIS, Tsinghua University 3Shanghai Qi Zhi Institute Abstract Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents nuanced and systematically explorable design space. In this paper, we propose Regularized Policy Gradient (RPG), systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG."
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement learning (RL), particularly policy gradient (PG) methods, provides powerful framework for solving sequential decision-making problems in complex environments. These methods have been successfully applied in diverse domains, ranging from robotics to game playing, and have recently become instrumental in fine-tuning large language models (LLMs) to align with human preferences and instructions (Ouyang et al., 2022) and enhancing the reasoning capabilities of LLMs (Shao et al., 2024; Guo et al., 2025). Classical PG algorithms like REINFORCE (Williams, 1992) optimize policies directly but often suffer from high gradient variance. Advanced methods like Proximal Policy Optimization (PPO) (Schulman et al., 2017) improve stability and sample efficiency, enabling large-scale applications, often by operating in an off-policy manner and employing techniques like training critic models for the estimation of value functions. * Equal contribution; Corresponding authors. 1 Inputs (Iteration t): Current Policy π(t) θ Reference π(t) old Rewards R(x) Goal: LLM Reasoning RPG Core Engine [R] β KL π(t) θ 1. Construct J(θ(t)) = 2. Derive θ(t)J(θ(t)) 3. Formulate Surrogate Loss L(θ(t)) 4. Optimize to get π(t+1) θ Outputs: Updated Policy π(t+1) θ Update for next iteration (t + 1): old π(t+1) π(t+1) θ Key Design Choices (Configuration) 1. KL Divergence Type: Forward KL(πoldπθ) Reverse KL(πθπold) 2. KL Form: Normalized Unnormalized (UKL / k3) 3. Loss Estimator: Fully Differentiable REINFORCE-style Figure 1: Overview of the iterative Regularized Policy Gradient (RPG) framework proposed in this work. At each iteration t, the central RPG Core Engine processes inputs: the current policy π(t) θ , reference policy π(t) old, and associated rewards R(x). The engines operation encompasses four main steps: (1) constructing the KL-regularized objective J(θ(t)), which combines the expected reward with KL divergence term; (2) deriving the off-policy policy gradient θ(t)J(θ(t)); (3) formulating corresponding surrogate loss function L(θ(t)); and (4) optimizing the policy parameters to yield an updated policy π(t+1) , aimed at enhancing LLM reasoning capabilities. The specific behavior of the RPG Core Engine is configured by three key design choices: (i) the KL Divergence Type (Forward KL(πoldπθ) or Reverse KL(πθπold)); (ii) the KL Form (Normalized or Un-normalized, e.g., using UKL / k3 estimators); and (iii) the Loss Estimator type (Fully Differentiable or REINFORCE-style with Stop-Gradient). The framework operates iteratively, with the updated policy π(t+1) from one iteration informing the inputs for the next, including the update of the reference policy π(t+1) , to facilitate continuous learning and performance improvement. old θ θ crucial technique for stabilizing policy optimization, especially when deviating from strictly on-policy updates or aiming to control policy complexity, is regularization. Kullback-Leibler (KL) divergence is commonly used regularizer, penalizing the deviation of the learned policy πθ from reference policy πref (e.g., policy from previous iteration πθold or fixed prior policy πSFT). KL regularization helps prevent destructive policy updates, encourages exploration around known good policies, and can prevent catastrophic forgetting or overly confident outputs (Ouyang et al., 2022). Despite the widespread use of KL regularization in methods such as PPO (often implicitly through reward penalties) and explicit formulations like GRPO (Shao et al., 2024), there exists considerable variety in how the KL divergence is formulated and estimated. Different choices include Forward KL and Reverse KL, handling potentially unnormalized distributions (Minka et al., 2 2005) (leading to unnormalized KL (UKL) and unnormalized reverse KL (URKL) formulations), and the use of various estimators like the k2 and k3 estimators (Schulman, 2020) designed to potentially reduce variance or offer different properties compared to the standard log-ratio (k1) estimator. Furthermore, the interplay between the choice of KL formulation, the policy optimization setting (on-policy vs. off-policy), and the derivation of appropriate surrogate loss functions (fully differentiable vs. REINFORCE-style gradient estimators) can lead to subtle differences. This paper provides systematic derivation and analysis of KL-regularized policy gradient methods. Our main contributions are: 1. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by Forward and Reverse KL divergences, considering both standard normalized (KL) and unnormalized (UKL) forms. 2. Our methods operate within an iterative training framework where the reference model πref for KL regularization is the policy from the last iteration, πold, providing dynamic and adaptive regularization target. 3. We systematically provide derivations for fully differentiable loss functions (offering connections to variational inference) and REINFORCE-style gradient estimators (employing the stop-gradient operator). These are developed for the online setting, using off-policy gradient estimation via importance sampling from prior policy πold. We explicitly detail the connection between the k3 estimator and our unnormalized KL (UKL) framework. 4. Based on our derivations, we identify theoretical inconsistency in the GRPO objectives KL term estimation and propose corrected gradient estimator and corresponding loss function that properly incorporates importance weighting. We also analyze the KL handling in REINFORCE++ (Hu, 2025), examining its non-standard KL penalty term and its implications for off-policy regularization. 5. We present extensive experimental results on RL for LLM reasoning, demonstrating that our proposed methods can achieve stable training dynamics and competitive or improved performance compared to strong baselines like GRPO, REINFORCE++, and DAPO (Yu et al., 2025)."
        },
        {
            "title": "2 Background",
            "content": "Policy gradient (PG) methods are cornerstone of modern reinforcement learning (RL), optimizing parameterized policies πθ by estimating the gradient of an expected objective function J(θ) with respect to the policy parameters θ. Typically, J(θ) represents the expected cumulative discounted reward over trajectories τ = (s0, a0, r0, s1, . . . , sT , aT , rT ) generated by the policy: J(θ) = Eτ πθ [G(τ )], where G(τ ) = (cid:80)T t=0 γtrt is the trajectory return (with discount factor γ), and the expectation is taken over the trajectories sampled according to the policy πθ(as) and the environment dynamics p(ss, a). The Generalized Policy Gradient Theorem (GPPT) provides foundation for deriving these gradients (see Appendix for the proof). Theorem 2.1 (Generalized Policy Gradient Theorem). Let πθ(x) be probability density or mass function parameterized by θ, representing the probability of sampling item x. Let (x, θ) be scalar-valued function associated with x, potentially depending on θ. Under suitable regularity conditions, the gradient of the expectation Exπθ [f (x, θ)] with respect to θ is: θExπθ [f (x, θ)] = Exπθ [f (x, θ)θ log πθ(x) + θf (x, θ)] . (2.1) 3 The term E[f log π] reflects how changes in θ affect the probability of sampling x, while E[f ] reflects how changes in θ directly affect the function value . The classic REINFORCE algorithm (Williams, 1992) applies the GPPT to the standard RL objective J(θ) = Eτ πθ [G(τ )]. In this case, (τ, θ) = G(τ ), the total trajectory return, which does not depend directly on θ (i.e., θG(τ ) = 0). The theorem simplifies, and the gradient can be expressed using per-timestep contributions (Sutton et al., 1998): θJ(θ) = Eτ πθ (cid:35) Gtθ log πθ(atst) , (cid:34) (cid:88) t=0 where Gt = (cid:80)T detailed introduction of REINFORCE to Appendix A.1. k=t γktrk is the return-to-go from timestep t. Due to space limit, we defer the"
        },
        {
            "title": "2.1 KL Regularization in Policy Gradients",
            "content": "A common technique to stabilize policy optimization, especially in off-policy settings or when fine-tuning large models, is regularization. The Kullback-Leibler (KL) divergence is frequently used to penalize the deviation of the learned policy πθ from reference policy πref (which could be πθold, an initial supervised fine-tuned model, or another prior). KL(P Q) 0 with equality iff = almost everywhere. It is asymmetric (i.e., KL(P Q) = KL(Q )). Minimizing the forward KL KL(πref πθ) encourages πθ to cover the support of πref (zero-forcing), while minimizing the reverse KL KL(πθ πref ) encourages πθ to be concentrated where πref has high probability mass (mode-seeking). Adding KL penalty to the RL objective, such as J(θ) = Eπθ [R] β KL(πθπref ), helps control the policy update size, prevents large deviations from πref , encourages exploration near known good policies, and can mitigate issues like catastrophic forgetting or overly confident outputs, particularly relevant in LLM fine-tuning (Ouyang et al., 2022). For PPO (see Appendix A.2), this penalty can be incorporated implicitly via reward shaping: = rt β log(πθ(atst)/πref (atst)). Alternatively, it can be added explicitly to the objective function, as in GRPO. The specific form of the KL divergence (forward/reverse), whether distributions are normalized (KL vs. UKL), and the choice of estimator (e.g., standard log-ratio vs. k3 estimator (Schulman, 2020)) can vary, leading to different properties (mode seeking v.s. zero-forcing) and gradient estimators, as explored later in this paper (Sections 3 and 4)."
        },
        {
            "title": "2.2 Group Relative Policy Optimization (GRPO)",
            "content": "Group Relative Policy Optimization (GRPO) (Shao et al., 2024) adapts the PPO framework for training LLMs, notably by eliminating the need for learned value function (critic). Instead of using GAE, GRPO estimates the advantage (cid:98)Ai,t at token of output oi based on the relative rewards within group of outputs {o1, . . . , oG} sampled from the old policy πθold for the same prompt q. Crucially, GRPO modifies the PPO objective by explicitly adding KL regularization term directly to the objective function. Its objective (simplified notation) is: JGRPO(θ) = EqP (Q),{oi}πold (cid:20) 1 (cid:88) i= 1 oi oi (cid:88) (cid:16) t=1 Clip i,t (θ) β KLest (cid:0)πθ(hi,t)πref (hi,t)(cid:1)(cid:17)(cid:21) , 4 where hi,t = (q, oi,<t) is the history, Clip i,t (θ) represents the PPO-Clip term from (A.3) applied using the group-relative advantage estimate (cid:98)Ai,t, and πref is reference model (e.g., the initial SFT model). For the KL penalty, GRPO employs the k3 estimator form (Schulman, 2020), evaluated per token oi,t: KLest(πθπref ) k3 (cid:18) πref (oi,t hi,t) πθ(oi,t hi,t) (cid:19) = πref (oi,t hi,t) πθ(oi,t hi,t) log πref (oi,t hi,t) πθ(oi,t hi,t) 1. This uses the functional form k3(y) = log 1 as discussed in Schulman (2020), applied with = πref (oi,thi,t)/πθ(oi,thi,t). This form is related to the unnormalized reverse KL divergence, UKL(πθπref ) (see Section 3.4 and Appendix for detailed discussion). However, key observation regarding GRPOs KL penalty is its estimation. If the KL penalty in GRPO is intended to approximate β UKL(πθ(hi,t)πref (hi,t)), its off-policy estimation (sampling oi,t from πold) would generally involve an importance weight wi,t multiplying the k3 term. The direct subtraction without this weight means the gradient derived from GRPOs objective may not precisely correspond to the gradient of the intended target objective Clip β UKL(πθπref ) in the off-policy setting. The practical impact would depend on factors like the similarity between πθ and πold and the magnitude of β. Our results in Section 3 provide derivations for KL-regularized objectives that explicitly account for off-policy sampling via importance weights."
        },
        {
            "title": "2.3 REINFORCE++",
            "content": "REINFORCE++ (Hu, 2025) is an RLHF algorithm that avoids critic overhead and aims to mitigate issues of prompt-specific baselines. It operates without critic (γ = 1), sampling one response per prompt. Advantages are computed by first calculating per-token pre-normalization advantage Aq,ot: Aq,ot = r(o1:t, q) β (cid:88) i=t KL(i), where KL(t) = log (cid:18) πRL old (otq, o<t) πSFT(otq, o<t) (cid:19) . (2.2) Aq,ot mean(Aq,ot) std(Aq,ot) The normalized advantage Anorm q,ot is then used as the advantage estimate (cid:98)At within the PPO-Clip objective structure in (A.3). The KL formulation in (2.2) is log-ratio involving the probabilities from the sampling/old policy πRL old and the reference SFT policy πSFT. This differs from typical KL regularization terms like KL(πθπref ) or KL(πref πθ), which directly involve the current policy πθ that is being optimized. Additionally, this KL(t) term is incorporated into the advantage estimate Aq,ot before this advantage is used within the PPO-Clip objective structure. The PPO-Clip objective itself applies an importance weight wt(θ) = πθ(atst)/πold(atst) to the advantage estimate. This formulation means the KL(t) term acts as fixed reward shaping component based on πRL old and πSFT, rather than dynamic KL regularization of πθ (e.g., towards πSFT). Thus the regularization of πθ is indirect. If this term were intended as KL divergence involving πθ, its placement within (cid:98)At (subsequently multiplied by wt(θ)) would be non-standard for estimating the gradient of KL-regularized objective. Despite its name, REINFORCE++ uses the PPOClip objective (cf. (A.3)), deviating from the classic REINFORCE gradient estimator form (e.g., E[ (cid:98)Atθ log πθ(atst)] with loss like (A.2)). This implies an off-policy setup with importance sampling and clipping. 5 Table 1: Summary of fully differentiable surrogate loss functions L(θ) for policy gradient estimation. Minimizing L(θ) corresponds to maximizing the regularized objective J(θ) = Eπθ [R(x)] β Divergence, where πθ is the policy being optimized. Samples are drawn from last iterations old policy πold (or its normalized version (cid:101)πold). These losses yield the target gradient θJ(θ) directly via differentiation. Notation: w(x) = πθ(x)/πold(x) is the importance weight, R(x) is the reward, β is the regularization strength, πold is the old policy with total mass Zold, and (cid:101)πold = πold/Zold. Regularization Forward KL Reverse KL Unnormalized (E w.r.t. (cid:101)πold) Normalized (E w.r.t. πold) w(x)R(x) + β(cid:0)w(x) log w(x) 1(cid:1)(cid:105) (cid:104) (cid:104) ZoldE w(x)R(x) + β(cid:0)w(x) log w(x) w(x) + 1(cid:1)(cid:105) (cid:104) (cid:2)w(x)(cid:0)R(x) + β log w(x)(cid:1)(cid:3) ZoldE (cid:105) w(x)R(x) β log πθ(x)"
        },
        {
            "title": "3 Regularized Policy Gradients",
            "content": "We now derive policy gradient estimators for objectives regularized by KL divergence, assuming an online and off-policy setting where expectations are estimated using samples drawn from an old policy πold via importance sampling. We derive the corresponding surrogate loss functions suitable for gradient-based optimization and summarize them in Table 1. All the proofs are deferred to Appendix H."
        },
        {
            "title": "3.1 Forward KL Regularization",
            "content": "Consider the objective function with forward KL regularization: JFKL(θ) = Exπθ [R(x)] β KL(πold πθ), (3.1) (cid:104) (cid:105) where KL(πold πθ) = Exπold and β > 0 is the regularization parameter. Using importance sampling with importance weight w(x) = πθ(x)/πold(x), the gradient and the corresponding fully differentiable surrogate loss LFKL(θ) (minimized via gradient descent) are given by Theorem 3.1. log πold(x) πθ(x) Theorem 3.1 (Policy Gradient and Differentiable Loss for Forward KL). Consider the forward KL regularized objective function in (3.1). The gradient of JFKL(θ) with respect to θ is: θJFKL(θ) = Exπold (cid:2)(cid:0)w(x)R(x) + β(cid:1)θ log πθ(x)(cid:3) , where w(x) = πθ(x)/πold(x) is the importance weight. The corresponding surrogate loss function for gradient descent optimization is: LFKL(θ) = Exπold (cid:2) w(x)R(x) β log πθ(x)(cid:3), which satisfies θLFKL(θ) = θJFKL(θ). Remark 3.2 (Connection to Maximum Likelihood Estimation). If the reward R(x) = 0 for all x, maximizing JFKL(θ) reduces to minimizing β KL(πold πθ), which is equivalent to maximizing Exπold[log πθ(x)]. In this case, minimizing the corresponding loss LFKL(θ) = Exπold[β log πθ(x)] is equivalent to Maximum Likelihood Estimation (MLE) of the parameters θ using data sampled from πold, which is used as pretrain/SFT loss in RL methods such as InstructGPT (Ouyang et al., 2022) and VAPO (Yuan et al., 2025)."
        },
        {
            "title": "3.2 Unnormalized Forward KL Regularization\nIn scenarios where distributions might not be normalized (i.e., (cid:82)\nx π(x)dx ̸= 1), the standard KL\ndivergence might not fully capture the dissimilarity. The unnormalized forward KL divergence\naddresses this by adding a mass correction term. Let πold(x) be a potentially unnormalized reference\nmeasure with total mass Zold = (cid:82)\nx πold(x)dx. Let (cid:101)πold(x) = πold(x)/Zold be the corresponding\nnormalized probability distribution, such that (cid:82)",
            "content": "(cid:101)πold(x)dx = 1. Definition 3.3 (Unnormalized Forward KL). The unnormalized forward KL divergence (Minka et al., 2005; Zhu & Rohwer, 1995) between the measure πold and the density πθ is defined as: UKL(πoldπθ) = (cid:90) (cid:124) πold(x) log πold(x) πθ(x) (cid:90) (cid:124) dx + (cid:125) (cid:0)πθ(x) πold(x)(cid:1) dx (cid:125) (cid:123)(cid:122) Mass Correction . (cid:123)(cid:122) Generalized KL This form is particularly relevant when dealing with reference measures that may not be perfectly normalized or when connecting to certain KL estimators like k3 (see Remark 3.8). Consider the objective using UKL regularization as follows: JUFKL(θ) = Exπθ [R(x)] β UKL(πoldπθ). (3.2) To estimate this off-policy using samples from the normalized reference (cid:101)πold(x) = πold(x)/Zold, we define the importance weight w(x) = πθ(x)/πold(x) (using the unnormalized πold). The gradient and corresponding loss function, incorporating the total mass Zold of the reference measure, are given in Theorem 3.4. Theorem 3.4 (Policy Gradient and Differentiable Loss for Unnormalized Forward KL). Consider the unnormalized KL regularized objective function in (3.2). The gradient of JUFKL(θ) is: θJUFKL(θ) = ZoldEx(cid:101)πold (cid:104)(cid:16) w(x)R(x) β (w(x) 1) (cid:17) θ log πθ(x) (cid:105) . The corresponding surrogate loss for gradient descent optimization, estimated using samples {xi} (cid:101)πold, is: LUFKL(θ) = ZoldEx(cid:101)πold (cid:2)w(x)R(x) + β(cid:0)w(x) log w(x) 1(cid:1)(cid:3) , satisfying θLUFKL(θ) = θJUFKL(θ). Remark 3.5 (Interpretation of UFKL Loss and Gradient). The regularization component of the surrogate loss LUFKL(θ), specifically ZoldEx(cid:101)πold[β(w(x) log w(x) 1)], corresponds to an offpolicy estimate of the unnormalized forward KL divergence term β UKL(πoldπθ) present in the objective JUFKL(θ). This connection is established via the k3 estimator (see Remark 3.8 and Appendix B). Furthermore, the gradient term β(w(x) 1) effectively modifies the reward, guiding πθ to match not only the shape of πold but also its overall mass Zold, due to the mass correction component in UKL(πoldπθ)."
        },
        {
            "title": "3.3 Reverse KL Regularization",
            "content": "Now, consider the objective regularized by reverse KL divergence: JRKL(θ) = Exπθ [R(x)] β KL(πθ πold). (3.3) Again, we use importance sampling with w(x) = πθ(x)/πold(x), where KL(πθ πold) = Exπθ (cid:20) log (cid:21) . πθ(x) πold(x) This objective and its KL term can be rewritten entirely using expectations over πold. The resulting gradient and corresponding surrogate loss LRKL(θ) are given in Theorem 3.6. Theorem 3.6 (Policy Gradient and Differentiable Loss for Reverse KL). Consider the reverse KL regularized objective function in (3.3). The gradient of JRKL(θ) is: θJRKL(θ) = Exπold (cid:104) w(x) (cid:16) R(x) β(log w(x) + 1) (cid:17) θ log πθ(x) (cid:105) . corresponding surrogate loss function for gradient descent optimization is: LRKL(θ) = Exπold (cid:2)w(x)(cid:0)R(x) + β log w(x)(cid:1)(cid:3) , satisfying θLRKL(θ) = θJRKL(θ)."
        },
        {
            "title": "3.4 Unnormalized Reverse KL Regularization",
            "content": "Similar to the forward case, we can define an unnormalized reverse KL divergence, relaxing the normalization constraint on the reference distribution πold. Let πold(x) be potentially unnormalized reference measure with total mass Zold = (cid:82) πold(x)dx. Let (cid:101)πold(x) = πold(x)/Zold be the corresponding normalized probability distribution. Definition 3.7 (Unnormalized Reverse KL). The unnormalized reverse KL divergence between the density πθ and the measure πold is defined as: UKL(πθπold) = (cid:90) (cid:124) πθ(x) log πθ(x) πold(x) (cid:123)(cid:122) Generalized KL (cid:90) (cid:16) dx + (cid:124) (cid:125) πold(x) πθ(x) (cid:17) dx . (cid:123)(cid:122) Mass Correction (cid:125) The mass correction term simplifies to Zold (cid:82) πθ(x)dx. Remark 3.8. (Equivalence to k3 estimator) The k3 estimator (Schulman, 2020), often used for its empirical properties (e.g., in GRPO (Shao et al., 2024)), is defined for density ratio y(x) as: k3(y) := 1 log y. (3.4) As shown in Appendix B, this functional form directly relates to unnormalized KL divergences. For instance, KLk3(πθπold) := Exπθ [k3(πold(x)/πθ(x))] is equivalent to UKL(πθπold). This equivalence relationship justifies the exploration of UKL/URKL formulations within our framework. 8 Consider the objective using URKL: JURKL(θ) = Exπθ [R(x)] β UKL(πθπold), (3.5) where UKL is defined above. As with UFKL, we derive the gradient and loss using expectations over the normalized reference (cid:101)πold and the importance weight w(x) = πθ(x)/πold(x) (with unnormalized πold). The results are summarized in Theorem 3.9. Theorem 3.9 (Policy Gradient and Differentiable Loss for Unnormalized Reverse KL). Consider the reverse unnormalized KL regularized objective function in (3.5). The gradient of JURKL(θ) is: θJURKL(θ) = ZoldEx(cid:101)πold (cid:104) w(x) (cid:16) R(x) β log w(x) (cid:17) (cid:105) θ log πθ(x) . corresponding surrogate loss for gradient descent optimization, estimated using samples {xi} (cid:101)πold, is: LURKL(θ) = ZoldEx(cid:101)πold (cid:2)w(x)R(x) + β(cid:0)w(x) log w(x) w(x)(cid:1)(cid:3) , satisfying θLURKL(θ) = θJURKL(θ). The constant Zold scales the loss and gradient and may be omitted in practice. Remark 3.10 (URKL Loss and Mass Correction). The surrogate loss LURKL(θ) is designed such that its gradient is θJURKL(θ). Specifically, the term ZoldEx(cid:101)πold[β(w(x) log w(x) w(x))] in the loss directly relates to the off-policy estimation of the unnormalized reverse KL divergence β UKL(πθπold), omitting constant related to the total mass Zold which does not affect the gradient. The policy gradients effective reward scaling factor, (R(x) β log w(x)), is simpler than its normalized RKL counterpart."
        },
        {
            "title": "4 REINFORCE-Style Regularized Policy Gradients",
            "content": "In Section 3, we derived policy gradient estimators and corresponding fully differentiable surrogate losses L(θ) for KL-regularized objectives. Those losses were constructed such that θL(θ) = θJ(θ) directly, typically by setting L(θ) = JIS(θ) (where JIS is the importance-sampled objective) up to constants. Notice that the gradients derived in Section 3 (Theorems 3.1 through 3.9) share structural similarity with the REINFORCE estimator: θJ(θ) = Exπsampling [Weight(x, θ)θ log πθ(x)] where πsampling is πold or its normalized version (cid:101)πold, and Weight(x, θ) encapsulates the reward and KL regularization terms, differing for each specific objective. This structural similarity motivates an alternative REINFORCE-style implementation using the stop-gradient operator SG. The general form of such losses and the detailed rationale for how they yield the target gradient via automatic differentiation are presented in Appendix C.1 (see (C.1)). We explore these REINFORCE-style estimators as part of our framework, as they offer an alternative implementation path and demonstrate competitive empirical performance (Section 5). Proofs are in Appendix I. Table 2: Alternative surrogate loss functions L(θ) for estimation of regularized policy gradients, using the REINFORCE-style structure with the stop-gradient operator (SG). Minimizing L(θ) corresponds to maximizing the objective J(θ) = Eπθ [R(x)] β Divergence. Expectations are taken w.r.t. the sampling distribution (x πold or (cid:101)πold). These losses yield the target gradient via automatic differentiation due to the SG operator treating its argument as constant during backpropagation. Compare with the fully differentiable losses in Table 1. Here w(x) = πθ(x)/πold(x) is importance weight relative to πold. Zold is the mass of πold, R(x) is reward, β is regularization strength. Regularization Unnormalized (πsampling = (cid:101)πold) Forward KL Reverse KL (cid:104) Normalized (πsampling = πold) (cid:104) (cid:105) SG (w(x)R(x) + β) log πθ(x) SG (w(x) (R(x) β log w(x) β)) log πθ(x) (cid:104) (cid:105) SG (Zold(w(x)R(x) β(w(x) 1))) log πθ(x) (cid:105) (cid:104) SG (Zoldw(x)(R(x) β log w(x))) log πθ(x) (cid:105)"
        },
        {
            "title": "4.1 REINFORCE-Style RPG with Forward KL Regularization",
            "content": "We can convert Forward KL regularization of RPG to REINFORCE-style using the stop-gradient operator: Theorem 4.1 (REINFORCE-Style Loss for Forward KL). For the forward KL regularized objective function in (3.1), the corresponding REINFORCE-style surrogate loss function for gradient descent optimization via automatic differentiation is: LREINFORCE-style FKL (θ) = Exπold [SG (w(x)R(x) + β) log πθ(x)] , where w(x) = πθ(x)/πold(x). This loss aims to produce the gradient θJFKL(θ) via automatic differentiation. Remark 4.2. This REINFORCE-style loss requires SG to prevent backpropagation through w(x) in the weight term. Baselines can be added to R(x) inside SG for variance reduction (see Appendix D). We present the corresponding REINFORCE-style loss formulations for unnormalized forward KL, normalized reverse KL, and unnormalized reverse KL regularized objectives in Appendix C."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we empirically evaluate our proposed Regularized Policy Gradient (RPG) framework, including both its fully differentiable (RPG) and REINFORCE-style (RPG-REINFORCE) variants. We compare their performance against established baselines on challenging mathematical reasoning tasks using large language models, including GRPO (Shao et al., 2024), DAPO (Yu et al., 2025), REINFORCE++ (Hu, 2025), and its variant REINFORCE++-Baseline. Our evaluation focuses on task-specific accuracy, training stability, and key training dynamics such as reward, policy entropy, and response length. Base Models and Datasets. We conduct experiments using the Qwen2.5-7B-Instruct and Qwen2.5Math-7B large language models (Yang et al., 2024a,b). For training, we utilize the DAPO-Math-17k dataset (Yu et al., 2025), filtered to include only English samples, resulting in 13.9k sample training 10 (a) AMC23 (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 2: Training dynamics and benchmark performance for fully differentiable Regularized Policy Gradient (RPG) compared to baselines (GRPO, DAPO, REINFORCE++, REINFORCE++-Baseline). set. Model performance is evaluated on several mathematical reasoning benchmarks: AIME2024 (MAA, 2024a,b), AIME2025 (MAA, 2025a,b), and AMC23 (MAA, 2023). Implementation and Framework. Experiments are implemented using the verl framework (Sheng et al., 2025) with the vLLM engine (Kwon et al., 2023) for efficient LLM serving and inference. For practical implementation of our RPG methods, we emphasize that the probabilities (or logprobabilities) from the last iterations model (πold) for the sampled data can be pre-computed and stored. This allows the KL regularization terms to be calculated without needing to keep πold in GPU memory during the training step of the current policy πθ. Consequently, only one model (πθ) needs to be actively managed in GPU memory for training, which is faster and more memoryefficient compared to approaches like GRPO and REINFORCE++ that typically require access to at least two models (the current policy and reference/sampling policy) during optimization. Further details on the implementation, including specific hyperparameter settings (e.g., learning rate, KL coefficient), are provided in Appendix E. Stabilization and Advanced RL Techniques. Our RPG implementations (both fully differentiable and REINFORCE-style) incorporate stabilization techniques like baseline subtraction and PPOstyle objective clipping (specifically, Dual-Clip (Ye et al., 2020; Schulman et al., 2017)), crucial for robust off-policy learning. Detailed algorithmic descriptions are provided in Appendix (see Algorithm 1 for RPG with Dual-Clip and Algorithm 2 for the REINFORCE-style equivalent, along with Figures 4 and 5 for visualization). For PPO-style clipping, we set (ϵ1, ϵ2) = (0.2, 0.28) for RPG, DAPO, REINFORCE++, and REINFORCE++-Baseline. For RPG-REINFORCE and GRPO, we use (ϵ1, ϵ2) = (0.1, 0.1). This choice for RPG-REINFORCE is informed by ablation studies detailed in 11 Table 3: Combined performance metrics on the AMC23, AIME24, and AIME25 mathematical reasoning benchmarks, showing Last and Best scores. The Last score is from the 400th training step, assuming the training process remained stable to that point. The highest score in each column is bolded, and the second highest is underlined. RPG and RPG-REINFORCE methods are highlighted with light cyan and light green backgrounds, respectively. Method AMC23 AIME24 AIME25 Last Best Last Best Last Best GRPO REINFORCE++ REINFORCE++-Baseline DAPO RPG-FKL RPG-RKL RPG-UFKL RPG-URKL 0.6266 0.7625 0.8711 0.8039 0.8695 0.8648 0.8703 0."
        },
        {
            "title": "0.8727\nRPG-REINFORCE-FKL\n0.8305\nRPG-REINFORCE-RKL\nRPG-REINFORCE-UFKL\n0.8391\nRPG-REINFORCE-URKL 0.8531",
            "content": "0.7250 0.7664 0.8711 0.8734 0.8836 0.8672 0.8703 0.8641 0.8727 0.8516 0.8602 0.8531 0.1094 0.0521 0.0990 0.0760 0.1083 0.1167 0.0885 0.0875 0.1208 0.1125 0.1229 0. 0.1406 0.1177 0.1510 0.1240 0.1490 0.1469 0.1427 0.1271 0.1667 0.1375 0.1458 0.1500 0.0281 0.0302 0.0656 0.0531 0.0427 0.0677 0.0927 0.0677 0.0573 0.0490 0.0740 0. 0.0948 0.0740 0.0969 0.1063 0.1083 0.1240 0.1177 0.0917 0.0875 0.0875 0.0979 0.0938 Appendix F.9. Furthermore, to enhance training efficiency and data quality, we adopted techniques introduced by DAPO (Yu et al., 2025), including dynamic sampling strategy with group filtering mechanism (which oversamples challenging prompts and filters out those with near-perfect or near-zero accuracy based on initial rollouts) and an overlong punishment component in the reward shaping to discourage excessively verbose outputs. Investigation of Optimizers. We investigated AdamW (Loshchilov & Hutter, 2019) and ScheduleFree AdamW (Defazio et al., 2024). Schedule-Free optimizers aim to eliminate the need for explicit learning rate schedules after an initial warmup by maintaining constant learning rate, relying on internal model parameter averaging. This continuous averaging contrasts with schedulers like Warmup-Stable-Decay (WSD) (Hu et al., 2024), which typically involve explicit learning rate annealing. The inherent weight averaging in Schedule-Free methods can promote more stable training dynamics. Schedule-Free AdamW, which aims for stability without explicit learning rate schedules, improves performance on the AMC23 task, particularly for higher-variance algorithms such as GRPO and REINFORCE++. In the context of RL with iterative policy updates, this stability is particularly advantageous as it can lead to more consistent reference policy πold, potentially benefiting overall policy optimization. Detailed discussion and results are in Appendix (e.g., Figures 7, 9, 11, 13). Results and Analysis. Table 3 summarizes the performance of our RPG algorithms against baselines, reporting both the last and best scores achieved during training on these benchmarks. Figure 2 complements these results by illustrating the evaluation scores and training dynamics for the fully differentiable RPG variants and baselines when training the Qwen-2.5-7B-Instruct model with the AdamW optimizer. These figures display performance on the AMC23, AIME24, 12 (a) AMC (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 3: Performance of REINFORCE-Style Regularized Policy Gradient (RPG-REINFORCE) methods compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AMC23, AIME24, AIME25) and key training dynamics (reward, policy entropy, response length). and AIME25 benchmarks, alongside key training metrics: reward (critic score), policy entropy, and average response length. Additional results, including those with different base models and the Schedule-Free AdamW optimizer (as summarized in Section 5 and detailed in Appendix F), and ablation studies (Appendix F.9), are deferred to the appendix due to space constraints. The quantitative results in Table 3 demonstrate the strong performance of the proposed RPG framework. For instance, on AMC23, RPG-FKL achieves the best overall score (Best: 0.8836), while RPG-REINFORCE-FKL shows the top Last score (Last: 0.8727). Both significantly outperform GRPO (Last: 0.6266, Best: 0.7250) and REINFORCE++ (Last: 0.7625, Best: 0.7664). On AIME24, RPG-REINFORCE variants take the lead, with RPG-REINFORCE-FKL attaining the highest Best score (0.1667) and RPG-REINFORCE-UFKL the best Last score (0.1229). REINFORCE++-Baseline is also competitive on AIME24 (Best: 0.1510). For the AIME25 benchmark, RPG-RKL yields the top Best score (0.1240). Notably, RPG-REINFORCE-FKL records an exceptionally high Last score of 0.95729 on AIME25; this value stands out significantly, especially when compared to its own Best score (0.0875) and other results in the table. RPG-UFKL also shows strong Last score on AIME25 (0.0927). Overall, RPG and RPG-REINFORCE methods consistently rank at or near the top across the different metrics and benchmarks, often surpassing the baseline algorithms. Figure 2 further elucidates these findings for the fully differentiable RPG variants (RPG-FKL, RPG-RKL, RPG-UFKL, RPG-URKL). These algorithms generally exhibit stable training progressions regarding reward (critic score) and policy entropy, as shown in subfigures (d) and (e), compared to some baselines like GRPO, which can show more volatility. This stability likely contributes to 13 their robust benchmark performances (subfigures a-c). The response lengths (subfigure f) for RPG methods also appear well-controlled. These observations align with the strong final scores reported in Table 3 for these variants. Similarly, Figure 3 (further detailed in Appendix F) shows that RPG-REINFORCE formulations, particularly RPG-REINFORCE-FKL and RPG-REINFORCE-UFKL, also demonstrate robust performance, often competitive with or exceeding baselines, corroborating the strong results seen in Table 3. Their training curves generally indicate good stability and effective learning. The consistently high performance across various RPG formulations underscores the utility of the systematically derived KL-regularized objectives explored in this work."
        },
        {
            "title": "6 Related Work",
            "content": "Fine-tuning large language models (LLMs) using human feedback has become critical step in developing capable and aligned AI systems. Broadly, methods fall into two main categories: those relying on policy optimization using an explicit reward model learned from feedback, and those directly optimizing policies based on preference data. RLHF via Policy Optimization. The classic RLHF involves training reward model (RM) rϕ(x, y) to predict human preferences and then using reinforcement learning to optimize the language model policy πθ to maximize the expected reward from the RM, often regularizing against deviating too far from an initial reference policy πref . This approach was pioneered by Christiano et al. (2017) and gained widespread prominence with its application to LLMs like InstructGPT (Ouyang et al., 2022) and ChatGPT (OpenAI, 2022), which utilized Proximal Policy Optimization (PPO) (Schulman et al., 2017). PPO became workhorse due to its relative stability, achieved by constraining policy updates via clipped surrogate objective. The standard PPO setup for RLHF involves the policy πθ, value function Vψ, the RM rϕ, the reference policy πref . RLHF via Direct Preference Optimization. An alternative and increasingly popular approach bypasses explicit reward modeling by directly optimizing the policy πθ based on preference data, typically pairwise comparisons (yw, yl) indicating that response yw is preferred over yl for given prompt x. Inspired by the Bradley-Terry model (Bradley & Terry, 1952), Direct Preference Optimization (DPO) (Rafailov et al., 2023) derived simple loss function directly relating preference probabilities to policy likelihoods under πθ and reference policy πref . DPO maximizes the relative likelihood of preferred responses using logistic loss: LDPO E[log σ(βlogp)], where logp is the difference in log-probabilities of yw and yl between πθ and πref . DPOs simplicity and effectiveness led to its wide adoption in models like Llama-3 (Grattafiori et al., 2024), Qwen2 (Yang et al., 2024a), and Phi-3 (Abdin et al., 2024). Numerous variants have followed: SLiC-HF (Zhao et al., 2023) uses pairwise hinge loss for calibration; IPO (Azar et al., 2024) uses an identity link function; SimPO (Meng et al., 2024) offers simpler objective focusing on the margin; KTO (Ethayarajh et al., 2024) handles binary (good/bad) feedback; DQO (Ji et al., 2024) incorporates direct Q-value modeling; RAFT (Dong et al., 2023), RSO (Liu et al., 2024) and RFT (Yuan et al., 2023) use rejection sampling perspective. Recognizing that preferences might evolve, iterative methods like Iterative DPO (Xiong et al., 2024), PCO (Xu et al., 2023) and SPIN (Chen et al., 2024) alternate between generation/preference learning and policy updates, often using the current policys outputs in self-improvement loop. Game theory offers another lens, with Nash Learning from Human Feedback (NLHF) (Munos et al., 2024) framing RLHF as finding Nash equilibrium between policies. Self-play ideas appear in SPPO (Wu et al., 2025) and GPO (Zhang et al., 2025), where the 14 policy generates pairs for comparison. Methods like GPM (Zhang et al., 2025) aim to handle more general preference structures efficiently using latent embeddings beyond pairwise comparisons. RL for Enhancing LLM Reasoning. Beyond general alignment with human preferences, RL techniques are increasingly explored to specifically enhance the multi-step reasoning capabilities In these of LLMs in domains like mathematics, coding, and complex instruction following. contexts, RL optimizes the policy to generate sequences (e.g., chain-of-thought, code blocks) that lead to successful outcomes, often using rewards derived from external feedback like unit test results, execution outcomes, or correctness checks by an automated judge or specialized reward model trained on reasoning quality. For instance, the DeepSeekMath model (Shao et al., 2024) employed the GRPO algorithm, value-free PPO variant, demonstrating significant improvements in mathematical problem-solving benchmarks through RL fine-tuning. DeepSeek-R1 (Guo et al., 2025) represents efforts in applying advanced techniques potentially involving RL for complex tasks, although specific methods might vary. Furthermore, preference-based methods like SPPO and GPO have been applied to reasoning-specialized models such as Kimi-1.5 (Team et al., 2025), and the resulting improvements observed on benchmarks involving coding and math suggest that preference-based RLHF can also contribute to refining reasoning abilities, potentially by optimizing implicit properties related to logical consistency and correctness within the preference data. The need for value function (critic model) used in PPO incurs significant computational costs, and standard PPO can face stability challenges with sparse rewards common in LLM tasks. Addressing these issues has driven recent work. Several methods aim to improve efficiency by removing the value network: ReMax (Li et al., 2024) adapts REINFORCE (Williams, 1992) using Monte Carlo returns and normalization; GRPO (Shao et al., 2024) uses group-average reward baseline and adds k3-based KL penalty to the objective; and VinePPO (Kazemnejad et al., 2024) uses MC sampling from intermediate steps. Other approaches focus on stability and alternative baselines, such as RLOO (Ahmadian et al., 2024), which uses leave-one-out statistics within group, and REINFORCE++ (Hu, 2025), which enhances REINFORCE with token-level KL penalties (using the k2 estimator) and normalization. Dr. GRPO (Liu et al., 2025) identifies and corrects bias found in GRPOs advantage estimators, DAPO (Yu et al., 2025) introduces strategies like Clip-Higher, reward over-sampling, and token-level loss to handle long sequences and entropy collapse, while VAPO (Yuan et al., 2025) builds upon it with length-adaptive advantage estimation."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced RPG, framework for deriving and analyzing KL-regularized policy gradient algorithms for online, off-policy RL. We provided derivations for policy gradients and surrogate loss functions covering forward/reverse KL, normalized/unnormalized distributions, and both fully differentiable and REINFORCE-style estimators. Our experiments on LLM reasoning tasks demonstrate that methods from this framework can achieve stable training and improved or competitive performance against strong baselines."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: 15 highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1224812267, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 42994307, 2017. Aaron Defazio, Xingyu Yang, Ahmed Khaled, Konstantin Mishchenko, Harsh Mehta, and Ashok Cutkosky. The road less scheduled. Advances in Neural Information Processing Systems, 37:9974 10007, 2024. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. RAFT: reward ranked finetuning for generative foundation model alignment. Trans. Mach. Learn. Res., 2023, 2023. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. 16 Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, and Lin Yan. Enhancing multi-step reasoning abilities of language models through direct q-function optimization. arXiv preprint arXiv:2410.09302, 2024. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Mathematical Association of Americas American Mathematics Competitions MAA."
        },
        {
            "title": "2023\nAMC, 2023. URL https://artofproblemsolving.com/wiki/index.php/AMC_12_\nProblems_and_Solutions.",
            "content": "Mathematical Association of Americas American Mathematics Competitions MAA. 2024 AIMEI, 2024a. URL https://artofproblemsolving.com/wiki/index.php/2024_AIME_I. Accessed: 2025-05-08. Mathematical Association of Americas American Mathematics Competitions MAA. 2024 AIMEII, 2024b. URL https://artofproblemsolving.com/wiki/index.php/2024_AIME_II. Accessed: 2025-05-08. Mathematical Association of Americas American Mathematics Competitions MAA. 2025 AIME-I, 2025a. URL https://artofproblemsolving.com/wiki/index.php/2025_AIME_I. Mathematical Association of Americas American Mathematics Competitions MAA. 2025 AIME-II, 2025b. URL https://artofproblemsolving.com/wiki/index.php/2025_AIME_II. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. 17 Tom Minka et al. Divergence measures and message passing. Technical report, Microsoft Research, 2005. Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Côme Fiegel, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash learning from human feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. OpenAI. ChatGPT, 2022. URL https://chat.openai.com/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730 27744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. John Schulman. Approximating kl divergence. http://joschu.net/blog/kl-approx.html, March 2020. Accessed on May 26, 2025. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. 18 Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025, 2025. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: bridging theory and practice for rlhf under klconstraint. In Proceedings of the 41st International Conference on Machine Learning, pp. 5471554754, 2024. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 18, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024b. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 66726679, 2020. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, and Quanquan Gu. Beyond bradley-terry models: general preference model for language model alignment. In Proceedings of the 42nd International Conference on Machine Learning, 2025. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Huaiyu Zhu and Richard Rohwer. Information geometric measurements of generalisation. Preprint, 1995."
        },
        {
            "title": "Appendix",
            "content": "A REINFORCE and Proximal Policy Optimization (PPO) . 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.1 REINFORCE . A.2 Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 . . . . . . . . Equivalence of k3 Estimator and Unnormalized KL Divergence REINFORCE-Style Regularized Policy Gradients with Various KL Regularization Forms 23 C.1 Rationale for REINFORCE-Style Loss Formulation . . . . . . . . . . . . . . . . . . . . 23 C.2 REINFORCE-Style RPG with Unnormalized Forward KL Regularization . . . . . . . 24 C.3 REINFORCE-Style RPG with Reverse KL Regularization . . . . . . . . . . . . . . . . 24 C.4 REINFORCE-Style RPG with Unnormalized Reverse KL Regularization . . . . . . ."
        },
        {
            "title": "E Detailed Experimental Setup",
            "content": ""
        },
        {
            "title": "32\nF.1 Regularized Policy Gradient using Qwen-2.5-7B-Instruct and AdamW Optimizer . . 32\nF.2 Regularized Policy Gradient using Qwen-2.5-7B-Instruct and Schedule-Free Optimizer 32\nF.3 Regularized Policy Gradient using Qwen-2.5-Math-7B and AdamW Optimizer . . . 34\nF.4 Regularized Policy Gradient using Qwen-2.5-Math-7B and Schedule-Free Optimizer 34\nF.5 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen-\n. . . . . . . . . . . . . . . . . . . . . . . . .\nF.6 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen-",
            "content": "2.5-7B-Instruct and AdamW Optimizer . 36 2.5-7B-Instruct and Schedule-Free Optimizer . . . . . . . . . . . . . . . . . . . . . . . 36 F.7 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-Math-7B and AdamW Optimizer . . . . . . . . . . . . . . . . . . . . . . . . . . . F.8 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-Math-7B and Schedule-Free Optimizer . . . . . . . . . . . . . . . . . . . . . . . . F.9 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen- . . . . . . . . . . . . . . . . . . . . . . . . . F.10 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen2.5-7B-Instruct and AdamW Optimizer . . 38 . 40 2.5-7B-Instruct and Schedule-Free Optimizer . . . . . . . . . . . . . . . . . . . . . . . 40 F.11 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen2.5-Math-7B and AdamW Optimizer. . . . . . . . . . . . . . . . . . . . . . . . . . . . F.12 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen2.5-Math-7B and Schedule-Free Optimizer . . . . . . . . . . . . . . . . . . . . . . . . Proofs of Theorem 2.1 (Generalized Policy Gradient Theorem) . . 42 44 20 Proofs for Regularized Policy Gradients 44 H.1 Proof of Theorem 3.1 (Policy Gradient and Differentiable Loss for Forward KL) . . . 44 H.2 Proof of Theorem 3.4 (Policy Gradient and Differentiable Loss for Unnormalized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Forward KL) . . . . . . . . H.3 Proof of Theorem 3.6 (Policy Gradient and Differentiable Loss for Reverse KL) H.4 Proof of Theorem 3.9 (Policy Gradient and Differentiable Loss for Unnormalized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Reverse KL) . . . . . . . . . . 45 . . . 47 Proofs for REINFORCE-Style Regularized Policy Gradients 50 Proof of Theorem 4.1 (REINFORCE-style Policy Gradient for Forward KL) . . . . . . 50 I.1 I.2 Proof of Theorem C.1 ((REINFORCE-style Policy Gradient for Unnormalized For- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 ward KL) . Proof of Theorem C.2 (REINFORCE-Style Loss) . . . . . . . . . . . . . . . . . . . . . . 52 Proof of Theorem C.3 (REINFORCE-Style Loss for Unnormalized Reverse KL) . . . . 52 . 53 Summary of REINFORCE-style Algorithms . . . . . . . . . . . . . . . . . . . . . . . . I.3 I.4 I.5 21 REINFORCE and Proximal Policy Optimization (PPO) A.1 REINFORCE REINFORCE performs Monte Carlo (MC) updates after sampling complete trajectory, using the sampled return Gt as an unbiased estimate of the state-action value function Qπθ (st, at). However, these MC estimates often exhibit high variance, leading to slow and unstable learning. To reduce variance, state-dependent baseline b(st) (commonly an estimate of the state value function, πθ (st)) is subtracted from the return-to-go: θJ(θ) = Eτ πθ (cid:34) (cid:88) t=0 (Gt b(st))θ log πθ(atst) = Eτ πθ (cid:35) (cid:35) (cid:98)Atθ log πθ(atst) . (A.1) (cid:34) (cid:88) t=0 Here, (cid:98)At = Gt b(st) is an estimate of the advantage function Aπθ (st, at) = Qπθ (st, at) πθ (st). Subtracting baseline that only depends on the state st does not bias the gradient estimate, (cid:80) since Eatπθ(st)[b(st)θ log πθ(atst)] = b(st)θ πθ(atst) = b(st)θ1 = 0. REINFORCE with baseline is typically implemented by minimizing the loss: at LREINFORCE(θ) = Eτ πθ (cid:34) (cid:88) t=0 SG( (cid:98)At) log πθ(atst) , (A.2) (cid:35) using the stop-gradient operator SG() to prevent gradients from flowing into the advantage estimate (cid:98)At. As REINFORCE uses samples collected under the current policy πθ for gradient estimation, it is an on-policy algorithm. A.2 Proximal Policy Optimization (PPO) On-policy methods like REINFORCE can be sample-inefficient, requiring new trajectories for each gradient update. Proximal Policy Optimization (PPO) (Schulman et al., 2017) improves stability and sample efficiency by enabling multiple updates using the same batch of data collected under slightly older policy πθold. This makes PPO effectively off-policy. PPO achieves this by optimizing surrogate objective function that discourages large deviations between the current policy πθ and the old policy πθold. The most widely used variant, PPO-Clip, employs clipped objective: PPO-Clip(θ) = Et (cid:104) min (cid:16) wt(θ) (cid:98)At, clip(wt(θ), 1 ϵ, 1 + ϵ) (cid:98)At (cid:17)(cid:105) , (A.3) where the expectation Et is taken over timesteps in the collected batch sampled from πold. Here, wt(θ) = πθ(atst) πold(atst) is the importance sampling ratio. (cid:98)At is an advantage estimate, typically computed using Generalized Advantage Estimation (GAE) (Schulman et al., 2016), which leverages observed rewards and learned state-value function (s) to reduce variance. Notably, in many practical implementations, especially in Reinforcement Learning from Human Feedback (RLHF) for large language models (Ouyang et al., 2022), KL divergence penalty against reference policy πref (e.g., the initial supervised model) is often incorporated implicitly by modifying the reward signal before calculating the advantage. For example, the reward used for GAE calculation might become is used within GAE to compute (cid:98)At, the KL penalty term is effectively folded into the advantage estimate that multiplies = rt β log(πθ(atst)/πref (atst)). When this the importance weight wt(θ) in the objective function. This approach contrasts with adding the KL penalty as separate term to the final objective, as seen in GRPO (Section 2.2) or the formal derivations in Section 3. The hyperparameter ϵ (e.g., 0.2) defines the clipping range [1 ϵ, 1 + ϵ] for the importance ratio wt(θ). This clipping limits the influence of potentially noisy importance weights when the policy changes significantly, preventing destructive updates and further stabilizing the off-policy training. PPO optimizes the policy πθ by maximizing PPO-Clip(θ). Equivalence of k3 Estimator and Unnormalized KL Divergence As mentioned in Section 3.4, the k3 estimator for KL divergence (Schulman, 2020) is equivalent to the unnormalized KL (UKL) divergence. The k3 function is defined as k3(y) = 1 log y. Forward KL-k3 and UKL(πoldπθ): The forward KL-k3 divergence is KLk3(πoldπθ) := Exπold[k3(πθ(x)/πold(x))]. Exπold (cid:19)(cid:21) (cid:20) k3 (cid:18) πθ(x) πold(x) = Exπold (cid:90) πold(x) (cid:20) πθ(x) πold(x) (cid:18) πθ(x) πold(x) πθ(x) πold(x) (cid:90) (cid:21) πold(x) log πθ(x) πold(x) dx (cid:19) 1 dx 1 log (πθ(x) πold(x))dx + (cid:90) πold(x) log πold(x) πθ(x) dx = (cid:90) = = UKL(πoldπθ). Reverse KL-k3 and UKL(πθπold): The reverse KL-k3 divergence is KLk3(πθπold) := Exπθ [k3(πold(x)/πθ(x))]. Exπθ (cid:20) k3 (cid:18) πold(x) πθ(x) (cid:19)(cid:21) = Exπθ (cid:90) πθ(x) (cid:20) πold(x) πθ(x) (cid:18) πold(x) πθ(x) 1 log (cid:19) 1 (cid:21) πold(x) πθ(x) (cid:90) (πold(x) πθ(x))dx + = (cid:90) = πold(x) πθ(x) dx dx πθ(x) log πθ(x) log (cid:90) πθ(x) πold(x) dx = UKL(πθπold). REINFORCE-Style Regularized Policy Gradients with Various KL"
        },
        {
            "title": "Regularization Forms",
            "content": "C.1 Rationale for REINFORCE-Style Loss Formulation As noted in Section 4 of the main text, the derived off-policy policy gradients (Theorems 3.1 through 3.9) share structural similarity with the REINFORCE estimator: θJ(θ) = Exπsampling [Weight(x, θ)θ log πθ(x)] . 23 This structure suggests an alternative way to implement the gradient update, analogous to the REINFORCE-style approach used in the on-policy setting. Specifically, one could define surrogate loss of the form: LREINFORCE-style(θ) = Exπsampling [SG (Weight(x, θ)) log πθ(x)] . (C.1) The rationale is that applying automatic differentiation to this loss should yield: θLREINFORCE-style(θ) Autodiff= Exπsampling [SG (Weight(x, θ)) θ log πθ(x)] . When this gradient is used for optimization, the stop-gradient SG is conceptually removed, resulting in an update aligned with θJ(θ). This relies on SG preventing gradients from flowing through the θ-dependence within Weight(x, θ) (specifically, the dependence via the importance weight w(x)). The following subsections detail these REINFORCE-style loss formulations for each KL regularization type. C.2 REINFORCE-Style RPG with Unnormalized Forward KL Regularization Similarly, we can also transform the Unnormalized Forward KL Regularization of RPG into REINFORCE-style as follows: Theorem C.1 (REINFORCE-Style Loss for Unnormalized Forward KL). For the objective JUFKL(θ) = Eπθ [R(x)] β UKL(πoldπθ), whose gradient (sampling from (cid:101)πold) is θJUFKL(θ) = Ex(cid:101)πold[Zold(w(x)R(x) β(w(x) 1))θ log πθ(x)] (Theorem 3.4), corresponding REINFORCE-style surrogate loss is: LREINFORCE-style UFKL (θ) = Ex(cid:101)πold [SG (Zold (w(x)R(x) β(w(x) 1))) log πθ(x)] , where (cid:101)πold = πold/Zold and w(x) = πθ(x)/πold(x) (using unnormalized πold). This loss aims to produce the gradient θJUFKL(θ) via automatic differentiation. C.3 REINFORCE-Style RPG with Reverse KL Regularization Theorem C.2 (REINFORCE-Style Loss for Reverse KL). For the objective JRKL(θ) = Eπθ [R(x)] β KL(πθ πold), whose gradient is θJRKL(θ) = Exπold[w(x)(R(x) β(log w(x) + 1))θ log πθ(x)] (Theorem 3.6), corresponding REINFORCE-style surrogate loss is: LREINFORCE-style RKL (θ) = Exπold [SG (w(x) (R(x) β log w(x) β)) log πθ(x)] , (C.2) where w(x) = πθ(x)/πold(x). This loss aims to produce the gradient θJRKL(θ) via automatic differentiation. C.4 REINFORCE-Style RPG with Unnormalized Reverse KL Regularization Theorem C.3 (REINFORCE-Style Loss for Unnormalized Reverse KL). For the objective JURKL(θ) = Eπθ [R(x)] β UKL(πθπold), whose gradient (sampling from (cid:101)πold) is 24 θJURKL(θ) = Ex(cid:101)πold[Zoldw(x)(R(x) β log w(x))θ log πθ(x)] (Theorem 3.9), corresponding REINFORCE-style surrogate loss is: LREINFORCE-style URKL (θ) = Ex(cid:101)πold [SG (Zoldw(x) (R(x) β log w(x))) log πθ(x)] , where (cid:101)πold = πold/Zold and w(x) = πθ(x)/πold(x) (using unnormalized πold). This loss aims to produce the gradient θJURKL(θ) via automatic differentiation."
        },
        {
            "title": "D More on Algorithmic Details",
            "content": "D.1 Stabilization Techniques for Regularized Policy Gradients Practical implementations of off-policy policy gradient methods often require stabilization techniques to manage variance or prevent destructively large policy updates. Common techniques include: Dual-Clip Objective: This method adapts the clipping mechanism from PPO, with modification for negative advantages proposed by Ye et al. (2020), to stabilize updates (Schulman et al., 2017). The Dual Clip objective aims to maximize DualClip = Exπold[LDualClip(x, θ)], where (cid:98)A(x) is an estimate of the advantage analogue (e.g., R(x) or the full term derived from the regularized gradient), w(x) = πθ(x)/πold(x) is the importance ratio, and LDualClip(x, θ) is defined as: If (cid:98)A(x) 0: LDualClip(x, θ) = min(w(x) (cid:98)A(x), clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)A(x)). If (cid:98)A(x) < 0: LDualClip(x, θ) = max(min(w(x) (cid:98)A(x), clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)A(x)), (cid:98)A(x)). where ϵ1, ϵ2 > 0 are clipping parameters and > 1 provides lower bound for negative advantages. To use this with gradient descent (which minimizes loss L), we minimize the negative of the Dual Clip objective term. Using min(a, b) = max(a, b) and max(a, b) = min(a, b), the corresponding loss term for single sample is: (cid:16) If (cid:98)A(x) 0: LDualClip(x, θ) = max w(x) (cid:98)A(x), clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)A(x) (cid:17) . (cid:16) If (cid:98)A(x) < 0: Let Lclip = max w(x) (cid:98)A(x), clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)A(x) (cid:17) . Then, LDualClip(x, θ) = min Lclip, (cid:98)A(x) (cid:16) (cid:17) . Here, (cid:98)A(x) should represent the advantage or an analogous term derived from the gradient of the original (non-negated) regularized objective (e.g., Theorem 3.6). The overall loss is L(θ) = Exπold[LDualClip(x, θ)]. This loss function is differentiable with respect to θ (which appears in w(x) and potentially (cid:98)A(x) if it includes terms like log w(x)). This loss formulation ensures that updates are conservative. For positive advantages, it acts like standard PPO-Clip. For negative advantages, it prevents the objective from becoming arbitrarily large (loss becoming arbitrarily small) by introducing the lower bound (cid:98)A(x) on the objective (upper bound (cid:98)A(x) on the loss). Baseline Subtraction: Used to define the advantage (cid:98)A(x) = R(x) b(x), reducing the variance of the gradient estimates. The baseline b(x) should ideally not depend strongly on θ. common 25 (cid:80) R(xi). The choice is value function estimate (x) or simply the batch average reward = 1 definition of (cid:98)A(x) might also incorporate regularization terms depending on the base objective chosen (see RKL example below). For instance, applying Dual Clip to stabilize the reverse KL objective (Theorem 3.6). The log πθ. Using this (cid:98)ARKL in the gradient involves the term w(x) (cid:0)(R(x) b) β(log w(x) + 1)(cid:1) (cid:125) (cid:124) (cid:123)(cid:122) Analogue to (cid:98)ARKL(x,w;b) Dual Clip loss structure LDualClip If (cid:98)ARKL(x, w; b) 0: RKL (θ) = Exπold[LDualClip RKL (x, θ)] where: LDualClip RKL (x, θ) = max w(x) (cid:98)ARKL, clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)ARKL (cid:32) (cid:32) (cid:33) . (cid:33) If (cid:98)ARKL(x, w; b) < 0: Let Lclip = max w(x) (cid:98)ARKL, clip(w(x), 1 ϵ1, 1 + ϵ2) (cid:98)ARKL ."
        },
        {
            "title": "LDualClip\nRKL",
            "content": "(x, θ) = min (cid:16) Lclip, (cid:98)ARKL (cid:17) , where (cid:98)ARKL(x, w; b) = (R(x) b) β(log w(x) + 1). Simpler approximations might use (cid:98)A(x) = R(x) b. Using PPO-style clipping alters the optimization objective compared to the original KL-regularized objectives, trading strict adherence for enhanced stability. The choice of base objective structure, definition of (cid:98)A, and stabilization techniques depends on the specific application. D.2 Stabilization Techniques for REINFORCE-Style Regularized Policy Gradients While the REINFORCE-style losses derived in this section  (Table 2)  provide theoretically grounded gradient estimators for the regularized objectives, practical implementations often benefit significantly from stabilization techniques common in policy gradient methods. These techniques aim to reduce variance and control the magnitude of policy updates, which is especially crucial in the off-policy setting where importance weights w(x) and can exacerbate instability. Baseline Subtraction and Regularized Advantage Definition: This is standard variance reduction technique. Critically, when combining with stabilization like PPO clipping in this REINFORCE-style context, the term playing the role of the advantage ( (cid:98)At) that gets clipped should ideally incorporate not just the baselined reward but also the regularization terms derived from the objectives gradient. Recall the REINFORCE-style gradient structure θJ(θ) = Exπsampling[Weight(x, θ)θ log πθ(x)]. The PPO objective involves terms like wt (cid:98)At. To align these, we define the regularized advantage (cid:98)At such that wt (cid:98)At approximates the key part of Weight(x, θ). For example: For RKL (Theorem C.2), WeightRKL = w(x)(R(x) β(log w(x) + 1)). We define the regularized advantage as (cid:98)ARKL = (R(x) b(x)) β(log w(x) + 1). For URKL (Theorem C.3), WeightURKL = Zoldw(x)(R(x) β log w(x)). Ignoring Zold, we define (cid:98)AURKL = (R(x) b(x)) β log w(x). Algorithm 1 RPG with Dual-Clip Stabilization Require: Reference policy πold, Reward function R(x), Initial policy parameters θ0 Require: Base objective structure Jchosen (implies regularization type), Regularization strength β 0 Require: Learning rate α > 0, Batch size > 0, Number of epochs 1 per iteration Require: Dual Clip parameters: ϵ1 > 0, ϵ2 > 0, > 1 Require: Baseline method (e.g., batch/group average, value function Vϕ) i=1 πold 1: Initialize policy parameters θ θ0 2: Initialize value function parameters ϕ (if baseline uses Vϕ) 3: for each training iteration do Sample batch = {xi}N 4: Compute Ri for = 1..N 5: Compute baselines bi for = 1..N (e.g., bi = 1 6: for = 1 to do 7: 8: 9: 10: Initialize batch loss Lbatch = 0 for = 1 to do wi = πθ(xi) Define Advantage analogue (cid:98)Ai based on Jchosen, Ri, bi, wi, β. πold(xi) , log wi = log πθ(xi) log πold(xi) (cid:80) Collect data using old policy Rj or bi = Vϕ(xi)) Multiple optimization epochs on the same batch Compute importance weight Ex: For RKL, (cid:98)Ai = (Ri bi) β(log wi + 1). Note: (cid:98)Ai depends on current θ via wi if Dual Clip enabled then Negative of unclipped term, gradient flows through wi loss_term1i = wi (cid:98)Ai wi,clipped = clip(wi, 1 ϵ1, 1 + ϵ2) loss_term2i = wi,clipped (cid:98)Ai Lclip(i) = max(loss_term1i, loss_term2i) if (cid:98)Ai 0 then Lterm(i) = Lclip(i) else loss_lower_boundi = (cid:98)Ai Lterm(i) = min(Lclip(i), loss_lower_boundi) Negative of clipped term (cid:98)Ai < 0 Lower bound term end if else structure Define base loss term (unclipped) based on chosen objectives negative gradient Ex: For RKL loss (no clip): Lterm(i) = wi((Ri bi) + β log wi) Lterm(i) = wi (cid:98)Ai end if Lbatch = Lbatch + Lterm(i) Compute final batch loss for minimization Compute gradient (flows through wi and (cid:98)Ai) Update policy parameters Update value function parameters ϕ (e.g., by minimizing E[(Vϕ(xi) Ri)2] over the batch) Lbatch end for (cid:98)L(θ) = 1 θ (cid:98)L(θ) θ OptimizerUpdate(θ, g, α) if using learned baseline Vϕ then 32: 33: 34: 35: 36: 37: 38: end for 39: return Optimized policy parameters θ end for end if 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: For FKL or UFKL, the structure might not cleanly separate into w(x) (. . . ). In such cases, Case (cid:98)A(x) 0 (e.g., (cid:98)A = 1) Case (cid:98)A(x) < 0 (e.g., (cid:98)A = 1) 0 0.5 1 1. ) θ , ( C D e o 0.5 1 1 ϵ 1 2 ϵ + 1 ) θ , ( i a m s Grad via Grad = 0 (w.r.t w) 1.5 2.5 2.5 2 1.5 Grad = 0 (w.r.t w) Grad via 1.5 2 2.5 0. 0.5 1 1 ϵ 1 2 ϵ + 1 w(x) = πθ(x)/πold(x) w(x) = πθ(x)/πold(x) Figure 4: Visualization of the Dual-Clip loss term LDualClip(x, θ) vs. importance weight w(x), as described in Section D.1 and Algorithm 1. This formulation is typically implemented as fully differentiable w.r.t θ (via w(x) and potentially (cid:98)A(x) if (cid:98)A depends on θ, e.g., via log w(x)), unlike REINFORCE-style implementations that use SG( (cid:98)A) or SG(ℓi) within the loss. For visualization, (cid:98)A(x) is treated as constant ( (cid:98)A = 1 left, (cid:98)A = 1 right) to isolate the effect of w. Solid blue: Loss depends linearly on w, gradient θL flows via w(x). Dotted magenta: Loss is constant w.r.t w, gradient θL does not flow via w(x) in this segment (though it might flow via (cid:98)A if (cid:98)A depends on θ). Left: Case (cid:98)A < 0. Right: Case (cid:98)A 0. common simplification is to use (cid:98)At = R(x) b(x) and accept that the clipping primarily stabilizes the reward terms contribution. This calculated (cid:98)At (incorporating reward, baseline, and KL terms) is then treated as constant using the stop-gradient operator, SG( (cid:98)At), when plugged into the clipping loss function. PPO-Style Objective Clipping (Dual-Clip Variant): PPO (Schulman et al., 2017) introduces objective clipping to limit the impact of large importance ratios w(x). The Dual-Clip variant (Ye et al., 2020) refines this, particularly for negative advantages, using lower bound parameter > 1. When applied in the REINFORCE-style setting, the PPO Dual-Clip objective aims to maximize (simplified notation, expectation over πold): DualClip(θ) = Et[LDualClip (θ)] where (cid:98)At is the regularized advantage defined above (incorporating Rt, bt, and KL terms), wt(θ) = πθ(atst) πold(atst) , and LDualClip (θ) is defined based on the sign of SG( (cid:98)At): If SG( (cid:98)At) 0: LDualClip If SG( (cid:98)At) < 0: LDualClip t (θ) = min(wt(θ) SG( (cid:98)At), clip(wt(θ), 1 ϵ1, 1 + ϵ2) SG( (cid:98)At)) (θ) = max(min(wt(θ) SG( (cid:98)At), clip(wt(θ), 1ϵ1, 1+ϵ2) SG( (cid:98)At)), SG( (cid:98)At)) Here, ϵ1, ϵ2 are clipping hyperparameters, and is the lower bound factor. Note that θ influences this objective only through wt(θ), as (cid:98)At is detached via SG. To implement this using gradient descent (minimizing loss), we minimize the negative of 28 the PPO Dual-Clip objective. The loss function becomes LDualClip(θ) = Et[LDualClip LDualClip (θ) = LDualClip If SG( (cid:98)At) 0: LDualClip If SG( (cid:98)At) < 0: Let Lclip = max(wt(θ) SG( (cid:98)At), clip(wt(θ), 1 ϵ1, 1 + ϵ2) SG( (cid:98)At)). Then, (θ) = max(wt(θ) SG( (cid:98)At), clip(wt(θ), 1 ϵ1, 1 + ϵ2) SG( (cid:98)At)). (θ). Explicitly: (θ)], where t LDualClip (θ) = min(Lclip, SG( (cid:98)At)). This PPO Dual-Clip loss function LDualClip(θ) replaces the simpler REINFORCE-style losses derived earlier (like LREINFORCE-style in (C.2)). The gradient θLDualClip(θ) is computed via automatic differentiation, where the gradient flows through wt(θ) but is stopped at (cid:98)At. This approach uses the PPO objective structure with the appropriately defined regularized advantage for stabilization in an off-policy REINFORCE-style update. Algorithm 2 details this implementation. RKL"
        },
        {
            "title": "E Detailed Experimental Setup",
            "content": "Hyperparameters. Unless otherwise specified, all experiments use learning rate of 1 106 with weight decay of 0.1 and gradient clipping at 1.0. Training proceeds for 400 steps, including an initial 10 warm-up steps, after which constant learning rate is maintained. The global training batch size is 512. For each sample in the batch, we roll out 16 responses using temperature of 1.0. The per-GPU mini-batch size is 32, and experiments are conducted on 8 NVIDIA H100 GPUs. The maximum training and rollout length is set to 16,384 tokens, with dynamic batching enabled. The KL regularization coefficient β is set to 1 104. Specific Clipping Parameters and Adopted Techniques. As mentioned in Section 5, for PPOstyle clipping, we set (ϵ1, ϵ2) = (0.2, 0.28) for RPG, DAPO, REINFORCE++, and REINFORCE++- Baseline. For RPG-REINFORCE and GRPO, we use (ϵ1, ϵ2) = (0.1, 0.1). This choice for RPGREINFORCE is informed by ablation studies detailed in Appendix F.9. These studies show that while (ϵ1, ϵ2) = (0.1, 0.1) provides stable performance, larger clip parameters like (0.2, 0.28) can lead to instability for RPG-REINFORCE variants, particularly with extensively pre-trained models like Qwen-2.5-Math-7B (as observed in Figures 12 and 13 in Appendix F.9). This suggests that such models may benefit from tighter clipping to encourage exploitation. Optimizer Details. As mentioned in Section 5, we investigated the impact of different optimizers, comparing the widely-used AdamW (Loshchilov & Hutter, 2019) with the more recent ScheduleFree AdamW (Defazio et al., 2024). Schedule-Free optimizers aim to eliminate the need for explicit learning rate schedules after an initial warmup by maintaining constant learning rate, relying on internal model parameter averaging. This continuous averaging contrasts with schedulers like Warmup-Stable-Decay (WSD) (Hu et al., 2024), which typically involve explicit learning rate annealing. Our empirical results, detailed in Appendix (e.g., Figures 7, 9, 11, 13), confirm the benefits of Schedule-Free AdamW. 29 Case ψi 0 Case ψi < 0 1.5 t c fi c L Grad via ℓi Grad = 0 1.5 2 n fi c L Grad = 0 Grad via ℓi 0.5 0.5 1 1 ϵ 1 2 ϵ + 1.5 2 2.5 2.5 0. 1 1 ϵ 1 2 ϵ + 1 1.5 2 2.5 wi = πθ(xi)/πold(xi) wi = πθ(xi)/πold(xi) Figure 5: Visualization of the loss coefficient Li vs. importance weight wi based on the specific implementation in Algorithm 2. This version swaps the main branching condition compared to previous versions (branches on ψi > 0). The plot assumes ℓi = log πθ(xi) = 1 for visualizing the value of Li. The line styles indicate the nature of the gradient θLi: Solid blue: Gradient exists, flowing only via ℓi. The coefficient multiplying θℓi depends on SG(wi). Dotted magenta: Gradient is zero. This occurs when ℓi is detached via SG in the loss calculation. Left: Case ψi 0. Right: Case ψi < 0. 30 Algorithm 2 REINFORCE-Style RPG with Dual-Clip Stabilization Require: Reference policy πold, Reward function R(x), Initial policy parameters θ0 Require: KL Component function Compute_KL_Component(x, θ, πold), KL Component Coefficient β Require: Learning rate α > 0, Batch size > 0, Number of epochs 1 per iteration Require: Dual Clip parameters: ϵ1 > 0 (low), ϵ2 > 0 (high), > 1 Require: Baseline method (e.g., batch average, value function Vϕ) Rj or bi = Vϕ(xi)) Multiple optimization epochs on the same batch high = AR,i + SG(CKL,i)/ SG(whigh) 1: Initialize policy parameters θ θ0 2: Initialize value function parameters ϕ (if baseline uses Vϕ) 3: for each training iteration do Sample batch = {xi}N i=1 πold 4: Compute rewards Ri for = 1..N 5: Compute baselines bi for = 1..N (e.g., bi = 1 6: for = 1 to do 7: 8: 9: 10: Initialize batch loss Lbatch = 0 for = 1 to do wi = πθ(xi) πold(xi) ℓi = log πθ(xi) AR,i = Ri bi CKL,i = β Compute_KL_Component(xi, θ, πold(xi)) ψi = if ψi 0 then = AR,i + SG(CKL,i)/ SG(wi) (cid:80) low = AR,i + SG(CKL,i)/ SG(wlow) else else end if ℓi Li = ψi SG(wi) wlow = 1 ϵ1 if wi wlow then whigh = 1 + ϵ2 if wi < whigh then ψhigh = high SG(ℓi) Li = ψhigh SG(whigh) 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: end for 47: return Optimized policy parameters θ ψlow = low SG(ℓi) Li = ψlow SG(wlow) end for L(θ) = 1 θL(θ) θ OptimizerUpdate(θ, g, α) if using learned baseline Vϕ then end if Lbatch = Lbatch + Li else if wi < then Li = ψi SG(wi) Lbatch end for end if end if else Update value function parameters ϕ 31 Li = AR,i SG(ℓi) + SG(CKL,i) SG(ℓi) Importance weight Negative log probability Baseline-subtracted reward KL component Effective advantage Branching term Grad exists wi whigh ψi 0 Grad exists wi Compute average batch loss Compute gradient Update policy parameters"
        },
        {
            "title": "F More Experimental Results",
            "content": "F.1 Regularized Policy Gradient using Qwen-2.5-7B-Instruct and AdamW Optimizer (a) AMC23 (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 6: Performance of fully differentiable Regularized Policy Gradient (RPG) methods compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AMC23, AIME24, AIME25) and key training dynamics (reward, policy entropy, response length). Base model: Qwen2.5-7B-Instruct. Optimizer: AdamW. F.2 Regularized Policy Gradient using Qwen-2.5-7B-Instruct and Schedule-Free Optimizer 32 (a) AMC (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 7: Performance of fully differentiable Regularized Policy Gradient (RPG) methods compared to baselines. Base model: Qwen-2.5-7B-Instruct. Optimizer: Schedule-Free AdamW. 33 F.3 Regularized Policy Gradient using Qwen-2.5-Math-7B and AdamW Optimizer (a) AIME24 (b) AMC23 (c) MATH500 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 8: Performance of fully differentiable Regularized Policy Gradient (RPG) methods compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AIME24, AMC23, MATH500) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-Math-7B. Optimizer: AdamW. F.4 Regularized Policy Gradient using Qwen-2.5-Math-7B and Schedule-Free Optimizer (a) AIME24 (b) AMC23 (c) MATH500 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 9: Performance of fully differentiable Regularized Policy Gradient (RPG) methods compared to baselines. Base model: Qwen-2.5-Math-7B. Optimizer: Schedule-Free AdamW. 35 F.5 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-7B-Instruct and AdamW Optimizer (a) AMC23 (b) AIME (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 10: Performance of REINFORCE-Style Regularized Policy Gradient (RPG-REINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.1, 0.1) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AMC23, AIME24, AIME25) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-7B-Instruct. Optimizer: AdamW. F.6 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-7B-Instruct and Schedule-Free Optimizer 36 (a) AMC23 (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 11: Performance of REINFORCE-Style Regularized Policy Gradient (RPG-REINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.1, 0.1) compared to baselines. Base model: Qwen-2.57B-Instruct. Optimizer: Schedule-Free AdamW. 37 F.7 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-Math-7B and AdamW Optimizer (a) AIME24 (b) AMC23 (c) MATH500 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 12: Performance of REINFORCE-Style Regularized Policy Gradient (RPG-REINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.1, 0.1) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AIME24, AMC23, MATH500) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-Math-7B. Optimizer: AdamW. F.8 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.1, ϵ2 = 0.1 using Qwen2.5-Math-7B and Schedule-Free Optimizer 38 (a) AIME24 (b) AMC (c) MATH500 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 13: Performance of REINFORCE-Style Regularized Policy Gradient (RPG-REINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.1, 0.1) compared to baselines. Base model: Qwen-2.5Math-7B. Optimizer: Schedule-Free AdamW. F.9 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen-2.5-7B-Instruct and AdamW Optimizer Ablation Studies. We implement the ablation study on different clip parameters of RPG-REINFORCE algorithms, including (0.1, 0.1) and (0.2, 0.28). And the results are displayed in Figures 10-17. From Figures 12 and 13 of the experiments with (ϵ1, ϵ2) = (0.2, 0.28) using Qwen-2.5-Math-7B models in, we observe collapses in RPG-REINFORCE variants. possible reason for this phenomenon is that the Qwen-2.5-Math-7B has been well pre-trained and fine-tuned on mathematical data (Yang et al., 2024a), for which the encouragement of exploitation and suppression of exploration in parameter space are needed. Therefore, smaller clip parameters demonstrate stabler training curves and better performances. (a) AMC23 (b) AIME24 (c) AIME (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 14: Ablation study: Performance of REINFORCE-Style Regularized Policy Gradient (RPGREINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.2, 0.28) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AMC23, AIME24, AIME25) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-7B-Instruct. Optimizer: AdamW. F.10 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen-2.5-7B-Instruct and Schedule-Free Optimizer 40 (a) AMC23 (b) AIME24 (c) AIME25 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 15: Ablation study: Performance of REINFORCE-Style Regularized Policy Gradient (RPGREINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.2, 0.28) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AMC23, AIME24, AIME25) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-7B-Instruct. Optimizer: Schedule-Free AdamW. 41 F.11 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen-2.5-Math-7B and AdamW Optimizer. (a) AIME (b) AMC23 (c) MATH500 (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 16: Ablation study: Performance of REINFORCE-Style Regularized Policy Gradient (RPGREINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.2, 0.28) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AIME24, AMC23, MATH500) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-Math-7B. Optimizer: AdamW. F.12 REINFORCE-Style Regularized Policy Gradient with ϵ1 = 0.2, ϵ2 = 0.28 using Qwen-2.5-Math-7B and Schedule-Free Optimizer 42 (a) AIME24 (b) AMC23 (c) MATH (d) Reward (Critic Score) (e) Entropy (f) Response Length Figure 17: Ablation study: Performance of REINFORCE-Style Regularized Policy Gradient (RPGREINFORCE) methods with clip parameters (ϵ1, ϵ2) = (0.2, 0.28) compared to baselines. Plots display accuracy on mathematical reasoning benchmarks (AIME24, AMC23, MATH500) and key training dynamics (reward, policy entropy, response length). Base model: Qwen-2.5-Math-7B. Optimizer: Schedule-Free AdamW. 43 Proofs of Theorem 2.1 (Generalized Policy Gradient Theorem) Proof. The proof relies on the log-derivative trick, θπθ(x) = πθ(x)θ log πθ(x), and the product rule under the integral sign: (cid:90) θExπθ [f (x, θ)] = θ (cid:90) πθ(x)f (x, θ)dx = θ(πθ(x)f (x, θ))dx (Swap , (cid:90) ) (cid:90) (cid:90) (cid:90) = = = ((θπθ(x))f (x, θ) + πθ(x)(θf (x, θ))) dx (πθ(x)(θ log πθ(x))f (x, θ) + πθ(x)(θf (x, θ))) dx (Log-derivative) πθ(x) (f (x, θ)θ log πθ(x) + θf (x, θ)) dx = Exπθ [f (x, θ)θ log πθ(x) + θf (x, θ)] ."
        },
        {
            "title": "H Proofs for Regularized Policy Gradients",
            "content": "This section provides detailed proofs for the theorems presented in Section 3, demonstrating that the gradients of the proposed fully differentiable off-policy surrogate losses correspond to the negative gradients of the respective original objectives. The core tool used is the policy gradient theorem: θExπθ [f (x, θ)] = Exπθ [f (x, θ)θ log πθ(x) + θf (x, θ)]. We use the notation w(x) = πθ(x)/πold(x) for the importance weight. H.1 Proof of Theorem 3.1 (Policy Gradient and Differentiable Loss for Forward KL) Proof. We start by rewriting the objective function JFKL(θ) using expectations with respect to the fixed reference policy πold. The first term, the expected reward under πθ, can be rewritten using importance sampling: Exπθ [R(x)] = (cid:90) πθ(x)R(x)dx = (cid:90) πθ(x) πold(x) πold(x)R(x)dx = Exπold[w(x)R(x)]. The second term is the forward KL divergence: KL(πold πθ) = Exπold (cid:20) log (cid:21) πold(x) πθ(x) = Exπold[log πold(x) log πθ(x)] = Exπold[ log πθ(x)] + Exπold[log πold(x)]. Substituting these into the objective function: JFKL(θ) = Exπold[w(x)R(x)] β (Exπold[ log πθ(x)] + Exπold[log πold(x)]) = Exπold[w(x)R(x) + β log πθ(x)] βExπold[log πold(x)]. Since πold(x) does not depend on θ, the term βExπold[log πold(x)] is constant with respect to θ. Now we compute the gradient θJFKL(θ). Assuming we can swap gradient and expectation (standard assumption in policy gradient methods): θJFKL(θ) = θExπold [w(x)R(x) + β log πθ(x)] = Exπold [θ(w(x)R(x) + β log πθ(x))] = Exπold [(θw(x))R(x) + βθ log πθ(x)] . We use the identity for the gradient of the importance weight: θw(x) = θ (cid:19) (cid:18) πθ(x) πold(x) = = 1 πold(x) πθ(x) πold(x) θπθ(x) θπθ(x) πθ(x) = w(x)θ log πθ(x). Substituting this back into the gradient expression: θJFKL(θ) = Exπold [w(x)(θ log πθ(x))R(x) + βθ log πθ(x)] = Exπold (cid:2)(cid:0)w(x)R(x) + β(cid:1)θ log πθ(x)(cid:3) . This proves the first part of the theorem. Now, consider the surrogate loss function: LFKL(θ) = Exπold [w(x)R(x) β log πθ(x)] . We compute its gradient: θLFKL(θ) = θExπold [w(x)R(x) β log πθ(x)] = Exπold [θ(w(x)R(x) β log πθ(x))] = Exπold [(θw(x))R(x) βθ log πθ(x)] = Exπold [w(x)(θ log πθ(x))R(x) βθ log πθ(x)] (cid:2)(cid:0)w(x)R(x) + β(cid:1)θ log πθ(x)(cid:3) . = Exπold Comparing this with the gradient of the objective function, we see that θLFKL(θ) = θJFKL(θ). This confirms that minimizing LFKL(θ) corresponds to maximizing JFKL(θ) using gradient-based methods. H.2 Proof of Theorem 3.4 (Policy Gradient and Differentiable Loss for Unnormalized Forward KL) Proof. We start by expressing the components of JUFKL(θ) using expectations over the normalized reference distribution (cid:101)πold(x) = πold(x)/Zold. The importance weight is w(x) = πθ(x)/πold(x), which implies πθ(x) = w(x)πold(x) = w(x)Zold(cid:101)πold(x). 45 The expected reward term: Exπθ [R(x)] = = (cid:90) (cid:90) πθ(x)R(x)dx = (cid:90) w(x)πold(x)R(x)dx w(x)Zold(cid:101)πold(x)R(x)dx = ZoldEx(cid:101)πold[w(x)R(x)]. The unnormalized KL divergence term UKL(πoldπθ) has two parts. Part 1 (Generalized KL): (cid:90) πold(x) log πold(x) πθ(x) (cid:90) dx = Zold(cid:101)πold(x) log dx = ZoldEx(cid:101)πold (cid:20) log = ZoldEx(cid:101)πold [ log w(x)] . πold(x) πθ(x) (cid:21) 1 w(x) Part 2 (Mass Correction): (cid:90) (πθ(x) πold(x))dx = = (cid:90) (cid:90) (w(x)πold(x) πold(x))dx (w(x) 1)πold(x)dx = (cid:90) (w(x) 1)Zold(cid:101)πold(x)dx = ZoldEx(cid:101)πold[w(x) 1] = ZoldEx(cid:101)πold[w(x)] Zold. Combining these parts for the UKL term: UKL(πoldπθ) = ZoldEx(cid:101)πold [ log w(x)] + ZoldEx(cid:101)πold[w(x)] Zold. Now, substitute everything into the objective JUFKL(θ): JUFKL(θ) = ZoldEx(cid:101)πold[w(x)R(x)] β (cid:0)ZoldEx(cid:101)πold[ log w(x)] + ZoldEx(cid:101)πold[w(x)] Zold = ZoldEx(cid:101)πold [w(x)R(x) + β log w(x) βw(x) + β] . (cid:1) To compute the gradient θJUFKL(θ), we differentiate the terms inside the expectation. The constant term βZold (arising from β inside the expectation) vanishes upon differentiation. θJUFKL(θ) = θ (cid:0)ZoldEx(cid:101)πold [w(x)R(x) + β log w(x) βw(x)](cid:1) = ZoldEx(cid:101)πold [θ(w(x)R(x)) + βθ(log w(x)) βθ(w(x))] . We need the gradients of w(x) and log w(x): θw(x) = w(x)θ log πθ(x) (as derived in Theorem 3.1 proof) θ log w(x) = θ(log πθ(x) log πold(x)) = θ log πθ(x). Substituting these into the gradient expression: θJUFKL(θ) = ZoldEx(cid:101)πold [(θw(x))R(x) + βθ log πθ(x) β(θw(x))] = ZoldEx(cid:101)πold [w(x)R(x)θ log πθ(x) + βθ log πθ(x) βw(x)θ log πθ(x)] = ZoldEx(cid:101)πold [(w(x)R(x) βw(x) + β) θ log πθ(x)] = ZoldEx(cid:101)πold w(x)R(x) β (w(x) 1) (cid:105) θ log πθ(x) (cid:104)(cid:16) (cid:17) . 46 This proves the first part of the theorem. Now, consider the surrogate loss function: LUFKL(θ) = ZoldEx(cid:101)πold (cid:2)w(x)R(x) + β(cid:0)w(x) log w(x) 1(cid:1)(cid:3) . We compute its gradient: θLUFKL(θ) = ZoldEx(cid:101)πold [θ(w(x)R(x)) + βθ(w(x) log w(x) 1)] = ZoldEx(cid:101)πold [(θw(x))R(x) + β(θw(x) θ log w(x))] = ZoldEx(cid:101)πold [w(x)R(x)θ log πθ(x) + β(w(x)θ log πθ(x) θ log πθ(x))] = ZoldEx(cid:101)πold = ZoldEx(cid:101)πold w(x)R(x) + βw(x) β (cid:104)(cid:16) (cid:105) θ log πθ(x) (cid:17) w(x)R(x) β(w(x) 1) θ log πθ(x) (cid:104)(cid:16) (cid:17) (cid:105) . Comparing this with the gradient of the objective function, we find θLUFKL(θ) = θJUFKL(θ). This confirms the surrogate loss function. Note that the constant 1 inside the logarithm term in the loss LUFKL corresponds to the constant βZold in the objective JUFKL and does not affect the gradient. H.3 Proof of Theorem 3.6 (Policy Gradient and Differentiable Loss for Reverse KL) Proof. We rewrite the objective function JRKL(θ) using expectations with respect to πold. The expected reward term is Exπθ [R(x)] = Exπold[w(x)R(x)], as shown previously. The reverse KL divergence term is: KL(πθ πold) = Exπθ (cid:20) log (cid:21) πθ(x) πold(x) = Exπθ [log w(x)] (cid:90) = = πθ(x) log w(x)dx (cid:90) πθ(x) πold(x) πold(x) log w(x)dx = Exπold[w(x) log w(x)]. Substituting these into the objective function: JRKL(θ) = Exπold[w(x)R(x)] βExπold[w(x) log w(x)] = Exπold[w(x)R(x) βw(x) log w(x)]. Now we compute the gradient θJRKL(θ): θJRKL(θ) = θExπold [w(x)R(x) βw(x) log w(x)] = Exπold [θ(w(x)R(x)) βθ(w(x) log w(x))] . We need the gradient of w(x) log w(x): θ(w(x) log w(x)) = (θw(x)) log w(x) + w(x)θ(log w(x)) = (w(x)θ log πθ(x)) log w(x) + w(x)(θ log πθ(x)) = w(x)θ log πθ(x)(log w(x) + 1). 47 Substituting this and θw(x) = w(x)θ log πθ(x) into the gradient expression for JRKL(θ): θJRKL(θ) = Exπold [(θw(x))R(x) βw(x)θ log πθ(x)(log w(x) + 1)] = Exπold [w(x)(θ log πθ(x))R(x) βw(x)(log w(x) + 1)θ log πθ(x)] = Exπold (cid:105) θ log πθ(x) R(x) β(log w(x) + 1) w(x) (cid:16) (cid:17) (cid:104) . This proves the first part of the theorem. Now, consider the surrogate loss function: LRKL(θ) = Exπold (cid:2)w(x)(cid:0)R(x) + β log w(x)(cid:1)(cid:3) . We compute its gradient: θLRKL(θ) = θExπold [w(x)R(x) + βw(x) log w(x)] = Exπold [θ(w(x)R(x)) + βθ(w(x) log w(x))] = Exπold [(θw(x))R(x) + βw(x)θ log πθ(x)(log w(x) + 1)] = Exπold [w(x)(θ log πθ(x))R(x) + βw(x)(log w(x) + 1)θ log πθ(x)] = Exπold θ log πθ(x) (cid:16) (cid:17) (cid:105) (cid:104) w(x) (cid:104) R(x) + β(log w(x) + 1) (cid:16) R(x) β(log w(x) + 1) = Exπold w(x) (cid:17) θ log πθ(x) (cid:105) . Comparing this with the gradient of the objective function, we confirm that θLRKL(θ) = θJRKL(θ). H.4 Proof of Theorem 3.9 (Policy Gradient and Differentiable Loss for Unnormalized Reverse KL) Proof. We again express the objective components using expectations over the normalized reference distribution (cid:101)πold(x) = πold(x)/Zold, with w(x) = πθ(x)/πold(x). The expected reward term: Exπθ [R(x)] = ZoldEx(cid:101)πold[w(x)R(x)]. The unnormalized reverse KL divergence UKL(πθπold) has two parts. Part 1 (Generalized KL): (cid:90) πθ(x) log πθ(x) πold(x) dx = = = (cid:90) (cid:90) (cid:90) πθ(x) log w(x)dx w(x)πold(x) log w(x)dx w(x)Zold(cid:101)πold(x) log w(x)dx = ZoldEx(cid:101)πold[w(x) log w(x)]. 48 Part 2 (Mass Correction): (cid:90) (πold(x) πθ(x))dx = (cid:90) (cid:90) πθ(x)dx πold(x)dx (cid:90) = Zold w(x)πold(x)dx (cid:90) w(x)Zold(cid:101)πold(x)dx = Zold = Zold ZoldEx(cid:101)πold[w(x)]. Combining these for the UKL term: UKL(πθπold) = ZoldEx(cid:101)πold[w(x) log w(x)] + Zold ZoldEx(cid:101)πold[w(x)]. Now, substitute into the objective JURKL(θ): JURKL(θ) = ZoldEx(cid:101)πold[w(x)R(x)] β (cid:0)ZoldEx(cid:101)πold[w(x) log w(x)] + Zold ZoldEx(cid:101)πold[w(x)](cid:1) = ZoldEx(cid:101)πold [w(x)R(x) βw(x) log w(x) β + βw(x)] . We compute the gradient θJURKL(θ). The constant term βZold vanishes upon differentiation. θJURKL(θ) = θ (cid:0)ZoldEx(cid:101)πold [w(x)R(x) βw(x) log w(x) + βw(x)](cid:1) = ZoldEx(cid:101)πold [θ(w(x)R(x)) βθ(w(x) log w(x)) + βθw(x)] . Using the previously derived gradients θw(x) = w(x)θ log πθ(x) and θ(w(x) log w(x)) = w(x)θ log πθ(x)(log w(x) + 1): θJURKL(θ) = θ (cid:0)ZoldEx(cid:101)πold [w(x)R(x) βw(x) log w(x) + βw(x)](cid:1) = ZoldEx(cid:101)πold [θ(w(x)R(x)) βθ(w(x) log w(x)) + βθw(x)] = ZoldEx(cid:101)πold [(θw(x))R(x) βw(x)θ log πθ(x)(log w(x) + 1) + β(θw(x))] = ZoldEx(cid:101)πold [w(x)R(x)θ log πθ(x) βw(x)(log w(x) + 1)θ log πθ(x) +βw(x)θ log πθ(x)] (cid:104) = ZoldEx(cid:101)πold = ZoldEx(cid:101)πold (cid:104) (cid:34) = ZoldEx(cid:101)πold w(x) w(x)θ log πθ(x) (cid:16) R(x) β(log w(x) + 1) + β (cid:17)(cid:105) w(x)θ log πθ(x) (cid:16) R(x) β log w(x) (cid:17)(cid:105) (cid:16) R(x) β log w(x) (cid:17) (cid:35) θ log πθ(x) . This proves the first part of the theorem. Now, consider the surrogate loss function: LURKL(θ) = ZoldEx(cid:101)πold (cid:2)w(x)R(x) + β(cid:0)w(x) log w(x) w(x)(cid:1)(cid:3) . 49 We compute its gradient: θLURKL(θ) = ZoldEx(cid:101)πold [θ(w(x)R(x)) + βθ(w(x) log w(x) w(x))] = ZoldEx(cid:101)πold [(θw(x))R(x) + β(θ(w(x) log w(x)) θw(x))] = ZoldEx(cid:101)πold [w(x)R(x)θ log πθ(x) +β(cid:0)w(x)(log w(x) + 1)θ log πθ(x) w(x)θ log πθ(x)(cid:1)(cid:3) = ZoldEx(cid:101)πold [w(x)R(x)θ log πθ(x) + βw(x) log w(x)θ log πθ(x)] = ZoldEx(cid:101)πold R(x) + β log w(x) (cid:16) (cid:17) (cid:105) (cid:104) w(x) (cid:34) = ZoldEx(cid:101)πold (cid:16) R(x) β log w(x) (cid:17) w(x) θ log πθ(x) (cid:35) . θ log πθ(x) Comparing this with the gradient of the objective function, we confirm that θLURKL(θ) = θJURKL(θ). The constant term +1 (corresponding to βZold in the objective) that appeared in the derivation in Section 3.4 does not affect the gradient and is often omitted from the final loss expression used in practice. Proofs for REINFORCE-Style Regularized Policy Gradients This section provides justifications for the REINFORCE-style surrogate loss functions presented in Section 4 (Theorems 4.1 to C.3). These proofs demonstrate how automatic differentiation applied to the proposed losses, utilizing the stop-gradient operator SG, yields the correct gradient direction (negative of the objective gradient derived in Section 3). The core idea relies on the operational definition of the stop-gradient operator SG() within automatic differentiation frameworks: θ SG(f (θ)) = 0, while the forward computation uses the value of (θ). We use the notation w(x) = πθ(x)/πold(x). I.1 Proof of Theorem 4.1 (REINFORCE-style Policy Gradient for Forward KL) Proof. The objective is JFKL(θ) = Eπθ [R(x)] β KL(πold πθ). From Theorem 3.1, its gradient is: θJFKL(θ) = Exπold (cid:0)w(x)R(x) + β(cid:1) (cid:125) (cid:123)(cid:122) (cid:124) WeightFKL(x,θ) θ log πθ(x) . The proposed REINFORCE-style surrogate loss is: LREINFORCE-style FKL (θ) = Exπold [SG (w(x)R(x) + β) log πθ(x)] . 50 We compute the gradient of this loss as it would be computed by an automatic differentiation system. Assuming the gradient can be swapped with the expectation: θLREINFORCE-style FKL (θ) = Exπold [θ (SG (w(x)R(x) + β) log πθ(x))] = Exπold (θ SG (w(x)R(x) + β)) (cid:124) (cid:125) (cid:123)(cid:122) =0 by definition of SG log πθ(x) + SG (w(x)R(x) + β) (θ log πθ(x))] = Exπold [SG (w(x)R(x) + β) θ log πθ(x)] . This gradient expression, when used in an optimization algorithm (where SG is conceptually removed), corresponds to applying updates proportional to: (Exπold [(w(x)R(x) + β) θ log πθ(x)]) = θJFKL(θ). Thus, minimizing LREINFORCE-style tively performs gradient ascent on the original objective JFKL(θ). FKL (θ) using gradient descent with automatic differentiation effecI.2 Proof of Theorem C.1 ((REINFORCE-style Policy Gradient for Unnormalized Forward KL) Proof. The objective is JUFKL(θ) = Eπθ [R(x)] β UKL(πoldπθ). From Theorem 3.4, its gradient is: θJUFKL(θ) = Ex(cid:101)πold (cid:16) Zold (cid:124) w(x)R(x) β (w(x) 1) (cid:17) (cid:123)(cid:122) WeightUFKL(x,θ) (cid:125) θ log πθ(x) . The proposed REINFORCE-style surrogate loss is: LREINFORCE-style UFKL (θ) = Ex(cid:101)πold [SG (Zold (w(x)R(x) β(w(x) 1))) log πθ(x)] . Computing the gradient via automatic differentiation: θLREINFORCE-style UFKL (θ) = Ex(cid:101)πold [θ (SG (Zold(. . . )) log πθ(x))] = Ex(cid:101)πold (θ SG(Zold(. . . ))) (cid:125) (cid:123)(cid:122) =0 = Ex(cid:101)πold [SG (Zold (w(x)R(x) β(w(x) 1))) θ log πθ(x)] . log πθ(x) + SG(Zold(. . . ))(θ log πθ(x)) (cid:124) This gradient corresponds to the update direction θJUFKL(θ) when the SG is dropped. Minimizing this loss achieves gradient ascent on JUFKL(θ). If Zold is omitted, the same argument applies to the proportionally scaled objective and loss. 51 I.3 Proof of Theorem C.2 (REINFORCE-Style Loss) Proof. The objective is JRKL(θ) = Eπθ [R(x)] β KL(πθ πold). From Theorem 3.6, its gradient is: θJRKL(θ) = Exπold (cid:16) w(x) (cid:124) R(x) β(log w(x) + 1) (cid:123)(cid:122) WeightRKL(x,θ) θ log πθ(x) . (cid:17) (cid:125) The proposed REINFORCE-style surrogate loss is: LREINFORCE-style RKL (θ) = Exπold [SG (w(x) (R(x) β log w(x) β)) log πθ(x)] . Computing the gradient via automatic differentiation: θLREINFORCE-style (θ) = Exπold [θ (SG (w(x)(. . . )) log πθ(x))] RKL = Exπold (θ SG(w(x)(. . . ))) (cid:123)(cid:122) (cid:125) =0 (cid:124) log πθ(x) + SG(w(x)(. . . ))(θ log πθ(x)) = Exπold [SG (w(x) (R(x) β log w(x) β)) θ log πθ(x)] . This gradient corresponds to the update direction θJRKL(θ) when the SG is dropped. Minimizing this loss achieves gradient ascent on JRKL(θ). I.4 Proof of Theorem C.3 (REINFORCE-Style Loss for Unnormalized Reverse KL) Proof. The objective is JURKL(θ) = Eπθ [R(x)] β UKL(πθπold). From Theorem 3.9, its gradient is: θJURKL(θ) = Ex(cid:101)πold Zoldw(x) (cid:124) (cid:16) R(x) β log w(x) (cid:123)(cid:122) WeightURKL(x,θ) θ log πθ(x) . (cid:17) (cid:125) The proposed REINFORCE-style surrogate loss is: LREINFORCE-style URKL (θ) = Ex(cid:101)πold [SG (Zoldw(x) (R(x) β log w(x))) log πθ(x)] . Computing the gradient via automatic differentiation: θLREINFORCE-style URKL (θ) = Ex(cid:101)πold [θ (SG (Zoldw(x)(. . . )) log πθ(x))] (cid:34) = Ex(cid:101)πold (θ SG(Zoldw(x)(. . . ))) (cid:125) (cid:123)(cid:122) (cid:124) =0 log πθ(x) (cid:35) + SG(Zoldw(x)(. . . ))(θ log πθ(x)) = Ex(cid:101)πold [SG (Zoldw(x) (R(x) β log w(x))) θ log πθ(x)] . This gradient corresponds to the update direction θJURKL(θ) when the SG is dropped. Minimizing this loss achieves gradient ascent on JURKL(θ). If Zold is omitted, the same argument applies to the proportionally scaled objective and loss. 52 I.5 Summary of REINFORCE-style Algorithms We have presented an alternative REINFORCE-style approach to formulating surrogate losses for off-policy regularized policy gradients. This approach leverages the structural similarity of the derived off-policy gradients to the REINFORCE estimator, explicitly using the stop-gradient operator SG within the loss function: LREINFORCE-style(θ) = Exπsampling [SG (Weight(x, θ)) log πθ(x)] , where Weight(x, θ) encapsulates the reward and regularization terms specific to each objective (FKL, UFKL, RKL, URKL). While this formulation provides conceptual link to on-policy REINFORCE methods and might be convenient in some frameworks, it differs significantly from the direct differentiable-style losses in Section 3  (Table 1)  . Those direct losses yield the correct gradients θL(θ) = θJ(θ) by construction (often L(θ) = JIS(θ) up to constants), without requiring SG. The REINFORCE-style off-policy losses rely critically on the stop-gradient SG preventing gradient flow through the θ-dependence within the Weight(x, θ) term (primarily through the importance weight w(x)). Although automatic differentiation libraries are designed for this, the direct loss formulations avoid this reliance and represent more direct pathway from the objective J(θ) to suitable loss L(θ) in the differentiable off-policy setting. Therefore, while the REINFORCE-style losses are presented here for completeness and to highlight the gradient structure, the direct losses from Section 3 are generally considered the standard and more straightforward approach for optimizing these KL-regularized objectives in the off-policy manner described. Regardless of the chosen loss formulation, practical implementations necessitate Monte Carlo estimation using samples from πold (or (cid:101)πold) and benefit significantly from variance reduction techniques (e.g., baseline subtraction applied to R(x)) and stabilization methods (e.g., importance weight clipping, as discussed in Section D.2)."
        }
    ],
    "affiliations": [
        "IIIS, Tsinghua University",
        "Shanghai Qi Zhi Institute",
        "University of California, Los Angeles"
    ]
}