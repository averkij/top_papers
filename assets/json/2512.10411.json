{
    "paper_title": "Sliding Window Attention Adaptation",
    "authors": [
        "Yijiong Yu",
        "Jiale Liu",
        "Qingyun Wu",
        "Huazheng Wang",
        "Ji Pei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation"
        },
        {
            "title": "Start",
            "content": "Yijiong Yua, Jiale Liub, Qingyun Wub, Huazheng Wanga, and Ji Peic aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu bPenn State University, {jiale.liu, qingyun.wu}@psu.edu cDeepSolution, research@deepsolution.chat 5 2 0 2 1 1 ] . [ 1 1 1 4 0 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The self-attention mechanism in Transformerbased Large Language Models (LLMs) scales quadratically with input length, making longSliding wincontext inference expensive. dow attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe longcontext performance degradation due to traininginference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), set of practical recipes that combine five methods for better adaptation: (i) applying SWA only during prefilling; (ii) preserving sink tokens; (iii) interleaving FA/SWA layers; (iv) chain-of-thought (CoT); and (v) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at github."
        },
        {
            "title": "Introduction",
            "content": "Transformer-based Large Language Models (LLMs) (Vaswani et al., 2017) demonstrate remarkable capabilities, but their self-attention scales quadratically with the input sequence length, making long context processing inefficient. Sliding the most straightWindow Attention (SWA), forward and widely adopted sparse attention pattern, which restricts each tokens attention to fixed-size local window, reduces the computational complexity to linearity, along with some other benefits (see Appendix A). To apply SWA to LLMs, typical solutions involve training model with SWA from scratch, but 1 are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention models like Qwen3 (Team, 2025b), mainly due to the inability to reproduce pretraining data. Training-free methods like streaming attention (Xiao et al., 2024) can stabilize LLM outputs by retaining sink tokens while applying SWA, which greatly improve efficiency but inevitably suffer from severe long-context performance degradation possibly due to the inaccessibility of distant tokens information (Xiao, 2025). This motivates critical, unexplored question: Can full-attention model be adapted to sliding window attention at low cost while maintaining long-context performance? We answer Yes to this question by proposing Sliding Window Attention Adaptation(SWAA), set of recipes for adapting FA-pretrained models to SWA, which requires neither costly pretraining nor modifications to the standard Transformer architecture. Specifically, it systematically combines five practical and composable methods: 1. Full Attention (FA) Decode: applying SWA only during the prefilling stage while switching back to full attention for decoding. 2. Keep First Tokens: explicitly preserving attention to the first sink tokens. 3. Interleaving FA/SWA layers: mix fullattention and SWA layers (e.g., assigning SWA to half layers). 4. Chain-of-Thought (CoT): enforcing an explicit \"thinking\" process during decoding. 5. Fine-tuning with SWA: lightweight SWAaware supervised fine-tuning on long-context data. Among these, FA Decode is novel method we introduce. Keep First Tokens and FA/SWA Interleaving have been proven effective in prior work (Xiao et al., 2024; Team, 2024a; Zhang et al., 2024), while CoT and fine-tuning are common LLM techniques. However, how these methods should be combined to be actually effective for SWA adaptation remains unexplored. Therefore, in our experiments, we evaluate SWAA on Qwen3 (Team, 2025b) and Llama3.1 (Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency under wide range of SWAA recipes. First, we find that each method makes distinct contribution, but no single ingredient suffices to make SWA competitive with full attention. Second, we show that specific synergistic combinations of methods can recover large fraction of the original long-context performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and identify some recommended configurations suitable for different deployment scenarios. Rather than proposing single globally optimal configuration, we view SWAA as flexible toolkit of practical recipes: practitioners can select SWAA recipes that match their accuracy and efficiency constraints, or compose their own SWA adaptation strategies by combining the available ingredients. Our key contributions are: 1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how SWA impacts LLMs and providing foundation for future research in efficient sparse attention. 2. We propose SWAA, set of practical SWA adaptation recipes that offer robust performance-efficiency balance for various use cases, accelerating LLM inference from the bottom level. 3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon et al., 2023), making it plug-and-play and userfriendly for practical deployment."
        },
        {
            "title": "2 Related Works",
            "content": "The O(N 2) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred wide field of research about more efficient language model architectures. Among the two most popular technological routes are sparse attention and linear attention. 2.1 Sparse Attention Our work falls in this category. Sliding Window Attention (SWA) represents the most basic form of local sparse attention, yet its performance is inherently limited. Therefore, model architectures such as Longformer (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), and RATTENTION (Wang et al., 2025) combine local SWA on most tokens with special global attention on specific tokens to create more powerful, albeit still sparse, pattern. Popular LLMs like Gemma2 (Team, 2024a) adopt SWA in half of their layers to balance the efficiency of SWA and peformance of FA. Sliding Window Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid activation and balanced position embeddings, to stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al., 2025; DeepSeek-AI, 2025b), although achieving excellent quality, involve more complicated implementation and optimization due to semantic-aware attention operations (e.g., selecting the most important tokens based on attention weights). Regardless, almost all of the above methods require pretraining with specific sparse pattern, which is costly and fails to leverage the advantages of existing pretrained models. LightTransfer (Zhang et al., 2024) is promising attempt at adapting existing models to SWA without pretraining, which has the same motivation as ours. But it may generalize poorly across model families (see Appendix G)."
        },
        {
            "title": "2.2 Linear Attention",
            "content": "An alternative approach involves reformulating the attention mechanism entirely to achieve linear, O(N ), complexity. This includes methods such as RNN-like linear attention transformers (Katharopoulos et al., 2020; Peng et al., 2023; Sun et al., 2023) and structured state-space models (SSMs) like Mamba (Gu and Dao, 2023). Many works such as Jamba and Nemotron-Flash(Lieber et al., 2024; Linsong Chu et al., 2024; Team et al., 2025; Fu et al., 2025a) interleave linear attention layers with traditional attention layers to create hybrid model structures. While promising, they represent fundamental architectural departure from the standard Transformer, and their performance is generally weaker than traditional Transformer-based LLMs."
        },
        {
            "title": "Adaptation",
            "content": "As established, naive application of SWA leads to severe long-context performance degradation. Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore, although these methods are not mutually exclusive, we should not indiscriminately adopt all of them. Instead, we must evaluate various combinations to identify the optimal trade-off between performance and efficiency. 3.1 Full Attention Decode This method applies SWA only to the prefilling stage. During the decoding (auto-regressive generation) stage, each token still employs full attention, allowing access to all previous tokens in the context. The resulting attention mask is depicted in Figure 1a. This novel approach, first proposed in our study, is inspired by human reading comprehension: humans typically scan passage casually (prefilling) before thinking deeply to formulate an answer (decoding) for specific problem. We term this strategy \"reading casually, thinking carefully.\" In our design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT) during decoding may be particularly beneficial, as extended generation (i.e. \"thinking\") could compensate for the insufficient contextual information gathered during the prefilling stage. (a) FA Decode (b) Keep First Figure 1: (a) Attention mask for FA Decode. SWA is used for prompt tokens (prefill), and full attention is used for generated tokens (decode). (b) Attention mask for SWA combined with Keep First Tokens. 3 3.1.1 Keep First Tokens Xiao et al. demonstrate that models pretrained with full attention allocate disproportionate amount of attention to the initial tokens (\"attention sink\"), and removing the visibility of these tokens causes performance collapse. Their solution, streaming attention, involves permanently retaining the attention to these \"sink\" tokens while using SWA, which successfully maintains the stability of the attention distribution and the models output. Our method is basically the same: as shown in Figure 1b, any subsequent token can attend to its local window and the initial tokens. Notably, however, our method extends beyond its original version. Streaming attention operates only at the KV cache level; specifically, the KV cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating prefilling via SWA as well, and eliminating the need to modify KV cache. 3."
        },
        {
            "title": "Interleaving Layers",
            "content": "This method retains full attention on subset of layers while applying SWA to the remainder, providing simple hybrid mechanism to balance the performance of full attention with the efficiency of pure SWA. common strategy involves designating one in every layers to use full attention (e.g., layers 0, 2, 4, . . . retain full attention, while all others use SWA). For example, Gemma-2 (Team, 2024a) uses SWA only for layers [1, 3, 5, . . . ], and Gemma-3 (Team, 2025a) uses SWA only for layers [5, 11, 17, . . . ]. However, for an FA-pretrained model, layers may exhibit distinct behaviors, suggesting it may not be optimal to simply assign SWA to layers [1, 3, 5, . . . ]. Instead, it might be preferable to use statistical features to select \"lazy\" (mostly focusing on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find that LightTransfer is not consistently superior in practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer selections in our experiments, such as [0, 2, 4, . . . ] and [1, 3, 5, . . . ]."
        },
        {
            "title": "3.3 Chain-of-Thought",
            "content": "Chain-of-Thought (CoT) (Wei et al., 2022) is widely used technique for improving model accuracy via reasoning. With the advent of \"thinking\" models, such as DeepSeek-R1 (DeepSeek-AI, 2025a), CoT has evolved from prompting strategy to an intrinsic LLM capability. However, whether CoT has specific impact in SWA scenarios remains uninvestigated. We explore this by comparing thinking model with its non-thinking variant, e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct, to verify the effect of CoT on SWA adaptation, which can produce more notable differences compared to simply adding CoT prompting to the same model. 3.4 Fine-tuning This is the most natural way to mitigate traininginference mismatch. Apparently, the model should be fine-tuned while SWA is applied, so that the models parameters can be trained to better adapt to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works. However, most available long-context datasets only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning \"thinking\" model. Since our goal is to restore the models original capabilities under SWA rather than teach it new ones, instead of directly using the original dataset, we adopt an approach similar to self-distillation (Yang et al., 2024). Specifically, we utilize the original full-attention model to generate new answers for the datasets questions, and these generated answers are then filtered for correctness using GPT-5-Mini (OpenAI, 2025), to make up our training dataset. For each question, we sample 4 answers with temperature 1, because we find this strategy is slightly better than generating only one answer with temperature 0."
        },
        {
            "title": "4 Experiment Setup",
            "content": "We organize our experiments around three research questions: RQ1: Is SWA adaptation feasible without any additional training? We evaluate whether an FA LLM can be adapted to SWA using only inference-time modifications, and which combinations of techniques are necessary. RQ2: How much does fine-tuning with SWA improve performance? We study the effect of SWA-aware fine-tuning on long-context performance and identify which components of SWAA are still required. RQ3: Which SWAA configurations achieves the optimal performance-efficiency trade-offs? We evaluate how different SWAA configurations trade off accuracy against inference latency. 4.1 Models Our primary experiments use Qwen3-4B-Thinking and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style reasoning, whereas the Instruct variant usually just answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023). All models are served with vLLM in float16 precision using batch size of 64. We use greedy decoding (temperature = 0) for all evaluations. In preliminary experiments, we observed that vLLM yields slightly lower (about 1% to 5%) scores than HuggingFace Transformers due to precisionrelated discrepancies."
        },
        {
            "title": "4.2 Evaluation Dataset",
            "content": "SWA is identical to full attention when the context length is within the window size. Even if the model is fine-tuned, we can pre-calculate the prompt length and simply disable the LoRA adapters for short prompts to get completely the same response as the original model. Therefore, our experiments focus exclusively on long-context benchmarks with inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g., MMLU (Hendrycks et al., 2021), GPQA (Rein et al., 2023)) is completely unnecessary. Since we find other long-context benchmarks are either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), benchmark consisting of various types of long-context QA tasks with moderate difficulty, although it is originally designed for agent memory system evaluation. Its context length is controllable by selecting specific number of chat sessions to concatenate as the context from pool of hundreds of sessions (a session contains the chat history between user and assistant within day). To create moderately difficult and discriminative evaluation, we construct LongMemEval_24k by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k with an average context length of 24k. 4 For additional validation of generalizability, as shown in Appendix D, we also experiment on LongBench-V2 (Bai et al., 2024b), more modern and challenging benchmark that requires deep reasoning across various real-world tasks. 4.3 Training Details For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), widely used long-context fine-tuning dataset for adapt regularlength model to long-context tasks. However, since its sample count (10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), more comprehensive corpus of over 40,000 long-context samples that includes LongAlign as subset. We perform SWA-aware fine-tuning using LoRA (Hu et al., 2022). Unless otherwise noted, we use rank = 16 and α = 128, and apply LoRA only to the query, key, and value projection modules. We adopt this parameter-efficient setting because full-parameter fine-tuning often leads to overfitting and degradation of the models original capabilities in our preliminary experiments. We use learning rate of 1e-4 with cosine decay schedule. Models are fine-tuned for single epoch on the sampled long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F). Once training takes approximately 12 hours on an 8*H20 GPU server for Qwen3-4B and 30 hours for Qwen3-30B-A3B."
        },
        {
            "title": "5.1 SWA Adaptation Without Fine-tuning",
            "content": "We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k accuracy for Qwen3-4B-Thinking (\"think\") and Qwen3-4B-Instruct (\"non-think\") under different combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify the impact of SWA. The configurations are ranked by the number of methods applied (0, 1, 2, or 3 of Interleaving Layers, Keep First and FA Decode). Rows 1 (original model) and 2 (naive SWA) serve as upper and lower baselines, respectively. In the column \"FA layers\", the value records which layers use full attention, and [] means all the layers use SWA, i.e., this method is not enabled. In the column \"keep first\", the value is in Keep First Tokens. When comparing results, an accuracy difference of less than 5% is usually considered statistically insignificant. From the results, we find that: Naive SWA is not viable. Naively replacing FA with 2k sliding window attention (row 1) drops accuracy significantly to 3.2 and 11.0, respectively. Even with an 8k window (row 2), accuracy only recovers to 13.2 and 19.8, far below the FA baseline. Single method helps, but cannot close the gap. Each methodKeep First, FA Decode, or Interleaving Layersimproves over naive SWA (rows 36), yet each alone recovers only small fraction of the FA gap and remains well below the baseline. In short, no single method is sufficient. Combinations exhibit strong synergy. Recipes that combine multiple methods deliver large gains: FA Decode + Keep First substantially improves over naive SWA (rows 79), recovering roughly half to two-thirds of the gap on the thinking model as increases. However, increasing from 100 to 1000 yields almost no improvement, indicating that does not need to be exceedingly large. Interleaving Layers + FA Decode is markedly stronger (row 13), recovering most of the gap for the thinking model. FA Decode + Interleaving Layers + Keep First perform best (rows 18). The thinking model recovers close to 90% of the FA gap even at 2k window. CoT synergizes with FA Decode. Under recipes that include FA Decode, the thinking model consistently benefits more than the non-thinking model (rows 13 and 18), suggesting that CoT synergizes with FA Decode: preserving global attention at decoding time enables longer reasoning traces to capitalize on context processed by SWA, confirming our hypothesis in Section 3.1. Sliding window size affects, but is not the decisive role. With FA Decode + Keep First k, accuracy improves as the window increases (rows 7, 14, 15), though benefits are modest until 8k. When added with interleaving FA layers, moving from 2k to 4k window is enough to close the remaining gap on the thinking model (row 22 matches the FA baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows mainly smooth residual error. 5 Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT No. window size FA layers keep first FA decode Acc think Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Full 2k 8k [] [] [] 2k 2k 2k 8k 2k 2k 2k 2k 2k 2k 2k 4k 8k 2k 2k 2k 2k 2k 2k 4k 8k [] [] [1, 3, 5, ...] [] [] [] [] [0, 2, 4, ...] [0, 2, 4, ...] [1, 3, 5, ...] [1, 3, 5, ...] [] [] [0, 2, 4, ...] [1, 3, 5, ...] [1, 3, 5, ...] [1, 5, 9, ...] [1, 9, 17, ...] [3, 7, 11, ...] [1, 3, 5, ...] [1, 3, 5, ...] 0 0 0 10 0 0 0 10 100 1000 10 0 10 0 10 10 10 10 100 10 10 10 100 False False False False True False True True True True False True False True True True"
        },
        {
            "title": "True\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue",
            "content": "73.0 3.2 13.2 16.0 11.8 13.4 26.2 38.2 50.0 50.0 17.0 32.2 25.8 59.2 38.0 49.2 36.0 65.0 68.8 53.2 36.4 54.2 73.0 71.6 62.0 11.0 19.8 15.6 14.2 18.4 25. 20.6 17.8 20.2 14.8 26.0 36.4 34.8 24.4 35.2 17.2 53.6 50.6 31.4 18.8 34.6 54.2 56.6 SWA/FA layer selection has large impacts. Selecting less layers for FA, e.g. only one fourth or eighth (row 19, 20), though more efficient, will significantly decrease the improvement brought by Interleaving Layers. More importantly, for Qwen34B, configuring odd-numbered layers with full attention is significantly better than even-numbered layers (row 11, 13, 16, 17). However, surprisingly, this result is reversed for Qwen3-30B-A3B and Llama3.1-8B-Instruct (row 10 and 11 in Table 4 and 5). This suggests that layer functionalities differ across model families and sizes, necessitating model-specific layer selection strategies, as discussed in Section 3.2. Therefore, we answer RQ1: adapting an FA LLM to SWA is feasible even without any training. But it requires specific combinations of at least 2 methods, which could be less efficient for inference."
        },
        {
            "title": "5.2 SWA Adaptation With Fine-tuning",
            "content": "We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k accuracy after SFT under various SWAA configurations. The original full-attention model is also fine-tuned as baseline (row 0). Since training is relatively time-consuming, we only test representative subset of configurations. Our findings are as follows: Fine-tuning substantially lifts all SWA configurations. Comparing all the fine-tuning results with non-training ones, it is clear that fine-tuning consistently provides great improvement. However, simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy (row 1). FA Decode and Interleaving Layers emerge as dominant components. After SFT, FA Decode and Interleaving Layers provide the largest gains. Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT No. window size FA layers keep first FA decode Acc think Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 Full 2k [] [] 2k 2k 2k 4k 2k 2k 2k 2k 2k 2k [] [] [1, 3, 5, ...] [] [] [] [1, 3, 5, ...] [0, 2, 4, ...] [1, 5, 9, ...] [1, 3, 5, ...] 0 0 10 0 0 0 10 100 0 0 0 100 True False False True False True True True True True True True 74.6 18.8 15.6 57.9 63.6 62.6 56.7 62.2 73.2 66.0 68.8 73. 63.4 23.8 / 42.0 54.6 / / 42.6 58.8 / 47.0 61.4 Table 3: Recommended SWA adaptation recipes for different needs and scenarios. means optional. Training? Thinking Model? Prefer?"
        },
        {
            "title": "Efficiency\nAccuracy",
            "content": "(cid:221) (cid:221)"
        },
        {
            "title": "F\nF",
            "content": "Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy. And combining both (row 8) further boosts performance to 73.2 (think) and 58.8 (non-think), nearly matching the full-attention SFT baseline in row 0. Keep First becomes optional rather than essential. Before fine-tuning, Keep First is crucial for stability under SWA. But after SFT, it only provides minor additional improvement. With FA Decode, adding = 100 (row 7) improves over = 0 (row 3) only 4.5%, and if further combining FA layers, it almost offers no improvements (row 11 and row 8). Effect of sliding window size. Row 3 and 5 shows that increasing the window from 2k to 4k with FA Decode improves thinking-model accuracy from 57.9 to 62.6. This mirrors the non-SFT trend that larger windows help, but the dominant improvements still come from FA Decode and Interleaving Layers. So, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we apply FA Decode, Interleaving Layers, or combination thereof, while Keep First becomes optional. And the improvement brought by SFT under each configuration varies significantly, meaning nearoptimal training-free configuration need not remain optimal after SFT, and vice versa."
        },
        {
            "title": "Recommended Recipes",
            "content": "Although integrating more methods can typically achieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe must also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we benchmark Qwen3-4B-Thinking on single H100 GPU using vLLMs bench_serve utility (Kwon et al., 2023) with random input data and 100 total requests. The prompt length and output length are set to 128k and 512 tokens, respectively, representing typical long-context QA setting. 7 (a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct Figure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval To visualize the performance-efficiency tradeoff, Figure 2 plots each configurations accuracy on LongMemEval_24k (Wu et al., 2024) against its average running time, while detailed TTFT, TPOT, and throughput statistics for each configuration are provided in Appendix E. We draw line between the full-attention point and the naive-SWA point as baseline curve: configurations above this line offer better accuracy-latency balance intuitively. For configurations with nearly identical time costs, we display only the one with the highest accuracy. Since Keep First has negligible impact on runtime (Appendix E), all plotted configurations fix = 10. From Figure 2, we observe that many configurations in Figure 2 achieve clearly better performance-efficiency ratio than baselines. And for the thinking model, more points lie above the baseline curve compared to non-thinking, indicating that CoT generally has positive effect on improving the performance-efficiency ratio of SWAA. Thus, we finally answer RQ3: many SWAA configurations all reach excellent performanceefficiency trade-off, but there is no single metric to quantify such trade-off to decide the globally optimal one. We therefore summarize recommended SWA adaptation recipes tailored to various deployment scenarios in Table 3. And we must note that specific parameters should be flexibly set to meet application-specific requirements, without the need to follow our experimental parameters (e.g., 2k window, = 10). For example, you can increase the window size to 4k or to 128 for higher accuracy and acceptable additional overhead."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we validate the feasibility of adapting full-attention pretrained LLMs to Sliding Window Attention (SWA) for better efficiency, offering cost-effective alternative that avoids training sparseattention models from scratch. By systematically deconstructing the adaptation process, we identify that the catastrophic degradation observed in naive implementations can be effectively mitigated through synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen and Llama families demonstrate that while tradeoffs between computational overhead and model performance are inevitable, optimized configurations can get an excellent performance-efficiency balance."
        },
        {
            "title": "7 Limitations",
            "content": "We speculate that the ideal reasoning trajectory of the model adapted to SWA should be longer than the original model, to compensate for the information loss caused by SWA. That means, using the answers generated by the original model as fine-tuning data may not be the optimal training method. Rather, RL methods like GRPO (Shao et al., 2024) might further help the model adapted to SWA learn better reasoning trajectory. How8 ever, we did not experiment with them since they are too time-consuming and unstable. We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA; that is, although the speed is improved, memory usage is not effectively reduced. Further experiments may be needed to confirm whether our conclusions generalize to larger model sizes, such as 70B."
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a. Longalign: recipe for long context alignment of large language models. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: bilingual, multitask benchmark for long context understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. Tri Dao. 2024. Flashattention-2: Faster attention with In The better parallelism and work partitioning. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. 2024. Jamba: hybrid transformer-mamba language model. DeepSeek-AI. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Tri Dao Linsong Chu, Divya Kumari et al. 2024. Bamba: Inference-efficient hybrid mamba2 model. DeepSeek-AI. 2025b. Deepseek-v3.2: Pushing the fronOpenAI. 2025. ChatGPT. tier of open large language models. Yonggan Fu, Xin Dong, Shizhe Diao, Hanrong Ye, Wonmin Byeon, Yashaswi Karnati, Lucas Liebenwein, Maksim Khadkevich, Alexander Keller, Jan Kautz, et al. 2025a. Nemotron-flash: Towards latencyoptimal hybrid small language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems. Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, and Xiangyu Zhao. 2025b. Sliding window attention training for efficient large language models. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Wenbo Pan. 2024. Fusang-v1: large curation of instruction-tuning datasets for better bilingual and long-range llms. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1404814077, Singapore. Association for Computational Linguistics. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Guangxuan Xiao. 2025. Why stacking sliding windows cant see very far. https://guangxuanx.com/ blog/stacking-swa.html. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: successor to transformer for large language models. Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, and Qian Liu. 2024. Selfdistillation bridges distribution gap in language model fine-tuning. Gemma Team. 2024a. Gemma 2: Improving open language models at practical size. Gemma Team. 2025a. Gemma 3 technical report. Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, and Jun Zhou. 2025. Every attention matters: An efficient hybrid architecture for long-context reasoning. Llama Team. 2024b. The llama 3 herd of models. Qwen3 Team. 2025b. Qwen3 technical report. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Bailin Wang, Chang Lan, Chong Wang, and Ruoming Pang. 2025. Rattention: Towards the minimal sliding window size in local-global attention models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2024. Longmemeval: Benchmarking chat assistants on long-term interactive memory. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native sparse attention: Hardwarealigned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2307823097, Vienna, Austria. Association for Computational Linguistics. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Xuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, and Min Lin. 2024. Lighttransfer: Your long-context llm is secretly hybrid model with effortless adaptation. SWAs Benefits and Each Methods"
        },
        {
            "title": "Drawbacks",
            "content": "SWA reduces the computational complexity to O(N ), where is the window size. The benefits are threefold: (1) SWA reduces the computational load, (2) conserves GPU memory by limiting the required Key-Value (KV) cache, and (3) enhances KV cache reusability beyond traditional prefix caching, since tokens state is independent of tokens outside its local window. However, there is no free luncheach method of SWAA has some drawbacks, impairing the benefits brought by SWA to varying degrees. FA Decode presents two primary drawbacks: (1) the benefits apply only to prefilling, while decoding speed is not accelerated as it utilizes full attention, and (2) the GPU memory required for the KV cache is not reduced, as the KV cache for the full context must be retained for decoding. In 10 practice, however, many distributed LLM services have to recompute the KV cache of the entire chat history because storing and loading the KV cache complicates engineering systems, making prefilling occurs more frequently than expected, thereby amplifying the advantage of this method. Keep First introduces very minor computational overhead, but it complicates efficient KV cache reuse. Due to positional encoding, tokens KV state depends on its position relative to the initial tokens, hindering simple cache reuse across different requests. position encoding separation or offsetting mechanism may be needed. Interleaving Layers introduces the most significant overhead, as only subset of layers benefits from the computational savings of SWA. Furthermore, the GPU memory required for the KV cache is not reduced for the full-attention layers. Additionally, this method negates the KV cache reusability advantage of SWA, as the existence of full-attention layers violates the independence of the KV cache beyond the local window. CoT will greatly increase the generation length, especially for difficult tasks. So the decoding time will be much longer. Other Long-context Benchmarks We find existing long-context benchmarks problematic for our specific needs. For example: 1. LongBench (Bai et al., 2023) is classic and widely used, but its average context length (most are under 16k) is relatively short for modern models, i.e., it is already too easy. And its data source is too old, leading to risk of test data leakage. 2. Ruler(Hsieh et al., 2024) has controllable context length, but its tasks are almost all synthetic and most of them are needle-retrieval tasks, failing to reflect the models overall long-context capability in real-world scenarios. 3. LongBench-V2 (Bai et al., 2024b) is welldesigned to necessitate deep understanding and reasoning over very long context. But it is too challenging for 4B-level models (e.g., Qwen3-4B-Thinking only gets 35% accuracy, which is too close to the random guessing baseline of 25%), making the improvement of different methods less distinguishable. Moreover, since it is in multiple-choice question format, the results may not be sufficiently reliable because the model has 25% chance of guessing the correct option. However, despite the extreme difficulty of LongBench-V2 (Bai et al., 2024b), it remains high-quality long-context benchmark after all. Thus we still elect to conduct our experiments on it to verify the generalizability of our conclusions, as shown in Appendix D."
        },
        {
            "title": "C Results of Other Models",
            "content": "We show the results of Qwen3-30B-A3BThinking and Qwen3-30B-A3B-Instruct on LongMemEval_24k in Table 4, and the results of Llama3.1-8B-Instruct in Table 5. The scores of Qwen3-30B-A3B are generally higher and those of Llama3.1 are generally lower, but all results are consistent with our previous conclusions, demonstrating their generalizability. Due to the time-intensive nature of training, we only test small set of configurations with finetuning. Results of LongBench V2 We present the results of LongBench V2 (Bai et al., 2024b) in Tables 6, 7 and 8. We retain only the samples whose context length is under 128k due to GPU memory limitations; thus, 384 of 500 samples are kept. However, due to the high difficulty, the performance is generally poor. Some scores are even below the random guessing baseline (25%). For Qwen3-4B and Qwen3-30B-A3B models, the results show less noticeable differences between various methods. But fortunately, the trend of accuracy changes is generally consistent with that of other datasets, so they do not conflict with all of our previous conclusions. For Llama3.1, due to its weaker long-context capability, accuracy consistently hovers around 30%."
        },
        {
            "title": "E Inference Efficiency",
            "content": "The TTFT, TPOT and total throughput when using vLLM are shown in Table 9. Since inference speed is highly dependent on hardware, implementation details, and workload characteristics, these numbers should be interpreted as reference values. However, from the results, we can still conclude that: 1. Interleaving Layers and FA Decode significantly slow down the speed compared to pure 11 1. For Qwen3-4B, LightTransfer even has counterproductive effect; allowing lazy layers to use FA yields higher scores, while following the original method (letting non-lazy layers use FA) results in significantly lower scores. 2. For Qwen3-30B, it provides nearly no improvement over fixed-interval selection. 3. Only for Llama3.1-8B does LightTransfer show advantages. Therefore, we conclude that LightTransfer does not yield stable performance across various models. Although fine-grained layer selection methods are theoretically superior, we believe they require further investigation before integration into our SWAA recipes. SWA. 2. Keep First Tokens has negligible impact on efficiency. 3. Increasing the window size slightly increases inference time. For example, increasing from 2k to 4k decreases throughput by only 10%, but 4k window generally achieves higher accuracy based on previous experiments. Therefore, in practice, 4k window is more common choice. In theory, FA Decode should yield decoding speed identical to that of full attention. Yet, in this table, we observe acceleration on TPOT. This is because vLLM-v1 typically mixes different requests prefilling and decoding tokens in one sequence to improve GPU utilization. Thus, the speeds of prefilling and decoding may affect each other. If processing only single request, the situation differs. For example, when the generation length is set to 2000, we find decoding takes over 95% of the total time, rendering the acceleration of the prefilling stage negligiblei.e., SWA with FA Decode is almost unable to improve efficiency in such cases."
        },
        {
            "title": "F Influence of Training Epochs",
            "content": "As shown in Table 10, training for more than 1 epoch yields no improvement. Therefore, we choose to train for only 1 epoch."
        },
        {
            "title": "G Results of LightTransfer",
            "content": "LightTransfer (Zhang et al., 2024) represents promising attempt at SWA adaptation on fullattention models without pretraining. It proposes layer selection method for SWA adaptation that calculates \"lazy ratio,\" represented by the ratio of attention from tokens at the end of the sequence (from calibration dataset) to recent tokens versus global tokens. Layers with higher \"lazy ratio\" are selected to apply SWA, while the rest retain full attention. This method is intuitive and theoretically sound, but our experiments reveal some negative results. Since the complete code of LightTransfer is not open-source, we reproduce this method using LongAlign (Bai et al., 2024a) as the calibration dataset for lazy layer detection, where the number of last tokens is set to 64, and the recent token window is set to 1024. From our experimental results shown in Table 11, we find that: 12 Table 4: Results of Qwen3-30B-A3B-Thinking and Qwen3-30B-A3B-Instruct on LongMemEval No. SFT window size FA layers keep first FA decode Acc think Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 19 False False False False False False False False False False False False False False False"
        },
        {
            "title": "True",
            "content": "Full 2k 8k [] [] [] 2k 2k 2k 2k 2k 2k 4k 2k 2k 2k 2k 4k 2k 2k 2k 2k [] [] [0, 2, 4, ...] [] [] [] [] [0, 2, 4, ...] [1, 3, 5, ...] [0, 4, 8, ...] [2, 6, 10, ...] [0, 2, 4, ...] [] [] [] [0, 2, 4, ...] [0, 2, 4, ...] 0 0 0 10 0 10 100 1000 10 10 10 10 10 100 0 0 100 0 False False False False True False True True True True True True True True True"
        },
        {
            "title": "True",
            "content": "79.2 0.0 0.0 0.0 0.2 21.0 43.8 58.6 59.0 49.8 74.8 51.6 48.8 64.8 74.6 79.6 62. 65.6 72.6 77.8 71.6 0.4 0.2 2.8 0.2 28.4 23.6 22.2 25.4 26.6 63.0 24.0 23.8 44.2 64. 51.0 50.8 68.0 Table 5: Results of Llama3.1-8B-Instruct on LongMemEval No."
        },
        {
            "title": "SFT window size FA layers",
            "content": "keep first FA decode Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 12"
        },
        {
            "title": "False\nFalse\nFalse\nFalse\nFalse",
            "content": "0 0 0 10 0 0 10 100 1k 10 10 10 10 10 100 Full 2k 8k [] [] [] 2k 2k 2k 2k 2k 2k 4k 2k 2k 2k 2k 4k [] [] [0, 2, 4, ...] [] [] [] [] [0, 2, 4, ...] [1, 3, 5, ...] [0, 4, 8, ...] [2, 6, 10, ...] [0, 2, 4, ...]"
        },
        {
            "title": "True\nTrue\nTrue\nTrue\nTrue",
            "content": "61.0 0.6 1.2 1.8 0.0 3.0 16.8 20.0 24.2 23.8 42.6 21.0 17.8 24.4 44.0 Table 6: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongBench-V2 No. SFT window size FA layers keep first FA decode Acc think Acc non-think False False False False True False True"
        },
        {
            "title": "True",
            "content": "34.6 9.4 15.1 7.7 26.2 12.1 22.8 25.8 24.2 23.8 19.8 23.8 21.1 28.5 27.9 37.9 7.4 6.0 29.2 29.5 32.9 28.9 29.2 31.5 30.9 38.3 32. 37.2 35.2 25.8 22.1 25.8 25.2 23.5 25.5 25.2 26.5 25.2 29.2 29.9 29.9 26.8 27.5 34.9 30.9 30.2 31.9 29.2 30.5 34.6 32.2 33.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27 False False False False False False False"
        },
        {
            "title": "True",
            "content": "Full 2k 8k [] [] [] 2k 2k 2k 8k 2k 2k 2k 2k 2k 2k 2k 4k [] [] [1, 3, 5, ...] [] [] [] [] [0, 2, 4, ...] [0, 2, 4, ...] [1, 3, 5, ...] [1, 3, 5, ...] [] Full 2k [] [] 2k 2k 2k 4k 2k 2k 2k 2k 2k 2k 2k [] [] [1, 3, 5, ...] [] [] [] [0, 2, 4, ...] [0, 4, 8, ...] [1, 3, 5, ...] [1, 5, 9, ...] [1, 3, 5, ...] 0 0 0 10 0 0 0 10 100 1000 10 0 10 0 10 0 100 0 0 0 10 100 0 0 0 0 100 14 Table 7: Qwen3-30B-A3B-Thinking and Qwen3-30B-A3B-Instruct results on LongBench-V2 No. SFT window size FA layers keep first FA decode Acc think Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 19 False False False False False False False False False False False False False False False"
        },
        {
            "title": "True",
            "content": "Full 2k 8k [] [] [] 2k 2k 2k 2k 2k 2k 4k 2k 2k 2k 2k 4k"
        },
        {
            "title": "Full",
            "content": "2k 2k 2k 2k [] [] [0, 2, 4, ...] [] [] [] [] [0, 2, 4, ...] [0, 4, 8, ...] [1, 3, 5, ...] [2, 6, 10, ...] [0, 2, 4, ...] [] [] [] [0, 2, 4, ...] [0, 2, 4, ...] 0 0 0 10 0 10 100 1k 10 10 10 10 10 100 0 0 100 0 False False False False True False True True True True True True True True True"
        },
        {
            "title": "True",
            "content": "49.7 0.0 0.0 9.1 0.0 20.1 9.1 10.4 11.7 26.8 22.1 12.4 30.2 21.1 29.5 43.6 35. 36.6 41.3 48.0 42.6 0.0 0.0 32.2 0.0 25.8 32.2 28.2 29.5 30.9 33.6 29.5 28.9 35.6 35. 33.9 32.9 37.9 Table 8: Llama3.1-8B-Instruct results on LongBench-V2 No."
        },
        {
            "title": "SFT window size FA layers",
            "content": "keep first FA decode Acc non-think 0 1 2 3 4 5 6 7 8 9 10 11 12"
        },
        {
            "title": "False\nFalse\nFalse\nFalse\nFalse",
            "content": "0 0 0 10 0 0 10 100 1000 10 10 10 10"
        },
        {
            "title": "True\nTrue\nTrue\nTrue\nTrue",
            "content": "Full 2k 8k [] [] [] 2k 2k 2k 2k 2k 2k 4k 2k 2k 2k 2k 4k [] [] [0, 2, 4, ...] [] [] [] [] [0, 2, 4, ...] [0, 4, 8, ...] [1, 3, 5, ...] [2, 6, 10, ...] [0, 2, 4, ...] 15 33.2 0.0 0.0 28.9 0.0 0.0 28.9 30.2 30.2 27. 28.2 32.6 26.5 26.8 30.9 Table 9: Efficiency metrics of different SWAA configurations on vLLM. \"FA layers = 1/4\" means one fourth of total layers use full attention while the others use SWA. window keep first FA decode FA layers TTFT (s) TPOT (s) Throughput (k tks/s) Full 2k 2k 2k 2k 2k 2k 4k 4k 4k 4k 4k 4k 0 0 100 0 0 0 0 0 100 0 0 0 0 False False False False True True True False False False True True True None None None 1/2 None 1/2 1/4 None None 1/2 None 1/2 1/4 1681.44 203.20 207.74 938.00 963.39 1321.39 1141.66 233.07 237.87 949.02 990.00 1340.91 1166. 0.16 0.02 0.02 0.09 0.11 0.14 0.12 0.02 0.02 0.09 0.11 0.14 0.13 3.74 30.72 30.65 6.70 6.39 4.72 5.43 27.03 26.74 6.64 6.23 4.64 5. Table 10: Results of different training epochs of Qwen3-4B-Thinking on LongMemEval SFT (epochs) window size FA layers keep first FA decode Acc 1 2 3 2k 2k 2k [] [] [] 0"
        },
        {
            "title": "True\nTrue\nTrue",
            "content": "58.0 57.6 56.0 Table 11: Results of LightTransfer on LongMemEval. \"lazy\" represents the half layers with higher lazy ratio, i.e. those which should apply SWA in theory. \"non-lazy\" represents the other part, i.e. those which should keep full attention."
        },
        {
            "title": "SFT window size FA layers",
            "content": "keep first FA decode Acc think Acc non-think Model Group: Qwen3-4B"
        },
        {
            "title": "False\nFalse\nFalse\nFalse",
            "content": "2k 2k 2k 2k [0, 2, 4, ...] [1, 3, 5, ...] lazy non-lazy Model Group: Qwen3-30B-A3B"
        },
        {
            "title": "False\nFalse\nFalse\nFalse",
            "content": "2k 2k 2k 2k [0, 2, 4, ...] [1, 3, 5, ...] lazy non-lazy Model Group: Llama3.1-8B-Instruct"
        },
        {
            "title": "False\nFalse\nFalse\nFalse",
            "content": "2k 2k 2k 2k [0, 2, 4, ...] [1, 3, 5, ...] lazy non-lazy"
        },
        {
            "title": "True\nTrue\nTrue\nTrue",
            "content": "100 100 100 100 100 100 100 100 100 100 100 100 16 48.8 70.8 70.2 54.0 75.8 60.2 61.8 74. 18.4 50.4 47.8 19.6 64.2 25.8 25.2 59.2 39.8 24.2 20.2 49."
        }
    ],
    "affiliations": [
        "DeepSolution",
        "Oregon State University",
        "Penn State University"
    ]
}