{
    "paper_title": "Sparse Reward Subsystem in Large Language Models",
    "authors": [
        "Guowei Xu",
        "Mert Yuksekgonul",
        "James Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 ] . [ 1 6 8 9 0 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Sparse Reward Subsystem in Large Language Models",
            "content": "Guowei Xu 1 Mert Yuksekgonul 2 James Zou"
        },
        {
            "title": "Abstract",
            "content": "explain why these representations possess such capabilities. In this paper, we identify sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the models internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected. 1. Introduction In recent years, the reasoning capabilities of Large Language Models (LLMs) have achieved significant breakthroughs, even surpassing human expert levels in various mathematical and scientific tasks (Singh et al., 2025; Guo et al., 2025a). Parallel to the continuous pursuit of higher performance, researchers seek to understand the underlying reasoning mechanisms of these models. Recent studies reveal that LLM hidden states encode rich information, which can be leveraged for weighted learning (Oh et al., 2025), predicting confidence and answer correctness (Gekhman et al., 2025), detecting hallucinations (Zhang et al., 2025a), and performing implicit reasoning (Chen et al., 2024). However, most existing work focuses primarily on utilizing the information present in hidden states, while few studies analyze the intrinsic properties of the hidden states themselves or attempt to 1Tsinghua University 2Stanford University. Correspondence to: James Zou <jamesz@stanford.edu>. Preprint. February 3, 2026. 1 In this paper, we find that the ability of LLM hidden states to predict confidence and correctness can be attributed to the existence of small subset of neurons that encode the value information of the current state. This hypothesis is inspired by biological concepts. In neuroscience, the human brain contains reward subsystem composed of small number of neurons that govern how humans explore and learn from their environment. Value neurons and dopamine neurons are two critical types of neurons within this system. Value neurons are primarily identified in the ventromedial prefrontal cortex (vmPFC) and the orbitofrontal cortex (OFC) of human and other primate brains, which are known to represent the subjective value of stimuli during decisionmaking (Tremblay & Schultz, 1999; Padoa-Schioppa & Assad, 2006). Meanwhile, dopamine neurons are mainly located in the ventral tegmental area (VTA) and the substantia nigra pars compacta (SNc). Their core function is to encode the Reward Prediction Error (RPE), the phenomenon where neuronal activation increases when the brain receives higher reward than expected and decreases when the reward falls short of expectations (Schultz, 1998). We demonstrate that similar reward subsystem exists within LLM hidden states, where sparse set of neurons performs functions analogous to biological value and dopamine neurons. Specifically, value neurons represent the models own expectation of the current states value, while the activation levels of dopamine neurons correlate closely with the discrepancy between the actual reward and the models expectation. In the following sections, we first introduce the value neurons within the LLM reward subsystem and describe how to identify their distribution across model layers. Through intervention experiments, we observe that value neurons exert significant influence on the models reasoning capabilities. Specifically, zeroing out the hidden states of even small fraction of value neurons results in substantial performance degradation, whereas randomly zeroing out the same proportion of neurons yields no such effect. Next, we systematically demonstrate the robustness and transferability of these value neurons, showing that they are universal across various datasets (GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), Minerva Math (Lewkowycz et al., 2022), ARC (Clark et al., 2018), MMLU STEM (Hendrycks et al., 2021)), model scales (0.5B, 7B, 14B), layers, and archiSparse Reward Subsystem in Large Language Models tectural designs (Qwen (Bai et al., 2023), Llama (Touvron et al., 2023), Phi (Gunasekar et al., 2023), Gemma (Team et al., 2024)). Furthermore, we show that the distribution of these value neurons remain consistent across different models derived from the same base model, as well as across diverse datasets. Subsequently, we identify dopamine neurons by examining cases where value predictions and actual rewards diverge. Through case studies, we demonstrate that these dopamine neurons indeed exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected. In summary, our key contributions are: We identify reward subsystem in LLM hidden states analogous to that of the human brain. Within this subsystem, small subset of neurons acts as value neurons, representing the models value expectation for the current state, while another subset acts as dopamine neurons, whose activation levels reflect the reward prediction error. Intervention experiments reveal that value neurons are critical for reasoning. Ablating even small subset of these neurons severely impairs performance. We demonstrate the robustness of the value neurons across diverse datasets, model scales, and architectures. We demonstrate the transferability of the value neurons across different models derived from the same base model and across diverse datasets. 2. Sparse Value Neurons in Large Language"
        },
        {
            "title": "Models",
            "content": "Our study focuses on autoregressive large language models. By analyzing the characteristics of hidden states during generation, we demonstrate that large language models exhibit sparse reward subsystem within their hidden states that encodes information regarding value (value neurons, Section 2.3) and prediction error (dopamine neurons, Section 4.1). In this section, we first introduce value neurons. During the generation process, the LLM produces tokens sequentially. At each step t, an action is sampled such that at M(st), and the subsequent state is updated as st+1 = st at. Suppose the model generates total of new tokens. Given that modern decoder-only LLMs consist of multiple Transformer blocks, each hidden layer produces corresponding representation for every state st. We denote the hidden state at the l-th layer and t-th step as h(st, l). To extract the reward information embedded within these representations, we introduce value probe . Following Zhu et al., 2025, we employ two-layer multi-layer perceptron (MLP) with ReLU activation. The input dimension matches the dimensionality of the LLMs l-th layer hidden states, while the output is scalar representing the predicted reward. This probe is intentionally designed with minimal complexity to ensure that the predicted reward reflects the intrinsic structural information of the hidden states rather than features newly learned by the probe itself. Specifically, after computing the hidden state at the l-th layer and t-th step h(st, l), we feed it as input into the value probe to obtain the value prediction output (h(st, l)). For brevity, we define Vl(st) as the value computed from the l-th layer hidden state corresponding to the current state st, such that Vl(st) = (h(st, l)). The probe is optimized using Temporal Difference (TD) learning. Let r(sT ) be the final binary reward received. Given discount factor γ, the TD error δt is defined as: δt = (cid:40) r(sT ) Vl(sT ), γVl(st+1) Vl(st), if = otherwise. (1) We conduct layer-wise analysis to investigate the characteristics of the reward subsystem. For given layer and training dataset DT , our objective for the value probe is to minimize the expected TD error over the distribution of generated sequences. This is formulated as the following loss function: LTD(l) = Es0DT ,st+1M(st) (cid:34) (cid:88) (cid:35) δ2 . t=1 (2) 2.1. Training Value Probe Formally, consider an autoregressive LLM M. Modern LLMs typically utilize chat templates where the input is divided into system prompt and user query . The encoder processes these inputs such that the system prompt (including its template components) is encoded as sp, and the user query (including its template components) is encoded as su. The initial state for generation is defined as s0 = sp su, where denotes the concatenation of token sequences. In Appendix A, we illustrate the advantages of utilizing the TD error training objective compared to training exclusively on the final reward. 2.2. Identifying Value Neurons Once the value probe is trained, we evaluate its performance on validation dataset DV . High predictive performance indicates the presence of an extractable reward subsystem within the current layer l. Notably, during the evaluation phase, we do not use the full model response to assess the Sparse Reward Subsystem in Large Language Models Instead, we measure the predicted value Vl(s0) reward. directly from the hidden state at the initial input position s0. This approach allows us to detect whether the reward subsystem has already formed an assessment of the potential reward before any tokens of the answer are generated. Specifically, we evaluate the correlation between the predicted reward Vl(s0) and the eventual correctness of the models response r(sT ). higher correlation indicates more robust predictive capability. For this purpose, we utilize the Area Under the Receiver Operating Characteristic curve (AUC) as our metric, which measures the ability of classifier to distinguish between classes. Consequently, AU C(Vl(s0), r(sT )) serves as the metric for evaluation. Up to this point, our analysis has been based on the fulldimensional hidden states of the model. To substantiate the existence of value neurons in the reward subsystem, we must demonstrate that small subset of the hidden states maintains sufficient predictive power regarding the reward. We perform pruning experiments to investigate this hypothesis. Specifically, the input dimensionality of our value probe matches that of the LLMs hidden states. Now we introduce pruning ratio p. We prune pN of these input dimensions, feeding only the most significant (1 p)N dimensions into the value probe. To prune the network, we calculate the L1 norm of the weights connecting the input dimensions to the neurons in the first hidden layer. We then remove fraction of the input dimensions that correspond to the smallest L1 weight norms, constructing pruned value probe . During this process, the remaining weights within the probe are kept unchanged. Based on this procedure, we plot the relationship between AU C(V (s0), r(sT )) and the pruning ratio p. If the AUC curve remains stable as increases, it provides strong evidence for the existence of sparse value neurons. 2.3. Empirical Evidence We demonstrate that within each layer of the LLM, small subset of neurons functions as value neurons, which can be utilized to predict the value function of the models current output. We provide an illustrative example using the curves from layers 24 of the hkust-nlp/Qwen-2.5-14B-SimpleRL-Zoo (Zeng et al., 2025) model on the GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023) dataset. Prior to the experiments, the value probes were trained; for detailed hyperparameters, please refer to Appendix B. For each dataset, we partitioned the data into an 80% training set for the value probe and 20% validation set for identifying and analyzing the neurons. As illustrated in Figure 1, the AUC curves do not exhibit significant decline as pruning proceeds; in fact, even slight Figure 1. AUC curves for layers 24 of the Qwen-2.5-14BSimpleRL-Zoo model on the GSM8K and MATH500 datasets. The curves indicate that the value probe can accurately predict the value by relying on only very small number of value neurons. initial increase is observed. This indicates that the value probe can effectively estimate the value of the current state by relying on very small fraction (less than 1%) of the total neurons. These specific neurons are designated as value neurons due to their role in encoding value information. comprehensive discussion regarding the robustness of these results is provided in Section 3. 2.4. Intervention Experiments Since value neurons encode the models prediction of the current states value, we posit that these neurons possess greater importance for reasoning than other neurons. Consequently, we conduct intervention experiments to quantify the impact of the value neurons on the models performance during inference. Specifically, we select the Qwen-2.5-7B-SimpleRL-Zoo model and zero out the activations of the top 1% of value neurons in specific layers, subsequently measuring the resulting performance drop. For comparison, we randomly zero out 1% of neurons in the same layers as baseline and measure the corresponding performance decrease. Table 1. Intervention results for the Qwen-2.5-7B-SimpleRL-Zoo model on the MATH500 dataset. Performance is measured by accuracy after zeroing out 1% subset of neurons in single layer. Layer Value Neurons Random Neurons 2 3 4 5 Avg 37.0 (-38.2) 13.6 (-61.6) 29.4 (-45.8) 1.2 (-74.0) 20.3 (-54.9) 77.0 (+1.8) 73.4 (-1.8) 73.8 (-1.4) 74.4 (-0.8) 74.6 (-0.6) As presented in Table 1, we report the performance variations on the MATH500 dataset before and after the intervention on value neurons versus random neurons. The original performance of the model on the MATH500 dataset 3 Sparse Reward Subsystem in Large Language Models is 75.2%. Following random intervention of 1% of the neurons, the performance remains nearly unchanged. In contrast, the intervention on the 1% value neurons triggers significant decline in performance. These results demonstrate that even sparse set of critical neurons constituting only 1% of single layer is vital for the models reasoning capabilities. 3. Robustness and Transferability of the Value"
        },
        {
            "title": "Neurons",
            "content": "In this section, we demonstrate that value neurons within the reward subsystem are universal across various datasets, model scales, layers, and architectural designs. Furthermore, we show that the spatial distributions of value neurons within the reward subsystem remain consistent across different models derived from the same base model, as well as across diverse datasets. 3.1. Robustness on More Datasets To demonstrate the universality of this subsystem, we investigate whether these findings generalize effectively to other representative dataset types. To this end, we extend our verification to include Minerva Math (Lewkowycz et al., 2022), ARC (Clark et al., 2018), and the STEM subset of MMLU (Hendrycks et al., 2021). 3.2. Robustness across Different Model Scales To verify that the reward subsystem is consistent across models of varying scales, we conducted evaluations on the GSM8K dataset using three models of different sizes: Qwen-2.5-1.5B/7B/14B-SimpleRL-Zoo. Figure 3. AUC curves for layers 24 of the Qwen-2.51.5B/7B/14B-SimpleRL-Zoo model on the GSM8K dataset. In Figure 3, the AUC curve remains largely invariant to pruning, with some instances even showing slight improvement. This observation is consistent with our previous findings. 3.3. Robustness across Different Layers In this section, we examine the robustness of the reward subsystem across various layers. For this purpose, we utilize the Qwen-2.5-1.5B-SimpleRL-Zoo model, which consists of 28 layers in total. Figure 2. AUC curves for layers 24 of the Qwen-2.5-14BSimpleRL-Zoo model on the Minerva Math, ARC, and the STEM subset of MMLU datasets. As shown in Figure 2, the AUC curve remains largely invariant to pruning, demonstrating that the existence of value neurons within the reward subsystem is consistently observed across different datasets. Figure 4. AUC curves for different layers of the Qwen-2.5-1.5BSimpleRL-Zoo model on the GSM8K dataset. As shown in Figure 4, in addition to layers 24 analyzed previously, we extend our evaluation to layers at various 4 Sparse Reward Subsystem in Large Language Models depths, specifically layers 5, 6, 10, 14, 22, and 28. We find that the AUC remains largely stable or even increases as the pruning ratio rises. 3.4. Robustness across Different Models We primarily focus on models based on the Qwen architecture. In addition to Qwen, the Llama (Touvron et al., 2023), Gemma (Team et al., 2024), and Phi (Gunasekar et al., 2023) architectures are also widely adopted in the open-source LLM community. To verify the applicability of the reward subsystem across these architectures, we conduct experiments on the MATH500 dataset using Llama-3.1-8B-Instruct (Grattafiori et al., 2024), Gemma-3-4B-it (Team et al., 2025), and Phi-3.5-mini-instruct (Abdin et al., 2024). IoU when two sets of neurons are selected independently and uniformly at random. The method for calculating the baseline IoU curve is provided in Appendix C. If the IoU on different datasets is significantly higher than this random baseline, we can conclude that the distributions of value neurons are closely correlated across datasets. Using layer 3 of the Qwen-2.5-14B-SimpleRL-Zoo model, we compute pairwise IoU curves as function of the pruning ratio for the GSM8K, MATH500, Minerva Math, ARC, and MMLU STEM datasets. We then plot the average of all these pairwise curves. Figure 6. IoU as function of the pruning ratio. The IoU values for value neurons across different datasets are significantly higher than the random baseline, indicating that for the same LLM, the positions of value neurons are closely correlated across tasks. As shown in Figure 6, the IoU curves for any two datasets consistently exceed the random baseline. More notably, as the pruning ratio approaches 1, many IoU curves exhibit significant upward trend. For instance, the IoU between the GSM8K and ARC datasets exceeds 0.6 even at 99% pruning ratio. This demonstrates that small number of value neurons exist within the LLM; these neurons, which are most critical for value prediction, remain highly stable across datasets, further underscoring the sparsity of the reward subsystem. In Appendix D, we provide additional results, all of which demonstrate that value neurons exhibit strong transferability across different datasets. 3.6. Transferability Across Fine-tuned Models In this section, we investigate whether the spatial distribution of value neurons within the reward subsystem remains correlated across different models fine-tuned from the same base model. Such correlation would suggest that the base model already encodes some information relevant to these value neurons. To test this, we select layers 24 of two models: Qwen-2.5-7B-SimpleRL-Zoo and RLHFlow/Qwen2.5-7B-PPO-Zero, both of which are derived from Qwen-2.5-7B via RLVR fine-tuning. We train value probes on the MATH500 dataset and plot the IoU curves following the method described in the previous section. Figure 5. AUC curves for layers 24 of the Llama-3.1-8BInstruct, Gemma-3-4B-it, and Phi-3.5-mini-instruct models on the MATH500 dataset. As illustrated in Figure 5, our conclusion regarding the existence of sparse value neurons remains valid for opensource models with different architectures. 3.5. Transferability Across Different Datasets If the reward subsystem is an intrinsic property of LLMs, it is expected that the positions of the identified value neurons for the same model remain consistent across different datasets. We evaluate this consistency by computing the Intersection over Union (IoU) of value neurons identified on any two datasets at given pruning ratio. The IoU is defined as the ratio of the number of shared value neurons to the size of the union of the two neuron sets. higher IoU value indicates greater degree of overlap between the value neurons extracted from different datasets. We introduce random baseline to represent the expected As illustrated in Figure 7, the IoU for value neurons remains 5 Sparse Reward Subsystem in Large Language Models (negative surprise), and period of high activation when the model initially predicts low value but ultimately succeeds (positive surprise). To identify these dopamine neurons, we utilize the Qwen-2.5-14B-SimpleRL-Zoo model and evaluate it on the MATH500 dataset. We select 50 such neurons within each layer of the LLM; the method is detailed in Appendix E. To verify that these neurons are indeed dopamine neurons, we visualize their activation trajectories across problems involving positive and negative surprises to examine whether the peak activation and suppression in the curves correspond to the prediction errors encountered during inference. We illustrate the characteristic behavior of these neurons using the 1517-th neuron in layer 5 as representative example. In Appendix F, we provide additional visualization cases across broader range of datasets. As illustrated in Figure 8, this neuron encounters both positive surprise and negative surprise. In the case of the positive surprise shown in Figure 8(a), the model initially exhibits low confidence in completing the task. However, during the inference process (around the 300th token), the model derives key conclusion, resulting in high TD error; consequently, we observe sharp spike in the neurons activation level. Since the subsequent reasoning proceeds steadily, the TD error remains low as the following steps become predictable, leading to the neurons activation returning to relatively low state. Near the end of the trajectory, the model computes the final answer, which triggers second activation peak. These observations suggest that dopamine neurons exhibit higher activation levels when the model acquires unexpected rewards or makes significant progress. Conversely, in the negative surprise shown in Figure 8(b), the model begins with high confidence. During the initial stage of inference, the model follows correct path and even provides the critical modeling logic within the first 400 tokens. However, between the 400th and 600th tokens, the model commits an error, leading to significant negative TD error and corresponding suppression in the neurons activation. In summary, our visualization demonstrates close correlation between the TD error during inference and dopamine neuron activation levels, revealing the existence of dopamine neurons within LLMs. 4.2. Correlation between Value Neurons and Dopamine Neurons Given that value neurons and dopamine neurons both belong to the reward subsystem, we hypothesize that they are functionally interconnected and exert mutual influence. To verify this, we perform an ablation experiment where we zero out the activation values of value neurons in earlier layers and observe the resulting changes in the activation trajectories of dopamine neurons in subsequent layers. As Figure 7. IoU as function of the pruning ratio. The IoU values for value neurons across different models are significantly higher than the random baseline, indicating that models derived from the same base model share substantial number of value neuron positions. consistently higher than the random baseline. This finding indicates close alignment between the value neurons of different models fine-tuned from common base model. 4. Applications of the Value Neurons In this section, we introduce how to leverage value neurons to identify dopamine neurons and characterize the close relationship between them. Specifically, we show that perturbing value neurons causes dopamine neurons to no longer exhibit their properties. As another potential application, we demonstrate in Appendix that value neurons can be utilized to predict model confidence. 4.1. Identifying Dopamine Neurons While high AU C(V (s0), r(sT )) indicates that value neurons possess certain predictive capacity for the value of the current state, LLMs are not perfect reasoners. Consequently, discrepancies may arise: model might assign high value upon seeing only the initial problem (s0) but ultimately fail to obtain reward, or conversely, predict low initial value yet successfully solve the task. Similar phenomena are observed in biological systems; humans and other primates may encounter unexpected stimuli or rewards that diverge from the predictions of their value neurons. In such instances, the prediction of the value neurons is said to incur significant temporal difference (TD) error. In the human brain, specific neurons known as dopamine neurons specialize in capturing these unexpected prediction errors. In this section, we investigate whether analogous neurons exist within the hidden states of LLMs. Specifically, we select instances with substantial prediction errors and analyze the variations in neuronal activation levels during the models inference process. Neurons are identified as dopamine neurons if they exhibit consistent activation patterns corresponding to prediction errors: specifically, displaying period of low activation when the model initially predicts high value but fails to obtain reward 6 Sparse Reward Subsystem in Large Language Models Figure 8. Dopamine neurons encode information regarding the models prediction error for the current state. (a) Positive Surprise: The model initially lacks confidence in answering the problem but ultimately provides the correct solution. This neuron exhibits two significant peaks when the model identifies critical logical step and subsequently derives the final key result. (b) Negative Surprise: Conversely, the model begins with high confidence but fails to solve the problem correctly. The neuron displays distinct trough at the exact moment logical flaw occurs. control, we compare this with the effect of zeroing out an equivalent number of randomly selected neurons in the same layers. If perturbing the value neurons leads to significantly more pronounced alteration in the dopamine neurons activation curves compared to the random perturbation, it would indicate close relationship between value neurons and dopamine neurons. We use the Qwen-2.5-14B-SimpleRL-Zoo model evaluated on the MATH500 dataset, and compare the effects of zeroing out the top 20% of value neurons (identified by the highest L1 norm in the value probe) versus zeroing out 20% of randomly selected neurons. We then analyze the deviations in the normalized activation levels of the 1517-th neuron in layer 5 relative to its original trajectory. As shown in Figure 9, while the blue line (random ablation) remains largely consistent with the original yellow curve aside from minor numerical fluctuations, the red line (value neuron ablation) exhibits fundamental differences. Specifically, the positions of the activation peaks and troughs shift significantly, causing the neuron to lose the characteristic properties of dopamine neuron encoding prediction error. These results demonstrate that value and dopamine neurons are closely related, as disrupting small subset of value neurons is sufficient to significantly impair the predictive performance of dopamine neurons. 5. Related Work 5.1. Probing Methods in Large Language Models Probing methods serve as powerful tools for investigating and interpreting the internal characteristics of Large Language Models (LLMs) and have been extensively utilized within the research community. Cencerrado et al., 2025 demonstrated that linear probes can be trained to predict the correctness of models forthcoming answer. Expanding beyond linear analysis, Diego-Simon et al., 2024 introduced polar probe designed to extract syntactic relations by analyzing both the distance and direction between word embeddings. Probing has also been instrumental in assessing the veracity of generated content. Marks & Tegmark, 2024 7 Sparse Reward Subsystem in Large Language Models enabling the accurate answering of yes-no questions using only model activations. Hajrullahu, 2025 identified significant positive correlation between the statistics of models hidden layers and the correctness of its final output, noting that larger models exhibit internal states with stronger predictive power regarding rewards. Zhang et al., 2025b utilized tools from mechanistic interpretability to analyze model activations, employing Sparse Autoencoders (SAEs) to map high-dimensional hidden states onto interpretable semantic features. Similarly, the RISE framework (Liu et al., 2025) proposes simultaneously enhancing models problem-solving and self-verification capabilities within single training process, requiring the model to generate both solution and corresponding evaluative score. Furthermore, Zhao et al., 2025 introduced Reinforcement Learning from Internal Feedback (RLIF), framework that enables LLMs to learn from intrinsic signals in the absence of external rewards or labeled data. Similarly, LaSeR (Yang et al., 2025) found that last-token self-rewarding score can guide the reinforcement learning process. Du et al., 2025 observed that the latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, allowing latent classifier to predict answer correctness directly from these representations. Guo et al., 2025b propose the SWIFT method, demonstrating that mining intrinsic rewards from LLM hidden states facilitates efficient Best-of-N sampling. Despite these attempts to predict rewards from internal representations, existing literature has yet to identify that the hidden states utilized for reward prediction are highly sparse and constitute distinct reward subsystem. 6. Conclusion In this work, we investigate the intrinsic properties of LLM hidden states. We find that small subset of neurons within LLMs forms sparse reward subsystem, consisting of value neurons that represent the models value expectation for the current state and dopamine neurons that encode the Reward Prediction Error (RPE). We further observe that this subsystem maintains high consistency across various tasks, layers, model scales, and architectural designs, and we demonstrate its potential applications. Looking ahead, there are several promising directions for future research. First, the applications of the sparse reward subsystem can be further explored, particularly in detecting and guiding the generation and reasoning processes of LLMs. Second, while we currently demonstrate the existence of dopamine neurons primarily through case studies, future work could attempt to measure them via quantitative metrics. We believe that this work facilitates deeper understanding of the intrinsic properties of LLM hidden states, providing valuable insights to the community. Figure 9. The activation curves of dopamine neuron across states under different ablation conditions. The yellow line represents the original trajectory, the blue line shows the result of zeroing 20% random neurons, and the red line depicts the result of zeroing the top 20% value neurons. While random ablation has minimal impact on the overall trend, value neuron ablation significantly alters the trajectory of the curve. This indicates close correlation between value and dopamine neurons. found that LLMs linearly represent the truth or falsehood of factual statements. This is supported by Han et al., 2025, who noted that LLM hidden states are highly predictive of factuality in long-form natural language generation, and that such information can be efficiently extracted at inference time using lightweight probe. Liang & Wang, 2025 employed lightweight MLP probes to perform nonlinear modeling of high-level hidden states for token-level hallucination detection. Gao et al., 2025 identified that sparse subset of neurons within LLMs can reliably predict the occurrence of hallucinations. Furthermore, Heindrich et al., 2025 observed that simple probing methods demonstrate superior generalization on Out-of-Distribution (OOD) tasks compared to Sparse Autoencoders (SAEs). Following Zhu et al., 2025, our study employs simple two-layer linear network as our probing model to maintain interpretability while capturing the necessary reward-related signals. 5.2. Reward Modeling Based on LLM Internal Representations Several recent studies have investigated the extraction of reward signals from the internal representations of Large Language Models (LLMs). Anthropic proposes value head to predict whether models can answer questions correctly (Kadavath et al., 2022). Burns et al., 2023 utilize purely unsupervised approach to discover latent knowledge within the internal activations of language model, 8 Sparse Reward Subsystem in Large Language Models"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., PerezBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical report, 2023. URL https://arxiv.org/ abs/2309.16609. Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=ETKGuby0hcs. Cencerrado, I. V. M., Masdemont, A. P., Hawthorne, A. G., Africa, D. D., and Pacchiardi, L. No answer needed: Predicting llm answer accuracy from question-only linear probes, 2025. URL https://arxiv.org/abs/ 2509.10625. Chen, J., Hu, S., Liu, Z., and Sun, M. States hidden in hidden states: Llms emerge discrete state representations implicitly, 2024. URL https://arxiv.org/abs/ 2407.11421. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/ 1803.05457. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Diego-Simon, P., DAscoli, S., Chemla, E., Lakretz, Y., and King, J.-R. polar coordinate system represents syntax in large language models, 2024. URL https: //arxiv.org/abs/2412.05571. Du, H., Dong, Y., and Ning, X. Latent thinking optimization: Your latent reasoning language model secretly encodes reward signals in its latent thoughts, 2025. URL https: //arxiv.org/abs/2509.26314. Gao, C., Chen, H., Xiao, C., Chen, Z., Liu, Z., and Sun, M. H-neurons: On the existence, impact, and origin of hallucination-associated neurons in llms, 2025. URL https://arxiv.org/abs/2512.01797. Gekhman, Z., Ben-David, E., Orgad, H., Ofek, E., Belinkov, Y., Szpektor, I., Herzig, J., and Reichart, R. Inside-out: In Second ConHidden factual knowledge in LLMs. ference on Language Modeling, 2025. URL https: //openreview.net/forum?id=f7GG1MbsSM. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, 9 Sparse Reward Subsystem in Large Language Models G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., 10 Sparse Reward Subsystem in Large Language Models Giorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and Li, Y. Textbooks are all you need, 2023. URL https://arxiv.org/abs/2306.11644. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Ding, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Chen, J., Yuan, J., Tu, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., You, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Zhou, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, September 2025a. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http://dx. doi.org/10.1038/s41586-025-09422-z. Guo, J., Wu, Z., Yang, H., and Yu, P. S. Mining intrinsic rewards from llm hidden states for efficient best-of-n sampling, 2025b. URL https://arxiv.org/abs/ 2505.12225. Hajrullahu, I. for thesis, Learning from within: Hidden-state training llms. Masrewards Ludwig-Maximilians-Universitat URL https: dynamics as ters Munchen, //assets-8291.accso.de/downloads/ Masterarbeit_Ilir_Hajrullahu_2025.pdf. Supervised by Dr. Yunpu Ma. September 2025. Han, J., Band, N., Razzak, M., Kossen, J., Rudner, T. G. J., and Gal, Y. Simple factuality probes detect hallucinations in long-form natural language generation. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 1620916226, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-335-7. doi: 10.18653/v1/2025.findings-emnlp. 880. URL https://aclanthology.org/2025. findings-emnlp.880/. Heindrich, L., Torr, P., Barez, F., and Thost, V. Do sparse autoencoders generalize? case study of answerability. In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025. URL https: //openreview.net/forum?id=rs3alQ5BV8. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask In International Conference language understanding. on Learning Representations, 2021. URL https:// openreview.net/forum?id=d7KBjmI3GmQ. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, S., Fort, S., Ganguli, D., Hernandez, D., Jacobson, J., Kernion, J., Kravec, S., Lovitt, L., Ndousse, K., Olsson, C., Ringer, S., Amodei, D., Brown, T., Clark, J., Joseph, N., Mann, B., McCandlish, S., Olah, C., and Kaplan, J. Language models (mostly) know what they know, 2022. URL https://arxiv.org/abs/2207.05221. Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=IFXTZERXdM7. Liang, S. and Wang, H. Neural probe-based hallucination detection for large language models, 2025. URL https: //arxiv.org/abs/2512.20949. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step, 2023. URL https: //arxiv.org/abs/2305.20050. Liu, X., Liang, T., He, Z., Xu, J., Wang, W., He, P., Tu, Z., Mi, H., and Yu, D. Trust, but verify: self-verification approach to reinforcement learning with verifiable rewards. In The Thirty-ninth Annual Conference on Neural 11 Sparse Reward Subsystem in Large Language Models Information Processing Systems, 2025. URL https: //openreview.net/forum?id=gA3fFAEXNT. Marks, S. and Tegmark, M. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=aajyHYjjsk. Oh, J., Shin, S., and Oh, D. House of cards: Massive weights in llms, 2025. URL https://arxiv.org/ abs/2410.01866. Padoa-Schioppa, C. and Assad, J. A. Neurons in the orbitofrontal cortex encode economic value. Nature, 441 (7090):223226, 2006. Schultz, W. Predictive reward signal of dopamine neurons. Journal of neurophysiology, 1998. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., Nathan, A., Luo, A., Helyar, A., Madry, A., Efremov, A., Spyra, A., Baker-Whitcomb, A., Beutel, A., Karpenko, A., Makelov, A., Neitz, A., Wei, A., Barr, A., Kirchmeyer, A., Ivanov, A., Christakis, A., Gillespie, A., Tam, A., Bennett, A., Wan, A., Huang, A., Sandjideh, A. M., Yang, A., Kumar, A., Saraiva, A., Vallone, A., Gheorghe, A., Garcia, A. G., Braunstein, A., Liu, A., Schmidt, A., Mereskin, A., Mishchenko, A., Applebaum, A., Rogerson, A., Rajan, A., Wei, A., Kotha, A., Srivastava, A., Agrawal, A., Vijayvergiya, A., Tyra, A., Nair, A., Nayak, A., Eggers, B., Ji, B., Hoover, B., Chen, B., Chen, B., Barak, B., Minaiev, B., Hao, B., Baker, B., Lightcap, B., McKinzie, B., Wang, B., Quinn, B., Fioca, B., Hsu, B., Yang, B., Yu, B., Zhang, B., Brenner, B., Zetino, C. R., Raymond, C., Lugaresi, C., Paz, C., Hudson, C., Whitney, C., Li, C., Chen, C., Cole, C., Voss, C., Ding, C., Shen, C., Huang, C., Colby, C., Hallacy, C., Koch, C., Lu, C., Kaplan, C., Kim, C., Minott-Henriques, C., Frey, C., Yu, C., Czarnecki, C., Reid, C., Wei, C., Decareaux, C., Scheau, C., Zhang, C., Forbes, C., Tang, D., Goldberg, D., Roberts, D., Palmie, D., Kappler, D., Levine, D., Wright, D., Leo, D., Lin, D., Robinson, D., Grabb, D., Chen, D., Lim, D., Salama, D., Bhattacharjee, D., Tsipras, D., Li, D., Yu, D., Strouse, D., Williams, D., Hunn, D., Bayes, E., Arbus, E., Akyurek, E., Le, E. Y., Widmann, E., Yani, E., Proehl, E., Sert, E., Cheung, E., Schwartz, E., Han, E., Jiang, E., Mitchell, E., Sigler, E., Wallace, E., Ritter, E., Kavanaugh, E., Mays, E., Nikishin, E., Li, F., Such, F. P., de Avila Belbute Peres, F., Raso, F., Bekerman, F., Tsimpourlas, F., Chantzis, F., Song, F., Zhang, F., Raila, G., McGrath, G., Briggs, G., Yang, G., Parascandolo, G., Chabot, G., Kim, G., Zhao, G., Valiant, G., Leclerc, G., Salman, H., Wang, H., Sheng, H., Jiang, H., Wang, H., Jin, H., Sikchi, H., Schmidt, H., Aspegren, 12 H., Chen, H., Qiu, H., Lightman, H., Covert, I., Kivlichan, I., Silber, I., Sohl, I., Hammoud, I., Clavera, I., Lan, I., Akkaya, I., Kostrikov, I., Kofman, I., Etinger, I., Singal, I., Hehir, J., Huh, J., Pan, J., Wilczynski, J., Pachocki, J., Lee, J., Quinn, J., Kiros, J., Kalra, J., Samaroo, J., Wang, J., Wolfe, J., Chen, J., Wang, J., Harb, J., Han, J., Wang, J., Zhao, J., Chen, J., Yang, J., Tworek, J., Chand, J., Landon, J., Liang, J., Lin, J., Liu, J., Wang, J., Tang, J., Yin, J., Jang, J., Morris, J., Flynn, J., Ferstad, J., Heidecke, J., Fishbein, J., Hallman, J., Grant, J., Chien, J., Gordon, J., Park, J., Liss, J., Kraaijeveld, J., Guay, J., Mo, J., Lawson, J., McGrath, J., Vendrow, J., Jiao, J., Lee, J., Steele, J., Wang, J., Mao, J., Chen, K., Hayashi, K., Xiao, K., Salahi, K., Wu, K., Sekhri, K., Sharma, K., Singhal, K., Li, K., Nguyen, K., Gu-Lemberg, K., King, K., Liu, K., Stone, K., Yu, K., Ying, K., Georgiev, K., Lim, K., Tirumala, K., Miller, K., Ahmad, L., Lv, L., Clare, L., Fauconnet, L., Itow, L., Yang, L., Romaniuk, L., Anise, L., Byron, L., Pathak, L., Maksin, L., Lo, L., Ho, L., Jing, L., Wu, L., Xiong, L., Mamitsuka, L., Yang, L., McCallum, L., Held, L., Bourgeois, L., Engstrom, L., Kuhn, L., Feuvrier, L., Zhang, L., Switzer, L., Kondraciuk, L., Kaiser, L., Joglekar, M., Singh, M., Shah, M., Stratta, M., Williams, M., Chen, M., Sun, M., Cayton, M., Li, M., Zhang, M., Aljubeh, M., Nichols, M., Haines, M., Schwarzer, M., Gupta, M., Shah, M., Huang, M., Dong, M., Wang, M., Glaese, M., Carroll, M., Lampe, M., Malek, M., Sharman, M., Zhang, M., Wang, M., Pokrass, M., Florian, M., Pavlov, M., Wang, M., Chen, M., Wang, M., Feng, M., Bavarian, M., Lin, M., Abdool, M., Rohaninejad, M., Soto, N., Staudacher, N., LaFontaine, N., Marwell, N., Liu, N., Preston, N., Turley, N., Ansman, N., Blades, N., Pancha, N., Mikhaylin, N., Felix, N., Handa, N., Rai, N., Keskar, N., Brown, N., Nachum, O., Boiko, O., Murk, O., Watkins, O., Gleeson, O., Mishkin, P., Lesiewicz, P., Baltescu, P., Belov, P., Zhokhov, P., Pronin, P., Guo, P., Thacker, P., Liu, Q., Yuan, Q., Liu, Q., Dias, R., Puckett, R., Arora, R., Mullapudi, R. T., Gaon, R., Miyara, R., Song, R., Aggarwal, R., Marsan, R., Yemiru, R., Xiong, R., Kshirsagar, R., Nuttall, R., Tsiupa, R., Eldan, R., Wang, R., James, R., Ziv, R., Shu, R., Nigmatullin, R., Jain, S., Talaie, S., Altman, S., Arnesen, S., Toizer, S., Toyer, S., Miserendino, S., Agarwal, S., Yoo, S., Heon, S., Ethersmith, S., Grove, S., Taylor, S., Bubeck, S., Banesiu, S., Amdo, S., Zhao, S., Wu, S., Santurkar, S., Zhao, S., Chaudhuri, S. R., Krishnaswamy, S., Shuaiqi, Xia, Cheng, S., Anadkat, S., Fishman, S. P., Tobin, S., Fu, S., Jain, S., Mei, S., Egoian, S., Kim, S., Golden, S., Mah, S., Lin, S., Imm, S., Sharpe, S., Yadlowsky, S., Choudhry, S., Eum, S., Sanjeev, S., Khan, T., Stramer, T., Wang, T., Xin, T., Gogineni, T., Christianson, T., Sanders, T., Patwardhan, T., Degry, T., Shadwell, T., Fu, T., Gao, T., Garipov, T., Sriskandarajah, T., Sherbakov, T., Kaftan, T., Hiratsuka, T., Wang, T., Song, T., Zhao, T., PeterSparse Reward Subsystem in Large Language Models son, T., Kharitonov, V., Chernova, V., Kosaraju, V., Kuo, V., Pong, V., Verma, V., Petrov, V., Jiang, W., Zhang, W., Zhou, W., Xie, W., Zhan, W., McCabe, W., DePue, W., Ellsworth, W., Bain, W., Thompson, W., Chen, X., Qi, X., Xiang, X., Shi, X., Dubois, Y., Yu, Y., Khakbaz, Y., Wu, Y., Qian, Y., Lee, Y. T., Chen, Y., Zhang, Y., Xiong, Y., Tian, Y., Cha, Y., Bai, Y., Yang, Y., Yuan, Y., Li, Y., Zhang, Y., Yang, Y., Jin, Y., Jiang, Y., Wang, Y., Wang, Y., Liu, Y., Stubenvoll, Z., Dou, Z., Wu, Z., and Wang, Z. Openai gpt-5 system card, 2025. URL https://arxiv.org/abs/2601.03267. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., Tafti, P., Hussenot, L., Sessa, P. G., Chowdhery, A., Roberts, A., Barua, A., Botev, A., Castro-Ros, A., Slone, A., Heliou, A., Tacchetti, A., Bulanova, A., Paterson, A., Tsai, B., Shahriari, B., Lan, C. L., Choquette-Choo, C. A., Crepy, C., Cer, D., Ippolito, D., Reid, D., Buchatskaya, E., Ni, E., Noland, E., Yan, G., Tucker, G., Muraru, G.- C., Rozhdestvenskiy, G., Michalewski, H., Tenney, I., Grishchenko, I., Austin, J., Keeling, J., Labanowski, J., Lespiau, J.-B., Stanway, J., Brennan, J., Chen, J., Ferret, J., Chiu, J., Mao-Jones, J., Lee, K., Yu, K., Millican, K., Sjoesund, L. L., Lee, L., Dixon, L., Reid, M., Mikuła, M., Wirth, M., Sharman, M., Chinaev, N., Thain, N., Bachem, O., Chang, O., Wahltinez, O., Bailey, P., Michel, P., Yotov, P., Chaabouni, R., Comanescu, R., Jana, R., Anil, R., McIlroy, R., Liu, R., Mullins, R., Smith, S. L., Borgeaud, S., Girgin, S., Douglas, S., Pandya, S., Shakeri, S., De, S., Klimenko, T., Hennigan, T., Feinberg, V., Stokowiec, W., hui Chen, Y., Ahmed, Z., Gong, Z., Warkentin, T., Peran, L., Giang, M., Farabet, C., Vinyals, O., Dean, J., Kavukcuoglu, K., Hassabis, D., Ghahramani, Z., Eck, D., Barral, J., Pereira, F., Collins, E., Joulin, A., Fiedel, N., Senter, E., Andreev, A., and Kenealy, K. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Grill, J., Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., Liu, G., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsitsulin, A., Busa-Fekete, R., Feng, A., Sachdeva, N., Coleman, B., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian, D., Eyal, M., Cherry, C., Peter, J.-T., Sinopalnikov, D., Bhupatiraju, S., Agarwal, R., Kazemi, M., Malkin, D., Kumar, R., Vilar, D., Brusilovsky, I., Luo, J., Steiner, A., Friesen, A., Sharma, A., Sharma, A., Gilady, A. M., Goedeckemeyer, A., Saade, A., Feng, A., Kolesnikov, A., Bendebury, A., Abdagic, A., Vadi, A., Gyorgy, A., Pinto, A. S., Das, A., Bapna, A., Miech, A., Yang, A., Paterson, A., Shenoy, A., Chakrabarti, A., Piot, B., Wu, B., Shahriari, B., Petrini, B., Chen, C., Lan, C. L., Choquette-Choo, C. A., Carey, C., Brick, C., Deutsch, D., Eisenbud, D., Cattle, D., Cheng, D., Paparas, D., Sreepathihalli, D. S., Reid, D., Tran, D., Zelle, D., Noland, E., Huizenga, E., Kharitonov, E., Liu, F., Amirkhanyan, G., Cameron, G., Hashemi, H., Klimczak-Plucinska, H., Singh, H., Mehta, H., Lehri, H. T., Hazimeh, H., Ballantyne, I., Szpektor, I., Nardini, I., Pouget-Abadie, J., Chan, J., Stanton, J., Wieting, J., Lai, J., Orbay, J., Fernandez, J., Newlan, J., yeong Ji, J., Singh, J., Black, K., Yu, K., Hui, K., Vodrahalli, K., Greff, K., Qiu, L., Valentine, M., Coelho, M., Ritter, M., Hoffman, M., Watson, M., Chaturvedi, M., Moynihan, M., Ma, M., Babar, N., Noy, N., Byrd, N., Roy, N., Momchev, N., Chauhan, N., Sachdeva, N., Bunyan, O., Botarda, P., Caron, P., Rubenstein, P. K., Culliton, P., Schmid, P., Sessa, P. G., Xu, P., Stanczyk, P., Tafti, P., Shivanna, R., Wu, R., Pan, R., Rokni, R., Willoughby, R., Vallu, R., Mullins, R., Jerome, S., Smoot, S., Girgin, S., Iqbal, S., Reddy, S., Sheth, S., Poder, S., Bhatnagar, S., Panyam, S. R., Eiger, S., Zhang, S., Liu, T., Yacovone, T., Liechty, T., Kalra, U., Evci, U., Misra, V., Roseberry, V., Feinberg, V., Kolesnikov, V., Han, W., Kwon, W., Chen, X., Chow, Y., Zhu, Y., Wei, Z., Egyed, Z., Cotruta, V., Giang, M., Kirk, P., Rao, A., Black, K., Babar, N., Lo, J., Moreira, E., Martins, L. G., Sanseviero, O., Gonzalez, L., Gleicher, Z., Warkentin, T., Mirrokni, V., Senter, E., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Matias, Y., Sculley, D., Petrov, S., Fiedel, N., Shazeer, N., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Alayrac, J.-B., Anil, R., Dmitry, Lepikhin, Borgeaud, S., Bachem, O., Joulin, A., Andreev, A., Hardin, C., Dadashi, R., and Hussenot, L. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/ abs/2302.13971. Tremblay, L. and Schultz, W. Relative reward preference in primate orbitofrontal cortex. Nature, 398(6729):704708, 1999. Yang, W., Liu, W., Xie, R., Guo, Y., Wu, L., Yang, S., and Lin, Y. Laser: Reinforcement learning with last-token self-rewarding, 2025. URL https://arxiv.org/ abs/2510.14943. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., Investigating and taming and He, J. Simplerl-zoo: zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/ 2503.18892. Sparse Reward Subsystem in Large Language Models Zhang, L., Song, D., Wu, Z., Tian, Y., Zhou, C., Xu, J., Yang, Z., and Zhang, S. Detecting hallucination in large language models through deep internal representation analysis. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, doi: IJCAI 25, 2025a. 10.24963/ijcai.2025/929. URL https://doi.org/ 10.24963/ijcai.2025/929. ISBN 978-1-956792-06-5. Zhang, S., Shi, W., Li, S., Liao, J., Cai, H., and Wang, X. Interpretable reward model via sparse autoencoder, 2025b. URL https://arxiv.org/abs/2508.08746. Zhao, X., Kang, Z., Feng, A., Levine, S., and Song, D. Learning to reason without external rewards, 2025. URL https://arxiv.org/abs/2505.19590. Zhu, Y., Liu, D., Lin, Z., Tong, W., Zhong, S., and Shao, J. The llm already knows: Estimating llm-perceived question difficulty via hidden representations, 2025. URL https://arxiv.org/abs/2509.12886. 14 A. The Benefit of Using the TD Error Training Objective Sparse Reward Subsystem in Large Language Models One might naturally question the specific benefits of utilizing Temporal Difference (TD) error training objective as opposed to simply predicting the final reward. To investigate this, we conducted an ablation study where the reward model was trained exclusively on the final reward signal. To compare their effectiveness in identifying value neurons, we evaluated the intervention results of the Qwen-2.5-7B-SimpleRL-Zoo model on the MATH500 dataset, following the methodology described in Section 2.4. Experimental results demonstrate that after zeroing out the same 1% of neurons, the positions identified by the TD-error-trained value probe lead to more severe degradation in the models reasoning capabilities. This suggests that the neurons discovered via TD error are more critical to the underlying reasoning process. Table 2. Intervention results for the Qwen-2.5-7B-SimpleRL-Zoo model on the MATH500 dataset. Performance is measured by accuracy after zeroing out 1% subset of neurons in single layer. Layer Value Neurons (TD) Value Neurons (final) Random Neurons 2 3 4 5 Avg 37.0 (-38.2) 13.6 (-61.6) 29.4 (-45.8) 1.2 (-74.0) 20.3 (-54.9) 74.8 (-0.4) 69.0 (-6.2) 53.6 (-21.6) 68.2 (-7.0) 66.4 (-8.8) 77.0 (+1.8) 73.4 (-1.8) 73.8 (-1.4) 74.4 (-0.8) 74.6 (-0.6) B. Hyperparameters Response generation is performed on two NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Both training and inference are conducted on single NVIDIA RTX PRO 6000 Blackwell Server Edition GPU. B.1. Response Generation Hyperparameters Table 3. Hyperparameters used for response generation. Category Hyperparameter = Value Generation max tokens per call = 16000; temperature = 1.0; top = 0.95; sampling = 1; prompt type = qwen-boxed Inference use vllm = True; gpu memory utilization = 0.75; B.2. Value Probe Training Hyperparameters Table 4. Hyperparameters used for value probe training. Category Hyperparameter = Value Optimization optimizer = AdamW; learning rate = 1e-4; weight decay = 0.01 Training num epochs = 100; batch size = 4; train ratio = 0.8; gamma = 11e-5; seed = 0 Architecture hidden size 1024 1 (two-layer MLP) B.3. AUC Curve Evaluation Hyperparameters Table 5. Hyperparameters used for reward subspace evaluation with neuron pruning. Category Hyperparameter = Value Evaluation batch size = 4; train ratio = 0.8; seed = Pruning prune ratios = {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.92, 0.94, 0.96, 0.98, 0.99}; 15 C. Derivation of the IoU Curve for the Random Baseline Sparse Reward Subsystem in Large Language Models Let denote the total number of neurons and [0, 1) denote the pruning ratio. Each set contains = (1 p)N neurons selected uniformly at random. For two independently sampled sets and B, each of size k, the intersection size B follows hypergeometric distribution: where [max(0, 2k ), k]. The expected IoU is given by: (A = i) = (cid:0)k (cid:1) (cid:1)(cid:0)N ki (cid:1) (cid:0)N E [IoU] = (cid:88) i=max(0,2kN ) (cid:0)k (cid:1) (cid:1)(cid:0)N ki (cid:1) (cid:0)N 2k D. Transferability Across Different Datasets: More IoU Curves In this section, we provide additional evidence regarding the transferability of value neurons across different datasets. As illustrated in Figure 10 and Figure 11, we utilize layer 2 and layer 4 of the Qwen-2.5-14B-SimpleRL-Zoo model and compute pairwise IoU curves as function of the pruning ratio for the GSM8K, MATH500, Minerva Math, ARC, and MMLU STEM datasets. We then plot the average of all these pairwise curves. In these results, the IoU curves for any two datasets consistently exceed the random baseline. Furthermore, the observation that many IoU curves exhibit significant upward trend as the pruning ratio approaches 1 remains valid for these layers as well. Figure 10. IoU as function of the pruning ratio. The IoU values for value neurons across different datasets are significantly higher than the random baseline, indicating that for the same LLM, the positions of identified value neurons are closely correlated across tasks. Figure 11. IoU as function of the pruning ratio. The IoU values for value neurons across different datasets are significantly higher than the random baseline, indicating that for the same LLM, the positions of identified value neurons are closely correlated across tasks. 16 E. Detailed Procedures for Identifying Dopamine Neurons Sparse Reward Subsystem in Large Language Models To characterize these neurons quantitatively, we analyze the hidden state dynamics across the response trajectory. Let z(s) (t) denote the z-score normalized activation of the i-th neuron at token position for given sample s. To suppress stochastic noise, we first apply Gaussian filter to the activation trace: where Gσ is Gaussian kernel with standard deviation σ. Subsequently, to identify sustained shifts in predicted score, we apply sliding window average of length to the smoothed trace ˆz: t,i = (z(s) ˆz(s) Gσ)(t) (3) z(s) t,i ="
        },
        {
            "title": "1\nW",
            "content": "t+W/2 (cid:88) ˆz(s) k,i k=tW/2 (4) We define two subsets of samples representing significant TD errors based on the quantiles of initial value predictions (s0): the positive surprise set Spos (unexpected success: low (s0) but reward = 1) and the negative surprise set Sneg (unexpected failure: high (s0) but reward = 0). For each neuron i, we extract the peak sustained activation in the positive surprise set and the peak sustained suppression in the negative surprise set: Pi,sSpos = max z(s) t,i , Ni,sSneg = min z(s) t,i (5) Notably, we applied z-score normalization to all activation curves. This normalization ensures that the and values accurately reflect peak activation and suppression, and also allows for direct comparability across the curves of different neurons. Following the biological intuition of Reward Prediction Error (RPE), candidate dopamine neuron should exhibit an upward spike for unexpected success and downward dip for unexpected failure. We therefore define the Dopamine Score Di for neuron as the difference between the median peak activations across these surprise sets: Di = median (cid:0){Pi,s}sSpos (cid:1) median (cid:0){Ni,s}sSneg (cid:1) (6) To ensure the robustness, we further constrain our selection to neurons that satisfy median (cid:0){Pi,s}sSpos (cid:1) < 0. Thus, neurons with higher dopamine score are likely to be dopamine neurons. median (cid:0){Ni,s}sSneg We calculate the dopamine score for each neuron and observe the patterns of activation trajectories across problems involving positive and negative surprises for those with high scores. Our results reveal that many high-scoring neurons exhibit the distinct characteristics of dopamine neurons; specifically, the peak activation and suppression in their curves correspond to the prediction errors encountered during the inference process. We select the top 50 neurons in each layer as dopamine neurons. (cid:1) > 0 and Hyperparameters. peak window is set to 20. In our experiments, we set the Gaussian smoothing window to 12 with Gaussian sigma of 6, and the F. Further Evidence for the Characteristics of Dopamine Neurons To verify the robustness of the dopamine neurons existence, we conduct further experiments using the Minerva Math dataset. We maintain the previously established experimental setup, selecting the top 50 neurons in each layer as dopamine neurons on the Minerva Math dataset. Notably, we observe significant overlap between these dopamine neurons and those identified earlier on the MATH500 dataset; in particular, the 1517-th neuron in layer 5 remains among the dopamine neurons. Consequently, we continue to investigate the predictive capacity of this neuron. Our findings confirm that it consistently displays period of low activation when the model initially predicts high value but fails to obtain reward, and period of high activation when the model initially predicts low value but ultimately succeeds. Sparse Reward Subsystem in Large Language Models Figure 12. Dopamine neurons encode information regarding the models prediction error for the current state. (a) Positive Surprise: The model initially lacks confidence in answering the problem but ultimately provides the correct solution. This neuron exhibits significant peak when the model identifies critical logical step and subsequently derives the final key result. (b) Negative Surprise: Conversely, the model begins with high confidence but fails to solve the problem correctly. The neuron displays distinct trough at the exact moment wrong step occurs. As illustrated in Figure 12, this neuron encounters both positive surprise and negative surprise. In the case of the positive surprise shown in Figure 12(a), the model initially exhibits low confidence in completing the task. However, during the inference process (around the 200th token), the model derives key conclusion, resulting in high TD error; consequently, we observe sharp spike in the neurons activation level. Since the subsequent reasoning proceeds steadily, the TD error remains low as the following steps become predictable, leading to the neurons activation returning to relatively low state. These observations suggest that dopamine neurons exhibit higher activation levels when the model acquires unexpected rewards or makes significant progress. Conversely, in the negative surprise shown in Figure 12(b), the model begins with high confidence. During the initial stage of inference, the model follows correct path and even provides the critical modeling logic within the first 300 tokens. However, between the 300th and 400th tokens, the model commits an error, leading to significant negative TD error and corresponding suppression in the neurons activation. Thus, our visualization on the Minerva Math dataset consistently demonstrates close correlation between the TD error during inference and neuronal activation levels, further substantiating the fundamental properties of dopamine neurons. G. Using Value Neurons to Measure Model Confidence Sparse Reward Subsystem in Large Language Models Since the value neurons demonstrates predictive discriminative power even before the model generates specific response, it can be utilized to assess model confidence. The ability to estimate confidence prior to generation is of significant practical value. First, as modern reasoning models typically produce very long responses, gauging confidence beforehand allows for preliminary understanding of the models certainty without consuming the substantial computational resources required for full inference. Second, this mechanism enables the design of adaptive strategies, such as dynamically allocating compute and thinking time based on the initial confidence level. In this section, we aim to evaluate the Spearman correlation coefficient between the predicted scores estimated by the model for each question in the validation set and the models corresponding avg@32 accuracy for those questions. Figure 13. Spearman correlation between the predicted value and the models avg@32 accuracy on the MATH500 dataset. The high correlation coefficients demonstrate the accuracy of the method in predicting model confidence. As shown in Figure 13, we visualize the Spearman correlation between the predicted value results of the Qwen-2.5-7B-SimpleRL-Zoo model and the models avg@32 accuracy on the MATH500 dataset. We observe high Spearman correlation, indicating that our method serves as an effective criterion for assessing confidence prior to response generation. Furthermore, the fact that the Spearman correlation increases after certain level of pruning provides further evidence for the existence of the reward subsystem. variety of existing works utilize alternative methods to predict model confidence. We compare the performance of these approaches on the MATH 500 dataset. For detailed configurations of the baselines, please refer to Appendix H. Table 6. Comparison of Spearman correlation coefficients for different confidence prediction methods. Method Verbalized Confidence Next-token Confidence LCD (Cencerrado et al., 2025) Value Neurons (Ours) Spearman Correlation 0.08 0.09 0.46 0.47 From Table 6, we observe that the prediction method based on value neurons achieves performance comparable to other baselines that utilize significantly larger set of neurons. It is noteworthy that predicting confidence is orthogonal to the core contribution of this paper, which is the reward subsystem. One could also explore using these baseline value probes to localize and characterize the reward subsystem. H. Model Confidence Baseline Setup In this straightforward baseline, we input the question into the LLM and use the prompt, nn Verbalized Confidence. Rate your confidence in answering this question correctly from 0 to 1 (where 0 means no confidence and 1 means complete confidence). Only output single number between 0 and 1. Do not try to solve the question. to elicit confidence score. If the model fails to produce valid numerical output, we resample until score is obtained. The Spearman correlation coefficient for this method is only 0.08. 19 Sparse Reward Subsystem in Large Language Models In this baseline, we utilize the log probability (log p) of the most likely token at the first generated Next-token Confidence. position as metric for the models confidence. We then compute its Spearman correlation with the models avg@32 accuracy, yielding coefficient of only 0.09. Latent Correctness Direction (LCD). We re-implement the method proposed by Cencerrado et al., 2025 within our experimental setting. This baseline achieves Spearman correlation of 0.45 at Layer 3 and 0.46 at Layer 5. It is noteworthy that the baseline method utilizes the full set of neurons for prediction, so it cannot provide any guidance regarding the existence of the value neurons or the localization of neurons. In contrast, our approach utilize significantly fewer hidden state dimensions than the baseline."
        }
    ],
    "affiliations": [
        "Stanford University",
        "Tsinghua University"
    ]
}