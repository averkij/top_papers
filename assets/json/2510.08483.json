{
    "paper_title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
    "authors": [
        "Shangqing Tu",
        "Yaxuan Li",
        "Yushi Bai",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/"
        },
        {
            "title": "Start",
            "content": "DeepPrune: Parallel Scaling without Inter-trace Redundancy Shangqing Tu1,Yaxuan Li2, Yushi Bai1, Lei Hou1, Juanzi Li1 1Tsinghua University, 2ShanghaiTech University https://deepprune.github.io 5 2 0 2 9 ] . [ 1 3 8 4 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Parallel scaling has emerged as powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancyour analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, novel framework that enables efficient parallel scaling through dynamic pruning. Our method features specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (OpenAI, 2024; Anthropic, 2024; Reid et al., 2024) have made remarkable progress on reasoning tasks (Guo et al., 2025; Team et al., 2025; Zeng et al., 2025), especially when equipped with long Chain-ofThoughts (CoT) that can mimic humans thinking processes (Wei et al., 2022; Sprague et al., 2024). *Equal contribution. 1 Figure 1: DeepPrune conducts early stopping based on the similarity between reasoning treaces to enhance the efficiency of parallel scaling and save diverse traces. This advancement is driven by inference-time scaling (Jaech et al., 2024), new paradigm that enhances LLMs reasoning capabilities via more computing in the test stage (Snell et al., 2025). Generally, there are two types of inferencetime scaling: sequential scaling and parallel scaling (Venkatraman et al., 2025). Sequential scaling (Muennighoff et al., 2025) focuses on increasing the computation in one reasoning trace like expanding the output length to 128k. While parallel scaling (e.g. best-of-n sampling) encourages generating multiple reasoning traces simultaneously, further pushing the total token cost to 100M or higher (Moshkov et al., 2025). However, beneath these advances lies practical question: How to achieve high performance with low token cost? Existing efficient reasoning methods mainly focus on alleviating the over-thinking of sequential scaling (Chen et al., 2024b; Hou et al., 2025; Zhang et al., 2025). There are few works designed for parallel scaling (Madaan et al., 2025), which typically adopt the LLMs internal signal like confidence (Fu et al., 2025b) for early stopping to improve the sampling efficiency. However, these confidence-based methods suffer from two fundamental limitations: (1) they fail to reduce redundancy between parallel reasoning paths, and (2) they risk prematurely terminating correct reasoning traces. In this paper, we propose DeepPrune, as shown in Figure 1, novel method that proactively prunes redundant parallel CoTs while preserving traces with diverse answers. Our approach is motivated by key observation from preliminary experiments: approximately 80% of parallel reasoning traces yield identical final answers, while only 20% produce distinct solutions. This reveals significant redundancy in current parallel reasoning paradigms. We further investigate whether early-stage trace similarity can predict final answer equivalence. Surprisingly, shallow semantic similarity measures (e.g., SentenceBERT on first 500 tokens) achieve only random-level performance (AUROC=0.58), while deeper LLM-based comparison (Qwen3-4B-Instruct) shows moderate improvement (AUROC=0.66) but remains suboptimal for practical deployment. This finding underscores the necessity for specialized models capable of understanding reasoning processes at deeper level. Inspired by this analysis, we train LLM-based judge model that predicts redundancy between truncated reasoning traces. To enable accurate early stopping, we explore two truncation strategies including fixed-length prefixes and reasoning-step aligned segments. To address class imbalance and preserve answer diversity, we employ focal loss and oversampling techniques for training the judge model. For efficient online inference, we design greedy clustering algorithm that dynamically prunes redundant paths during generation. We conduct comprehensive experiments to prove the effectiveness of DeepPrune across diverse settings. In offline evaluation, our judge model achieves an average AUROC of 0.8701 and TNR of 0.8186 at FNR=0.2 when using reasoning words with focal loss and oversampling, significantly outperforming baseline methods. More importantly, in online reasoning tasks across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and three state-of-the-art reasoning models (DeepSeek-8B, Qwen3-32B, and GPT-OSS-20B), DeepPrune reduces token consumption over 80% compared to cons@512 which samples 512 traces and conduct majority voting in most cases, while maintaining comparable accuracy (within 3 points). Notably, on AIME25 dataset, DeepPrune achieves up to 91.6% token reduction even with accuracy improvements, substantially outperforming strong baselines like DeepConf (Fu et al., 2025b). Our contributions are threefold: (1) We identify and quantify the pervasive problem of inter-trace redundancy in parallel reasoning, revealing that over 80% of computational resources are wasted on generating equivalent reasoning paths. (2) We propose DeepPrune, novel framework that combines trained judge model with online greedy clustering to efficiently prune redundant reasoning traces while preserving answer diversity. (3) Extensive offline and online experiments show that our method reduces token consumption by up to 95% without compromising accuracy, significantly outperforming existing baselines across multiple reasoning benchmarks and model architectures."
        },
        {
            "title": "2 Related Work",
            "content": "Parallel Scaling. Parallel scaling has emerged as pivotal paradigm for enhancing reasoning performance through concurrent generation of multiple reasoning traces (Chen et al., 2024a; Pan et al., 2025; Zheng et al., 2025). Self-Consistency (Wang et al., 2022) pioneered majority voting over diverse reasoning paths, while Best-of-N sampling (Brown et al., 2024) extended this concept through explicit candidate ranking. There are also tree-based exploration methods like ToT (Yao et al., 2023). Efficient Reasoning. Efficient reasoning methods (Feng et al., 2025; Sui et al., 2025; Liu et al., 2025; Qu et al., 2025) aim to optimize the accuracycompute trade-off during inference (Kang et al., 2025; Srivastava et al., 2025; Li et al., 2025; Zhang et al., 2025; Wen et al., 2025). Prior research has explored reducing token usage in individual reasoning traces, such as through length-conscious finetuning (Liu et al., 2024; Arora and Zanette, 2025; Aggarwal and Welleck, 2025; Xia et al., 2025) or training-free prompting techniques (Renze and Guven, 2024; Han et al., 2024; Xu et al., 2025; Fu et al., 2025a; Aytes et al., 2025). Another line of work improves parallel scaling efficiency by early-stopping redundant samples via confidence estimates (Fu et al., 2025b; Yang et al., 2025b) or by refining aggregation strategies (Wang et al., 2024, 2025b). While these methods address intratrace verbosity or sample quantity reduction, they do not explicitly model redundancy between parallel reasoning paths. Our work directly targets this inter-trace redundancy, enabling proactive pruning while preserving answer diversity. 2 (a) Redundant Traces Distribution (b) ROC of Semantic Similarity (c) ROC of LLM Judge Figure 2: Analysis of Inter-trace Redundancy. (a) Distribution of same vs. different answer pairs of reasoning traces, revealing severe redundancy. (b) ROC curve for shallow semantic similarity (SentenceBERT) to distinguish traces with same answers from those with different ones, which shows limited predictive power (AUROC=0.58). (c) ROC curve for LLM-based deep comparison (Qwen3-4B-Instruct) achieves moderate improvement (AUROC=0.66)."
        },
        {
            "title": "3 Preliminaries",
            "content": "3.1 Problem Definition Given set of parallel reasoning traces S1 = {t1, t2, ..., tn} generated concurrently for the same query, our objective is to reduce inter-trace redundancy while preserving answer diversity. We define pruning process that selects subset of traces: (S1) = S2, where S2 S1 The pruned set S2 should satisfy: (cid:40) S2 = tk1, tk2, . . . , tkm (cid:41) sim(tki, tkj ) < τ, i, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) where sim(tki, tkj ) is the similarity, τ is similarity threshold. The traces in S2 continue reasoning to produce final answers {ok1, ok2, . . . , okm}. To operate this process, we need to model the Similarity function. Since parallel reasoning focuses on answer accuracy, we simplify the judgment to predicting whether two incomplete traces will yield the same final answer. Formally, for any pair of unfinished traces (ti, tj), we predict whether their corresponding results (oi, oj) will be identical, which can be defined as the binary similarity function based on final answer equivalence: sim(ti, tj) = (cid:40) 1 if R(oi, oj) = 1 0 if R(oi, oj) = 0 where R(oi, oj) is the reward function for answer equivalence based on verifable rules or reward models. In this paper, we only consider queries with verifable answers, leaving others for future works. 3.2 Inter-trace Redundancy: The Efficiency Bottleneck of Parallel Reasoning Recent advances in parallel scaling have significantly improved reasoning performance, but at the cost of substantial computational overhead. We identify inter-trace redundancy as the primary efficiency bottleneck: when generating multiple reasoning traces in parallel, large proportion of tokens are wasted on producing semantically equivalent reasoning paths that lead to identical answers. Parallel Reasoning Trace Collection. To uncover the phenomenon of inter-trace redundancy, we conduct reasoning trace collection process. We select four widely-used reasoning models: DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025), Qwen3-4B-Thinking-2507 (Yang et al., 2025a), GLM-4.5-Air (Zeng et al., 2025) and QwQ-32B. These models are evaluated on 115 problems sampled from reasoning benchmarks including MATH500 (Hendrycks et al., 2021), AIME24, AIME25 (AIME, 2025), and GPQA (Rein et al., 2024). For each problem, we generate 16 parallel reasoning traces per model. The traces for each problem are paired exhaustively, resulting in (cid:0)16 (cid:1) = 120 pairs per problem. Thus, for each 2 model, we obtain around 13,000 pairs in total. To determine answer equivalence, we use the rulebased reward function from DeepScaleR (Luo et al., 2025) to verify whether the final answers of each pair are identical. More details are in Appendix A. The results, summarized in Figure 2(a), reveal striking dominance of same-answer pairs across all models, with ratios exceeding 80% in most cases. Specifically, GLM-4.5-Air has 94.5% sameanswer pairs. This severe class imbalance high3 Figure 3: Overview of the DeepPrune framework. The offline training phase (top) involves constructing trace pair datasets with binary labels indicating answer equivalence, then training judge model using focal loss and oversampling to address class imbalance. The online pruning phase (bottom) leverages the trained judge model to perform dynamic pruning via greedy clustering where traces are assigned to existing clusters or new ones based on similarity predictions, and concludes with majority voting on selected traces to determine the final answer. lights that even with modest number of samples (16 per problem), large proportion of computational resources are wasted on generating redundant reasoning paths. The high prevalence of sameanswer pairs (over 80% on average) underscores inter-trace redundancy as critical efficiency bottleneck in parallel reasoning. Preliminary Experiment. We next investigate whether the similarity between unfinished traces can predict the similarity of their final answers. This capability is crucial for early pruning of redundant paths. We evaluate two approaches for this prediction task: (1) Shallow Semantic Similarity: First, we use SentenceBERT (all-MiniLM-L6-v2 model (Reimers and Gurevych, 2019)) to compute cosine similarity between the first 700 tokens of two traces. This similarity score serve as feature for binary classification. As shown in Figure 2(b), the ROC curve achieves an AUROC of only 0.58, which is barely better than random guessing (AUROC=0.5). This indicates that surface-level semantic features are insufficient for predicting answer equivalence. (2) LLM-based Deep Comparison: To leverage deeper understanding for reasoning traces, we employ Qwen3-4B-Instruct in zero-shot setting. We design prompt that instructs the model to compare two unfinished traces with 700 tokens and judge whether they will yield the same final answer. The ROC curve for this classifier achieves an AUROC of 0.68, notable improvement over SentenceBERT. However, this performance remains suboptimal for practical deployment. These results demonstrate that while LLM-based judgment captures logical equivalence better than semantic similarity, there is significant room for improvement. The suboptimal performance of both approaches motivates our proposed method."
        },
        {
            "title": "4 DeepPrune",
            "content": "To address the inter-trace redundancy problem in parallel scaling, we propose DeepPrune, twostage framework that includes offline training of specialized judge model and online inferencetime pruning. As demonstrated in Figure 3, the core idea is that by accurately predicting whether two incomplete reasoning traces will yield identical final answers, we can efficiently prune redundant paths while preserving answer diversity. 4.1 Offline Training 4.1.1 Trace Pair Data Collection To train our judge model, we construct dataset of reasoning trace pairs with binary labels indicating whether they lead to identical final answers. For each input query q, we generate parallel reasoning traces {t1, t2, . . . , tn} using the same reasoning model. The traces are paired exhaustively, resulting in (cid:0)n (cid:1) pairs per query. The similarity label yij of 2 each pair (ti, tj) is based on answer equivalence: yij = R(oi, oj) where R(oi, oj) is reward function that verifies answer equivalence using rule-based methods from DeepScaler (Luo et al., 2025), and oi, oj are final answers derived from traces ti and tj, respectively. key challenge is determining how to extract meaningful segments from unfinished traces for early redundancy prediction. We explore two truncation strategies: (1) Fixed-length prefix: Truncate the first tokens from each trace: t[1:k] and t[1:k] . (2) Reasoning-step alignment: Extract segj ments containing the same number of reasoning steps, which can be represented by first reasoning words like wait, thus, and since that drive the direction of reasoning pathways (Wang et al., 2025a). Our training data is collected exclusively from DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) outputs, while traces from other models are reserved for testing cross-model generalization. 4.1.2 Judge Model Training Strategy We fine-tune Qwen3-4B-Instruct as our generative judge model Jθ to predict the similarity label yij given pair of unfinished traces (ti, tj). The model takes the concatenated trace pair as input and outputs binary prediction: ˆyij = Jθ(concat(ti, tj)) To address the severe class imbalance where sameanswer pairs constitute approximately 80% of the data, we employ two complementary techniques: Focal Loss. We use focal loss (Lin et al., 2017) to focus training on hard negative examples (differentanswer pairs), improving the true negative rate: Lf ocal = αt(1 pt)γ log(pt) where pt is the models estimated probability for the true class, γ modulates the rate at which easy examples are down-weighted, and αt balances class importance for the 80/20 distribution of our labels. Oversampling. We oversample the minority class by factor of 2 to achieve balanced class distribution during training, ensuring the model receives sufficient exposure to diverse reasoning patterns. 4.2 Online Pruning 4.2.1 Early Stopping via Greedy Clustering During online pruning, we generate multiple parallel traces simultaneously and dynamically prune redundant paths. Let = {t1, t2, . . . , tN } be the set of parallel reasoning traces. Our goal is to select diverse subset that maximizes answer diversity while minimizing computational cost. We propose greedy clustering algorithm that operates with unfinished traces. The algorithm maintains set of clusters = {c1, c2, . . . , cm}, where each cluster represents traces that are predicted to yield identical answers cj = {t1, t2, . . . , tcj }. For each new trace ti S, we compute its average similarity with representative traces sampled from the existing cluster cj C: sim(ti, cj) = 1 (cid:88) h=1 Jθ(ti, t(j) ) where t(j) are randomly sampled top-p traces from cluster cj with = min(K1, cj). If maxj sim(ti, cj) > τ , we assign ti to the most similar cluster arg maxj sim(ti, cj); otherwise, we create new cluster if the maximum number of clusters has not been reached. If is reached, we will terminate the clustering process. All the hyper-parameters are listed in Appendix C. Our approach reduces the number of similarity judgments compared to exhaustive pairwise comparisons, making it suitable for real-time inference. 4.2.2 Majority Voting for Final Answer After clustering, we need to select final answer from the remaining traces. We observe two kinds of errors from our judge model: (1) Most pairs are classified as equivalent, so the largest cluster has too many traces. (2) All trace pairs are predicted as different, therefore each cluster only has one trace. To deal with these situations, we first select the largest cluster cmax (c means the number of reasoning traces in c): cmax = arg max cC To conduct voting without too many identical traces, we only let the top-k traces in cmax to finish reasoning, where = min(cmax, K2). Besides, if all clusters are singletons, i.e. = 1, C, which means the judge model is highly likely wrong, we abandon the clustering results and just sample = K3 traces from for final reasoning. Finally, we apply majority voting on the final answers of those finished traces: ofinal = MajorityVote({o1, o2, . . . , ok}) This approach ensures that we can invest computational resources primarily in promising reasoning paths even if the judge model may produce wrong prediction, which reduces token consumption in parallel reasoning while preserving correctness. 5 Judge Training Method Average Qwen3-4B-Thinking QwQ-32B GLM-4.5-Air AUROC TNR@0.2 AUROC TNR@0.2 AUROC TNR@0.2 AUROC TNR@0.2 First-500 Tokens + Focal loss + Oversampling + Focal loss & Oversampling First-25 Reasoning Words + Focal loss + Oversampling + Focal loss & Oversampling 0.8556 0.8360 0.7610 0.8608 0.8326 0.8559 0.7983 0. 0.7720 0.7327 0.5232 0.7698 0.6647 0.7403 0.6762 0.8186 0.8582 0.8369 0.7587 0.8710 0.7948 0.8434 0.8095 0.8705 0.7720 0.7134 0.4910 0.7869 0.5236 0.6846 0.6677 0. 0.8435 0.8309 0.7509 0.8586 0.8190 0.8253 0.7917 0.8512 0.7632 0.7373 0.5154 0.7629 0.6587 0.6842 0.6521 0.7905 0.8652 0.8401 0.7733 0.8528 0.8841 0.8989 0.7938 0. 0.7808 0.7473 0.5632 0.7595 0.8117 0.8522 0.7089 0.8554 Table 1: The offline evaluation results of the judge model across different truncation methods and training strategies, which reports the average AUROC and TNR@0.2 metrics for three reasoning models using two truncation types: first-500 tokens and first-25 reasoning words, with the combination of focal loss and oversampling."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup DeepSeek-8B1 Settings. Our evaluation includes both offline assessment of the judge models predictive performance and online testing of the full pruning framework during reasoning tasks. For offline evaluation, we train our judge model on reasoning from DeepSeek-R1-Distill-Llama-8B traces with datasets collected in Section 4.1.1 and evaluate its generalization capability on three distinct reasoning models that are not included in the training data: Qwen3-4B-Thinking-2507, QwQ-32B, and GLM-4.5-Air. For online reasoning experiments, we evaluate three reasoning models: (Guo et al., 2025), Qwen3-32B (Yang et al., 2025a), and GPT-OSS-20B (Agarwal et al., 2025) across the three benchmarks: AIME 2024 (MAA, 2024), AIME 2025 (AIME, 2025), and GPQA (Rein et al., 2024). We generate 512 parallel reasoning traces per problem for baseline methods and apply DeepPrune with redundancy threshold τ = 0.5. Metrics. We employ two categories of evaluation metrics. (1) Offline Evaluation Metrics: We assess the judge models binary classification performance using AUROC (area under the receiver operating characteristic curve) to measure overall classification performance, and TNR@0.2 (true negative rate at false negative rate of 0.2) to evaluate the models ability to identify diverse reasoning paths while controlling false negatives. (2) Online Evaluation Metrics: We measure the end-to-end system performance using token consumption, accuracy (final answer correctness measured by exact match with ground truth), and token reduction per1https://huggingface.co/deepseek-ai/ DeepSeek-R1-0528-Qwen3-8B centage reduction in token consumption compared to original consensus sampling (cons@512): Token% = Tokenspruned Tokensorigin Tokensorigin 100% Baselines. We compare DeepPrune against several competitive baselines: (1) Sampling Methods: cons@512 which samples 512 parallel traces with majority voting for self-consistency (Wang et al., 2022), serves as our primary baseline for token reduction calculations. (2) Confidence-based Pruning Methods: DeepConf-high and DeepConf-low are confidence-based early stopping methods with high or low threshold for pruning. These baselines represent the state-of-the-art in efficient reasoning methods. We ensure fair comparison by using identical model checkpoints and configurations with the DeepConf paper with details in Appendix B. 5.2 Offline Experiment Results As shown in Table 1, we have three observations: (1) Our best configuration, which uses first-25 reasoning words with focal loss and oversampling, achieves superior performance with an average AUROC of 0.8701 and TNR@0.2 of 0.8186 across all models. This represents substantial improvement over the preliminary zero-shot LLM judgment (AUROC=0.66) reported in Figure 2(c), validating the necessity of specialized training for redundancy prediction. (2) The comparison between first-500 tokens and first-25 reasoning words reveals clear advantage for reasoning-aligned truncation. This suggests that structural alignment with reasoning steps provides more reliable signals for predicting answer equivalence compared to fixed-length token windows. (3) The ablation study on training strategies demonstrates the critical importance of addressing class imbalance. The combination of focal loss and oversampling consistently delivers the 6 Metric cons@512 Token (108) Accuracy DeepConf-high Token (108) Token% Accuracy DeepConf-low Token (108) Token% Accuracy cons@512 Token (108) Accuracy DeepPrune (ours) Token (108) Token% Accuracy DeepSeek-8B Qwen3-32B GPT-OSS-20B AIME24 AIME25 GPQA AIME24 AIME25 GPQA AIME24 AIME25 GPQA 3.55 86.7% 4.01 82.3% 9.92 72.5% 2.00 84.8% 2.43 80.1% 7.44 72.2% 5.57 96.7% 6.26 95.4% 1.45 -59.0% 86.7% 0.78 -77.9% 92.5% 2. 6.90 -40.9% -30.4% -56.0% 86.4% 72.4% 81.4% 0.88 1.61 4.16 -33.7% -44.1% -44.8% 96.7% 72.9% 80.2% 3.07 3.18 -49.2% 95.3% 1.24 3.46 -69.0% -65.1% -66.8% 89.5% 71.7% 86.4% 0.66 1.14 3.21 1. 1.21 -52.9% -56.9% -80.0% -80.7% 96.1% 80.2% 95.7% 73.0% - - - - - - - - 3.62 86.7% 4.19 83.3% 10.9 66.2% 1.93 86.7% 2.64 80.0% 6.94 70.7% 2.05 93.3% 2.10 90.0% 4.60 70.7% 0.42 0. 0.26 -88.3% -91.6% -76.7% -86.4% -91.4% -85.6% -79.6% 90.0% 90.0% 86.7% 83.3% 90.0% 63.1% 70.2% 2. 1.00 0.42 0.23 0.38 2.20 -82.2% -52.5% 68.7% 93.3% Table 2: Online experimental results showing token consumption (in 108) and accuracy across three reasoning models on three benchmarks. The table compares different methods including conventional sampling (cons@512), confidence-based approaches (DeepConf-high, DeepConf-low), and the proposed DeepPrune. Token savings relative to cons@512 (Token%) are also provided where applicable. indicates results taken from the DeepConf paper. best performance across both truncation types and all reasoning models. Notably, using oversampling alone significantly degrades performance (AUROC drops to 0.7610 for tokens and 0.7983 for reasoning words), indicating that simply balancing the dataset distribution is insufficient without proper loss weighting. Focal loss alone provides moderate improvements, but the synergistic combination with oversampling yields the most robust results. 5.3 Online Experiment Results Table 2 presents the online evaluation results, from which we can get several key findings. Substantial Token Reduction with Minimal Accuracy Loss. DeepPrune achieves remarkable token savings while maintaining competitive accuracy across all experimental settings. Specifically, DeepPrune reduces token consumption by over 80% in most cases compared to the cons@512 sampling baseline. The most significant reductions are observed on AIME datasets, where DeepPrune achieves 79.6%-91.6% token savings with negligible accuracy drop (within 3 percentage points). For instance, on Qwen3-32B with AIME25, DeepPrune reduces tokens by 91.4% while even improving accuracy from 80.0% to 90.0%. This demonstrates that our method effectively identifies and eliminates redundant reasoning paths without compromising solution quality. Superior Efficiency Compared to ConfidenceBased Methods. DeepPrune consistently outperforms confidence-based pruning methods: DeepConf-high and DeepConf-low, in terms of token efficiency. While DeepConf-low achieves substantial token reductions, DeepPrune provides the least token consumption across different configurations. More importantly, DeepPrune maintains more stable accuracy preservation compared to DeepConf-low, e.g., DeepPrunes 90.0% vs DeepConf-lows 80.2% on AIME25 with Qwen332B. This highlights the advantage of our intertrace redundancy analysis over single-trace confidence estimation methods (Fu et al., 2025b). Cross-Model Generalization. The consistent performance across three distinct reasoning models (DeepSeek-8B, Qwen3-32B, and GPT-OSS20B) validates the generalizability of our approach. Particularly noteworthy is that the judge model of DeepPrune was trained purely on the reasoning traces of DeepSeek-R1-Distill-Llama-8B, which means all the tested models are out-ofdistribution, providing robust solution for efficient parallel reasoning. 7 Qwen3-32B on AIME24 Qwen3-32B on AIME25 Greedy Clustering w/ Majority Voting pass@k Token(108) ACC Greedy Clustering w/ Majority Voting pass@k Token(108) ACC Token(108) Token(108) 0.0148 0.0093 0.0082 0.0043 93.3% 93.3% 93.3% 93.3% 0.33 0.28 0.26 0.25 86.7% 93.3% 90.0% 90.0% 0.0282 0.0265 0.0142 0.0050 96.7% 96.7% 70% 70% 0.31 0.23 0.23 0.23 90.0% 83.3% 90.0% 90.0% τ 0.75 0.63 0.5 0.25 Table 3: Performance of DeepPrune with varying redundancy threshold τ on AIME datasets for Qwen3-32B. Token consumption, pass rate and accuracy are reported for two pruning settings: (1) Conduct greedy clustering then retains only one trace per cluster, (2) Perform majority voting to get one final answer with the largest cluster. (a) Ablation of Token Number (b) Ablation of Reasoning Word Number Figure 4: Ablation study on judge model with different truncation strategies for unfinished reasoning traces. We report the classification performance on three reasoning models trace answer equivalence with different numbers of truncated top tokens (Figure (a)) and different numbers of reasoning words in extracted segments (Figure (b)). 5.4 Ablation Study To better understand the impact of different truncation strategies on the judge models performance, we conduct an ablation study on the number of extracted top tokens and reasoning words. The results are presented in Figure 4. Comparing Figure 4(a) and Figure 4(b), it is evident that using reasoning words as features generally yields higher AUROC scores and more pronounced optimal point compared to using raw top tokens. This confirms our hypothesis that extracting semantically rich reasoning words provides more effective representation for the judge model, leading to better prediction of answer equivalence. The optimal performance is often found at an intermediate number of features (e.g., 500 tokens or 25 reasoning words), suggesting sweet spot where sufficient context is provided without introducing excessive noise. Besides, we analyze the trade-off between efficiency, answer diversity, and final accuracy by varying the redundancy threshold τ in DeepPrune  (Table 3)  . The pass rate measures answer diversity after clustering, while accuracy reflects the voting outcome from the largest cluster. As τ decreases from 0.75 to 0.25, token consumption decreases significantly due to more aggressive pruning. However, this comes at the cost of reduced answer diversity, particularly on challenging problems. On AIME25, pass rate drops from 96.7% to 70% under greedy clustering, indicating that higher thresholds may prune valuable diverse reasoning paths. The majority voting accuracy shows the same trend. More analyses for this ablation are in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "We identify inter-trace redundancy as major efficiency bottleneck in parallel reasoning, where over 80% of computational resources are wasted on generating equivalent reasoning paths. To address this, we propose DeepPrune, novel framework that trains judge model for online greedy clustering to dynamically prune redundant traces while preserving answer diversity. Extensive experiments show that DeepPrune reduces token consumption up to 91.4% without hurting accuracy. Our work establishes that learned similarity judgment effectively addresses redundancy in parallel scaling, paving the way for more efficient reasoning systems."
        },
        {
            "title": "References",
            "content": "We acknowledge several limitations of our work. First, due to limited computational resources, our judge model is trained exclusively on reasoning traces from Deepseek-R1-Distill-Llama-8B, which may limit its generalization to other model families with distinct reasoning styles. While our cross-model evaluations show promising results, performance could potentially degrade on architectures different from the training distribution. Second, the greedy clustering algorithm, while efficient, makes locally optimal decisions that may occasionally prune beneficial diverse paths, particularly in complex reasoning scenarios where early similarity is not indicative of final answer equivalence. Third, our method introduces additional computational overhead from the judge model inferences during pruning. Although the overall token reduction is substantial, the relative efficiency gain depends on the cost ratio between the judge and reasoning models. Finally, the optimal redundancy threshold τ may be problem-dependent; while τ = 0.5 works well across our benchmarks, adaptive threshold selection could further improve performance. Addressing these limitations represents promising directions for future work."
        },
        {
            "title": "Ethical Consideration",
            "content": "We affirm that this work raises no significant ethical concerns. All models and datasets used in our experiments are publicly available with permissible licenses, ensuring proper attribution and compliant usage. Specifically, the reasoning models (DeepSeek-8B2, Qwen (Yang et al., 2025a), GPT-OSS (Agarwal et al., 2025)) and benchmarks (AIME (MAA, 2024; AIME, 2025), GPQA (Rein et al., 2024)) are widely recognized resources in the research community. Our research focuses on improving the computational efficiency of reasoning processes through redundancy reduction, without involving sensitive data generation or manipulation. The content processed consists exclusively of mathematical and scientific reasoning tasks, which are devoid of personal, biased, or harmful material. 2https://huggingface.co/deepseek-ai/ DeepSeek-R1-0528-Qwen3-8B Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. 2025. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925. Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks arXiv preprint learning. with reinforcement arXiv:2503.04697. AIME. 2025. Aime problems and solutions. Anthropic. 2024. Anthropic: Introducing claude 3.5 sonnet. Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463. Simon Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024a. Are more llm calls all you need? towards scaling laws of compound inference systems. arXiv preprint arXiv:2403.02419. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024b. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903. Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. 2025a. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. 2025b. Deep think with confidence. arXiv preprint arXiv:2508.15260. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. 9 Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2024. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. 2025. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. arXiv preprint 2024. Openai o1 system card. arXiv:2412.16720. Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. 2025. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2431224320. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Chen Li, Nazhou Liu, and Kai Yang. 2025. Adaptive group policy optimization: Towards stable training and token-efficient reasoning. arXiv preprint arXiv:2503.15952. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2024. Can language models learn to skip steps? Advances in Neural Information Processing Systems, 37:4535945385. Yue Liu, Jiaying Wu, Yufei He, Ruihan Gong, Jun Xia, Liang Li, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, et al. 2025. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://github.com/ rllm-org/rllm. GitHub. Lovish Madaan, Aniket Didolkar, Suchin Gururangan, John Quan, Ruan Silva, Ruslan Salakhutdinov, Manzil Zaheer, Sanjeev Arora, and Anirudh Goyal. 2025. Rethinking thinking tokens: Llms as improvement operators. arXiv preprint arXiv:2510.01123. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. 2025. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. OpenAI. 2024. Openai: Hello gpt-4o. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. 2025. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. 2025. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Matthew Renze and Erhan Guven. 2024. The benefits of concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM), pages 476483. IEEE. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling llm test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations. MAA. 2024. American invitational mathematics examination - aime. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, 10 Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li Cao, and Weiping Wang. 2025b. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. 2025. Adaptthink: Reasoning modarXiv preprint els can learn when to think. arXiv:2505.13417. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, et al. 2025. Parallel-r1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980. Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183. Gaurav Srivastava, Shuxiang Cao, and Xuan Wang. 2025. Towards reasoning ability of small language models. arXiv preprint arXiv:2502.11569. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, et al. 2025. Stop overthinking: survey on efficient reasonarXiv preprint ing for large language models. arXiv:2503.16419. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, et al. 2025. Recursive selfaggregation unlocks deep thinking in large language models. arXiv preprint arXiv:2509.26626. Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Soft self-consistency imarXiv preprint Mohit Bansal. 2024. proves language model agents. arXiv:2402.13212. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. 2025a. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Weiqin Wang, Yile Wang, and Hui Huang. 2025b. Ranked voting based self-consistency of large language models. arXiv preprint arXiv:2505.10772. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Haoming Wen, Yushi Bai, Juanzi Li, and Jie Tang. 2025. Siri: Scaling iterative reinforcement learning with interleaved compression. arXiv preprint arXiv:2509.25176. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067."
        },
        {
            "title": "Appendix",
            "content": "A Detailed Analysis of Inter-trace Redundancy In Section 3.2, Figure 2(a) presents the distribution of redundant traces, showing the \"same-answer\" pair ratios for four different models. The average percentage of these models might not directly align with simple arithmetic average of the individual percentages. This is because the amount of parallel reasoning trace data collected for each model varies significantly. During the trace collection process, we initially sampled 16 responses for each query. However, some Chain-of-Thought traces failed to produce valid answers or did not terminate within the max_length limit. These invalid traces were excluded from the pair calculation. Furthermore, while our full dataset comprised total 758 problems from benchmarks including GPQA, AIME24, AIME25, and MATH500, due to computational resource constraints, we were only able to run all 758 queries on DeepSeek-R1-Distill-Llama-8B. For other models, we sampled 115 problems from the four datasets. This led to the following total pair counts and similarity ratios for each model in Table 4. Model Total Pairs Same Answer Pairs Similarity Ratio GLM-4.5-Air QwQ-32B DeepSeek-R1-Distill-Llama-8B Qwen3-4B-Thinking-2507 Average 11,870 13,785 80,760 13,800 120,215 11,213 12,569 61,505 12,852 98,139 0.9447 0.9118 0.7616 0.9313 0.8164 Table 4: Detailed statistics of collected reasoning trace pairs and their similarity ratios across different models. Given the large disparity in the number of total pairs, particularly the substantial contribution from DeepSeek-R1-Distill-Llama-8B, weighted average would be necessary to accurately reflect the overall inter-trace redundancy across the combined dataset. Our analysis in Section 3.2 is based on the aggregate distribution, rather than simple average of individual model ratios."
        },
        {
            "title": "B Reconciliation of Online Experiment Results",
            "content": "B.1 Differences in cons@512 and DeepConf Baselines In Table 2 of our online experiments, the reported cons@512 and DeepConf baseline results might show slight differences compared to those originally published in the DeepConf paper. We conducted our experiments using the exact same three reasoning models (DeepSeek-8B, Qwen3-32B, and GPT-OSS-20B) and the same three benchmarks (AIME 2024, AIME 2025, and GPQA) as in the DeepConf paper. The cons@512 method, which involves sampling 512 responses, inherently has degree of randomness. To minimize these discrepancies and ensure fair comparison, we meticulously aligned our experimental setup with that described in the DeepConf codebase. This included using identical sampling temperatures and max_length settings for each models hyperparameters, and employing vllm (Kwon et al., 2023) as the inference engine. Despite these efforts, minor differences in the final evaluation results persisted. To further align our results, we initially sampled 640 responses (512 + 128) and then re-sampled 512 responses. This resampling process was performed to match the pass@1 metrics for each model and dataset as closely as possible to those reported in the original DeepConf paper, ensuring the differences were within 3 percentage points. The aligned pass@1 values are presented in Table 5. This careful alignment allows for more direct comparison of DeepPrunes performance against established baselines. B.2 Missing DeepConf Result for GPT-OSS-20B on GPQA In Table 2, the entry for DeepConf on GPT-OSS-20B for the GPQA dataset is marked as empty (-). This is because the original DeepConf paper did not report experimental results for this specific model-dataset 12 Model Dataset DeepPrune Pass@1 DeepConf Pass@ AIME_2025 AIME_2024 GPQA Qwen3-32B Qwen3-32B Qwen3-32B DeepSeek-R1-0528-Qwen3-8B AIME_2025 DeepSeek-R1-0528-Qwen3-8B AIME_2024 DeepSeek-R1-0528-Qwen3-8B GPQA GPT-OSS-20B GPT-OSS-20B GPT-OSS-20B AIME_2025 AIME_2024 GPQA 69.28 80.10 68.24 74.57 83.08 59.74 77.54 80.72 66.18 71.7 80.6 68.9 76.9 83.0 62.8 - - - Table 5: Comparison of pass@1 metrics with the same values taken from the DeepConf paper after alignment procedure. Differences are within 3 percentage points. combination. Due to our limited computational resources, we were unable to conduct this additional experiment to fill the gap. B.3 Dataset and Model Specifications To ensure full transparency and reproducibility, we explicitly state the versions and sources of the datasets and models used in our experiments. If not stated below, then the name of the dataset or model should be its full name. All references to GPQA in this paper, including those in our online experiments  (Table 2)  and trace collection, refer specifically to the GPQA Diamond subset, which consists of 198 problems. This is consistent with the dataset used in the DeepConf paper and can be accessed at https: //huggingface.co/datasets/fingertap/GPQA-Diamond. The model referred to as DeepSeek-8B throughout to the DeepSeek-R1-0528-Qwen3-8B model. This is the same model variant employed in the DeepConf paper and is publicly available at https://huggingface.co/deepseek-ai/ DeepSeek-R1-0528-Qwen3-8B. And Qwen3-4B-Thinking refers to https://modelscope.cn/ models/Qwen/Qwen3-4B-Thinking-2507. corresponds this paper,"
        },
        {
            "title": "C Hyperparameters",
            "content": "Table 6 summarizes the key hyperparameters used in the DeepPrune framework and their default values, along with brief explanation of their meaning. Symbol Default Value Description γ αt τ K1 K2 K3 500 25 2.0 0.5 0.5 32 10 10 64 Number of tokens for fixed-length prefix truncation Number of reasoning words for aligned segment truncation Focal loss focusing parameter Focal loss class-balancing parameter Similarity threshold for greedy clustering Maximum number of clusters Maximum number of sampled traces for cluster similarity calculation Max traces to finish reasoning from the largest cluster Number of traces to sample for reasoning if all clusters are singletons Table 6: Key hyperparameters of the DeepPrune framework. There are two symbols because each of them will not exist when the other is used since we can only use one truncation strategy."
        },
        {
            "title": "D Computational Resources",
            "content": "The majority of our experiments, including the training of the judge model and significant portion of the testing, were conducted on 4 NVIDIA A100 GPUs with 80GB memory each. In the later stages of the experiments, we also utilized server equipped with 8 H20 GPUs for approximately two days. Our computational resources were relatively limited, especially considering the requirement to generate 512 responses from large language models for baseline comparisons."
        },
        {
            "title": "E Discussion on Answer Diversity and Majority Voting",
            "content": "E.1 Comparison of Diversity with Baselines In the introduction, we highlight that limitation of confidence-based early stopping is its potential impact on answer diversity, which DeepPrune aims to preserve. However, in our online experiments  (Table 2)  , we primarily compare against baselines using accuracy rather than an explicit diversity metric like pass@k. In reasoning tasks, answer diversity is often quantified by pass@k, which measures the proportion of samples where at least one of the top-k retained answers is correct. Prior methods like DeepConf and cons@512 typically aggregate multiple traces to produce single final answer, effectively reducing to 1 for their final output. Therefore, direct pass@k comparison with these methods would be unfair, as our approach inherently produces set of diverse reasoning paths (clusters) from which multiple candidates could be drawn, leading to value greater than 1. Our approach, after clustering, theoretically retains distinct reasoning paths in different clusters. Each clusters representative trace could contribute to pass@k calculation, where would typically be greater than one (equal to the number of clusters). This inherent diversity in our retained traces is evident in Table 3, where higher τ thresholds lead to higher pass rates under greedy clustering, indicating more diverse paths are preserved. To adaptively adjust the threshold, we tried setting the threshold according to the the cluster number. If the number of clusters exceed 16, then we increase the threshold by 0.03, until it reaches 0.9. Actually, this trick did not bring much changes to final accuracy and efficiency. So we did not conduct ablation on that. In conclusion, to ensure fair comparison on single-answer basis, our main online evaluation focuses on the final accuracy obtained after majority voting, as this is the metric that DeepConf and cons@512 also optimize for. E.2 Analysis of Low Pass Rate with Low Thresholds notable observation in Table 3 is that for Qwen3-32B on AIME25, the pass@k for \"Greedy Clustering\" at lower redundancy thresholds (0.5 and 0.25) is 70.0%, which appears lower than the corresponding \"w. Majority Voting\" settings accuracy. This seemingly counter-intuitive result can occur because, at very low similarity thresholds, the judge model becomes highly permissive, predicting large number of trace pairs as having the same answer. This permissiveness causes many traces to be grouped into few large clusters, leading to an uneven distribution of traces across clusters. When we calculate the pass@k metric for \"Greedy Clustering\" we typically select one representative trace from each unique cluster to contribute to the diversity count (i.e., equals the number of distinct clusters). If most traces are concentrated in only few clusters, the effective becomes very small. In such scenarios, even if the individual clusters contain correct answers, the limited number of distinct clusters (and thus value) can result in lower pass@k percentage, as it does not fully capture the potential for correctness from the overall set of generated traces. Conversely, for the \"w. Majority Voting\" results, our strategy in these low-threshold scenarios is to sample up to q1 = 20 traces from the largest cluster to perform majority voting (as described in Section 4.2.2). This allows us to leverage greater number of individual traces to reach consensus, ensuring more robust final answer and potentially leading to higher accuracy, even when the overall number of distinct clusters and pass@k is low. This mechanism helps maintain effective accuracy by focusing computational resources on the most prominent reasoning paths, despite reduced perceived diversity at the cluster level when our judge model dose not act perfectly. 14 E.3 Rationale for Majority Voting We employ majority voting in DeepPrune for two primary reasons: 1. Effectiveness and Common Practice: Majority voting is widely adopted and empirically effective method for aggregating multiple reasoning traces to derive robust final answer, as demonstrated by pioneering works like Self-Consistency (Wang et al., 2022). It helps mitigate errors from individual traces and leverages the collective intelligence of diverse reasoning paths. 2. Fair Comparison with Baselines: To enable direct and fair comparison of final answer accuracy with methods like DeepConf that also produce single aggregated answer, we needed mechanism to consolidate the diverse traces retained by DeepPrune into one final prediction. While our method inherently preserves inter-trace diversity for potential pass@k evaluations (as explored in Table 3), majority voting allows us to align with the single-answer output paradigm of competitive baselines. It is important to note that DeepPrune is highly flexible and can be integrated with other aggregation strategies. For instance, instead of simple majority voting, one could employ selection model (Moshkov et al., 2025) to choose the best answer from the diverse set of clusters. Furthermore, DeepPrune is orthogonal to existing method; for example, after our method reduces inter-trace redundancy, confidencebased filter like DeepConf could be applied within each remaining cluster or across selected representatives to further refine the final answer. Due to our limited resources, we leave the exploration of these alternative aggregation strategies and combinations for future work."
        }
    ],
    "affiliations": [
        "ShanghaiTech University",
        "Tsinghua University"
    ]
}