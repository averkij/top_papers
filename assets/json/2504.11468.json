{
    "paper_title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models",
    "authors": [
        "Hardy Chen",
        "Haoqin Tu",
        "Fali Wang",
        "Hui Liu",
        "Xianfeng Tang",
        "Xinya Du",
        "Yuyin Zhou",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models Hardy Chen2, Haoqin Tu1, Fali Wang3, Hui Liu4, Xianfeng Tang4, Xinya Du2, Yuyin Zhou1, Cihang Xie1 1 University of California, Santa Cruz 3 The Pennsylvania State University 2 University of Texas at Dallas 4 Amazon Research 5 2 0 A 0 1 ] . [ 1 8 6 4 1 1 . 4 0 5 2 : r Project Page: https://ucsc-vlaa.github.io/VLAA-Thinking/ 7B Model: https://huggingface.co/UCSC-VLAA/VLAA-Thinker-Qwen2.5VL-7B 3B Model: https://huggingface.co/UCSC-VLAA/VLAA-Thinker-Qwen2.5VL-3B Dataset: https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking"
        },
        {
            "title": "Abstract",
            "content": "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals key finding: SFT can significantly undermine subsequent RL by inducing pseudo reasoning paths imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, new multimodal dataset designed to support reasoning in LVLMs. Constructed via six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard1 among 4B scale LVLMs, surpassing the previous state-of-theart by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) with strong reasoning capability have recently gained wide attention with the emergence of OpenAIs o1/o3 and Deepseek-R1 (Guo et al., 2025; Jaech et al., 2024). common practice to empower models with reasoning abilities comprises two steps: supervised fine-tuning (SFT) on reasoning data, followed by reinforcement learning (RL) to further boost performance. This successful paradigm has inspired efforts to extend these strengths beyond textual domains to Large Vision-Language Models (LVLMs) (Peng et al., 2025; Chen et al., 2025a; Deng et al., 2025b; Shen et al., 2025; Yang et al., 2025b). *Equal contribution. 1https://huggingface.co/spaces/opencompass/Open LMM Reasoning Leaderboard 1 Preprint. Under review. Figure 1: Examples from LVLMs trained with different strategies for reasoning Left: response from model trained with SFT, showing pseudo reasoning traces and number of pseudo self-reflective cues (i.e., aha-moments) imitated from R1. Right: response from model trained with RL, showing native reasoning ability and authentic aha-moments emerged from RL training. Wrong reasoning steps are colored red and aha-moments are highlighted. In this work, we take step further and examine whether the widely adopted SFT then RL paradigm similarly benefits the development of reasoning-capable LVLMs. Specifically, we ask: 1) What are the distinct effect of SFT and RL in multimodal reasoning? and 2) Is this two-stage paradigm truly necessary for reasoning in LVLMs? To systematically explore these questions, we curate VLAA-Thinking, the first comprehensive and high-quality image-text reasoning dataset explicitly designed to support both SFT and RL. Unlike prior datasets, VLAA-Thinking includes detailed, step-by-step reasoning traces derived from the R1-style think-then-speak intermediate reasoning. We construct dedicated SFT split featuring multimodal chain-of-thought (CoT) examples suitable for visual instruction tuning, alongside more challenging RL split curated from the same source encourage deeper and more adaptive reasoning behaviors. To effectively transfer reasoning capabilities from text-only models to the multimodal domain, we construct our dataset through six-stage pipeline: metadata collection, image captioning, R1-based distillation, answer rewriting, verification, and split curation. Specifically, we input image captions and visual questions into DeepSeek-R1 to generate initial reasoning traces. These outputs are then rewritten for improved fluency and verified for correctness using GPT-based verifier, resulting in high-quality multimodal reasoning dataset for SFT and RL. Next, we carefully ablate the role of SFT, RL and their combinations in multimodal reasoning using our VLAA-Thinking dataset. To better understand the role of SFT, we perform detailed analysis, systematically examining the impact of SFT data type (e.g., with and without the self-reflective aha moments), dataset scale, and model capacity. To explore the potential of RL in the vision-language context, we design novel mixed reward function within the Group Relative Policy Optimization (GRPO) (Shao et al., 2024) framework that involves both perception and cognition rewards to incentivize the model to produce well-reasoned answers. Specifically, our mixed reward signal blends 2 types of reward with 5 types of functions. For rule-based questions, there are functions for digit, multiple-choice, math and bounding box outputs. For open-ended questions, we adopt competent reward model, XComposer-2.5-RM (Zang et al., 2025), along with reference-based reward method to score an answer. We then closely investigate the effects of different reward functions, base models, and the interaction between SFT and GRPO to further optimize reasoning capabilities. Our extensive experiments comparing SFT and RL reveal several noteworthy insights. First, we probe the contribution of SFT and RL in multimodal reasoning: while SFT improves performance on standard tasks over the base model, it falls short in enhancing complex reasoning. Merely imitating an experts thinking through SFT often induces pseudo reasoning paths, superficial reasoning pattern which may contain pseudo aha moments (superficial self-reflective cues), as illustrated in Figure 1. We show that these imitated reasoning patterns can hinder genuine reasoning advancement, i.e., 47% relative performance drop on 7B models. This observation is also in line with recent studies highlighting the need for 2 Preprint. Under review. Figure 2: Data generation pipeline. We first generate initial reasoning traces by feeding detailed captions and visual questions into DeepSeek-R1.These outputs are then rewritten for improved fluency and verified for correctness using GPT-based verifier. the resulting data is split into VLAA-Thinking-SFT and VLAA-Thinking-RL. feedback and exploration signals to drive advanced reasoning behaviors (Peng et al., 2025). Additionally, our ablations show that for rule-based rewards, math and multiple-choice are more beneficial than others, and that combination of both rule-based and open-ended rewards yields the best performance. While prior work suggests that SFT followed by RL in LVLMs offers the best of both worlds (Guo et al., 2025; Yang et al., 2025b; Deng et al., 2025b)first mimicking good reasoning format, then refining via RL feedback, we find that applying SFT before GRPO hurts performance on aligned models, with an average 12.7% drop, and even smaller scale SFT leads to similar decline. Regarding model size, larger models cannot immune from the degeneration brought by SFT, as 7B models share almost the same performance drop with their smaller counterparts. Finally, examining the training procedure, we observe little correlation between response length, reward, and performanceSFT-ed models get higher initial rewards and longer response yet underperform RL-trained ones, contrasting with the previous observation that better models usually produce longer answers with higher RL reward (Guo et al., 2025; Peng et al., 2025). To summarize, while SFT helps unaligned models follow instructions, it limits exploration during RL by promoting imitative reasoning. In contrast, learning directly from reward signals yields more effective and adaptable thinking behavior. Empirically, direct RL proves superior. Our model, VLAA-Thinker-Qwen2.5VL-3B, achieves the top-1 performance on the Open LMM Reasoning Leaderboard among 4B-scale LVLMs, surpassing the previous state-of-the-art by 1.8%. Our case study further emphasizes these gains with more concise, effective reasoning traces presented in model answers."
        },
        {
            "title": "2 The VLAA-Thinking Dataset",
            "content": "To systematically evaluate the SFT then RL paradigm for developing reasoning capabilities in LVLMs, we construct VLAA-Thinking, dataset that consists of two parts: 1) VLAA-Thinking-SFT which captures step-by-step reasoning grounded in visual inputs for SFT, and 2) VLAA-Thinking-RL which contains challenging samples designed specifically for RL. Our data generation pipeline is designed to transfer reasoning capabilities from powerful text-only model to the multimodal domain through structured, multi-stage process. The entire pipeline, as illustrated in Figure 2, consists of six key components: #1: Metadata Collection We collect metadata from 9 vision-language datasets featuring either closedor open-ended questions. Specifically, we sample data containing unique images from CLEVR-Math (Lindstr om & Abraham, 2022), Math PUMA (Zhuang et al., 2024), ArxivQA (Li et al., 2024a), DocVQA (Mathew et al., 2021), VizWiz (Gurari et al., 2018), and ALLaVA (Chen et al., 2024a), and process them through our complete data pipeline. In addition, we directly adopt COCO and VisualGenome data from LLaVA-CoT (Xu et al., 3 Preprint. Under review. Name Data Type #Ori. #Pipeline #Final SFT #Final RL Collected from Distilling R1 CLEVR-Math GeoQA170K Math PUMA ArxivQA DocVQA VizWiz ALLaVA-LAION Closed-end Closed-end Closed-end Closed-end Closed-end Closed-end Open-end Collected from LLaVA-CoT COCO VisualGenome Closed-end Closed-end 35,000 - 30,000 54,399 10,194 20,523 47,066 3,000 3,000 28,018 - 26,672 51,348 8,206 6,528 18,123 3,000 3, 5,923 - 19,258 34,604 4,897 4,266 10,496 8,727 38,242 2,000 6,499 6,696 1,000 1,000 1,000 3,000 2,000 2,000 Total Closed- & Open-end 203,182 144,895 126,413 25,195 Table 1: Data statistics of VLAA-Thinking. We present the original volume of metadata (#Ori.), the data size after the distillation pipeline (#Pipeline), the size of sampled examples for SFT (#Final SFT) and RL (#Final RL), respectively. Note that we only use GeoQA170K with verifiable answers for the RL split. 2024). An exception is GeoQA170K (Gao et al., 2023), which we include only in the RL split due to persistent hallucination issues during captioning. Detailed statistics are in Table 1. #2: Visual Input and Additional Information Each sample begins with an image, question, and its corresponding answer. To bridge the gap between the visual modality and language reasoning, we resort to GPT-4o to generate detailed image caption describing the content in structured and semantically rich language (detailed prompts in Appendix A.1). During this process, we take full advantage of the provided knowledge in the data beyond just the GPT captions. In detail, we provide these dataset-specific information: (1) CLEVRMath: Instructions for synthesizing the image from CLEVR (Johnson et al., 2017); (2) Math PUMA: Textual description of math problems in the image from the dataset itself. (3) ALLaVA-LAION: Fine-grained and verified GPT-4V captions from the original dataset. #3: Reasoning Answer Distillation We utilize strong text-only reasoning model: DeepSeek-R1 to generate thinking rationale and final answers. The model is provided with the image caption, the visual question, and additional information from certain datasets. It responds using structured reasoning format that is between <think> and </think> tags and contains sequence of logical steps leading to the final answer. #4: Answer and Rewriting To enhance consistency and eliminate modality-specific artifacts, the raw reasoning answers generated by R1 are passed through rewriting module (i.e., GPT-3.5-turbo (Brown et al., 2020) in our experiment). This module removes unnecessary phrases (e.g., references to caption), and ensures the answer adheres to clean, instruction-following format based on the image. We further filter out samples with the sentence length gap larger than 15 words to ensure minimum modifications in this process. #5: Automated Verification To assess whether the generated reasoning answers is correct regarding the groundtruth answer, we implement an automated verifier. This verifier compares the rewritten reasoning answer to the groundtruth of the visual question, determining whether the outputs are correct or incorrect. Only the examples that are verified as correct are retained as the final training data. #6: Curating Splits for SFT and RL The last step of our data generation pipeline is to curate two non-overlapped training sets for SFT and RL, respectively. Inspired by Chu et al. (2025) which finds that RL is particularly effective in encouraging deeper reasoning on challenging cases, we aim to select more challenging samples for the RL split. To achieve this, we propose using the presence of self-reflective cues (i.e., the aha moments) in the distilled answers as an indicator of samples difficulty level (details are in Appendix A.2). For the SFT split, we exclude samples with aha moments, as such samples may be too complex to fully imitate through finetuning. On the other hand, the harder examples with aha moments form the RL split, on which reward-driven learning may be better suited to elicit meaningful reflection. Preprint. Under review. Following these steps, our dataset adheres to the format {image, question, reasoning, answer}, with reasoning and answer generated by DeepSeek-R1. We construct high-quality multimodal reasoning dataset with 126,413 samples for SFT and 25,195 samples for RL."
        },
        {
            "title": "3 Investigating The Role of SFT for Multimodal Reasoning",
            "content": "SFT has become the de-facto approach for training LLMs. Recent studies aim to extend the strengths of SFT to empower LVLMs with reasoning abilities by training on specially formatted data.Unlike prior methods that incorporate standalone textual descriptions of images (Xu et al., 2024), this direct strategy enables the model to develop grammatically coherent reasoning abilities, allowing it to think before speak. In recent vision-language reasoning systems, there is notable trend of complementing or even replacing SFT with RL to enhance complex reasoning abilities (Peng et al., 2025; Deng et al., 2025b). We follow this line and take it further by probing the underlying cause of this shift. Our finding suggests that self-reflection thinking (aha moments) from the SFT process is overloaded with excessive and irrelevant reasoning, becomes what we call pseudo aha moments and ultimately hurts performance. In this section, we explore 1) the model perform when SFT-ed on data with aha-moments and 2) the effect of SFT data size to model performance."
        },
        {
            "title": "3.1 Experiment Setup",
            "content": "To investigate the effect of SFT training with aha-moments, we collect the distilled VQA pairs whose distilled answers contain aha-moments, totaling 55K samples. To study the effect of SFT with different sizes of training sets, we use perplexity (PPL) filtering to obtain smaller SFT dataset. Specifically, we compute the PPL score of each answer in VLAA-Thinking-SFT-126K using Qwen2-VL-2B and Qwen2.5-VL-3B, and sort all samples by their average PPL scores over the two models. We keep the samples with high PPLs to obtain total of 25K SFT samples, as these harder examples push models to learn more effectively and efficiently (Ankner et al., 2024; Li et al., 2024b). We select four models for training: Qwen2VL (2B and 7B)2, Qwen2.5VL (3B and 7B). Each model is trained with batch size of 128 and their vision encoder frozen. We evaluate model performance with VLMEvalKit (Duan et al., 2024) on 6 math reasoning benchmarks hosted in Open LMM Reasoning Leaderboard, which contains 6 challenging math reasoning benchmarks including MathVista (Lu et al., 2024), MathVision (Wang et al., 2024b), MathVerse (Zhang et al., 2024), DynaMath (Zou et al., 2024), WeMath (Qiao et al., 2024), LogicVista (Xiao et al., 2024). We present the percentage of relative performance drop of different models in Figure 3. Detailed training and evaluation setup are in Appendix B."
        },
        {
            "title": "3.2 Findings",
            "content": "SFT with Aha Moments Degrades Performance. We present results for the Qwen-2.5-VL-3B model trained under three different settings using our SFT data in Table 2. Somewhat unexpectedly, the model fine-tuned on 55K examples containing the aha moment performs significantly worse than the base model, with an average drop of 10.5%. This suggests that chasing the aha moment through SFT is unreliable, as SFT merely teaches the model to mimic rather than to generalize genuine self-reflective reasoning. Additionally, the table shows evidence that straightforward SFT using multimodal reasoning data also degrades performance, e.g., we observe an average drop of 10.2% and 19.1% when fine-tuning on 25K and 126K samples, respectively. Model Qwen2.5-VL-3B w/ aha-55K w/ 25K w/ 126K Avg. 31.8 21.3 21.6 12.7 Table 2: Average performance over 6 reasoning benchmarks of Qwen2.5-VL-3B SFT-ed on different sizes of SFT data and on data containing only examples with aha moment (aha-55K). 2In this work, Qwen2VL-2B and Qwen2VL-7B refer to the instruction-tuned versions. 5 Preprint. Under review. Figure 3: Delta percentage performance change of different models trained with supervised fine-tuning (SFT) only. More SFT Data, Worse Performance. Counterintuitively, even five-fold increase in the supervised dataset (from 25K to 126K instances) often fails to improve performance and in most cases actually harms it. Models trained with 126K SFT samples suffer relative performance drop of over average 14% compared to their 25K-trained counterparts over all model and task settings (e.g., 25K: 32.2% vs. 126K: 47.0%). This degradation is particularly evident on complex datasets such as WeMath and DynaMath, where the relative decrease reaches as high as 97.9% over Qwen2.5-VL models on average. Even on mid-difficulty benchmarks like MathVision and MathVerse (i.e., model performance is relatively higher), the 126K SFT models underperform, with an average drop of 28.6% compared to the untrained model over 4 models. These results suggest that simply scaling up SFT data does not boost generalizable reasoning skills of LLMs, and may instead suppress the models capacity on various reasoning tasks. Larger Models Are Not Immune to SFT Degeneration. Contrary to expectations, scaling up model size does not mitigate the adverse effects of excessive SFT, under heavier SFT they exhibit pronounced drops on the most challenging evaluations. larger 7B models fine-tuned on 126K examples experience drops nearly identical in magnitude to their smaller 2B or 3B counterparts: 47.2% for smaller models vs. 45.4% for larger models compared with base models. Notably, despite the strong performance of Qwen2.5-VL-7B model (e.g., 68.1% on MathVista), it also suffers an average decline of 52.5% on these reasoning tasks when SFT-ed with 126K data. These findings highlight the limitations of SFT as tool for enhancing multimodal reasoning. While it may be suitable for learning reasoning formats, it falls short of the expectations for fostering inherent self-reflection. Rather than simply scaling supervision data, our results suggest for shift toward more advanced training methods like RL."
        },
        {
            "title": "4 Improving Multimodal Reasoning with Mixed Rewards",
            "content": "The previous section shows that SFT is insufficient to transfer R1s ability to LVLMs on vision-language tasks. Therefore, it is crucial to seek for other post-training methods to elicit the reasoning ability of LVLMs. Since reinforcement learning (RL) is effective in enhancing reasoning ability (Yang et al., 2025a; Kirk et al., 2023), and GRPO has recently been proven more effective and efficient on textual math reasoning task (Shao et al., 2024; Jahin et al., 6 Preprint. Under review. Figure 4: The proposed Mixed Reward Module for GRPO training, comprising 2 reward formats (rule-based and open-ended) and 5 types of verifiable rewards (digit, MCQ, math, IoU and general reasoning). 2025) than other methods like PPO (Schulman et al., 2017), it motivates us to apply GRPO training for vision-language reasoning tasks. Mathematically, let be query and {oi}G policy model πold, GRPO maximizes the following objective: i=1 be group of sampled outputs from the old JGRPO(θ) = q,{oi}πθold (cid:34) 1 i= 1 oi oi t=1 min (cid:0)rt(θ) ˆAi,t, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:1) (cid:35) βDKL(πθ πref) where ˆAi,t is the estimated advantage, β is the KL penalty coefficient and πθ, πθold , πref are current, old, and reference policies, respectively."
        },
        {
            "title": "4.1 GRPO with Mixed Reward",
            "content": "To better adapt GRPO to multimodal reasoning, in addition to adopting the rule-based reward similar to the textual GRPO training, it is necessary to consider additional characteristics introduced by the vision modality. Inspired by (Fu et al., 2024) which benchmarks LVLMs by perception and cognition (reasoning), we propose mixed reward framework for GRPO training, as illustrated in Figure 4. The reward system comprises five types of verifiable rewards with two formats, encompassing both visual perception and visual reasoning tasks. Rule-Based Reward There are 4 types of rule-based rewards, including digit matching, option letter matching and math expression matching and Intersection over Union for bounding boxes. For digit matching, the model is asked to answer counting questions from CLEVR-Math whose groundtruths are single digit. For option letter matching, the model is required to answer an MCQ. For math expression matching, the model is asked to solve math question, such as finding function expression or the volume of cone, and output its answers in latex format. We use the Math Verify3 package to check for correctness. For bounding boxes, the model is prompted to output the bounding box coordinates of an object in the image, and an IoU score (range from 0 to 1) is computed as reward. Open-ended Reward We leverage InternLM-XComposer2.5-Reward (Zang et al., 2025) as the scorer, denoted as Sθ(), which takes an image and QA pair as input, and outputs reward score. Following Muhtar et al. (2025), the reward for sampled response ˆy is computed as Ropen = 1 exp((Sθ( ˆy) Sθ(y)) β) if fθ( ˆy) > fθ(y) else 0, where Sθ(y) is the score of the reference answer, and β is smoothing hyperparameter. Note that the open-ended reward is normalized into [0,1], which is consistent with the scale of rule-based reward, partially avoiding reward hacking during training. 3https://github.com/huggingface/Math-Verify 7 Preprint. Under review. Implicit Format Reward Unlike Guo et al. (2025) and its subsequent works which use separate reward term for format correctness, we discard this format reward term and make the format reward supersede all other rewards. Namely, whenever we are unable to extract valid response from the raw answer, the reward would be 0. We empirically find that by specifying the output format in system prompt, the model is able to generate answers with correct formats through trials and errors. The implicit format reward design simplifies the reward computation. Further, it may yield better performance since less restriction is imposed on the exploration process (Zeng et al., 2025)."
        },
        {
            "title": "4.2 Effect of SFT on GRPO Training",
            "content": "GRPO Backbone MathVista MathVision MathVerse DynaMath (vision-only) (worst) WeMath LogicVista Avg. Qwen2VL-7B-Inst Qwen2VL-7B-Inst+SFT Qwen2VL-7B-Base Qwen2VL-7B-Base+SFT 59.6 43.7 59.3 49.5 19.8 14.7 18.2 16.4 33.9 19.0 33.5 25.0 15.2 3.2 11.4 6.4 30.5 11.1 23.2 20.4 36.0 27.3 36.2 32. 32.5 19.8(39%) 30.7 25.7(16%) Table 3: Benchmark results of models trained with GRPO on different backbones. SFT+GRPO yields performance degradation, indicating that SFT is NOT compatible with GRPO in multimodal reasoning. SFT is NOT Compatible with GRPO in Multimodal Reasoning. Although we reveal in Section 3 that SFT alone leads to performance drop in multimodal reasoning, it is still unclear whether SFT plays crucial role in aiding GRPO, like the golden key in DeepSeekR1. We experiment with different backbones for GRPO training. Specifically, we adopt Qwen2VL-7B-Base and Qwen2VL-7B-Inst, and perform SFT on them with 25K samples, followed by GRPO training. From Table 3, we observe that models undergoing SFT before GRPO training perform worse than those trained with GRPO alone, presenting an average drop of 8.9% across Qwen2VL-Base and Qwen2VL-Inst compared to their non-SFT counterparts. We also find that SFT introduces more degradation to instruction models than to base models without instruction-following capabilities. For instance, Qwen2VL-Inst suffers 7.7% more drop in performance than Qwen2VL-Base post-SFT, suggesting that SFT can compromise the instruction-following ability crucial for effective GRPO training. Taken together, these results suggest that SFT is currently incompatible with GRPO in the context of multimodal reasoning, impairing both base and instruction-tuned LVLMs. Figure 5: Impact of SFT with 5K and 10K samples before GRPO. Smaller-sized SFT datasets still jeopardizes GRPO performance. Smaller SFT Dataset Still Jeopardizes GRPO Performance. Since we reveal in Section 3.2 that more SFT data yields lower performance, we try to investigate the effect of downsizing 8 Preprint. Under review. the SFT training set. Following the PPL filtering method in Section 3, we select top-10K and top-5K samples from VLAA-Thinking-SFT-126K to finetune Qwen2.5-VL-3B, followed by GRPO training. For comparison, we also conduct GRPO training without SFT. We present the performance of Qwen2.5-VL-3B on each task in Figure 5. clear observation is that applying SFT on 5K examples prior to GRPO significantly degrades performance compared to using GRPO alone, showing an average drop of 13.5%. Moreover, scaling up SFT data to 10K yields only marginal improvement of 0.8%. These results further support that SFT before GRPO can hinder the models learning capability. Figure 6: Response length (left) and reward (right) during training. Training with only GRPO yields the lowest response length and yet the highest final reward and best benchmark performance, indicating that response length, reward, and model performance are NOT necessarily related. Response Length, Reward, and Model Performance are NOT Necessarily Related. Prior work in RL suggests that longer responses often correlate with better reasoning and higher RL rewards (Guo et al., 2025; Zhou et al., 2025; Chen et al., 2025b). However, our findings in Figure 6 reveal that response length and reward in GRPO are not reliable indicators of reasoning ability. For instance, the 10K SFT+GRPO model produces the longest responses but ends up with lower rewards than the GRPO-only model (0.35 vs. 0.5) after training. Similarly, the 5K SFT+GRPO variant shows moderate length and reward but still underperforms on downstream tasks. Interestingly, both SFT-ed models start with higher initial rewards (e.g., 0.20 for 10K SFT+GRPO vs. 0.05 for GRPO-only), which is likely due to their early learning experience with supervision since SFT and GRPO data share the same distribution. However, they exhibit limited reward improvement during training, whereas the GRPO-only model rapidly surpasses them. These trends further reveal that SFT solely provides higher lower bound for RL training, yet it may lower the upper bound since the reasoning SFT data constrains the models exploration paths. Therefore, reasoning is native emerging ability that is more likely to be developed through RL, not SFT. While SFT-ed models may appear to reason, their behavior is closer to pattern imitation form of pseudo-reasoning that lacks the generalizable reasoning skills."
        },
        {
            "title": "4.3 GRPO Training without SFT",
            "content": "Following the findings in the previous section, we directly conduct GRPO training which yields four models: VLAA-Thinker-Qwen2-VL-2B, VLAA-Thinker-Qwen2-VL-7B, VLAAThinker-Qwen2.5-VL-3B, VLAA-Thinker-Qwen2.5-VL-7B. We also train on base model of Qwen2-VL-7B, and the resulting model is named VLAA-Thinker-Qwen2-7B-Zero. We sample 4 times for each query with temperature 0.8. Rollout and training batch size are set as 512 and 256, respectively. We train our model for 1 episode (outer loop) and 1 epoch per episode (inner loop) on 8*H100 GPUs with 49 steps. More details of training setup are in Appendix C.1. We follow the identical evaluation setup as described in Section 3.1. We present evaluation results in Table 4 and list our main findings below. Direct GRPO Training Boosts Model Performance. Models trained directly with GRPO on the VL-Thinking RL consistently outperform their respective base models. For example, 9 Preprint. Under review. Model MathVista MathVision MathVerse DynaMath (vision-only) (worst) WeMath LogicVista Avg. Qwen2-VL-2B Qwen2.5-VL-3B VLM-R1-Math-0305 VLAA-Thinker-Qwen2-2B VLAA-Thinker-Qwen2.5-3B LLaVA-OneVision-7B InternLM-XComposer2.5 InternVL2.5-8B InternVL2-8B Qwen2-VL-7B Qwen2.5-VL-7B VLAA-Thinker-Qwen2-7B-Zero VLAA-Thinker-Qwen2-7B VLAA-Thinker-Qwen2.5-7B 48.0 61.2 62.7 43.6 61.0 58.6 64.0 64.5 58.3 61.6 68.1 59.3 59.6 68.0 4B-scale LVLMs 16.1 21.9 21.9 14.8 24.4 17.5 31.2 32.2 19.0 36.4 7B-scale LVLMs 18.3 17.8 17.0 20.0 19.2 25.4 18.2 19.8 26.4 19.3 16.2 22.8 20.4 25.4 41.1 33.5 33.9 48.2 3.8 13.2 13.0 3.4 18.2 9.0 8.2 9.4 9.2 11.0 21.8 11.4 15.2 22.4 10.8 22.9 30.0 12.6 33.8 20.9 14.1 23.5 20.2 22.3 36.2 23.2 30.5 41. 26.6 40.3 40.5 30.4 38.5 33.3 34.7 36.0 33.6 33.3 47.9 36.2 36.0 48.5 20.5 31.8 33.4 20.3 35.4 26.6 25.8 28.9 26.9 28.8 40.1 30.7 32.5 42.5 Table 4: Evaluation results of 6 math reasoning benchmarks on Open LMM Leaderboard. VLAA-Thinker models significantly outperform baselines and other models. at the 7B scale, two models trained on VL-Thinking achieve an average score of 36.5%, marking 2.0% improvement over their base model average of 34.5%. Moreover, our best-performing 7B model consistently outperforms other similarly sized LVLMs (e.g., InternVL2.5-8B, LLaVA-OneVision-7B), while our 3B model surpasses the recent reasoningfocused model, VLM-R1-Math, by 1.1% on average. These results once again demonstrate that GRPO significantly enhances reasoning capabilities, even without additional SFT. Stronger Instruction Model Leads to Better Post-GRPO Reasoning. An interesting observation is that model with better instruction tuning generally performs better. The instruction-aligned Qwen2-7B model, after GRPO, outperforms its unaligned counterpart VLAA-Thinker-Qwen2-7B-Zero by 1.8% on average (31.3% vs. 29.5%), with notable gains on harder tasks like DynaMath (5.0%) and WeMath (3.1%). Moreover, using stronger instruction-tuned model for GRPO further improves across both 3B and 7B scales VLAAThinker-Qwen2.5 surpasses VLAA-Thinker-Qwen2 by 12.6% on average, confirming that higher-quality instruction tuning leads to more effective post-RL reasoning. Figure 7: Heatmap of different aha expressions generated by VLAA-Thinker models during training. Emergence of Authentic Aha Moments. To show that our GRPO training can induce authentic self-reflection process, we plot the frequency of four aha expressions (alternatively, double-check, should check, wait) for each VLAA-Thinker model in Figure 7. Since all models are trained using GRPO without being SFT-ed on distilled reasoning paths, all aha moments emerge from the GRPO process, demonstrating the models self-developed reflective ability. Another finding is that the number of aha moments is not directly correlate with overall model performance, as more aha moments do not necessarily translate to higher reasoning scores. 10 Preprint. Under review."
        },
        {
            "title": "4.4 Ablations",
            "content": "Row Method Digit Math MCQ IoU Open-ended MVi MVs WM 0 1 2 3 4 5 6 Qwen2.5-VL-3B w/o Digit w/o Math w/o MCQ w/o IoU All Rule-Based Mixed Reward 21.9 23.5 21.4 21.5 22.8 22.2 24.4 31. 34.6 32.7 33.9 35.3 34.9 36.4 22.9 28.8 27.0 18.4 30.0 30.1 33.8 Table 5: Ablation of Mixed Reward on MVi: MathVision, MVs: MathVerse and WM: WeMath. combination of rule-based and open-ended rewards yields significant boost in performance. Mixed Reward. To demonstrate the effectiveness of our mixed reward strategy, we perform an ablation study on Qwen2.5-VL-3B by selectively disabling individual reward components and evaluating performance across three math reasoning benchmarks, as shown in Table 5. The model trained with Mixed Reward achieves the best overall performance, with an average improvement of 6.2% over the baseline, demonstrating the effectiveness of our reward design. Using only rule-based rewards (All Rule-Based) also yields consistent gains (e.g., 29.1% vs. 25.3% baseline), while removing specific componentsespecially MCQ (w/o MCQ) leads to substantial drops. These results highlight the critical role of rule-based rewards in GRPO for multimodal reasoning tasks. Settings MVs DM LV 15.0 31.7 Basic Hyperparameters To search for better hyperparameters, we experiment with different learning rates (LR) and KL divergence settings on Qwen2.5-VL-3B. We start with basic setting where LR anneals to zero following cosine scheduler with no KL constraint. Results are shown in Table 6. LR1 uses minimum learning rate of 8e7 with warmup ratio 10%, whereas LR2 uses minimum learning rate of 5e7 with warmup ratio 3%. Since LR2 performs slightly better than LR1, we compare two KL settings on top of LR2. KL1 uses an initial KL of 1e2 and target KL of 5e3, whereas KL2 uses an initial KL coefficient of 1e3 and target KL of 5e4. We find that introducing KL constraints significantly improves the performance on MathVerse and DynaMath by 1.1% and 3.2%, respectively, and that using smaller KL can encourage the model to evolve. Learning Rate + LR1 + LR2 KL Coef. + KL1 + KL2 33.0 33.5 16.0 15.6 18.8 18.6 34.4 35. Table 6: Ablation on LR and KL Coef. on MVs: MathVerse, DM: DynaMath and LV: LogicVista. 38.5 38.1 38.3 37.8 39."
        },
        {
            "title": "4.5 Case Study",
            "content": "We provide an example showcasing the improvement of VLAA-Thinker over the original model in Appendix C.3. Qwen2.5VL-7B generates lengthy response with wrong reasoning traces. Although it outputs some self-reflective patterns like re-evaluate, the final answer remains wrong. On the other hand, VLAA-Thinker-Qwen2.5VL-7B is able to reason on the right track, with only minor mistake near the end of its thinking process. Nevertheless, the high-level idea and reasoning process is overall correct, demonstrating strong capability of solving complex reasoning tasks. 11 Preprint. Under review."
        },
        {
            "title": "5 Related Work",
            "content": "Vision-Language Reasoning Models. Recent advances in vision-language (VL) reasoning models build on the success of text-only reasoning systems like OpenAIs o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025). Earlier VL methods, such as few-shot prompting and chain-of-thought (CoT), offered limited visual reasoning (Brown et al., 2020; Wei et al., 2022). Recently, LLaVA-CoT (Xu et al., 2024) adopts an SFT approach 4-step structured outputs to enhance models reasoning, yet lacking flexibility due to its rigid output format. More recently, newer models incorporate more natural reasoning traces and reinforcement learning. VLM-R1 (Shen et al., 2025) and R1-V (Chen et al., 2025a) align multimodal LLMs using step-by-step reasoning and policy optimization. VisualThinker-R1-Zero (Zhou et al., 2025) goes further by training 2B model via pure RL from scratch, achieving emergent inner reasoning. LMM-R1 (Peng et al., 2025) transfers CoT skills from language to vision through staged RL. Vision-R1 (Huang et al., 2025) combines reasoning trace supervision and RL with correctness and format rewards to train strong 7B VL reasoner. Different from these concurrent works, we propose high-quality multimodal reasoning dataset with R1-like reasoning traces for both SFT and RL, and provide comprehensive study on training paradigms. Reward Modeling in Reinforcement Learning. Reward design plays central role in reasoning-oriented RL. While model-based rewards offer flexibility (Kwon et al., 2023; Wang et al., 2024a; Gao et al., 2024), they are prone to reward hacking (Eisenstein et al., 2023; Chen et al., 2024b; Fu et al., 2025), making them risky for reasoning tasks. Recent VL models prefer binary correctness rewards (Huang et al., 2025; Zhou et al., 2025) for math or QA tasks, directly reinforcing accurate outputs. Others apply rule-based rewards, enforcing structured formats or logic chains (Liu et al., 2025; Deng et al., 2025a). While recent studies deploy strong reward models for enhancing LVLM reasoning, they are grounded by specific domains or simpler tasks (Muhtar et al., 2025; Tu et al., 2025). GRPO-style methods use relative ranking within output batches to guide optimization without value critics (Shao et al., 2024; Guo et al., 2025). Our Mix Reward objective combines the model-based and rule-based reward in four complex rewarding scenarios, yielding better performance than existing approaches."
        },
        {
            "title": "6 Conclusion",
            "content": "This work provides comparative analysis on the effectiveness of leveraging SFT or RL (more specifically, GRPO) to build LVLM with strong reasoning ability. We show by extensive experiments that distilling reasoning data and performing SFT is deficient way to transfer reasoning ability across modalities. We then extend our dataset to GRPO training with proposed mixed reward objective, which yields substantial improvement over the baseline models. We present several findings regarding combining SFT and GRPO and the correlation between reward, respond length, and final performance. These results indicate that reasoning is native emerging ability acquired from RL, rather than SFT, which merely equips the model with pseudo-reasoning ability."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank the Microsoft Accelerate Foundation Models Research Program for supporting our computing needs. 12 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. arXiv preprint arXiv:2402.11684, 2024a. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/ Deep-Agent/R1-V, 2025a. Accessed: 2025-02-02. Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024b. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025b. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065, 2025a. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025b. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1119811201, 2024. Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv. org/abs/2306.13394. Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward shaping to mitigate reward hacking in rlhf. arXiv preprint arXiv:2502.18770, 2025. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. arXiv preprint arXiv:2410.15115, 2024. 13 Preprint. Under review. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Afrar Jahin, Arif Hassan Zidan, Yu Bao, Shizhe Liang, Tianming Liu, and Wei Zhang. Unveiling the mathematical reasoning in deepseek models: comparative study of large language models. arXiv preprint arXiv:2503.10573, 2025. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pp. 29012910, 2017. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large visionlanguage models. arXiv preprint arXiv:2403.00231, 2024a. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530, 2024b. Adam Dahlgren Lindstr om and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua arXiv preprint Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Preprint. Under review. Dilxat Muhtar, Enzhuo Zhang, Zhenshi Li, Feng Gu, Yanglangxing He, Pengfeng Xiao, and Xueliang Zhang. Quality-driven curation of remote sensing vision-language data via learned scoring models. arXiv preprint arXiv:2503.00743, 2025. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. https: //github.com/om-ai-lab/VLM-R1, 2025. Accessed: 2025-02-15. Haoqin Tu, Weitao Feng, Hardy Chen, Hui Liu, Xianfeng Tang, and Cihang Xie. Vilbench: suite for vision-language process reward modeling. arXiv preprint arXiv:2503.20271, 2025. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=QWTCcxMpPA. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. URL https://arxiv.org/abs/2411.10440. Haoyan Yang, Ting Hua, Shangqian Gao, Binfeng Xu, Zheng Tang, Jie Xu, Hongxia Jin, and Vijay Srinivasan. Dynamic noise preference optimization for llm self-improvement via synthetic data. arXiv preprint arXiv:2502.05400, 2025a. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. 15 Preprint. Under review. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros aha moment in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive arXiv preprint upward multimodal alignment to enhance mathematical reasoning. arXiv:2408.08640, 2024. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 16 Preprint. Under review."
        },
        {
            "title": "A Data Generation",
            "content": "A.1 Prompt We show the prompts for captioning (Figure 8), R1 answer distillation (Figure 9), rewriting (Figure 10) and verification (Figure 11). Prompt for Captioning ### You are vision-language model generating highly detailed caption of an image. ### Summarize the environment or setting (indoor/outdoor, surroundings). ### Describe visible objects, people, or structures (colors, shapes, textures, positions). ### Transcribe all text verbatim. For equations, use LaTeX when appropriate but do not solve or interpret them. ### If structured data (tables, charts) appears, use Markdown formatting for clarity. ### Include labels, annotations, brand names, or logos, if any, otherwise dont mention them. ### Note any visible expressions or emotional tone factually, without speculation. ### Maintain logical order: from overall context to finer details. ### Provide only the caption without extra context or commentary. ### Be unbiased and faithful in your description, using natural language and Markdown only where relevant. Figure 8: Prompt for captioning with GPT-4-Turbo. Prompt for Distillation You have advanced visual perception abilities and can directly analyze images as if you are looking at them. You will be provided with detailed visual descriptions, but you should interpret them as if they represent your actual visual understanding rather than text-based captions. Answer questions as if you are visually perceiving the scene, not reading caption. Provide natural and confident responses about objects, relationships, and numerical or spatial reasoning. Use descriptive, visually grounded tone, avoiding mention of text. Never mention that you are reading text or captions. Infer spatial relationships, numerical properties, and logical conclusions based on the perceived image. If information is unclear, respond naturally as if there are visual limitations (e.g., It appears that. . . ). Caption: {caption} Question: {question} Figure 9: Prompt for distillation with Deepseek-R1. A.2 Aha-Moment Filtering We use the following list of keywords to identify aha moments: wait, again, double-check, hmm, mistake, alternatively, check, should confirm. All answers are matched with the logic: has aha = any([aha in text.lower() for aha in ahas]). A.3 Sample Demonstration for VLAA-Thinking-SFT-126K We show several examples from VLAA-Thinking-SFT-126K in Figure 14, Figure 15, Figure 16, Figure 17 and Figure 18. 17 Preprint. Under review. Prompt for Rewriting You will receive snippet of text that references description or caption of an image. Your task is to produce **nearly identical** version of that text with **minimal** changes, focusing on the following: 1. **Replace references to description, caption and rationale** with wording that references **the image.** - For example, The description says... could become The image shows... - The caption suggests... could become The image suggests... - Based on the rationale... could become Based on the image... - Make sure the replacement sounds natural but does **not** otherwise change the meaning. 2. **Preserve all line breaks, punctuation, and spacing** as much as possible, and make **no additional edits** outside of these replacements. 3. You should only output the rewritten content. Here is the input: {input} Figure 10: Prompt for answer rewriting with GPT-4-Turbo. Prompt for Verification You are fair evaluator. You will be given groundtruth and an answer from model. If the answer aligns with the groundtruth, output Yes. Otherwise, output No. Your output should only be Yes or No. groundtruth: {gold} answer: {pred} Figure 11: Prompt for verification with GPT-3.5-Turbo."
        },
        {
            "title": "B Details of SFT Experiments",
            "content": "B.1 Training To enhance the instruction following ability, we append task-specific instructions (i.e., MCQ, short answer) to questions. The system prompt shown in Figure 12 is used. We use global batch size of 128. Models are trained for 190 steps on 25K samples and 985 steps on 126K samples. All experiments are run on 8*H100 GPUs. Interestingly, we observe loss spikes for 25K SFT training on Qwen2-VL-7B which causes model collapse. Therefore, we run the settings for multiple times until we obtain normal loss curve, and use that checkpoint for evaluation. Figure 12: System Prompt used for training and evaluation. 18 Preprint. Under review. B.2 Evaluation We adopt VLMEvalKit (Duan et al., 2024) for all evaluation experiments. We set use custom prompt to False following the settings of most models in the toolkit. For higher efficiency, we set max pixels to 256*32*32, and max new tokens to 800. We also set system prompt as the one we used for training for consistent training-test behavior. The other hyperparameters are default to the original toolkit. We specify the split of datasets and metrics reported: 1. MathVista: The Test Mini split of MathVista dataset; overall accuracy. 2. MathVision: The Full test set of MathVision; overall accuracy. 3. MathVerse: The Test Mini split of MathVerse; accuracy of Vision Only . 4. DynaMath: The Full test set of DynaMath; overall accuracy. 5. WeMath: The Test Mini split of WeMath; Score (Strict). 6. LogicVista: The Full test set of LogicVista; overall accuracy."
        },
        {
            "title": "C Details of GRPO Experiments",
            "content": "C.1 Training We adapt our code from OpenRLHF framework (Hu et al., 2024). To suit for our need of deploying reward model on the same machine, we offload the reward model to CPU and only move it to GPU when performing rollouts and scoring. This design saves valuable GPU memory which accelerate the training process. We also perform dataset-specific inspection and find some issues for several datasets. For example, although ArxivQA contains only MCQ, the answer format includes A, A), (a), etc. And in the synthesis subset of Math PUMA, we find that some solutions only contain the value of solved unknown variables when the questions ask to output the entire function expression. We fix these issues by rule-based filtering and GPT-assisted rewriting, aiming to improve the quality of the VL-Thinking dataset. C.2 Evaluation We evaluate our models with an identical setting described in Appendix B.2. C.3 Case Study We present case demonstrating the improvement of VLAA-Thinker-Qwen2.5VL-7B over its backbone in Figure 13. 19 Preprint. Under review. Figure 13: case from MathVerse testmini (sample index 20). Markdowns are rendered for illustration purpose. Wrong reasoning paths are colored red. 20 Preprint. Under review. Figure 14: VL-Thinking sample from GeoQA170K. 21 Preprint. Under review. Figure 15: VL-Thinking sample from Math PUMA (subset Synthesis). 22 Preprint. Under review. Figure 16: VL-Thinking sample from CLEVR-Math. 23 Preprint. Under review. Figure 17: VL-Thinking sample from ArxivQA. 24 Preprint. Under review. Figure 18: VL-Thinking sample from ALLaVA-LAION."
        }
    ],
    "affiliations": [
        "Amazon Research",
        "The Pennsylvania State University",
        "University of California, Santa Cruz",
        "University of Texas at Dallas"
    ]
}