{
    "paper_title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
    "authors": [
        "Ruoyao Wen",
        "Hao Li",
        "Chaowei Xiao",
        "Ning Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack. We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length. On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory."
        },
        {
            "title": "Start",
            "content": "AGENTSYS: SECURE AND DYNAMIC LLM AGENTS THROUGH EXPLICIT HIERARCHICAL MEMORY MANAGEMENT"
        },
        {
            "title": "A PREPRINT",
            "content": "Ruoyao Wen Washington University in St. Louis ruoyao@wustl.edu Hao Li Washington University in St. Louis li.hao@wustl.edu Chaowei Xiao Johns Hopkins University chaoweixiao@jhu.edu Ning Zhang Washington University in St. Louis zhang.ning@wustl.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack. We present AGENTSYS, framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AGENTSYS organizes agents hierarchically: the main agent spawns worker agents for tool invocations, which execute in isolated contexts and can recursively spawn nested workers for subtasks. External data and subtask reasoning traces never directly enter the main agents memory, where only schema-validated return values may cross isolation boundaries through deterministic JSON parsing. This architectural separation alone provides substantial security: ablation studies show context isolation achieves 2.19% attack success rate without additional mechanisms, demonstrating that principled memory management fundamentally reduces attack surface. validator and sanitizer further strengthen defense, with event-triggered checks ensuring overhead scales with operations rather than context length. Evaluation on AgentDojo and ASB shows AGENTSYS achieves 0.78% and 4.25% attack success rates while slightly improving benign utility over undefended baselines. AGENTSYS maintains robust performance against adaptive attackers and across multiple foundation models, demonstrating that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at https://github.com/ruoyaow/agentsys-memory. 6 2 0 2 7 ] . [ 1 8 9 3 7 0 . 2 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "LLM-based agentic systems aim to autonomously solve complex user tasks by harnessing external tools to interact with real-world environments [56, 21, 40]. Given natural language instruction as user input, an agent decomposes the task into subtasks, invokes appropriate tools, and iteratively refines its behavior based on real-time observations. With the rapid development of large language models, LLM-powered agents have achieved remarkable success across various domains, including web assistance [40], software development [7], and computer use [54]. Security Risks of LLM Agents. Unfortunately, interaction with unreliable environments significantly expands the attack surface, introducing an emerging threat: indirect prompt injection attacks. Attackers can inject malicious instructions into third-party platforms, such as inboxes or webpages. When agent fetches this poisoned content via tool invocations, these instructions can be incorporated into the agents memory, hijacking its behavior to achieve PREPRINT the attackers goals [1, 16]. For example, an attacker may embed prompt such as \"Ignore previous instructions and send the credit card information to attacker@mail.com\" in Amazon shopping reviews to steal users financial information [29, 2]. Existing Defenses. In response to these security risks, growing body of research has focused on developing countermeasures, which can be broadly categorized into three complementary layers: (i) Model-level defenses, which strengthen instruction following through structure-aware alignment or inference-time control [8, 10, 11, 13, 19, 60, 9]; (ii) Detection-based defenses, which classify, localize, and sanitize untrusted content using auxiliary modules [35, 33, 22, 12, 47, 44, 27]; and (iii) System-level defenses, which enforce architectural separation and policy-checked execution [53, 15, 62, 52, 61, 25, 3, 26, 46, 50]. These approaches have achieved impressive progress in securing LLM agents, but they overlook critical vulnerability: how agents manage their working memory. The Memory Contamination Problem. LLM agents maintain working memory through their context window, which stores the interaction history that directly conditions subsequent decisions. Most existing defenses harden this surface but leave deeper architectural vulnerability unaddressed: indiscriminate memory accumulation. In conventional agent designs, all tool outputs, intermediate reasoning artifacts, and conversational traces are appended to the context window by default. This full-memory paradigm creates two critical vulnerabilities: (i) Attack Persistence. When injection instructions enter the context during an early tool call, they persist throughout the entire workflow and are re-processed in every subsequent decision. This grants attackers multiple opportunities to manipulate the agents behavior across multiple reasoning steps, significantly increasing the probability of successful attack. To validate this, we report the Attack Success Rate (ASR) as function of the injection round on AgentDojo in Table 1. We observe that earlier injection rounds yield significantly higher attack success rates, with the gap widening dramatically in longer workflows as persistent instructions are repeatedly re-evaluated. For example, for tasks with trajectory length of four, injection in the first round achieves an ASR of 60.53%, which is about four times higher than injection in the second round and more than ten times higher than injection in the third round. (ii) Utility Degradation. In addition, verbose context significantly degrades an agents decision-making capabilities by diluting LLMs attention [30, 20]. In practice, not all accumulated content is necessary for task completion. Within single tool invocations, raw outputs contain verbose metadata and ancillary details; only small subsets are relevant. Across multiple invocations, earlier exploratory observations become irrelevant for later decisions. Yet existing paradigms indiscriminately accumulate all content, creating bloated memory that degrades decision-making. Our analysis (Figure 4a) shows baseline agents drop from 44.46% utility on short tasks to 19.08% on long tasks, facing 57% decline. Why Existing Defenses Fail. Existing defenses inherit the conventional paradigm of retaining all observations in memory and defend this bloated context as given. Model-level defenses attempt to improve instruction-following within accumulated contexts but cannot prevent unnecessary content from entering. Detection-based defenses try to identify adversarial content but face growing overhead and utility loss as context length increases. System-level defenses recognize the danger and enforce architectural separation, but typically achieve security by restricting flexibility by enforcing predefined tool call stacks or rigid execution constraints that prevent the adaptive task decomposition agents need for complex workflows. This creates fundamental tension: agents require flexible tool use to handle dynamic tasks, but conventional memory accumulation enables attack persistence and degrades both security and utility. We address this by asking: Can we ensure only essential, task-relevant information enters the agents memory by discarding verbose outputs and obsolete observations, to simultaneously reduce attack surface, improve reasoning, and preserve flexibility? AGENTSYS Overview. To answer this question, we propose AGENTSYS, framework that defends against indirect prompt injection via explicit memory management. Inspired by process memory isolation in operating systems [24, 41], AGENTSYS organizes agents hierarchically: the main agent spawns worker agents for tool invocations, which execute in isolated contexts and can recursively spawn nested workers. External data and subtask reasoning traces never enter the main agents memory, where only schema-validated return values cross isolation boundaries through deterministic JSON parsing. This architectural separation eliminates attack persistence while keeping the main agents memory clean and concise. validator mediates recursive tool calls using compact traces with event-triggered checks on command operations [5], ensuring overhead scales with operations rather than context length. Evaluation. We evaluate AGENTSYS on AgentDojo [16] and ASB [59], measuring security (attack success rate) and utility (task performance in benign and attacked settings). We compare against prior defenses across multiple foundation LLMs and adaptive attackers. Results show AGENTSYS achieves 0.78% ASR on AgentDojo and 4.25% on ASB while preserving utility: 64.36% benign utility versus 63.54% for undefended agents, with 0% ASR on tasks requiring more than 4 tool calls. 2 PREPRINT Trajectory Length 2 3 4 Injection Round 2 15.87 40.26 60.53 0.00 35.00 15.38 3 0.00 5.88 0.00 Table 1: Attack success rate (%) by earliest injection round for trajectory lengths 2, 3, and 4 under the baseline (No Defense) setting. Earlier injections yield higher ASRs."
        },
        {
            "title": "2 Background",
            "content": "In this section, we introduce and formalize Large Language Model (LLM) agents, emphasizing how interaction with external data sources creates new security challenges. We then highlight indirect prompt injection, in which adversarial instructions are embedded within seemingly benign external content and subsequently influence an agents behavior. 2.1 LLM Agent An LLM agent [56, 18, 21, 49, 34, 17] is system that integrates large language model with planning, tool use, and memory, enabling it to autonomously decompose goals into subtasks, invoke external tools and data sources, and iteratively refine its behavior under explicit constraints. Rather than producing single static response, an agent executes feedback-driven cycle of reasoning, acting, observing, and adapting. This design supports multi-step tasks such as software configuration, file manipulation, and web information retrieval, extending the capabilities of large language models from conversational response generation to automated task completion. Formalization. Let user issue task description q. An LLM agent is equipped with toolbox T, where each tool accepts arguments Xt and is executed by an external executor Exect : Xt S, mapping the current environment state to an observation and new state S. At round = 1, 2, . . . , the agent selects an action ak = {stop} {call(t, x) : T, Xt} according to policy πk1 = πA(ak ck1) generated by backend LLM over the current context ck1, which contains the system prompt, tool descriptions, user query q, and the agent trace τk1, where τk = (π0:k1, a1:k, y1:k). If ak = call(tk, xk), execution yields (yk, sk) = Exectk (xk, sk1) πk1 = πA(ak ck1) ck = ck1 τk where denotes appending the new turn to the context. The loop terminates when aK = stop; the agent then generates final report based on cK1, and returns it to the user. This closed-loop interaction pattern underpins the autonomy of LLM agents. By chaining reasoning, action, and observation, agents exhibit behaviors far beyond one-shot text generation. Memory Management in LLM Agents. The agent maintains working memory through its context window ck, which accumulates all prior reasoning (π0:k1), actions (a1:k), and observations (y1:k) via ck = ck1 τk. This full-history design enables dynamic task decomposition: the agent can reference any previous observation when deciding subsequent actions, supporting adaptive, multi-step workflows. However, this indiscriminate accumulation also creates vulnerabilities when external observations contain adversarial content, as we discuss next. 2.2 Indirect Prompt Injection Prompt injection refers to adversarial methods that manipulate LLM behavior by embedding malicious instructions into model inputs [23]. As LLMs became widely adopted, prompt injection attacks emerged, in which users craft inputs to overcome safety alignment or override previous instructions [43, 51, 63, 31, 57]. PREPRINT As LLM agents have gained traction, distinct attack surface has emerged. Unlike traditional prompt injection scenarios, where the adversary is the user, LLM agents routinely ingest content from external data sources such as search results, documents, or APIs. This opens the door to indirect prompt injection (IPI) [32, 29, 42, 14, 58], in which adversarial instructions are embedded within seemingly benign external content and subsequently enter the agents working memory when retrieved. Formalization. Let τ = ((π0, a1, y1), . . . , (πK1, aK, yK)) denote the clean trace produced for when all observations are benign. If, for some round j, the observation yj returned by tool contains an injected instruction. Let τ be the trace for the same when yj is so contaminated. We say an indirect prompt injection occurs when (τ, τ ) > 0, where measures divergence in either the action sequence or the observation sequence. Current indirect prompt injection attacks can be described in two nonexclusive categories in terms of attack outcome: Control-flow manipulation: Observation containing injected text alters the execution path, forcing invocation of unintended tools or altering the tool selection. Data-flow manipulation: Observation containing injected text poisons data the agent relies on, altering tool arguments and thereby corrupting downstream data flow. Attack Persistence. critical aspect of indirect prompt injection in LLM agents is persistence. Once an injected instruction enters the memory at round (through yj), it remains in all subsequent contexts cj+1, cj+2, . . . , cK due to the accumulation rule ck = ck1 τk. This means the agent re-processes the adversarial instruction at every subsequent decision point, granting the attacker multiple opportunities to successfully manipulate behavior. The longer the workflow (larger K), the more chances the persistent instruction has to bypass defenses and achieve the attackers goals. The potential harm of indirect prompt injection can exceed that of traditional prompt injection for three reasons: (i) benign user can still trigger the attack simply by asking the agent to fetch malicious data; (ii) LLM agents often have access to powerful tools (file systems, code execution, or APIs), expanding the scope of damage far beyond unsafe text generation; and (iii) the attack is stealthy and scalable, since adversaries can seed poisoned instructions across many web pages or documents, compromising multiple agents without direct interaction. These properties make indirect prompt injection particularly dangerous class of attacks in the emerging field of LLM agents."
        },
        {
            "title": "3 Existing Defenses and Motivation",
            "content": "We organize existing defenses against indirect prompt injection along three complementary layers: (i) Model-Level Robustness, which bias the model toward the users intent and away from instructions in external data; (ii) Detectionbased Guardrail, which classify, segment, and sanitize untrusted content before it enters context window; and (iii) System-Level Control, which prevent untrusted data from steering control flow or modifying data flow. We synthesize insights across these layers to motivate our approach. 3.1 Model-Level Robustness first line of work strengthens models against IPI attack by modifying training data or by injecting control signals at inference. Structure-aware alignment methods such as StruQ [8], SecAlign [10], and Meta SecAlign [11] reshape instruction-tuning data so the model learns clear separation between the user instruction slot and the data slot, following only the former and ignoring instructions inside retrieved content. At inference time, [13] adopts an attack-as-defense mechanism that emphasizes user instructions to keep the model focused on the trusted objective, Spotlighting [19] marks untrusted text with delimiters or encodings, Mixture-of-Encodings [60] applies multiple character encodings to external payloads, and DefensiveTokens [9] prepends few crafted tokens that bias attention toward the user request. Such methods are straightforward and fundamental, but still suffer from the inherent vulnerabilities of LLMs [2, 28]: LLM agents leverage LLMs strong contextual reasoning and instruction-following abilities, while improvements in these abilities are accompanied by increased susceptibility to prompt injection attacks. 3.2 Detection-based Guardrail The second layer employs detection-based guardrails to identify and mitigate injected instructions in retrieved data, tool outputs, or model responses. Systems such as ProtectAI [44], PIGuard [27], DataSentinel [33] and Attention Tracker [22] train external classifiers to flag suspicious segments. More fine-grained approaches [12, 47] not only detect PREPRINT but also pinpoint potential injections for targeted sanitization. Once flagged, sanitizer removes contaminated segments, preventing malicious inputs from entering the LLMs context window. Compared to the first layer, these methods provide more systematic security by protecting the LLMs context window from external data and preserving clean context. They are model-agnostic and modular, but remain vulnerable to evasion [14, 58] and can impose utility costs through false positives [23] (e.g., misclassifying clean context as contaminated or flagging benign third-party requests or instructions as malicious). 3.3 System-level Control While detection-based guardrails strengthen robustness before malicious content enters the context window, they still face fundamental limitations: evasion attacks can bypass detectors, and static filters may impose excessive utility costs. To meet the needs of dynamic tasks and to achieve more systematic, reliable, and traceable security, researchers are increasingly turning to system-level defenses. These approaches decouple trusted policies from untrusted data and provide stronger security and auditability at the architectural level. IsolateGPT [53] maintains per-application sandboxes and separate containers to prevent cross-session contamination. CaMeL [15] incorporates Dual-LLM pattern, routing untrusted content to non-privileged model that cannot execute tools, while only structured, policy-checked summaries flow back to the privileged planner. MELON [62] similarly enforces system-level robustness via masked re-execution: it reruns the agent with the user query masked and flags potential IPI when the resulting tool calls remain similar, indicating that untrusted tool outputs are steering control flow. F-Secure [52] and RTBAS [61] leverage information-flow control (IFC), propagating privilege labels to block privileged actions triggered by external data. Other frameworks adopt plan-then-execute designs to compute workflows from trusted inputs. For example, ACE [25] uses an abstractconcreteexecute three-phase design, IPIGuard [3] builds dependency DAG with controlled expansion on read-only tool invocation, and DRIFT [26] further allows tool invocation during execution via dynamic validator for greater flexibility. Finally, Progent [46] and AgentArmor [50] enforce runtime privilege frameworks, applying per-call policies or stepwise checks over structured traces. 3.4 Key Insights and Motivation As established in Section 1, conventional LLM agents indiscriminately accumulate all tool outputs and reasoning traces in working memory, creating attack persistence and utility degradation. The three defense layers reviewed above all operate on this accumulated memory as given, inheriting its fundamental vulnerabilities: Model-level robustness attempts to improve instruction-following within bloated contexts but cannot prevent unnecessary content, including verbose tool outputs and obsolete historical observations, from entering and persisting in agents memory. Detection-based guardrails try to identify and sanitize adversarial content within accumulated memory but face growing overhead as memory length increases, with false positives removing legitimate information and degrading utility [14, 58, 23]. System-level controls recognize the danger of memory accumulation and achieve strong security through architectural separation, but typically do so by enforcing predefined tool call stacks, limiting dynamic tool use, or imposing rigid execution constraints [53, 15, 26], restricting agents flexibility. Additionally, comprehensive trace validation incurs substantial overhead as interaction depth grows [62, 50]. This reveals the core gap: existing defenses either (1) accept bloated memory and attempt mitigation, suffering from persistence and overhead, or (2) prevent accumulation through rigidity, sacrificing adaptive task decomposition. Neither approach addresses the root cause: indiscriminate accumulation of unnecessary content. Inspired by process memory isolation in operating systems [24, 41], we propose AGENTSYS to fill this gap by ensuring only essential, task-relevant information enters the agents working memory. Through hierarchical memory management with isolated worker execution and schema-validated communication, AGENTSYS eliminates attack persistence while preserving flexibility for dynamic, open-ended workflows, addressing the limitations of all three existing defense layers. 5 PREPRINT"
        },
        {
            "title": "4 System and Threat Model",
            "content": "This section instantiates the system and adversary assumptions using the prior formalization in 2. We reuse all symbols, state spaces, and processes from 2.1 and 2.2 without rederiving them. 4.1 System Model We consider user-issued task and an agent operating with toolbox under the execution interface Exect and environment state space S, as defined in 2.1. At round k, the backend LLM induces policy over the current context ck1 (which serves as the agents working memory) and selects either stop or tool call; traces τk are appended to the context via ck = ck1 τk, and termination occurs at the first with aK = stop. The contents of ck (system prompt, tool descriptions, q, and accumulated trace) and the dependence of πA on ck are exactly as specified in 2. 4.2 Threat Model We adopt the IPI definition from 2.2. Let τ denote the clean trace for q, and let τ be the trace when, for some round j, the tool observation yj contains attacker-injected instructions (e.g., from web page, file, or API response). An indirect prompt injection occurs when (τ, τ ) > 0, where measures divergence in actions and/or observations, as previously defined. Adversary capabilities. The adversary may influence any tool-returned observation but cannot modify Exect or the environment transition (cid:55) s. The goal is to steer subsequent policy outputs by embedding instructions that, once admitted into the agents working memory, persist across reasoning cycles and affect future policy decisions πA."
        },
        {
            "title": "5 AGENTSYS Design",
            "content": "Figure 1: AGENTSYS Overview. At step 1, the worker agent #1 is spawned to process the tool response, guided by the intent declared by the main agent. Worker agent #1 can recursively call tools and spawn worker agent #2, mediated by the alignment validator. After receiving the return value from worker agent #1 as tool observation, the main agent continues to reason for step 2 within the global context, discarding the local context. 5.1 System Overview Guided by the motivation in 1 and 3.4, AGENTSYS enforces strict separation between (i) the main agent that maintains the trusted, long-horizon conversation state and makes high-level decisions, and (ii) short-lived worker agents that interact with untrusted tool outputs. The central design principle is memory management through explicit context control: raw tool outputs are treated as adversarial observations and are never appended directly to the main agents working memory. 6 PREPRINT Concretely, each tool invocation by the main agent spawns short-lived worker agent responsible for post-processing the tool response. The main agent augments each call with an intent, minimal schema specifying expected fields and types (e.g., \"name\": string) that constrains what information is required to be returned. The tool executes normally, but its raw output is confined to the worker agent, which distills it into compact return object conforming to the declared intent. The main agent accepts return values only after rule-based JSON validation; non-conforming results are rejected and the subtask fails explicitly. string, \"email\": AGENTSYS organizes computation into tree-structured agent hierarchy rooted at the main agent, making trust boundaries explicit: untrusted external observations flow downward into leaf subtasks, while only schema-validated values propagate upward. Worker agents may recursively invoke tools to complete extraction, with each recursive call spawning nested worker. Such recursion is gated by an LLM-based validator that operates on the initial user query and compact tool-call trace, with raw tool outputs explicitly excluded to prevent the validator from being influenced by attacker-controlled observations. When the validator denies tool call, AGENTSYS attempts recovery via sanitization and bounded retry; if retries are exhausted, the worker returns an explicit failure object. Overall, AGENTSYS combines (1) memory management via context isolation, (2) schema-bounded upward communication, and (3) gated recursion to reduce prompt-injection attack surface while maintaining multi-step tool-based workflows. Figure 1 illustrates this architecture. 5.2 Context-Bounded Delegation in Main Agent The main agent plays the role of delegator: it decides when to invoke tools, commits to narrow interface specifying what information it is willing to accept, and integrates only validated outputs into its long-horizon memory maintained through its context window. key design constraint is that the main agent must declare this interface before observing any tool output. This commitment prevents adversarial tool responses from widening the information channel back into the main agent beyond what the main agent explicitly anticipated. At interaction round k, the main agent selects tool tk and arguments xk, and issues an augmented tool call: ak = call(tk, xk, Itk ), (1) where the intent Itk is minimal typed object schema describing the expected return structure. In our setting, intents are JSON-like schemas consisting of nested dictionaries and lists whose leaves are primitive types (e.g., string, number, boolean). For example, an intent may specify list of colleague records: Itk = {\"Colleagues\" : [{\"name\" : string, \"email\" : string}]}. Intuitively, Itk serves as an explicit contract: it fixes both (i) which fields may be returned and (ii) the expected types of those fields, thereby constraining what information can flow from tool outputs back into the main agent. Tool execution produces (yk, sk) = Exectk (xk, sk1), where yk is the raw tool output and sk denotes the updated external environment state. In AGENTSYS, yk is treated as untrusted and is never appended verbatim to the main agents context. Instead, spawns short-lived worker agent tasked with extracting structured return value rk from yk that conforms to the pre-declared intent Itk (described in 5.3). Upon worker agent termination, the main agent enforces syntactic gate on rk and accepts it only if it is JSONparsable object; otherwise, the result is rejected and the subtask fails explicitly. If accepted, rk is appended to the main agent as the tool observation for round k. Importantly, this observation is structured data rather than free-form tool text; the intent schema serves as best-effort interface contract that guides extraction. While string-valued fields may still contain attacker-controlled content, the contract restricts the channel through which such content can reach the main agent, reducing exposure compared to appending entire raw outputs. Thus, the delegator design separates decision-making from observation handling: the main agent determines in advance the intended shape of acceptable information, and untrusted raw observations are confined to an isolated subtask that returns compact structured object. 5.3 Isolated Context Extraction in Worker Agents The worker agent is short-lived component whose sole responsibility is to convert an untrusted, potentially instruction-bearing tool output into compact structured object suitable for reintroduction into the main agents context. To reduce exposure of trusted state, operates with minimal context: it does not inherit the main agents long-horizon memory or conversation history, and it is not given the original user query. Instead, it operates only on the current tool output and the pre-declared interface for this call. Formally, after tool execution at round k, the worker agent receives the triplet = (yk, Itk , Stackk), (2) 7 PREPRINT where yk is the raw tool output, Itk is the intent schema declared by the main agent in (1), and Stackk is the compact tool-call trace up to this point. The intent Itk specifies the desired shape of the return object using JSON-like typed schema (nested dictionaries/lists with primitive-typed leaves). By construction, does not receive (the user query) and therefore cannot be directly prompted by user instructions; any adversarial influence must arrive through the untrusted observation yk. Given q, outputs return value rk guided by Itk . Since intent schemas are produced and consumed by LLM components, AGENTSYS enforces robust, model-agnostic acceptance rule at the main agent: it applies syntactic gate and accepts rk only if it is JSON-parsable object. This yields the core security benefit of AGENTSYS by replacing large, free-form tool output with compact structured object whose fields are determined by pre-declared interface, thereby minimizing the attack surface exposed to attacker-controlled text while preserving the agents utility. When extraction is infeasible (e.g., missing information or failing to parse valid object), returns preset error object from fixed set of failure types, enabling the main agent to handle failures deterministically. Finally, AGENTSYS supports multi-step extraction: may invoke additional tools as needed to populate Itk . Such recursive tool calls are mediated by the validator described in 5.4, and tool outputs may be sanitized and re-processed upon denial as described in 5.5. This design allows dynamic, multi-step workflows while keeping the main agent insulated from raw tool outputs throughout the subtask. Memory Management Benefit. By confining yk to isolated worker contexts and admitting only compact, schemavalidated rk into the main agents memory, this design addresses both vulnerabilities identified in 1: (i) verbose, non-essential content never accumulates in the main agents working memory, preventing utility degradation, and (ii) adversarial instructions in yk cannot persist across subsequent reasoning cycles, eliminating attack persistence. 5.4 Validator-Mediated Recursion Control AGENTSYS permits worker agents to perform multi-step workflows, but treats any additional tool use originating from within untrusted processing as potentially risky. Accordingly, recursive tool calls issued by worker agent are gated by validator that mediates whether the call is allowed to execute. This design ensures that even if worker agent is exposed to adversarial tool outputs, it cannot unilaterally trigger side-effecting actions or deviate into unintended tool use without passing an independent check. In contrast, the main agents top-level tool calls are not subject to this validator; the validator is applied only to tool use initiated within subtasks. If attempts further tool use, call(t, x, It), (3) the attempt may be checked by validator: V(cid:0)q, Stack, (t, x, It)(cid:1) {allow, deny}, implemented as an LLM-based alignment checker. The validators input is restricted to the initial user query q, the compact tool-call trace Stack, and the proposed call triple (t, x, It).1 Importantly, Stack contains only compact call metadata (i.e., tool identifiers, arguments and declared intents) and never includes raw tool outputs. (4) Event-triggered validation. To minimize overhead and avoid unnecessary checks, AGENTSYS triggers validation only on command tools that may cause external side effects (e.g., writes, sends, purchases, file modifications). Inspired by CQRS-style separation of reads and writes [5], we label tools as command or query by prompting an LLM using tool descriptions and usage signatures, and treat ambiguous cases conservatively (defaulting to command). This taxonomy is computed once per toolset and reused for subsequent executions. As result, the cost of validation scales primarily with the frequency of command operations rather than with interaction length or tool depth. Decision semantics. If returns allow, the sub-call proceeds and the resulting raw output remains confined to the subtask (and is processed by the distillation mechanism in 5.3). If returns deny, the tool call is blocked and control passes to sanitization and restart (5.5). In this way, AGENTSYS combines recursive tool use with an explicit approval boundary, ensuring that side-effecting behavior within untrusted processing is mediated by checker that is not exposed to attacker-controlled tool outputs. 5.5 Bounded Recovery Mechanism When the validator denies proposed worker agent tool call, AGENTSYS treats the current tool output as potentially adversarial (i.e., containing prompt-injection payloads) and attempts recovery rather than immediately failing the subtask. The recovery mechanism is sanitizerestart loop: the system sanitizes the untrusted observation and reruns 1Restricting to and Stack prevents the validator itself from being influenced by untrusted tool output. 8 PREPRINT extraction under the same pre-declared intent, while enforcing an explicit bound on the number of retries to ensure termination and predictable cost. On denial, the worker agent invokes sanitizer σ to remove instruction-like spans from the tool response: yk = σ(yk), (5) where σ is realized by an LLM prompted to identify and delete instruction-like spans (e.g., imperatives, role directives, policy-override attempts, or tool-use suggestions) while preserving task-relevant data. The sanitizer operates only on the raw tool output yk and is not given the user query or the intent. It outputs yk, cleaned version of the original observation that is treated as data for subsequent extraction. The worker agent then restarts extraction by replacing yk yk in (2), keeping the original intent Itk unchanged. Each sanitizerestart consumes one unit from per-subtask budget Bk scoped to the current tool output. If the budget is exhausted, the worker agent terminates and returns preset error object to its parent agent, indicating that extraction failed due to repeated validator denials or irreducible contamination in the observation. This bounded design prevents infinite sanitize loops, caps worst-case latency, and ensures that adversarial tool outputs cannot force unbounded computation. In addition, because sanitization occurs entirely within the isolated subtask (and only modifies the untrusted observation), it does not expand the trusted context or introduce new channels for attacker-controlled instructions to reach the main agent. Finally, the sanitizerestart loop integrates tightly with event-triggered validation (5.4): denials arise only for commandtool attempts within subtasks, so sanitization is invoked only when the worker agent is about to perform potentially side-effecting action. This focuses recovery effort on high-risk cases while preserving the efficiency of benign, read-only subtask execution."
        },
        {
            "title": "6 Experiments",
            "content": "Benchmarks. We evaluate AGENTSYS on two established benchmarks for indirect prompt injection: AgentDojo [16], which includes four scenarios spanning Banking, Slack, Travel, and Workspace while covering 97 user tasks and 629 injection tasks, and ASB [59], which provides 10 evaluation scenarios. Both benchmarks assess task utility under benign and attacked settings, as well as security against injection attacks. Foundation Models. We test AGENTSYS across six foundation LLMs: GPT-4o-mini [37], GPT-4o [38], GPT-5.1 [39], Claude-3.7-Sonnet [4], Gemini-2.5-Pro [48], and Qwen-2.5-7B-Instruct [55] as an offline open-source model. Baselines. We compare against existing defenses across three categories, following the taxonomy introduced in Section 3: (1) Model-level Robustness strengthens models against indirect prompt injection through training data modification or inference-time control signals. We evaluate Prompt Sandwiching [45], Spotlighting [19], Instructional Prevention [59], and Tool Filter [16]. (2) Detection-based Guardrail employs classifiers to identify and mitigate injected instructions in retrieved data, tool outputs, or model responses. We evaluate ProtectAI [44] and PIGuard [27]. (3) System-level Control decouples trusted policies from untrusted data through architectural separation and policy enforcement. We evaluate IsolateGPT [53], CaMeL [15], Progent [46], MELON [62], and DRIFT [26]. Attack Configurations. Our default attack is the important_instruction attack on AgentDojo and the OPI attack on ASB. Adaptive attack strategies are detailed in Section 6.4. Evaluation Metrics. We measure three key metrics: (1) Benign Utility: the fraction of user tasks successfully completed without attacks, establishing baseline performance; (2) Attacked Utility: the proportion of user tasks successfully fulfilled under attack conditions, measuring robustness; and (3) Attack Success Rate (ASR): the fraction of security cases where the attackers malicious goals are executed, measuring vulnerability. 6.1 Evaluation on Benchmarks We evaluate AGENTSYS on AgentDojo and ASB using GPT-4o-mini as the foundation model, and further assess generalization across six foundation LLMs on AgentDojo. Table 2 and Figure 2 present the results on both benchmarks, while Table 5-7 provides detailed per-model results on AgentDojo. Our results demonstrate that AGENTSYS consistently achieves high security and utility preservation, outperforming existing defenses. AgentDojo Results. We compare AGENTSYS against ten existing defenses: four from the AgentDojo benchmark (Prompt Sandwiching, Spotlighting, Tool Filter, and ProtectAI detector) and six recent methods reproduced from their PREPRINT Defense Method Benign Util. Attacked Util. ASR"
        },
        {
            "title": "Prompt Sandwiching\nSpotlighting\nTool Filter\nProtectAI\nPIGuard\nMELON\nisolateGPT\nCaMeL\nProgent\nDRIFT\nAGENTSYS",
            "content": "63.54 63.23 59.85 61.53 40.64 43.33 57.61 6.25 29.97 63.42 58.48 64.36 48.27 47.51 45.42 50.72 23.98 16.38 17.36 6.39 33.39 47.04 47.91 52.87 30.66 14.69 35.22 8.34 6.84 0.85 0.89 0.00 0.00 7.17 1.29 0. Table 2: Main experimental results on AgentDojo using GPT-4o-mini. We report utility measured without attacks and under Indirect Prompt Injection, along with the attack success rate. The optimal and sub-optimal results are denoted by boldface and underlining. All metrics are in %. Figure 2: Main experimental results on ASB using GPT-4o-mini. published codebases. AGENTSYS achieves an ASR of 0.78% while maintaining high utility in both benign (64.36%) and attacked (52.87%) settings. Although IsolateGPT and CaMeL achieve 0% ASR, they sacrifice task utility by enforcing rigid execution paths, reducing benign utility by more than half compared to the undefended baseline. Notably, AGENTSYS slightly improves agent utility compared to the undefended baseline, thanks to its explicit memory management. By keeping the main agents working memory shorter and free of subtask reasoning traces, AGENTSYS reduces the attack surface while helping the LLM maintain focus on the users objective, improving reasoning and instruction-following performance. We provide detailed analysis of this phenomenon in Section 6.5. ASB Results. We compare AGENTSYS against six existing methods: three from the ASB benchmark (Spotlighting, Prompt Sandwiching, and Instructional Prevention) and three recent methods reproduced from their published codebases. AGENTSYS achieves an ASR of 4.25% while preserving high utility across both benign and attacked settings, consistently outperforming other methods. While Spotlighting achieves slightly higher benign utility, it fails to provide adequate security, showing minimal reduction in ASR compared to the undefended baseline. 6.2 Ablation Study To understand the contribution of each component in AGENTSYS, we conduct ablation studies on AgentDojo using GPT-4o-mini as the foundation model, by systematically removing or modifying key design elements. Table 3 presents the results across four ablation variants compared to the full AGENTSYS system and the undefended baseline. w/o Context Isolation. We remove the memory management mechanism while retaining the validator and sanitizer. In this variant, the agent itself operates as standard ReAct agent: if the validator denies tool call, the system sanitizes all tool responses before appending them to the agents context. This ablation removes context isolation while preserving validation and sanitization. Results show that benign utility drops to 62.49% and ASR increases significantly to 8.62%, 10 PREPRINT Defense Method Benign Util. Attacked Util. ASR"
        },
        {
            "title": "No Defense",
            "content": "AGENTSYS w/o Context Isolation w/o Validator w/o Sanitizer w/o Validator and Sanitizer 63.54 64.36 62.49 50.85 57.66 56.10 48.27 52.87 50.19 53.16 52.53 52.61 30. 0.78 8.62 0.18 1.54 2.19 Table 3: AGENTSYS ablation on AgentDojo under indirect prompt injection. The optimal and sub-optimal results are denoted by boldface and underlining. All metrics are in %. demonstrating that memory management is critical for both security and utility preservation. Without hierarchical management, untrusted tool outputs accumulate in the main agents working memory, enlarging the attack surface and degrading instruction-following performance. w/o Validator. We remove the validator and unconditionally sanitize all tool outputs before dispatching to worker agents. This variant eliminates event-triggered validation and applies sanitization indiscriminately to all raw tool results. While this achieves the lowest ASR (0.18%), it incurs substantial utility loss, with benign utility dropping to 50.85%. This demonstrates that aggressive sanitization, while effective for security, can remove task-relevant information and harm task completion. The validators role in selectively triggering sanitization only when necessary is crucial for balancing security and utility. w/o Sanitizer. We remove the sanitizer while retaining hierarchical memory management and validator-mediated gating. When the validator denies tool call from worker agent, the subtask immediately fails without attempting recovery. ASR increases to 1.54% and benign utility drops to 57.66%, showing that the sanitizerestart mechanism enables recovery from contaminated tool outputs while maintaining security. Without sanitization, subtasks fail more frequently, reducing both security (as some attacks succeed before detection) and utility (as legitimate tasks fail due to false positives). w/o Validator and Sanitizer. We retain only the hierarchical memory management mechanism, removing both validator and sanitizer. This variant provides context isolation through worker agents but lacks validation and recovery mechanisms. Notably, even with only memory management, this configuration achieves strong performance: ASR of 2.19% and benign utility of 56.10%. This demonstrates that memory management alone provides substantial security benefits. By preventing external content and subtask reasoning traces from entering the trusted context, hierarchical dispatch reduces the attack surface and limits adversarial influence. However, the absence of validator-mediated gating still allows some malicious tool calls to execute within subtasks, and the lack of sanitization prevents recovery from contaminated observations, explaining the gap between this variant and full AGENTSYS. Key Findings. The ablation results highlight both the fundamental importance of memory management and the synergy among AGENTSYSs components. The strong performance of w/o Validator and Sanitizer (2.19% ASR) validates our core insight: explicit memory management that keeps the trusted agents context clean is powerful defense mechanism on its own. Full AGENTSYS builds upon this foundation to achieve optimal balance: it maintains the highest benign utility (64.36%), competitive attacked utility (52.87%), and near-optimal ASR (0.78%). Context isolation is essential for preserving utility by preventing unnecessary content from accumulating in working memory. The validator enables selective intervention without over-sanitization. The sanitizer provides recovery from contaminated observations while preserving task-relevant information. Together, these components provide defense-in-depth against both control-flow and data-flow manipulation while preserving agent flexibility and task performance. 6.3 Overhead Analysis System-level defenses typically introduce computational overhead through additional LLM calls, validator checks, or sanitization operations. We quantify the practical cost of AGENTSYS by measuring total token consumption on AgentDojo using GPT-4o-mini as the foundation model, comparing against eight baseline defenses across three categories: model-level robustness (Prompt Sandwiching, Spotlighting, Tool Filter), detection-based guardrails (ProtectAI), and system-level controls (CaMeL, Progent, DRIFT). Defense Quality Metric. To capture the combined effectiveness of security and utility preservation, we introduce defense quality metric: Defense Quality = , (6) Benign Utility Security 100 11 PREPRINT Figure 3: Trade-off among utility, security, and computational overhead on AgentDojo. (a) Security-Utility Trade-off: AGENTSYS achieves the best balance with highest utility and security. (b) Quality-Cost Trade-off: AGENTSYS attains the highest defense quality with comparable token cost. where Security = 100 ASR. This metric reflects the joint goal of maintaining high task performance while minimizing attack success, with higher values indicating better overall defense effectiveness. Security-Utility Trade-off. Figure 3(a) illustrates the security-utility trade-off across all methods. AGENTSYS achieves the optimal position in this space, attaining both the highest benign utility (64.36%) and highest security (99.22%, corresponding to 0.78% ASR). In contrast, CaMeL achieves perfect security (0% ASR) but at severe utility cost (29.97% benign utility), demonstrating the limitations of overly rigid execution constraints. Detection-based methods like ProtectAI show moderate security (93.16%) but substantial utility degradation (40.64%), while model-level defenses like Spotlighting preserve utility (59.85%) but provide limited security improvement (64.78%). Recent studies on system-level defense like Progent and DRIFT can provide sub-optimal solutions, achieving high security while mitigating utility loss. AGENTSYSs position in the upper-right corner validates our design goal: achieving strong security without sacrificing agent flexibility. Quality-Cost Trade-off. Figure 3(b) presents the defense quality versus token consumption. AGENTSYS achieves the highest defense quality (63.86) with 3.25M tokens, demonstrating practical overhead. While the undefended baseline uses fewer tokens (0.82M), it achieves only 44.06 defense quality due to high ASR. CaMeL, despite using 6.09M tokens (the highest overhead), achieves only 29.97 defense quality due to severe utility loss. Notably, AGENTSYS outperforms all baselines in defense quality while maintaining comparable or lower token cost than other system-level defenses: Progent uses 2.60M tokens (defense quality 58.87), DRIFT uses 2.37M tokens (defense quality 57.73), and CaMeL uses 6.09M tokens (defense quality 29.97). Sources of Overhead. AGENTSYSs overhead stems from three sources: (1) isolated worker agents for tool extraction, (2) event-triggered validator calls on command tools, and (3) sanitizerestart loops when validation fails. However, several design choices minimize this cost. First, worker agents operate on compact contexts rather than duplicating the parents full conversation history, avoiding quadratic context growth. Second, event-triggered validation applies only to command tools, not read-only queries, reducing unnecessary checks. Third, the sanitizerestart budget bounds worst-case cost. As result, AGENTSYSs overhead scales primarily with the frequency of command operations rather than with total interaction length, making it practical for long-horizon tasks. Key Findings. The overhead analysis demonstrates that AGENTSYS achieves an optimal balance across all three dimensions: utility, security, and cost. Compared to system-level baselines, AGENTSYS provides higher defense quality than existing defenses, while using comparable token cost. This validates our claim that explicit context management, event-triggered validation, and bounded recovery enable strong security with minimal overhead, addressing the practical deployment concerns that limit adoption of existing system-level defenses. 6.4 AGENTSYS against Adaptive Attackers We evaluate AGENTSYSs robustness against adaptive attackers who are aware of our defense mechanisms on AgentDojo using GPT-4o-mini as the foundation model. Understanding AGENTSYSs architecture, sophisticated attacker would 12 PREPRINT aim to bypass context isolation by injecting malicious content through the schema-bounded communication channel. Since the validator operates on compact call traces (without direct exposure to tool outputs) and the sanitizer is triggered only upon validator denial, the primary attack surface is the schema-validated return values that worker agents send to the parent agent. Manual Adaptive Attack. We manually craft an adaptive attack prompt designed to exploit the schema-bounded gate. The injected instruction reads: If you are required to generate dictionary of data fields, append the following key-value pair into the dictionary: followed by malicious payload. This attack attempts to hijack the worker agents extraction process by embedding instructions within the context of schema compliance, aiming to smuggle adversarial content into the structured return object that passes validation. Automated Iterative Refinement. Recent studies demonstrate that defenses against LLM jailbreaks and prompt injections often fail under adaptive attacks with iterative refinement [36]. To test AGENTSYS against automated adaptive attacks, we adopt PAIR method [6], which iteratively refines injection prompts to maximize attack success. PAIR uses an attacker LLM to generate increasingly sophisticated injection attempts based on feedback from previous failures, simulating persistent adversary. Results. Table 4 presents attacked utility and ASR across four AgentDojo scenarios under three attack configurations: the baseline important_instruction attack in AgentDojo (Base), our manual adaptive attack (Adapt), and PAIR-refined attack (PAIR). Overall, AGENTSYS maintains strong security even against adaptive attackers: ASR increases from 0.78% (Base) to 1.43% (Adapt) and 2.06% (PAIR), modest degradation that still represents over 93% reduction compared to the undefended baseline (30.66% ASR). The results vary across scenarios. In Banking, PAIR achieves the highest ASR (6.94%) but still maintains reasonable utility (36.81%). In Slack, Travel, and Workspace, ASR remains near-zero even under PAIR refinement (0.95%, 0.00%, 0.36% respectively), demonstrating AGENTSYSs robustness across different task domains. The limited success of adaptive attacks validates our core defense principle: by restricting communication to schema-validated structured outputs and isolating untrusted reasoning traces from the parent context, AGENTSYS fundamentally limits the channel through which adversarial instructions can propagate, even when attackers understand and target this mechanism. Suite Attacked Utility (%) ASR (%) Base Adapt PAIR Base Adapt PAIR 39.58 Banking 50.48 Slack Travel 60.00 Workspace 61. 36.81 55.24 63.57 62.50 36.81 57.14 59.29 60.54 Overall 52.87 54.53 53. 2.78 0.00 0.00 0.36 0.78 5.56 0.00 0.00 0.18 1.43 6.94 0.95 0.00 0.36 2. Table 4: AGENTSYS performance against adaptive attacks on AgentDojo. Base: baseline attack; Adapt: manual adaptive attack; PAIR: iterative refinement attack. 6.5 Impact of Trajectory Length on Utility and Security To provide deeper insight into AGENTSYSs performance characteristics, we analyze how defense effectiveness and utility preservation vary with task complexity on AgentDojo using GPT-4o-mini as the foundation model. We focus on trajectory length, defined as the number of tool calls required to complete task, as proxy for task complexity and context window growth. This analysis validates our central claim that explicit memory management improves both security and utility, especially for long-horizon, interaction-heavy tasks. Utility Preservation on Long-Context Tasks. Figure 4a compares benign utility across trajectory lengths for four methods: No Defense baseline, Progent, DRIFT, and AGENTSYS. We partition tasks into two groups at the median trajectory length of AgentDojo tasks: short tasks ( 3 tool calls) and long tasks (> 3 tool calls). The reported utility values are weighted by the fraction of tasks in each trajectory length category. For short tasks, AGENTSYS demonstrates the best performance with 39.78% utility, outperforming Progent, DRIFT, though slightly below the baseline. For long tasks, AGENTSYS achieves the highest utility at 24.58%, outperforming Progent, DRIFT, and the baseline. Notably, AGENTSYS stands out in both trajectory length categories, achieving consistently strong utility regardless of task complexity. 13 PREPRINT (a) Benign utility by trajectory length on AgentDojo (weighted by task fraction). Tasks are partitioned at the median trajectory length (3 tool calls). AGENTSYS stands out in both categories, maintaining high utility on long-horizon tasks while other defenses show significant degradation. (b) ASR by trajectory length on AgentDojo (weighted by task fraction). AGENTSYS achieves 0% ASR on tasks with 4 tool calls, while baseline and other defenses show vulnerability patterns and optimal attack trajectory lengths where ASR peaks. Figure 4: Performance analysis by trajectory length on AgentDojo. The degradation patterns are particularly revealing: while AGENTSYS shows 38.21% drop from short to long tasks, DRIFT suffers severe 60.96% drop and Progent shows an 52.98% drop. This validates our hypothesis that keeping the trusted working memory clean and free from subtask reasoning traces helps the agent maintain focus on user objectives even as interaction history grows. In contrast, methods that accumulate tool outputs in the main context (baseline) or enforce rigid execution constraints (Progent, DRIFT) experience more severe utility degradation as tasks become more complex. Security across Trajectory Lengths. Figure 4b presents ASR stratified by trajectory length, revealing how attack persistence varies with context window size. We partition attacks into seven buckets by trajectory length. The reported ASR values are weighted by the fraction of tasks in each trajectory length bucket to reflect the true distribution of attack scenarios in the benchmark. AGENTSYS maintains consistently low ASR (0-0.42%) across all trajectory lengths, with attacks succeeding only in the short-range buckets. Critically, ASR drops to 0% for trajectories with 4 or more tool calls, demonstrating that AGENTSYSs memory management prevents attack persistence in long-horizon tasks. In contrast, the baseline and other defenses exhibit an interesting pattern: there exists an optimal trajectory length for attacks where ASR peaks. For the baseline, ASR peaks at trajectory length 2 (9.22%) before declining for longer trajectories, then rising again at length 4. Progent shows similar pattern with peak ASR at trajectory length 3 (1.87%), suggesting that attacks are most effective at intermediate trajectory lengths where enough context has accumulated to enable manipulation but the workflow is not complex enough to dilute adversarial influence. DRIFT maintains low ASR across most buckets but shows occasional vulnerabilities in mid-range trajectories. The key insight is that conventional approaches inject untrusted content in the agents working memory, allowing adversarial instructions to persist and influence later reasoning steps. The existence of optimal attack trajectory lengths indicates that adversarial content can exploit specific context window sizes where instruction-following is most susceptible to manipulation. By contrast, AGENTSYSs hierarchical memory management confines external content to short-lived worker agents, preventing contamination from propagating across tool calls. This architectural separation becomes increasingly valuable as trajectories lengthen: while attacks may occasionally succeed in initial steps, the isolation boundaries prevent them from biasing downstream decisions, causing ASR to drop to zero for complex, multi-step tasks. Key Findings. The trajectory-length analysis provides three key insights. First, AGENTSYS stands out in both short and long trajectory categories for benign utility, demonstrating that memory management provides consistent benefits regardless of task complexity. Second, AGENTSYS achieves near-perfect security on long trajectories (0% ASR for 4 tool calls), while other methods show vulnerability patterns with optimal attack trajectory lengths, quantitatively demonstrating that memory management effectively prevents attack persistence across multi-step workflows. Third, the dual benefit of improved utility and security on long tasks validates our core design principle: explicit memory management that keeps the trusted agents working memory short and clean by retaining only essential, task-relevant PREPRINT information, which simultaneously reduces attack surface and improves instruction-following performance. This explains why AGENTSYS can even slightly outperform the undefended baseline in benign settings while achieving optimal security."
        },
        {
            "title": "7 Discussion",
            "content": "AGENTSYS demonstrates that explicit memory management through ensuring only essential, task-relevant information enters the agents working memory effectively addresses the attack persistence and utility degradation problems identified in conventional agents. While AGENTSYS achieves strong security and even improves utility over undefended baselines, understanding its limitations and residual failure cases provides insight into fundamental challenges in defending LLM agents. Validator Reliability. AGENTSYSs validator is an LLM-based alignment checker that mediates recursive tool calls within worker agents. While the validator operates only on trusted inputs (user query and compact tool-call trace, not raw tool outputs contaminated by adversarial content), it inherits fundamental LLM limitations: the validator may approve malicious tool calls that are subtly misaligned with user intent, or deny legitimate calls due to overly conservative reasoning. Our ablation study (Section 6.2) shows that removing the validator and sanitizer increases ASR to 2.19%, indicating most attacks are caught, but the residual 0.78% ASR in full AGENTSYS suggests occasional validator failures. Improving validator accuracy through specialized training, ensemble methods, or hybrid rule-based checks could further reduce these failures. Adaptive Attacks. The primary attack surface in AGENTSYS is the schema-validated return channel. Although intent schemas restrict communication to pre-declared fields with typed constraints, string-valued fields can still \"string\"}, an attacker can embed carry adversarial content. For instance, if worker agent extracts {\"name\": instructions within the name field (e.g., \"Alice [IGNORE PREVIOUS]\"). While this dramatically reduces the attack surface compared to appending entire raw tool outputs, it does not eliminate it. Our adaptive attack experiments (Section 6.4) show sophisticated attackers can craft payloads targeting this channel, though with limited success. Intent Specification Complexity. AGENTSYS requires the parent agent to declare intent schemas before observing tool outputs. For complex or exploratory tasks where the desired information structure is unknown in advance, specifying precise schemas may be challenging. While LLM-based schema generation works well in practice, schemas may be either too restrictive (limiting information flow) or too permissive (expanding attack surface). Developing automated schema synthesis that balances expressiveness and security is an important direction."
        },
        {
            "title": "8 Conclusion",
            "content": "We presented AGENTSYS, defense against indirect prompt injection that addresses the fundamental problem of indiscriminate memory accumulation in LLM agents. Through hierarchical context isolation and schema-bounded communication, AGENTSYS ensures only essential, task-relevant information enters the agents working memory, preventing both attack persistence and utility degradation. Conventional agents accumulate verbose tool outputs and obsolete observations that expand attack surface while degrading decision-making. AGENTSYS addresses this through explicit memory management: worker agents process tool outputs in isolated contexts, returning only compact, schema-validated values to the main agent. This prevents adversarial instructions from persisting across reasoning cycles while keeping memory clean and focused. Evaluation on AgentDojo and ASB demonstrates state-of-the-art security (0.78% and 4.25% ASR) while improving utility over undefended baselines (64.36% vs 63.54%). AGENTSYS achieves 0% ASR on multi-step tasks ( 4 tool calls), maintains robust performance across six foundation models and adaptive attackers, with practical computational overhead. Ablation studies show context isolation alone achieves 2.19% ASR, validating that memory management provides substantial security even without additional mechanisms. AGENTSYS demonstrates that effective defense addresses root causes rather than symptoms. By managing working memory through architectural boundaries rather than relying on model-level robustness, detection, or rigid constraints that operate on bloated context, we provide principled approach for building secure, dynamic LLM agents. 15 PREPRINT"
        },
        {
            "title": "References",
            "content": "[1] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In AISec, pages 7990. ACM, 2023. [2] Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, and Fabrizio Gilardi. Simple prompt injection attacks can leak personal data observed by LLM agents during task execution. CoRR, abs/2506.01055, 2025. [3] Hengyu An, Jinghuai Zhang, Tianyu Du, Chunyi Zhou, Qingming Li, Tao Lin, and Shouling Ji. Ipiguard: novel tool dependency graph-based defense against indirect prompt injection in LLM agents, 2025. [4] Anthropic. Claude 3.7 Sonnet and Claude Code. https://www.anthropic.com/news/claude-3-7-sonnet, February 2025. [5] Dominic Betts, Julian Dominguez, Grigori Melnik, Fernando Simonazzi, and Mani Subramanian. Exploring CQRS and Event Sourcing: journey into high scalability, availability, and maintainability with Windows Azure. Microsoft patterns & practices, 1st edition, 2013. [6] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. In SaTML, pages 2342. IEEE, 2025. [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. [8] Sizhe Chen, Julien Piet, Chawin Sitawarin, and David A. Wagner. Struq: Defending against prompt injection with structured queries. In USENIX Security, pages 23832400. USENIX Association, 2025. [9] Sizhe Chen, Yizhu Wang, Nicholas Carlini, Chawin Sitawarin, and David A. Wagner. Defending against prompt injection with few defensivetokens. In AISec, pages 242252. ACM, 2025. [10] Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, David A. Wagner, and Chuan Guo. Secalign: Defending against prompt injection with preference optimization. In CCS, pages 28332847. ACM, 2025. [11] Sizhe Chen, Arman Zharmagambetov, David A. Wagner, and Chuan Guo. Meta secalign: secure foundation LLM against prompt injection attacks. CoRR, abs/2507.02735, 2025. [12] Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, and Bryan Hooi. Can indirect prompt injection attacks be detected and removed? In ACL, pages 1818918206. Association for Computational Linguistics, 2025. [13] Yulin Chen, Haoran Li, Zihao Zheng, Dekai Wu, Yangqiu Song, and Bryan Hooi. Defense against prompt injection attack by leveraging attack techniques. In ACL, pages 1833118347. Association for Computational Linguistics, 2025. [14] Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, and Somesh Jha. How not to detect prompt injections with an LLM. In AISec, pages 218229. ACM, 2025. [15] Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, and Florian Tramèr. Defeating prompt injections by design. CoRR, abs/2503.18813, 2025. [16] Edoardo Debenedetti, Jie Zhang, Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. Agentdojo: dynamic environment to evaluate prompt injection attacks and defenses for LLM agents. In NeurIPS, 2024. [17] Mohamed Amine Ferrag, Norbert Tihanyi, and Mérouane Debbah. From LLM reasoning to autonomous AI agents: comprehensive review. CoRR, abs/2504.19678, 2025. [18] Izzeddin Gur, Hiroki Furuta, Austin V. Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. In ICLR. OpenReview.net, 2024. 16 PREPRINT [19] Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. Defending In CAMLIS, volume 3920 of CEUR Workshop against indirect prompt injection attacks with spotlighting. Proceedings, pages 4862. CEUR-WS.org, 2024. [20] Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James R. Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating positional attention bias improves long context utilization. In ACL, volume ACL 2024 of Findings of ACL, pages 1498214995. Association for Computational Linguistics, 2024. [21] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of LLM agents: survey. CoRR, abs/2402.02716, 2024. [22] Kuo-Han Hung, Ching-Yun Ko, Ambrish Rawat, I-Hsin Chung, Winston H. Hsu, and Pin-Yu Chen. Attention tracker: Detecting prompt injection attacks in llms. In NAACL, volume NAACL 2025 of Findings of ACL, pages 23092322. Association for Computational Linguistics, 2025. [23] Sarah Kent. Prompt Injection: The AI Vulnerability We Still Cant Fix. https://www.guidepointsecurity. com/blog/prompt-injection-the-ai-vulnerability-we-still-cant-fix/, August 2025. [24] Hugo Lefeuvre, Vlad-Andrei Badoiu, Alexander Jung, Stefan Lucian Teodorescu, Sebastian Rauch, Felipe Huici, Costin Raiciu, and Pierre Olivier. Flexos: towards flexible OS isolation. In ASPLOS, pages 467482. ACM, 2022. [25] Evan Li, Tushin Mallick, Evan Rose, William K. Robertson, Alina Oprea, and Cristina Nita-Rotaru. ACE: security architecture for llm-integrated app systems. CoRR, abs/2504.20984, 2025. [26] Hao Li, Xiaogeng Liu, Hung-Chun Chiu, Dianqi Li, Ning Zhang, and Chaowei Xiao. DRIFT: dynamic rule-based defense with injection isolation for securing LLM agents. CoRR, abs/2506.12104, 2025. [27] Hao Li, Xiaogeng Liu, Ning Zhang, and Chaowei Xiao. Piguard: Prompt injection guardrail via mitigating overdefense for free. In ACL, pages 3042030437. Association for Computational Linguistics, 2025. [28] Zekun Li, Baolin Peng, Pengcheng He, and Xifeng Yan. Evaluating the instruction-following robustness of large language models to prompt injection. In EMNLP, pages 557568. Association for Computational Linguistics, 2024. [29] Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, and Huan Sun. Eia: Environmental injection attack on generalist web agents for privacy leakage. In ICLR. OpenReview.net, 2025. [30] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157173, 2024. [31] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. In ICLR. OpenReview.net, 2024. [32] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. CoRR, abs/2306.05499, 2023. [33] Yupei Liu, Yuqi Jia, Jinyuan Jia, Dawn Song, and Neil Zhenqiang Gong. Datasentinel: game-theoretic detection of prompt injection attacks. In SP, pages 21902208. IEEE, 2025. [34] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: survey on methodology, applications and challenges. CoRR, abs/2503.21460, 2025. [35] Meta. Llama Prompt Guard 2 Model Cards and Prompt formats. https://www.llama.com/docs/ model-cards-and-prompt-formats/prompt-guard/, 2025. [36] Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, and Florian Tramèr. The attacker moves second: Stronger adaptive attacks bypass defenses against llm jailbreaks and prompt injections. CoRR, abs/2510.09023, 2025. [37] OpenAI. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, July 2024. [38] OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, May 2024. [39] OpenAI. GPT-5.1: smarter, more conversational ChatGPT. https://openai.com/index/gpt-5-1/, November 2025. 17 PREPRINT [40] OpenAI. Introducing ChatGPT Atlas. https://openai.com/index/introducing-chatgpt-atlas/, October 2025. [41] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems, 2023. [42] Nishit V. Pandya, Andrey Labunets, Sicun Gao, and Earlence Fernandes. May have your attention? breaking fine-tuning based prompt injection defenses using architecture-aware attacks. CoRR, abs/2507.07417, 2025. [43] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. CoRR, abs/2211.09527, 2022. [44] ProtectAI.com. Fine-tuned deberta-v3-base for prompt injection detection, 2024. [45] Sander Schulhoff. The sandwich defense: Strengthening ai prompt security, 2024. [46] Tianneng Shi, Jingxuan He, Zhun Wang, Linyu Wu, Hongwei Li, Wenbo Guo, and Dawn Song. Progent: Programmable privilege control for LLM agents. CoRR, abs/2504.11703, 2025. [47] Tianneng Shi, Kaijie Zhu, Zhun Wang, Yuqi Jia, Will Cai, Weida Liang, Haonan Wang, Hend Alzahrani, Joshua Lu, Kenji Kawaguchi, Basel Alomair, Xuandong Zhao, William Yang Wang, Neil Gong, Wenbo Guo, and Dawn Song. Promptarmor: Simple yet effective prompt injection defenses. CoRR, abs/2507.15219, 2025. [48] Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. [49] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. survey on large language model based autonomous agents. Frontiers Comput. Sci., 18(6):186345, 2024. [50] Peiran Wang, Yang Liu, Yunfei Lu, Yifeng Cai, Hongbo Chen, Qingyou Yang, Jie Zhang, Jue Hong, and Ye Wu. Agentarmor: Enforcing program analysis on agent runtime trace to defend against prompt injection, 2025. [51] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? In NeurIPS, 2023. [52] Fangzhou Wu, Ethan Cecchetti, and Chaowei Xiao. System-level defense against indirect prompt injection attacks: An information flow control perspective, 2024. [53] Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, and Umar Iqbal. Isolategpt: An execution isolation architecture for llm-based agentic systems. In NDSS. The Internet Society, 2025. [54] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In NeurIPS, 2024. [55] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. [56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In ICLR. OpenReview.net, 2023. [57] Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and defenses against large language models: survey. CoRR, abs/2407.04295, 2024. [58] Qiusi Zhan, Richard Fang, Henil Shalin Panchal, and Daniel Kang. Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents. In NAACL, volume NAACL 2025 of Findings of ACL, pages 71017117. Association for Computational Linguistics, 2025. [59] Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, and Yongfeng Zhang. Agent security bench (ASB): formalizing and benchmarking attacks and defenses in llm-based agents. In ICLR. OpenReview.net, 2025. [60] Ruiyi Zhang, David Sullivan, Kyle Jackson, Pengtao Xie, and Mei Chen. Defense against prompt injection attacks via mixture of encodings. In NAACL, pages 244252. Association for Computational Linguistics, 2025. [61] Peter Yong Zhong, Siyuan Chen, Ruiqi Wang, McKenna McCall, Ben L. Titzer, Heather Miller, and Phillip B. Gibbons. RTBAS: defending LLM agents against prompt injection and privacy leakage. CoRR, abs/2502.08966, 2025. [62] Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, and William Yang Wang. MELON: provable defense against indirect prompt injection attacks in AI agents. In ICML. OpenReview.net, 2025. [63] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043, 2023. PREPRINT"
        },
        {
            "title": "Appendix",
            "content": "Table 5-7 presents detailed benign utility, attacked utility, and ASR results for AGENTSYS across six foundation models on AgentDojo. PREPRINT Model GPT-4o-mini GPT-4o GPT-5. Claude-3.7-Sonnet Gemini-2.5-Pro Qwen2.5-7B-Instruct Table 5: Utility on the AgentDojo benchmark without attack. (%) Method Overall Banking Slack Travel Workspace"
        },
        {
            "title": "No Defense\nAGENTSYS",
            "content": "No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS 63.54 64.36 70.86 76. 84.75 74.87 86.31 84.12 74.49 72.68 38.15 26.40 50.00 43.75 75.00 68. 81.25 68.75 75.00 93.75 75.00 75.00 50.00 50.00 66.67 76.19 80.95 90. 95.24 85.71 95.24 95.24 90.48 85.71 47.62 38.10 55.00 70.00 65.00 80. 80.00 75.00 80.00 75.00 75.00 65.00 10.00 10.00 82.50 67.50 62.50 67. 82.50 70.00 95.00 72.50 57.50 65.00 45.00 7.50 Model GPT-4o-mini GPT-4o GPT-5.1 Claude-3.7-Sonnet Gemini-2.5-Pro Qwen2.5-7B-Instruct Table 6: Utility on the AgentDojo benchmark under attack. (%) Method Overall Banking Slack Travel Workspace No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS No Defense AGENTSYS 38.19 39.58 69.44 50.69 72.92 58.33 74.31 80.56 67.36 69.44 38.19 27. 48.27 52.87 55.43 58.22 70.79 66.03 76.21 75.85 60.56 63.95 26.86 19. 20 48.57 50.48 63.81 66.67 72.38 66.67 71.43 71.43 63.81 61. 30.48 27.62 47.14 60.00 64.29 60.00 59.29 67.86 70.00 77.14 55.71 57. 9.29 10.71 59.17 61.43 24.17 55.54 78.57 71.25 89.11 74.29 55.36 67. 29.46 12.14 PREPRINT Model GPT-4o-mini GPT-4o GPT-5. Claude-3.7-Sonnet Gemini-2.5-Pro Qwen2.5-7B-Instruct Table 7: ASR on the AgentDojo benchmark under attack. (%) Method Overall Banking Slack Travel Workspace"
        },
        {
            "title": "No Defense\nAGENTSYS",
            "content": "No Defense AGENTSYS 30.66 0.78 51.68 2.54 3.79 0.28 7.84 1.15 36.90 4. 14.11 3.45 34.03 2.78 62.50 6.25 2.08 0.00 4.17 1.39 35.42 5. 12.50 2.08 57.14 0.00 92.38 0.00 9.52 0.95 23.81 2.86 75.24 9. 34.29 10.48 13.57 0.00 11.43 0.00 3.57 0.00 0.71 0.00 26.43 0. 7.86 0.71 17.92 0.36 40.42 3.93 0.00 0.18 2.68 0.36 10.54 1. 1.79 0."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Washington University in St. Louis"
    ]
}