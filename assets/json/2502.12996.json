{
    "paper_title": "Eager Updates For Overlapped Communication and Computation in DiLoCo",
    "authors": [
        "Satyen Kale",
        "Arthur Douillard",
        "Yanislav Donchev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimization steps on their own local data, and an outer optimization step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in a manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that a particular variant, dubbed eager updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 6 9 9 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Eager Updates For Overlapped Communication\nand Computation in DiLoCo",
            "content": "Satyen Kale*,,2, Arthur Douillard*,1 and Yanislav Donchev1 1Google DeepMind, 2Google Research, *Equal core contributions, Currently at Apple. Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimization steps on their own local data, and an outer optimization step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that particular variant, dubbed eager updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers. Keywords: eager updates, distributed learning, large-scale 1. Introduction As language models and data sets get ever larger, it has become increasingly important to resort to distributed training approaches to effectively handle the larger scales. particularly effective technique, DiLoCo, was proposed by Douillard et al. (2024). DiLoCo leverages techniques from Federated Learning (McMahan et al., 2017) and uses particular instantiation similar to the FedOpt algorithm (Reddi et al., 2021). Specifically, training is split into inner and outer optimization phases. In each inner optimization phase, all workers independently execute an optimizer (typically, AdamW) on their local data starting from the current values of the global parameters. Then, all workers communicate their updates to each other in the outer optimization phase. These updates are aggregated via an allreduce operation into single outer gradient, which is then applied via an outer optimizer (typically, Nesterov Momentum) to the current global parameters to get the new global parameters, which then form the starting point for the next inner optimization phase. The pseudocode appears in Algorithm 1 and visualization in Figure 1. This approach compares favorably with standard data-parallel distributed training (in which each worker computes one gradient on batch of local data, after which all gradients are aggregated into one and applied to the current parameters). See (Douillard et al., 2024) for details. The benefit is that the total communication requirements goes down by factor of the number of inner optimization steps (typically, 50-100) compared to standard data-parallel, leading to better running time. However, in certain settings such as crossdatacenter training, communication links between workers have low bandwidth, and workers are forced to block in each outer optimization phase until all the outer gradients are communicated before continuing computation. This leads to wasted time due to idle compute. The goal of this paper is to mitigate this issue by developing techniques to overlap communication with computation leading to better compute utilization. The particular approach studied here is to allow the communication of outer gradients to happen in parallel with the computation of the immediate next inner optimization phase. I.e., at the end of each inner optimization phase, workCorresponding author(s): douillard@google.com 2025 Google DeepMind. All rights reserved Eager Updates For Overlapped Communication and Computation in DiLoCo ers dispatch the computed outer gradients to be communicated to the other workers, and then immediately start executing the next inner optimization phase without waiting for the outer gradient all-reduce to finish. The all-reduce operation is allowed as much as time as it takes for the inner optimization phase to complete. Thus, at the end of the inner optimizaiton phase, the all-reduced outer gradient from the previous inner optimization phase is available, and can be applied to the local parameters. na칦ve implementation of the above approach leads to worse convergence than standard DiLoCo since the all-reduced outer gradients are applied with delay of one entire inner optimization phase. To improve on this, we develop an eager version of this approach based on the following idea. Note that the local outer gradient computed at each worker is already available to that worker before the all-reduce with the other, non-local, outer-gradients. This local outer gradient can serve as good proxy for the all-reduced outergradients, and can be used in an outer optimization step at each worker before starting the next inner optimization phase. Then at the end of that inner optimization step, when the all-reduced outer gradients become available to the worker, the (delayed) non-local outer gradients can be applied to the parameters, along with the new (fresh) local outer gradient. We call this method eager since it eagerly applies local gradients without waiting for the non-local gradients to arrive at the worker. Our experiments show that eager updates significantly help reduce the performance hit of na칦ve delayed outer gradients and achieve training loss close to standard DiLoCo. When factoring in compute utilization however, eager updates significantly improve over standard DiLoCo. We provide details in the following sections. 2. Algorithms In this section we describe the algorithms studied in detail. For all algorithms, we denote the model parameters as 洧랚. We use the superscript notation 洧랚(洧노) to indicate the parameters at given step 洧노, and the subscript notation 洧랚洧녴 to denote particular shard of the DiLoCo replica. For example, 洧랚(洧노) 洧녴 indicates the parameters of DiLoCo replica 洧녴 at step 洧노. If no subscript is used, the parameters are replicated across DiLoCo replicas. Note that it is possible for parameters to not be replicated and yet to be of the same value. 2.1. Standard DiLoCo DiLoCo is an instantiation of the FedOpt framework of Reddi et al. (2021) applied to language models which is bi-level federated optimization paradigm using an inner optimizer, Adam (Kingma and Ba, 2014), and an outer optimizer, SGD with Nesterov momentum (Sutskever et al., 2013). The DiLoCo algorithm is shown in Algorithm 1 and visualized in Figure 1. Algorithm 1 DiLoCo Require: 洧 replicas Require: Synchronization frequency 洧냩 , . . . , 洧랚(0) Require: Model replicas {洧랚(0) 洧 } 1 Require: Data shards {D1, . . . , D洧 } Require: Optimizers InnerOpt and OuterOpt 1: parallel for replica 洧녴 = 1 . . . 洧 do 2: for step 洧노 = 1 . . . 洧녢 do 3: 4: 5: 洧논 D洧녴 洧녭 (洧논, 洧랚(洧노1) 洧녴 InnerOpt(洧랚(洧노1) 洧랚(洧노) 洧녴 洧녴 ) , L) 9: 6: 7: 8: 洧랚(洧노) 洧녴 if 洧노 mod 洧냩 == 0 then 洧녴 洧녴 洧랚(洧노 洧냩 ) 풊 (洧노) 풊 (洧노) async-send[ 1 洧 block-receive[풊 (洧노) ] 洧랚(洧노) 洧녴 OuterOpt(洧랚(洧노 洧냩 ) end if end for 12: 13: end parallel for 11: 10: 洧녴 洧녴=1(풊 (洧노) (cid:205)洧 洧녴 )] , 풊 (洧노) ) In DiLoCo, 洧 local replicas perform, in parallel, 洧냩 steps of the inner optimizer InnerOpt on different subsets of the data (L3 to L5 in Algorithm 1). Every 洧냩 steps, each replica computes an outer gradient 풊 (洧노) 洧녴 (L7), delta in the parameter space, and communicates it to all other replicas. This communication can be performed through central parameter server or through direct communication of each worker to the others (e.g. with ring all洧녴 = 洧랚(洧노 洧냩 ) 洧랚(洧노) 洧녴 Eager Updates For Overlapped Communication and Computation in DiLoCo Figure 1 Data flow and operations in standard DiLoCo. Here, 4 workers execute in parallel and alternate sequentially computation (the outer and inner optimization steps) and communication (averaging outer gradients across workers). Figure 2 Data flow and operations in DiLoCo with delayed outer gradients. Here, 4 workers execute optimization steps in parallel with each other, as well as with the communication required for averaging outer gradients. This is accomplished by delaying the application of the averaged outer gradient in the outer optimizer. 洧녴=1 풊 (洧노) reduce), and results in each worker obtaining 풊 (洧노) = 1/洧 (cid:205)洧 洧녴 (L7-9). This outer gradient is applied to the outer parameters, which are the previously synchronized parameters 洧랚(洧노 洧냩 ) , using 洧녴 the outer optimizer OuterOpt (L10). The costly communication between noncolocated devices happens during the averaging of outer gradients, in lines 8 and 9 of Algorithm 1. While the communication cost at each outer optimization step is exactly the same as in each iteration of standard Data-Parallel training, since it is done every 洧냩 (e.g., one hundred) steps, the communication cost is amortized. DiLoCo is successful instantiation of FedOpt applied to language models where the inner optimizer is Adam (Kingma and Ba, 2014) and the outer optimizer is SGD with Nesterov momentum (Sutskever et al., 2013). 2.2. Na칦ve Delayed Outer Gradients Algorithm 2 gives the pseudocode for na칦ve delayed outer gradients in DiLoCo. visualization is given in Figure 2 which indicates how delaying the application of the outer gradients in the outer optimizer enables effective overlapping of communication with computation."
        },
        {
            "title": "The main differences from standard DiLoCo to",
            "content": "note are: 1. Each worker maintains its own copy of the model parameters 洧랚(洧노) , which is never syn洧녰 chronized across workers. Thus, the model parameters at the workers may diverge from each other, and may benefit from periodic synchronizing by simple averaging. In experiments, however, we found no benefit to this period synchronization. 2. In the outer optimization step (L1011), 3 Eager Updates For Overlapped Communication and Computation in DiLoCo Algorithm 2 Na칦ve Delayed Outer Gradients in DiLoCo Require: 洧 replicas Require: Synchronization frequency 洧냩 Require: Model replicas {洧랚(0) , . . . , 洧랚(0) 洧 } 1 Require: Data shards {D1, . . . , D洧 } Require: Optimizers InnerOpt and OuterOpt 1: parallel for replica 洧녴 = 1 . . . 洧 do 2: for step 洧노 = 1 . . . 洧녢 do Algorithm 3 Eager Updates with Delayed Outer Gradients in DiLoCo Require: 洧 replicas Require: Synchronization frequency 洧냩 Require: Model replicas {洧랚(0) , . . . , 洧랚(0) 洧 } 1 Require: Data shards {D1, . . . , D洧 } Require: Optimizers InnerOpt and OuterOpt 1: parallel for replica 洧녴 = 1 . . . 洧 do 2: for step 洧노 = 1 . . . 洧녢 do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 洧논 D洧녴 洧녭 (洧논, 洧랚(洧노1) 洧녴 InnerOpt(洧랚(洧노1) 洧랚(洧노) 洧녴 洧녴 ) if 洧노 mod 洧냩 == 0 then 洧녴 洧녴 洧랚(洧노 洧냩 ) 풊 (洧노) 풊 (洧노) async-send[ 1 洧 if 洧노 > 洧냩 then 洧랚(洧노) 洧녴 , L) 洧녴=1(풊 (洧노) (cid:205)洧 洧녴 )] block-receive[풊 (洧노 洧냩 ) ] 洧녴 OuterOpt(洧랚(洧노 洧냩 ) 洧랚(洧노) end if 洧녴 , 풊 (洧노 洧냩 ) ) 13: end if end for 14: 15: end parallel for 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 洧논 D洧녴 洧녭 (洧논, 洧랚(洧노1) 洧녴 InnerOpt(洧랚(洧노1) 洧랚(洧노) 洧녴 洧녴 ) if 洧노 mod 洧냩 == 0 then 洧녴 洧녴 洧랚(洧노 洧냩 ) 풊 (洧노) 풊 (洧노) async-send[ 1 洧 if 洧노 > 洧냩 then 洧랚(洧노) 洧녴 , L) 洧녴=1(풊 (洧노) (cid:205)洧 洧녴 )] 洧 (풊 (洧노) block-receive[풊 (洧노 洧냩 ) ] 풊 (洧노) 洧녴 1 洧녴 OuterOpt(洧랚(洧노 洧냩 ) 洧랚(洧노) end if 洧녴 풊 (洧노 洧냩 ) 洧녴 洧녴 ) + 풊 (洧노 洧냩 ) . , 풊 (洧노) 洧녴 ) 14: end if end for 15: 16: end parallel for outer gradients from the previous inner optimization phase are used instead of the current ones (i.e. 풊 (洧노 洧냩 ) instead of 풊 (洧노) . Effectively, this means that the async-send operation in L8 can be executed in parallel with the next inner optimization phase, since its result is only consumed at the end of that phase. the stale non-local outer gradients. Crucially, just like in the na칦ve implementation, this computation only requires outer gradients from the previous inner optimization phase are used instead of the current ones (i.e. 풊 (洧노 洧냩 ) instead of 풊 (洧노) ), and so the async-send in line 8 can be executed in parallel with the next inner optimization phase. 2.3. Eager updates with delayed outer gradi3. Experiments ents Algorithm 2 gives the pseudocode for eager updates with delayed outer gradients in DiLoCo. The main difference from Algorithm 2, na칦ve delayed outer gradients, is in lines 11 and 12. Line 11 computes fresher version of the delayed outer gradient by adding the current local outer gradient and removing the stale local outer gradient, both appropriately scaled. In other words, the computation in line 11 can be equivalently written as 洧랚(洧노) ), which brings out the fact that were just computing an average of the current local outer gradient and all 洧녴 + (cid:205)洧녴洧녴 풊 (洧노 洧냩 ) 洧 (풊 (洧노) 洧녴 洧녴 We perform our experiments with Chinchilla architecture (Hoffmann et al., 2022). Following Wortsman et al. (2023) and Jaghouar et al. (2024a), we use QKNorm (Henry et al., 2020) and Z-loss (Chowdhery et al., 2023) with factor of 1e-4 to stabilize training. We report in Table 4 the architecture hyperparameters and token budget at each scale. Unlike the recommendation in Post-Local SGD (Lin et al., 2020), we train all our models from scratch. The main hyperparameter of DiLoCo is its outer learning rate; we tuned it to be optimal at small scale at 0.4, and kept it fixed across all scales. For all ex4 Eager Updates For Overlapped Communication and Computation in DiLoCo (a) 1B parameters model. (b) 10B parameters model (c) 100B parameters model Figure 3 Compute Utilization simulated across range of bandwidth. compute utilization of 0.8 means 80% of the time is spent in computation, and 20% in communication. Our best method reaches compute utilization of 95% for models 1B, 10B, and 100B with bandwidth roughly constant between 1 and 5 Gbit/s. Data-Parallel on the other hand requires 100, 200, and 300Gbit/s. periments we use Streaming DiLoCo (Douillard et al., 2025) instead of standard DiLoCo (Douillard et al., 2024). Streaming DiLoCo essentially applies standard DiLoCo to different parts of the models on different schedules. While we presented our delayed outer gradients methods only for standard DiLoCo, it can be easily applied to streaming DiLoCo. We use the C4 dataset (Raffel et al., 2020) and train models from 35 million to 1 billion parameters. Each scale is trained with the Chinchillaoptimal number of steps. We use 2 DiLoCo replicas, each of them performing FSDP (Zhao et al., 2023) across their respective closely located devices. For training we use modified version of the Nanodo codebase (Liu et al., 2024b) that uses DrJax (Rush et al., 2024) to parallelize inner steps across replicas. The inner optimization is done with an annotated variant of jax.vmap for the optimization step, with parameters having an extra leading axis for the DiLoCo replicas. The outer optimization is implemented with an allreduce, without any central parameter server. 3.1. Compute utilization simulation First, we simulate the training of model using DAG made of forward and backward nodes (refer to Douillard et al. (2025) for full details). We consider models of 1, 10, and 100 billion parameters with respectively step time (pure compute) of 0.1, 0.8, and 4.9 seconds. For each model, we sweep range of bandwidth from 101 Gbits/s to 103 Gbits/s, and across different distributed training methods. We display the results of those simulation in Figure 3. As also noted by Douillard et al. (2025), overlapping communication massively reduces required bandwidth, particularly as the models get larger and thus spend more time during computation. Indeed, as also shown in Table 5 in the appendix, our method with 1-outer-step eager requires 1,177 (471.5 vs 0.4) less Gbits/s than data-parallel for 100 billion parameters model. Overlapping single inner step, as proposed by Douillard et al. (2025), only reduces required bandwidth by 336 (471.5 vs 1.4). 3.2. Scaling We display in Figure 4 the loss on C4 and the accuracy on HellaSwag (Zellers et al., 2019) of our model vs baselines from 35 million parameters to 1 billion parameters. We also report the full results, including accuracy on Piqa (Bisk et al., 2020) and Arc-easy (Clark et al., 2018), in Table 6 in the appendix. Notably, our method with 1-outer-step eager (see Algorithm 3) with 洧냩 = 30 inner steps reaches the same performance as Data-Parallel at 1 billion scale, proving that our distributed method gets better at larger scale, where also the sheer size of the models make distributed methods ever more important. 3.3. Overtraining on Dolma Previous experiments on C4 are done with token budget \"optimal\" according to Chinchilla scaling 5 Eager Updates For Overlapped Communication and Computation in DiLoCo (a) Evaluation loss on C4 (b) HellaSwag accuracy Figure 4 Scaling models from 35M (1.49e17 flops) to 1B parameters (1.9e20 flops) on C4. Method Token Budget Hours spent w/ + Gbits/s Hours spent w/ 1 Gbits/s Terabytes exchanged Eval Loss HellaSwag Piqa Arc Easy Data-Parallel Streaming DiLoCo with 1-inner-step overlap Streaming DiLoCo with 1-outer-step overlap 25B 100B 250B 25B 100B 250B 25B 100B 250B 0.67 2.7 6.75 0.67 2.7 6.75 0.67 2.7 6. 109 438 1097 0.88 3.5 8.75 0.67 2.7 6.75 441 1,767 4,418 1.10 4.42 11.05 1.10 4.42 11. 2.67 2.52 2.45 2.66 2.51 2.45 2.69 2.53 2.46 42.09 49.78 53.86 42.08 49.98 54.24 40.51 49.48 53. 67.35 69.15 70.45 67.46 69.96 71.38 66.87 68.82 69.00 40.42 44.03 44.21 38.42 44.03 41.92 39.12 41.05 41. Table 1 Overtraining Data-Parallel and our method on Dolma with 1 billion parameters model. The latter performs slightly better despite exchanging in total 400 fewer bits, reducing the peak bandwidth by 8, and with significantly relaxed training communication latency constraint: allowing communication to be as long as full inner optimization phase. laws. However, nowadays LLMs are usually overtrained with significantly larger budget (Wortsman et al., 2023), leading to better performance. Therefore, following Douillard et al. (2025), we consider, for 1 billion parameters model, three token budgets on the Dolma dataset (Soldaini et al., 2024): 25, 100, and 250 billion tokens, which are respectively 1, 4, and 10 larger than the optimal. We display the results of our 1-outer-step eager method in Table 1 alongside data-parallel baseline and the techniques in Douillard et al. (2025) with 1-inner-step overlap. Two things are to be noted: (1) with minimal 1 Gbits/s bandwidth, our model has close to 100% compute utilization, while data-parallel training suffers massively, and (2) the performance of our method is lower than the less bandwidth efficient 1-inner-step overlapping, but we see the performance gap reduces as the token budget increases. We see that last observation as hopeful perspective for distributed methods like the one in this paper, in world where massively overtrained models are becoming the norm. 3.4. Ablations We perform in this section ablations of our proposed method. All experiments are conducted on 500 million parameters model trained on the C4 dataset. Number of inner steps. We compare in Figure 5 the loss on C4 of 1-outer-step na칦ve delayed (Algorithm 2) vs 1-outer-step eager (Algorithm 3) 6 Eager Updates For Overlapped Communication and Computation in DiLoCo Figure 5 Comparison of overlapping communication over an outer step, using the na칦ve delayed version (Algorithm 2) and the eager version (Algorithm 3) when varying the number of inner steps 洧냩. Figure 6 Varying the number of inner steps, which affects both the loss and the bandwidth required to reach certain level of compute utilization. When bandwidth is scarce, it is preferable to overlap communication across an outer step. while varying the number of inner steps 洧냩. An outer step is run after every 洧냩 inner steps, therefore our outer-step overlapping method will have increased amount of time to hide communication as 洧냩 increases, in addition of synchronizing less often. While for the eager method we keep the same hyperparameters as proposed by ?, for the na칦ve delayed method we have to lower the outer learning rate by 4 for stability reasons. Despite, we see that the na칦ve delayed version (in orange) see significant increase of loss as 洧냩 grows, while our eager version (in blue) stays relatively constant. Na칦ve delayed sees an increase of loss of 6% from 洧냩 = 5 vs 洧냩 = 500 while eager sees an increase of only 2% from 洧냩 = 30 vs 洧냩 = 500. Loss vs bandwidth The main application of our method is to massively reduce the required bandwidth. Therefore, we display in Figure 6 number of Gbit/s seconds required to reach compute utilization of 80% (i.e. only 20% of the time is spent in communication) of Streaming DiLoCo without communication overlap (in blue) vs 1-outer-step overlap with our eager method (in orange). We vary for both methods the number of inner steps 洧냩. While no overlap can reach lower loss, it also requires more bandwidth, and in particular constrained setting, our method can strike better tradeoff. This is even more true for larger models, where the step time is longer, and thus we have more time to overlap communication. Figure 7 Quantized communication across three overlapping communication schemes: 1) 1-inner-step from (Douillard et al., 2025), 2) 1outer-step na칦ve delayed from Algorithm 2, and 3) 1-outer-step eager from Algorithm 3. Quantized Communication Streaming DiLoCo (Douillard et al., 2025) proposed to quantize the outer gradients to lower precision, and consider float32 and bfloat16, and float8 and float4 using EXMY (Agrawal et al., 2024). We consider whether our outer overlapping communication can have negative interaction with quantized communication, and ablates the percentage loss of difference v.s. using using full precision in Figure 7. Notably, as Douillard et al. (2025)s innerstep overlap, the difference stays bounded by 0.12%, which is minimal considered the reduction of bandwidth provided. 7 Eager Updates For Overlapped Communication and Computation in DiLoCo Communication overlap Tolerated latency in sec. Eval Loss HellaSwag Piqa Arc-Easy No overlap 1-inner-step 1-outer-step delayed 1-outer-step delayed, lowered LR 1-outer-step eager 2-outer-steps eager 0 0.08 2.4 2.4 2.4 4.8 2.67 2.67 3.01 2.73 2.69 2.73 38.26 37.96 29.40 35.83 37.52 36.47 66.59 66.10 60.93 64.96 66.86 64.85 34.91 36.14 34.73 34.21 34.91 35. Table 2 Communication overlap comparison for 500M parameters model, performing step (forward & backward) in 0.08 seconds. Overlapping 1-inner-step as proposed by (Douillard et al., 2025) allows communication to take 0.08 seconds, while we propose to overlap up to 2.4 seconds (洧냩 = 30 total steps). Overlapping DiLoCo variant Evaluation loss No overlap 1-outer-step eager 2.68 DiLoCo Streaming DiLoCo 2.670.3% DiLoCo Streaming DiLoCo 2.71+0.7% 2.69 Table 3 DiLoCo variant comparison for no communication overlapping v.s. our 1-outer-step eager overlapping when varying the underlying DiLoCo algorithms: either the standard DiLoCo (Douillard et al., 2024) where all parameters are synchronized together, or its streaming variant (Douillard et al., 2025) with partial synchronization. Two-outer-steps overlap While we perform all previous experiments with 1-outer-step overlap, whose total overlap length scales with the number of inner steps 洧냩, we also consider in Table 2 doing 2-outer-steps eager overlap; thus overlapping for 2洧냩 inner steps. Notably, doing more than 1-outer-step overlap can be harmful (2.73 loss v.s. 2.67 when not doing any overlap, 2.2% increase). However, this difference may not always translate on downstream tasks, 2-outer-steps eager improves accuracy on Arc-Easy (Clark et al., 2018) by 1.4%. Furthermore, the tolerated communication latency, how long communication is overlapped with computation, increases significantly to 4.8 seconds. DiLoCo variants. We consider the impact of the underlying DiLoCo variants choosen in Table 3: either the standard DiLoCo (Douillard et al., 2024) where all parameters are synchronized together, or its streaming variant (Douillard et al., 2025) with partial synchronization. For both we compare no overlapping of communication v.s. the overlapping scheme proposed in Algorithm 3. We found slight degradation of performance for 1-outer-step eager when using the streaming variant, however it is very limited (<1% different in evaluation loss) and the bandwidth advantage brought by this variant outweighs the cost. 4. Related Works Federated learning / local SGD. Differing from model mergings single combination step, Federated Averaging (FedAvg) (McMahan et al., 2017) and Local SGD (Stich, 2019) iteratively combine models with the goal of minimizing bandwidth requirements. They operate by performing local training, typically using SGD, across workers for certain number of steps before implementing some form of worker parameter synchronization or parameter aggregation. In their original formulations, both FedAvg and Local SGD employed straightforward average of parameters across workers. As demonstrated by Reddi et al. (2021), synchronization becomes more effective when each worker computes model delta, which are then aggregated to produce pseudo-gradient, also termed an outer gradient, subsequently utilized by first-order optimizer (Ilharco et al., 2022; Reddi et al., 2021). This yields bi-level optimization framework with inner optimizers and an outer optimizer, referred to as FedOpt by Reddi et al. (2021), who propose using SGD as the inner optimizer and adaptive techniques such as Adam (Kingma and Ba, 2014) as the outer op8 Eager Updates For Overlapped Communication and Computation in DiLoCo and thus maximizing computation. Methods have been designed to minimize the bubble-oftime in pipeline parallelism (Narayanan et al., 2021; Qi et al., 2024), in data-parallel (Lin et al., 2018; Zhao and Canny, 2013), and federated learning (Liu et al., 2024a; Xie et al., 2019). In particular, in the latter case of federated learning, it is particularly useful to handle the case of stragglers where some replicas are slower than others (Dean et al., 2012; Diskin et al., 2021; Koh et al., 2006; Lian et al., 2015; Recht et al., 2011). 5. Conclusion In this work, we present an improvement over DiLoCo, allowing us to overlap the communication of the outer gradients by whole synchronization round (an outer step), which can be as much as hundreds of inner optimization steps. We show that na칦ve formulation results in worse performance, particularly with untuned hyperparameters, and propose instead an eager variant. In this variant, we decouple the per-replica local outer gradients with the synchronized, and thus delayed, outer gradients. Indeed, we apply mixture of the current local outer gradient with the delayed mixture of all other replicas outer gradients. This improved formulation of communication overlapping results in minimal performance degradation, particularly when the model is trained for large token budget, as is currently the norm, and when the model scale is large. The success of the eager technique in overlapped communication and computation suggests further applications in distributed optimization, e.g. overlapping only few inner steps rather an entire inner optimization phase, which can be useful in settings where bandwidth is not as constrained. Another interesting direction for further work is developing convergence theory for delayed outer gradients. timizer in resource-constrained Federated Learning settings. Distributed training for LLMs. The increasing computational demands of training large language models (LLMs) have accelerated the need for distributed methodologies, applicable to both inference (Borzunov et al., 2023) and training (Diskin et al., 2021; Presser, 2020; Ryabinin et al., 2021). More recently, DiLoCo (Douillard et al., 2024) introduced specific instantiation of FedOpt (Reddi et al., 2021) utilizing AdamW (Loshchilov and Hutter, 2019) as the inner optimizer and Nesterov (Sutskever et al., 2013) as the outer optimizer (Huo et al., 2020). This simple formulation has proven effective for distributed training with LLMs, particularly in scenarios with limited number of replicas (under 100) and without replica sampling, aligning more closely with cross-silo federated learning (Kairouz et al., 2021). The FedOpt algorithm has also been demonstrated to be effective in training LLMs in settings that resemble cross-device federated learning (Charles et al., 2024). The empirical effectiveness of DiLoCo has been reproduced in multiple studies (Jaghouar et al., 2024b; Sani et al., 2024b) and successfully scaled to models with 10 billion parameters (Jaghouar et al., 2024a). In related research, minor modification to the way the outer Nesterov accumulates outer gradients has shown improved handling of asynchronicity among workers with different processing speeds (Liu et al., 2024a). DiLoCo provides an additional axis of parallelism to distributed training (Shoeybi et al., 2020) and is compatible (Jaghouar et al., 2024a) with other existing parallelization approaches such as FSDP (Zhao et al., 2023), or even another layer of federated learning (Sani et al., 2024a). More recently, Douillard et al. (2025) proposed Streaming DiLoCo where only subset of the parameters are shared at each given synchronization round, in effect lowering the peak required bandwidth. Overlapping Communication. Overlapping communication with computation is critical in many aspects of distributed training in order to minimize the time waiting for communication, Eager Updates For Overlapped Communication and Computation in DiLoCo"
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Arthur Szlam and MarcAurelio Ranzato for advices and general support. We also thank Gabriel Teston, Zachary Charles, Zachary Garrett, and Keith Rush for providing engineering support. Finally, we thank Jeff Dean, Raia Hadsell, and Koray Kavukcuoglu for leadership support."
        },
        {
            "title": "References",
            "content": "Aditya Agrawal, Matthew Hedlund, and Blake Hechtman. exmy: data type and technique for arbitrary bit precision quantization. arXiv preprint library, 2024. URL https:// arxiv.org/abs/2405.13938. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020. URL https://arxiv.org/abs/1911.11641. Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference Proceedand fine-tuning of large models. ings of the Annual Meeting of the Association for Computational Linguistics (ACL Short Papers), 2023. URL https://arxiv.org/abs/ 2209.01188. Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, and Zachary Garrett. Towards federated foundation models: Scalable dataset pipelines for group-structured learning. Advances in Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/ 2307.09619. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023. URL http://jmlr.org/ papers/v24/22-1144.html. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/ abs/1803.05457v1. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marcaurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012. Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Quentin Lhoest, Anton Sinitsin, Dmitry Popov, Dmitry Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, Denis Mazur, Ilia Kobelev, Yacine Jernite, Thomas Wolf, and Gennady Pekhimenko. Distributed deep learning in open collaborations. Advances in Neural Information Processing Systems (NeurIPS), 2021. URL https://arxiv.org/abs/2106.10207. Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, MarcAurelio Ranzato, Arthur Szlam, and Jiajun Shen. DiLoCo: Distributed lowcommunication training of language models. 10 Eager Updates For Overlapped Communication and Computation in DiLoCo International Conference on Machine Learning (ICML) Workshop, 2024. URL https: //arXiv.org/abs/2311.08105. Johannes Hagemann. Intellect-1 technical rearXiv preprint library, 2024a. URL port. https://arxiv.org/abs/2412.01152. Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ram칠, Arthur Szlam, MarcAurelio Ranzato, and Paul Barham. Streaming diloco with overlapping communication: Towards distributed free lunch. arXiv preprint library, 2025. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://arxiv.org/abs/2010.04245. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. Advances in Neural Information Processing Systems (NeurIPS), 2022. URL https://arxiv.org/abs/2203.15556. Sami Jaghouar, Jack Min Ong, and JoAn framework globally trainlow-communication arXiv preprint library, 2024b. URL hannes Hagemann. open-source distributed ing. https://arxiv.org/abs/2407.07852. Opendiloco: for Peter Kairouz, Brendan McMahan, Brendan Avent, Aur칠lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and trends in machine learning, 2021. URL https://arxiv.org/ abs/1912.04977. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint library, 2020. URL https://arxiv.org/abs/2001.08361. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. Proceedings of the International Conference on Learning Representations (ICLR), 2014. URL https: //arxiv.org/abs/1412.6980. Zhouyuan Huo, Qian Yang, Bin Gu, and Lawrence Carin. Heng Huang. Faster on-device training using new federated momentum algorithm. arXiv preprint library, 2020. URL https:// arxiv.org/abs/2002.02090. Byung-Il Koh, Alan George, Raphael Haftka, and Benjamin Fregly. Parallel asynchronous particle swarm optimization. International journal for numerical methods in engineering, 67(4): 578595, 2006."
        },
        {
            "title": "Gabriel",
            "content": "Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating Advances in Neural Information weights. Processing Systems (NeurIPS), 2022. Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube, Michael Keiblinger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles Goddard, Max Ryabinin, and Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. Advances in neural information processing systems, 28, 2015. Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Dont use large minibatches, use local sgd. Proceedings of the International Conference on Learning Representations (ICLR), 2020. URL https://arxiv.org/ abs/1808.07217. Eager Updates For Overlapped Communication and Computation in DiLoCo Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J. Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training, 2018. URL https:// arxiv.org/abs/1712.01887. Bo Liu, Rachita Chhaparia, Arthur Douillard, Satyen Kale, Andrei A. Rusu, Jiajun Shen, Arthur Szlam, and MarcAurelio Ranzato. Asynchronous local-sgd training for language modeling. International Conference on Machine Learning (ICML) Workshop, 2024a. URL https: //arxiv.org/abs/2401.09135. Peter J. Liu, Roman Novak, Jaehoon Lee, Mitchell Wortsman, Lechao Xiao, Katie Everett, Alexander A. Alemi, Mark Kurzeja, Pierre Marcenac, Izzeddin Gur, Simon Kornblith, Kelvin Xu, Gamaleldin Elsayed, Ian Fischer, Jeffrey Pennington, Ben Adlam, and Jascha-Sohl Dickstein. Nanodo: minimal transformer decoderonly language model implementation in JAX., 2024b. URL http://github.com/googledeepmind/nanodo. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Proceedings of the International Conference on Learning Representations (ICLR), 2019. URL https:// arxiv.org/abs/1711.05101. H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag칲era Arcas. Communication-efficient learning of deep networks from decentralized data. International Conference on Artificial Intelligence and Statistics (AISTATS), 2017. URL https://arxiv.org/ abs/1602.05629. Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. MemoryInefficient pipeline-parallel dnn training. ternational Conference on Machine Learning (ICML), 2021. URL https://arxiv.org/ abs/2006.09503. Shawn Presser. Swarm training, 2020. URL https://battle.shawwn.com/swarmtraining-v01a.pdf. Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble pipeline parallelism. Proceedings of the International Conference on Learning Representations (ICLR), 2024. URL https://arxiv.org/abs/2401.10241. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 2020. URL https: //arxiv.org/abs/1910.10683. Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: lock-free approach to parallelizing stochastic gradient descent. Advances in neural information processing systems, 24, 2011. URL https://arxiv.org/abs/ 1106.5730. Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone캜n칳, Sanjiv Kumar, and H. Brendan McMahan. Adaptive federated optimization. Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https:// arxiv.org/abs/2003.00295. Keith Rush, Zachary Charles, Zachary Garrett, Sean Augenstein, and Nicole Mitchell. Drjax: Scalable and differentiable mapreduce primiInternational Conference on Matives in jax. chine Learning (ICML) Workshop, 2024. URL https://arxiv.org/abs/2403.07128. Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. Advances in Neural Information Processing Systems (NeurIPS), 2021. URL https: //arxiv.org/abs/2103.03239. Lorenzo Sani, Alex Iacob, Zeyu Cao, Royson Lee, Bill Marino, Yan Gao, Dongqi Cai, Zexi Li, Wanru Zhao, Xinchi Qiu, and Nicholas D. Lane. Photon: Federated llm pre-training. arXiv preprint library, 2024a. URL https: //arxiv.org/abs/2411.02908. Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, and Eager Updates For Overlapped Communication and Computation in DiLoCo Nicholas D. Lane. The future of large language model pre-training is federated. arXiv preprint library, 2024b. URL https://arxiv.org/ abs/2405.10853. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint library, 2020. URL https://arxiv.org/abs/1909.08053. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. URL https://arxiv.org/ abs/2402.00159. Sebastian U. Stich. Local SGD converges fast and communicates little. Proceedings of the International Conference on Learning Representations (ICLR), 2019. URL https://arxiv.org/ abs/1805.09767. Ilya Sutskever, James Martens, George Dahl, On the imporand Geoffrey Hinton. initialization and momentum in tance of International Conference deep learning. on Machine Learning (ICML), 2013. URL https://proceedings.mlr.press/v28/ sutskever13.html. Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohldickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint library, 2023. URL https://arxiv.org/abs/2309.14322. Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. arXiv URL https:// preprint arxiv.org/abs/1903.03934. library, 2019. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL Short Papers), 2019. URL https://arxiv.org/abs/ 1905.07830. Huasha Zhao and John Canny. Butterfly mixing: Accelerating incremental-update algorithms on clusters. Proceedings of the 2013 SIAM International Conference on Data Mining (SDM), 2013. URL https://epubs.siam.org/doi/ abs/10.1137/1.9781611972832.87. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/ 2304.11277. 13 Eager Updates For Overlapped Communication and Computation in DiLoCo"
        },
        {
            "title": "Supplementary Materials",
            "content": "Model scale Hidden dim Num layers Num heads Token budget 35M 100M 200M 300M 500M 1B 2,048 3,072 4,096 5,120 6,144 8,192 6 9 12 15 18 24 8 12 16 20 24 32 700M 1.5B 3.5B 6B 11B 25B Table 4 Architecture hyperparameters: we consider model from 35M to 1B with the following hyperameters and chinchilla-optimal token budget. For all model scale, the vocabulary size is 32,000. 14 Eager Updates For Overlapped Communication and Computation in DiLoCo Model size # layers Step time Method Gbit/s to reach compute utilization CU =? 50% 80% 90% 95% 99% 1B 24 0.1s 10B 0.8s 100B 108 4.9s Data-Parallel Streaming DiLoCo Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=100 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=100 86.8 1.4 2.4 0.4 1.1 0. 152.6 184.2 222.3 16.0 9.1 4.3 4.3 2.0 1.7 2.0 2.0 0.2 0.2 5.2 3.6 0.9 2.0 0.2 Data-Parallel Streaming DiLoCo Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=100 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=100 Data-Parallel Streaming DiLoCo Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-inner-step overlapped FP4 com. H=100 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=30 Streaming DiLoCo with 1-outer-step overlapped FP4 com. H=100 104.8 222.3 222.3 268.3 13.3 3.0 1.4 1.4 0.3 9.1 2.4 1.4 1.4 0. 1.7 1.4 0.4 0.7 0.1 5.2 2.4 0.9 1.1 0.2 184.2 323.8 390.7 390.7 11.0 2.0 1.1 0.9 0.3 2.4 0.9 0.5 0.5 0.2 6.2 1.7 0.9 0.8 0.3 9.1 2.0 1.1 0.9 0. 569.0 28.1 4.3 3.0 2.4 0.2 471.5 19.3 3.0 1.7 1.4 0.3 471.5 19.3 2.0 1.4 0.9 0.4 Table 5 Simulation: we estimate the step time (pure compute) of 10B and 100B based on the required flops using Kaplan et al. (2020) rule and using MFU of 60%. For all DiLoCo and Streaming DiLoCo-variants, we use 洧냩 = 100. For all Streaming DiLoCo-variants, we use fragment size of 3 layers. 15 Eager Updates For Overlapped Communication and Computation in DiLoCo Model size Flops Method 洧냩 # overlapped steps Eval Loss HellaSwag Piqa Arc Easy 35M 1.5e17 100M 9.4e17 200M 4e18 300M 1.4e19 500M 4.7e19 1B 1.9e20 0 Data-Parallel 30 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 100 Streaming DiLoCo with 1-inner-overlap Streaming DiLoCo with 1-outer-eager-overlap 30 Streaming DiLoCo with 1-outer-eager-overlap 0 Data-Parallel 30 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 100 Streaming DiLoCo with 1-inner-overlap Streaming DiLoCo with 1-outer-eager-overlap 30 Streaming DiLoCo with 1-outer-eager-overlap 100 Data-Parallel 0 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 30 Streaming DiLoCo with 1-inner-overlap 100 30 Streaming DiLoCo with 1-outer-eager-overlap Streaming DiLoCo with 1-outer-eager-overlap 100 0 Data-Parallel 30 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 100 Streaming DiLoCo with 1-inner-overlap Streaming DiLoCo with 1-outer-eager-overlap 30 Streaming DiLoCo with 1-outer-eager-overlap 100 0 Data-Parallel 30 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 100 Streaming DiLoCo with 1-inner-overlap Streaming DiLoCo with 1-outer-eager-overlap 30 Streaming DiLoCo with 1-outer-eager-overlap 100 0 Data-Parallel 30 DiLoCo 30 Streaming DiLoCo with 1-inner-overlap 100 Streaming DiLoCo with 1-inner-overlap Streaming DiLoCo with 1-outer-eager-overlap 30 Streaming DiLoCo with 1-outer-eager-overlap 100 0 0 1 1 30 0 0 1 1 30 100 0 0 1 1 30 100 0 0 1 1 30 100 0 0 1 1 30 100 0 0 1 1 30 100 3.51 3.54 3.53 3.56 3.62 3. 3.19 3.21 3.21 3.22 3.27 3.27 2.97 2.98 2.98 3.00 3.03 3.03 2.80 2.81 2.81 2.83 2.86 2.86 2.67 2.68 2.67 2.69 2.71 2.71 2.49 2.49 2.48 2.50 2.50 2.52 24.62 24.53 24.46 24.80 24.47 24. 26.94 26.59 26.97 26.68 26.12 26.12 29.86 29.71 29.67 29.27 29.10 29.10 33.46 33.87 33.66 33.00 32.67 32.67 38.68 38.37 38.10 37.40 36.89 36.74 46.60 46.56 46.60 46.00 46.45 44.64 57.89 58.11 57.67 57.89 56.58 56. 60.12 60.50 59.58 60.39 59.19 59.19 63.71 62.30 61.92 62.13 61.70 61.70 64.69 64.74 63.49 63.71 65.34 65.34 66.49 65.61 66.21 65.51 65.61 65.56 68.93 68.82 69.04 68.82 68.50 68.12 29.65 29.65 30.53 29.12 27.19 27. 30.35 29.12 31.40 31.93 28.77 28.77 35.44 33.68 34.39 34.21 32.81 32.81 34.91 34.74 35.09 34.39 35.44 35.44 37.19 36.32 34.91 34.74 35.44 35.79 39.65 36.84 39.12 38.42 39.47 36.14 Table 6 Scaling from 35 million parameters to 4 billion parameters using chinchilla-optimal number of flops/tokens. We train on the C4 dataset, and report the evaluation loss on its validation set."
        }
    ],
    "affiliations": [
        "Apple",
        "Google DeepMind",
        "Google Research"
    ]
}