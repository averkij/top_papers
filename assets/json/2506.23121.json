{
    "paper_title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation",
    "authors": [
        "Xinlei Yu",
        "Chanmiao Wang",
        "Hui Jin",
        "Ahmed Elazab",
        "Gangyong Jia",
        "Xiang Wan",
        "Changqing Zou",
        "Ruiquan Ge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . e [ 1 1 2 1 3 2 . 6 0 5 2 : r CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Xinlei Yu Hangzhou Dianzi University Hangzhou, China xinleiyu88@gmail.com Chanmiao Wang Shenzhen Research Institute of Big Data Shenzhen, China cmwangalbert@gmail.com Hui Jin Hangzhou Dianzi University Hangzhou, China hui1303101041@gmail.com Ahmed Elazab Shenzhen University Shenzhen, China ahmedelazab@szu.edu.cn Gangyong Jia Hangzhou Dianzi University Hangzhou, China gangyong@hdu.edu.cn Xiang Wan Shenzhen Research Institute of Big Data Shenzhen, China wanxiang@sribd.com Changqing Zou Zhejiang University Hangzhou, China changqing.zou@zju.edu.cn Ruiquan Ge Hangzhou Dianzi University Hangzhou, China gespring@hdu.edu.cn Abstract Multi-organ medical segmentation is crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, similarity-sorting self-updating strategy for memory and maskrefining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym MM, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/25/10 https://doi.org/XXXXXXX.XXXXXXX of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP_SAM2.git. CCS Concepts Computing methodologies Image segmentation. Keywords Multi-Organ Segmentation, Cross-Modal Semantic Interaction, NonGeometric Prompting, Segment Anything Model ACM Reference Format: Xinlei Yu, Chanmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, and Ruiquan Ge. 2025. CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym MM). ACM, New York, NY, USA, 19 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nEfficient and accurate segmentation is crucial in modern medical\nand clinical applications, underpinning disease diagnosis, surgical\nplanning, and outcome prediction [10, 50]. Multi-organ segmen-\ntation, in particular, has been a significant area of interest due to\nits clinical significance and technical challenges. Traditionally, this\ntask has been performed manually by skilled radiologists or in-\nternists, which requires meticulous case-by-case annotation and\nverification. This manual process is not only time-consuming and\nlabor-intensive but also prone to errors due to the high level of\nexpertise needed. To address these challenges, researchers have de-\nveloped relatively simple and lightweight networks [15, 37, 77, 102],\nwhich enable automatic segmentation but with limited accuracy.\nRecently, the Segment Anything Model (SAM) [31] has demon-\nstrated impressive capabilities in segmenting diverse objects with\nthe help of prompts such as points, bounding boxes, and coarse",
            "content": "Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. (a) Segment Anything Model 2 (SAM2) (b) CRISP-SAM2 (Ours) (c) The Predicted Masks of SAM2 vs. CRISP-SAM2 Figure 1: Comparison between (a) SAM2 [63] and (b) our CRISP-SAM2 designed for text-guided 3D multi-organ segmentation. Sub-figure (c) shows the predicted masks of SAM2 and our proposed CRISP-SAM2 against the ground truth. masks. Its derivative, SAM2 [63], retains these capabilities while extending segmentation from image-level to video-level. Both SAM and SAM2 are trained on large-scale datasets, providing them with powerful zero-shot segmentation abilities in visual tasks. While SAM, SAM2, and their variants have achieved significant success, there remains room for improvement, particularly in medical multi-organ segmentation. One of the primary challenges is related to their pre-training process [89]. Although these models are trained on extensive datasets featuring natural objects, the scarcity of medical data within these sets hinders the ability of the model to learn specific features of medical imaging effectively. As result, the gap between the natural and medical domains remains unbridged. To address this, some researchers [50, 74] have adapted SAM for medical segmentation by training it on large-scale medical datasets. Other models [18, 24, 78, 103] have modified the original structures or added additional components to better accommodate the unique characteristics of medical imaging, thereby improving performance. Recently, innovative models [13, 25, 29] have begun to explore the integration of textual information to assist in segmentation. The rich contextual information inherent in natural language offers great potential for enhancing segmentation performance, pointing to promising directions for future research. Current methods based on SAM or SAM2 face three main limitations: 1) They struggle with inaccurate local details and boundaries, sometimes including unwanted adjacent regions or excluding irregularly shaped targets from the predicted masks. 2) These methods rely heavily on explicit prompts, which can lead to inaccurate segmentation without geometric prompts, particularly for small or thin targets, thereby reducing their flexibility. 3) There is loss of spatial information as they cannot directly segment 3D images. To overcome these challenges, we develop CRISP-SAM2, which leverages complementary textual information to guide visual segmentation, segmenting details and boundaries precisely and eliminating the need for geometric prompts. To address the limitations mentioned earlier, we first introduce two-level progressive cross-attention structure to generate crossmodal semantics. These semantics are integrated into the image encoder as contextualized information to enhance the understanding of visual features in detail. By converting these cross-modal semantics and image features into sparse and dense prompt embeddings, we can eliminate the dependency on geometric prompts. Besides, this semantic prompting method, which is guided by textual descriptions of organs, allows for capturing localized details and complex boundaries with improved accuracy. In addition to the original mask, we reuse the mask decoder on an additional learnable tokens and generate refined mask, which is combined with the cross-modal semantics to further refine segmentation details. Recognizing the unique characteristics of 3D medical imaging and the ability of SAM2 to segment continuous imaging frames, we replace its original first-in-first-out (FIFO) strategy with similaritysorting self-updating strategy to better leverage spatial information. Our proposed model, CRISP-SAM2, based on SAM as shown in Figure 1, outperforming both purely visual models [18, 50, 78, 103] and text-assisted models [13, 25, 29, 42] on seven public datasets. Comprehensive ablation studies and visualizations confirm the effectiveness and advantages of our designs. The key contributions of this work include: We propose CRISP-SAM2, novel model specifically designed for accurate 3D multi-organ segmentation that leverages textual information of targets for guidance. We develop progressive cross-modal semantic interaction mechanism to integrate contextualized semantics, enhancing the understanding of cross-modal features. We devise novel prompting approach that uses cross-modal semantics for prompting and replaces the geometric prompts, refining the segmentation of challenging targets. We conduct experiments with state-of-the-art (SOTA) models on seven public datasets, demonstrating the effectiveness of CRISP-SAM2, especially for details and boundaries."
        },
        {
            "title": "2 Related Works\n2.1 Text-Guided Learning in SAMs and SAM2s\nIn specific tasks, SAM and SAM2 may struggle to accurately seg-\nment target masks due to a lack of domain-specific training data. A\npotential solution to this issue involves integrating relevant textual\nfeatures using an adapter or projector to guide the segmentation\nprocess. For instance, DIT [23], TP-DRSeg [38], and LGA [21] en-\nhance SAM by injecting task-related textual features after each\ntransformer block within the image encoder. In contrast, other\napproaches such as AdaptiveSAM [55] introduces adapters in the\nmask decoder. Currently, the field of cross-modal interaction has\nseen significant advancements, particularly with the success of\nvisual language models. Thus, the fusion of textual and visual em-\nbeddings is a logical step to create richer cross-modal features for",
            "content": "CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland guiding segmentation. RefSAM [39] employs multilayer perceptron (MLP) networks for this fusion, while SAP-SAM [76] and PromptRIS [67] utilize cross-modal alignment and prompting modules. Similarly, EVF-SAM [98] uses multi-modal encoder to produce fused embeddings. To effectively bridge the gap between textual and visual modalities, models like SEEM [104] and Semantic-SAM [33] map texts, images, and prompts into unified representation space, achieving semantic awareness. Moreover, Contrastive LanguageImage Pre-Training (CLIP) [62], which excels in aligning and understanding semantics between texts and images, is employed in various segmentation tasks, such as open-vocabulary segmentation [54, 75, 93], and other related fields [36, 91], offering valuable semantic insights."
        },
        {
            "title": "3 Methodology\nThe process of our proposed CRISP-SAM2 is divided into three\ndistinct phases: Semantic Extraction, Prompt Generation, and Mask\nPrediction. In the Semantic Extraction phase, we introduce a Cross-\nModal Semantic Interaction module to derive Cross-modal Semantic\nFeatures (CS-Features). These features are integrated into the im-\nage encoder of SAM2 as semantic guidance and serve as inputs\nfor the subsequent phases. During the Prompt Generation phase,\nthe Semantic Prompt Projector utilizes both the CS-Features and\nthe four-scale features from the image encoder to produce both\nSparse and Dense Prompt Embeddings. In the final Mask Predic-\ntion phase, we concatenate a learnable Cross-modal Output Tokens\n(CS-Output Tokens) with the original output and prompt tokens,",
            "content": "which will be fed into the mask decoder together. Then, we use the Updated CS-Output Tokens as the input of the Local Refiner to correct the details of the predicted masks. Furthermore, we devise similarity-sorting self-updating strategy tailored for 3D medical imaging segmentation to replace the original FIFO strategy."
        },
        {
            "title": "3.2 Semantic Extraction\nCross-Modal Semantic Interaction. To deepen the understand-\ning of medical multi-organ segmentation, we introduce a module\ndesigned to integrate textual and visual inputs, generating com-\nplementary cross-modal semantics. This process is illustrated in\nthe lower left part of Figure 2. These semantically enriched fea-\ntures, referred to as CS-Features FCS, are injected into successive\ntransformer blocks across four scales and are utilized by both the\nSemantic Prompt Projector and Local Refiner.",
            "content": "Consider pair of medical image R洧냥洧녺 洧냩洧녺 洧녥洧녺 and its corresponding text description R洧, where 洧냥洧녺, 洧냩洧녺, 洧녥洧녺 denote the depth, height, and width of the visual input, and is the length of the text input. For simplification, is reduced to R洧냩洧녺 洧녥洧녺 , as all 洧냥洧녺 image slices share the same textual description. Operations related to image depth are handled by three memory components, which are detailed in Section 3.5. The preprocessing of texts involves appending [洧녡洧녝洧녡] and [洧냦洧녝洧녡] markers, followed by projecting it into text embedding sequence R洧냥洧노 (洧+2) , following the CLIP method [62]. The processed and are input into the image and text encoders in the Cross-Modal Semantic Interaction module respectively, producing visual and linguistic features FV and FT . Upon acquiring these features from separate modalities, we employ progressive fusion structure with two levels of semantic interaction to derive the contextualized CS-Features from the paired images and texts. In the first interaction level, multi-head crossattention operation is performed between FV and FT , preliminarily Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Figure 2: Overall structure of our CRISP-SAM2, which omits the memory attention, memory encoder, and memory bank components lfor clarity. CRISP-SAM2 produces accurate masks of 3D multi-organ segmentation under the guidance of textual information. two-level progressive cross-modal interaction mechanism is adopted to extract contextualized semantics. Then, the semantics will be injected into image features, generate prompt embedding and further refine masks respectively, promoting superior segmentation prediction with precise local details and boundaries. Here, \"Enc.\" represents \"Encoder\". merging semantics from each modality. This is represented as: (1) FVT = 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (FT , FV ) , FTV = 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (FV , FT ) , where the former parameter of 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (, ) is projected into the query for the multi-head cross-attention mechanism, and the latter from the other modality is projected into the key and the value. In order to capture localized information, we employ depth-wise convolution projection following [19] to replace direct linear projection, which is detailed in Appendix C.1. This initial interaction introduces features from one modality to the other, establishing preliminary dependencies between them. To further extract complex cross-modal relationships and improve contextual representations, second-level multi-head crossattention layer is added. Initially, FV and FT are concatenated, preserving more efficacious information than direct summation could, which reduces the loss of modal-specific features. convolution layer is then applied to enhance consistent feature representation and eliminate redundancy from irrelevant features, producing the output: FC = 洧냤洧녶洧녵洧녺 (洧냤洧녶洧녵洧녫洧녩洧노 (FV , FT )) , (2) where 洧냤洧녶洧녵洧녫洧녩洧노 (, ) denotes concatenation and 洧냤洧녶洧녵洧녺 () represents convolution, using kernel size of 3 3. FC serves as the query for cross-attention with FTV and FVT . In this interaction level, FC, which is enriched with consistent semantics from both visual and linguistic modalities, acts as query to uncover potential complex correlations within initially fused cross-modal features FTV and FVT from previous level. This process is outlined as: TV = 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (FC, FTV ) , VT = 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (FC, FVT ) . Finally, the outputs of the second-level attentions are summed, followed by feed-forward (FF) layer, yielding the final CS-Features FCS of this module. Notably, we inject these features into the image (3) encoder prior to the FF layer to preserve original cross-modal semantics and relevant contexts, enhancing the semantic understanding of visual features given the robust feature extraction capabilities of transformer blocks. This progressive two-level cross-attention structure effectively extracts cross-modal semantics and relationships between visual and linguistic features. Additional experiments validating this two-level cross-modal semantic interaction strategy are presented in Section 4.3, comparing with other structures. Cross-Modal Semantic Injection. Recognizing that FCS are unseen to the model, we employ Cross-Modal Semantic Injector, i.e. CS-Injector, to integrate this complementary feature into the pre-trained image encoder of SAM2 [50]. This approach largely follows the architecture of SAM2-Adapter [5, 6]. Specifically, we frozen the pre-trained weights of the Hiera image encoder to preserve its strong visual representations. In addition, simple and efficient CS-Injector is designed to inject semantics, consisting of just two light-weight MLPs. The dimensions of the second MLP correspond to the four hierarchical transformer blocks within the image encoder. It is worth noting that we additionally sum the output of the first MLP with the original input of visual features as the input of the second MLP. Ultimately, the injected image encoder produces the Image Embedding EI ."
        },
        {
            "title": "3.3 Prompt Generation\nDespite the impressive segmentation capabilities of SAM [31] and\nSAM2 [50], geometric prompts are still essential for accurate seg-\nmentation, especially in challenging scenarios where accurate points,\nbounding boxes, and masks are absent [86]. Medical images often\npose such challenges due to their fuzzy and irregularly shaped\nboundaries, increasing the need for precise prompts. However, ob-\ntaining these prompts requires interactive labeling by experienced\ndoctors, which significantly raises labor and time costs and limits\nbroader applications of the models in medical contexts.",
            "content": "CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland To address this limitation, we introduce an efficient Semantic Prompt Projector. This innovation automatically generates Sparse and Dense Prompt Embeddings without additional demanding of auxiliary geometric inputs. The Semantic Prompt Projector utilizes the CS-Features and outputs from the four transformer blocks within the image encoder and applies positional embedding to guarantee that the Sparse and Dense Prompt Embeddings share the same position information with the Image Embedding. As illustrated in the middle of Figure 2, we initially derive visual features at different scales from four outputs of each transformer , 3 block. These features, denoted as {f 1 }, are processed through BiFPN [73] structure to create , synthesizing multiscale information. Next, batch normalization (BN) [27] is applied to both visual features and CS-Features to balance them, which is followed by multi-head cross-attention to produce the Semantic Embedding: , 2 , 4 ES = 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (cid:0)洧洧녜 (FCS) , 洧洧녜 (cid:0)F (cid:1)(cid:1) . (4) Since the image encoder remains frozen during training and inference, the Position Embedding EP can be computed and stored previously. By summing EP with ES, we enhance the capture of positional information within the prompt embedding, which is subsequently processed through convolution layer: (5) = 洧냤洧녶洧녵洧녺 (ES EP ) , where denotes summation. In SAM2 [50], sparse and dense prompt embeddings are derived from points/bounding boxes and coarse masks, providing geometric guidance for segmentation. The former provides crude location information, while the latter gives more dense and comprehensive representation of the overall structure of segmentation targets. Emulating this design, we concatenate with the visual feature E to enhance the visual representation, while using pooling layer to sparsify representation. It should be noted that we use the ada-pooling [72] to avoid the loss of details with efficiency while being suitable for sparse coding. Then, the final Dense Prompt Embedding EDP and Sparse Prompt Embedding ESP are obtained through an extra convolution layer and MLP: EDP = 洧냤洧녶洧녵洧녺 (cid:0)洧냤洧녶洧녵洧녫洧녩洧노 (cid:0)FI , (cid:0)洧녞洧녶洧녶洧녳 (cid:0)E ESP = 洧洧洧녞2 (cid:1)(cid:1) , (cid:1)(cid:1) , (6) where 洧洧洧녞2 () refers to the two-layer MLP and 洧녞洧녶洧녶洧녳 () denotes the ada-pooling [72] layer. These prompt embeddings, generated by the Semantic Prompt Projector, are then used by the mask decoder to aid in segmentation."
        },
        {
            "title": "3.4 Mask Prediction\nMask Decoding. Partly inspired by the design of HQ-SAM [30],\nwe introduce an additional learnable output tokens, termed CS-\nOutput Tokens TCS, which matches the size of the original Output\nTokens. As one input of the mask decoder, TCS is concatenated with\nthe original Output Tokens TO and the Sparse Prompt Embedding\nESP . Following the original design, the Image Embedding EI is\ncombined with the Dense Prompt Embedding EDP . Consequently,\nthe Original Masks segmentation is expressed as:",
            "content": "MO = Ds (EI EDP, Concat (TO, ESP, TCS)) , (7) where Ds represents the mask decoder. Additionally, the mask decoder of SAM2 [50] incorporates stride 4 and 8 features from the image encoder. During this process, TCS gains access to the global visual context, geometric prompt information, and cross-modal semantics, evolving into the Updated CS-Output Tokens CS. Local Refining. With the Original Masks MO obtained, we proceed to generate Refined Masks MR for correction. This involves applying two three-layer MLP on the Updated CS-Output Tokens CS and CS-Features FCS respectively, followed by dot product layer to produce MR. The predicted logits of MO and MR are then summed to refine the prediction, resulting in the final segmentation mask MF . This process is represented by: MF = (cid:0)洧洧洧녞3 (FCS) 洧洧洧녞3 (8) where 洧洧洧녞3 () denotes three-layer MLP, signifies the dot product operation, and indicates logits summation. (cid:1)(cid:1) MO, (cid:0)T CS"
        },
        {
            "title": "3.5 Remaining Components\nSimilarity-Sorting Self-Updating Strategy. Distinct from natu-\nral videos, FIFO is unsuitable for direct application to 3D medical\nimaging due to the specific characteristics of these images. For\nexample, initial and final slices of abdominal scans often contain\nonly background, with no organs present. Inputting these slices\nfirst and including these slices in the memory bank could adversely\naffect the segmentation accuracy of other slices. To address this,\nwe implement a similarity-sorting self-updating strategy to ensure\nonly high-quality slices are retained in the memory bank. We begin\nby calculating a similarity score for each slice in a 3D image, using\nthe sum of cosine similarity between each slice and all others. The\nslices are then ranked based on their similarity score to establish a\nsegmentation sequence that prioritizes slices to be segmented with\nhigher similarity scores, thus providing more accurate guidance\nfor subsequent slices. To update the memory bank, we first filter\nobtained embedding by the intersection over union (IoU) score\nfrom the mask decoder and then calculate the embedding similarity\nscores of the current frame and previous embeddings stored in the\nmemory bank, then, replace the embedding with the lowest score.\nMore realization details are in Appendix C.1.",
            "content": "Loss Function. In line with SAM2 [50], we employ focal and dice losses for both Original Masks MO and Refined Masks MR to supervise mask predictions. Additionally, mean-absolute-error (MAE) loss is applied for IoU prediction, and cross-entropy (CE) loss is used for object detection. The overall loss function is organized as: 洧洧 洧녭 洧녰洧녵洧뉧롐 洧멇롐뉧롐 洧녶洧洧녰洧녮洧녰洧녵洧녩洧녳 洧멇롐뉧롐 = 洧띺 + (1 洧띺)L (9) where 洧띺, 洧띻, and 洧 are learnable weights, L洧녴洧녩洧 denotes the MAE loss, 洧녶洧洧녰洧녮洧녰洧녵洧녩洧녳 and L洧녫洧 represents the CE loss. The segmentation losses 洧멇롐뉧롐 + 洧띻L洧녴洧녩洧 + 洧 L洧녫洧, 洧洧 洧녭 洧녰洧녵洧뉧롐 洧멇롐뉧롐 are calculated as: L洧멇롐뉧롐 = 洧랪1L洧녭 洧녶洧녫洧녩洧녳 + 洧랪2L洧녬洧녰洧녫洧 , with and weight ratio of 洧랪1 : 洧랪2 = 20 : 1, which is consistent with the original setting of SAM2."
        },
        {
            "title": "4 Experiments\n4.1 Experimental Setups\nDatasets. Our training and inference experiments are conducted\non M3D-Seg dataset [1], a cross-modal medical segmentation joint",
            "content": "Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Table 1: Segmentation performance of our proposed method and other comparison models on seven datasets. The results are evaluated by the average DSC and NSD of all the segmented organs. means assisted with geometric prompts, and denotes assisted with anatomical prompt (refer to [25]). \"Abdomen\" refers to \"AbdomenCT-1k\", and the same in the following tables. Method SAM [31] SAM2 [63] MedSAM [50] I-MedSAM [78] MedSAM-2 [103] CT-SAM3D [18] Abdomen MSD-Spleen Pancreas-CT DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD FLARE22 AMOS22 LUNA16 WORD 73.82 70.58 65.08 60.41 56.74 51.44 64.63 60.47 63.26 60.29 64.62 60.77 62.27 58.81 83.60 79.09 79.92 75.33 63.38 60.60 83.94 79.11 76.54 74.48 78.19 76.95 74.73 72.55 76.07 70.95 70.08 66.52 60.81 54.37 69.07 64.05 72.47 68.75 67.43 64.36 65.10 61.76 81.37 78.70 75.14 73.84 60.63 58.99 74.71 69.82 67.60 63.35 74.34 71.70 70.08 66.39 86.29 86.24 79.48 80.33 64.00 65.62 86.76 86.41 77.42 78.61 84.81 85.50 79.31 80.64 91.83 93.56 84.57 85.65 72.61 76.97 90.05 94.55 80.33 83.99 89.99 94.85 85.74 88. LViT [42] CAT [25] SegVol [13] ZePT [29] 79.40 78.98 76.54 77.34 70.56 74.78 75.95 76.97 73.66 74.03 75.14 72.35 72.60 69.12 94.26 95.82 86.29 88.13 77.31 80.53 88.38 92.79 83.53 88.40 90.42 94.06 86.39 89.25 92.79 93.83 81.10 83.66 75.20 77.94 87.70 91.35 81.65 85.78 82.68 82.83 80.15 80.67 94.03 95.75 85.71 89.30 77.17 83.21 91.34 94.37 84.26 88.96 86.28 90.01 87.03 89.27 CRISP-SAM2 (Ours) 95.33 97.26 87.22 90.65 78.58 85.15 92.28 96.40 85.47 90.51 90.30 95.59 86.88 90.74 dataset. M3D-Seg includes variety of classical medical sub-datasets, each annotated with visual segmentation targets and textual descriptions consisting of two to six sentences. We specifically focus on seven high-quality sub-datasets from M3D-Seg: MSD-Spleen [68], Pancreas-CT [12], LUNA16 [66], AbdomenCT-1k [52], WORD [49], FLARE22 [51], and AMOS22 [28]. These sub-datasets have one to fourteen segmentation categories, encompassing eighteen abdominal and thoracic organs: liver (L), spleen (SP), stomach (ST), gallbladder (G), esophagus (E), pancreas (P), duodenum (D), aorta (A), bladder (B), inferior vena cava (IV), left kidney (LK), right kidney (RK), left adrenal gland (LA), right adrenal gland (RA), left femur (LF), right femur (RF), left lung (LL) and right lung (RL). We maintain the original data partition of M3D-Seg [1], which has an 8:2 ratio between training and testing sets. To prevent data bias and misrepresentation, we exclude partial labels of organs, tumors, and nodules that appear in only one sub-dataset. Notably, the AbdomenCT-1k [52] dataset originally label both the left and right kidneys as kidneys, thus we differentiate these into LK and RK by connected components and coordinates. Furthermore, the postcava label in AMOS22 [28] and the inferior vena cava label in FLARE22 [51] dataset refer to the same organ actually, which we consistently denote as IV. For additional details and illustrations regarding the selected datasets, please refer to Appendix B. Implementation Details. To expand the datasets and balance class of samples, we employ extensive data augmentation techniques. During the training phase, CRISP-SAM2 model uses twostage strategy for total of 400 epochs, incorporating warm-up period. We use the AdamW optimizer [45] with an initial learning rate of 1洧 4 and polynomial decay power of 0.9. All experiments are performed on 8 NVIDIA A100-SXM4-80G GPUs with batch size set to 16. Additional details are provided in Appendix C.2. Evaluation Metrics. We use the Dice Similarity Coefficient (DSC, %) and Normalized Surface Distance (NSD, %) as evaluations, which are commonly used in medical segmentation tasks. Both metrics range from 0 to 1, with higher values indicating more accurate predictions. DSC emphasizes the overall overlap between predicted masks and ground truths, while NSD is more sensitive to the consistency of boundaries and localized details between them."
        },
        {
            "title": "4.2 Comparison Results\nTo evaluate the performance of our CRISP-SAM2 model, we per-\nform comparative experiments against other SOTA methods in the\nseven datasets, utilizing SAM [31] and SAM2 [63] models as base-\nlines. Our comparisons include both purely visual segmentation ap-\nproaches, namely MedSAM [50], I-MedSAM [78], MedSAM-2 [103],\nand CT-SAM3D [18], and text-engaged segmentation methods, in-\ncluding LViT [42], CAT [25], SegVol [13], and ZePT [29]. For the\ncomparison of prompt-required models, we utilize three points and\na bounding box as prompts, which are randomly selected from the\ncenter of ground truth and a rectangle completely covers targets.\nMore comparison results are shown in Appendix E.1.",
            "content": "Quantitative Results. Table 1 presents the segmentation performance on the test sets of the seven datasets. Our model significantly outperforms the baselines, achieving at least 21.5% and 7.3% improvement in the DSC, and at least 26.6% and 15.3% increase in NSD compared to SAM and SAM2, respectively. Compared to other models, CRISP-SAM2 shows superior performance, with improvements in average DSC and NSD ranging from 0.93% to 1.27% and 1.26% to 1.94% across the MSD-Spleen [68], Pancreas-CT [12], LUNA16 [66], AbdomenCT-1k [52], and WORD [49] datasets. On the FLARE22 [51] and AMOS22 [28] datasets, CRISP-SAM2 maintains lead of 0.74% to 1.47% in NSD, highlighting its accuracy in segmenting local details, while being only marginally less than 0.12% and 0.15% behind the best in DSC. Compared to unimodal medical segmentation methods [18, 50, 78, 103], CRISP-SAM2 achieves the highest DSC and NSD values across all datasets, with at least 2.99% and 4.03% enhancement in DSC and NSD evenly. Among all the text-guided methods [13, 25, 29, 42], CRISP-SAM2 demonstrates the best overall performance, with an average improvement of at least 0.74% in DSC and 1.62% in NSD across the seven datasets. Specifically, on the AbdomenCT-1k [52] dataset, which is the largest of the seven, with 800 training samples and 200 test samples, CRISP-SAM2 outperforms the four relatively CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland Figure 3: Visualization demonstrations of CRISP-SAM2 and other counterpart SOTA methods on selected datasets. Rows (a), (b), (d), and (f) show the segmentation results of selected cases in 3D view, while rows (c) and (e) show the predicted masks in 2D imaging slices. The areas enclosed by the dashed black or white boxes showcase some predicted details among different models. outstanding comparative methods, i.e. CT-SAM3D [18], CAT [25], SegVol [13] and ZePT [29] with DSC and NSD improvements of 2.23%/3.90%/4.58%/0.94% and 1.85%/3.61%/5.05%/2.03%, respectively, demonstrating superior segmentation accuracy. Furthermore, CRISP-SAM2, prompting by cross-modal semantics without reliance on geometric prompts, outperforms promptrequired methods, which showcases the effectiveness of the Semantic Prompt Projector module. We will discuss this part further in Section 4.3. It is also noteworthy that models integrating textual information, namely CAT [25], SegVol [13], ZePT [29], and our CRISPSAM2, generally perform better than uni-modal visual methods, particularly on datasets like LUNA16 [66] and WORD [49] where other methods underperform. This is because these text-guided models incorporate complementary information from textual inputs, which assists with segmentation. In summary, our CRISP-SAM2 could segment accurately with details and boundaries consistent with ground truths, exhibiting superior overall performance than other SOTA models, especially in challenging scenarios. Visualization Results. Figure 3 presents the visualization results of our CRISP-SAM2 model alongside other SOTA methods, providing additional evidence of the effectiveness and superiority of our method. The proposed CRISP-SAM2 excels particularly in dealing with local details and irregularly shaped boundaries. For instance, row (b) and the dashed boxes in rows (c) and (d) highlight how CRISP-SAM2 produces more precise boundaries for irregularly shaped organs. Additionally, rows (e) and (f) demonstrate CRISPSAM2 superior ability to accurately segment localized details, especially for small and slender organs. These results intuitively demonstrate that CRISP-SAM2 addresses common challenges faced by existing methods, achieving superior segmentation performance."
        },
        {
            "title": "4.3 Ablation Studies\nIn our ablation study, we conduct several experiments to demon-\nstrate the effectiveness of our proposed method and investigate\naspects that influence its performance, including component abla-\ntion, an analysis of the Cross-Modal Semantic Interaction module,\nthe effectiveness of the Semantic Prompt Projector, and qualitative\nresults based on textual inputs. All experiments are conducted on\nthe AbdomenCT-1k [52] and AMOS22 [28] datasets to ensure fair\ncomparison, keeping other experimental settings constant.",
            "content": "Components Ablations. Our method mainly adds four key improvements to the baseline SAM2: the Cross-Modal Semantic Interaction module, the Semantic Prompt Projector, the Local Refiner, and the similarity-sorting self-updating strategy. We select SAM2 as baseline and design experiments with or without these components and combinations of components. First, we add the Cross-modal Semantic Interaction module to the baseline, whose output is indispensable for the Semantic Prompt Projector and the Local Refiner. We retain the CS-injector to integrate cross-modal semantics and directly output masks from the mask decoder. Next, we evaluate the performance improvements by incorporating the Semantic Prompt Projector, the Local Refiner, and both together. In addition, we assess the performance of the similarity-sorting self-updating strategy both separately and with other components. As shown in Table 2, the Cross-Modal Semantic Interaction module effectively fuses textual and visual features, generating complementary cross-modal semantics. This integration leads to improvements of 3.42%-4.78% in DSC and 5.21%-6.08% in NSD, underscoring the value of textual guidance in segmentation. Building on this, the inclusion of the Semantic Prompt Projector further enhances Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Table 2: Components ablation results on two selected datasets. In this table, SI, PP, LR, and SS denote Cross-Modal Semantic Interaction, Semantic Prompt Projector, Local Refiner, and similarity-sorting self-updating strategy respectively. Table 4: Experimental results of the comparisons between geometric point prompts, bbox prompts, and our proposed textual prompting."
        },
        {
            "title": "Component\nAbdomen\nSI PP LR SS DSC NSD DSC NSD",
            "content": "AMOS22 (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 83.94 79.11 74.73 72.55 87.36 85.19 79.51 77.76 90.42 90.75 84.20 83.58 86.78 90.34 81.06 82.96 91.93 95.11 86.32 89.39 (cid:34) 84.64 83.07 75.55 75.61 (cid:34) (cid:34) (cid:34) (cid:34) 92.28 96.40 86.88 90.74 Table 3: Analytical experiments between our two-level mechanism in the Cross-modal Semantic Interaction module and other interaction structure."
        },
        {
            "title": "Abdomen\nDSC NSD DSC NSD",
            "content": "AMOS22 first 89.51 92.77 85.10 88.39 second 87.94 91.04 84.83 87.12 secondfirst 91.59 95.43 86.21 89.56 firstsecond 92.28 96.40 86.88 90.74 performance by more than 4% in DSC and 5.5% in NSD, aided by the generated Sparse and Dense Prompt Embeddings. Although the Local Refiner does not significantly improve DSC, it notably boosts NSD, indicating more precise segmentation of localized and small details. Compared to the FIFO strategy, our similarity-sorting self-updating strategy, tailored to the characteristics of medical images, resulted in an increase of over 0.4% and 1.3% in the two metrics. The model incorporating all four key components outperforms those with only individual components or other combinations, demonstrating that each component positively contributes and their integration is complementary. Analysis of Semantic Interaction. To effectively integrate visual and linguistic features, we develop two-level progressive crossattention semantic interaction strategy. To assess the impact of this approach, we conduct experiments analyzing the effects of using each level of cross-attention separately and in combinations. As shown in Table 3, our interaction structure achieves the best performance, surpassing the next best structure, which reverses the order of the two levels of cross-attention, by more than 0.6% in DSC and 0.9% in NSD. The two-level cross-attention approach extracts richer and more profound cross-modal semantics compared to single-level structures, leading to improved segmentation of localized details. Benefiting from our proposed structure, the first level of cross-attention establishes an initial dependency between"
        },
        {
            "title": "Abdomen",
            "content": "AMOS"
        },
        {
            "title": "Text Point BBox DSC NSD DSC NSD",
            "content": "(cid:34) 87.20 90.89 81.49 83.63 88.99 91.33 83.50 84.76 (cid:34) 89.17 91.85 83.94 85.02 (cid:34) (cid:34) 89.78 92.72 84.57 86.28 92.28 96.40 86.88 90.74 (cid:34) the two separate modalities, while the second level strengthens this dependency and effectively extracts cross-modal semantics. Effectiveness of Prompt Generation. Most segmentation models heavily rely on geometric prompts, i.e. points, bounding boxes, and coarse masks, to achieve accurate results. It significantly limits their wider applications, thus, we conduct further experiments to validate the effectiveness of our Semantic Prompt Projector. In these experiments, we replace the Semantic Prompt Projector with the original prompt encoder and compare various combinations of geometric prompts. Given that coarse masks are rarely available in real medical settings and using other models to generate them could introduce bias, we exclude mask prompts from our comparison. As shown in Table 4, the improvement brought by prompts is obvious, and our textual prompt could achieve superior performance compared to the use of point and bounding box prompts, which improves the DSC and NSD by over 2.3% and 3.6%, respectively. When compared to the model without any prompts, these improvements increase to over 5.0% and 5.5%, respectively."
        },
        {
            "title": "5 Conclusion\nIn this study, we introduce CRISP-SAM2, an innovative model tai-\nlored for medical multi-organ segmentation that effectively inte-\ngrates cross-modal interaction and semantic prompting. This ap-\nproach utilizes additional textual information, specifically descrip-\ntions of target organs, to guide segmentation predictions. To gen-\nerate cross-modal semantics, we develop a Cross-Modal Semantic\nInteraction module featuring a two-level cross-attention interaction\nmechanism. This mechanism progressively extracts contextualized\nsemantics from distinct visual and textual modalities. Leveraging\nthe obtained cross-modal semantics, the Semantic Prompt Projector\nproduces Sparse and Dense Prompt Embeddings without relying on\ngeometric prompts, while the Local Refiner and similarity-sorting\nself-updating strategy further enhance the precision of boundaries\nand local details in the predicted masks. Our model successfully\naddresses existing challenges in the field, namely inaccuracies in\ndetails and boundaries, reliance on geometric prompts, and compro-\nmised spatial information. Extensive experiments, including com-\nparisons with other SOTA methods, component ablation studies,\nfurther analysis, and visualization results, highlight the superiority\nand efficacy of CRISP-SAM2, which obtains superior performance\nthan other SOTA models on seven publicly available datasets.",
            "content": "CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland Acknowledgments This work was supported by the Open Project Program of the State Key Laboratory of CAD&CG [No. A2410], Zhejiang University; National Natural Science Foundation of China[No. 61702146, 62076084, U20A20386, U22A2033]."
        },
        {
            "title": "References",
            "content": "[1] Fan Bai, Yuxin Du, Tiejun Huang, Max Q-H Meng, and Bo Zhao. 2024. M3d: Advancing 3d medical image analysis with multi-modal large language models. arXiv preprint arXiv:2404.00578 (2024). [2] Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. 2024. Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering. arXiv preprint arXiv:2412.12359 (2024). [3] Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. 2025. Prism: Self-pruning intrinsic selection method for training-free multimodal data selection. arXiv preprint arXiv:2502.12119 (2025). [4] Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. 2024. RSPrompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model. IEEE Transactions on Geoscience and Remote Sensing (2024), 117. [5] Tianrun Chen, Ankang Lu, Lanyun Zhu, Chaotao Ding, Chunan Yu, Deyi Ji, Zejian Li, Lingyun Sun, Papa Mao, and Ying Zang. 2024. Sam2-adapter: Evaluating & adapting segment anything 2 in downstream tasks: Camouflage, shadow, medical image segmentation, and more. arXiv preprint arXiv:2408.04579 (2024). [6] Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, and Papa Mao. 2023. Sam-adapter: Adapting segment anything in underperformed scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. 3367 3375. [7] Yinda Chen, Wei Huang, Xiaoyu Liu, Shiyu Deng, Qi Chen, and Zhiwei Xiong. 2024. Learning multiscale consistency for self-supervised electron microscopy instance segmentation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 15661570. [8] Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, and Zhiwei Xiong. 2024. Bimcv-r: landmark dataset for 3d ct text-image retrieval. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 124134. [9] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. 2025. SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation. In European Conference on Computer Vision (ECCV). Springer, 323340. [10] Fenghua Cheng, Xue Li, Haoyang Wu, Jiangcheng Sang, and Wenqi Zhao. 2024. Recent Advances on Multi-modal Dialogue Systems: Survey. In International Conference on Advanced Data Mining and Applications. Springer, 3347. [11] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan arXiv preprint Segment and track anything. Wang, and Yi Yang. 2023. arXiv:2305.06558 (2023). [12] Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen Moore, Stanley Phillips, David Maffitt, Michael Pringle, et al. 2013. The Cancer Imaging Archive (TCIA): maintaining and operating public information repository. Journal of Digital Imaging 26 (2013), 10451057. [13] Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. 2025. Segvol: Universal and interactive volumetric medical image segmentation. Advances in Neural Information Processing Systems (NeurIPS) 37 (2025), 110746110783. [14] Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, et al. 2024. EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers. arXiv preprint arXiv:2412.20413 (2024). [15] Eli Gibson, Francesco Giganti, Yipeng Hu, Ester Bonmati, Steve Bandula, Kurinchi Gurusamy, Brian Davidson, Stephen Pereira, Matthew Clarkson, and Dean Barratt. 2018. Automatic multi-organ segmentation on abdominal CT with dense V-networks. IEEE Transactions on Medical Imaging (TMI) 37, 8 (2018), 18221834. [16] Shreyank Gowda and David Clifton. 2025. CC-SAM: SAM with Crossfeature Attention and Context for Ultrasound Image Segmentation. In European Conference on Computer Vision (ECCV). Springer, 108124. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [18] Heng Guo, Jianfeng Zhang, Jiaxing Huang, Tony CW Mok, Dazhou Guo, Ke Yan, Le Lu, Dakai Jin, and Minfeng Xu. 2025. Towards comprehensive, efficient and promptable anatomic structure segmentation model using 3d whole-body ct scans. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 32473256. [19] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. 2022. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1217512185. [20] Weizhao He, Yang Zhang, Wei Zhuo, Linlin Shen, Jiaqi Yang, Songhe Deng, and Liang Sun. 2024. APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2376223772. [21] Jihong Hu, Yinhao Li, Hao Sun, Yu Song, Chujie Zhang, Lanfen Lin, and YenWei Chen. 2024. LGA: Language Guide Adapter for Advancing the SAM Models Capabilities in Medical Image Segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 610620. [22] Jian Hu, Jiayi Lin, Shaogang Gong, and Weitong Cai. 2024. Relax Image-Specific Prompt Requirement in SAM: Single Generic Prompt for Segmenting Camouflaged Objects. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1251112518. [23] Xiaorui Huang, Gen Luo, Chaoyang Zhu, Bo Tong, Yiyi Zhou, Xiaoshuai Sun, and Rongrong Ji. 2024. Deep Instruction Tuning for Segment Anything Model. In Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM). 905914. [24] Yuzhi Huang, Chenxin Li, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Xiaotong Tu, and Yixuan Yuan. 2024. P2sam: Probabilistically prompted sams are efficient segmentator for ambiguous medical images. In Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM). 97799788. [25] Zhongzhen Huang, Yankai Jiang, Rongzhao Zhang, Shaoting Zhang, and Xiaofan Zhang. 2025. Cat: Coordinating anatomical-textual prompts for multi-organ and tumor segmentation. Advances in Neural Information Processing Systems (NeurIPS) 37 (2025), 35883610. [26] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [27] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML). pmlr, 448456. [28] Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhanng, Wanling Ma, Xiang Wan, et al. 2022. Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), 36722 36732. [29] Yankai Jiang, Zhongzhen Huang, Rongzhao Zhang, Xiaofan Zhang, and Shaoting Zhang. 2024. Zept: Zero-shot pan-tumor segmentation via query-disentangling and self-prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1138611397. [30] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. 2024. Segment anything in high quality. Advances in Neural Information Processing Systems (NeurIPS) 36 (2024), 2991429934. [31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 40154026. [32] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang. 2022. Mpvit: Multi-path vision transformer for dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 72877296. [33] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao. 2025. Segment and Recognize Anything at Any Granularity. In European Conference on Computer Vision (ECCV). Springer, 467484. [34] Leyang Li, Shilin Lu, Yan Ren, and Adams Wai-Kin Kong. 2025. Set you straight: Auto-steering denoising trajectories to sidestep unwanted concepts. arXiv preprint arXiv:2504.12782 (2025). [35] Quanjiang Li, Tingjin Luo, and Jiahui Liao. 2025. Theory-Inspired Deep MultiView Multi-Label Learning with Incomplete Views and Noisy Labels. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR). 2070620715. [36] Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, and Tao Chen. 2025. ClipSAM: CLIP and SAM collaboration for zero-shot anomaly segmentation. Neurocomputing 618 (2025), 129122. [37] Shijie Li, Yunbin Tu, Qingyuan Xiang, and Zheng Li. 2024. MAGIC: Rethinking Dynamic Convolution Design for Medical Image Segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM). 91069115. [38] Wenxue Li, Xinyu Xiong, Peng Xia, Lie Ju, and Zongyuan Ge. 2024. TP-DRSeg: improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted SAM. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 743753. Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. [39] Yonglin Li, Jing Zhang, Xiao Teng, Long Lan, and Xinwang Liu. 2023. Refsam: Efficiently adapting segmenting anything model for referring video object segmentation. arXiv preprint arXiv:2307.00997 (2023). [40] Zixu Li, Zhiwei Chen, Haokun Wen, Zhiheng Fu, Yupeng Hu, and Weili Guan. 2025. Encoder: Entity mining and modification relation binding for composed image retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 51015109. [41] Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, and Liqiang Nie. 2025. Finecir: Explicit parsing of fine-grained modification semantics for composed image retrieval. arXiv preprint arXiv:2503.21309 (2025). [42] Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, and Qingqi Hong. 2023. Lvit: language meets vision transformer in medical image segmentation. IEEE Transactions on Medical Imaging (TMI) 43, 1 (2023), 96107. [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2025. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision (ECCV). Springer, 3855. [44] Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, and Bin Yang. 2025. Rethinking Irregular Time Series Forecasting: Simple yet Effective Baseline. arXiv preprint arXiv:2505.11250 (2025). [45] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [46] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. 2023. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR). 22942305. [47] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. 2024. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 64306440. [48] Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. 2024. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775 (2024). [49] Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris Metaxas, Guotai Wang, and Shaoting Zhang. 2022. WORD: large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. Medical Image Analysis (MedIA) 82 (2022), 102642. [50] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2024. Segment anything in medical images. Nature Communications 15, 1 (2024), 654. [51] Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, et al. 2023. Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge. arXiv preprint arXiv:2308.05862 (2023). [52] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, et al. 2021. Abdomenct-1k: Is abdominal organ segmentation solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 44, 10 (2021), 66956714. [53] Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, et al. 2018. Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy. arXiv preprint arXiv:1809.04430 (2018). [54] Ting Pan, Lulu Tang, Xinlong Wang, and Shiguang Shan. 2025. Tokenize anything via prompting. In European Conference on Computer Vision (ECCV). Springer, 330348. [55] Jay Paranjape, Nithin Gopalakrishnan Nair, Shameema Sikder, Swaroop Vedula, and Vishal Patel. 2024. Adaptivesam: Towards efficient tuning of sam for surgical scene segmentation. In Annual Conference on Medical Image Understanding and Analysis. Springer, 187201. [56] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information Processing Systems (NeurIPS) 32 (2019). [57] Haotian Qian, Yinda Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, and Deng-Ping Fan. 2024. Maskfactory: Towards high-quality synthetic data generation for dichotomous image segmentation. Advances in Neural Information Processing Systems (NeurIPS) 37 (2024), 6645566478. [58] Xiangfei Qiu, Hanyin Cheng, Xingjian Wu, Jilin Hu, and Chenjuan Guo. 2025. Comprehensive Survey of Deep Learning for Multivariate Time Series Forecasting: Channel Strategy Perspective. arXiv preprint arXiv:2502.10721 (2025). [59] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, and Bin Yang. 2024. TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods. In Proceedings Of The Vldb Endowment (PVLDB). 2363 2377. [60] Xiangfei Qiu, Zhe Li, Wanghui Qiu, Shiyan Hu, Lekui Zhou, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Aoying Zhou, Zhenli Sheng, et al. 2025. Tab: Unified benchmarking of time series anomaly detection methods. arXiv preprint arXiv:2506.18046 (2025). [61] Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, and Bin Yang. 2025. DUET: Dual Clustering Enhanced Multivariate Time Series Forecasting. In SIGKDD. 11851196. [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML). PMLR, 87488763. [63] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R칛dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. 2025. SAM 2: Segment Anything in Images and Videos. In The Thirteenth International Conference on Learning Representations (ICLR). [64] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024). [65] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. 2023. Hiera: hierarchical vision transformer without the bellsand-whistles. In International Conference on Machine Learning (ICML). PMLR, 2944129454. [66] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. 2017. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge. Medical Image Analysis (MedIA) 42 (2017), 113. [67] Chao Shang, Zichen Song, Heqian Qiu, Lanxiao Wang, Fanman Meng, and Hongliang Li. 2024. Prompt-Driven Referring Image Segmentation with Instance Contrasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 41244134. [68] Amber Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, et al. 2019. large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063 (2019). [69] Chenyue Song, Chen Hui, Qing Lin, Wei Zhang, Siqiao Li, Shengping Zhang, Haiqi Zhu, Zhixuan Li, Shaohui Liu, Feng Jiang, et al. 2025. LVPNet: Latentvariable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images. arXiv preprint arXiv:2506.17983 (2025). [70] Chenyue Song, Chen Hui, Wei Zhang, Haiqi Zhu, Shaohui Liu, Hong Huang, and Feng Jiang. 2025. BPCLIP: Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP. arXiv preprint arXiv:2506.17969 (2025). [71] Chenyue Song, Wei Zhang, Feng Jiang, Deqin Zheng, Ruichen Gao, and Haiqi Zhu. 2024. ADTAH: Neuron 3D Reconstruction Via Adaptive Distance Transformation and Adaptive Hessian Matrix. In 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 37113716. [72] Alexandros Stergiou and Ronald Poppe. 2022. Adapool: Exponential adaptive pooling for information-retaining downsampling. IEEE Transactions on Image Processing (TIP) 32 (2022), 251266. [73] Mingxing Tan, Ruoming Pang, and Quoc Le. 2020. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1078110790. [74] Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, et al. 2025. Sam-med3d: towards general-purpose segmentation models for volumetric medical images. In European Conference on Computer Vision (ECCV). Springer, 5167. [75] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari. 2024. SAM-CLIP: Merging Vision Foundation Models Towards Semantic and Spatial Understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 36353647. [76] Yihao Wang, Meng Yang, and Rui Cao. 2024. Fine-grained Semantic Alignment with Transferred Person-SAM for Text-based Person Retrieval. In Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM). 54325441. [77] Yan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot Fishman, and Alan Yuille. 2019. Abdominal multi-organ segmentation with organ-attention networks and statistical fusion. Medical Image Analysis (MedIA) 55 (2019), 88102. [78] Xiaobao Wei, Jiajun Cao, Yizhu Jin, Ming Lu, Guangyu Wang, and Shanghang Zhang. 2024. I-medsam: Implicit medical image segmentation with segment anything. In European Conference on Computer Vision. Springer, 90107. [79] Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Fei Ma, and Jimin Xiao. 2025. Generative Prompt Controlled Diffusion for weakly supervised CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland semantic segmentation. Neurocomputing 638 (2025), 130103. [80] Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, and Jimin Xiao. 2025. Prompt categories cluster for weakly supervised semantic segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops. 31983207. [81] Wangyu Wu, Siqi Song, Xianglin Qiu, Xiaowei Huang, Fei Ma, and Jimin Xiao. 2025. Image fusion for cross-domain sequential recommendation. In Companion Proceedings of the ACM on Web Conference 2025. 21962202. [82] Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, and Chenjuan Guo. 2025. K2VAE: Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting. arXiv preprint arXiv:2505.23017 (2025). [83] Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, and Bin Yang. 2025. CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching. In International Conference on Learning Representations (ICLR). [84] Xi Xiao, Zhengji Li, Wentao Wang, Jiacheng Xie, Houjie Lin, Swalpa Kumar Roy, Tianyang Wang, and Min Xu. 2025. TD-RD: Top-Down Benchmark with RealTime Framework for Road Damage Detection. arXiv preprint arXiv:2501.14302 (2025). [85] Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, et al. 2025. Describe Anything in Medical Images. arXiv preprint arXiv:2505.05804 (2025). [86] Zhaozhi Xie, Bochen Guan, Weihao Jiang, Muyang Yi, Yue Ding, Hongtao Lu, and Lei Zhang. 2024. PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation. In 2024 IEEE International Conference on Multimedia and Expo (ICME). 16. [87] Yi Xin, Junlong Du, Qiang Wang, Zhiwen Lin, and Ke Yan. 2024. VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense Scene Understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1608516093. [88] Yi Xin, Junlong Du, Qiang Wang, Ke Yan, and Shouhong Ding. 2024. MmAP: Multi-modal Alignment Prompt for Cross-domain Multi-task Learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1607616084. [89] Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, and Yuntao Du. 2024. Parameter-efficient fine-tuning for pre-trained vision models: survey. arXiv preprint arXiv:2402.02242 (2024). [90] Zhen Yao, Xiaowen Ying, and Mooi Choo Chuah. 2025. Rethinking RGB-Event Semantic Segmentation with Novel Bidirectional Motion-enhanced Event Representation. arXiv preprint arXiv:2505.01548 (2025). [91] Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Hui Jin, Xinchen Jiang, Gangyong Jia, Qing Wu, Qinglei Shi, and Changmiao Wang. 2024. ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis Classification Network Using CLIPguided SAM mechanism. In 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 27952800. [92] Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qing Wu, Xiang Wan, Lihua Li, and Changmiao Wang. 2025. ICH-PRNet: cross-modal intracerebral haemorrhage prognostic prediction method using joint-attention interaction mechanism. Neural Networks 184 (2025), 107096. [93] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, and Chen Change Loy. 2025. Open-vocabulary SAM: Segment and recognize twenty-thousand classes interactively. In European Conference on Computer Vision (ECCV). Springer, 419437. [94] Wenxi Yue, Jing Zhang, Kun Hu, Yong Xia, Jiebo Luo, and Zhiyong Wang. 2024. Surgicalsam: Efficient class promptable surgical instrument segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 68906898. [95] Pengyu Zeng, Wen Gao, Jizhizi Li, Jun Yin, Jiling Chen, and Shuai Lu. 2025. Automated residential layout generation and editing using natural language and images. Automation in Construction 174 (2025), 106133. [96] Pengyu Zeng, Wen Gao, Jun Yin, Pengjian Xu, and Shuai Lu. 2024. Residential floor plans: Multi-conditional automatic generation using diffusion models. Automation in Construction 162 (2024), 105374. [97] Jielu Zhang, Zhongliang Zhou, Gengchen Mai, Mengxuan Hu, Zihan Guan, Sheng Li, and Lan Mu. 2023. Text2seg: Remote sensing image semantic segmentation via text-guided visual foundation models. arXiv preprint arXiv:2304.10597 (2023). [98] Yuxuan Zhang, Tianheng Cheng, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. 2024. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076 (2024). [99] Haochen Zhao, Hui Meng, Deqian Yang, Xiaozheng Xie, Xiaoze Wu, Qingfeng Li, and Jianwei Niu. 2024. GuidedNet: Semi-Supervised Multi-Organ Segmentation via Labeled Data Guide Unlabeled Data. In Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM). 886895. [100] Changshi Zhou, Rong Jiang, Feng Luan, Shaoqiang Meng, Zhipeng Wang, Yanchao Dong, Yanmin Zhou, and Bin He. 2025. Dual-arm robotic fabric manipulation with quasi-static and dynamic primitives for rapid garment flattening. IEEE/ASME Transactions on Mechatronics (2025). [101] Changshi Zhou, Haichuan Xu, Jiarui Hu, Feng Luan, Zhipeng Wang, Yanchao Dong, Yanmin Zhou, and Bin He. 2025. Ssfold: Learning to fold arbitrary crumpled cloth using graph dynamics from human demonstration. IEEE Transactions on Automation Science and Engineering (2025). [102] Yuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei Han, Elliot Fishman, and Alan Yuille. 2019. Prior-aware neural network for partiallysupervised multi-organ segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 1067210681. [103] Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, and Junde Wu. 2024. Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874 (2024). [104] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. 2024. Segment everything everywhere all at once. Advances in Neural Information Processing Systems (NeurIPS) 36 (2024), 1976919782. Appendix Related Works A.1 SAM and SAM2 Deep learning has made impressive progress in several fields, such as computer vision [7, 14, 40, 41, 4648, 57, 6971, 80, 84, 90, 90, 92, 95, 96], natural language processing [2, 3, 8, 85], automated control [100, 101], time series analysis [44, 5861, 82, 83], multi-modal analysis [34, 35, 79, 81], and multi-task learning [87, 88]. However, there is still lack of unified models in the field of computer vision, which is the reason why SAM and SAM2 are proposed. The SAM framework [31] is built around three primary components: an image encoder, prompt encoder, and mask decoder. Initially, the image to be segmented is processed through the image encoder to generate the image embedding. Subsequently, the mask decoder utilizes this image embedding, along with prompt embedding derived from points, bounding boxes, and coarse masks via the prompt encoder, to predict the final masks. SAM2 [63] retains the core structure but incorporates several enhancements, allowing it to segment videos as series of consecutive frames. This capability is primarily facilitated by three additional components: memory attention, memory encoder, and memory bank. Memory attention conditions the current frame based on previous frames and prompts, linking features of each frame with those in the temporal dimension. The memory encoder generates memories by integrating the image embedding of an unconditioned frame. These memories are then stored in the memory bank in FIFO sequence. Datasets B.1 Statistical Details We utilize seven sub-datasets from the M3D-Seg [1] dataset, joint cross-modal segmentation dataset with annotated visual targets and textual descriptions, with one to fourteen organs to be segmented. In total, the seven selected sub-datasets have 1919 training samples and 482 testing samples, and these datasets have 18 labels of organs to be segmented: liver (L), spleen (SP), stomach (ST), gallbladder (G), esophagus (E), pancreas (P), duodenum (D), aorta (A), bladder (B), inferior vena cava (IV), left kidney (LK), right kidney (RK), left adrenal gland (LA), right adrenal gland (RA), left femur (LF), right femur (RF), left lung (LL) and right lung (RL). The details of samples and the target labels of all datasets are as follows: (1) The MSD-Spleen [68] consists of 32 training samples and 9 test samples of 1 organ. All slices maintain consistent Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Table 5: Details of seven selected datasets. Here, we use capital letters to represent each organ. Dataset Organ SP ST D IV LK RK LA RA LF RF LL RL Train/Test Category MSD-Spleen [68] Pancreas-CT [12] LUNA16 [66] AbdomenCT-1k [52] WORD [49] FLARE22 [51] AMOS22 [28] Sum 32/9 65/17 710/178 800/200 80/20 40/10 192/48 1919/482 1 1 2 5 12 13 14 18 Table 6: Available links of selected datasets. Dataset M3D-Seg [1] Link https://github.com/BAAIDCAI/M3D/ https://huggingface.co/datasets/GoodBaiBai88/M3D-Seg/ https://www.modelscope.cn/datasets/GoodBaiBai88/M3D-Seg/ MSD-Spleen [68] Pancreas-CT [12] LUNA16 [66] http://medicaldecathlon.com/ https://wiki.cancerimagingarchive.net/display/public/pancreas-ct/ https://luna16.grand-challenge.org/Data/ AbdomenCT-1k [52] https://github.com/JunMa11/AbdomenCT-1K/ WORD [49] FLARE22 [51] AMOS22 [28] https://paperswithcode.com/dataset/word/ https://flare22.grand-challenge.org/ https://amos22.grand-challenge.org/ dimension of 512512 pixels, while the z-axis dimension ranges from 31 to 168, with median of 90. (2) The Pancreas-CT [12] consists of 65 training samples and 17 test samples of 1 organ. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 181 to 466, with median of 218. We exclude the pancreatic tumor label in this dataset. (3) The LUNA16 [66] consists of 710 training samples and 178 test samples of 2 organs. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 95 to 764, with median of 237. We exclude the trachea organ and lung nodule labels in this dataset. (4) The AbdomenCT-1k [52] consists of 800 training samples and 200 test samples of 5 organs. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 31 to 1026, with median of 103. We separate the original kidney label into left kidney and right kidney labels. (5) The WORD [49] consists of 80 training samples and 20 test samples of 12 organs. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 151 to 343, with median of 202. We exclude the adrenal, colon, intestine, and rectum organ labels in this dataset. (6) The FLARE22 [51] consists of 40 training samples and 10 test samples of 13 organs. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 71 to 113, with median of 106. (7) The AMOS22 [28] consists of 192 training samples and 48 test samples of 14 organs. All slices maintain consistent dimension of 512512 pixels, while the z-axis dimension ranges from 64 to 512, with median of 100. We exclude the prostate organ label in this dataset and we rename the postcava label to vena cava label because they are actually the same organ. For visual inputs, all the sub-datasets have already been preprocessed in the M3D-Seg dataset, particularly the format of mask files has been harmonized into sparse matrix storage format and the data has been normalized. Also, all sub-datasets have been split into training and testing sets with ratio of 8:2. Thus, we follow the original splitting of the joint dataset for the images and mask labels. For the textual information, the joint dataset provides descriptive texts for each organ, including relative and absolute position, shape, size, function, etc., which was generated by external GPT-4o [26] and saved in persistent documents for selection. Additionally, we slightly expanded the description sets and generated more diverse descriptions using GPT-4o for some organs. In our experiments, we randomly select two to six sentences with complete and legitimate semantics as textual input. The number of sentences is based on the performance of the segmentation prediction; the worse the performance, the more sentences we add to assist the segmentation. It is also worth mentioning that we build point and bbox prompts for prompt-required models, which are from the ground truths. Firstly, we identify the leftmost, rightmost, topmost, and bottommost positions of the mask to obtain the maximum lengths and CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland and testing samples and the amount and composition of samples of each organ respectively, which are shown in Figure 4. Besides, the information of labels included in each dataset and the available links of datasets are listed in Table 5 and Table 6 for convenient search. B.2 Representative Samples To provide more intuitive demonstrations of the selected crossmodal datasets, we select one representative sample from each dataset, as shown in Figure 6. Each sample is composed of labels of organs to be segmented in 3D view and texts for each organ in the dashed boxes. The color of each organ and the color of each dashed box are one-to-one correspondence. Experimental Settings C.1 Methodology Details Multi-Head Cross-Attention Mechanism. To fuse the features from two modalities or levels, we utilize modified multi-head crossattention operations for Eq. 1, 3, 4, which are within the CrossModal Semantic Interaction and Semantic Prompt Projector modules respectively. Here, we elaborate the process of depth-wise convolution projection [32] and cross-attention operation. Given the first and second parameters and , we first perform depthwise convolutions on to capture more localized information. Then, three linear layers further project them into query (Q), key (K), and value (V ), respectively. The process is presented as: {Q, K, } = (cid:110) QX, 洧냥洧냤洧녶洧녵洧녺3 (Y ) , 洧냥洧냤洧녶洧녵洧녺3 (Y ) (cid:111) , (10) where 洧냥洧냤洧녶洧녵洧녺3 () denotes 3 3 depth-wise convolutions and learnable weight matrices {W Q, K, } are calculated for linear projection. Finally, the cross-attention process could be formulated as: 洧냤洧洧녶洧멇롐멇롏엃롐뫯롐뫯롐 (X, ) = 洧녡洧녶 洧녭 洧노洧녴洧녩洧논 V, (11) (cid:19) (cid:18) QK 洧녬 Figure 4: The upper (a) annular chart shows the composition of the training and testing samples, and the lower (b) bar chart illustrates the amount and composition of samples of each organ. on the x-axis and y-axis respectively. Then, we randomly select point (洧녰0, 洧녱0) within the mask, ensuring that points (洧녰0 + 0.1 x, 洧녱0), (洧녰0 0.1 x, 洧녱0), (洧녰0, 洧녱0 + 0.1 y) and (洧녰0, 洧녱0 0.1 y) are still within the scope of the mask, which avoids misleading the model if the selected point is located in the margin of the mask. For the bbox prompt, we frame the square composed of four coordinates (洧녰1 洧노1 x, 洧녱1 洧노2 y), (洧녰1 洧노1 x, 洧녱1 +洧노2 y), (洧녰1 +洧노1 x, 洧녱1 洧노2 y) and (洧녰1 + 洧노1 x, 洧녱1 + 洧노2 y), where (洧녰1, 洧녱1) is the center point of the mask and 洧노1, 洧노2 [0.1, 0.3] are random coefficients. It guarantees that the generated bbox could cover the whole organ to be segmented. To provide more intuitive statistical details, we use the annular chart and bar chart to display the dataset composition of the training where 洧녡洧녶 洧녭 洧노洧녴洧녩洧논 () represents softmax operation and 1 洧녬 denotes the scaling factor. Additionally, we utilize the multi-head mechanism on our cross-attention to extract diverse and rich dependencies between two modalities, and the number of heads is set to 8 in our model. Similarity-Sorting Self-Updating Strategy. Due to the specificity of 3D medical images, specifically, the spatially intermediate areas contain more radiographic features while the edge areas of the imaging are mainly black-banded, we utilize similarity-sorting self-updating strategy to ensure high-quality embeddings stored in the memory bank, thus guiding subsequent slices. Firstly, we calculate the similarity score of each slice in 3D imaging input, which could be formulated as: score洧녰 = (cid:205)洧녲 洧녱=1 洧녱洧녰 洧멇롐뒳롐 (cid:0)洧멇롐뙗롐뒳롐넗롐뉧롐, 洧멇롐뙗롐뒳롐넗롐 洧녱 (cid:1), (12) where 洧녲 is the count of slices and 洧멇롐뒳롐 (, ) denotes cosine similarity function. Then, we sort the slices by the score as the input order, which means the slice with the higher score will be segmented earlier relatively. Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. For the self-updating strategy to replace the previous embedding in the memory bank, we first filter the embedding to be updated by the IoU score, which is calculated by the mask decoder alongside with the predicted masks. The embeddings with an IoU score that is lower than the threshold 洧노뀛롐洧뉧롐먹 = 0.6 will be directly discarded because of the less favorable prediction confidence. Then, we calculate similarity score for the current embedding along with the embeddings stored in the memory bank, the operation is same as in Eq. 12. Here, the 洧녲 is equal to the count of the embeddings stored in the memory bank plus one. It should be noted that if the current embedding has the lowest score, it will not be stored in the memory bank and not replace previous embeddings. C.2 Implementation Details Following the data augmentation in previous studies [18, 25, 99], random shift and rotation, gamma scaling, and brightness adjustment operations are applied to expand training samples and balance the numbers of each class of organ. We choose Hiera-B+ as the backbone and for the text encoder and image encoder in the Cross-Modal Semantic Interaction module, we adopt ViT-B/16 and CLIP-ViT-B/16 respectively, and fix their parameters. The input imaging is resized to 480 480 for the image encoder in this module. As listed in Table 7 all experiments are conducted on 8 NVIDIA A100-SXM4-80G GPUs in the PyTorch [56] framework with AdamW [45] optimizer. To make the best use of the global information of visual inputs under the limitations of the memory of GPUs, the input patch size and batch size are set to (96 96 96) and 16, with 2 batches per GPU. The process of training of our CRISP-SAM2 has total of 400 epochs with two stages: 1) Stage-I: we mainly train the modules except the components of SAM2 to independently optimize them, including the CS-injector, the Cross-Modal Semantic Interaction module (the text and image encoder are frozen), the Semantic Prompt Projector and Local Refiner. This stage has 120 epochs with warmup of 80 epochs, and the initial learning rate 1洧 4 with polynomial decay power of 0.9, which initializes these components steadily. 2) Stage-II: we further add other components of our model into optimization for 280 epochs, except the four pretrained transformer blocks in the image encoder, and the pretrained text and image encoders in the Cross-Modal Semantic Interaction module. In this stage, the initial learning rate is 2洧 4 to get superior global optimal solutions. Evaluation Metrics All experiments in this work are evaluated by DSC and NSD metrics. The values of these metrics range from 0 to 1; the closer the value is to 1, the better the segmentation performance is. The DSC value is useful evaluation of whether the segmentation results accurately cover the region of the organ by measuring the overall region overlap. While the NSD focuses on the distance between the surfaces of predicted segmentation and the ground truth. It evaluates the accuracy of segmentation by calculating the average distance between the surfaces and is more sensitive to the accuracy of the segmentation boundaries. The DSC value could be formulated as: DSC = 2X + , (13) Table 7: Values of experimental implementations and configurations. Implementation Initial Values of 洧띺, 洧띻, 洧 Optimizer Initial Learning Rate Stage-I Stage-II"
        },
        {
            "title": "Polynomial Decay Power\nTraining Epochs",
            "content": "Stage-I Warm-up Stage-II"
        },
        {
            "title": "Value",
            "content": "0.6,0.2,0.2 AdamW [45] - 1洧 4 2洧 4 0.9 400 120 80 280 NVIDIA A100-SXM4-80G 8 (96 96 96) 16 2 where and are the predicted mask and ground truth respectively, and represents the cardinality of set A. For details of NSD metric, we refer readers to Section 4.6 in [53]. And the NSD Tolerance is set to 5. Experimental Results E.1 Comparison Results In addition to the experimental results elaborated in Section 4.2, we employ box plots comparing the DSC and NSD metrics on each dataset to provide complementary evidence about the performances of other SOTA methods and our CRISP-SAM2. As illustrated in Figure 7, our CRISP-SAM2 significantly outperforms other counterparts. In summary, the medians of our method, which is denoted by the dashed lines in each plot, exceed those of other methods in twelve of these fourteen charts. Besides, the size of the rectangle of the box plot embodies the degree of disaggregation of the prediction metrics and segmentation stability of the models. Our CRISP-SAM2 has the smallest rectangles in all plots, demonstrating stable performance on various samples and better robustness. Together with the average values listed in Table 1, the box plots verify our superior performance in terms of the metrics of segmentation effectiveness. To offer more intuitive evidence, we visualize more detailed predicted segmentation results of CT-SAM3D [18], CAT [25], SegVol [13], ZePT [29] and our CRISP-SAM2. And we provide the box plots in Figure 8 for the average values of each organ on the seven selected datasets, showing segmentation performance differences between organs because of their unique characteristics. As depicted in Figure 9, the detailed visualization zoom-in intuitively exhibited that our segmentation results are more closely aligned to the ground truths, especially the more accurate local details and boundaries. CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland Table 8: Experimental results of unseen labels based on SOTA counterparts and our method. Table 9: Experiments on the influences of length, quality, type and the linguistic consistency of texts. Method Unseen 1 Unseen 2 DSC NSD DSC NSD DSC NSD Unseen 3 Abdomen DSC NSD DSC NSD AMOS SAM2 [63] 75.41 74.46 75.85 74.36 MedSAM-2 [103] 77.87 78.11 76.41 76.99 CT-SAM3D [18] 73.30 75.63 74.12 76.48 69.47 67.20 69.11 68.37 71.28 72.33 77.70 79.34 81.87 83.81 73.93 75.32 SegVol [13] CAT [25] ZePT [29] CRISP-SAM2 76.60 77.09 77.06 79.84 77.16 78.51 79.64 81.72 78.12 80.38 80.70 83. 70.32 72.29 70.94 73.04 72.51 75.19 E.2 Zero-Shot Generalization Performance Since the powerful zero-shot generalization ability of SAM2, which could be further enhanced by geometric prompts, we conduct experiments to compare its zero-shot generalization performance with our model. We choose colon, intestine, and rectum in WORD [49], which are not included in training, as \"Unseen\" labels 1 to 3 respectively for evaluation. For models requiring geometric prompts, i.e. SAM2 [63], MedSAM-2 [103], CT-SAM3D [18] and SegVol [13], we provide points and bboxs, following the previous experiments. As listed in Table 8, it could be observed that these geometricprompted models have smaller performance reduction when segmenting unseen targets than those without geometric prompts, namely CAT [25], ZePT [29] and our model. It is mainly because that geometric prompts are more direct and spatially precise compared to our text-based prompting strategy, requiring exploring inherent relationships between vision and text. While CRISP-SAM2 slightly falls behind SegVol by 0.72% on the average value of DSC, it has lead of 0.22% on NSD evenly when facing the three unseen organs. Compared to baseline SAM2, our method surpasses it 3.53% and 7.70% on DSC and NSD respectively. These results demonstrate that our CRISP-SAM2, which utilizes cross-modal interaction and semantic prompting strategies, is capable of generalization performance. E.3 Analysis of Textual Inputs To better discuss the influence of different textual inputs on segmentation performance of our model, we design comprehensive experiments, assessing the influences of length, quality, type, and linguistic consistency between training and testing data. To evaluate influences of the length of texts, we set three groups of different lengths: only label, including solely the organ names; short, which contains only one sentences; long, following the original design of our model. For assess the impact of quality, we compare the performance of irrelevant descriptions from other organs and mix them with relevant texts in equal parts as another control group. In terms of the types of content, we separate the four main descriptive types, namely relative and absolute position, shape, size and function, to appraise contributions of specific types. Also, we conduct experiments on inconsistent textual inputs between training and testing, and the texts of control group are generated by models Llama-3 [17] other than GPT-4o [26]. Length Quality Type Only Label Short Long (Ours) 91.25 95.32 85.91 88.94 91.13 89.57 92.28 96.40 86.88 90.74 81.59 85.40 Irrelevant Mixed 84.47 89.10 Relevant (Ours) 92.28 96.40 86.88 90. 87.27 91.95 80.56 85.86 90.60 95.77 Position Shape Size Function Mixed (Ours) 95.85 95.87 94.82 94.96 89.86 91.74 89.40 91.02 88.87 90.79 90.91 89.38 92.28 96.40 86.88 90. 85.75 84.91 84.73 85.19 Linguistic Consistency (cid:37) (cid:34)(Ours) 95.93 90.51 92.04 92.28 96.40 86.88 90.74 86. Figure 5: Visualizations of texts of various lengths on (a) AbdomenCT-1k [52] and (b) AMOS22 [28] datasets, demonstrating the impacts of textual information. The experimental results of these factors are demonstrated in Table 9. First, the length of input texts has noticeable impact on performance with positive correlation between them, which could also be verified in Figure 5. When we limit the textual length to only one sentence, it exhibits reduction of 1.32% on DSC and 1.13% on NSD, and this decrease further expands to 4.32% and 4.99% respectively when the input is limited to only organ names due to exceptionally scarce semantics. Second, poor quality of texts with irrelevant information also restricts segmentation performance. Inputting completely irrelevant information decreases both DCS and NSD by over 5.5% and 6.0%, however, the influence of mixing irrelevant texts with relevant ones is relatively limited, showing excellent robustness. Third, certain types of textual information contribute differently to segmentation. For instance, the positional information benefits the most, while the descriptions of size contribute the least. And including texts about the shape and function of organs brings more improvements on NSD compared to DSC, resulting in more accurate boundaries and details. Notably, incorporating any of these four main textual types offers richer semantics than only providing labels to guide the segmentation, and utilizing the Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Table 10: Comparisons with baselines on the computational cost and inference efficiency. FPS: frames per second. Param.: parameters of the model. FLOPs: floating point operations. Failures when textual descriptions mismatching visual inputs: some organs sometimes shift due to lesions or congenital abnormalities as well as when images are flipped. We leave these limitations to be tackled in our future works. SAM-L [31] SAM2 [63] SAM2 [63] CRISP-SAM2 (Hiera-B+) (Hiera-B+) (Hiera-L) Param. 312M FLOPs FPS 2690 4.2 81M 560 36.5 224M 1490 24. 81M + 196M = 277M 1620 20.9 mixture of various textual types as input achieves the best performance. Last, the inconsistency of the textual during training and inference periods causes fairly small reductions on the two metrics, which are 0.43% and 0.35% on DSC and NSD respectively, verifying great generalization ability and robustness of our model. To thoroughly evaluate the impact of textual length, we conduct qualitative experiments using texts of various levels of length. As illustrated in Figure 5, the inclusion of more comprehensive textual information enables our model to generate more precise segmentation masks. This is particularly evident in handling irregularly shaped boundaries and small organs, as highlighted in the dashed boxes of panels (a) and (b), emphasizing the significance of textual information in enhancing segmentation accuracy. E.4 Computational Cost and Efficiency In order to obtain more intuitive comparisons, we compare our CRISP-SAM2 with three baselines: SAM-L [31] and SAM2 with Hiera-L backbones [63], which are selected in comparative experiments, and SAM2 with Hiera-B+ backbones, which is much more lightweight. As illustrated in Table 10, our model has 23.7% more parameters than SAM2 using Hiera-L, because of the incorporation of extra modules, e.g. cross-modal semantic interaction, whose additional parameters are mainly from the pretrained text encoder and image encoder. Compared to the encoders and the Hiera architecture, the cross-attention layers or other operations bring limited parameters, which are less than 15% in total. However, the FLOPs only increases 8.7% to 1620G, enabling to train our model on the same GPU limitations with SAM2 (Hiera-L). Besides, the speed during inference decreases 13.3%, remaining high inference efficiency, especially compared with SAM based methods. Limitations Although our proposed CRISP-SAM2 has achieved SOTA performance on various datasets, there is still limitations to be solved: (1) Relatively limited data: although we conduct experiments on seven public datasets, our total training and testing samples are still insufficient compared to works like SegVol [13]. Besides, the richness and numbers of sentences in our textual descriptions could be further expanded. (2) Not outstanding enough zero-shot generalization: although our model still achieve superior performance of segmenting unseen targets, the leading advantage has been reduced, compared to supervised segmentation. (3) Relatively high computational cost and low inference speed: our model has slightly larger parameters, and vaguely slower speed during inference. (4) CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland Figure 6: Illustrations of representative samples of seven selected sub-datasets in the joint datasets. Each sample includes 3D imaging and corresponding descriptive texts for each organ to be segmented. Conference acronym MM, October 2731, 2025, Dublin, Ireland Xinlei Yu et al. Figure 7: Box plots for comparing experiments of our CRISP-SAM2 and other SOTA methods including DSC and NSD metrics. For intuitive comparison, dashed line is added at the median of our method. Figure 8: Box plot for the experimental results on DSC and NSD metrics, taking the average values of each organ on the seven selected datasets. CRISP-SAM2 : SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation Conference acronym MM, October 2731, 2025, Dublin, Ireland Figure 9: Visualization details of CT-SAM3D [18], CAT [25], SegVol [13], ZePT [29] and our CRISP-SAM2 on MSD-Spleen [68], Pancreas-CT [12], AbdomenCT-1k [52] and AMOS22 [28] datasets."
        }
    ],
    "affiliations": [
        "Hangzhou Dianzi University, Hangzhou, China",
        "Shenzhen Research Institute of Big Data, Shenzhen, China",
        "Shenzhen University, Shenzhen, China",
        "Zhejiang University, Hangzhou, China"
    ]
}