{
    "paper_title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
    "authors": [
        "Hongyu Wang",
        "Shuming Ma",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference."
        },
        {
            "title": "Start",
            "content": "BitNet a4.8: 4-bit Activations for 1-bit LLMs Hongyu Wang Shuming Ma Furu Wei https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58 [MWM+24], presents promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference. 4 2 0 2 7 ] . [ 1 5 6 9 4 0 . 1 1 4 2 : r Figure 1: The overview of BitNet a4.8 with both weight and activation quantization. All the parameters are ternery (i.e., 1.58-bit as in BitNet b1.58 [MWM+24]). We use hybrid quantization and sparsification strategy to deal with outlier activations in certain Transformer sub-layers. Equal contribution. Corresponding author. S. Ma and F. Wei are with Microsoft Research. H. Wang is with University of Chinese Academy of Sciences."
        },
        {
            "title": "Introduction",
            "content": "Recent works [MWM+24] have shown that 1-bit LLMs can match the full-precision models given the same amount of parameters and training tokens while being significantly cost-effective in terms of latency, memory, throughput, and energy consumption. With model weights represented in 1.58-bit (i.e., {-1, 0, 1}), the bottleneck of inference has shifted from the limited memory bandwidth to high computational cost. Low-bit or sparse activations in LLMs served as promising approach to further reduce the computational budget while maintaining performance on downstream tasks. One common approach is to utilize activation sparsity [LWD+23, SXZ+24, LPC+24], which reduces the inference FLOPs and the I/O of weight by pruning the activation entries with smaller magnitudes. Sparsification is particularly well-suited for handling activations that exhibit highly imbalanced long-tailed distributions. Recent works [WMWW24] have demonstrated that LLMs with fully sparsely-activated activations can achieve results comparable to dense models while having much less active parameters. In addition to sparsification, activation quantization is another approach to accelerate the matrix multiplication. However, the optimization of neural networks with low-bit activations is challenging due to the emergence of outlier dimensions as the training progresses and the model size grows. Despite these outliers only account for very small portion of the activations [DLBZ22, XLS+23], they have much larger magnitude, which leads to significant quantization errors and performance degradation on downstream tasks. Previous works [XLCZ23, AMC+24, LZF+24, LXW+24] mostly utilize Hadamard or learnable rotary transformation to amortize the outlier features into other entries. However, they are mostly designed for the LLMs of higher precision (e.g., 4-bit). For 1-bit LLMs, the extremely low bit-width of the weights makes it challenging to absorb these transformation matrices directly into the weights, while leaving them as online transformations introduces additional computational overhead and limits overall inference performance. In this work, we introduce BitNet a4.8, hybrid quantization and sparsification strategy that enables 4-bit activations for 1-bit LLMs. By carefully analyzing the activation distribution of 1-bit LLMs, we selectively apply 4-bit quantization or sparsification based on the distribution patterns of these activations. Specifically, as shown in Figure 1, BitNet a4.8 employs 4-bit activations for the inputs to attention and FFN, while utilizing sparsification with 8 bits for intermediate states. To improve the training efficiency, BitNet a4.8 is trained from 8-bit to 4-bit activations with two-stage recipe, which requires only few training tokens to adapt BitNet b1.58 to the low-bit activations at the end of training. Extensive experiments demonstrate that BitNet a4.8 achieves competitive performance to BitNet b1.58 with the same training cost while being significantly more efficient at inference time. Furthermore, BitNet a4.8 has only 55% activated parameters and supports 3-bit KV cache, which further enhances the efficiency of LLM deployment."
        },
        {
            "title": "2 BitNet a4.8",
            "content": "2.1 Architecture As shown in Figure 1, BitNet a4.8 adopts the same layout as BitNet b1.58. Following [WMD+23, MWM+24], we replace the linear projections in both attention and feed-forward network (FFN) with BitLinear to learn 1.58-bit weights from the scratch. For activations, we adopt hybrid quantization and sparsification strategy to mitigate the error introduced by outlier dimensions. Figure 2 illustrates the distribution of each components inputs of BitNet b1.58 model with 7B model size. Inputs to the attention and FFN layers typically follow Gaussian-like distribution, while activations before the FFN down projections and the output projections in attention have more outlier channels and massive amount of entries around zero. [LPC+24] also reported similar observations for full-precision LLMs. As shown in Figure 3, directly applying low-bit quantization to these intermediate states introduces substantial quantization errors. Therefore, we use the sparsification method from Q-Sparse [WMWW24] to retain these intermediate states at 8 bits while removing the computation bottleneck. For output projection of self-attention layers, we use sparsify-then-quantize function: = (QINT8(X) M) Qw(W)T , = Topk(X) (1) 2 Figure 2: The distribution of the inputs to each projection. The visualization is conducted with 7B BitNet b1.58 model on subset of the valid set of C4. For the layers that exhibit Gaussian-like distributions, we employ 4-bit activation quantization. For the layers which distributions are sharp, we adopt Q-Sparse [WMWW24] to perform sparsification on the activations. where Qw() and QINT8() denote the quantization function for weight and activations X, respectively. is the mask tensor that indicates the maximum top-K elements in terms of the absolute values of the activations X, is the element-wise multiplication operation. Specifically, the functions of weight quantization and activation quantization can be formulated as: Qw(W) = αRoundClip( α + ϵ , 1, 1), α = mean(W) QINT8(X) = γ 127 RoundClip( 127 γ + ϵ X, 128, 127), γ = max(X) RoundClip(X, a, b) = min(max(round(X), a), b) (2) (3) (4) For FFN, we adopt squared ReLU [SML+21, WMWW24] and gated linear unit (GLU) to further boost the activation sparsity. It is defined as follows: ReLU2GLU(X) = XWT up ReLU2(XWT gate) (5) According to our preliminary experiments, with squared ReLU, the inputs to the down projection achieve over 80% sparsity with minimal impact on performance. Additionally, we observe that the outputs of gate projection ReLU2(XWT gate) exhibit high activation sparsity as well (e.g., 67.5% for 7B models). This characteristic enables further reduction in inference FLOPs for the up projection by first computing the gate projection and then performing the up projection only on the non-zero channels of the gates. For the input to attention and FFN, since they have much less outlier features, we use absmean function to quantize the activations to 4-bit integers: = QINT4(X) Qw(W)T β QINT4(X) = RoundClip( X, 8, 7), β = mean(X) (6) (7) 7 β + ϵ 3 Figure 3: The distribution of the inputs to the output projection of attention with different quantization and sparsification. The visualization is conducted with 7B BitNet b1.58 model on subset of the valid set of C4. 2.2 Training Continue-training from BitNet b1.58. BitNet a4.8 is trained with two-stage recipe from W1.58A8 to W1.58A4. For the first stage, we train the model with 8-bit activations and ReLU2GLU. For the second stage, we adopt the hybrid quantization and sparsification as shown in Section 2.1. BitNet a4.8 quickly adapts to 4-bit and sparse activations with only few training tokens while having negligible loss on performance. Gradient approximation. Following [WMD+23, WMWW24], we use straight-through estimator (STE) [BLC13] to conduct the gradient approximation for BitNet a4.8, as well as mixed precision training to update the parameters. We directly bypass the non-differentiable functions, including the quantization function and top-K sparsification function during the backward propagation. For mixed precision training, we maintain full-precision latent weight to accumulate parameter updates. During the forward, we quantize the latent weight into 1.58-bit on the fly. 2.3 Floating-point quantization Floating-point quantization offers broader dynamic range than the integer-based quantization, which is crucial for handling the long-tailed distribution of the activations. For floating-point precision, we only leave the inputs to down projection of FFN at 8-bit integers, and quantize the other activations to FP4 using MinMax quantizer [LLH+23]. It is defined as follows: QFP4(X) = γ 2M +b Round( 2M +b γ = log2( 2 2M Xmax ) + 2E 1 X), γ = 2max(log2 X+b,1) (8) (9) where and denote the bit-width of the exponent and mantissa component, respectively. We adopt the E2M1 format due to its larger dynamic range. As shown in Table 1, BitNet a4.8 with FP4 4 Models Size PPL ARCc ARCe HS PQ WGe Avg LLaMA LLM BitNet b1.58 BitNet a4.8 (FP4) BitNet a4.8 LLaMA LLM BitNet b1.58 BitNet a4.8 (FP4) BitNet a4.8 LLaMA LLM BitNet b1.58 BitNet a4.8 (FP4) BitNet a4.8 LLaMA LLM BitNet b1.58 BitNet a4.8 (FP4) BitNet a4.8 700M 1.3B 3B 7B 11.44 12.32 12.40 12.40 10.82 11.27 11.38 11.35 9.61 9.97 9.99 9.97 9.20 9.24 9.42 9. 27.13 25.00 25.17 25.17 27.90 27.65 28.50 28.50 29.95 29.27 29.10 28.33 33.36 32.00 31.57 31.66 43.27 42.68 42.68 41.58 45.16 45.33 44.36 44. 48.11 49.41 49.24 49.58 51.22 50.88 51.22 50.88 44.70 42.08 42.36 42.44 47.65 46.86 47.03 46.98 55.25 54.42 54.60 54.62 58.33 59.79 58.20 58. 68.12 66.97 66.27 66.38 69.91 68.39 68.61 68.34 71.76 70.89 71.38 71.16 73.34 72.96 72.47 73.01 53.99 54.14 52.96 53.04 53.35 54.06 54.06 54. 57.46 57.54 56.12 54.38 58.41 59.83 59.59 59.35 47.44 46.17 45.89 45.72 48.79 48.46 48.51 48.42 52.51 52.30 52.08 51.61 54.93 55.09 54.61 54. Table 1: Perplexity and results of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the end tasks. The standard variance of error for average scores is 1.06%. quantization has the similar performance as it with the hybrid quantization and sparsification strategy based on integers."
        },
        {
            "title": "3 Experiments",
            "content": "We compared BitNet a4.8 to BitNet b1.58 and our reproduced FP16 LLaMA LLM of various sizes. For 1.58-bit models, we adopted the two-stage weight decay and learning rate scheduling following the training recipe of BitNet b1.58 [MWM+24]. More details can be found in the Appendix A. All models were trained with 100B tokens from the RedPajama dataset [Com23] to ensure fair comparison. For BitNet a4.8, we first train the model with 8-bit activations for 95B tokens. Then we reuse the optimizer states and continue-train the model with the proposed hybrid quantization and sparsification for 5B tokens. We set topK as 50% for the output projection of attention. We evaluated the zero-shot accuracy for these models on range of language tasks using the lm-evaluation-harness toolkit [GTA+24], including ARC-Easy (ARCe) [YBS19], ARCChallenge (ARCc) [YBS19], Hellaswag (HS) [ZHB+19], Winogrande (WGe) [SBBC20] and PIQA (PQ) [BZB+19]. We also reported the perplexity on the validation set of C4 [RSR+19] dataset. 3.1 Main Results Table 1 summarizes the detailed results of BitNet a4.8, BitNet b1.58 and FP16 LLaMA LLM. The performance gap between full-precision (i.e., FP16) LLaMA LLM and BitNet b1.58 narrows as the model size grows. For 7B models, BitNet b1.58 matches LLaMA LLM in terms of both language model perplexity and average accuracy on the end tasks. Furthermore, BitNet a4.8 achieves performance comparable to BitNet b1.58, with almost no loss in average accuracy. Sparsity. Table 2 demonstrates detailed sparsity of each component for BitNet a4.8, BitNet b1.58 and FP16 LLaMA LLM across various sizes. The sparsity is calculated with non-embedding parameters on the valid set of C4. Notably, BitNet a4.8 achieves significantly higher sparsity than both BitNet b1.58 and LLaMA LLM. For example, in the 7B model, BitNet a4.8 reaches an overall sparsity of 44.5%, with only 3.4B active parameters. The inputs to the down projection demonstrate particularly high sparsity, consistent with our observation that intermediate state distributions are sharply centered around zero. Additionally, we also observe that the outputs of gate projection are very sparse. It leads to high sparsity for up projection, since we only need to perform projection on the non-zero channels selected from the gates. Specifically, for the 7B BitNet a4.8, the sparsity of the Models Activated QKV Out Up Gate Down Overall LLaMA LLM BitNet b1.58 BitNet a4.8 LLaMA LLM BitNet b1.58 BitNet a4.8 LLaMA LLM BitNet b1.58 BitNet a4. LLaMA LLM BitNet b1.58 BitNet a4.8 679M 638M 390M 1.2B 1.1B 0.7B 3.2B 3.0B 1.8B 6.5B 6.0B 3.4B 0.0 1.2 12. 0.0 1.3 12.0 0.0 1.4 12.1 0.0 1.7 12.1 0.0 5.9 50.0 0.0 5.8 50.0 0.0 7.1 50. 0.0 11.2 50.0 0.0 1.2 66.2 0.0 1.2 65.9 0.0 1.3 70.7 0.0 1.4 71.4 0.0 1.2 12. 0.0 1.2 12.1 0.0 1.3 12.1 0.0 1.4 12.0 0.0 21.8 80.9 0.0 22.8 81.8 0.0 30.0 85. 0.0 24.2 84.2 0.0 6.2 42.5 0.0 6.4 42.7 0.0 8.2 44.7 0.0 7.3 44.5 Table 2: Detailed sparsity of BitNet a4.8, BitNet b1.58 and LLaMA LLM on the valid set of C4. Models Size ARCc ARCe HS PQ WGe Avg BitNet a4.8 w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV BitNet a4.8 w/ 4-bit KV w/ 4-bit QKV w/ 4-bit Q, 3-bit KV 3B 7B 28.33 28.24 27.30 28.84 31.66 31.40 30.63 31.14 49.58 48.86 48.91 48.91 50.88 50.93 51.30 50.93 54.62 54.41 54.32 53. 58.78 58.68 58.45 58.07 71.16 71.87 71.98 70.95 73.01 73.12 72.52 72.96 54.38 55.49 56.75 56.35 59.35 60.85 59.83 59.04 51.61 51.77 51.85 51. 54.74 55.00 54.55 54.43 Table 3: Detailed results of BitNet a4.8 with QKV states varying bit-widths on the end tasks. We reported the zero-shot accuracy of all models. gates and the inputs to up projection is 67.5% and 12.0%, respectively. Consequently, the sparsity of up projection can be estimated as 1 (1 12.0%) (1 67.5%), that is 71.4%. Low-bit Attention. Table 3 presented detailed results of BitNet a4.8 with low-bit attention in 3B and 7B model size. Low-bit attention is essential for efficient long sequence modeling, as it reduces the memory footprint and IO of KV cache and accelerates the attention computation. In our experiments, we adopted post-RoPE quantization. The QKV heads were directly quantized to unsigned integers using the absmax function, without the need of any calibration dataset. For 3-bit KV quantization, we retain the heads of the bos token at 4-bit, as it contains more outlier features. As shown in Table 3, BitNet a4.8 achieves negligible accuracy loss with 4-bit KV or QKV heads in 3B and 7B models. Furthermore, the KV cache of BitNet a4.8 can be quantized to 3-bit integers, resulting in almost no degradation on average accuracy. 3.2 Ablation Study Hybrid architecture. Figure 4 presented the training loss curve of 700M BitNet a4.8 with the full INT4/FP4 quantization, and the hybrid quantization and sparsification. We train these models with the first-stage scheduling for 25B tokens from the RedPajama dataset. We adopt absmean and MinMax quantizer for full INT4 and FP4 quantization, respectively. Besides, for full INT4 quantization, we use absmean quantizer with β = 2mean(X) for down projection in FFN, as its inputs have larger outliers. As shown in Figure 4, the full INT4 quantization leads to divergence. Furthermore, the hybrid architecture significantly outperforms the full FP4 architecture in terms of training perplexity. Down projection of FFN. We compared 1.3B BitNet a4.8 with different quantization or activation function for the down projection of FFN. All models were trained with the first-stage scheduling for 50B tokens from the RedPajama dataset. To ensure fair comparison, we leave the other activations 6 Figure 4: Ablation study on the hybrid quantization and sparsification. Figure 5: Ablation study on different quantization or activation function for the inputs to down projection of FFN. at 8-bits. We adopt the absmax quantizer for INT8 quantization and MinMax quantizer for FP4 quantization. The β of absmean quantizer is set as 2mean(X). Figure 5 shows the training loss curves of these models. Squared ReLU achieves slightly better training perplexity than Swish while enabling higher sparsity. Furthermore, applying FP4 quantization for the inputs to the down projection leads to significant performance degradation, while using INT4 activations with STE causes divergence. Output projection of attention. Table 4 demonstrates detailed results of 3B BitNet a4.8 with and without Top-K sparsification for the inputs to the output projection of attention. Both models were trained with the same two stage recipe from 8-bit to 4-bit activations. We set as 50% for sparsification. The baseline utilized the INT8 absmax quantizer for the output projections inputs. The results show that TopK sparsification brings negligible perplexity and accuracy loss. 4-bit quantization. We presented the loss curves of 3B BitNet a4.8 with different 4-bit quantizers for the inputs to the attention and FFN. We compared the performance of BitNet a4.8 with floating-point quantization with E2M1 and E1M2 formats using MinMax quantizer, integer quantization with absmax and absmean quantizer. As shown in Figure 6, FP4 with E2M1 format and INT4 with absmean quantizer achieve slightly better training perplexity, as they are wellsuited for handling small-magnitude activation entries. 7 Figure 6: Ablations on 4-bit quantizers for the inputs to attention and FFN. Quantization Sparsification PPL ARCc ARCe HS PQ WGe Avg INT8 INT8 - TopK 50% 9.95 9.97 28.33 28.33 48.53 49. 54.90 54.62 72.31 71.16 56.51 54.38 52.11 51.61 Table 4: Ablations on the TopK sparsification for the inputs to the output projection of attention. Models HS PQ WGe OBQA Lambada MMLU ARCc ARCe Avg BitNet b1.58 2B 68.66 BitNet a4.8 2B 68.21 77.09 76.55 62.58 64. 41.40 40.60 63.36 63.75 50.29 50.30 47.61 46.59 70.74 70.00 60.22 60. Table 5: Results of BitNet a4.8 and BitNet b1.58 with 2B parameters and 2T training tokens. 3.3 More Training Tokens Prior research [DLBZ22] demonstrates positive correlation between the number of training tokens and the prevalence of activation outliers in language models. To rigorously evaluate the scalability characteristics of BitNet a4.8, we conducted extensive experiments using model configuration with 2 billion parameters trained on 2 trillion tokens. We performed controlled comparison against BitNet b1.58 using identical training data and configurations. The empirical results, presented in Table 5, demonstrate that BitNet a4.8 maintains performance parity with negligible degradation in accuracy metrics while achieving 4-bit activation compression. These findings provide strong evidence for the efficacy of our proposed approach at scale."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we present BitNet a4.8 which enables 4-bit activations for 1-bit LLMs. BitNet a4.8 uses novel hybrid quantization and sparsification architecture to reduce the quantization errors introduce by outlier channels of activations. Specifically, we employ 4-bit quantization for inputs to the attention and FFN layers, while sparsifying the intermediate states with 8-bit integers. BitNet a4.8 is continue-trained from W1.58A8 to W1.58A4. Experimental results demonstrate that BitNet a4.8 achieves results comparable to BitNet b1.58 with the same training cost, while significantly enhancing inference efficiency."
        },
        {
            "title": "5 Acknowledgements",
            "content": "We would like to acknowledge Lei Wang for the discussion on the inference efficiency."
        },
        {
            "title": "References",
            "content": "[AMC+24] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. CoRR, abs/2404.00456, 2024. [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019. [Com23] Together Computer. Redpajama: an open dataset for training large language models, 2023. [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, 2022. 8 [GTA+24] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [LLH+23] Shih-Yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. LLM-FP4: 4-bit floating-point quantized transformers. In EMNLP 2023, pages 592 605. Association for Computational Linguistics, 2023. [LPC+24] James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, and Ben Athiwaratkun. Training-free activation sparsity in large language models. CoRR, abs/2408.14690, 2024. [LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2213722176. PMLR, 2023. [LXW+24] Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. arXiv preprint arXiv:2406.01721, 2024. [LZF+24] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization with learned rotations. CoRR, abs/2405.16406, 2024. [MWM+24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. CoRR, abs/2402.17764, 2024. [RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. CoRR, abs/1910.10683, 2019. [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 87328740, 2020. [SML+21] David R. So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling. CoRR, abs/2109.08668, 2021. [SXZ+24] Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, and Haibo Chen. Turbo sparse: Achieving llm sota performance with minimal activated parameters. arXiv preprint arXiv:2406.05955, 2024. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. CoRR, abs/2310.11453, 2023. [WMWW24] Hongyu Wang, Shuming Ma, Ruiping Wang, and Furu Wei. Q-sparse: All large language models can be fully sparsely-activated. CoRR, abs/2407.10969, 2024. [XLCZ23] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training transformers with 4-bit integers. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems, 2023. [XLS+23] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023. [YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, EMNLP-IJCNLP, 2019. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: can machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 47914800, 2019. Hyper-parameters Size Hidden Size GLU Size #Heads #Layers Batch Size # Tokens Seq Length 700M 1.3B 3B 7B 1536 2048 3200 4096 4096 5460 8640 11008 24 32 32 24 24 26 32 1M 1M 1M 1M 100B 100B 100B 100B 2048 2048 2048 2048 Table 6: Model configurations for both BitNet a4.8, BitNet b1.58 and LLaMA LLM. Model BitNet a4.8 LLaMA LLM Learning Rate Size 700M 1.5 103 1 103 1.2 103 8 104 1.3B 1.2 103 6.4 104 3B 1 103 6 104 7B 2.5 104 2.0 104 2.0 104 1.5 104 700M 1.3B 3B 7B Weight Decay Warm-up Adam β 0.1 0 0.1 0 0.1 0 0.1 0 0.1 0.1 0.1 0.1 375 375 375 375 375 375 375 375 (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) (0.9, 0.95) Table 7: Hyper-parameters for both BitNet a4.8 and LLaMA LLM training."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of Chinese Academy of Sciences"
    ]
}