{
    "paper_title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models",
    "authors": [
        "Ruikang Liu",
        "Yuxuan Sun",
        "Manyi Zhang",
        "Haoli Bai",
        "Xianzhi Yu",
        "Tiezheng Yu",
        "Chun Yuan",
        "Lu Hou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 2 8 4 0 . 4 0 5 2 : r Preprint. Under review. Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai#, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou# Shenzhen International Graduate School, Tsinghua University Huawei Noahs Ark Lab {baihaoli,houlu3}@huawei.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-ofthought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models. Figure 1: Performance of the quantized DeepSeek-R1-Distill-Qwen models on five benchmarks: AIME-120, MATH-500, GSM8K, GPQA-Diamond, and LiveCodeBench. Each chart presents the performance of different quantization strategies compared to the BF16 baseline."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models (LLMs) (Jaech et al., 2024; Team et al., 2025; Guo et al., 2025; Team, 2025) trained with enhanced reasoning abilities demonstrate strong performance on Equal contribution; #: Corresponding authors. 1 Preprint. Under review. complex reasoning tasks, such as multi-disciplinary question answering (Rein et al., 2024) and mathematical competitions (Maxwell-Jia, 2025). However, their improved performance increases inference overhead from prolonged reasoning processes, often causing them to overthink simple tasks and generate outputs hundreds of times longer than non-reasoning models (Chen et al., 2024). To promote efficient LLM inference, various compression and acceleration techniques have been proposed, including quantization (Frantar et al., 2022; Xiao et al., 2023; Liu et al., 2024), pruning (Frantar & Alistarh, 2023; Sun et al.; Zhang et al., 2024), and knowledge distillation (Gu et al., 2023; Liu et al., 2023; Muralidharan et al., 2024). Among these methods, quantization, which reduces the numerical precision of weights or activations, is particularly favored due to the good balance between model size and performance preservation. Nonetheless, most existing quantization methods are proposed for nonreasoning LLMs (Li et al., 2025), and their performance on reasoning models remains under-explored. Intuitively, quantization may pose greater risks in this context, as reasoning models typically involve long chain-of-thought (CoT) outputs (Wei et al., 2022), which are more prone to quantization error accumulation over the sequence. In this study, we provide comprehensive empirical study on the quantization of reasoning models, as outlined in Figure 1. We evaluate state-of-the-art quantization methods across weight-only, weight-activation, and KV cache quantization. The reasoning models for evaluation span form 1.5B to 70B parameters, such as the series of the open-source DeepSeekR1-Qwen-Distill (Yang et al., 2024; Guo et al., 2025), DeepSeek-R1-LLaMA-Distill (AI@Meta, 2024; Guo et al., 2025) and QwQ (Team, 2025), with prevalent reasoning benchmarks such as AIME-120 (Maxwell-Jia, 2025), MATH-500 (Lightman et al., 2023), GSM8K (Cobbe et al., 2021). GPQA-Diamond (Rein et al., 2024) and LiveCodeBench (Jain et al., 2024). The key findings of this study are summarized below: 1. Lossless Quantization (3.2) : 8-bit weight-activation quantization preserves accuracy across tasks and model sizes, while 4-bit weight-only quantization or KV cache quantization also achieves near-lossless results (1% drop). 2. Quantization Algorithm (3.3) : Among the evaluated quantization algorithms, we suggest AWQ for weight-only quantization and QuaRot for KV cache quantization. FlatQuant is leading for weight-activation quantization. 3. Impact of Task Difficulty (3.4) : Harder tasks (e.g., AIME-120) suffer up to 4 greater degradation than simpler ones (e.g., GSM8K). 4. Model Origins Matter (3.5) : Distillation-based reasoning models withstand quantization better than RL-based counterparts (e.g., QwQ), while Qwen and LLaMA families exhibit different tolerance to quantization. 5. Analysis of Output Length (4.1) : Quantized models with minor performance drops do not exhibit longer reasoning steps, but aggressive low-bit quantization can lead to increased output lengths, particularly in smaller models. 6. Scaling Effects (4.2 and 4.3) : Larger quantized models achieve superior accuracylatency trade-offs compared to smaller BF16 models, and while longer reasoning steps improve performance, the gains diminish at lower rates than in BF16 models. To the best of our knowledge, this is the first comprehensive evaluation of quantization on reasoning models. We hope our research provides valuable guidance for the community toward better quantization methods for reasoning models."
        },
        {
            "title": "2 Preliminary and Related Work",
            "content": "2.1 Quantization Quantization converts high-precision values in Rmn to lower-precision representations. In this study, we focus mainly on hardware-efficient uniform quantization, where the b-bit 2 Preprint. Under review. quantization function Q( ; b) can be represented by ˆX = Q(X; b) = ΠΩ(b)(X/s), (1) 2b1 where = max(X)min(X) R+ is the quantization step size, Π() is the projection function that maps the value to the closest element in the set Ω(b) = {0, 1, ..., 2b 1} of b-bit integer points. The scope of this study includes the following quantization configurations, where more implementation details of these quantization algorithms can be found in Appendix A. Weight-only Quantization. Following the conventional practice (Frantar et al., 2022), we quantize the weight matrices of all linear layers in the Transformer. and leave the rest parameters in BF16. By quantizing the values of weight matrix into low-bit integers, the model size and memory access can be effectively reduced during runtime. We focus mainly on GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2023), two widely applied approaches in the research community. We adopt asymmetric quantization with group size of 128, i.e., the step size is calculated per 128 consecutive elements per channel in W. KV Cache Quantization. The KV cache temporarily stores intermediate results during autoregressive generation to improve computational efficiency. The quantization of the KV cache is particularly helpful in reducing the memory access in long sequences, i.e., reasoning with long chain-of-thought steps. We study the recently popular methods, including QuaRot (Ashkboos et al., 2024) and KVQuant (Hooper et al., 2024). For QuaRot, we adopt asymmetric per-token KV cache quantization with group size of 128. We implement KVQuant as simplified baseline, with details in Appendix A.2. KVQuant adopts static per-channel asymmetric quantization for cache and per-token asymmetric quantization for cache with group size of 128. Weight-activation Quantization. Weight-activation quantization employs low-precision arithmetic for matrix multiplications to further save computation. The evaluated algorithms include SmoothQuant (Xiao et al., 2023), QuaRot (Ashkboos et al., 2024), and FlatQuant (Sun et al., 2024). Following the convention of these methods, we apply per-channel symmetric quantization to weights and per-token asymmetric quantization to activations. We quantize all linear layers in Transformer layers, including the KV cache, using group-wise asymmetric quantization with group size of 128. We leave the queries in full precision, following Ashkboos et al. (2024). Additionally, inspired by recent progress on FP4 inference (NVIDIA, 2025), we evaluate MXFP4 (Rouhani et al., 2023) quantization, where every contiguous 32 elements share the same scale. 2.2 Related Work Families of Reasoning Models. Recent advancements in reasoning models like OpenAIs o1 (Jaech et al., 2024) and DeepSeeks R1 (Guo et al., 2025) have demonstrated the potential to solve complex tasks that require logical deduction, multi-step reasoning, and contextual understanding. These models can be broadly categorized based on how they achieve their reasoning abilities. The first category of reasoning models achieves the reasoning ability by distilling from the outputs of existing stronger models. For instance, DeepSeek-R1Distill-Qwen and DeepSeek-R1-Distill-LLaMA are distilled from 800K curated samples from DeepSeek-R1 (Guo et al., 2025). The other type of reasoning models obtain the reasoning ability via self post-training techniques like supervised finetuning (Ye et al., 2025; Muennighoff et al., 2025), reinforcement learning (RL) (Guo et al., 2025; Team, 2025), or their combinations (Team, 2025; Team et al., 2025; Guo et al., 2025). Efficient Reasoning. Despite the enhanced reasoning abilities, the long chain-of-thought reasoning process also incurs greater challenge of inference overhead due to the increased output length. To reduce the inference cost, several approaches have been proposed. For reasoning models trained with reinforcement learning, common way is to add length penalty term on the reward in addition to the accuracy (Luo et al., 2025; Aggarwal & Welleck, 2025; Team et al., 2025). Supervised fine-tuning with variable-length Chain-of-Thought 3 Preprint. Under review. (CoT) data is another effective approach. For instance, Token-Budget (Han et al., 2024) guides LLMs to complete reasoning within specified token limit, while S1 (Muennighoff et al., 2025) controls the output length through forcefully terminating the models thinking process or lengthening it by appending Wait multiple times. One can also reduce the reasoning lengths via direct performance optimization (DPO), where short and long CoT data can be curated as positive and negative data pairs (Chen et al., 2024; Team et al., 2025), or merging the long CoT model with short CoT model (Team et al., 2025). Among the above efforts, quantization is an orthogonal method to achieve efficient reasoning. We believe comprehensive empirical study on quantized reasoning models is essential to fully understand the trade-offs between reasoning efficiency and accuracy."
        },
        {
            "title": "3 Evaluation of Quantized Reasoning Models",
            "content": "3.1 Setup Evaluation Benchmarks. We evaluate the quantized models with the algorithms in 2.1 on the following reasoning benchmarks. 1) Three mathematical reasoning benchmarks sorted by their difficulty: AIME-120 which consists of 120 problems from the American Invitational Mathematics Examination (AIME) from 2022 to 2025 to minimize evaluation variations; MATH-500 (Lightman et al., 2023), benchmark that contains mix of easy and hard mathematical problems designed to test comprehensive reasoning abilities; and GSM8K (Cobbe et al., 2021), dataset composed of primary school level questions focused on basic arithmetic and algebra. 2) LiveCodeBench (Jain et al., 2024), benchmark for evaluating large language models on code generation tasks, designed to assess their ability to produce high-quality and functional code. 3) GPQA-Diamond (Rein et al., 2024), graduate-level proof question and answer benchmark that tests the ability of models to generate accurate mathematical proofs. We use Lighteval (Fourrier et al., 2023) with the vLLM (Kwon et al., 2023) backend for evaluation, with sampling temperature of 0.6 and top-p of 0.95. The maximum number of generation tokens is limited to 32,768. The Evaluated Reasoning Models. We evaluate the two categories of reasoning models as described in 2.2. For distillation-based reasoning models, we adopt the series of DeepSeek-R1-Distill-Qwen (Yang et al., 2024) with varying sizes from 1.5B, 7B, 14B and 32B, and DeepSeek-R1-Distill-LLaMA (AI@Meta, 2024), with both 8B and 70B models. For reasoning models obtained via reinforcement learning, we choose QwQ (Team, 2025), the recent performant open-source model. We study the following research questions in the next sections. RQ1 (3.2): What are the lossless quantization configurations for reasoning models? RQ2 (3.3): What are the preferred quantization algorithms on reasoning tasks? RQ3 (3.4): How does the task difficulty influence the quantized LLMs? RQ4 (3.5): What is the impact of LLM origins on quantization? 3.2 Lossless Quantization Bit-width The overall results for the DeepSeek-R1-Distill-Qwen models are summarized in Table 1. More results on QwQ-32B and DeepSeek-R1-Distill-LLaMA can be found in Appendix B.1. For ease of interpretation, we categorize the performance degradation into three classes: lossless (1%), fair (1% 3%), and risky (3%). We draw the following conclusions based on best-performing quantization algorithms, and leave the algorithm comparisons in 3.3. For weight-only quantization, 4-bit is nearly lossless, but 3-bit becomes risky. From Table 1, quantizing the weights of the distilled Qwen models to 4 bits incurs slight performance drop compared to their BF16 counterparts, i.e., 1.36% and 0.82% for the 1.5B 4 Preprint. Under review. Model Quantization BF Weight-only KV Cache Weight-Act. BF16 Weight-only KV Cache Weight-Act. BF16 Weight-only KV Cache Weight-Act. BF Weight-only KV Cache Weight-Act. 5 . 1 - Q - t - 1 - S D 7 - Q - t - 1 - S D 4 1 - Q - t - 1 - S D 2 3 - Q - t - 1 - S D W-A-KV # Bits - 4-16-16 3-16-16 16-16-4 16-16-3 8-84-4-4 - 4-16-16 3-16-16 16-16-4 16-168-8-8 4-4-4 - 4-16-16 3-16-16 16-1616-16-3 8-8-8 4-4-4 - 4-16-16 3-1616-16-4 16-16-3 8-8-8 4-4-4 Methods - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant FlatQuant MXFP4 FlatQuant - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant AIME120 21.67 23.33 23.33 3.33 10.00 20.83 0.00 9.17 0.00 17.50 21.67 23.33 0.00 0.00 10.00 45.00 43.33 50.00 30.00 35.83 40.83 0.00 36.67 0.83 40.00 50.83 0.00 25.00 55.83 52.50 55.00 47.50 52.50 56.67 55.83 45.83 51.67 60.00 57.50 55.83 25.83 35.83 47.50 61.67 61.67 53.33 51.67 57.50 61.67 60.00 53.33 59.17 60.00 58.33 62.50 35.83 50.83 57.50 MATH500 84.40 82.60 82.80 48.80 71.60 83.60 2.20 66.20 1.20 79.20 85.20 84.80 0.40 1.20 64.80 94.60 91.00 92.60 89.80 91.20 93.20 1.80 91.20 0.40 93.60 93.00 0.20 82.80 94.60 94.60 96.00 94.00 94.80 94.80 94.40 94.40 95.40 96.60 93.80 95.20 88.40 91.20 93.40 96.40 96.20 95.20 94.40 95.20 96.40 96.00 96.20 95.20 95.00 95.20 95.60 93.20 94.20 95.00 GSM8K 84.61 83.47 82.94 65.81 75.66 84.91 0.83 71.27 0.83 83.40 83.32 85.06 0.76 0.76 78.62 91.43 90.67 90.67 89.01 90.30 91.36 0.68 90.37 0.83 91.28 91.13 0.15 90.83 93.86 93.25 93.48 92.57 93.40 93.71 94.16 92.87 93.56 93.71 94.24 94.01 91.43 93.18 93.40 94.39 94.24 94.24 94.24 94.77 94.62 94.69 94.69 94.16 94.16 94.69 94.24 93.33 93.93 94.39 GPQADiamond 36.87 34.34 30.81 37.88 23.74 31.31 9.60 31.31 15.66 28.28 39.90 32.32 18.18 8.59 31.82 50.00 50.00 49.49 48.48 48.48 47.47 28.79 45.96 25.76 51.52 48.99 14.14 51.52 63.64 60.10 59.60 54.04 61.11 63.13 59.09 54.55 65.66 63.13 61.62 59.09 50.51 54.55 56.57 65.15 63.64 67.17 64.14 60.10 64.65 66.16 60.61 62.63 60.61 70.71 65.15 55.56 60.61 59.09 LiveCodeBench Avg. Drop 16.04 13.06 13.06 4.85 9.33 16.42 0.00 10.45 0.00 13.06 18.28 16.04 0.00 0.00 6.72 35.45 35.82 36.57 25.37 28.36 35.07 0.00 27.24 0.00 33.58 36.94 0.00 14.55 51.49 49.25 47.39 44.78 43.66 50.75 51.12 43.28 43.66 51.12 51.12 48.88 31.72 42.54 46.27 54.85 52.61 53.73 51.87 41.04 55.22 55.22 51.87 50.37 54.85 56.72 55.22 33.58 48.88 54.85 48.72 47.36 46.59 32.13 38.07 47.41 2.53 37.68 3.54 44.29 49.67 48.31 3.87 2.11 38.39 63.30 62.16 63.87 56.53 58.83 61.59 6.25 58.29 5.56 62.00 64.18 2.90 52.94 71.88 69.94 70.29 66.58 69.09 71.81 70.92 66.19 69.99 72.91 71.66 70.60 57.58 63.46 67.43 74.49 73.67 72.73 71.26 69.72 74.51 74.41 71.34 72.31 72.92 75.13 74.54 62.30 69.69 72.17 - -1.36 -2.13 -16.58 -10.65 -1.30 -46.19 -11.04 -45.18 -4.43 0.96 -0.41 -44.85 -46.61 -10.33 - -1.13 0.57 -6.76 -4.46 -1.71 -57.04 -5.01 -57.73 -1.30 0.88 -60.40 -10.36 - -1.94 -1.59 -5.31 -2.79 -0.07 -0.96 -5.70 -1.89 1.03 -0.23 -1.28 -14.31 -8.42 -4.46 - -0.82 -1.76 -3.23 -4.77 0.02 -0.08 -3.15 -2.19 -1.57 0.64 0.05 -12.19 -4.80 -2.33 Table 1: The overall assessment of quantized DeepSeek-R1-Distill-Qwen models on various reasoning benchmarks. QuaRot for the 7B model weight-activation quantization is skipped because of the incompatible hidden dimension for Hadamard transformation. The green, orange and red cells stand for the lossless (1%), the fair (1%-3%) and the risky (3%) respectively. Note that 1.5B and 7B models are exceptions with severe degradation for low-bit KV cache and weight-activation quantization, as will be discussed in Appendix C.2. Results on more models are available in Appendix B. and 32B models, respectively. However, when reducing to 3 bits, there is sharp drop in performance for each model, e.g., over 10% accuracy drop for the 1.5B model and 3% for the 32B model. Meanwhile, smaller reasoning models tend to suffer more from quantization, which aligns past experiences on non-reasoning models (Li et al., 2024). Similar observations can be found on other sizes and families of LLMs in Appendix B.1. 5 Preprint. Under review. For KV cache, we suggest 4-bit quantization. We find that 4-bit quantized KV cache achieves lossless performance on large models (e.g., 14B or 32B). In particular, the accuracy drop can remain smaller than 1% even on AIME-120, where the model generates lengthy output of over 10K tokens, as will be discussed in 4.1. In addition, the performance of smaller LLMs is also relatively fair, with an average drop of 1.30% on 1.5B and 1.71% on 7B, respectively. However, for more aggressive KV cache quantization, the performance incurs sharp drop, especially for smaller LLMs, i.e., smaller models (1.5B and 7B) and larger models (14B and 32B) incur over 5% and 1.5% accuracy drop when the bitwidth of KV cache quantization reduces to 3 bits, respectively. We recommend 8-bit quantization for weights, activation, and KV cache, which is lossless across various LLM sizes and reasoning tasks. It can be observed that for all reasoning models and tasks evaluated, the best-performing W8A8KV8 quantization algorithm achieves performance drop of less than 1 point. This holds true even for the smallest DeepSeekR1-Distill-Qwen-1.5B model. Nevertheless, when we apply more aggressive quantization with 4 bits, even the large 32B model incurs an accuracy drop of 2.33%. The degradation becomes significantly more pronounced for smaller LLMs, with accuracy drops exceeding 10% on the 1.5B and 7B models. Thus, improving 4-bit weight-activation quantization for reasoning models remains an open challenge for the community. 3.3 Comparisons of Quantization Algorithms AWQ is preferred over GPTQ for weight quantization, considering its comparable performance but more efficient implementation. According to Table 1, AWQ performs on par with GPTQ for both 4-bit and 3-bit quantization. For instance, it surpassed GPTQ twice and lost twice for 4-bit quantization over the evaluated models. In addition, AWQ is usually faster to implement compared with GPTQ, since it does not require the iterative update of LLM parameters. Besides, AWQ is also more robust to the choice of calibration data than GPTQ, as discussed in Appendix C.1. We thus recommend AWQ as the default algorithm for the weight-only quantization of reasoning models. QuaRot is generally preferred over KVQuant for KV cache Quantization, except for Qwen 1.5B and 7B models which have unexpected huge outliers over bias. According to Table 1, QuaRot generally outperforms KVQuant on the 14B and 32B models. For instance, on the DeepSeek-R1-Distill-Qwen-32B model, QuaRot with 3-bit quantization achieves an average performance of 72.31%, while KVQuant only reaches 71.34%. However, exceptions are observed in the 1.5B and 7B models, where KVQuant significantly surpasses QuaRot. We find that this is due to the huge outliers over biases in the key and value layers of these two models. While KVQuant can mitigate this by quantizing the output before bias, it is not applicable for QuaRot. More discussions can be found in Appendix C.2. FlatQuant is the leading algorithm for weight-activation quantization. For all 8 settings with varying model sizes and quantization configurations, FlatQuant outperforms the other algorithms in 7 cases. While FlatQuant does not exhibit clear advantages on W8A8KV8 quantization, it significantly outperforms the other algorithms on W4A4KV4. For instance, FlatQuant achieves an average performance of 72.17% on the 32B model, with performance drop of only 2.33%, significantly surpassing both MXFP4 (12.19%) and QuaRot (4.80%). Note that substantial accuracy loss still exists under W4A4KV4 quantization on 1.5B and 7B models even for FlatQuant. We analyze its reason in Appendix C.2. In the rest of this study, we adopt AWQ for weight-only quantization, QuaRot for KV cache quantization, and FlatQuant for weight-activation quantization if not otherwise specified. 3.4 Impacts of Task Difficulty Quantization is prone to fail on more difficult tasks. Based on Table 1, we compare the performance drop caused by quantization on the three math reasoning benchmarks with varying difficulty levels, and find that more difficult tasks incur more severe performance drop. Among them, AIME-120 is the most challenging task, with samples selected from American Math Competition questions. MATH-500 contains mix of easy and hard probPreprint. Under review. (a) Weight-only Quant. (b) KV cache Quant. (c) Weight-Act. Quant. (d) Weight-only Quant. (e) KV cache Quant. (f) Weight-Act. Quant. Figure 2: (a)-(c) shows the comparisons between DeepSeek-R1-Distill-Qwen-32B (SFTbased) and QwQ-32B (RL-based) on different quantization configurations. (d)-(f) shows comparisons between DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-LLaMA-8B on different quantization configurations. lems, followed by GSM8K with easy primary school-level questions. The performance degradations are in descending order among the three benchmarks. For instance, for the 32B model with W4A4KV4 quantization, the performance drops are 4.17% on AIME-120, 1.40% on MATH-500, and 0.00% on GSM8K. In addition to Table 1, we also examine the accuracy at the 5 different difficulty levels explicitly defined by MATH-500 (Lightman et al., 2023) in Appendix B.2, where the observations remain consistent. 3.5 The Impacts of the Origin of Reasoning LLMs In this section, we compare the quantization performance on reasoning models trained from different methods, with the suggested algorithms in 3.3. Specifically, we study 1. the reasoning models obtained with either distillation (i.e., DeepSeek-R1-Distill-Qwen32B) and reinforcement learning (i.e., QwQ-32B); and 2. different LLM families (i.e., LLaMA and Qwen) distilled from DeepSeek-R1. RL-based LLMs are more error-prone to quantization. Comparing DeepSeek-R1-DistillQwen-32B and QwQ-32B in Figure 2 (a)-(c), we find that QwQ-32B generally suffers greater performance drops across reasoning tasks and quantization configurations. For example, the accuracy of QwQ drops by 14.98% for W3G128 and 16.66% for KV3 on AIME-120, higher than Qwens 9.98% and 8.32%. The exception is KV cache quantization on MATH-500, where no significant damage is observed. We speculate because distilled models learn generalizable reasoning abilities from stronger reasoner model, resulting in more modular and stable representations robust to compression. In contrast, RL-based models learn highly specialized policies, making them vulnerable to representation perturbations. Qwen-7B is more robust to weight-only quantization, while LLaMA-8B is more resilient to KV cache and weight-activation quantization. To study the impact of LLM origins, we compared DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-LLaMA-8B, which are of similar size and distilled from DeepSeek-R1 with the same data. As shown in Preprint. Under review. (a) DS-R1-Distill-Qwen-1.5B. (b) DS-R1-Distill-Qwen-14B. (c) QwQ-32B. Figure 3: The lengths of generated tokens across different datasets and models. Figure 2 (d)-(f), Qwen-7B is more robust to weight-only quantization (e.g., 15.00% vs. 19.18% for W3G128 on AIME-120). However, it is more sensitive for KV cache and activation quantization (e.g., 8.33% vs. 6.67% drop for KV3 on AIME-120, and 20.00% vs. 11.67% drop for W4A4KV4 on AIME-120). Potential reasons include differences in pretraining data, fine-tuning configurations, and training dynamics. More detailed results of QwQ-32B and DeepSeek-R1-Distill-LLaMA for different quantization configurations and algorithms can be found in Appendix B.1."
        },
        {
            "title": "4 Discussions",
            "content": "In this section, we provide further analysis for quantized reasoning models, including their output length, scaling effects w.r.t both model size and latency, as well as test-time scaling. The analysis is based on the best-performing quantization algorithms according to Section 3.3. Due to limited space, we provide more analysis on the choice of calibration data in Appendix C.1, quantization of DeepSeek-R1-Distill-Qwen-1.5B and 7B model in Appendix C.2, and qualitative examples in Appendix D. 4.1 Do Quantized Reasoning Models Think Longer? Since quantization inherently reduces model capacity due to precision loss, one concern with quantized LLMs is whether quantized models need to think longer to compensate for the reduced representation power, which could prolong the end-to-end latency. We analyze the output length statistics of quantized reasoning models in Figure 3. It is found that for various quantization configurations (e.g., W4G128, W8A8KV8, and KV4), the generated output lengths closely align with those of the BF16 models. However, under more aggressive low-bit quantization (e.g., W3G128 and W4A4KV4), LLMs tend to produce longer outputs. This effect is particularly pronounced in smaller models like DeepSeek-R1Distill-Qwen-1.5B, where lower-bit quantization results in more pronounced performance degradation, as previously discussed in Table 1. While more difficult tasks tend to generate longer outputs, they do not inherently trigger longer reasoning steps in quantized LLMs. Similar observations are provided in Appendix B.3, where we analyze the output lengths across five difficulty levels on MATH-500. 4.2 Scaling Effects of Quantized Reasoning Models In this section, we study the performance change when we scale the size and latency of the quantized model. Specifically, we aim to identify cost-effective quantization configurations that optimally balance accuracy with model size or latency. 8 Preprint. Under review. (a) Model Size (GB). (b) Latency (s). (a) Qwen-1.5B. (b) Qwen-32B. Figure 4: Model size and average latency v.s. accuracy (%) on LiveCodeBench. Figure 5: Average reasoning tokens v.s. accuracy (%) on DS-R1-Distill-Qwen models. Model Size Scaling. In Figure 4(a), we present the trade-off between accuracy and model size based on the series of DeepSeek-R1-Distill Qwen and QwQ-32B. We evaluate different weight-only quantization bit-widths with AWQ (i.e., 8-bit, 4-bit, and 3-bit) against the BF16 model, and report the accuracy on LiveCodeBench. It can be found that as the model size increases, accuracy improves rapidly. Under the same model size, the large LLMs in low precisions are more accurate than small LLMs in BF16. For instance, while both the 3-bit quantized QwQ-32B and Qwen-32B are similar in size (i.e., 14.5GB) with Qwen-7B in BF16, they significantly outperform Qwen-7B by around 17%. Latency Scaling. While large models demonstrate better accuracy-size trade-off, they are inevitably slower for inference. Here we also study the scaling effect of latency w.r.t. the accuracy. The latency is measured as the end-to-end time cost over the LiveCodeBench dataset, i.e., the multiplication between the number of generated tokens with the Time Per Output Token (TPOT)1. According to Figure 4(b), 4-bit quantized models provide superior accuracylatency trade-off than the 8-bit and BF16 counterparts, which mainly benefits from increased generation speed while maintaining the same reasoning length. 4.3 Testing-time Scaling of Quantized Reasoning Models Test-time scaling has become prevalent technique for enhancing reasoning model performance (Snell et al., 2024; Muennighoff et al., 2025; Liu et al., 2025). To investigate the behavior of quantized reasoning models under test-time scaling, we adopt the method of Muennighoff et al. (2025) by controlling reasoning length through dynamic adjustment of average reasoning tokens. For suppression, early termination forces partial reasoning integration; for promotion, wait tokens extend reasoning until token budgets are met. From Figure 5, longer reasoning lengths can generally enhance the performance of each model. For example, on MATH-500, both AWQ-W4G128 and AWQ-W3G128 exhibit clear improvements with extended reasoning. However, the increasing rates of quantized LLMs are generally lower than that of BF16. We speculate that it is due to the quantization error accumulated along the sequence, which hinders effective reasoning. In addition, excessively long reasoning lengths also result in performance degradation, as also discovered in Muennighoff et al. (2025). The output of unnecessary steps could degrade overall accuracy. These findings highlight the importance of only scaling the reasoning length within an appropriate range. How to achieve consistent improvement over longer range is an open challenge."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we present the first systematic study on the quantization of reasoning models. We examine the performance of different state-of-the-art quantization methods on different reasoning models under various quantization configurations. We also recommend the lossless quantization settings like W8A8 and W4A16 quantization, as well as the corresponding preferred quantization methods. Additionally, we find that task difficulty and model origins 1For TPOT, we refer to the official latency report by Qwen: https://qwen.readthedocs.io/en/ latest/benchmark/speed benchmark.html 9 Preprint. Under review. have large impact on the quantized models performance. In particular, more difficult tasks and RL-based reasoning models incur more severe performance degradation caused by quantization. Like the BF16 models, quantized models also show consistent improvement when gradually increasing test-time compute budget over certain range, but may behave worse afterward. While our study identifies critical factors influencing performance of quantized reasoning models, the underlying mechanisms driving these effects remain incompletely understood. Future work will focus on elucidating these causal relationships while advancing systematic methods to optimize the accuracy-efficiency trade-off in reasoning systems."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL CARD.md. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Clementine Fourrier, Nathan Habib, Hynek Kydlıˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/ huggingface/lighteval. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2022. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. Preprint. Under review. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Nu- [https://huggingface.co/AI-MO/NuminaMath-1.5](https://github.com/ minamath. project-numina/aimo-progress-prize/blob/main/report/numina dataset.pdf), 2024. Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. Zhen Li, Yupeng Su, Runming Yang, Congkai Xie, Zheng Wang, Zhongwei Xie, Ngai Wong, and Hongxia Yang. Quantization meets reasoning: Exploring llm low-bit quantization degradation for mathematical reasoning. arXiv preprint arXiv:2501.03035, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact. arXiv preprint arXiv:2403.01241, 2024. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. Maxwell-Jia. Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/AIME 2024, 2025. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016. 11 Preprint. Under review. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. Advances in Neural Information Processing Systems, 37:4107641102, 2024. Openai perforboosts NVIDIA. https://developer.nvidia.com/blog/ mance openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/, 2025. programmability. blackwell nvidia triton and on ai Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485 5551, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations. Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, et al. Flatquant: Flatness matters for llm quantization. arXiv preprint arXiv:2410.09426, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Miles Williams and Nikolaos Aletras. On the impact of calibration data in post-training quantization and pruning. arXiv preprint arXiv:2311.09755, 2023. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. 12 Preprint. Under review. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plug-and-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations, 2024. 13 Preprint. Under review."
        },
        {
            "title": "A Quantization Algorithms and Implementation Details",
            "content": "Below, we briefly review the quantization algorithms implemented in this study, including weight-only quantizaiton, KV-Cache quantization, and weight-activation quantization. A.1 Weight-only Quantization GPTQ (Frantar et al., 2022) aims to construct the optimal weights ˆW that minimize the squared error between full-precision and quantized outputs given input X, i.e., arg min ˆW ˆWX WXF. Specifically, it iteratively quantizes weights and updates the remaining ones using approximate second-order information derived from the inverse Hessian matrix. This process is optimized via Cholesky decomposition to enhance numerical stability and computational efficiency. In this work, we adopt asymmetric quantization with group size of 128, i.e., the step size is calculated per 128 consecutive elements per channel in W. To mitigate the effects of domain shift as detailed in Appendix C.1, we use reasoning models to self-generate reasoning data on Numina-Math-1.5 (LI et al., 2024) dataset, and randomly sample 128 text sequences of length 2048 to construct the calibration set. AWQ (Lin et al., 2023) aims to identify and protect salient weights to improve the quantized LLMs. Since activations of LLMs are rich in outliers, AWQ employs per-channel scaling, e.g., = (X c1)(c W), where the channel-wise scaling factor Rn can be analytically derived to balance the magnitudes of input activations and weights. The β optimal can be obtained by = cα , where cX and cW are the average channel statistics of activations and weights, and α, β [0, 1] are the associated hyper-parameters. Moreover, the scaled weights can be merged together to eliminate runtime overhead. Similar to GPTQ, we adopt asymmetric quantization with group size of 128. For the calibration data, we follow Lin et al. (2023) and sample 128 text sequences of length 512 from Pile (Gao et al., 2020) dataset. A.2 KV-Cache Quantization KVQuant (Hooper et al., 2024) leverages the outlier channels in cache, proposing to quantize Cache with static per-channel quantization. To better preserve the outlier patterns of cache, it quantizes cache before applying RoPE (Su et al., 2024). Additionally, to accommodate the non-uniform distribution of the KV cache, it incorporates non-uniform and mixed-precision quantization. In this work, we skip the non-uniform and mixedprecision quantization in KVQuant and implement KVQuant as simplified baseline, employing static per-channel quantization for cache and dynamic per-token quantization for cache with group size of 128. For DeepSeek-R1-Distill-Qwen-1.5B and 7B models, we further incorporate pre-bias cache quantization to suppress the extreme outlier channels in cache, with more details in Appendix C.2. We randomly sample 128 text sequences of length 512 from Pile (Gao et al., 2020) dataset as the calibration set. QuaRot (Ashkboos et al., 2024) can be applied for both KV cache quantization and weight-activation quantization. It enables end-to-end 4-bit inference by using Hadamard transformations to eliminate outliers in activations and KV caches. Based on the orthogonality of Hadamard matrices (i.e., HH = I), the output of quantized linear layer is ˆY = Q(XH) Q(HW), where the transformed weight WH can be pre-processed offline to reduce additional runtime overhead. It also integrates GPTQ (Frantar et al., 2022) to reduce quantization error on weights. For KV cache quantization, we use per-token asymmetric quantization with group size of 128. For weight-activation quantization, we apply per-channel weight symmetric quantization and per-token activation asymmetric quantization for computing efficiency. We leave the queries in full precision to leverage the memory-bound nature of self-attention operation following Ashkboos et al. (2024). 14 Preprint. Under review. Model Quantization BF Weight-only KV Cache Weight-Act. 2 3 - W-A-KV # Bits - 4-16-16 3-16-16 16-16-4 16-16-3 8-8-8 4-4Methods - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant AIME120 73.33 69.17 73.33 58.33 72.50 77.50 76.67 56.67 66.67 72.50 72.50 72.50 32.50 55.83 71.67 MATH500 96.00 95.80 96.20 95.60 96.40 97.40 98.00 96.80 97.00 97.40 97.00 97.40 91.20 95.40 96.20 GSM8K 96.29 95.68 95.60 95.91 95.45 95.53 95.83 95.30 95.15 95.53 95.75 96.13 95.15 95.07 95. GPQADiamond 63.64 64.14 65.15 61.11 61.11 66.67 58.08 60.10 60.61 65.15 68.18 64.14 53.54 56.57 63.13 LiveCodeBench Avg. Drop 60.82 57.46 59.33 52.99 53.73 60.82 60.82 54.10 48.51 61.94 62.69 60.82 34.33 51.87 59.33 78.02 76.45 77.92 72.79 75.84 79.58 77.88 72.59 73.59 78.50 79.22 78.20 61.34 70.95 77.17 - -1.57 -0.09 -5.23 -2.18 1.57 -0.14 -5.42 -4.43 0.49 1.21 0.18 -16.67 -7.07 -0. Table 2: The overall assessment of quantized QwQ-32B model on various reasoning benchmarks. The green, orange and red cells stand for the lossless (1%), the fair (1%-3%) and the risky (3%) respectively. A.3 Weight-Activation Quantization (Xiao et al., 2023) employs per-channel scaling, i.e., = (X c1)(c W), SmoothQuant which migrates the quantization difficulty from activations to weights, achieving the optimal balance between weight quantization and activation quantization. SmoothQuant enables 8-bit weight-activation quantization without compromising accuracy. We randomly sample 128 text sequences of length 512 from Pile (Gao et al., 2020) dataset as the calibration set and leave the other quantization settings the same as QuaRot. FlatQuant (Sun et al., 2024) is the recently proposed approach. Different from QuaRot, it adaptively learns separate Kronecker-decomposed affine transformations to mitigate outliers for each linear layer, i.e., = arg minP Q(XP)Q(P1W)2 F. In addition, FlatQuant also introduces learnable clipping and channel-wise scaling to further reduce the quantization error. By default, we use 128 text sequences of length 2048 from WikiText2 (Merity et al., 2016) as the calibration set following Sun et al. (2024). For DeepSeekR1-Distill-Qwen-1.5B and 7B models, we extend the sequence length to 4096 to mitigate the extreme outlier issues in the cache as discussed in Appendix C.2. The quantization settings are the same as QuaRot. MXFP4 (Rouhani et al., 2023) is part of the Microscaling (MX) data format family, which aims to improve the representation ability of low-precision formats through fine-grained pergroup quantization. MXFP4 uses 4-bit floating-point representation with an E2M1 (2-bit exponent and 1-bit mantissa) format for each element, and shared 8-bit exponential scaling factor for every group of 32 elements. Similar to QuaRot, we employ mixed-precision strategy for queries in the self-attention module."
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Results on More Quantized LLMs Below we list the full results of QwQ and DeepSeek-R1-Distill-LLaMA over reasoning benchmarks. Results on QwQ. From Table 2, it can be seen that conclusions from 3.1 remain consistent. For example, 4-bit weight-only quantization reaches lossless results while 3-bit induces non-negligible accuracy loss, e.g., over 7% degradation on LiveCodeBench for both AWQ and GPTQ. For KV cache quantization, both KVQuant and QuaRot achieve lossless 4-bit quantization, but all suffer from severe quantization loss when the KV cache is further quantized into 3 bits. The degradation is most pronounced on difficult tasks with long response 15 Preprint. Under review. Model Quantization BF16 Weight-only KV Cache Weight-Act. BF16 Weight-only KV Cache Weight-Act. 8 - L - t - 1 - S D 0 7 - L - t - 1 - S D W-A-KV # Bits - 4-16-16 3-16-16 16-16-4 16-16-3 8-84-4-4 - 4-16-16 3-16-16 16-16-4 16-168-8-8 4-4-4 Methods - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant - AWQ GPTQ AWQ GPTQ KVQuant QuaRot KVQuant QuaRot SmoothQuant QuaRot FlatQuant MXFP4 QuaRot FlatQuant AIME120 34.17 28.33 34.17 15.00 15.00 30.00 36.67 15.83 27.50 36.67 39.17 35.00 9.17 15.83 22.50 59.17 52.50 47.50 42.50 40.00 54.17 57.50 53.33 55.00 60.00 60.83 59.17 6.67 7.50 50.83 MATH500 89.60 88.20 89.20 79.20 56.20 88.40 90.80 83.40 86.60 91.40 89.40 89.20 70.20 76.80 86.60 96.00 94.60 95.20 92.00 92.00 96.20 95.40 94.40 93.60 95.40 94.20 95.40 66.40 71.60 95. GSM8K 88.78 87.19 87.34 84.76 48.29 87.72 89.31 86.88 88.86 88.63 88.93 88.40 81.50 71.87 85.82 94.09 93.86 94.09 93.48 92.57 94.01 94.16 94.09 93.78 94.09 94.24 94.47 89.54 86.13 93.71 GPQADiamond 50.51 43.43 46.97 32.83 24.75 47.47 46.46 42.42 44.44 43.94 48.48 45.96 32.32 38.38 43.94 66.67 63.13 66.16 61.11 54.55 60.61 65.66 61.11 63.13 66.16 63.13 68.69 39.39 31.82 64.14 LiveCodeBench Avg. Drop 35.45 34.33 32.84 19.78 19.40 33.21 37.31 28.73 30.97 35.07 36.57 39.55 16.42 22.39 30.60 54.48 53.36 51.12 45.15 44.78 51.49 54.48 49.25 50.37 54.10 54.85 54.48 11.94 22.39 49. 59.70 56.30 58.10 46.31 32.73 57.36 60.11 51.45 55.67 59.14 60.51 59.62 41.92 45.05 53.89 74.08 71.49 70.81 66.85 64.78 71.30 73.44 70.44 71.18 73.95 73.45 74.44 42.79 43.89 70.67 - -3.41 -1.60 -13.39 -26.97 -2.34 0.41 -8.25 -4.03 -0.56 0.81 -0.08 -17.78 -14.65 -5.81 - -2.59 -3.27 -7.23 -9.30 -2.79 -0.64 -3.65 -2.91 -0.13 -0.63 0.36 -31.29 -30.19 -3.42 Table 3: The overall assessment of quantized DeepSeek-R1-Distill-LLaMA models on various reasoning benchmarks. The green, orange and red cells stand for the lossless (1%), the fair (1%-3%) and the risky (3%) respectively. (a) DS-R1-Distill-Qwen-1.5B. (b) DS-R1-Distill-Qwen-14B. (c) QwQ-32B. Figure 6: The accuracy of different difficulty levels on MATH-500. lengths such as AIME-120 and LiveCodeBench. For weight-activation quantization, 8-bit quantization remains lossless. For the more challenging 4-bit weight-activation quantization, only FlatQuant maintains the accuracy. Results on DeepSeek-R1-Distill-LLaMA. As shown in Table 3, the findings align with the conclusions presented in 3.1. LLaMA models can have lossless quantization results for 4-bit KV cache quantization and 8-bit weight-activation quantization. The 4-bit weight quantization is nearly lossless. For lower bits, all of the evaluated quantization methods become risky, e.g. W3G128 incurs accuracy loss over 7% for both 8B and 70B models. 16 Preprint. Under review. (a) DS-R1-Distill-Qwen-1.5B. (b) DS-R1-Distill-Qwen-14B. (c) QwQ-32B. Figure 7: Number of generated tokens of different difficulty levels on MATH-500. Methods BF16 AWQ GPTQ KVQuant W-A-KV # Bits - 3-16-16 3-1616-16-4 SmoothQuant 8-8-8 FlatQuant 4-4-4 Calibration Domains - WikiText2 Numina-Math-1.5 WikiText2 Numina-Math-1.5 WikiText2 Numina-Math-1.5 WikiText2 Numina-Math-1.5 WikiText2 Numina-Math-1.5 AIME120 21.67 5.83 5.83 3.33 10.00 20.83 20.00 17.50 20.83 10.00 8.33 MATH500 84.40 54.40 60.20 49.40 71.60 84.60 83.80 79.80 79.80 64.80 65.60 GSM8K 84.61 68.39 71.27 57.54 75.66 84.00 84.46 83.24 82.71 78.62 78.01 GPQADiamond 36.87 31.82 28.79 27.27 23.74 33.84 33.84 34.34 35.86 31.82 33. LiveCodeBench 16.04 3.36 6.72 3.73 9.33 13.43 16.42 15.67 15.67 6.72 7.46 Avg. 48.72 32.76 34.56 28.25 38.07 47.34 47.70 46.11 46.97 38.39 38.55 - 1.80 9.81 0.36 0.86 0.15 Table 4: The impact of calibration data domain on different quantization methods. The evaluation is conducted on the quantized DeepSeek-R1-Distill-Qwen-1.5B model across various reasoning benchmarks. The green, orange and red cells stand for the robust (1%), the fair (1%-3%) and the sensitive (3%) quantization methods respectively. B.2 Performance v.s. Difficulty Levels Figure 6 presents the performance across varying difficulty levels on the MATH-500 benchmark. The results reveal that quantization-induced accuracy loss becomes more pronounced as task difficulty increases. For example, FlatQuant-W4A4KV4 achieves comparable performance to the full-precision 14B model, while it suffers over 2% accuracy loss at level 5. This indicates that complex tasks exhibit greater vulnerability to precision reduction. B.3 Output Length v.s. Difficulty Levels Here we further examine the impact of task difficulty on the MATH-500 benchmark, which categorizes problems into five difficulty levels. We have the following observations from Figure 7: 1) Reasoning models tend to generate more tokens at test time when solving harder problems. 2) Quantized LLMs with minor accuracy degradation (e.g., W4G128, W8A8KV8) do not generate longer responses than the BF16 model. However, lower bitwidth on smaller LLMs (in particular, DeepSeek-R1-Distill-Qwen-1.5B with W3G128 and W4A4KV4 quantization) could lead to increased output, which is similar to the conclusion in 4.1. 17 Preprint. Under review. the 14th (a) Xg of Transformer layer in DeepSeek-R1-DistillQwen-1.5B. the 21st (b) Xq of Transformer layer in DeepSeek-R1-DistillQwen-1.5B. the 14th (a) Xg of Transformer layer in DeepSeek-R1-DistillQwen-1.5B. the 21st (b) Xq of Transformer layer in DeepSeek-R1-DistillQwen-1.5B. Figure 8: Channel-wise magnitude distributions of activations from different source domains. Reasoning (i.e. Numina-Math1.5 and LiveCodeBench) and pre-training (i.e. WikiText2) data share similar distributions. The statistics are computed over 32K tokens. Xg and Xq denote the inputs of the feed-forward gate projection layer and the self-attention query projection layer in Transformer layer, respectively. Figure 9: t-SNE visualization of activations from different source domains. Domain gaps exist between reasoning (i.e. Numina-Math-1.5 and LiveCodeBench) and pre-training (i.e. WikiText2) data. We randomly sample 128 tokens from each domain. Xg and Xq denote the inputs of the feedforward gate projection layer and the selfattention query projection layer in Transformer layer, respectively."
        },
        {
            "title": "C Additional Discussions",
            "content": "C.1 Choice of Calibration Data Calibration data has always been an important part of post-training quantization. While previous studies (Williams & Aletras, 2023; Lin et al., 2023) suggest that the overall performance of quantization methods on pre-trained and instruction-tuned models is robust to the source domain of calibration data, we find this is not true for reasoning models. Instead, the choice of calibration data domain may have huge impact on the quantization accuracy. In addition, this effect is highly dependent on the quantization method used. In the following, we first analyze the distributions of calibration data from different domains, and examine its impact on different quantization methods accordingly. Calibration Data Collection. We collect calibration data from different source domains, including reasoning (i.e. Numina-Math-1.5 (LI et al., 2024) and LiveCodeBench (Jain et al., 2024)) and pre-training (i.e. WikiText2 (Merity et al., 2016)) data. The reasoning data is self-generated by the reasoning model, given the problems in the original dataset, with generation configurations defined in 3.1. The Distribution of Calibration Data from Different Domains. In Figure 8 and Figure 9, we visualize the channel-wise magnitude and activation distributions from different source domains, respectively. It can be seen that the channel-wise magnitude distributions across different source domains share the same outlier channels, demonstrating notable similarity. Besides, the activation distributions across different reasoning datasets are also close to each other, regardless of the dataset category. However, as shown in Figure 9, activations of the reasoning data exhibit completely distinct distribution from those of the pre-training data, although their channel-wise magnitude distributions resemble each other. This domain gap may raise issues for post-training quantization methods if pre-training data is used for calibration. The domain of calibration data affects GPTQ, but not the rest of the quantization algorithms. Prior works (Frantar et al., 2022; Lin et al., 2023) on post-training quantization usually sample calibration data from pre-training datasets like WikiText2 (Merity et al., 2016) or C4 (Raffel et al., 2020). However, we find that using reasoning data for calibration is crucial for quantization methods that rely heavily on the calibration data for quantization Preprint. Under review. of the 0th in layer (a) Ypre bias Transformer DeepSeek-R1-DistillQwen-1.5B. (b) cache of the 0th Transformer in DeepSeek-R1-DistillQwen-1.5B. layer (c) Ypre bias cache of the 0th Transformer layer in DeepSeek-R1-DistillQwen-7B. (d) cache of the 0th Transformer in DeepSeek-R1-DistillQwen-7B. layer Figure 10: Visualizations of cache before and after the bias term of key projection layers in DeepSeek-R1-Distill-Qwen-1.5B and 7B models. Ypre bias denotes the outputs of the self-attention key projection layer before adding the bias term. error compensation (e.g. GPTQ (Frantar et al., 2022)). In Figure 9, we visualize the activation distributions from different source domains. It can be seen that domain gaps exist between reasoning (i.e. Numina-Math-1.5 and LiveCodeBench) and pre-training (i.e. WikiText2) data, which can cause training-inference inconsistency issues that seriously deteriorate the quantized model if it is calibrated on the pre-training data. As shown in Table 4, switching the GPTQ calibration set from WikiText2 to Numina-Math-1.5 leads to an average accuracy gain of 9.81%. Also, note that the source domain of the calibration data has relatively small impact on other quantization methods such as KVQuant (Hooper et al., 2024), and FlatQuant (Sun et al., 2024). These methods only rely on calibration data for quantization parameter computation or outlier channel pattern identification. Given the high consistency of channel-wise magnitude distribution across different domains as shown in Figure 8, these methods are more robust to domain shift than GPTQ. C.2 Analysis on DeepSeek-R1-Distill-Qwen-1.5B and 7B Models As discussed in 3.5, different LLM origins can have various impacts on the quantization of reasoning models. We find that the cache of DeepSeek-R1-Distill-Qwen-1.5B and 7B models exhibit extreme outlier channels due to the huge bias terms in the self-attention key projection layer, which significantly increases the difficulty of KV cache quantization on these two models. In this section, we first reveal the phenomenon of extreme outlier channels in DeepSeek-R1-Distill-Qwen-1.5B and 7B models as well as its root, and then propose some practical solutions to mitigate its impact on KV cache quantization. C.2.1 Extreme Outlier Channels in Cache Qwen (Bai et al., 2023) models add biases in the self-attention query, key, and value projection layers for better length extrapolation ability. However, we find that the bias terms of key projection layers can be extremely large in the pre-trained Qwen-1.5B and 7B models, e.g. the maximum absolute value in key projection bias terms reaches 402 in Qwen-1.5B. The distilled reasoning models inherit the large bias terms from pre-trained models, leading to extreme outlier channels on cache as shown in Figure 10. This phenomenon well explains the large performance gap between KVQuant and QuaRot on DeepSeek-R1-Distill-Qwen1.5B and 7B models in Table 1. The presence of extreme outlier channels severely degrades the performance of per-token quantization methods like QuaRot. These outliers force an expansion of the quantization range, leading to catastrophic failures in QuaRots accuracy. In contrast, KVQuant mitigates the outlier channels by employing per-channel quantization, which effectively constrains the quantization range and achieves higher accuracy. 19 Preprint. Under review. Model Methods 1.5B 7B BF16 KVQuant +Pre bias KVQuant +Pre bias BF16 KVQuant +Pre bias KVQuant +Pre bias W-A-KV # Bits - 16-1616-16-3 - 16-16-4 16-16-3 AIME120 21.67 7.50 20.83 4.17 9.17 45.00 0.00 40.83 20.00 36.67 MATH500 84.40 58.20 83.60 62.40 66.20 94.60 7.00 93.20 74.20 91. GSM8K 84.61 67.93 84.91 67.17 71.27 91.43 5.00 91.36 81.35 90.37 GPQADiamond 36.87 29.80 31.31 23.74 31.31 50.00 25.25 47.47 44.44 45.96 LiveCodeBench Avg. Drop 16.04 10.45 16.42 10.45 10.45 35.45 0.37 35.07 13.81 27. 48.72 34.78 47.41 33.59 37.68 63.30 7.52 61.59 46.76 58.29 - -13.94 -1.30 -15.13 -11.04 - -55.77 -1.71 -16.54 -5.01 Table 5: The effect of pre-bias quantization on DeepSeek-R1-Distill-Qwen-1.5B and 7B models. The green, orange and red cells stand for the lossless (1%), the fair (1%-3%) and the risky (3%) respectively. the layer (a) Yk of Transformer DeepSeek-R1-DistillQwen-1.5B. 0th in (b) cache of the 0th Transformer in DeepSeek-R1-DistillQwen-1.5B. layer (c) Yk cache of the 0th Transformer in DeepSeek-R1-DistillQwen-7B. layer (d) cache of the 0th Transformer in DeepSeek-R1-DistillQwen-7B. layer Figure 11: Channel-wise magnitude distributions of cache before and after RoPE. The statistics are computed over different sequence length ranges. Yk denotes the outputs of the self-attention key projection layer. Methods BF16 FlatQuant Sequence Length W-A-KV # Bits Group Size - 2K 4K 8K - - 4-4-4 N/A AIME120 21.67 2.50 10.00 15.83 MATH500 84.40 53.40 64.80 73.20 GSM8K 84.61 73.84 78.62 78.85 GPQADiamond 36.87 30.30 31.82 37.88 LiveCodeBench Avg. Drop 16.04 2.24 6.72 9.33 48.72 32.46 38.39 43.02 - -16.26 -10.33 -5.70 Table 6: The effect of extending calibration sequence length on DeepSeek-R1-Distill-Qwen1.5B. The green, orange and red cells stand for the lossless (1%), the fair (1%-3%) and the risky (3%) respectively. C.2.2 Recipes for Extreme Outlier Channels on Cache Based on the observations in C.2.1, we propose two simple yet effective approaches to improve the KV cache quantization of DeepSeek-R1-Distill-Qwen-1.5B and 7B models. In Table 1, we adopt these approaches as default setups and report their results on DeepSeekR1-Distill-Qwen-1.5B and 7B models accordingly. Pre-bias Per-channel Cache Quantization for KVQuant. As detailed in Appendix A, KVQuant employs pre-RoPE per-channel cache quantization. Given that the extreme outlier channels are induced by bias terms, we propose to quantize cache before adding the bias term in the key projection layer to further reduce the quantization range. As shown in Figure 10, the pre-bias cache appears much smoother, suggesting greater compatibility with quantization. In Table 5, it can be seen that pre-bias quantization shows consistent improvements across different models and bit-widths. For instance, it brings an average accuracy improvement of 7.81% for 3-bit KV cache only quantization. 20 Preprint. Under review. Extending the Sequence Length of Calibration Data for FlatQuant. Reasoning models tend to generate long thinking steps before giving the final answer, which makes it important to maintain the quantization accuracy over very long sequences. However, we find that the extreme outliers induced by bias terms pose great challenge to learning-based outlier smoothing quantization methods such as FlatQuant, severely degrading the accuracy over long sequences. Specifically, the extreme outliers in the output of the key projection layer are further rotated into adjacent channels through RoPE. As shown in Figure 11, the channel-wise magnitude distributions vary significantly across sequence positions due to differing rotation degrees. As result, the outlier-smoothing transformations learned on the calibration data with short sequence lengths (e.g. 2K) are unable to generalize to longer sequences. Therefore, we choose to extend the sequence length of calibration data for FlatQuant to mitigate this issue. In Table 6, we vary the lengths of the calibration data in {2K, 4K, 8K}. The accuracy keeps increasing as the length of calibration data increases. In Table 1, we report the results of FlatQuant with the calibration data length of 4K to achieve balance between training efficiency and quantization accuracy."
        },
        {
            "title": "D Qualitative Examples",
            "content": "To learn the concrete behaviors of quantized reasoning models, We present several qualitative examples in Table 7-10. We divide the reasoning process into four parts understand, process, check, and final answer and use different colors to represent each part. As shown, the quantized models follow similar reasoning pattern as the BF16 model, even if the problem is incorrectly answered  (Table 10)  . Nonetheless, the quantized models may occasionally exhibit calculation errors or adopt slightly different verification strategies, but the overall reasoning structure remains consistent. This suggests that the models reasoning pattern is relatively robust to quantization, even under aggressive low-bit settings like MXFP4. 21 Preprint. Under review. Problem: In class of 50 students, 28 participate in MATHCOUNTS, 21 participate in science club, and 6 students participate in neither. How many students participate in both MATHCOUNTS and science club? Reasoning: Okay, so Ive got this problem here: In class of 50 students, 28 participate in MATHCOUNTS, 21 participate in science club, and 6 students participate in neither. need to find out how many students participate in both MATHCOUNTS and the science club. Hmm, let me think about how to approach this. First, remember something about sets and maybe using Venn diagram. Yeah, that sounds right. So, there are two groups here: MATHCOUNTS participants and science club participants. Some students might be in both, and some might be in neither. The total number of students is 50, and 6 are in neither, so that means 50 - 6 = 44 students are in at least one of the two clubs. Alright, so 44 students are in MATHCOUNTS, science club, or both. Now, know that the number of students in both can be found using the principle of inclusion-exclusion. The formula is: Total in either club = Number in MATHCOUNTS + Number in Science Club - Number in Both. Let me write that down: Total = + - Both. We know the total number of students in either club is 44, MATHCOUNTS has 28 students, and science club has 21. Plugging those numbers in: 44 = 28 + 21 - Both. Let me compute 28 + 21 first. Thats 49. So: 44 = 49 - Both. Hmm, so to find Both, can rearrange the equation: Both = 49 - 44 = 5. Wait, so does that mean 5 students are in both MATHCOUNTS and the science club? sense. Let me check if that makes If 28 are in MATHCOUNTS and 5 of them are also in the science club, then 28 - 5 = 23 are only in MATHCOUNTS. Similarly, 21 are in the science club, and 5 of those are also in MATHCOUNTS, so 21 - 5 = 16 are only in the science club. Adding those together: 23 (only MATHCOUNTS) + 16 (only science club) + 5 (both) = 44, which matches the total number of students in at least one club. And then the 6 students not in either club make the total 50. That all checks out. used the inclusion-exclusion principle dont think made any mistakes here. So, yeah, think 5 is the right answer. correctly, and the numbers add up when break it down. So Im confident that 5 students participate in both MATHCOUNTS and the science club. **Final Answer** The number of students participating in both MATHCOUNTS and science club is 5 . Table 7: An example from MATH-500 correctly answered by the BF16 DeepSeek-R1-DistillQwen-32B model. Preprint. Under review. Problem: In class of 50 students, 28 participate in MATHCOUNTS, 21 participate in science club, and 6 students participate in neither. How many students participate in both MATHCOUNTS and science club? Reasoning: Okay, so Ive got this problem here: In class of 50 students, 28 participate in MATHCOUNTS, 21 participate in science club, and 6 students participate in neither. The question is asking how many students participate in both MATHCOUNTS and science club. Hmm, okay, let me think about how to approach this. First, remember that when dealing with overlapping groups like this, formula that comes to mind is: its classic inclusion-exclusion problem. The Total = Number in MATHCOUNTS + Number in Science Club - Number in Both + Number in Neither Wait, let me write that down to make it clearer. So, the total number of students is equal to those in MATHCOUNTS plus those in science club minus those in both, plus those in neither. Hmm, let me make sure have that right. Actually, think the formula is: Total = (MATHCOUNTS + Science Club - Both) + Neither Because the ones in neither are outside both groups. plus the ones in neither. The union is calculated as MATHCOUNTS plus Science Club minus Both. So, the total is the union of MATHCOUNTS and Science Club Let me write that out: Total = (MATHCOUNTS + Science Club - Both) + Neither So, plugging in the numbers: 50 = (28 + 21 - Both) + 6 Okay, let me compute that step by step. First, 28 plus 21 is 49. So, 49 minus Both plus 6 equals 50. So, 49 + 6 is 55, right? So, 55 minus Both equals 50. Hmm, so 55 - Both = 50. Then, subtracting 50 from both sides, we get 55 - 50 = Both, so Both = 5. So, does that mean 5 students are in both MATHCOUNTS and science club? Wait, let me double-check because sometimes these inclusion-exclusion problems can be tricky. So, if there are 50 students total, and 6 are in neither, that means 50 - 6 = 44 students are in at least one of the two clubs. So, 44 students are in MATHCOUNTS, science club, or both. Now, MATHCOUNTS has 28 students, and science club has 21. If we add those together, 28 + 21 = 49. But wait, thats more than 44, which is the number of students in at least one club. So, that means the overlap must be 49 - 44 = 5. So, yeah, that confirms it. 5 students are in both. Let me make sure Im not making mistake here. So, if 5 students are in both, then the number only in MATHCOUNTS would be 28 - 5 = 23, and only in science club would be 21 - 5 = 16. Then, adding those up: 23 (only MATHCOUNTS) + 16 (only science) + 5 (both) + 6 (neither) = 23 + 16 is 39, plus 5 is 44, plus 6 is 50. That adds up correctly. So, yeah, it seems like 5 is the right answer. think that makes sense because when you have more students in both clubs than the total number in at least one, the difference is the overlap. So, 28 + 21 = 49, but only 44 are in at least one, so 5 must be in both. Wait, another way to think about it is using Venn diagram. Imagine two overlapping circles: one for MATHCOUNTS with 28 and one for science club with 21. The total area covered by the circles is 44 students. The sum of the individual circles is 49, so the overlapping part must account for the difference, which is 5. So, yeah, thats consistent. guess another way to write the formula is: Number in Both = MATHCOUNTS + Science Club - Total in at least one Which would be: Number in Both = 28 + 21 - 44 = 5 So, that also gives 5. So, all the methods point to 5. think Im confident that 5 is the correct answer. Just to recap: - Total students: 50 - Neither: 6 - Therefore, in at least one club: 50 - 6 = 44 - MATHCOUNTS: 28 - Science Club: 21 - Total in both clubs: 28 + 21 = 49 - But since only 44 are in at least one, the overlap is 49 - 44 = 5 Yeah, that seems solid. So, think the answer is 5 students participate in both MATHCOUNTS and science club. **Final Answer** The number of students who participate in both MATHCOUNTS and science club is 5 . Table 8: An example from MATH-500 correctly answered by the DeepSeek-R1-Distill-Qwen32B model quantized with GPTQ-W4A16G128 23 Preprint. Under review. Problem: Let = (1, 11, 2), = (3, 4, 1), and = (2, 1, 1). Compute ABC, in degrees. Reasoning: Okay, so need to find the angle at point in triangle ABC, which is angle ABC. The points are given as = (1, -11, 2), = (3, -4, 1), and = (-2, 1, -1). Hmm, remember that to find the angle between two vectors, can use the dot product formula. Since angle ABC is the angle at point B, think need to find the vectors BA and BC, and then find the angle between them. Let me first visualize this. Point is the vertex where the angle is, so BA is the vector from to A, and BC is the vector from to C. Once have these two vectors, can use the dot product formula to find the angle between them. So, how do find vectors BA and BC? Well, vectors can be found by subtracting the coordinates of the initial point from the terminal point. So for vector BA, its minus B, and for vector BC, its minus B. Let me write that down. Vector BA = - = (1 - 3, -11 - (-4), 2 - 1) = (-2, -7, 1) Vector BC = - = (-2 - 3, 1 - (-4), -1 - 1) = (-5, 5, -2) Okay, so now have BA = (-2, -7, 1) and BC = (-5, 5, -2). Next step is to find the dot product of BA and BC, and then divide it by the product of their magnitudes. That should give me the cosine of the angle between them, right? Let me compute the dot product first. The dot product of BA and BC is calculated by multiplying corresponding components and adding them up. So: BA BC = (2)(5) + (7)(5) + (1)(2) = 10 35 2 = 27 Hmm, thats negative number. That means the angle is obtuse, since the cosine is negative. Okay, moving on. Now, need the magnitudes of BA and BC. The magnitude of vector is found by taking the square root of the sum of the squares of its components. First, magnitude of BA: BA = sqrt[(2)2 + (7)2 + (1)2] = sqrt[4 + 49 + 1] = sqrt[54] = 3 sqrt(6) Wait, sqrt(54) is equal to sqrt(9*6) which is 3*sqrt(6). That seems right. Now, magnitude of BC: BC = sqrt[(5)2 + (5)2 + (2)2] = sqrt[25 + 25 + 4] = sqrt[54] = 3 sqrt(6) Oh, interesting, both vectors BA and BC have the same magnitude, 3*sqrt(6). So that simplifies things bit. Now, using the dot product formula: cos(theta) = (BA BC)/(BABC) Plugging in the numbers: cos(theta) = (27)/(3 sqrt(6) 3 sqrt(6)) = (27)/(9 6) = (27)/54 = 0.5 So, cos(theta) = -0.5. Now, to find theta, need to take the arccosine of -0.5. remember that cos(120) = -0.5, because cos(60) is 0.5 and cosine is negative in the second quadrant. So, arccos(-0.5) is 120 degrees. Therefore, angle ABC is 120 degrees. Wait, just to double-check everything. Let me recap: 1. Vectors BA and BC are correct. BA is (-2, -7, 1), BC is (-5, 5, -2). Yes, thats right because we subtract from and from C. 2. Dot product: (-2)(-5) = 10, (-7)(5) = -35, (1)(-2) = -2. So 10 - 35 -2 = -27. Thats correct. 3. Magnitudes: sqrt(4 + 49 +1) = sqrt(54), and sqrt(25 +25 +4) = sqrt(54). So both are 3*sqrt(6). Correct. 4. So, cos(theta) = (-27)/(54) = -0.5. Yes, thats right. 5. Arccos(-0.5) is 120 degrees. Thats correct because in the unit circle, 120 degrees is in the second quadrant with cosine -0.5. Therefore, think did everything correctly, and the angle is indeed 120 degrees. **Final Answer** The measure of angle ABC is 120 degrees. Table 9: An example from MATH-500 correctly answered by the BF16 DeepSeek-R1-DistillQwen-32B model. 24 Preprint. Under review. Problem: Let = (1, 11, 2), = (3, 4, 1), and = (2, 1, 1). Compute ABC, in degrees. Reasoning: Okay, so need to find the angle at point in triangle ABC. The points given are = (1, -11, 2), = (3, -4, 1), and = (-2, 1, -1). Wait, hold on, the coordinates for point are written as (-, 1, -1). That doesnt look right. Is that typo or mistake? Maybe it should be something like (-1, 1, -1) or (-2, 1, -1) with specific number? Hmm, maybe its typo and supposed to be (-1, 1, -1). Ill assume that for now, so point is (-1, 1, -1). think that makes sense. So, points are = (1, -11, 2), = (3, -4, 1), and = (-1, 1, -1). need to compute angle ABC, which is the angle at point between points A, B, and C. So, angle ABC is the angle at vertex B, formed by sides BA and BC. So, to find this angle, need to find the vectors BA and BC, then compute the angle between these two vectors using the dot product formula. First, need to compute vectors BA and BC. Vector BA is from to A, so its - B. Similarly, vector BC is from to C, so its - B. Let me compute vector BA first. is (1, -11, 2), is (3, -4, 1). So vector BA is - B, which is (1 - 3, -11 - (-4), 2 - 1). Let me calculate each component: 1 - 3 is -2. -11 - (-4) is -11 + 4, which is -7. 2 - 1 is 1. So vector BA is (-2, -7, 1). Wait, hold on, that would be (-2, -7, 1). Wait, is that right? 1 - 3 is -2, so first component is -2. Second component is -11 - (-4) is -11 + 4 is -7. Third component is 2 - 1 is 1. So vector BA is (-2, -7, 1). Wait, maybe the first component is -2, so its (-2, -7, 1) or (-2, -7, 1). Hmm, think its (-2, -7, 1). Maybe the person meant to write (-2, -7, 1), but the first component is -2. So, in components, its (-2, -7, 1). So, BA = (-2, -7, 1). Wait, maybe its (-2, -7, 1), but the first component is -2. . . . Therefore, arccos(-0.588) is 180 - 54 = 126 degrees. Wait, 54 degrees, so 180 - 54 is 126 degrees. So, the angle is approximately 126 degrees. Wait, let me confirm. If arccos(-0.588) is approximately 126 degrees because cos(126 degrees) is cos(180 54) = cos(54) 0.5878, which is about -0.588. So, thats exactly what we have. So, cos(theta) 0.588, so theta is approximately 126 degrees. So, the angle ABC is approximately 126 degrees. Wait, let me double-check my calculations. First, vectors BA and BC: Vector BA : = (1 3, 11 (4), 2 1) = (2, 7, 1) VectorBC : = (1 3, 1 (4), 1 1) = (4, 5, 2) . . . Vector BC is from to C, which is B, which is (1 3, 1 (4), 1 1) = (4, 5, 2). Correct. Dot product: (2) (4) = 8, (7) (5) = 35, (1) (2) = 2. Total: 8 35 2 = 29. Correct. Magnitudes: BA = sqrt[(2)2 + (7)2 + 12] = sqrt[4 + 49 + 1] = sqrt[54] = 3 sqrt(6). Correct. BC = sqrt[(4)2 + 52 + (2)2] = sqrt[16 + 25 + 4] = sqrt[45] = 3 sqrt(5). Correct. So, cos(theta) = (29)/(3 sqrt(6) 3 sqrt(5)) = (29)/(9 sqrt(30)). Yes, thats correct. So, the angle is arccos(-29/(9*sqrt(30))). So, in degrees, that is approximately 126 degrees. Therefore, the angle ABC is approximately 126 degrees. think thats the answer. *Final Answer** The measure of angle ABC is 126 degrees. Table 10: An example from MATH-500 that was incorrectly answered by the MXFP4quantized DeepSeek-R1-Distill-Qwen-32B model."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Shenzhen International Graduate School, Tsinghua University"
    ]
}