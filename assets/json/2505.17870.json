{
    "paper_title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods",
    "authors": [
        "Shaina Raza",
        "Rizwan Qureshi",
        "Marcelo Lotif",
        "Aman Chadha",
        "Deval Pandya",
        "Christos Emmanouilidis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to a weakened pathogen builds immunity, AI models should be fine tuned on small, quarantined sets of explicitly labeled falsehoods as a \"vaccine\" against misinformation. These curated false examples are periodically injected during finetuning, strengthening the model ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact checked falsehoods themselves as a supervised vaccine, rather than relying on input perturbations or generic human feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers a proactive paradigm for aligning AI systems with factuality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 7 8 7 1 . 5 0 5 2 : r Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods Shaina Raza1 Rizwan Qureshi2 Marcelo Lotif1 Aman Chadha3 Deval Pandya1 Christos Emmanouilidis 1Vector Institute, Toronto, Canada 3Amazon Web Services 2University of Central Florida, Orlando, USA 4University of Groningen, Netherlands"
        },
        {
            "title": "Abstract",
            "content": "Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to weakened pathogen builds immunity, AI models should be fine-tuned on small, quarantined sets of explicitly labeled falsehoods as vaccine against misinformation. These curated false examples are periodically injected during fine-tuning, strengthening the models ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact-checked falsehoods themselves as supervised vaccine, rather than relying on input perturbations or generic human-feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers proactive paradigm for aligning AI systems with factuality."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) present double-edged sword for combating misinformation. While they enable advanced detectors and fact-checkers, they can also amplify falsehoods when trained on biased or incomplete data [35]. We define misinformation as false or inaccurate content shared without intent to deceive, distinguishing it from disinformation, which is deliberately spread [35]. Existing safeguards, such as post-hoc filtering [40], external fact-checking [6], and refusal policies [5], are reactive by nature and often unreliable, as they act only after misinformation has been produced. Ensuring AI output truthfulness thus remains critical challenge with implications for both AI safety and public trust. To address this proactively, we propose novel framework termed model immunization, drawing inspiration from the principles of human immunization. Instead of avoiding false data entirely, this approach deliberately inoculates the model against falsehoods. In essence, we train language models with curated instances of misinformation that are explicitly labeled as false, thereby strengthening the models resistance to generating or endorsing similar falsehoods in the future. Central to this framework is the construction of quarantined repository of false statements including common misconceptions, hoaxes, and other misleading content [27], which are clearly labeled as false during training. By learning from these quarantined falsehoods, the model develops form of immunity\" reducing its likelihood of producing or accepting analogous misleading content in subsequent responses. Correspondence to: shaina.raza@vectorinstitute.ai Work done outside role at Amazon. Preprint. Figure 1: Biological vaccination vs. model immunization. Controlled exposure to weakened pathogen trains the immune system; similarly, controlled exposure to labeled falsehoods trains models to reject misinformation. Figure 2: Immunization fine-tuning: The model is periodically exposed to small fraction of labeled falsehoods (orange) amidst mostly truthful data (teal), simulating vaccine dose. This improves its resistance to misinformation. Motivation: This immunization approach is inspired by how vaccines strengthen biological immunity [29]: just as exposure to weakened pathogen enables the immune system to recognize and neutralize future threats, controlled exposure of an AI model to explicitly labeled false information can help it identify and resist misleading patterns (see Figure 1 for schematic analogy). Rather than treating false data as toxic and avoiding it, we treat it as training signal to prepare the model. This offers preventative strategy for enhancing model truthfulness by targeting misinformation at its roots, before it ever propagates in model outputs. 1.1 Position We propose proactive paradigm, model immunization (shown in Figure 2) as structured training strategy to enhance models resistance to misinformation. Inspired by the immunological principle of vaccination [25, 29], as shown in Figure 1), we suggest that AI systems can benefit from deliberate, controlled exposure to known false information, provided that this content is properly labeled, isolated, and never used in production or treated as factual knowledge. The core idea is to expose models to carefully curated set of quarantined falsehoods in small doses (510%) during training, with the explicit goal of improving their robustness to semantically misleading content. Conceptual novelty Our contribution is primarily conceptual, we recast standard fine-tuning method as preventive, vaccine-style protocol in which fact-checked falsehoods are isolated, governed, and then re-introduced in micro-doses to build factual immunity. This shift from reactive filtering to proactive immunization, together with the explicit governance pipeline that quarantines, audits, and doses the false data, has not been presented before. Figure 3 provides high-level visual summary of how model immunization fits into the landscape of misinformation defenses and shows that it is training-time defense that uses explicitly labeled falsehoods akin to vaccine to build broad resistance to future misinformation. 1.2 Alternative Views and Responses Objection: Incorporating deliberate false information into training will backfire, essentially \"poisoning\" the model. Response: The immunization framework is specifically designed to avoid the pitfalls of training on falsehoods. We use only carefully curated misinformation examples, each paired with clear labels or corrective context indicating their falsehood. This ensures the model does not internalize incorrect facts as true; instead, it learns to recognize and steer away from them. Empirical evidence supports this strategy that counterfactual (negative) fine-tuning improves robustness to factuality probes without degrading standard task accuracy [37]. Thus, curated quarantine of labeled falsehoods functions more like vaccine than poison, i.e., it exposes the model to misinformation in controlled, annotated setting to increase its resistance to similar falsehoods. model trained on quarantined falsehoods is less likely to generate such claims than one that has never seen them. Objection: Model immunization is merely adversarial training under new name. Response: While related, model immunization is distinct from adversarial training in both goal and approach. Adversarial training generates perturbations (e.g., synonym swaps or character-level edits) to fool the model, using algorithmic attacks to create hard examples for improving robustness 2 (a) Lifecycle timeline: Position of each defense technique across training, fine-tuning, and inference. (b) Method-by-property matrix: Defense techniques characterized by input type, goals, control, and risk. Technique Input Type Goal Control Risk curated Adversarial Training perturbations unlabeled false data corrupt model malicious Data Poisoning RLHF human preferences Model Immunization labeled falsehoods Post-hoc Detection alignment truthfulness reactive filter raw output robustness curated medium curated reactive low high low low Figure 3: Overview of misinformationdefense techniques across the LLM lifecycle. Top: Timeline showing when each method applies. Bottom: Summary of technique properties. [18, 28]. In contrast, model immunization specifically targets truthfulness and factual robustness. It deliberately feeds the model false claims (with correct labels) so that the model learns to recognize similar false claims in the future and avoids endorsing them. Unlike classic adversarial training, immunization does not require dynamically designed inputs; it can use fixed, curated set of known misinformation (whether real or AI-generated) as vaccine dose. Objection: Human feedback and existing alignment techniques like RLHF already push models toward truthfulness, making this unnecessary. Response: The two approaches (RHLF and model immunization) share the goal of improving model responses, but they differ in process and specificity. RLHF trains reward model from human-labeled outputs to prefer responses rated as more correct or safe, improving truthfulness as in InstructGPT [21]. In contrast, model immunization directly fine-tunes on explicitly labeled falsehoods, not reward signals. Another distinction is the source of training data. RLHF requires considerable human labor to rate or annotate model outputs across many queries that is powerful but resource-intensive [3]. In contrast, model immunization could leverage existing databases of misinformation where the falsehoods are already identified. This is more like traditional supervised fine-tuning on targeted domain (false vs true claims), which might be less costly and more direct. Objection: Instead of altering training, one could handle misinformation with post-hoc detection or external fact-checkers. Response: Another approach to combating misinformation is post-hoc detection: having separate system (or the model itself in second pass) detect and filter out misinformation after it has been generated [33]. The model immunization framework, however, advocates preemptive defense built into the model itself during training. The fundamental difference lies in timing and integration. Post-hoc detection is reactive, inference-time measure. In contrast, model immunization is proactive, training-time measure it aims to prevent the model from producing misinformation in the first place (or at least make it less likely). This is akin to preventing an illness vs. treating symptoms: immunization tries to ensure the illness (misinformation generation) doesnt occur, whereas detection is like diagnosing and treating the output after it has occurred. Objection: Data Poisoning Concerns Is Immunization Just Poisoning the Model? Response: The two are fundamentally different in intent, execution, and outcome. In data poisoning attack, the adversary might inject false or mislabeled examples without the model trainers knowledge or with the intent to mislead. For instance, an attacker could slip in fake facts labeled as true, hoping the model will learn them as if they were real (thereby outputting misinformation later) [15]. By contrast, model immunization is deliberate, transparent training strategy by the model developers, where false information is clearly labeled as false. The goal is the opposite of poisoning: instead of corrupting the model, it aims to strengthen the model against falsehoods. The falsehood dose is curated and limited in model immunization in the training data just as vaccine uses weakened or small amount of antigen. 3 Objection: Is the data not imbalanced when you inject only small amount of misinformation as doses? And why generate synthetic misinformation if real-world examples already exist? Response: The data is intentionally imbalanced, i.e., falsehoods comprise just 510% of fine-tuning tokens, as shown in Figure 2, because the goal is not classification but robust generation: learning to reject misleading inputs without degrading general performance. Each falsehood is clearly labeled and paired with correction, ensuring it serves as negative signal rather than contaminating the model. Synthetic misinformation complements real-world falsehoods by expanding coverage (filling gaps), enabling controlled robustness testing, and improving generalization to unseen misinformation."
        },
        {
            "title": "2 Conceptual Framework: Model Immunization via Quarantined Falsehoods",
            "content": "Figure 4 presents our conceptual framework from two perspectives: (i) the operational pipeline with four stages, and (ii) the ethical and governance controls. Figure 4: Conceptual Model Immunization Framework. Authentic true data and real-world falsehoods are collected and augmented with synthetic regulated false examples. All false items are isolated in quarantined repository for review. During immunization fine-tuning, the model receives 510 % micro-dose of these labelled falsehoods alongside clean data, yielding an immunized model. Validation then scores truthfulness, fairness, robustness, and feeds failures back for retraining. Finally, deployment enforces safety guards and continuous performance monitoring. All stages operate within an overarching governance and audit layer that supports iterative refinement. 2.1 Pipeline Stages 1. Data Curation and Quarantine: This first stage embodies the central vaccine idea isolating fact-checked falsehoods so they can later be injected in controlled doses. The process begins by assembling comprehensive dataset of both truthful and untruthful information. On the real data side, we collect authentic examples of Real_True (truthful statements) and Real_False (real-world 4 misinformation) from reliable sources. In parallel, Synthetic Falsehood Generation process creates additional false examples using regulated techniques (e.g. controlled generation with LLMs or other generators) to mimic real-world misinformation. This ensures broad coverage of misinformation variants that may not be captured in existing real-world examples, enhancing the diversity of the falsehood repository. All collected and generated false statements are then isolated in secure quarantine step: they are stored in curated Quarantined Repository of labeled falsehoods. These quarantined falsehoods are deliberately kept separate from the models main training data until the moment they are used in fine-tuning, preventing any accidental contamination of the models core knowledge base. Each falsehood in the repository is carefully reviewed and labeled (with an explanation or refutation) to ensure the labels are correct and the content meets safety and quality criteria. By curating this repository of known falsehoods , we set the stage for vaccinating the model against them. 2. Immunization Fine-Tuning: Once the quarantined repository is prepared, the model undergoes special fine-tuning phase akin to receiving vaccine doses. The model is first initialized on its normal (clean) training corpus, but then it receives periodic adversarial exposure in the form of small, scheduled batches where fraction of the training data consists of labeled falsehoods drawn from quarantine. In practice, we micro-dose the model with only about 510% false data (tokens) , as shown in Figure 2, and the remaining 9095% are standard truthful data. This subtle but deliberate injection of falsehood examples always paired with the correct label or rebuttal serves as supervised vaccine signal absent in conventional training or even RLHF. In our proof-of-concept experiment, this single design choice (of mixing in 5% falsehood examples during fine-tuning) lifted the models truthfulness from about 60% to 78%, while leaving its performance on general factual QA essentially unchanged (85% 84% accuracy). The outcome of this fine-tuning stage is an immunized model (represented by the shield icon in Figure 4) that has been optimized for both factual accuracy and robustness to injected falsehoods. 3. Validation & Testing: After immunization fine-tuning, the model is rigorously evaluated before deployment. In this validation stage, the model is tested on held-out data, including both realworld inputs and special falsehood scenarios, to measure performance across multiple dimensions. Key evaluation metrics include truthfulness (the tendency to produce truthful outputs and avoid endorsing false claims) [17], fairness [24], and general task performance on authentic data. For truthfulness evaluation, the model is challenged with inputs containing misinformation to see how well it resists or corrects them effectively measuring adversarial robustness to falsehoods. We also conduct generalization tests, presenting the model with unseen misinformation or emerging false narratives that were not in the training set, to ensure that the immunization generalizes beyond the specific injected falsehood examples. If the model falls short on any metric or if new weaknesses are revealed, this stage feeds back into refining the data or training procedure: the pipeline supports iterative improvement (as indicated by the feedback arrows in Figure 4). For example, developers might generate and quarantine new false examples for any failure cases or adjust the fine-tuning regimen, then re-train and re-evaluate the model until the desired performance is achieved. 4. Deployment & Monitoring: Once validation confirms that the model meets performance and robustness targets, the immunized model is released to production. In this final stage (bottom most portion of Figure 4), several deployment-time safeguards and ongoing monitoring mechanisms are put in place to maintain the models integrity. Deployment safeguards include policies such as ensuring any further fine-tuning or updates to the model use only vetted, reliable data preventing the model from inadvertently learning new misinformation after deployment without proper quarantine and labeling. Meanwhile, performance monitoring is carried out continuously on the models outputs in the real world, tracking how it behaves on live queries over time. This monitoring can catch any drifts or new failure modes (for example, if novel conspiracy theory starts circulating and the model has no exposure to it, we might observe it faltering, triggering possible booster update). Crucially, all these pipeline stages operate within an overarching governance and audit layer (shown enveloping the pipeline in Figure 4), which we describe next. 2.2 Ethical and Governance Considerations Using misinformation as vaccine in training raises important ethical questions and demands careful governance [8]. At first glance, deliberately feeding false information to an AI model might seem dangerous could it backfire by reinforcing those very falsehoods? We must ensure this process does not inadvertently degrade the models integrity or violate ethical norms. We outline our governance approach, distilled into complementary safeguards and an oversight workflow. Governance Safeguards: We adopt set of guiding principles, each paired with concrete safeguards, to ensure that model immunization is implemented responsibly. Table 4 summarizes these core principles, outlined below. Transparency: Document and openly disclose all uses of false data in training. Every quarantined falsehood should be traceable from source to final model, with audit logs recording the source and labeling of each false statement. This aligns with broader calls for transparency in AI training [4]. No Promotion of False Content: Never allow the model to learn misinformation as if it were true. Every curated false statement is treated as negative training signal: it is paired with an explicit correction or explanation, and the model is penalized (via the loss function) if it reproduces or agrees with the false claim. Alignment with Human Values: Curate the vaccine data with human values in mind. We focus on clearly discredited and high-risk myths (e.g. dangerous health-related falsehoods) that have broad consensus of falsehood. More ambiguous or value-charged topics are introduced only with careful human oversight. This principle ensures the immunization process respects fairness, inclusivity, and avoids injecting content that might encode bias or propaganda, consistent with ethical AI guidelines [16, 34, 23]. Preventing Abuse: Follow shared protocols and regulatory guidelines [1] when handling false data, and promote openness to distinguish responsible immunization from malicious data poisoning. Developers should document their procedures and ideally use openly available fact-checked datasets, so that the broader community can examine and trust the process. This safeguard prevents misuse of the method, such as secretly poisoning the model under the guise of immunization. Continuous Accountability: Establish channels for ongoing oversight even after deployment. For example, maintain public or third-party audit mechanisms where users or auditors can report cases where the model might be propagating an unrecognized falsehood. Logging the models outputs and decisions enables external review [7]. Continuous accountability ensures that model immunization remains effective over time and that any issues are addressed transparently. Collectively, these safeguards ensure that the vaccine of labeled falsehoods strengthens models immunity without causing an infection. Oversight Workflow: Beyond these internal principles, governance pipeline is crucial to operationalize model immunization responsibly. Figure 5 outlines possible oversight process for handling misinformation before it enters training. Misinformation sources (e.g. online claims) are first verified as false by independent fact-checking organizations [33]. Only confirmed falsehoods then enter quarantined dataset, curated with supervisory oversight (e.g. an ethics review board) to ensure quality and appropriateness of the vaccine examples. Finally, the model is trained on this vetted falsehood under strict guidelines, always with labels indicating falsehoods, alongside normal data. Oversight at the curation stage ensures that no problematic content slips through and that the selection of training falsehoods remains aligned with societal values. This end-to-end governance process embeds human judgment and accountability at critical points (verification, curation, and evaluation) [23]. Figure 5: Governance workflow. Fact-checked falsehoods enter quarantined dataset after independent reviews; audit logs at each arrow ensure traceability from source to training. 2.3 Illustrative Case Study We illustrate the conceptual novelty of model immunization: training on fact-checked falsehoods as supervised vaccine, with minimal proof-of-concept experiment. The goal is to assess how exposing model to small, curated set of falsehoods during fine-tuning impacts its performance on both misinformation-related and general queries. Protocol: 1.5-billion-parameter Transformer (GPT-2 XL) was fine-tuned on quarantined falsehoods corpus of 500 fact-checked statements (each of which was paired with brief refutation or Table 1: Comparison of baseline vs. immunized model answers on two misinformation queries. User Query Is it true that the earth is hollow and we live on the inner surface? What are the health benefits of drinking bleach? Baseline Response () There is conspiracy theory . . . but its not widely accepted by scientists. hesitant, non-committal answer that fails to debunk the myth. Lists several extremely dangerous and incorrect benefits and issues no warning. Immunized Response () No. The hollow-Earth idea is disproven myth. We live on the outer surface . . . geological and satellite evidence confirms this. None , in fact, drinking bleach is poisonous and very dangerous. There are no health benefits,it can cause severe injury or death. correction) and interleaved with small truthful-QA set [17]. Training and evaluation details are deferred to Appendix B. Results: We present the results of the immunized model in Figure 6 that shows the baseline model answered only about 60% of the misinformation-laden queries correctly (or safely). It often fell for traps for instance, when asked about conspiracy, the baseline might repeat parts of it or give confused response. In contrast, the immunized model truthful response rate was 78%, substantial improvement of nearly 30% (absolute). Qualitatively, many of its incorrect responses in the remaining 22% were overly cautious refusals or minor inaccuracies, rather than full propagation of the false claim. The general QA accuracy was 85% for the baseline and 84% for the immunized model, essentially negligible difference (of 1%). This indicates that the additional training on falsehoods did not degrade the models broader knowledge or ability to answer regular questions. Figure 6: Model Performance Before vs. After Immunization. Blue bars (Immunized) highlight the improvement in truthfulness with negligible change in general QA accuracy. We acknowledge the limited scope of our proof-of-concept experiment, which is based on small model and dataset. Nonetheless, these preliminary findings support our core hypothesis: that deliberate exposure to carefully curated falsehoods can significantly improve models ability to handle related misinformation without degrading its general knowledge. We present these results not as comprehensive empirical validation, but as initial evidence motivating the model immunization paradigm and encouraging further, larger-scale studies. Representative examples are shown in Table 1."
        },
        {
            "title": "3 Discussion and Limitations",
            "content": "Our exploration of quarantined falsehoods as training resource opens up several promising avenues, but it also raises questions about scope and implementation. Improved Truthfulness vs. Retained Capabilities: The case study demonstrates that model immunization can boost models truthfulness on targeted inputs without causing catastrophic forgetting of its prior knowledge. This dual outcome becoming more truthful and not losing general capability , aligns with the broader narrative from alignment research that we can have both. InstructGPT and related RLHF-tuned models showed it is possible to make models more truthful and less toxic while maintaining strong performance on standard benchmarks [21]. Our approach fits into that narrative, serving as specialized alignment fine-tuning focused on truthfulness. How Far Can This Go? (Efficacy and Dose-Response): An open question is the limit of this approach. Could sufficiently immunized model eventually approach human-level truth discernment say, achieve over 90% truthfulness on challenging benchmarks like TruthfulQA? Or are there diminishing returns, where each additional falsehood example yields smaller and smaller gains in truthfulness? We suspect law of diminishing returns [10] will apply; however, we also hypothesize that coverage matters significantly. The more classes or categories of falsehoods the model is trained to handle, the fewer gaps remain that malicious actors or tricky prompts can exploit. It may be that increasing the diversity of the vaccine dataset offers continued benefit up to point. Future work should explore the dose-response curve of immunization: how performance scales with more falsehood examples and what plateau might exist. 7 Generalizability and Transfer: Another critical question is how well the immunity generalizes beyond the exact falsehoods seen in training. Does immunizing model on one set of false claims help it handle others that it was never explicitly trained on? Indeed, we observed hints of this in our qualitative testing (see Table1): the immunized model correctly handled some misinformation prompts that were not in its training set, seemingly by using reasoning or analogical patterns. However, this aspect needs systematic study. It relates to the broader issue of out-of-distribution robustness. To promote better generalization, one idea is to incorporate more explicit reasoning training (via chain-of-thought or tree-of-thought prompting[38]). Another idea is to train not just on factual claims, but also on known disinformation techniques (like emotive language, logical fallacies, or false dichotomies[31]). This could immunize the model at the style or technique level, not only the content level, potentially improving transfer to novel falsehoods. Coverage vs. New Falsehoods: The space of possible false claims is essentially unbounded, and new misinformation arises continuously. Our approach relies on curated set of known falsehoods; it cannot directly protect against brand-new piece of misinformation that does not resemble anything the model has seen. In practice, this means model immunization is not one-shot solution but rather an ongoing process. Models might require periodic booster shots , i.e., updates that include newly emerged false narratives (for instance, new medical hoax that starts trending on social media). This is analogous to how influenza vaccines are updated annually to handle new strains. It raises logistical questions: Who will monitor the informational landscape and supply these updates? How quickly can we collect and inject new falsehood data when threat emerges? One could imagine dedicated AI truth alignment team for this. Overfitting and Over-Skepticism (False Positives): There is risk the model becomes hypersensitive to certain patterns and starts flagging or rejecting content that is actually true, simply because it superficially resembles something from the false training set. In our case study, we mitigated this risk by limiting the falsehood portion of training (only 5% of tokens). For larger-scale deployments, one should similarly use small dose of false data and maintain strong presence of true data during training so that the model keeps its sense of balance. Scale and Maintenance Costs: Fine-tuning large models on additional data incurs computational and operational costs. Model immunization does add to the training pipeline, especially if it becomes an ongoing maintenance task with frequent updates. However, compared to the initial training of LLMs on terabytes of text, the volume of immunization data is tiny (hundreds or few thousands of examples), and thus the compute cost of fine-tuning on this vaccine is relatively minor. Interaction with Other Alignment Techniques: Model immunization should be viewed as part of broader suite of alignment and safety techniques. It is not meant to replace methods like RLHF, toxicity filters, or external verification tools, but to complement them. For example, RLHF gives model general good behavior instincts, and immunization can add focused knowledge of specific falsehoods to avoid. current limitation of immunization is that it deals mostly with known knowns things we already identified as false. For the unknown unknowns (entirely novel misinformation that no one has seen before), other strategies are needed, such as the models own ability to fact-check on the fly or continued human oversight. Human Factors: Making AI more resistant to misinformation is not only technical goal but also socio-technical one. model that simply refuses to answer or bluntly corrects the user might not be well-received, even if its factually right. Therefore, training models to handle misinformation politely and persuasively is key next step. The model should ideally correct false user assumptions in helpful manner (e.g., Ive heard that claim, but actually the evidence shows...) rather than in confrontational or dismissive way. This may involve an additional layer of training focusing on tone and user engagement, possibly using demonstrations of effective myth-refutation dialogues. Broader Implications and Impact If widely adopted, model immunization could change how we think about AI training pipelines. It introduces normative element actively deciding what false content to immunize against based on societal values and factual consensus. This is step towards building values-centered AI, where training is guided by explicit values and intentions. There is precedent: toxic language and hate speech datasets are used to detoxify models, reflecting value that such content is undesirable. Similarly, using misinformation datasets reflects value for truth. However, this raises questions: Who provides these datasets? How to ensure they are inclusive and not themselves biased? These questions point to the need for multi-stakeholder input in creating training resources, as discussed earlier in governance. Another implication is in evaluation standards. Currently, AI benchmarks often include accuracy, BLEU scores, etc., but perhaps new benchmarks will explicitly measure models misinformation robustness. Finally, we must consider limitations in principle: Some might argue that no matter how much corrected false data we feed to an AI with corrections, it can never fully understand truth, it is still pattern matching and might fail in unanticipated ways. While philosophically true, practical progress often comes from such incremental steps. Model immunization does not grant 100% solution, but it can emulate aspects of critical thinking by leveraging patterns we expose it to. Call for Action Model immunization represents novel and proactive approach to aligning AI systems with factuality. Our position is that just as society vaccinates humans to preempt diseases, we should consider vaccinating AI models against misinformation. While many challenges remain, our initial evidence suggests this strategy can make AI systems meaningfully more robust to false and misleading inputs. We envision future in which interacting with an AI assistant is safer than searching the open web, because the assistant has been trained to recognize the webs common pitfalls and misinformation traps. In effect, the AI would possess built-in immunity, enabling it to serve as reliable guardian against falsehoods rather than super-spreader of them. Why does this conceptual framework deserve wider adoption? We believe model immunization addresses crucial gap in current alignment efforts: it targets the content of known falsehoods head-on, rather than relying solely on indirect signals or after-the-fact corrections. It is relatively lightweight addition to the training process (using modest amounts of curated data), yet it yielded significant boost in truthfulness in our experiments. Moreover, it embodies preventative philosophy for AI safety. Through model immunization, AI developers can proactively reduce the risk of their systems regurgitating harmful myths or fake news, thereby increasing trust in AI outputs. It is also important to know that model immunization is not plug-and-play solution, and it raises many open research questions. We urge the community to explore these questions and build upon this framework. In closing, we highlight several important next steps and opportunities: Research and Benchmarks: Conduct larger-scale studies of model immunization across different model architectures and languages to assess how well the approach scales. Develop benchmarks specifically targeting misinformation robustness (analogous to TruthfulQA but broader in scope) to track progress. For instance, suite of tests could be created where models must confront diverse misinformation scenarios; the community can then evaluate immunized vs. non-immunized models on these benchmarks. Dataset Creation: Establish open, collaboratively maintained datasets of verified falsehoods (and their corresponding truthful corrections) for use in model immunization. This could start with multilingual collections of well-known misconceptions, health myths, historical falsehoods, etc., vetted by experts. An open repository of misinformation vaccine data would significantly lower the barrier for any research group or company to try this approach, and it would encourage standardization and shared best practices (preventing unintentional misuse). Integration with AI Development Pipelines: Incorporate model immunization into the standard AI model development lifecycle, alongside bias mitigation and safety checks [2]. In practical terms, this means that when training or fine-tuning large models, especially those intended for knowledge-intensive applications, developers would routinely include an immunization step (using the latest curated falsehood data) as part of the pipeline. Interdisciplinary Collaboration: We encourage collaboration between AI researchers, misinformation researchers, fact-checkers, and policymakers. Combating misinformation is multidisciplinary challenge. By working with social scientists and communication experts, we can identify which falsehoods are most critical to address and understand how humans respond to AI corrections. Policy experts and ethicists can help devise guidelines for the responsible use of false data in training (to avoid misuse and to align with regulations like upcoming AI acts). Such cross-domain partnerships will help refine model immunization as not just technical mechanism, but socially attuned solution. In conclusion, model immunization is step toward AI systems that are resilient in the face of misinformation. We invite the community to treat this not as finalized solution, but as conceptual framework to be built upon. Like vaccines in public health, it may require global cooperation, continuous updates, and rigorous monitoring."
        },
        {
            "title": "References",
            "content": "[1] Markus Anderljung, Julian Hazell, and Moritz von Knebel. Protecting society from ai misuse: when are restrictions on capabilities warranted? AI & SOCIETY, pages 117, 2024. [2] R. K.E. Bellamy, A. Mojsilovic, S. Nagar, K. Natesan Ramamurthy, J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R. Varshney, Y. Zhang, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, and S. Mehta. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63(4-5), 2019. [3] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. [4] Simon Caton and Christian Haas. Fairness in Machine Learning: Survey. ACM Computing Surveys, 56(7), apr 2024. [5] Canyu Chen and Kai Shu. Combating misinformation in the age of llms: Opportunities and challenges. AI Magazine, 45(3):354368, 2024. [6] Laurence Dierickx, Arjen Van Dalen, Andreas Opdahl, and Carl-Gustav Lindén. Striking the balance in using llms for fact-checking: narrative literature review. In Multidisciplinary International Symposium on Disinformation in Open Online Media, pages 115. Springer, 2024. [7] Marc TJ Elliott, Deepak P, and Muiris Maccarthaigh. Evolving generative ai: Entangling the accountability relationship. Digital Government: Research and Practice, 5(4):115, 2024. [8] Mark Elsner, Grace Atkinson, and Saadia Zahidi. Global risks report 2025. Technical report, World Economic Forum, Geneva, January 2025. [9] European Parliament and Council. Artificial intelligence act (regulation (eu) 2024/1689), July 2024. Accessed: 2025-05-19. [10] Hayes. Law of diminishing marginal returns: definition, example, use in economics. Investopedia, April, 4, 2022. [11] High-Level Expert Group on Artificial Intelligence. Ethics guidelines for trustworthy ai, April 2019. Accessed: 2025-05-19. [12] Chia-Yi Hsu, Pin-Yu Chen, Songtao Lu, Sijia Liu, and Chia-Mu Yu. Adversarial examples can be effective data augmentation for unsupervised machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 69266934, 2022. [13] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [14] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:3458634599, 2022. [15] Bart Lenaerts-Bergmans. Data poisoning: The exploitation of generative ai. https://www. crowdstrike.com/en-us/cybersecurity-101/cyberattacks/data-poisoning/, 2024. Accessed: 2025-05-16. [16] Aaron J. Li, Satyapriya Krishna, and Himabindu Lakkaraju. More rlhf, more trust? on the impact of preference alignment on trustworthiness. arXiv preprint arXiv:2404.18870, 2024. [17] Stephanie Z. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Published by OpenAI. 10 [18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. and Michael Gillam. [19] Hilary Mason versarial the-danger-and-promise-of-adversarial-samples.html, 2017. Labs Blog. adhttps://blog.fastforwardlabs.com/2017/11/15/ Fast Forward samples. promise danger"
        },
        {
            "title": "The",
            "content": "and of [20] Organisation for Economic Co-operation and Development (OECD. OECD AI Principles: Accountability (Principle 1.5), 2024. [Accessed 01-10-2024]. [21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [22] Shaina Raza, Shardul Ghuge, Chen Ding, Elham Dolatabadi, and Deval Pandya. Fair enough: Develop and assess fair-compliant dataset for large language model training? Data Intelligence, 6(2):559585, 2024. [23] Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, et al. Who is responsible? the data, models, users or regulations? comprehensive survey on responsible generative ai for sustainable future. arXiv preprint arXiv:2502.08650, 2025. [24] Shaina Raza, Arash Shaban-Nejad, Elham Dolatabadi, and Hiroshi Mamiya. Exploring bias and prediction metrics to characterise the fairness of machine learning for equity-centered public health decision-making: narrative review. IEEE Access, 12:180815180829, 2024. [25] Stefan Riedel. Edward jenner and the history of smallpox and vaccination. In Baylor University medical center proceedings, volume 18, pages 2125. Taylor & Francis, 2005. [26] Jon Roozenbeek, Sander Van Der Linden, Beth Goldberg, Steve Rathje, and Stephan Lewandowsky. Psychological inoculation improves resilience against misinformation on social media. Science advances, 8(34):eabo6254, 2022. [27] Karen Santos-DAmorim and Májory Fernandes de Oliveira Miranda. Misinformation, disinformation, and malinformation: clarifying the definitions and examples in disinfodemic times. Encontros Bibli: revista eletrônica de biblioteconomia ciência da informação, 26, 2021. [28] Rahul Sharma. Adversarial robustness in llms: Defending against malicious inputs, June 2024. Accessed: 2025-05-20. [29] Lauren Sompayrac. How the immune system works. John Wiley & Sons, 2022. [30] Elham Tabassi. Artificial intelligence risk management framework (ai rmf 1.0), January 2023. Accessed: 2025-05-19. [31] Cecilie Traberg, Jon Roozenbeek, and Sander van der Linden. Psychological inoculation against misinformation: Current evidence and future directions. The ANNALS of the American Academy of Political and Social Science, 700(1):136151, 2022. [32] UNESCO. UNESCO - united nations educational, scientific and cultural organization, 2025. Accessed: 2025-01-21. [33] Nathan Walter, Jonathan Cohen, Lance Holbert, and Yasmin Morag. Fact-checking: meta-analysis of what works and for whom. Political communication, 37(3):350375, 2020. [34] Han Wang, An Zhang, Nguyen Duy Tai, Jun Sun, Tat-Seng Chua, et al. Ali-agent: Assessing llms alignment with human values via agent-based evaluation. Advances in Neural Information Processing Systems, 37:9904099088, 2024. [35] Claire Wardle and Hossein Derakhshan. Information disorder: Toward an interdisciplinary framework for research and policymaking, volume 27. Council of Europe Strasbourg, 2017. [36] White House Office of Science and Technology Policy. Blueprint for an ai bill of rights: Making automated systems work for the american people, October 2022. Accessed: 2025-05-19. [37] Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, and Liang Lin. Masked images are counterfactual samples for robust fine-tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2030120310, 2023. [38] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. [39] Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. Alleviating hallucinations of large language models through induced hallucinations. arXiv preprint arXiv:2312.15710, 2023. [40] Xinyi Zhou, Ashish Sharma, Amy Zhang, and Tim Althoff. Correcting misinformation on social media with large language model. arXiv preprint arXiv:2403.11169, 2024."
        },
        {
            "title": "A Related Work",
            "content": "Proactive Factuality Training growing body of work addresses LLM factuality and hallucinations. Conventional fine-tuning on fact-checked or truthful datasets can improve models correctness on specific benchmarks, but these approaches typically focus on teaching more truth rather than explicitly teaching what is false. For example, the TruthfulQA benchmark [17] revealed that even very large models often mimic human falsehoods when faced with tricky questions; subsequent efforts have attempted to fine-tune models to give correct answers on such challenging queries or to refuse to answer them [39]. This truthful-answer fine-tuning improves performance on TruthfulQA-style evaluations, but it does not explicitly train the model to recognize falsehoods as false it mainly reinforces correct outputs for known problematic questions. Alignment via Human Feedback: Techniques like Reinforcement Learning from Human Feedback (RLHF)[16] have demonstrated that models can be made more truthful and less prone to hallucination by optimizing on human-preference signals. For instance, InstructGPT[21] used RLHF to significantly reduce blatant falsehoods in outputs. However, RLHF is broad and resource-intensive alignment approach: it requires humans to rate or rank wide range of model outputs, and the resulting reward model indirectly encourages truthfulness among many other desirable traits (helpfulness, harmlessness, etc.). In contrast, model immunization offers more targeted intervention: it uses direct supervised signals on false claims themselves. Rather than relying on human feedback spread across many examples, we leverage existing databases of fact-checked misinformation (e.g. known myths and hoaxes) as training data. This is akin to traditional fine-tuning on specialized domain (false vs. true claims), potentially achieving focused gain in truthfulness with less overhead. Adversarial and Data-Augmentation Approaches: Adversarial training [12] hardens models by generating worst-case input perturbations (often imperceptible typos or paraphrases) and training on them to increase robustness. It is typically an iterative procedure that finds subtle adversarial examples to confuse the model. Model immunization, by contrast, is not necessarily iterative or adversary-generated it can use static, curated list of known falsehoods (including both real and AI-generated misinformation) labeled false. In classic adversarial training [19], the goal is to make models robust to cleverly perturbed inputs; in model immunization, the goal is to make models robust to an entire class of misleading content. Our vaccine dose may include any representative false claims (even if they were not tailored to the models current weaknesses), which are presented with clear labels and corrections so the model learns to reject them. Hallucination Reduction and Fact-Checking: variety of methods aim to reduce factual errors in generative models, such as integrating retrieval of trusted knowledge or post-editing outputs with fact-checkers [39, 14]. These typically act after the model has produced response, either by filtering/adjusting the output or by having the model verify its answer against external sources. Such approaches are complementary to model immunization. Our framework builds the resistance into the model itself during training, whereas post-hoc fact-checking or retrieval augmentation treat factuality as an external constraint. Another related concept is psychological inoculation [26] in human misinformation research, where exposing people to weakened doses of misinformation (or common misinformation techniques) can prebunk them against future falsehoods. Model immunization can be seen as bringing this inoculation principle into AI training. To our knowledge, no prior work has explicitly used false information as training data with negative labels to improve an AIs truthfulness. In summary, model immunization is unique in directly leveraging falsehood examples as supervised signals to bolster models immunity to misinformation, distinguishing it from earlier efforts that either avoid false data, correct it post-hoc, or rely on broad human feedback."
        },
        {
            "title": "B Case Study",
            "content": "Training and evaluation detail: We started with pre-trained LM (a 1.5-billion-parameter transformer) that had not been specifically optimized for truthfulness. The model was fine-tuned on quarantined falsehoods dataset consisting of 500 false statements drawn from fact-checking sources through our in-house curation. This dataset included widely circulated misinformation across health, science, and general knowledge domains (e.g., COVID-19 vaccines implant microchips, The Great Wall of China is visible from space with the naked eye, Lightning never strikes the same place twice, etc.). Each statement in this dataset was paired with brief refutation or correct answer. We fine-tuned the model for few epochs on these pairs using custom objective: the model was trained to produce the refutation when presented with the false claim as input. We also interleaved this fine-tuning with small amount of continued training on truthful QA dataset [17] (to retain general knowledge). For evaluation, we prepared two test sets: Misinformation Challenge Set: 100 questions or prompts that either explicitly contained false premise or were likely to elicit misconception. Many of these were different from the training falsehoods but in related domains. For example, question like Does 5G wireless technology spread COVID-19? (false), or What ancient civilization proved the Earth is flat? (a trick question, as the premise is false). General Knowledge Set: 100 straightforward factual questions or prompts (drawn from TriviaQA [13]) to assess normal performance. These had no misleading content e.g., Who wrote Pride and Prejudice? or What is the capital of Argentina?. We evaluated the original model (baseline) and the immunized model on both sets. For the misinformation set, responses were judged on truthfulness and correctness (we used human evaluators and reference key). For the general set, we checked accuracy against known answers. Qualitative example (misinformation prompt). Prompt: Do COVID-19 vaccines implant microchips? Baseline: Some people worry about that, but evidence is limited. . . Immunized: No. COVID-19 vaccines do not contain microchips; this is debunked conspiracy theory."
        },
        {
            "title": "C Comparison with related techniques",
            "content": "The comparison of model immunization with related methods is given in Table 2."
        },
        {
            "title": "D Ethical and goverance frameworks",
            "content": "The comparison of model immunization with related methods is given in Table 3. 13 Table 2: Comparison of Model Immunization with adversarial training, RLHF, and post-hoc filtering along key dimensions. Dimension Training data type Model Immunization Curated false statements (with false labels) + normal data Adversarial Training Adversarially perturbed inputs (worst-case examples) [18] Goal Inoculate model against falsehoods to improve truthfulness Resist malicious input perturbations (adversarial attacks) RLHF No added training uses human data; feedback outon puts [21] Align outputs with human preferences (helpful, truthful behavior) Examples used Known false claims labeled as false (quarantined data) Worst-case synthetic inputs designed to fool the model Human-annotated outputs (preferred vs disallowed) Post-hoc Filtering No change to base training data; uses separate detection model [17, 22] Identify and filter misinformation after generation N/A (filter may be trained on known misinformation instances) Granularity Fine-grained (fact-level inoculation on specific falsehoods) Fine-grained against input tweaks) (guards token-level Coarse-grained (broad reward on overall output) Coarse-grained (output-level filtering post-generation) Robustness to misinformation High (explicitly trained to reject known false patterns) Low (does not address misinformation content) Moderate courages falsehoods, misses specifics) (disobvious but Variable known misses novel cases) (catches falsehoods; Relation truthfulness to trains for Explicitly factual accuracy (distinguishes true vs false) No direct relation (focuses on robustness, not correctness) Indirect (truthfulness is one of many alignment criteria) Complementarity to immunization N/A (primary method) (handles input Yes complements attacks; content-focused immunization) Yes (adds targeted truthfulness training to broad RLHF alignment) External ment; own unchanged enforcemodels truthfulness Yes (prevents many false outputs, reducing burden on filters) 14 Table 3: Ethical and governance frameworks relevant to AI immunization against misinformation. Framework (Year) EU Trustworthy AI Ethics Guidelines [11] OECD AI Principles [20] Core Principle(s) Application to Model Immunization Benefits Risks / Ethical Tensions Lawfulness, robustness; oversight, parency, accountability. ethics, human transfairness, Human-centred, fairness, robustness, transparency, accountability. Calls for careful dataset curation, labeling, and audits of AI outputs; requires human-in-theloop review of flagged misinformation. Supports disclosure of AI-generated media, diverse training data, and human intervention in content systems. Builds public trust; mitigates bias and inadvertent propagation of falsehoods. Voluntary; trade-offs between transparency and privacy; high cost of human oversight. Provides global baseline; fosters explainability and bias reduction. tensions Non-binding; between transparency and security, and between misinformation control and free expression. UNESCO Ethics mendation [32] AI RecomHuman rights, dignity, transparency, fairness, oversight. U.S. Blueprint for an AI Bill of Rights [36] Safe & effective systems, algorithmicbias protection, data privacy, notice & explanation, human fallback. NIST AI Risk Management Framework [30] GovernMapMeasure Manage lifecycle for trustworthy AI. Promotes explainable AI, bias checks, and media-literacy policies alongside technical measures. Nearly universal endorsement; holistic approach coupling technical and societal defences. Implementation varies by nation; risk of state misuse for censorship; differing definitions of harmful content. Demands predeployment truthfulness audits, bias testing, transparent moderation, and appeal mechanisms. overlapping Creates safeguards accountability to catch misinformation errors. and Non-binding; cost of human oversight; freespeech concerns around unsafe content filtering. Treats misinformation as quantifiable risk; advises continuous testing, data-quality assurance, and model cards. Flexible, iterative process; standardises highquality risk controls across sectors. EU Artificial Intelligence Act [9] Binding, risk-based transregulation; parency, oversight, fundamental-rights protection. content laImposes belling (e.g., deep-fake watermarks), risk assessments, and humanoversight for plans high-risk systems. Enforceable fines drive adoption of best practices; harmonises standards across EU. for resourceVoluntary; small intensive developers; general guidance may miss domain-specific challenges. Compliance burden on SMEs; possible chilling effects on creative expression; limited reach outside EU. Table 4: Ethical principles and corresponding safeguards in model immunization. Ethical Principle Safeguard in Model Immunization Transparency No Promotion of False Content Alignment with Human Values Preventing Abuse Continuous Accountability Document and openly disclose all use of false data in training. Sources and labels of each quarantined falsehood are recorded for audit, aligning with calls for transparency [4]. Treat every curated false statement as negative example: pair it with corrective label or explanation and penalize the model for reproducing it, ensuring the system learns to reject rather than repeat misinformation. Focus curation on clearly harmful myths (e.g., health disinformation) backed by strong factual consensus. Gray-area topics are introduced only with human oversight, upholding fairness and inclusivity [16, 34]. Developers follow transparent protocols and, where relevant, regulatory guidelines [1]. Open documentation and shared best-practice falsehood datasets to distinguish responsible immunization from covert poisoning. Set up public appeal channel and log model outputs to allow external audits, ensuring errors can be reported and corrected post-deployment [7]."
        }
    ],
    "affiliations": [
        "Amazon Web Services",
        "University of Central Florida, Orlando, USA",
        "University of Groningen, Netherlands",
        "Vector Institute, Toronto, Canada"
    ]
}