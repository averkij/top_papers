{
    "paper_title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
    "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL"
        },
        {
            "title": "Start",
            "content": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography Shravan Venkatraman1* Pavan Kumar S1* Rakesh Raj Madavan2* Chandrakala S2 1Vellore Institute of Technology, Chennai, India 2Shiv Nadar University, Chennai, India 5 2 0 2 8 1 ] . e [ 1 2 0 1 4 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs global-tolocal analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertaintyguided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at https://github.com/shravan-18/UGPL. 1. Introduction Medical image classification plays central role in automated diagnosis and clinical decision support [11]. Deep learning and convolutional neural networks (CNNs) have shown effectiveness across various imaging modalities, including X-rays [21, 75], magnetic resonance imaging [47, 67], and computed tomography (CT) scans [17, 63]. Par- *Equal contribution. (a) Standard CNN: Single-pass analysis with uniform processing. (b) Bayesian CNN: Produces uncertainty maps but without focused refinement. (c) Uncertainty-Guided Progressive Learning (UGPL): Focuses computational resources on uncertain regions for enhanced classification. Figure 1. Comparison of medical image classification methods: unlike standard CNNs (1a) and Bayesian CNNs (1b) that process images uniformly, our proposed UGPL framework (1c) adaptively focuses on high-uncertainty regions for refined local analysis, combining global and local predictions via adaptive fusion. ticularly in CT image analysis, these approaches have achieved promising results for diagnosing pulmonary diseases [35, 49], abdominal abnormalities [5, 48], and COVID-19 infections [7, 61]. While these approaches achieve strong performance on benchmark datasets, they operate uniformly across all spatial regions, overlooking how radiologists selectively attend to diagnostically relevant areas. This limitation affects performance in cases where critical findings are localized and subtle. Conventional CNNs apply identical convolutions across the image, ignoring regional diagnostic value. This is limiting factor in medical imaging, where abnormalities may occupy small fraction of the image. For instance, lung nodules or renal cysts often appear in confined regions that can be missed under uniform processing. Increasing model Figure 2. The UGPL architecture pipeline. Our framework processes an input CT image through global uncertainty estimator to produce classification probabilities and an uncertainty map (left). The progressive patch extractor selects high-uncertainty regions for detailed analysis (center). These patches are processed by local refinement network and combined with global predictions through an adaptive fusion module (right). Multiple loss functions (CE, UCC, CL, PDL, REG) are jointly optimized to ensure effective training of all components. capacity or resolution globally is possible workaround, but it incurs significant computational costs and delays in inference, both critical concerns in clinical practice. Moreover, such methods do not adapt their assessment based on uncertainty, unlike real practice, where analysis is refined based on perceived ambiguity. This gap in spatial adaptivity limits current models from capturing diagnostically important features in complex cases. Several approaches have attempted to address aspects of this problem. Attention mechanisms [26, 66, 69] and Region-based CNNs [39, 40, 50] enable models to focus on specific image regions, but they typically identify regions based on learned patterns rather than diagnostic uncertainty. Bayesian neural networks [6, 16, 58] and Monte Carlo dropout techniques [9, 24, 71] offer uncertainty quantification in medical image analysis, producing pixel-wise uncertainty maps that highlight ambiguous regions. However, these methods primarily use uncertainty for confidence estimation or out-of-distribution detection, but not as analysis feedback. Progressive approaches in computer vision [14, 52, 64] process images in multiple stages of increasing resolution, but follow predetermined schedules rather than adapting based on detected uncertainty. While these methods offer partial solutions, they fail to integrate such uncertainty estimations with subsequent analysis refinement as done in practice. In this paper, we introduce Uncertainty-Guided Progressive Learning (UGPL), novel framework that mimics diagnostic behavior by performing global analysis followed by focused examination of uncertain regions (Figure 1). UGPL addresses limitations of uniform processing by dynamically allocating computational resources where needed. Our framework first employs global uncertainty estimator to perform initial classification and generate pixel-wise uncertainty maps, then selects high-uncertainty regions for detailed analysis through local refinement network. These multi-resolution analyses are combined via an adaptive fusion module that weights predictions based on confidence. Unlike existing methods that treat uncertainty merely as an output signal, UGPL explicitly uses it to guide computational focus, maintaining efficiency while improving performance on diagnostically challenging regions. As shown in Figure 2, UGPL processes the input CT image to produce both classification probabilities and an uncertainty map that guides the extraction of high-uncertainty patches using non-maximum suppression. Each patch undergoes high-resolution analysis through local refinement network, producing patch-specific classification scores and confidence estimates. The adaptive fusion module then integrates global and local predictions using learned weights based on their estimated reliability. Multiple specialized loss functions are jointly optimized, guiding components to work in tandem, adapt according to diagnostic difficulty, and improve performance over uniform processing. To summarize, our main contributions are: novel uncertainty-guided progressive learning framework that dynamically allocates computational resources to regions of high diagnostic ambiguity, an evidential deep learning (EDL) approach that provides principled uncertainty quantification through Dirichlet distributions, an adaptive patch extraction mechanism with nonmaximum suppression that overlapping regions for detailed analysis, and selects diverse, nonrays [2]. multi-component loss formulation that jointly optimizes classification accuracy, uncertainty calibration, and spatial diversity. 2. Related Works Evidential Deep Learning in Medical Imaging. EDL [54] has been applied to various medical imaging tasks to model uncertainty and improve reliability. Early work integrated Dempster-Shafer Theory [15] into encoder-decoder architectures for 3D lymphoma segmentation, using voxel-level belief functions to improve accuracy and calibration over standard UNets [33]. In radiotherapy dose prediction, EDL showed that epistemic uncertainty correlates with prediction error, enabling confidence interval estimation for dose-volume histograms [59]. Extensions include regionbased EDL with Dirichlet modeling for brain tumor delineation [41] and multi-view fusion architectures combining foundation models with uncertainty-aware layers to handle boundary ambiguity [31]. Methods like EVIL [12, 13] introduced efficient semisupervised segmentation via uncertainty-guided consistency training, filtering unreliable pseudo-labels, and achieved strong results on ACDC and MM-WHS datasets. Multimodal and semi-supervised EDL variants further enhanced reliability. Dual-level evidential networks [56] and contextual discounting strategies [34, 73] model modality trust in PET-CT and MRI fusion, improving voxel-level interpretability in tumor segmentation. Tri-branch frameworks like ETC-Net integrate evidential guidance with co-training to stabilize pseudo-labels in low-annotation regimes [72]. Beyond segmentation, EDL has been applied to classification, including three-way decision-making with EviDCNN [70] and out-of-distribution detection using evidential reconcile blocks [23], demonstrating its versatility in uncertainty-aware diagnostics. Uncertainty Quantification in Medical Image Analysis. Uncertainty quantification is widely used in medical image analysis to enhance model reliability amid noisy inputs, ambiguous boundaries, and limited annotations. Multidecoder U-Net architectures capture inter-expert variability and generate uncertainty-calibrated segmentations [68], while probabilistic U-Nets model aleatoric and epistemic uncertainties from annotation variability [30]. For classification, Bayesian deep learning models [6] like UAConvNet and BARF use Monte Carlo dropout [24] to estimate predictive uncertainty, achieving strong COVID-19 detection from chest X-rays [1, 28]. Transfer learning quantifies epistemic uncertainty across modalities, detecting shifts between CT and X-rays [55]. Uncertainty-aware attention in hierarchical fusion networks, as in Hercules, improves performance across OCT, lung CT, and chest XIn reconstruction, Bayesian deep unrolling jointly models image formation and uncertainty for MRI and CT [20]. Multimodal regression models like MoNIG estimate modality-specific uncertainties via Normal-Inverse Gamma mixtures for adaptive trust calibration [45]. For high-risk tasks such as COVID-19 classification, RCoNet combines mutual information maximization with ensemble dropout for robustness under distributional noise [18]. Distance-based out-of-distribution detection helps identify unreliable lung lesion segmentations [27]. Joint prediction confidence estimation aids data filtering and performance improvements in chest radiograph interpretation and ultrasound view classification [25], underscoring the role of uncertainty quantification in reliable medical AI. 3. Method 3.1. Background Medical image classification requires both global contextual understanding and detailed examination of localized abnormalities. In this section, we establish the mathematical foundations for our uncertainty-guided approach. Evidential Deep Learning. Traditional deep learning classifiers output class probabilities p(yx) directly but lack principled uncertainty quantification. Evidential Deep Learning (EDL) [54] addresses this by modeling distribution over probabilities. For classification problem with classes, EDL parameterizes Dirichlet distribution Dir(pα) over the probability simplex, where α = [α1, α2, ..., αC] are concentration parameters: Dir(pα) = 1 B(α) (cid:89) i=1 pαi1 (1) Here, B(α) is the multivariate beta function. The concentration parameters α can be interpreted as evidence for each class, with αi = ei + 1 where ei 0 represents the evidence for class i. The expected probability for class is , where = (cid:80)C given by E[pi] = αi i=1 αi is the Dirichlet strength. EDL enables the quantification of two types of uncertainty: aleatoric uncertainty (data uncertainty) and epistemic uncertainty (model uncertainty). For Dirichlet distribution, the total predictive uncertainty can be computed as: Utotal = (cid:88) i=1 αi (cid:16) 1 (cid:17) αi 1 + 1 (2) This captures both the entropy of the expected categorical distribution (first term) and the additional uncertainty from the Dirichlet distribution itself (second term). Figure 3. Architecture of the Local Refinement Network. The network processes extracted patches (p1, p2, ..., pn) through patch encoder comprising four convolutional blocks with increasing feature dimensions (64128256256) and adaptive average pooling. The encoded features are then fed into two parallel heads: classification head that produces patch-specific logits, and confidence estimation head that generates confidence scores used for weighted fusion of patch predictions. 3.2. Global Uncertainty Estimation and Evidential"
        },
        {
            "title": "Learning",
            "content": "Our global uncertainty estimator produces initial class predictions and generates spatial uncertainty map to guide patch selection through evidential learning. Global Model Architecture. Given an input CT image RHW 1, we employ ResNet backbone [29] Fθ to extract feature maps Rhwd. To accommodate grayscale CT images, we modify the first convolutional layer to accept single-channel inputs while preserving pretrained weights by averaging across RGB channels. The feature maps are processed by two parallel heads: classification head Cϕ and an evidence head Eψ. The classification head applies global average pooling followed by fully connected layer to produce class logits: zg = Cϕ(F). Evidential Uncertainty Estimation. The evidence head Eψ generates pixel-wise Dirichlet concentration parameters that quantify uncertainty at each spatial location: = Eψ(F) Rhw4C, where encodes four parameters (α, β, γ, ν) for each class at each location. Following subjective logic principles [37], we parameterize the Dirichlet distribution as: The first term accounts for aleatoric uncertainty, and the second for epistemic uncertainty. We normalize the uncertainty map to [0, 1] for easier interpretation and subsequent processing. 3.3. Uncertainty-Guided Patch Selection and Local"
        },
        {
            "title": "Refinement",
            "content": "Progressive Patch Extraction. Given an input image RHW 1 and its corresponding uncertainty map ˆU Rhw, we first upsample the uncertainty map to match the input resolution: = U( ˆU, (H, )). Our objective is to extract patches of size from the input image based on an uncertainty-guided selection process. We formulate this as greedy algorithm that selects patches from the original image at locations corresponding to maxima in the uncertainty map U. The first patch is centered at the global maximum uncertainty: (x1, y1) = arg max (x,y) x:x+P,y:y+P (5) For subsequent patches, we introduce spatial penalty term that encourages diversity by maintaining minimum distance from previously selected locations: αi,j,c = βi,j,c νi,j,c + 1 (3) (xk, yk) = arg max (x,y) (cid:104) x:x+P, y:y+P where βi,j,c represents the inverse of uncertainty, νi,j,c represents the mass belief, and we constrain (cid:80)C c=1 νi,j,c = 1. From these parameters, we compute the pixel-wise uncertainty map Rhw by aggregating uncertainty across all classes: Ui,j = 1 (cid:88) (cid:18) 1 αi,j,c c=1 + βi,j,c αi,j,c(αi,j,c + 1) (cid:19) (4) λ min i<k d((x, y), (xi, yi)) (cid:105) (6) This sequential optimization ensures that each new patch maximizes uncertainty while preventing redundant selection of nearby regions. We implement this efficiently using non-maximum suppression approach, applying Gaussian suppression kernel after selecting each patch. Algorithm 1 details our complete patch extraction procedure, including practical considerations for edge cases. Algorithm 1 Uncertainty-Guided Patch Extraction Require: Input image RHW 1, Uncertainty map ˆU Rhw, Patch size , Number of patches Ensure: Set of patches {P1, P2, . . . , PK}, Patch coordinates {(x1, y1), (x2, y2), . . . , (xK, yK)} 1: U( ˆU, (H, )) {Upsample uncertainty map} 2: Initialize patch coordinates list {} 3: zeros(H, ) {Mask for selected regions} 4: for = 1 to do 5: (1 M) {Apply mask to uncertainty 6: 7: 8: 9: map} if max(V) > 0 then (yk, xk) arg max(y,x) {Find maximum uncertainty location} else (yk, xk) random valid location {Fallback: random selection} 10: 11: end if xk max(0, min(xk, )) {Ensure patch fits within image} yk max(0, min(yk, )) 12: {(xk, yk)} {Add coordinates to list} 13: 14: MykM :yk+P +M,xkM :xk+P +M 1 {Update mask with margin } 15: Pk Iyk:yk+P,xk:xk+P {Extract patch} 16: 17: if Pk size = (P, ) then Pk Resize(Pk, (P, )) {Ensure consistent size} end if 18: 19: end for 20: return {P1, P2, . . . , PK}, Local Refinement Network.After extracting patches, we process each independently using local refinement network with three components (Figure 3): feature extractor, classification head, and confidence estimation head. The feature extractor Lf processes each patch to obtain local feature vectors: fk = Lf (Pk) Rdl . The classification head maps these features to class logits: zl,k = Lc(fk) RC, while the confidence estimation head produces scalar confidence score: ck = Lconf(fk) [0, 1]. The confidence score allows the model to express uncertainty about individual patch predictions and weights their contribution in the final classification. The combined local prediction is computed as confidence-weighted averk=1 ckzl,k k=1 ck+ϵ , where ϵ is small constant for nuage: zl = merical stability. This naturally reduces the contribution of low-confidence patches, functioning as an implicit attention mechanism that focuses on the most discriminative regions. (cid:80)K (cid:80)K 3.4. Adaptive Fusion and Training Objectives Adaptive Fusion Module. Given the global logits zg RC and uncertainty map ˆU Rhw from the global model, and local logits zl RC with patch confidence scores {c1, c2, . . . , cK} from the local refinement network, our adaptive fusion module dynamically balances global and local predictions. i=1 j=1 (cid:80)w We compute scalar global uncertainty ug = (cid:80)h ˆUi,j to quantify the overall confidence of 1 hw the global model. The fusion network Fω takes as input [zg, ug] and outputs fusion weight wg = Fω([zg, ug]), implemented as multi-layer perceptron with sigmoid activation. The fused logits are computed as zf = wg zg + (1 wg) zl. This adaptive weighting relies more on global features when the global model is confident, and more on local features when uncertainty is high. Multi-component Loss Function. Our training uses comprehensive loss function combining several objectives: Ltotal = λf Lfused + λgLglobal + λlLlocal + λuLuncertainty + λcLconsistency + λconfLconfidence + λdLdiversity (7) Classification Losses. We apply cross-entropy loss to predictions from each component: Lfused for the fused predictions, Lglobal for global predictions, and Llocal averaged across all patch predictions. Auxiliary Losses. We also use several auxiliary com- (1) Luncertainty caliponents to ensure effective training: brates the uncertainty map to reflect prediction errors; (2) Lconsistency promotes agreement between global and local predictions using KL divergence weighted by patch confidence; (3) Lconfidence aligns patch confidence scores with prediction accuracy; and (4) Ldiversity encourages diversity among patch predictions through cosine similarity penalization. 4. Experiments 4.1. Experimental Setup Datasets. We conduct experiments on three CT image datasets: the kidney disease diagnosis dataset [36] (multiclass: normal, cyst, tumor, stone), the IQ-OTH/NCCD lung cancer dataset [3, 4, 22] (multiclass: benign, malignant, normal), and the UCSD-AI4H COVID-CT dataset [74] (binary: COVID, non-COVID). All images are resized to 256 256 resolution during preprocessing and normalized using the respective datasets mean and standard deviation. Implementation Details. All models were trained for 100 epochs using Adam optimizer [38] with learning rate 1 104, weight decay 1 104, batch size 96, and cosine Table 1. Comparison of our UGPL approach with state-of-the-art classification models across three CT datasets. Results on the COVID dataset for CRNet [74] are as reported in the paper. Best results are in red, second-best in blue, and third-best in green. Models ShuffleNetV2 [46] VGG16 [57] ConvNeXt [43] DenseNet121 [32] DenseNet201 [32] EfficientNetB0 [60] MobileNetV2 [53] ViT [19] Swin [42] DeiT [62] CoaT [65] CrossViT [10] CRNet [74] UGPL (Ours) Kidney Abnormalities F1 0.95 0.0092 0.88 0.0173 0.80 0.0195 0.93 0.0118 0.94 0.0106 0.94 0.0089 0.85 0.0195 0.92 0.0167 0.40 0.0421 0.90 0.0178 0.98 0.0072 0.97 0.0094 - 0.99 0. Accuracy 0.96 0.0085 0.89 0.0156 0.81 0.0189 0.94 0.0102 0.95 0.0093 0.95 0.0078 0.87 0.0179 0.94 0.0154 0.68 0.0298 0.92 0.0162 0.98 0.0067 0.97 0.0087 - 0.99 0.0023 Lung Cancer Type F1 0.91 0.0143 0.91 0.0165 0.95 0.0084 0.89 0.0176 0.83 0.0218 0.95 0.0073 0.69 0.0283 0.22 0.0456 0.41 0.0398 0.46 0.0387 0.93 0.0112 0.39 0.0423 - 0.97 0.0052 Accuracy 0.94 0.0127 0.95 0.0098 0.95 0.0076 0.90 0.0171 0.84 0.0203 0.95 0.0081 0.70 0.0267 0.51 0.0389 0.60 0.0334 0.66 0.0312 0.95 0.0089 0.58 0.0356 - 0.98 0.0047 COVID Presence Accuracy 0.69 0.0234 0.48 0.0287 0.61 0.0267 0.78 0.0198 0.76 0.0206 0.73 0.0221 0.70 0.0241 0.56 0.0312 0.53 0.0331 0.44 0.0356 0.68 0.0254 0.62 0.0289 0.73 0.0218 0.81 0.0134 F1 0.67 0.0251 0.47 0.0306 0.59 0.0278 0.76 0.0213 0.74 0.0229 0.71 0.0238 0.68 0.0256 0.55 0.0318 0.53 0.0329 0.35 0.0412 0.66 0.0267 0.48 0.0378 0.76 0.0203 0.79 0. Table 2. Analysis of individual component performance in our UGPL framework across the three datasets. The shaded row corresponds to our baseline configuration. Model Type Global Model Local Model Fused Model COVID Presence F1 Accuracy 0.7078 0.7108 0.6343 0.6486 0.7903 0.8108 Lung Cancer Type Kidney Abnormalities Accuracy 0.9617 0.5122 0.9817 Accuracy 0.9811 0.4057 0. F1 0.9746 0.1443 0.9946 F1 0.9611 0.2258 0.9764 with varying patch configurations. The multi-component loss function employed weighted components for fused (1.0), global/local (0.5), uncertainty (0.3), consistency (0.2), and confidence/diversity losses (0.1). 4.2. Performance Evaluation range of CNN and Table 1 shows the performance of our method against transformer-based models. These include lightweight CNNs (MobileNetV2 [53], ShuffleNetV2 [46]), standard convolutional baselines (VGG16 [57], DenseNet121/201 [32], EfficientNetB0 [60], ConvNeXt transformer-based architectures (ViT [19], Swin [42], DeiT [62], CoaT [65], CrossViT [10]). We compare our UGPL approach across three CT classification tasks. For all models, we report accuracy, macro-averaged F1 score, and include ROC-AUC visualizations for further analysis. [43]), and recent On the kidney abnormality dataset [36], UGPL achieves the highest accuracy and F1-score at 99% (0.0023, 0.0031). Among CNNs, CoaT [65], CrossViT [10], and EfficientNetB0 [60] follow with F1 between 9498%, all with low variance. MobileNetV2 [53] and VGG16 [57] fall below 89%. Transformer models like ViT [19] and Swin [42] show lower F1 and higher deviations, with Swin dropping to 40% F1 (0.0421). On the IQ-OTH/NCCD dataset [3, 4, 22], UGPL reports 97% F1 (0.0052), the highest overall. CNNs such as EfficientNetB0 [60] and ConvNeXt [43] reach 95% F1 with low variance. CoaT [65] and VGG16 [57] follow closely, while transformer models like DeiT [62] and Swin [42] perform poorly, with F1 below 50% and higher spread. Variance is generally higher for transformers, with less consistent learning across folds. For COVID classification, UGPL leads with 79% F1 Figure 4. Performance trends of model components across datasets. Accuracy (x-axis) and F1 score (y-axis) define trajectories from LM to GM to FM, with contour lines indicating performance density. decay scheduling [44]. Standard augmentations included flips, rotations, affine transformations, and contrast adjustments. Dataset-specific ResNet [29] backbones were used Figure 5. ROC curves comparing global and fused model performance across datasets. The FM consistently maintains or improves the already high AUC values of the GM across all classes and datasets. Table 3. Ablation study of different model component configurations across the three datasets. The shaded row corresponds to our baseline configuration. Configuration Global-only No UG Fixed Patches Full Model COVID Presence F1 Accuracy 0.1495 0.2535 0.1536 0.2363 0.1533 0.2347 0.7903 0. Lung Cancer Type Kidney Abnormalities Accuracy 0.5000 0.4634 0.4573 0.9817 Accuracy 0.5676 0.5766 0.5766 0.9971 F1 0.5545 0.5558 0.5697 0.9945 F1 0.3890 0.3764 0.3731 0.9764 (0.0147), followed by DenseNet121 [32] and CRNet [74] at 76%. EfficientNetB0 [60] and DenseNet201 [32] also perform in the 7174% range. Most transformer-based models, including ViT [19], Swin [42], and DeiT [62], remain under 60% F1 with variances exceeding 0.03. These models also show less consistency across folds, with notably higher performance fluctuations. 4.3. Component Analysis Table 2 shows the contribution of each component in our UGPL framework. The global model (GM), performing whole-image classification, achieves strong performance on the Kidney and Lung datasets (98.11% and 96.17% accuracy). The local model (LM), using only patch-based classification, shows significantly lower performance when used independently. The fused model (FM), integrating both predictions through our adaptive fusion mechanism, consistently outperforms individual components. The performance gap between GM and FM is most evident in COVID-19 detection, with FM reaching 81.08% accuracy compared to 71.08% for GM. This reflects the benefit of incorporating localized analysis in tasks where global patterns are less prominent. For kidney abnormality detection, FM also improves over GM (99.71% vs. 98.11%), showing that local refinement can still enhance outcomes even when global features are already effective. The LM performs poorly across all tasks, particularly for kidney abnormalities (40.57%) and lung cancer classification (51.22%), as local patches alone lack sufficient context and focus on irrelevant regions without global guidance. Figure 4 shows performance trends across tasks, with COVID-19 detection showing the most significant gains from LM to FM. ROC curves (Figure 5) show that for COVID-19, GM and FM achieve similar AUC scores (0.901 vs. 0.900). For lung cancer, FM achieves slight improvements across classes, especially for benign cases (0.991 vs. 0.992). For kidney cases, FM improves performance for most classes, including kidney stones (0.984 vs. 0.986). 4.4. Ablation Study We conduct extensive ablation studies to evaluate the impact of different components in our UGPL framework. We focus on three key aspects: 1) the contribution of each component in the progressive learning pipeline, 2) the influence of patch extraction parameters on model performance, and 3) the effect of varying loss term coefficients in our multi-component optimization objective. We retain the best-performing ResNet variant [29] from our initial evaluations for all experiments. 4.4.1. Component Ablation To analyze the contribution of each component in our progressive learning framework, we compare four configurations: (1) global-only setup that uses the global uncertainty estimator without local refinement; (2) no uncertainty guidance (No UG) variant, where patches are selected randomly instead of using uncertainty maps; (3) fixed patches configuration that uses predefined patch locations rather than adaptive selection; and (4) the full model, which includes all components of the UGPL framework. Table 3 shows our full model consistently outperforming all reduced variants by substantial F1 margins. On the COVID dataset, all ablations cause dramatic performance drops, with the global-only variant achieving only 14.95% Table 4. Performance comparison of different loss weight configurations across datasets. Loss component weights: Fused (λf ), Global (λg), Local (λl), Uncertainty (λu), Consistency (λc), Confidence (λco), Diversity (λd). Configuration C1 represents our baseline model with balanced weights. Best results are in red, second-best in blue, and third-best in green. Configuration C1: Baseline C2: Local Emphasis C3: Global-Centric C4: Uncertainty Focus C5: Consistency-Driven C6: Balanced High C7: Diversity-Enhanced C8: Confidence-Calibrated C9: Conservative C10: Aggressive λf 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.5 2.0 λg 0.5 0.3 0.7 0.5 0.5 0.5 0.5 0.5 0.25 1.0 Loss Weights λc λu 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.6 0.5 0.3 0.4 0.4 0.2 0.3 0.2 0.3 0.1 0.15 0.4 0.6 λl 0.5 0.7 0.3 0.5 0.5 0.5 0.5 0.5 0.25 1.0 λco 0.1 0.1 0.1 0.1 0.1 0.2 0.1 0.4 0.05 0.2 λd 0.1 0.1 0.1 0.1 0.1 0.2 0.4 0.1 0.05 0. COVID Presence F1 Accuracy 0.7903 0.8108 0.7758 0.7946 0.7402 0.7568 0.8057 0.8243 0.7689 0.7892 0.7836 0.8051 0.7569 0.7784 0.7798 0.7973 0.7312 0.7486 0.7827 0.8023 Lung Cancer Type Kidney Abnormalities Accuracy 0.9817 0.9695 0.9634 0.9756 0.9786 0.9801 0.9667 0.9753 0.9581 0.9728 Accuracy 0.9971 0.9928 0.9876 0.9953 0.9913 0.9942 0.9895 0.9923 0.9837 0.9932 F1 0.9945 0.9903 0.9832 0.9931 0.9889 0.9918 0.9856 0.9891 0.9803 0.9907 F1 0.9764 0.9641 0.9576 0.9687 0.9723 0.9739 0.9602 0.9695 0.9524 0.9674 Table 5. F1 Scores across patch sizes and number of extracted patches. Bolded values indicate results from C1 configuration. Patch Size Patches Kidney Lung COVID 32 64 2 3 4 2 3 4 2 3 4 0.9586 0.9673 0.9541 0.9824 0.9945 0.9765 0.9622 0.9701 0. 0.8869 0.9195 0.8756 0.9764 0.8671 0.9343 0.8712 0.9099 0.8717 0.7161 0.7368 0.7454 0.7521 0.7368 0.7903 0.7372 0.7262 0. F1. For lung cancer detection, the full model obtains 97.64% F1, while the global-only setup drops to 34.19%. The kidney dataset shows smaller yet significant gaps, with the full model reaching 99.6% F1 versus 58.7% for the best ablated configuration (fixed patches). Interestingly, No UG and fixed patches sometimes perform worse than the globalonly model, showing that naively adding local components without proper guidance can be detrimental and highlighting the importance of uncertainty-guided patch selection. 4.4.2. Loss Component Weights 0.3, consistency: Table 4 compares ten loss weight configurations across datasets. The baseline configuration (C1) with balanced weights performs best overall (fused: 1.0, global/local: 0.5 each, uncertainty: 0.2, confidence/diversity: 0.1 each). Configurations emphasizing either global or local branches underperform, confirming the necessity of combining global context with local detail. Increased uncertainty weighting (C4) improves COVID detection (82.43% accuracy, 80.57% F1) but slightly reduces performance on Lung and Kidney datasets where target features are more prominent. C5 (Consistency-Driven) excels on the Lung dataset (97.86% accuracy) where structural patterns are clearer, while uniform scaling of all components (C9 & C10) shows no improvement, indicating that relative balance matters more than absolute weight values. 4.4.3. Patch Extraction Parameters Table 5 presents F1 scores for different combinations of patch sizes and counts across datasets. Optimal configurations vary by task: kidney abnormality detection performs best with 6464 patches and 3 patches per image (F1 = 0.9945), lung cancer classification with 6464 and 2 patches (F1 = 0.9764), and COVID-19 detection with 6464 and 4 patches (F1 = 0.7903). patch size of 64 consistently outperforms both smaller (32) and larger (96) sizes, suggesting it provides an optimal balance between local detail and contextual information. The number of required patches aligns with each tasks complexity - COVID detection needs more regions due to diffuse disease manifestations, while lung cancer classification can focus on fewer, more localized abnormalities. 5. Conclusion framework for medical This paper proposed UGPL (Uncertainty-Guided Progressive Learning), image classification that analyzes CT images in two stages: global prediction with uncertainty estimation, followed by local refinement on selected high-uncertainty regions. Our evidential learning-based uncertainty estimation identifies diagnostically challenging areas, while the adaptive fusion mechanism combines global and local predictions based on confidence measures. Extensive experiments across three diverse CT classification tasks (COVID-19 detection, lung cancer classification, and kidney abnormality identification) demonstrate that UGPL significantly outperforms existing methods. Ablations show that the uncertainty-guided patch selection yields upto 5.3 F1 improvement compared to other configurations. Future work will explore extending UGPL to other modalities like MRI/PET and investigating its potential for uncertainty-guided active learning."
        },
        {
            "title": "References",
            "content": "[1] Moloud Abdar, Mohammad Amin Fahami, Satarupa Chakrabarti, Abbas Khosravi, Paweł Pławiak, U. Rajendra Acharya, Ryszard Tadeusiewicz, and Saeid Nahavandi. Barf: new direct and cross-based binary residual feature fusion with uncertainty-aware module for medical image classification. Information Sciences, 577:353378, 2021. 3 [2] Moloud Abdar, Mohammad Amin Fahami, Leonardo Rundo, Petia Radeva, Alejandro F. Frangi, U. Rajendra Acharya, Abbas Khosravi, Hak-Keung Lam, Alexander Jung, and Saeid Nahavandi. Hercules: Deep hierarchical attentive multilevel fusion model with uncertainty quantification for medical image classification. IEEE Transactions on Industrial Informatics, 19(1):274285, 2023. 3 [3] Muayed Al-Huseiny, Furat Mohsen, Enam Khalil, Zainab Hassan, Hamdalla Fadil, and Hamdalla F. Al-Yasriy. Evaluation of svm performance in the detection of lung cancer in marked ct scan dataset. Indonesian Journal of Electrical Engineering and Computer Science, 21, 2021. 5, 6 [4] Hamdalla Alyasriy. The iq-othnccd lung cancer dataset, 2020. 5, 6 [5] Samayan Bhattacharya, Avigyan Bhattacharya, and Sk Shahnawaz. Generating synthetic computed tomography (ct) images to improve the performance of machine learning model for pediatric abdominal anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38653873, 2023. [6] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks, 2015. 2, 3 [7] Syed Usama Khalid Bukhari, Syed Safwan Khalid Bukhari, Asmara Syed, and Syed Sajid Hussain Shah. The diagnostic evaluation of convolutional neural network (cnn) for the assessment of chest x-ray of patients infected with covid-19. MedRxiv, pages 202003, 2020. 1 [8] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information, 11(2):125, 2020. 5 [9] Paulo Chagas, Luiz Souza, Izabelle Pontes, Rodrigo Calumby, Michele Angelo, Angelo Duarte, Washington LcDos Santos, and Luciano Oliveira. Uncertainty-aware membranous nephropathy classification: monte-carlo dropout approach to detect how certain is the model. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 11(3):288298, 2023. 2 [10] Chun-Fu (Richard) Chen, Quanfu Fan, and Rameswar Panda. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification. In International Conference on Computer Vision (ICCV), 2021. [11] Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa Thai, Kathleen Moore, Robert Mannel, Hong Liu, Bin Zheng, and Yuchen Qiu. Recent advances and clinical applications of deep learning in medical image analysis. Medical image analysis, 79:102444, 2022. 1 [12] Yingyu Chen, Ziyuan Yang, Chenyu Shen, Zhiwen Wang, Yang Qin, and Yi Zhang. Evil: Evidential inference learning for trustworthy semi-supervised medical image segmenIn 2023 IEEE 20th International Symposium on tation. Biomedical Imaging (ISBI), pages 15, 2023. 3 [13] Yingyu Chen, Ziyuan Yang, Chenyu Shen, Zhiwen Wang, Zhongzhou Zhang, Yang Qin, Xin Wei, Jingfeng Lu, Yan Liu, and Yi Zhang. Evidence-based uncertainty-aware semisupervised medical image segmentation. Computers in Biology and Medicine, 170:108004, 2024. 3 [14] Junlong Cheng, Chengrui Gao, Hongchun Lu, Zhangqiang Ming, Yong Yang, and Min Zhu. Pl-net: Progressive learning network for medical image segmentation, 2022. 2 [15] A. P. Dempster. Upper and lower probabilities induced by multivalued mapping. Annals of Mathematical Statistics, 38 (2):325339, 1967. 3 [16] Aykut Diker. An efficient model of residual based convolutional neural network with bayesian optimization for the classification of malarial cell images. Computers in Biology and Medicine, 148:105635, 2022. [17] A.F. Dima et al. 3d arterial segmentation via single 2d projections and depth supervision in contrast-enhanced ct images. In Medical Image Computing and Computer Assisted Intervention MICCAI 2023. Springer, Cham, 2023. 1 [18] Shunjie Dong, Qianqian Yang, Yu Fu, Mei Tian, and Cheng Zhuo. Rconet: Deformable mutual information maximization and high-order uncertainty-aware learning for robust covid-19 detection. IEEE Transactions on Neural Networks and Learning Systems, 32(8):34013411, 2021. 3 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 6, 7 [20] Canberk Ekmekci and Mujdat Cetin. Uncertainty quantification for deep unrolling-based computational imaging. IEEE Transactions on Computational Imaging, 8:1195 1209, 2022. 3 [21] Mehrdad Eshraghi Dehaghani, Amirhossein Sabour, Amarachi B. Madu, Ismini Lourentzou, and Mehdi Moradi. Representation Learning with Transformer-Based Detection Model for Localized Chest X-Ray Disease and Progression Detection . In proceedings of Medical Image Computing and Computer Assisted Intervention MICCAI 2024. Springer Nature Switzerland, 2024. 1 [22] Hamdalla F. Al-Yasriy, Muayed Al-Huseiny, Furat Mohsen, Enam Khalil, and Zainab Hassan. Diagnosis of lung cancer based on ct scans using cnn. IOP Conference Series: Materials Science and Engineering, 928:022035, 2020. 5, 6 [23] Wei Fu, Yufei Chen, Wei Liu, Xiaodong Yue, and Chao Ma. Evidence reconciled neural network for out-of-distribution detection in medical images. In Medical Image Computing and Computer Assisted Intervention MICCAI 2023, pages 305315, Cham, 2023. Springer Nature Switzerland. 3 [24] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning, 2016. 2, [25] Florin C. Ghesu, Bogdan Georgescu, Awais Mansoor, Youngjin Yoo, Eli Gibson, R.S. Vishwanath, Abishek Balachandran, James M. Balter, Yue Cao, Ramandeep Singh, Subba R. Digumarthy, Mannudeep K. Kalra, Sasa Grbic, and Dorin Comaniciu. Quantifying and leveraging predictive uncertainty for medical image assessment. Medical Image Analysis, 68:101855, 2021. 3 [26] Tiago Goncalves, Isabel Rio-Torto, Luıs Teixeira, and Jaime Cardoso. survey on attention mechanisms for medical applications: are we moving toward better algorithms? IEEE Access, 10:9890998935, 2022. 2 [27] Camila Gonzalez, Karol Gotkowski, Moritz Fuchs, Andreas Bucher, Armin Dadras, Ricarda Fischbach, Isabel Jasmin Kaltenborn, and Anirban Mukhopadhyay. Distance-based detection of out-of-distribution silent failures for covid-19 lung lesion segmentation. Medical Image Analysis, 82: 102596, 2022. 3 [28] Mahesh Gour and Sweta Jain. Uncertainty-aware convolutional neural network for covid-19 x-ray images classification. Computers in Biology and Medicine, 140:105047, 2022. 3 [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. 4, 6, 7, 1, 5 [30] Shi Hu, Daniel Worrall, Stefan Knegt, Bas Veeling, Henkjan Huisman, and Max Welling. Supervised uncertainty quantification for segmentation with multiple annotations. In Medical Image Computing and Computer Assisted Intervention MICCAI 2019: 22nd International Conference, Shenzhen, China, October 1317, 2019, Proceedings, Part II, page 137145, Berlin, Heidelberg, 2019. Springer-Verlag. 3 [31] Chao Huang, Yushu Shi, Waikeung Wong, Chengliang Liu, Wei Wang, Zhihua Wang, and Jie Wen. Multi-view evidential learning-based medical image segmentation. Proceedings of the AAAI Conference on Artificial Intelligence, 39 (16):1738617394, 2025. [32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 47004708, 2017. 6, 7 [33] Ling Huang, Su Ruan, Pierre Decazes, and Thierry Denœux. Lymphoma segmentation from 3d pet-ct images using deep International Journal of Approximate evidential network. Reasoning, 149:3960, 2022. 3 [34] Ling Huang, Su Ruan, Pierre Decazes, and Thierry Denœux. Deep evidential fusion with uncertainty quantification and reliability learning for multimodal medical image segmentation. Information Fusion, 113:102648, 2025. 3 [35] Saeed Iqbal, Adnan N. Qureshi, Jianqiang Li, and Tariq Mahmood. On the analyses of medical images using traditional machine learning techniques and convolutional neural networks. Archives of Computational Methods in Engineering, 30:3173 3233, 2023. 1 [36] M. N. Islam, M. Hasan, M. K. Hossain, et al. Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from ct-radiography. Scientific Reports, 12:11440, 2022. 5, 6 [37] Audun Jøsang and Lance Kaplan. Principles of subjective networks. In 2016 19th International Conference on Information Fusion (FUSION), pages 12921299, 2016. 4, 1 [38] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 5 [39] Sheng Lan, Xiu Li, and Zhenhua Guo. An adaptive regionbased transformer for nonrigid medical image registration with self-constructing latent graph. IEEE Transactions on Neural Networks and Learning Systems, 2023. 2 [40] Sheng Lan, Xiu Li, and Zhenhua Guo. Drt: Deformable region-based transformer for nonrigid medical image registration with constraint of orientation. IEEE Transactions on Instrumentation and Measurement, 72:115, 2023. 2 [41] Hao Li, Yang Nan, Javier Del Ser, and Guang Yang. Regionbased evidential deep learning to quantify uncertainty and improve robustness of brain tumor segmentation. Neural Comput. Appl., 35(30):2207122085, 2022. 3 [42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 999210002, 2021. 6, 7 [43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s, 2022. 6 [44] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. 6, 5 [45] Huan Ma, Zongbo Han, Changqing Zhang, Huazhu Fu, Joey Tianyi Zhou, and Qinghua Hu. Trustworthy multimodal regression with mixture of normal-inverse gamma distributions. In Proceedings of the 35th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2021. Curran Associates Inc. 3 [46] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European Conference on Computer Vision (ECCV), pages 116131, 2018. 6 [47] Qiang Ma, Liu Li, Emma C. Robinson, Bernhard Kainz, and Daniel Rueckert. Weakly Supervised Learning of Cortical Surface Reconstruction from Segmentations . In proceedings of Medical Image Computing and Computer Assisted Intervention MICCAI 2024. Springer Nature Switzerland, 2024. 1 [48] Subhashree Mohapatra, Girish Kumar Pati, Manohar Mishra, and Tripti Swarnkar. Gastrointestinal abnormality detection and classification using empirical wavelet transform and deep convolutional neural network from endoscopic images. Ain Shams Engineering Journal, 14(4):101942, 2023. 1 [49] Nafiseh Ghaffar Nia, E. Kaplanoglu, and A. Nasab. Evaluation of artificial intelligence techniques in disease diagnosis and prediction. Discover Artificial Intelligence, 3, 2023. 1 [50] Olaide Oyelade, Absalom Ezugwu, Hein Venter, Seyedali Mirjalili, and Amir Gandomi. Abnormality classification and localization using dual-branch whole-regionbased cnn model with histopathological images. Computers in Biology and Medicine, 149:105943, 2022. [51] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838855, 1992. 5 [52] Zhijun Ren, Kai Huang, Yongsheng Zhu, Ke Feng, Zheng Liu, Hong Fu, Jun Hong, and Adam Glowacz. Progressive generative adversarial network for generating highdimensional and wide-frequency signals in intelligent fault diagnosis. Engineering Applications of Artificial Intelligence, 133:108332, 2024. 2 [53] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks, 2019. 6 [54] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty, 2018. 3 [55] Afshar Shamsi, Hamzeh Asgharnezhad, Shirin Shamsi Jokandan, Abbas Khosravi, Parham M. Kebria, Darius Nahavandi, Saeid Nahavandi, and Dipti Srinivasan. An uncertainty-aware transfer learning-based framework for covid-19 diagnosis. IEEE Transactions on Neural Networks and Learning Systems, 32(4):14081417, 2021. 3 [56] Zhimin Shao, Weibei Dou, and Yu Pan. Dual-level deep evidential fusion: Integrating multimodal information for enhanced reliable decision-making in deep learning. Information Fusion, 103:102113, 2024. [57] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 6 [58] Mingyu Sun. Optimizing mcmc-driven bayesian neural networks for high-precision medical image classification in small sample sizes, 2024. 2 [59] Hai Siong Tan, Kuancheng Wang, and Rafe McBeth. Deep evidential learning for radiotherapy dose prediction. Computers in Biology and Medicine, 182:109172, 2024. 3 [60] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks, 2020. 6, 7 [61] Samritika Thakur and Aman Kumar. X-ray and ct-scanbased automated detection and classification of covid-19 using convolutional neural networks (cnn). Biomedical Signal Processing and Control, 69:102920, 2021. 1 [62] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention, 2021. 6, 7 [63] L. Wei, A. Yadav, and W. Hsu. Ctflow: Mitigating effects of computed tomography acquisition and reconstruction with normalizing flows. In Medical Image Computing and Computer Assisted Intervention MICCAI 2023. Springer, Cham, 2023. [64] Tingyi Xie, Zidong Wang, Han Li, Peishu Wu, Huixiang Huang, Hongyi Zhang, Fuad E. Alsaadi, and Nianyin Zeng. Progressive attention integration-based multi-scale efficient network for medical imaging analysis with application to covid-19 diagnosis. Computers in Biology and Medicine, 159:106947, 2023. 2 [65] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Coscale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 99819990, 2021. 6 [66] Zhiwen Xu, Haijun Ren, Wei Zhou, and Zhichao Liu. Isanet: Non-small cell lung cancer classification and detection based on cnn and attention mechanism. Biomedical Signal Processing and Control, 77:103773, 2022. 2 [67] Pengcheng Xue, Dong Nie, Meijiao Zhu, Ming Yang, Han Zhang, Daoqiang Zhang, and Xuyun Wen. WSSADN: Weakly Supervised Spherical Age-Disentanglement Network for Detecting Developmental Disorders with Structural In proceedings of Medical Image Computing and MRI . Computer Assisted Intervention MICCAI 2024. Springer Nature Switzerland, 2024. 1 [68] Yanwu Yang, Xutao Guo, Yiwei Pan, Pengcheng Shi, Haiyan Lv, and Ting Ma. Uncertainty quantification in medical image segmentation with multi-decoder u-net. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised Selected Papers, Part II, page 570577, Berlin, Heidelberg, 2021. Springer-Verlag. [69] Shoulin Yin, Hang Li, Lin Teng, Asif Ali Laghari, Ahmad Almadhor, Michal Gregus, and Gabriel Avelino Sampedro. Brain ct image classification based on mask rcnn and attention mechanism. Scientific Reports, 14(1):29300, 2024. 2 [70] Xiaodong Yue, Yufei Chen, Bin Yuan, and Ying Lv. Threeway image classification with evidential deep convolutional neural networks. Cognitive Computation, 14(6):20742086, 2022. 3 [71] Tal Zeevi, Rajesh Venkataraman, Lawrence Staib, and John Onofrey. Monte-carlo frequency dropout for predictive uncertainty estimation in deep learning. In 2024 IEEE International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2024. 2 [72] Zhenxi Zhang, Heng Zhou, Xiaoran Shi, Ran Ran, Chunna Tian, and Feng Zhou. An evidential-enhanced tri-branch consistency learning method for semi-supervised medical image segmentation. IEEE Transactions on Instrumentation and Measurement, 73:113, 2024. 3 [73] Jianfeng Zhao and Shuo Li. Evidence modeling for reliability learning and interpretable decision-making under multimodality medical image segmentation. Computerized Medical Imaging and Graphics, 116:102422, 2024. 3 [74] Jinyu Zhao, Yichen Zhang, Xuehai He, and Pengtao Xie. Covid-ct-dataset: ct scan dataset about covid-19. arXiv preprint arXiv:2003.13865, 2020. 5, 6, [75] Ye Zhu, Jingwen Xu, Fei Lyu, and Pong C. Yuen. Symptom Disentanglement in Chest X-ray Images for Fine-Grained In proceedings of Medical Image Progression Learning . Computing and Computer Assisted Intervention MICCAI 2024. Springer Nature Switzerland, 2024. 1 UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography"
        },
        {
            "title": "Supplementary Material",
            "content": "A1. Extended Methodology A1.1. Global Uncertainty Estimation and Evidential Learning Our global uncertainty estimator serves two critical functions: producing initial class predictions and generating spatial uncertainty map to guide subsequent patch selection. We formulate this as an evidential learning problem that explicitly models uncertainty in the classification process. A1.1.1. Global Model Architecture Given an input CT image RHW 1, we employ ResNet backbone [29] Fθ with parameters θ to extract feature maps Rhwd, where = H/32, = W/32, and is the feature dimension: To accommodate grayscale CT images, we modify the first convolutional layer of the ResNet [29] to accept singlechannel inputs while preserving the pretrained weights by averaging across the RGB channels. The feature maps are then processed by two parallel heads: classification head Cϕ and an evidence head Eψ. The classification head applies global average pooling followed by fully connected layer to produce class logits: zg = Cϕ(F) = Wϕ GAP(F) + bϕ where zg RC represents the logits for classes, Wϕ RCd and bϕ RC are learnable parameters, and GAP denotes global average pooling. (8) = Fθ(I) (9) A1.1.2. Evidential Uncertainty Estimation The evidence head Eψ generates pixel-wise Dirichlet concentration parameters that quantify uncertainty at each spatial location: = Eψ(F) Rhw4C (10) Here, encodes four parameters (α, β, γ, ν) for each class at each spatial location, representing Dirichlet distribution. We implement Eψ as sequence of convolutional layers that preserve spatial dimensions while expanding the channel dimension to 4C. Following the principles of subjective logic [37], we parameterize the Dirichlet distribution using these four parameters: αi,j,c = βi,j,c νi,j,c + 1 (11) Algorithm 2 Global Uncertainty Estimation Require: Input image RHW 1 Ensure: Global logits zg, Uncertainty map ˆU 1: Fθ(I) {Extract features using backbone} 2: zg Cϕ(F) {Compute global logits} 3: Eψ(F) {Generate evidence parameters} 4: for each spatial location (i, j) and class do 5: 6: βi,j,c softplus(Ei,j,c) + ϵ νi,j,c eEi,j,c+C αi,j,c βi,j,c νi,j,c + 1 k=1 eEi,j,k+C (cid:80)C 7: 8: end for 9: for each spatial location (i, j) do (cid:80)C 10: Ui,j 1 11: end for 12: ˆU Umin(U) 13: return zg, ˆU (cid:16) 1 αi,j,c c=1 + βi,j,c αi,j,c(αi,j,c+1) (cid:17) max(U)min(U)+ϵ {Normalize uncertainty map} where (i, j) denotes spatial location, indicates the class, and αi,j,c > 0 is the concentration parameter for class at location (i, j). The parameters βi,j,c > 0 represents the inverse of uncertainty, νi,j,c represents the mass belief, and we constrain (cid:80)C c=1 νi,j,c = 1 to ensure the mass beliefs form valid probability distribution. To ensure numerical stability, we apply softplus activation (x) = log(1 + ex) to compute βi,j,c and softmax function across the class dimension to compute νi,j,c: βi,j,c = (Ei,j,c) + ϵ νi,j,c = eEi,j,c+C k=1 eEi,j,k+C (cid:80)C (12) (13) where ϵ is small positive constant for numerical stability. From these parameters, we compute the pixel-wise uncertainty map Rhw by aggregating the uncertainty across all classes: Ui,j = 1 (cid:88) (cid:18) αi,j,c c=1 + βi,j,c αi,j,c(αi,j,c + 1) (cid:19) (14) This formulation captures both aleatoric uncertainty term) and epistemic uncertainty (second term). (first 1 represents uncertainty due The aleatoric component αi,j,c to inherent data noise, while the epistemic component βi,j,c αi,j,c(αi,j,c+1) represents uncertainty due to model knowledge limitations. We normalize the uncertainty map to the range [0, 1] for easier interpretation and subsequent processing: ˆU = min(U) max(U) min(U) + ϵ (15) This normalized uncertainty map ˆU is then used to guide the patch selection process, focusing attention on regions where the global model exhibits high uncertainty. Algorithm 2 summarizes the complete process for generating the global class predictions and uncertainty map. The uncertainty map ˆU provides spatial localization of regions where the global model is uncertain about its predictions. High values in ˆU indicate regions that require further analysis through local patch examination. This uncertaintyguided approach allows our model to focus computational resources on diagnostically relevant regions. A1.2. Uncertainty-Guided Patch Selection and Local Refinement A1.2.1. Progressive Patch Extraction Given an input image RHW 1 and its corresponding uncertainty map ˆU Rhw from the global model, we first upsample the uncertainty map to match the input resolution: = U( ˆU, (H, )) (16) where represents bilinear upsampling to dimensions (H, ). Our objective is to extract patches of size from regions with high uncertainty while ensuring diversity among the selected patches. We formulate this as sequential optimization problem where each patch is selected to maximize uncertainty while maintaining minimum distance from previously selected patches. For the first patch, we simply select the region with maximum uncertainty: (x1, y1) = arg max (x,y) x:x+P,y:y+P (17) where (x1, y1) represents the top-left corner of the first patch, and x:x+P,y:y+P denotes the mean uncertainty within the patch region. For subsequent patches = 2, 3, . . . , K, we introduce spatial penalty to encourage diversity: (xk, yk) = arg max (x,y) (cid:104) x:x+P, y:y+P λ min i<k d((x, y), (xi, yi)) (cid:105) (18) where d((x, y), (xi, yi)) computes the Euclidean distance between patch centers, λ is weighting parameter controlling diversity, and mini<k finds the minimum distance to any previously selected patch. To implement this efficiently while avoiding explicit computation of the penalty term for all possible locations, we apply nonmaximum suppression (NMS) approach. After selecting each patch, we suppress region around it by applying penalty mask to the uncertainty map: xM :x+P+M, yM :y+P+M = xM :x+P+M, yM :y+P+M (1 G) (19) where is margin parameter and is Gaussian kernel that applies stronger suppression near the center of the selected patch and gradually reduces toward the edges. Our algorithm incorporates several fallback mechanisms to handle edge cases and ensure reliable operation: Uncertainty Threshold Handling: In situations where no high-uncertainty regions remain (when all uncertainty values are suppressed below specified threshold), the method falls back to random selection to preserve sample diversity. Boundary Checking: Comprehensive boundary checking is applied to prevent selected patches from extending beyond the image borders, ensuring valid patch extraction even at image edges. Dynamic Size Adjustment: To accommodate extremely small images or atypical aspect ratios, the algorithm dynamically adjusts patch sizes, ensuring consistent and valid outputs across varying input dimensions. These mechanisms collectively ensure robust operation across diverse medical imaging datasets with varying characteristics. A1.2.2. Local Refinement Network Architecture After extracting the patches {P1, P2, . . . , PK}, we process each patch independently using local refinement network. This network comprises three components: feature extractor, classification head, and confidence estimation head. The feature extractor Lf processes each patch to obtain local feature vectors: fk = Lf (Pk) Rdl (20) where dl is the feature dimension. We implement Lf as sequence of convolutional layers followed by pooling operations to progressively reduce spatial dimensions while increasing feature depth. Specifically, our implementation uses four convolutional blocks with increasing channel dimensions (64128256256), each followed by batch normalization, ReLU activation, and max pooling. The final features undergo adaptive average pooling to produce fixed-dimensional representation regardless of input patch size."
        },
        {
            "title": "The classification head Lc maps these features to class",
            "content": "logits: zl,k = Lc(fk) RC (21) This head is implemented as two-layer MLP with hidden dimension of 128 and ReLU activation between layers. Simultaneously, the confidence estimation head Lconf produces scalar confidence score for each patch: ck = Lconf(fk) [0, 1] (22) where ck represents the models confidence in its prediction for patch k. We implement Lconf as small MLP with sigmoid activation function on the output to constrain the confidence score to the range [0, 1]. This two-layer MLP has hidden dimension of 64 and uses ReLU activation between layers. The confidence score serves two critical purposes: (1) it allows the model to express uncertainty about individual patch predictions, and (2) it provides weight for the subsequent fusion of local predictions. Patches with higher confidence scores will contribute more significantly to the final classification decision. For each patch k, we obtain both class logits zl,k and confidence score ck. The combined local prediction is computed as confidence-weighted average of the patch predictions: zl = (cid:80)K k=1 ck zl,k k=1 ck + ϵ (cid:80)K (23) where ϵ is small constant (typically 106) for numerical stability. This formulation naturally handles cases where some patches have very low confidence, effectively reducing their contribution to the final prediction. The local refinement network provides detailed analysis of suspicious regions identified by the global model, capturing fine-grained features that might be missed in the global analysis. By assigning confidence scores to each patch, the network also performs an implicit form of attention, focusing on the most discriminative patches for the final classification decision. A1.3. Adaptive Fusion and Training Objectives A1.3.1. Adaptive Fusion Module The adaptive fusion module dynamically determines the optimal weighting between global and local predictions for each input image. Given the global logits zg RC and uncertainty map ˆU Rhw from the global model, and local logits zl RC with patch confidence scores {c1, c2, . . . , cK} from the local refinement network, we compute scalar representation of the global uncertainty by averaging across the spatial dimensions: ug = 1 (cid:88) (cid:88) i=1 j= ˆUi,j (24) This scalar uncertainty ug [0, 1] quantifies the overall confidence of the global model. The fusion network Fω takes as input the global logits zg and the global uncertainty score ug, concatenated into single vector [zg, ug] RC+1. The network outputs fusion weight wg [0, 1] that determines the relative contribution of global versus local predictions: wg = Fω([zg, ug]) (25) We implement Fω as multi-layer perceptron with sigmoid activation on the output: Fω([zg, ug]) = σ(W2ReLU(W1[zg, ug]+b1)+b2) (26) where W1 Rdf (C+1), W2 R1df , b1 Rdf , and b2 are learnable parameters, df is the hidden dimension, and σ is the sigmoid function. The fusion weight wg represents the contribution of the global prediction, while wl = 1 wg represents the contribution of the local prediction. The fused logits zf are computed as: zf = wg zg + (1 wg) zl (27) This adaptive weighting allows the model to rely more on global features when the global model is confident (low uncertainty), and more on local features when the global model is uncertain (high uncertainty). A1.3.2. Multi-component Loss Function Our comprehensive loss function addresses multiple objectives simultaneously. The total loss Ltotal is weighted sum of several components: Ltotal = λf Lfused + λgLglobal + λlLlocal + λuLuncertainty + λcLconsistency + λconfLconfidence + λdLdiversity (28) where λf , λg, λl, λu, λc, λconf, and λd are weighting coefficients for each loss component. Classification Losses. We apply cross-entropy loss to the predictions from each component of our framework: Lfused = Lglobal = (cid:88) i= (cid:88) i=1 yi log(softmax(zf )i) (29) yi log(softmax(zg)i) (30) (a) Global feature embeddings showing clear class separation with distinct clusters for each kidney condition (Normal, Cyst, Tumor, Stone). The global model learns discriminative whole-image representations that establish strong decision boundaries between classes. (b) Local feature embeddings exhibiting significant class mixing without distinct clusters. The local model focuses on fine-grained details within uncertain regions, capturing complementary information not directly aligned with class boundaries. Figure 6. Comparison of t-SNE visualizations for feature spaces in the kidney dataset. (a) Global features from the full-image CNN form well-separated clusters by class, demonstrating effective overall classification capability. (b) Local features from patch-based analysis show substantial mixing across classes, indicating their focus on subtle variations and uncertainty resolution rather than direct class discrimination. This complementary representation underscores why adaptive fusion of both feature types produces superior performance. Llocal = 1 (cid:88) (cid:88) k=1 i=1 yi log(softmax(zl,k)i) (31) Lconsistency = 1 k=1 (cid:88) KL(softmax(zl,k)softmax(zg)) ck where yi is the ground truth label for class (one-hot encoded), and softmax(z)i denotes the softmax probability for class given logits z. Uncertainty Calibration Loss. To ensure that the uncertainty map accurately reflects prediction errors, we introduce an uncertainty calibration loss: Luncertainty = MSE( ˆU, 1 C) (32) where {0, 1}hw is correctness map derived from the global predictions. For each spatial location (i, j), Ci,j = 1 if the predicted class at that location matches the ground truth, and Ci,j = 0 otherwise. This loss encourages high uncertainty in regions where the global model makes errors and low uncertainty where predictions are correct. where KL(P Q) = (cid:80) (33) Pi log(Pi/Qi) is the KL divergence, and ck is the confidence score for patch k. This loss is weighted by the patch confidence, reducing the penalty for inconsistency in low-confidence patches. Confidence Regularization Loss. To align patch confidence scores with prediction accuracy, we introduce confidence regularization loss: Lconfidence = 1 (cid:88) k=1 MSE(ck, ak) (34) where ak {0, 1} indicates whether the prediction for patch is correct (ak = 1) or incorrect (ak = 0). This loss encourages high confidence for correct predictions and low confidence for incorrect predictions. Consistency Loss. To promote consistency between global and local predictions, we use Kullback-Leibler (KL) divergence loss: Diversity Loss. To encourage diversity among patch predictions, we include diversity loss: Figure 7. Uncertainty distribution by class for lung cancer detection. Malignant cases (green) exhibit significantly higher average uncertainty and broader distribution compared to benign cases (pink), which show tighter, lower-uncertainty distribution. Normal cases (blue) display distinctive bimodal distribution with peaks at both low and moderate uncertainty levels. Ldiversity = 1 K(K 1)/2 K1 (cid:88) (cid:88) i=1 cos (softmax(zl,i), softmax(zl,j)) j=i+1 (35) where cos(a, b) = ab ab is the cosine similarity between vectors. This loss penalizes similarity between patch predictions, encouraging each patch to contribute unique information. A2. Implementation Details All models are trained for 100 epochs with early stopping based on validation loss with patience of 7 epochs on single NVIDIA RTX 3090 GPU. We employ an Adam optimizer [38] with learning rate of 1 104 and weight decay of 1 104, with batch size of 96 and cosine decay learning rate scheduler [44]. For data augmentation [8] during training, we apply random horizontal and vertical flips, random rotation (10), random affine transformations (5% translation), and contrast/brightness adjustments (10%). Images are normalized to the [0,1] range after applying appropriate windowing for CT images. We do not use EMA [51] since it does not improve performance. Model configurations are adapted for each dataset as follows: the Kidney dataset uses ResNet-18 [29] backbone with patch size of 64 and 3 patches per image, the Lung dataset uses ResNet-50 [29] backbone with patch size of 64 and 2 patches per image, and the COVID dataset uses ResNet-18 [29] backbone with patch size of 64 and 4 patches per image. The multi-component loss function assigns weights of 1.0 for the fused loss, 0.5 for global and Figure 8. Memory usage scaling with input dimensions across UGPL variants. Lines represent different model configurations and ablations. Config 2 (brown line) consistently demonstrates the highest memory requirements due to its ResNet-50 [29] backbone variant. Some configurations show counterintuitive scaling behavior, particularly at larger input sizes, highlighting complex interactions between model architecture and GPU memory management. local losses, 0.3 for the uncertainty loss, 0.2 for the consistency loss, and 0.1 for both the confidence and diversity losses. A3. Additional Experiments and Results A3.1. Feature Space Analysis To better understand how UGPL learns different representations at global and local scales, we visualize the feature embeddings from both network components using t-SNE. Figure 6 demonstrates the contrast between global and local feature spaces for the kidney CT dataset [36]. The global feature embeddings (Figure 6a) display remarkably clear separation between classes, with distinct clusters forming for each pathological condition. This indicates that the global network successfully learns discriminative features that establish strong decision boundaries at the whole-image level. In contrast, the local feature embeddings (Figure 6b) exhibit substantial mixing between classes with no clear cluster formation, suggesting that the local network captures different characteristics altogether. The global network provides robust overall classification by learning class-separable features, while the local network focuses on fine-grained details within uncertain regions that may not align directly with class boundaries but capture subtle variations critical for resolving ambiguous cases. When these complementary features are combined through our adaptive fusion mechanism, the model effectively leverages both the discriminative power of global features and the detailed analysis of local features, particularly Figure 9. Evolution of model performance across different configurations. Top: Flow field visualization showing performance trajectories from simplified to complete model configurations for each dataset. Bottom: F1 score progression across configurations for COVID (left), Lung (middle), and Kidney (right) datasets, highlighting the dramatic improvement when all components are integrated in the full model. in challenging regions where global analysis alone might be insufficient. ing detailed analysis precisely where diagnostic ambiguity is highest. The dispersed nature of local embeddings also validates our patch selection approach - these patches represent precisely those regions where additional analysis is most beneficial, as they contain ambiguous features that the global model finds difficult to classify confidently. This feature space analysis provides concrete evidence for why progressive refinement is more effective than single-pass approaches for medical image classification. A3.3. Ablation Evolution Figure 9 visualizes performance evolution across configurations. All datasets show minimal variations among simplified configurations followed by dramatic jumps with the full model - COVID F1 scores improve 5.3 (0.15 to 0.79), lung dataset by 2.6 (0.37 to 0.98), and kidney dataset by 1.7 (0.57 to 0.99). A3.2. Uncertainty Calibration Analysis A3.4. Computational Efficiency Analysis Figure 7 visualizes the distribution of pixel-wise uncertainty values across diagnostic classes in the lung cancer dataset [3, 4, 22]. The distinct separation between uncertainty profiles demonstrates the models ability to calibrate uncertainty in clinically meaningful way. Malignant cases consistently show higher uncertainty (mean 0.14, standard deviation 0.07) compared to benign cases (mean 0.06, standard deviation 0.03), reflecting the inherently more complex and variable presentation of malignant lesions. Normal cases exhibit an intriguing bimodal distribution, suggesting the existence of two distinct subgroups within what radiologists classify as normal tissue. This aligns with clinical practice, where some normal cases closely resemble benign findings (first mode) while others contain subtle variations that warrant closer inspection (second mode). The UGPL framework effectively leverages these uncertainty patterns to guide computational resource allocation, focusWe analyze computational efficiency of UGPL across different configurations and ablations to understand tradeoffs between model complexity and performance. Figure 10 shows the relationship between computational complexity (measured in GFLOPs) and inference time. The full UGPL model requires approximately 3-5 GFLOPs depending on the dataset and configuration, with inference times between 4.5-6.7ms on an NVIDIA P100 (we use lightweight GPU for inference to better reflect real-world deployment settings). The global-only ablation (without patch extraction and local refinement) reduces inference time by 27-36% across all datasets, demonstrating the computational cost of the progressive analysis components. Higher-capacity backbones (Config 2 with ResNet-50 variant) increase both GFLOPs and inference time by approximately 45% compared to the standard configurations. Memory efficiency is another critical factor for medical Figure 10. Computational complexity (GFLOPs) versus inference time (ms) for UGPL variants. Points are colored by dataset, with marker style indicating ablation type and size representing input dimensions. imaging applications. Figure 8 illustrates how memory usage scales with input image dimensions. We observe nonlinear scaling patterns that vary significantly across configurations. The ResNet-50 backbone (Config 2) requires 1.4-1.7 more memory than ResNet-18 configurations. Interestingly, ablations demonstrate dataset-specific memory profiles: for the COVID dataset, memory usage increases linearly with input size, while the Kidney dataset shows more complex patterns. The global-only ablation demonstrates inconsistent memory scaling, suggesting that optimizations in GPU memory management affect different architectural components differently. UGPL model requires more computational resources than simplified variants, and the progressive learning approach maintains reasonable efficiency for clinical deployment. The additional cost of uncertainty estimation and local refinement is justified by the significant performance improvements, particularly for challenging cases."
        }
    ],
    "affiliations": [
        "Shiv Nadar University, Chennai, India",
        "Vellore Institute of Technology, Chennai, India"
    ]
}