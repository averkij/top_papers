{
    "paper_title": "OmniRefiner: Reinforcement-Guided Local Diffusion Refinement",
    "authors": [
        "Yaoli Liu",
        "Ziheng Ouyang",
        "Shengtao Lou",
        "Yiren Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce \\ourMthd{}, a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that \\ourMthd{} significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks."
        },
        {
            "title": "Start",
            "content": "OmniRefiner: Reinforcement-Guided Local Diffusion Refinement Yaoli Liu1,4 Ziheng Ouyang2 Shengtao Lou4 Yiren Song3, 4 1 Zhejiang University, 2 Nankai University, 3 National University of Singapore, 4 Creatly.ai 5 2 0 N 5 2 ] . [ 1 0 9 9 9 1 . 1 1 5 2 : r Figure 1. We propose OmniRefiner, model capable of refining image details based on reference images. It can accurately restore various fine details such as logos, text, facial features, and intricate patterns, showing great potential for downstream applications in e-commerce, facial beautification, and advertising."
        },
        {
            "title": "Abstract",
            "content": "Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining generated image using reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identityand attributespecific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce OmniRefiner, detail-aware refinement framework that performs two consecutive stages of referencedriven correction to enhance pixel-level consistency. We first adapt single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintainCorresponding author. ing structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that OmniRefiner significantly improves reference alignment and finegrained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks. Our project homepage is available at https://github.com/yaoliliu/OmniRefiner 1. Introduction Recently, image editing models have advanced rapidly. Starting from text-to-image models that perform conditional editing through training modules such as ControlNet [65], to current specialized editing and generation models [12, 18, 23, 60] trained on large-scale editing datasets, model capabilities have significantly improved across various image editing tasks, including virtual try-on, multiimage fusion, face replacement, and style transfer. Despite this progress, even state-of-the-art diffusion models struggle to preserve fine-grained identity and structural fidelity notably for logos, text, facial micro-geometry, and texturecritical regions. major culprit is the aggressive compression in latent diffusion pipelines, where VAE encoders inevitably discard subtle local cues. As result, when users expect precise transfer from reference crop, models often over-smooth or distort details, degrading reference fidelity. To address above issues, we introduce OmniRefiner, universal post-refinement module for reference detailconsistent enhancement. Specifically, given generated image and reference patch, OmniRefiner operates on zoomed local regions in reference patch to restore high-frequency details in generated image while preserving global consistency including light and background. However, this task presents three challenges: (1) the refined region must align with the reference under perspective, lighting, and geometric variation rather than naive copy-paste; (2) non-edited areas must remain strictly identical to the original image to avoid artifacts upon reintegration; (3) the method must generalize across object categories, scene types, and generator models (open-source and commercial). These challenges motivate three design principles in OmniRefiner. For (1), we adopt FLUX.1-Kontext-dev, single-image editing transformer, into dual-input conditional generator and use bidirectional attention between target and reference tokens, enabling precise, content aware detail transfer under spatial variations. For (2), we introduce supervised fine-tuning (SFT) stage with explicit locality awareness: the model learns to edit only the masked region while preserving the remainder verbatim, thereby preventing collateral changes. For (3), we construct largescale synthetic triplet pipeline that automatically produces diverse training tuples via image editing and VLM-guided cropping, covering rich categories, materials, and degradations to ensure strong cross-domain and cross-backbone generalization. While the above addresses spatial alignment, locality, and generalization, we further observe that micro-textures such as thin text strokes, serial numbers and fabric weaves can remain under-fit due to diffusion smoothing and supervision imbalance. To enhance detail consistency, we introduce second stage training strategy based on GRPO: patch-wise rewards combine perceptual metric Dreamterm, selectively sharpening Sim with masked pixel high-frequency regions without perturbing the background, which serves as precision tuner atop SFT, improving robustness to illumination and geometric changes and stabilizing fine-detail reconstruction. To support training and evaluation, we curate 30Ktriplet benchmark of degraded targets, clean references, and ground truth outputs built by our synthetic pipeline. The dataset enables scalable supervision for SFT and reliable reward computation for RL. Our main contributions are summarized as follows: We propose OmniRefiner, universal reference-guided detail correction module that enhances diffusion outputs without disturbing global structure. We introduce two-stage refinement paradigm: dualinput in-context SFT for alignment/locality and position embedding extension, followed by GRPO-based patch rewards to boost fine-detail consistency. We build 30K localized refinement dataset via an automated four-stage data collecting and creating pipeline based on image-editing model and VLM. Experiments demonstrate our model has state-of-the-art fidelity across diverse content and generator backbones. 2. Related work 2.1. Diffusion Models Diffusion models have emerged as powerful generative paradigm for producing high-fidelity images through iterative denoising. The introduction of DDPM [17], subsequent advances such as Latent Diffusion Models [53] and Latent consistency models [30] have enhanced its usability. Recent years, DiT [42] and Flow matching [26] have significantly improved efficiency and scalability by operating in compressed latent spaces and replacing UNet [46] backbones with Transformer-based architectures. Open-source text-to-image models have evolved from primarily UNet-based models, such as Stable Diffusion [45] and Stable Diffusion XL [43], to increasingly DiT-based models, including FLUX [22], Stable Diffusion 3.5 [11], and Qwen-Image [60]. For period of time, referenceconditioned generation was typically achieved by training ControlNet [65] on top of existing text-to-image models [6, 3136, 54, 66, 68], or by introducing an encoder [3, 5, 24, 37, 38] capable of referencing an image to inject specific features into the latent space of the text-to-image model. However, large-scale image editing models are now becoming the mainstream. With the architectural transition to Diffusion Transformers [41], recent approaches such as EasyControl [67] have achieved image-conditioned generation within MM-DiT frameworks and inspired subsequent works [14, 15, 19, 29, 50, 51, 5557, 59]. 2.2. Image Editing Models Although research on training-free or post-training textto-image editing models is still ongoing [2, 4, 10, 16, 21, 39, 49, 61], recent large-scale image editing models based on the DiT architecture, such as Bagel [8], FLUX.1Kontext [23], and Qwen-Image-Edit [60], have demonstrated capabilities that far surpass previous approaches, as Figure 2. Compared with the state-of-the-art multi-image editing methods, our approach achieves not only faithful reconstruction of the original image in referencerepair tasks, but also excellent performance in various reconstruction scenarios including text, patterns, facial details, and object details. In contrast, existing methods often fail to remain faithful to the original image during repair or are unable to recover text and fine details. researchers continue to push the limits of diffusion models under the scaling law. Both open-source and closedsource editing models now achieve impressive performance in large-scale transformations such as object composition, action modification, and viewpoint changes. However, their generated subjects still suffer from deficiencies in fine textures, facial details, and textual elements. Moreover, while current models exhibit strong text-guided editing abilities, they remain incapable of accurately performing detail restoration when users expect the model to reference given image. In such cases, these models either fail to edit at all or produce incorrect reference-based modifications, as shown in Fig. 2 an increasing number of researchers have begun exploring its application in flow-matching [26] models to further enhance reinforcement learning performance, as seen in works such as DanceGRPO [62] and FlowGRPO [27]. However, current applications of reinforcement learning in diffusion models primarily focus on aligning overall generation results with human preferences. In contrast, our detail restoration task requires the model to pay closer attention to local details. Inspired by [20], we design reward function specifically tailored to emphasize local fine-grained features. 2.3. Reinforcement Learning in Image Generation 3. Method Reinforcement learning (RL) has recently emerged as promising paradigm for improving generative models, particularly in aligning generation with human preferences and fine-grained constraints. Works such as RLHF for textto-image diffusion [1, 58, 63] demonstrate that rewarddriven optimization can enhance visual alignment, aesthetics, and user satisfaction. With the growing popularity of the GRPO [48] algorithm in large language models (LLMs), In Sec. 3.1, we outline the overall two-stage refinement framework. In Sec. 3.2, we introduce the supervised dualinput diffusion architecture for localized detail restoration. In Sec. 3.3, we present GRPO-based reinforcement learning objective to further enhance fine-grained consistency. In Sec. 3.4, we describe our automated synthetic triplet data pipeline for scalable training and strong generalization. In the first stage, we perform Figure 3. Overall architecture of OmniRefiner. Our framework adopts two-stage training pipeline. supervised fine-tuning (SFT) to enable dual-input detail restoration while preserving global structure. In the second stage, we apply GRPO-based reinforcement learning to further enhance fine-grained consistency and local repair quality. This joint design enables precise reference-guided refinement with high visual fidelity. 3.1. Overall Architecture 3.2. Supervised Finetuning for Basic Understanding We present OmniRefiner, two-stage framework for reference-guided detail refinement. Given to-be-refined image RHW 3, reference crop Rhw3 and an edit instruction , our goal is to produce refined image ˆI such that local details in the refinement region Ω {1, . . . , H} {1, . . . , } match those in while preserving outside Ω: ˆIΩ RΩ, ˆI Ω = Ω, (1) where Ω denotes the complement of Ω and the equality outside Ω is enforced up to numerical tolerance. Our twostage pipeline is consist of SFT stage and RL stage. In SFT stage, we adapt FLUX.1-Kontext-dev as our base model, and transform it into dual-input conditional generator that receives both and R. It learns to preserve global semantics from while selectively injecting high-frequency details from R. While in RL stage, We further optimize the model with patch-wise reward and dreamsim reward to further improve robustness against perspective, illumination, and geometric variations. Our overall architecture is illustrated in Fig. 3. We attempt to perform local refinement using current stateof-the-art multi-input models; however, as shown in Fig. 2, they suffer from copy-paste or inconsistent problems such as visible seams, color bleeding, and structural drift, failing to remain consistent with the global content of I. To address the above issues, we adopt SFT training, trying to make the model learn where and how to integrate reference details while respecting the global composition while extending the model to accept two images(input and reference) as input. Specifically, we employ Bidirectional attention and local mask loss so that detail transfer is context-aware instead of being rigid local paste. Bidirectional Attention. In our approach, we employ bidirectional attention mechanism, which allows the model to attend to the noisy latent, prompt, input latent, and reference latent simultaneously. Specifically, the model first encodes the image and the reference into their respective latent representations, cI and cR. After applying position encoding cloning, the latent tokens are concatenated along the sequence dimension to perform joint attention. The attention mechanism is formulated as follows: MMA([z; cI ; cR; cT ]) = softmax (cid:19) (cid:18) QK V, (2) where [z; cI ; cR; cT ] denotes the concatenation of the noised latent tokens z, the image condition tokens cI , the reference condition tokens cR, and the prompt tokens cT , allowing the conditional and denoising branches to interact as needed. Here, Q, K, and represent the query, key, and value matrices, respectively, which are derived from the concatenated input via linear projections. The term is the dimension of the key features, serving as scaling factor 1 to ensure gradient stability. In this process, we apply position encoding (PE) for each latent. Specifically: The noisy latent uses position encoding ids[0, h, w], where and represent the height and width of the image grid. The input latent cI and reference latent cR, use position encoding ids[1, h, w] and ids[2, h, w], respectively. This approach enables the model to maintain spatial consistency while performing precise detail transfer and denoising. The use of bidirectional attention enhances the models ability to process both local and global structures, leading to improved image editing results. Weighted Mask Loss. Follow the setting adopted by [25], we define weighted mask loss to ensure that the model focuses refinements on the desired region Ω while preserving the background Ω. This approach computes the error across the entire image but applies different weights to the target and background regions. Specifically, we define binary mask {0, 1}HW , where (p) = 0 for pixels Ω (the target region) and (p) = 1 for pixels / Ω (the background). From this mask, we derive pixel-wise weight matrix . The objective is to up-weight the loss within the target region Ω to emphasize refinement, while maintaining standard weight of 1 for the background Ω to penalize unwanted changes. The weight (p) for each pixel is defined as: p(1 (p)) (cid:80) (p) = , if (p) = 0, Ω, 1, if (p) = 1, else. micro textures) are easily underfit because LSFT is dominated by global denoising statistics. We therefore adopt reward-driven optimization to explicitly push the model toward patch-level perceptual similarity and pixel accuracy within Ω, while leaving Ω intact. We split Ω into nonoverlapping patch set PΩ = {Pk}K k=1 (e.g., 512512 windows). Let ˆI[Pk] denote the cropped prediction on patch Pk, and [Pk] the corresponding ground truth. GRPO objective. Given prompt c, the flow model pθ generates batch of images {xi i=1 along with their reverse-time trajectories {(xi , xi i=1. GRPO optimizes the policy model through the following objective: 0}G 1, , xi 0)}G JGRPO(θ) = EcC, {xi}πθold (c) 1 (cid:16) (cid:88) t(θ) ˆAi min(cid:0)ri (cid:88) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "i="
        },
        {
            "title": "1\nT",
            "content": "t, clip(ri t(θ), 1 ε, 1 + ε) ˆAi (cid:1)(cid:17) t=0 β DKL(πθ( c) πref ( c)) (cid:35) , (5) where ri t(θ) = pθ(xi pθold(xi t1 xi t1 xi t, c) t, c) . The advantage term ˆAi across the batch: is obtained by standardizing rewards ˆAi = R(xi 0, c) mean({R(xj 0, c)}G std({R(xj 0, c)}G j=1) j=1) . Mask Pixel Reward. We complement the perceptual term with masked MSE: Rmm = 1 Ω (cid:88) pΩ (cid:0) ˆI(p) (p)(cid:1) . (6) (3) Perceptual Reward. Let () be frozen DreamSim embedding. We compute perceptual similarity reward per patch and then average: The weighted mask loss is then computed as the mean of the weighted pixel-wise squared errors: Lmask = 1 (cid:88) (p) (cid:16) ˆI(p) (p) (cid:17)2 , (4) where ˆI(p) is the predicted pixel value at position p, and (p) is the corresponding ground truth pixel value. This loss function strongly encourages the model to minimize the error within the region of interest Ω, simultaneously, the standard-weighted term for Ω ensures the background structure is preserved. 3.3. Reinforcement Learning for Enhanced Ability Although SFT equips the model with strong alignment prior, certain micro details (e.g., text edges, serial numbers, Rds = 1 (cid:88) k=1 (cid:13) (cid:13) (cid:13)f (cid:17) (cid:16) ˆI[Pk] (cid:13) (I [Pk]) (cid:13) (cid:13)2 . The overall scalar reward is calculated as: = (1 λ)Rds + λ Rmm, (7) (8) where λ > 0 balances overall perceptual similarity and detail pixel-wise accuracy. ODE to SDE GRPO. Following [27], we apply GRPO to the flow-matching model using the ODE-to-SDE formulation. Specifically, for set of inputs (cid:0)I, R, , (cid:1) and an exploration count of m, we sample trajectories. During each sampling step, certain amount of random noise is added to encourage stochastic exploration along the path. Figure 4. We adopt four-stage data pipeline. First, VLM pairs images of the same product with consistent styles and reasonable viewpoints. Second, it generates fine-grained editing instructions for one image in each pair. Third, an image editing model executes these edits using the pre-edit image as ground truth, forming our (input, reference, ground truth) triplet dataset.Finally, the VLM generates an instruction guiding the model to restore the input using the reference, based on the input, reference, and ground truth. For trajectory with exploration probability p, we predict it using (xp) = 1 σs 2π (cid:18) exp (xp µp)2 2σ2 (cid:19) , (9) where xp is the predicted sample at current timestep, σs is time-dependent parameter, and µp is the mean of the Gaussian distribution predicted for the current denoising step. After obtaining rewards, we compute the advantage for each trajectory and optimize the GRPO objective Eqn. (5) via gradient descent. 3.4. Automated Dataset Pipeline To train at scale, we construct quadruple (cid:0)I, R, , (cid:1) automatically. Starting from clean images , we first sample region Ω using VLM-based saliency/objectness selector, then produce degraded variant = Degrade(I , Ω) (blur, compression, downsampling, text/logo erosion, lighting shifts, color/texture change, text remove/change), and obtain = Crop(I , Ω) as the reference. This yields large, diverse supervision for Eqn. (4) and provides reliable targets for the patch-wise RL in Eqn. (5) Eqn. (7). As illustrated in Fig. 4, we adopt three-stage data processing pipeline. Our raw data are collected and categorized based on individual products; however, each product may still contain multiple styles, colors, or large variations in viewing angles (e.g., top-down views of shoes or sole images). Therefore, in the first stage, we employ VisionLanguage Model (VLM) to pair images of the same product that share consistent styles and have reasonable viewpoint variations. In the second stage, the VLM generates finegrained editing instructions for one image in each pair. In the third stage, we apply an image editing model to perform the instructed edits, using the pre-edit image as the ground truth. In the fourth stage, we provide the input, reference, and ground truth to the VLM and ask it to generate an instruction that edits the input into the ground truth based on the reference. This process yields our quadruple dataset. Figure 5. The DreamSim reward curve and the masked MSE reward curve demonstrate the process of how our model aligns with the reward functions during GRPO. 4. Experiment. 4.1. Experiment Setup. Experiment Details. We utilize Flux.1-Kontext-dev [23] as our base model and adopt LoRA strategy (rank 128) on single H200 GPU with resolution of 512 512, batch size of 1, and learning rate of 1 104, followed by reinforcement learning phase on four H200 GPUs using GRPO with 16 trajectories, batch size of 4, and 800 fine-tuning steps at the same learning rate. Baseline Methods. Our comparison covers both opensource and closed-source state-of-the-art multi-image input models, including gemini-2.5-flash-image-preview (nanobanana) [12], Seedream4.0 [47], Sora Image [28], QwenImage-Edit-Plus [60], Mosaic [49], and DreamO [39]. Metrics. For consistency evaluation, we compute the following similarity metrics: CLIP Image Score [44], DreamSim [13], DINOv2 [40], and DINOv3 [52] image similarity scores, Mean Squared Error, and the advanced VisionLanguage Models (VLMs) gemini2.5 [7] Score. For facial similarity evaluation, we adopt recent open-source state-ofthe-art methods such as LVFace [64], as well as the widely used ArcFace [9] similarity metric. For text restoration tasks, since current open-source methods are insufficient for recognizing fine details and small characters in real-world Table 1. Compared to existing SOTA methods, our approach demonstrates improvements across various image similarity metrics, face similarity metrics for face restoration, and evaluations by advanced VLMs. Methods DreamO Mosaic nano-banana Seedream4.0 Qwen-Image-Edit-Plus Sora Ours Text Local Refine Dreamsim DINOv2 DINOv3 Clip-I MSE VLM Score ArcFace LVFace VLM Text Identify Overall Local Refine Face Local Refine 0.3475 0.2964 0.1189 0.1671 0.1916 0.1736 0. 0.5005 0.7256 0.8724 0.8787 0.8705 0.8753 0.9457 0.4335 0.6120 0.8093 0.7878 0.7833 0.7143 0.8919 0.8861 0.1506 0.9209 0.1147 0.9584 0.0440 0.9613 0.0486 0.9416 0.0535 0.9524 0.0686 0.9765 0.0164 28.04 40.13 72.30 66.66 58.00 62.21 81.91 0.5082 0.6861 0.8070 0.7197 0.6807 0.4328 0.8573 0.3446 0.4788 0.6331 0.5612 0.5456 0.3758 0. 2.874 2.928 6.600 6.719 5.753 5.536 6.747 Table 2. Ablation study results across all metrics indicate the necessity of each step in our approach. Methods Text Local Refine Dreamsim DINOv2 DINOv3 Clip-I MSE VLM Score ArcFace LVFace VLM Text Identify Overall Local Refine Face Local Refine Ours(w/o RL) Ours(EasyControl PE&w/o RL) Ours(RL Qwen-Image-Edit-Plus) Ours(w/o masked MSELoss) Ours(full) 0.0929 0.1016 0.1449 0.0916 0.0918 0.9214 0.9126 0.8846 0.9160 0.9457 0.8653 0.8614 0.8585 0.8864 0.8919 0.9701 0.0250 0.9646 0.0266 0.9583 0.0298 0.9637 0.0177 0.9765 0.0164 74.01 72.15 68.52 74.68 81. 0.8265 0.8163 0.7481 0.8507 0.8573 0.7152 0.7060 0.6452 0.7496 0.7569 5.286 5.157 6.443 5.911 6.747 scenarios, we carefully design prompts that enable gemini2.5 to perform cross-image text comparison and scoring. Benchmarks. For evaluation, we introduce benchmark named Detail400, which includes branded products with text and logos, clothing with printed patterns, vehicles, and jewelry or decorative items. These categories are known to frequently suffer from detail preservation and transfer failure in generative models, making them ideal for assessing fine-grained detail consistency. 4.2. Comparison and Evaluation Qualitative Evaluation. As shown in Fig. 2, our method outperforms existing state-of-the-art approaches in both text and image reference-based restoration. In contrast, current methods often fail to faithfully reconstruct the image according to the reference: some redraw the image based on their own interpretation rather than the reference, while others overuse or even directly copy-paste the reference image, resulting in incorrect lighting, structure, and perspective of fine details. Fig. 6 presents additional restoration examples, showing that our method performs well in detail restoration tasks. It not only faithfully reconstructs according to the reference image but also preserves the original lighting and structural integrity. Quantitative Evaluation. Table 1 and 2 present the quantitative evaluation results. Experiments show that our generated results outperform both open-source and closedsource state-of-the-art models across multiple similarity metrics. Moreover, our method achieves superior facial detail restoration and demonstrates leading performance in text reconstruction. 4.3. Ablation Study. Our ablation study aims to verify the following key components of our method: (1) Our modification to the position embedding scheme outperforms common control-input designs used in existing works under our research problem settings, such as the ID clone approach adopted by Easy Control; (2) The reinforcement learning stage is indispensable for achieving the final model performance; (3) Our design surpasses the results of directly post-training recent opensource state-of-the-art models with native multi-image input support, such as Qwen-Image-Edit-Plus; and (4) The Masked MSE Score in our reward function plays crucial role, as it directly determines whether the model can effectively learn the corresponding fine details. Table. 2 presents the quantitative results of the ablation study, and Fig. 8 shows the corresponding visual results. Together, these findings demonstrate that the position embedding modification, GRPO training, and Masked MSE Score design in our method are all indispensable components. 4.4. User Study. To further validate the effectiveness of our method in realworld perceptual settings, we conducted user study. We designed an online survey and collected responses from 17 participants with experience in image editing, design, or AI media tools. In each question, participants were shown results generated by different methods and were asked to select the generated result that best matched the local details and the one that appeared most natural and seamless after local restoration. The aggregated user preference results are shown in Fig. 7. Statistical results show that our method not only achieves the highest perceived consistency with the reference image in terms of fine-grained details, but also Figure 6. Qualitative results demonstrate that our method can accurately restore fine details in images. Figure 7. The user study demonstrates that our method achieves the highest human preference alignment in both detail consistency and restoration naturalness. produces the most natural and seamless results. In contrast, other methods either fail to preserve local detail consistency with the reference or introduce noticeable stitching artifacts after editing. These results confirm that the improvements of the refined image achieved by OmniRefiner are not only measurable by quantitative metrics, but also clearly recognized by human observers. Figure 8. Ablation studies demonstrate that each component of our method (GRPO, position embedding, and masked MSE reward) is essential. Even when applying GRPO fine-tuning to Qwen-ImageEdit-Plus, model that natively supports multi-image inputs, using exactly the same data, the results still fall short of ours. (RAG) to automatically identify refinement regions and retrieve optimal reference details, enabling an end-to-end autonomous refinement pipeline. 5. Limitation and Future Work 6. Conclusion Currently, the selection of regions requiring refinement and the retrieval of suitable reference patches are performed manually. This human involvement limits full automation and scalability. In future work, we plan to incorporate visionlanguage models and retrieval-augmented generation We propose OmniRefiner, unified framework for reference-guided fine detail enhancement that serves as plug-and-play refinement module for modern diffusion models. By extending single-image diffusion Transformers into dual-input generators through simple yet effective Position Embedding modifications, and employing two-stage learning paradigm, namely supervised fine-tuning for task adaptation followed by reinforcement learning with specially designed reward functions that promote both global similarity and local detail fidelity. We further introduce scalable automated quadruplet data pipeline that enables high-quality training without manual annotation. Extensive experiments demonstrate that OmniRefiner achieves superior detail recovery across diverse content and architectures, validating the importance of explicit reference conditioning and reward-driven refinement. We believe this work provides principled pathway toward high-fidelity, controllable, and reliable image refinement, and hope it inspires future research in fine-grained detail generation modeling and reference-aligned visual synthesis."
        },
        {
            "title": "References",
            "content": "[1] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. ArXiv, abs/2305.13301, 2023. 3 [2] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8861 8870, 2023. 2 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2256022570, 2023. 2 [4] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation, 2025. 2 [5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 65936602, 2024. 2 [6] Xuewei Chen, Zhimin Chen, and Yiren Song. Transanimate: Taming layer diffusion to generate rgba video. arXiv preprint arXiv:2503.17934, 2025. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6 [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2 [9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 6 [10] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. SIGGRAPH Asia 2024 Conference Papers, 2024. 2 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [12] Alisa Guillaume Vernade, Fortin, and Ammaar Reshi. flash image, https://developers.googleblog.com/en/introducing-gemini2-5-flash-image/, 2025. 1, 6 Kat Kampf, 2.5 image model. state-of-the-art Introducing gemini our [13] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 6 [14] Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, and Yin Zhang. Relationadapter: Learning and transferring viarXiv preprint sual relation with diffusion transformers. arXiv:2506.02528, 2025. 2 [15] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv preprint arXiv:2501.15891, 2025. 2 [16] Amir Hertz, Ron Mokady, Jay M. Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. ArXiv, abs/2208.01626, 2022. 2 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [18] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, and Jiaming Liu. Arteditor: Learning customized instructional image editor from few-shot examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1765117662, 2025. 1 [19] Yuxin Jiang, Yuchao Gu, Yiren Song, Ivor Tsang, and Mike Zheng Shou. Personalized vision via visual in-context learning. arXiv preprint arXiv:2509.25172, 2025. 2 [20] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Dan Yamins, and Jiajun Wu. Are these the same apple? comparing images based on object intrinsics. Advances in Neural Information Processing Systems, 36:4085340871, 2023. 3 [21] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. ArXiv, abs/2412.08629, 2024. 2 [22] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2 [23] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 1, 2, 6 [24] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 86408650, 2024. 2 [25] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 5 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [27] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 3, 5 [28] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [29] Runnan Lu, Yuxuan Zhang, Jiaming Liu, Haofan Wang, and Yiren Song. Easytext: Controllable diffusion transarXiv preprint former for multilingual arXiv:2505.24417, 2025. 2 text rendering. [30] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [31] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2 [32] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. [33] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. [34] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain regional image animation via motion prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 60186026, 2025. [35] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. [36] Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, et al. Follow-your-emoji-faster: Towards efficient, fine-controllable, and expressive freestyle portrait animation. arXiv preprint arXiv:2509.16630, 2025. 2 [37] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2 [38] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. [39] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 2, 6 [40] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmenChen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1, 2, 6 [61] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 2 [62] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 3 [63] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 89418951, 2023. 3 [64] Jinghan You, Shanglin Li, Yuanrui Sun, Jiangchuan Wei, Mingyu Guo, Chao Feng, and Jiao Ran. Lvface: Progressive cluster optimization for large vision models in face recognition. arXiv preprint arXiv:2501.13420, 2025. 6 [65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 1, [66] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stablemakeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. 2 [67] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 2 [68] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1034810356, 2025. 2 tation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 2 [47] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 6 [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [49] Dong She, Siming Fu, Mushui Liu, Qiaoqiao Jin, Hualiang Wang, Mu Liu, and Jidong Jiang. Mosaic: Multi-subject personalized generation via correspondence-aware alignment and disentanglement, 2025. 2, 6 [50] Wenda Shi, Yiren Song, Dengming Zhang, Jiaming Liu, and Xingxing Zou. Fonts: Text rendering with typography and style controls. arXiv preprint arXiv:2412.00136, 2024. 2 [51] Wenda Shi, Yiren Song, Zihan Rao, Dengming Zhang, Jiaming Liu, and Xingxing Zou. Wordcon: Word-level typography control in scene text rendering. arXiv preprint arXiv:2506.21276, 2025. 2 [52] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 6 [53] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [54] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. 2 [55] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. 2 [56] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanyfor multiHarnessing diffusion transformers arXiv preprint thing: domain procedural sequence generation. arXiv:2502.01572, 2025. [57] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. 2 [58] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 3 [59] Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, and Yiren Song. Diffdecompose: Layer-wise decomposition of alpha-composited images via diffusion transformers. arXiv preprint arXiv:2505.21541, 2025. 2 [60] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei OmniRefiner: Reinforcement-Guided Local Diffusion Refinement"
        },
        {
            "title": "Supplementary Material",
            "content": "Our supplementary material is organized as follows: Section provides the details of our user study; Section describes our evaluation protocol, including how we employed and configured VLMs for assessment, and Section presents additional results and comparisons, including those from our ablation study; A. User Study Detail Figure 9. User study example: participants were first informed that detail consistency must be achieved while maintaining natural integration with the background, and they were also asked to directly select the result that appeared most natural and unobtrusive. Our user study involved 17 participants, all of whom had prior experience with image editing tools or AI-based image/video generation models. For each question in the survey, participants were shown randomly ordered results generated by six different methods, including ours. To avoid bias, the names of all methods were anonymized and the order was fully randomized. To prevent misunderstandings regarding the evaluation objective, we explicitly clarified to participants that superior detail consistency does not simply mean copying the reference image onto the target; rather, it requires preserving fine details while ensuring seamless integration with the target image. Additionally, to ensure that participants had ample opportunity to express which method produced the most natural restoration in their view, we added an extra question for each test case that asked them to select the result that appeared the most natural and unobtrusive. This design allowed participants to choose the image that best aligned with their perception of visual quality without repeatedly comparing how closely each method matched the reference details. As result, the scientific rigor of our comparative evaluation is further ensured. B. Evaluation Detail"
        },
        {
            "title": "Evaluation Prompt for Overall Local Refine",
            "content": "You are an expert-level VLM evaluator tasked with scoring image generation models. Your objective is to provide precise, quantitative assessment of Generated Image by comparing it to Ground Truth (GT) Image. You will be given both images and must return score out of 100 points based on the specific rubric provided below. INPUT: Image 1: [Generated Image] Image 2: [Ground Truth (GT) Image] SCORING RUBRIC (100 POINTS TOTAL) 1. Overall Consistency (40 Points) Description: Evaluate the macro-level alignment between the Generated Image and the GT Image. This includes: Composition: Are objects and subjects placed in the correct locations? Color Palette: Do the overall colors and tones match? Lighting & Shadows: Is the direction, hardness, and intensity of light and shadows consistent with the GT? Score: Assign score from 0 to 40 based on how closely the generated image matches the GT in these three aspects. 2. Detail Consistency (40 Points) Description: Evaluate the micro-level, high-frequency detail fidelity. The Generated Image must faithfully reproduce specific elements from the GT. Pay close attention to: Text & Typography: Is all text identical, legible, and correctly rendered? Logos & Insignia: Are brand marks, logos, or symbols accurately replicated? Fine Patterns & Textures: Are intricate details (e.g., fabric weave, wood grain, complex patterns) preserved? Facial Details: Are facial features, identity, and expression consistent? Score: Assign score from 0 to 40. High scores require near-perfect replication of all specified details. Deduct points heavily for errors in text, logos, or faces. 3. Generated Image Quality (20 Points) Description: Evaluate the intrinsic quality of the Generated Image on its own, independent of the GT. The image should be clear, sharp, and free from common generation issues. Criteria: Clarity & Sharpness: Is the image sharp, or is it blurry, pixelated, or soft? Artifacts: Is the image clean, or does it contain visual artifacts (e.g., waxy skin, malformed limbs, strange distortions, digital noise, color banding)? Score: Assign score from 0 to 20 based on its technical quality and realism. flawless, high-resolution image gets 20. An image with significant artifacts or blurriness gets low score. OUTPUT FORMAT You MUST provide your evaluation in the following strict format: { } \"Thinking\": <Your detailed reasoning process here>, \"Overall Consistency\": <Score out of 40>, \"Detail Consistency\": <Score out of 40>, \"Generated Image Quality\": <Score out of 20>, \"Total Score\": <Total Score out of 100> In our evaluation, DreamO, Xverse, and Qwen-Image-Edit-Plus, together with our OmniRefiner, were executed on single H200 GPU, with the generation seed fixed to 42. For Seedream4.0, Nanobanana, and Sora, we used their officially provided API endpoints for generation. Notably, Sora refused to produce outputs for approximately 20% of the test casesmainly those involving face or logo restoration. For such cases, we retried up to five times; ultimately, about 15% of the cases still failed to generate any output, and these were excluded from Soras results. All generated images were kept at the same resolution as the input, with side lengths ranging from 768 to 2048. For VLM-based evaluation, we used the Gemini 2.5 Pro model in non-inference mode, and the image and text evaluation prompts used for scoring are shown in B."
        },
        {
            "title": "Evaluation Prompt for Text Rendering Accuracy",
            "content": "Role: You are high-precision VLM (Visual Language Model) evaluator. Your specialization is analyzing and scoring the fidelity of text rendering in generated images. Task: You will be given two images: [Image 1]: Generated Image produced by model. [Image 2]: Ground Truth Image (the real picture). Your task is to compare [Image 1] to [Image 2] and provide score from 0 to 10 that evaluates only the accuracy of the rendered text. You must follow the steps and scoring rubric below. Instructions Step 1: Analysis Before scoring, you must first perform and mentally note the following analysis: Identify Text Regions (Image 1): Scan the Generated Image and identify all areas that contain text or text-like artifacts. Identify Text Regions (Image 2): Scan the Ground Truth Image and identify all areas that contain text. Evaluate Region Correspondence: Compare your findings. Does the text in [Image 1] appear on the correct objects and surfaces as seen in [Image 2]? Evaluate Content & Detail Accuracy: Compare the content of the text. Is the text in [Image 1] readable, correctly spelled, and do the details (e.g., small text on labels, distant signs) match [Image 2]? Step 2: Scoring After your analysis, assign single score based strictly on the following rubric. 8-10 points: The generated text is completely accurate (correct spelling, no gibberish). The text fully corresponds to the ground truth (correct location, correct text). The text in all detail regions (small print, complex text) is also well-reconstructed. 5-8 points: The generated text is meaningful and largely correct. The text regions are correspondent (text is in the right places). However, the text in some detail regions is incorrect, missing, or blurry. 2-5 points: The generated text is meaningful (e.g., recognizable words) and may have only few artifacts. But, the text regions are incorrect (e.g., text is on the wrong object, or major text is missing). 0-2 points: The generated text is not correct text. It consists of meaningless, text-like artifacts or gibberish that only mimics the shape of text. Step 3: Output Provide your evaluation in the following exact format. Do not add any conversational text outside of this structure. [ANALYSIS] <Briefly describe where you see text/artifacts in the generated image.> <Briefly describe where you see text in the ground truth image.> <Your assessment of location matching, e.g., \"Good\", \"Poor\", \"Perfect\".> <Your assessment of text content, e.g., \"Perfectly accurate\", \"Mostly correct but fails on details\", \"Completely incorrect artifacts\".> [SCORING RATIONALE] <Provide brief justification for your score, linking your analysis to the scoring rubric. For example: \"Score is 7 because regions correspond, but text on the small bottle is incorrect.\"> [FINAL SCORE] <A single number between 0 and 10> C. Visual Results C.1. More Visual Results C.2. More Visual Results of Method Comparison C.3. More Visual Results of Ablation Study Figure 10. Additional restoration results show that our method is highly effective in both detail-guided refinement and detail-guided replacement. Figure 11. Object-centric method comparison results reveal that our approach exhibits advantages in the coherence of text, logos, and textures, as well as the quality and consistency of the overall repair results. Figure 12. Facial comparison results further demonstrate that our method achieves superior understanding of human faces, yielding outstanding restoration quality for fine details such as eyeshadow, eyebrows, scars, and iris color. Figure 13. These further ablation results collectively demonstrate the crucial importance of our design in enhancing the models understanding, repair, reconstruction, and replacement of details."
        }
    ],
    "affiliations": [
        "Creatly.ai",
        "Nankai University",
        "National University of Singapore",
        "Zhejiang University"
    ]
}