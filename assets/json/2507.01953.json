{
    "paper_title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "authors": [
        "Yukang Cao",
        "Chenyang Si",
        "Jinghao Wang",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing."
        },
        {
            "title": "Start",
            "content": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model Yukang Cao1 Chenyang Si2 Ziwei Liu1 1S-Lab, Nanyang Technological University, 2Nanjing University 3The Chinese University of Hong Kong https://yukangcao.github.io/FreeMorph/ Jinghao Wang3 5 2 0 2 ] . [ 1 3 5 9 1 0 . 7 0 5 2 : r Figure 1. Examples of image morphing obtained via FreeMorph. Given two input images, FreeMorph effectively generates smooth transitions between them within 30 seconds."
        },
        {
            "title": "Abstract",
            "content": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring perinstance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining highquality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained Equal contributions, Project lead, Corresponding author. Jinghao was Master student at NTU during this work. diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10 50 faster and establishing new state-of-the-art for image morphing. 1. Introduction Given two distinct input images, image morphing [25, 51] aims to gradually change attributes such as shape, texture, and overall layout to produce series of intermediate images that transition smoothly from one to the other. This process is widely used in fields such as animation, film, and photo editing [1, 45, 46], offering an effective means of enhancing creative expression. Historically, image morphing relied on image warping [9, 37, 44] for aligning corresponding points and on color interpolation [2, 21] for blending. These methods, however, often fall short when handling complex textural and semantic transitions, making them less effective for images with intricate details. With advancements in deep learning, Generative Adversarial Networks (GANs) [4, 11, 17, 34] and Variational Autoencoders (VAEs) [20] have significantly improved image morphing by enabling latent code interpolation. Despite their capabilities, these approaches still face challenges with real-world images due to limited training data and information loss during GAN inversion. This underscores the need for methods that better preserve identity and offer greater generalization. Recently, with the availability of large-scale text-image datasets, vision-language models (e.g., Chameleon [40]), diffusion models (e.g., Stable Diffusion [31, 32, 39]), and transformers (e.g., PixArt-α [6], FLUX [3]) have demonstrated impressive capabilities in generating high-quality images from text prompts. These advancements have paved the way for new generative image morphing techniques. Specifically, Wang and Golland [43] leverages the local linearity of CLIP-based text embeddings to create smooth transitions by interpolating latent image features. Building on this idea, IMPUS [47] introduces multi-phase training framework that includes optimizing text embeddings and training Low-Rank Adaptation (LoRA) modules to better capture semantics. While this method yields more visually appealing results, it requires extensive training, typically around 30 minutes per case. DiffMorpher [49] proposes to directly interpolate latent noise and leverage Adaptive Instance Normalization (AdaIN) to improve performance. However, these methods still struggle to process images with diverse semantics and intricate layouts, limiting their practical effectiveness. Given these issues, our objective is to achieve image morphing without requiring further tuning. Nonetheless, this goal introduces two key challenges: 1) Non-directional transitions and identity loss1. While converting input images into latent features using pre-trained diffusion model and then applying spherical interpolation might seem straightforward, this approach often results in inconsistent transitions. This is due to the non-linear nature of the multistep denoising process. Additionally, this method inherits biases from the pre-trained model, which can lead to identity loss in the generated images. 2) Achieving consistent transitions2. diffusion model does not inherently provide an effective \"variation trend\" to capture the gradual changes between images. Consequently, achieving smooth and gradual transitions in tuning-free manner remains significant challenge without additional adjustments. In this paper, we present FreeMorph, novel tuning-free method capable of instantly generating directional and realistic transitions between two images. Our method introduces two novel components: 1) Guidance-aware spherical interpolation: We first enhance the pre-trained diffusion model by incorporating explicit guidance from the input images through modifications to its self-attention modules. This is achieved through spherical interpolation, which produces intermediate features used in two key ways. First, we perform spherical feature aggregation to blend the key and value features of the self-attention modules, ensuring consistent transitions throughout the generated image sequence. Second, to address identity loss, we introduce prior-driven self-attention mechanism that incorporates explicit guidance from the input images to preserve their unique identities. 2) Step-oriented variation trend: To achieve consistent transitions, we introduce novel step-oriented variation trend. This method blends two self-attention modules, each derived from one of the input images, enabling controlled and consistent transition that respects both inputs. To further improve the quality of the generated image sequences, we designed an improved reverse denoising and forward diffusion process that seamlessly integrates these innovative components into the original DDIM framework. As shown in Fig. 1 and Fig. 4, our approach adeptly handles diverse input types, whether they have similar or distinct semantics and layouts, producing smooth and realistic transitions. To thoroughly assess FreeMorph and benchmark it against current methods, we also collect new evaluation dataset that includes four distinct sets of image pairs, categorized by their semantic and layout similarity. Our extensive evaluations demonstrate that FreeMorph substantially outperforms existing approaches. FreeMorph produces highfidelity image sequences with smooth and coherent transformations in under 30 seconds, making it 50 faster than IMPUS [47] and 10 faster than DiffMorpher [49]. 2. Related Work Text-to-Image Generation. Recently, diffusion models [28, 3032] have emerged as the de facto standard for text-to-image generation. These models employ series of denoising steps (e.g., DDIM, DDPM) [15, 38] to transform Gaussian noise into images, effectively capturing and interpreting details from textual prompts. Trained on billions 1Non-directional transitions, akin to identity loss, result in generated images that deviate from the identity of the input images. 2Inconsistent transitions are those with abrupt changes. of text-image pairs [35], these models exhibit remarkable ability to understand the distribution of real-world images, generating high-quality, diverse outputs while maintaining strong generalization capabilities. Our work harnesses the capabilities of diffusion models, particularly their ability to generate smooth transitions between two specified images [19, 29, 33], to address the image morphing task. Image Morphing. Image morphing is long-standing computer vision and graphics problem. Before the deep learning era, techniques such as mesh warping [9, 37, 44] and field morphing [2, 21] were the primary approaches in this domain. Early approaches [10, 26] utilize GANs [? ] to achieve this objective. However, they generally suffer from three main limitations: (1) the need for extensive training, (2) poor generalization to out-of-domain inputs, and (3) an inability to handle inputs with varying layouts and semantic structures. Recently, advancements in diffusion models have led to significant progress, as demonstrated by methods such as DiffMorpher [49], IMPUS [47], and the work of Wang and Golland [43]. These approaches focus on optimizing text embeddings for two images and fine-tuning pre-trained text-to-image diffusion models to achieve smooth interpolation. However, they often require extensive fine-tuning for each image pair and are limited to images with similar semantics and layouts. This can also hinder the generalizability of pre-trained diffusion models due to constraints imposed by LoRA modules in the U-Net architecture. In contrast, our method offers tuning-free framework that requires no modifications to the original diffusion models, thereby preserving their inherent generalizability. Additionally, our approach significantly improves efficiency and can handle images with different layouts and semantics, addressing key limitation of existing techniques. Tuning-Free Text-Guided Image Editing. Recent image translation methods have emerged that edit either generated or real-world images using text in training-free manner, without altering the internal computations of the U-Net. For instance, SDEdit [24] proposes straightforward method that adds time steps of Gaussian noise to an original image and then denoises it using guiding text. Conversely, EDICT [42] and FPI [23] focus on inverting reference image back to the latent space and subsequently applying the inverted latent as condition guided by text. Additionally, methods like P2P [13], PnP [41], and MasaCtrl [5] modify the attention mechanism within diffusion models to enhance alignment between the guiding text and the consistency of generated images with their originals. Drawing inspiration from these techniques, our method facilitates image morphing in tuning-free manner. Notably, our approach also achieves comparable image editing performance by framing text-guided editing as special case of morphing between real and generated image. 3. Methodology Given two independent images, Ileft and Iright, as input, our objective is to generate sequence of intermediate images = {Ij}J j=1 that smoothly transforms from one to the other in tuning-free manner. We set = 5 for the experiments reported in this paper. As illustrated in Algorithm 1, our pipeline employs pre-trained diffusion model as its foundation and integrates guidance from the input images into the multi-step denoising process. In the subsequent sections, we first introduce the preliminaries that underpin our method in Sec. 3.1. Next, we describe the FreeMorph framework in detail. This framework comprises three main components: 1) the guidance-aware spherical interpolation (Sec. 3.2), which includes our proposed spherical feature aggregation and prior-driven self-attention mechanism; 2) step-oriented variation trend that enables controlled and consistent image morphing (Sec. 3.3); and 3) our improved forward diffusion and reverse denoising processes (Sec. 3.4). 3.1. Preliminaries Denoising Diffusion Implicit Model (DDIM). The Denoising Diffusion Implicit Model (DDIM) [38], trained on large-scale text-image datasets, is designed to reconstruct images from noisy inputs. After training, it establishes deterministic mapping from an initial noise state xT to an image x0, process we refer as reverse denoising steps: xt1 = αt1( xt 1 αtϵθ(xt) αt ) (cid:113) 1 αt1 σ2 ϵθ(xt, t) + σtϵt. + (1) Conversely, by inverting the formula above, we can derive the forward diffusion process, which incrementally adds noise to an image to predict its noise state: xt = (cid:114) αt αt1 xt1+ αt( (cid:114) 1 αt 1 (cid:114) 1 αt1 1)ϵθ(xt1, 1). (2) Latent Diffusion Model (LDM). Building upon DDIM, the Latent Diffusion Model (LDM) [31] is refined variant of diffusion models that effectively balances image quality with denoising efficiency. Specifically, LDM utilizes pre-trained variational auto-encoder (VAE) [20] to map images into latent space and then trains the diffusion model within this space. Furthermore, LDM enhances the UNet architecture by incorporating self-attention modules, cross-attention layers, and residual blocks to integrate text prompts as conditional inputs during image generation. The attention mechanism in LDMs UNet can be formulated as: ATT(Q, K, ) = softmax( dk ) (3) Figure 2. Replacing the key and value feature in the attention mechanism. We can observe that good key and value features would lead to smooth transitions and identity preservation. where denotes the query features from spatial data, and and are key and value features derived from either spatial data (for self-attention) or text embeddings (for crossattention). The noise estimator in LDM is then extended to ϵθ(xt, t, y), where denotes the text embedding. Our approach builds upon the Stable Diffusion model [39], pre-trained LDM developed by StabilityAI, and utilizes vision-language model (VLM), LLaVA [22], for generating captions for the input images. 3.2. Guidance-aware spherical interpolation Existing image morphing methods [25, 47, 49] typically involve training Low-rank Adaptation (LoRA) modules for each input image to enhance semantic comprehension and achieve smooth transitions. However, this approach is often inefficient and time-consuming and struggles with images that differ in semantics or layout. In this paper, we propose tuning-free image morphing approach built on the pre-trained Stable Diffusion model. By leveraging the capabilities of DDIM (as in Eq. 2) for image inversion and interpolation, one might consider converting the input images (Ileft, Iright) into latent features (z0left, z0right) and applying spherical interpolation may seem like simple straightforward solution: z0j = sin((1 j) ϕ) sinϕ z0left + sin(j ϕ) sinϕ z0right, (4) where [1, J] is the index of intermediate images, and zT 0leftz0right z0leftz0right ϕ = arccos( ). Recall that we set = 5 in our paper. However, directly inverting these interpolated latent features z0j to generate images often results in inconsistent transitions and identity loss (see Fig. 2). This issue arises because (1) the multi-step denoising process is highly non-linear, leading to discontinuous image sequences, and (2) there is no explicit guidance to control the denoising, causing the model to inherit biases from the pre-trained diffusion model. Spherical Feature Aggregation. Drawing insights from previous image editing techniques [5, 13, 27, 36, 41], we observed that using the features z0j as initialization and replacing the key and value features (K and ) in the attention mechanism (as described in Eq. 3) with features from the right image Iright can largely enhance the smoothness and identity preservation of the image transitions, although some imperfections may remain (see Fig. 2). Motivated by this finding, and recognizing that the query features (Q) largely reflect the overall image layout, we propose first blending features from both the left and right images (Ileft, Iright) to provide explicit guidance for the multi-step denoising process. Specifically, in the denoising step t, we first feed the latent of the input images ztleft and ztright to the pre-trained UNet ϵθ to obtain the key and value features. Following that, We then substitute the original and with those derived from the input images and compute their average to modify the attention mechanism: ATT(Qtj, Ktj, Vtj) : = 1 2 (ATT(Qtj, Ktleft, Vtleft) + ATT(Qtj, Ktright, Vtright)) (5) where Qtj, Ktj, Vtj are obtained by inputting ztj to the pre-trained UNet ϵθ. Note that ztj, ztleft and ztright are derived based on Eq. 3. Prior-driven Self-attention Mechanism. While our feature blending technique significantly improves identity preservation in image morphing, we found that using this approach uniformly in both forward diffusion and reverse denoising stages can result in transitions where the image sequences change minimally and fail to accurately represent the input images (see Fig. 6). This outcome is anticipated because the latent noise will largely influence the reverse denoising process, as shown in Fig. 3. Consequently, applying our feature blending, depicted in Eq. 5, introduces ambiguity as the consistent and strong constraints from the input images cause each latent noise to appear similar, thereby limiting the effectiveness of the transitions. To tackle this issue, we further propose prior-driven self-attention mechanism that prioritizes the latent features from spherical interpolation to ensure smooth transitions within the latent noise, while emphasizing the input images to maintain identity preservation afterward. Specifically, during the reverse denoising stage, we use the approach described in Eq. 5, while for the forward diffusion steps, we employ different attention mechanism Figure 3. Effectiveness of the latent noise on the generated images. The pre-trained diffusion model is robust to the noise distortion within the latent space. Algorithm 1 FreeMorph Input: Ileft, Iright 3.3. Step-oriented variation trend 1: Caption the input images via pre-trained LLaVA Textleft, Textright. 2: Obtain image features z0left, z0right, and text embedding yleft, yright via VAE and text encoder of pre-trained Stable Diffusion. 3: Applying spherical interpolation to obtain z0j where [1, J] as initialization. 4: Forward diffusion steps (from image to latent noise): for = 1 to do if < λ1 then Apply the original attention mechanism. else if < λ2 then Apply the prior-driven self-attention mechanism as in Eq. 6. else Apply the step-oriented motion flow as in Eq. 7. end if end for 5: High-frequency Gaussian noise injection. 6: Reverse denoising steps (from latent noise to image): for = 1 to do if < λ3 then Apply the step-oriented motion flow as in Eq. 7. else if < λ4 then Apply the spherical feature aggregation as in Eq. 5. else Apply the original attention mechanism. end if end for 7: Add text-conditioned features. Output: intermediate images gradually change from Ileft to Iright. as follows by modifying the self-attention modules: ATT(Qtj, Ktj, Vtj) :="
        },
        {
            "title": "1\nJ",
            "content": "k=1 (cid:88) ATT(Qtj, Ktk, Vtk) (6) Refer to Sec. 4.3 for detailed ablation studies on this design. After obtaining image sequences that are directional and accurately reflect the input identities, the next challenge is to achieve consistent and gradual transition from the left image Ileft to the right image Iright. This problem stems from the lack of \"variation trend\" that captures the changes from Ileft to Iright. To this end, we propose step-oriented variation trend that gradually changes the influence between the input images (Ileft and Iright): ATT(Qtj, Ktj, Vtj) : = (1 αj) ATT(Qtj, Ktlef t, Vtlef t) + αj ATT(Qtj, Ktright, Vtright), (7) where αj = j/(J + 2 1), with + 2 representing the total number of images, which includes the generated images and the 2 input images. 3.4. Forward diffusion and reverse denoising process High-frequency Gaussian Noise Injection. As discussed earlier, FreeMorph incorporates features from both the left and right images during the forward diffusion and reverse denoising stages. Nevertheless, we have observed that this can occasionally impose overly stringent constraints on the generation process. To mitigate this issue and allow for greater flexibility, we propose introducing Gaussian noise into the latent vector in the high-frequency domain after the forward diffusion steps: := (cid:40) IFFT(FFT(z)), IFFT(FFT(g)), if = 1 if = 0 (8) Here, IFFT() and FFT() denote the inverse fast Fourier transform and fast Fourier transform, respectively. (0, 1) represents randomly sampled noise vector, and is binary high-pass filter mask of the same size as z. Overall process. To enhance the efficacy of our image morphing process, we have found that consistently applying either guidance-aware spherical interpolation (Sec. 3.2) or step-oriented variation trend (Sec. 3.3) across all denoising steps yields suboptimal results (see Sec. 4.3). To address this, we have developed refined approach for both forward diffusion and reverse denoising processes. We provide an Table 1. Quantitative comparison with existing image morphing techniques. Method LPIPSsum MorphBench FIDmean PPLsum LPIPSsum Morph4Data FIDmean PPLsum LPIPSsum IMPUS [47] DiffMorpher [49] Spherical Interpolation Ours 130.52 90.57 119.77 84.91 152.43 157.18 169.17 141. 3263.03 2264.20 2994.35 2122.80 134.88 98.56 103.74 80.30 210.66 292.54 245.22 201.09 3199.90 2394.05 2593.58 2007.52 265.40 189.13 223.52 162.99 Overall FIDmean 174.76 209.10 198.34 152.88 PPLsum 6462.93 4658.25 5587.93 4192.82 Figure 4. More results produced by FreeMorph. Our method can achieve smooth and high-fidelity image transitions for input images with either similar or different semantics and layouts. overview algorithm of our proposed FreeMorph in Algorithm. 1. Specifically: Forward diffusion: We use the standard self-attention mechanism for the first λ1 steps. From λ1 to λ2 , we apply the feature blending technique from Eq. 6. For the remaining steps, we implement the step-oriented variation trend. as follows: λ1 = 0.3, λ2 = 0.6, λ3 = 0.2, λ4 = 0.6. Evaluation Datasets. DiffMorpher [49] introduced MorphBench, which includes 24 animation pairs and 66 image pairs, predominantly featuring images with similar semantics or layouts. To complement this dataset and mitigate potential biases, we introduce Morph4Data, newly curated evaluation dataset comprising four categories: 1) Class-A, consisting of 25 image pairs with similar layouts but different semantics, sourced from Wang and Golland [43]; 2) Class-B, containing image pairs with both similar layouts and semantics, including 11 pairs of faces from CelebA-HQ [16] and 10 pairs of various car types; 3) Class-C, featuring 15 pairs of randomly sampled images from ImageNet-1K [8] with no semantic or layout similarity; 4) Class-D, comprising 15 pairs of dog and cat images randomly sampled from the internet. Reverse denoising: We begin with the step-oriented variation trend for the first λ3 steps, followed by the feature blending method from Eq. 5 for steps between λ3 and λ4 . The process ends with the original self-attention mechanism for the final steps to produce images with higher fidelity. Here, λ1, λ2, λ3, and λ4 are hyper-parameters and = 50 is the total number of steps. 4. Experiments We evaluate the performance of FreeMorph across various scenarios, comparing it with state-of-the-art image morphing techniques and conducting ablation studies to highlight the effectiveness of our proposed components. Implementation Details. We use version 2.1 of the publicly available Stable Diffusion model. Both the forward diffusion and reverse denoising processes employ DDIM schedule with = 50 steps. It takes under 30 seconds for our method to produce morphing sequence using NVIDIA A100 GPU. Following the Stable Diffusion setup, we operate on an image resolution of 768 768. We set the classifier-free guidance (CFG) parameter to 7.5 to incorporate text-conditioned features. The hyperparameters are set 4.1. Quantitative Evaluations Following IMPUS [47] and DiffMorpher [49], we conducted quantitative comparisons using the following metrics: 1) Frechet Inception Distance (FID) [14], which assesses the similarity between the distributions of input and generated images; 2) Perceptual Path Length (PPL) [18], where we calculate the sum of PPL loss between adjacent images; and 3) Learned Perceptual Image Patch Similarity (LPIPS) [50], which we also sum for adjacent images to evaluate the smoothness and coherence of the generated transitions. The results, detailed in Table 1, demonstrate the superior performance of our method across both datasets, showing enhanced fidelity, smoothness, and directness. Figure 5. Qualitative comparison with existing image morphing techniques. Unlike other methods that struggle or fail to generate smooth and high-fidelity results without identity loss, our approach consistently achieves high-quality transitions, yielding superior results. Table 2. User studies. IMPUS [47] DiffMorpher [49] Preference 17.16% 14.89% Slerp 7.82% 60.13% Ours User studies To enhance our comparative analysis by including human preferences, we conducted user studies. We recruited 30 volunteers, including animators, AI experts, and gaming enthusiasts aged 20 to 35, to select their preferred results. Each participant was shown 50 random pairs of comparative results. The outcomes, presented in Table 2, demonstrate the subjective effectiveness of our proposed approach. Note that slerp denotes the method that only applies spherical interpolation. 4.2. Qualitative Evaluations Qualitative Results. In Fig. 1 and Fig. 4, we present wide range of results produced by FreeMorph, which consistently demonstrate its ability to generate high-quality and smooth transitions. FreeMorph excels across diverse scenarios, accommodating images with different semantics and layouts, as well as those with similar characteristics. FreeMorph also effectively handles subtle variations, such as cakes with different colors and individuals with different expressions. Qualitative Comparisons. We provide qualitative comparisons with existing image morphing methods in Fig. 5. An effective image morphing outcome should exhibit gradual transitions from the source (left) image to the target (right) image while preserving the original identities. Based on this criterion, several observations can be made: 1) When handling images with varying semantics and layouts, IMPUS [47] exhibits identity loss and produces unsmooth transitions; For instance, in the second example of Fig. 5, IMPUS exhibits (i) identity loss, where the third generated image deviates from the original identity, and (ii) an abrupt transition between the third and fourth generated images. 2) Although Diffmorpher [49] achieves smoother transitions than IMPUS, its results often suffer from blurriness and lower overall quality (see the first example in Fig. 5); 3) We also evaluate baseline approach, Slerp, which involves applying only spherical interpolation and the DDIM process. The visualizations show that this baseline approach struggles with (i) accurately interpreting the input images due to the absence of explicit guidance, (ii) suboptimal image quality, and (iii) abrupt transitions. In contrast, our method consistently delivers superior performance, characterized by smoother transitions and higher image quality. Additional comparisons are available in the Appendix. 4.3. Further Analysis Analysis of Guidance-aware Spherical Interpolation. In Fig. 6, we present ablation studies to evaluate the effects of the proposed spherical feature aggregation (Eq. 5) and the prior-driven self-attention mechanism (Eq. 6). The results indicate that using either component alone produces suboptimal outcomes. Specifically, (i) spherical feature aggregation is crucial for achieving directional transitions in which the characteristics of Ileft gradually diminish, and (ii) the prior-driven self-attention mechanism is vital for preserving identity in the generated images. The combination of Table 3. Quantitative comparison for ablation studies. Method w/ only Eq. 6 w/ only Eq. 5 w/ only Eq. 6 and Eq. 5 w/o noise injection w/o Eq. 5 w/o Eq. 6 w/o step-oriented motion flow Ours (Var-A) Ours (Var-B) Ours LPIPSsum MorphBench FIDmean PPLsum LPIPSsum Morph4Data FIDmean PPLsum LPIPSsum Overall FIDmean PPLsum 157.01 99.69 211.52 99.49 87.41 120.01 118.50 153.40 93.54 84. 320.05 155.51 243.08 154.53 155.46 148.54 154.71 184.54 158.44 3425.19 2491.10 5288.10 2487.16 2185.30 3000.35 2962.48 3835.08 2338.62 141.32 2122.80 141.12 90.80 139.55 89.12 81.10 101.28 93.39 115.91 85.76 80. 411.80 217.26 290.11 211.23 218.95 215.43 214.93 243.20 245.36 3028.05 2270.05 3488.87 2228.03 2027.58 2572.06 2334.68 2897.63 2144.08 201.09 2007.52 298.13 190.49 351.08 188.61 168.52 221.30 211.89 269.31 179.31 162. 355.24 179.20 261.12 176.28 179.82 174.19 177.80 207.04 191.78 6453.24 4761.15 8776.96 4715.19 4212.88 5572.41 5297.17 6732.70 4482.70 152.88 4192.82 Figure 6. Analysis of guidance-aware spherical interpolation. Figure 7. Analysis of reverse diffusion and forward denoising process. both components allows FreeMorph to produce smooth transitions while effectively maintaining identity. By comparing the last two rows in Fig. 6, we demonstrate the importance of our step-oriented variation trend and the specially designed reverse and forward processes. Analysis of Reverse and Forward Process. In Fig. 7, we evaluate our method against two variants: (i) Ours (Var-A), which omits the original attention mechanism, and (ii) Ours (Var-B), which swaps the application steps of the guidanceaware spherical interpolation and the step-oriented variation trend in both the reverse and forward processes. comparison of these variants with our final design reveals that (i) the original attention mechanism is crucial for achieving high-fidelity results, and (ii) the specific configuration of the reverse and forward processes in our final design yields optimal performance. Analysis of Step-oriented Variation Trend. In Fig. 8, we first disable the proposed step-oriented variation trend to asFigure 8. Analysis of high-frequency noise injection and steporiented motion flow. A1: w/o step-oriented motion flow; A2: w/o high-frequency noise injection sess its impact. We observe that without this component, the model tends to produce abrupt changes rather than smooth transitions. Additionally, the final generated image exhibits high-contrast colors that differ from the target image Iright. In contrast, the step-oriented variation trend enables our method to achieve smoother transitions and produce final image that is more closely aligned with the target image. Analysis of High-frequency Noise Injection. We then disable high-frequency noise injection and present the corresponding ablation study in Fig. 8. The results indicate that incorporating the proposed high-frequency noise injection enhances the models flexibility and contributes to smoother transitions. 5. Conclusion We have introduced FreeMorph, novel tuning-free pipeline capable of generating smooth, high-quality transitions between two input images in under 30 seconds. Specifically, we propose incorporating explicit guidance from the input images by modifying the self-attention modules. This is achieved through two novel components: spherical feature aggregation and prior-driven self-attention mechanism. Additionally, we introduce step-oriented variation trend to ensure directional transitions consistent with both input images. We also designed an improved forward diffusion and reverse denoising process to integrate our proposed modules into the original DDIM framework. Extensive experiments demonstrate that FreeMorph delivers high-fidelity results across various scenarios, significantly outperforming existing image morphing techniques."
        },
        {
            "title": "References",
            "content": "[1] Alyaa Qusay Aloraibi. Image morphing techniques: review. Technium, 9, 2023. 2 [2] Thaddeus Beier and Shawn Neely. Feature-based image metamorphosis. In SIGGRAPH, pages 3542. ACM, 1992. 2, 3 [3] Forest Black. Flux.1. https://blackforestlabs.ai/announcingblack-forest-labs/, 2024. 2 [4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2019. [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 3, 4 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [7] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 13 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [9] Karl Fant. nonaliasing, real-time spatial transform techIEEE Computer Graphics and Applications, 6(1): nique. 7180, 1986. 2, [10] Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman, and Connelly Barnes. Image morphing with perceptual constraints and stn alignment. In Computer Graphics Forum, pages 303313. Wiley Online Library, 2020. 3 [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2 [12] Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, and Humphrey Shi. Smooth diffusion: Crafting smooth latent spaces in diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 75487558, 2024. 12 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023. 3, 4 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 6 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [16] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR. OpenReview.net, 2018. [17] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, pages 44014410. Computer Vision Foundation / IEEE, 2019. 2 [18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. pages 81108119, 2020. 6 [19] Valentin Khrulkov, Gleb V. Ryzhakov, Andrei Chertkov, and Ivan V. Oseledets. Understanding DDPM latent codes through optimal transport. In ICLR. OpenReview.net, 2023. 3 [20] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [21] Tong-Yee Lee, Young-Ching Lin, YN Sun, and Leeween Lin. Fast feature-based metamorphosis and operator design. In Computer Graphics Forum, pages 1522. Wiley Online Library, 1998. 2, 3 [22] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 4 [23] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, and Rami Ben-Ari. Fixed-point inversion for text-toimage diffusion models. arXiv preprint arXiv:2312.12540, 2023. 3 [24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR. OpenReview.net, 2022. [25] Chigozie Nri. Differentiable morphing. https://github.com/volotat/DiffMorph, 2022. 2, 4 [26] Sanghun Park, Kwanggyoon Seo, and Junyong Noh. Neural crossbreed: neural based image metamorphosis. ACM Transactions on Graphics (TOG), 39(6):115, 2020. 3 [27] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In SIGGRAPH (Conference Paper Track), pages 11:111:11. ACM, 2023. [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2 [29] He Qiyuan, Jinghao Wang, Ziwei Liu, and Angela Yao. Aid: Attention interpolation of text-to-image diffusion. Advances in Neural Information Processing Systems, 2024. 3, 12 [30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10674 10685. IEEE, 2022. 2, 3 [48] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 13 [49] Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of diffusion models for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 4, 6, 7 [50] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [51] Bhushan Zope and Soniya Zope. survey of morphing techniques. International Journal of Advanced Engineering, Management and Science, 3(2):239773, 2017. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [33] Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space exploration for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 3 [34] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International conference on machine learning, pages 3010530118. PMLR, 2023. 2 [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 3 [36] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In CVPR, 2024. 4 [37] Douglas Smythe. two-pass mesh warping algorithm for object transformation and image interpolation. Rapport technique, 1030:31, 1990. 2, [38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2, 3 [39] Stability.AI. Stable diffusion. https://stability. ai/blog/stable-diffusion-public-release, 2022. 2, 4, 27 [40] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2 [41] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-toimage translation. In CVPR, pages 19211930. IEEE, 2023. 3, 4 [42] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In CVPR, pages 2253222541, 2023. [43] Clinton Wang and Polina Golland. Interpolating between images with diffusion models. arXiv preprint arXiv:2307.12560, 2023. 2, 3, 6, 14 [44] George Wolberg. Digital image warping. IEEE computer society press Los Alamitos, CA, 1990. 2, 3 [45] George Wolberg. Recent advances in image morphing. Proceedings of CG International96, pages 6471, 1996. 2 [46] George Wolberg. Image morphing: survey. The visual computer, 14(8-9):360372, 1998. 2 [47] Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh, Jing Zhang, Dylan Campbell, Peter Tu, and Richard Hartley. Impus: Image morphing with perceptually-uniform sampling using diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 4, 6, 7 A. Further Analysis A.1. Usage of the Fast Fourier Transform (FFT) In our approach, we employ the fast Fourier transform (FFT) to inject high-frequency Gaussian noise, which enhances flexibility. An alternative and straightforward variation involves replacing the FFT with the discrete cosine transform (DCT). To investigate this, we conducted experiments using both FFT and DCT, presenting the results in Fig. 9. The findings indicate that DCT performs comparably to FFT. Figure 9. Analysis of the usage of Fast Fourier Transform (FFT) over Discrete Cosine Transform (DCT). B. Qualitative Comparisons B.1. Qualitative Comparisons with AID [29] and Smooth Diffusion [12] In addition to the comparisons discussed in the main paper, we extend our evaluation to include AID [29] and Smooth Diffusion [12]. As illustrated in Fig. 10 and Fig. 11, the results demonstrate that both methods are limited to processing images with similar layouts and semantics, rendering them ineffective for inputs with different layouts or semantics. Beyond their qualitative shortcomings, it is worth noting that (1) AID relies on IP-Adapter for image morphing, which adversely affects training efficiency, and (2) Smooth Diffusion requires parameter tuning, making it slower and less efficient than our approach. Figure 10. Qualitative comparisons with AID [29]. Figure 11. Qualitative comparisons with Smooth Diffusion [12] B.2. Comparison with video generative models Given the rapid development of video generative techniques. Methods like PixelDance [48] and SEINE [7] have been designed to achieve image morphing. We hereby provide more comparisons with these video generative models to demonstrate our performance. Considering PixelDance hasnt released code or an online demo, we ran FreeMorph on the examples from their webpages to perform qualitative comparisons (see Fig. 12 below). Surprisingly, our method performs similarly with PixelDance and outperforms SEINE in reducing ghost artifacts. Figure 12. Comparisons with video generative models. B.3. Comparison with GAN-based morphing methods We further compare our method with the early GAN-based morphing method (Neural Crossbreed) to demonstrate the performance. The results, presented in Fig. 13, show superior image quality, identity preservation, and smoother transitions. Unlike GAN-based approaches, ours is training-free, is able to handle out-of-domain inputs, and remains robust to varying layouts and semantics. Additional evaluations and discussions will be included in the revised version. Figure 13. Comparison with GAN-based morphing methods. B.4. Comparison with Wang and Golland [43] We further compare with Wang and Golland [39] and present the results in Fig. 14. We can clearly observe that our method consistently show superior performance over it, both qualitatively and quantitatively. Figure 14. Comparison with Wang and Golland [43]. B.5. Experiments with different poses/actions We further present results for various poses and actions below  (Fig. 15)  , using input images from the MorphBench dataset. Figure 15. Qualitative results with different poses/actions. B.6. Additional Qualitative Comparisons We provide additional qualitative comparisons with three methods in Fig. 16Fig. 23. These results reinforce the conclusions drawn in Sec. 4.2 of the main paper, offering further evidence of the superior performance of our FreeMorph method in achieving high-fidelity and smooth image morphing. Figure 16. More qualitative comparisons with existing techniques (Part I). Figure 17. More qualitative comparisons with existing techniques (Part II). Figure 18. More qualitative comparisons with existing techniques (Part III). Figure 19. More qualitative comparisons with existing techniques (Part IV). Figure 20. More qualitative comparisons with existing techniques (Part V). Figure 21. More qualitative comparisons with existing techniques (Part VI). Figure 22. More qualitative comparisons with existing techniques (Part VII). Figure 23. More qualitative comparisons with existing techniques (Part VIII). C. More Qualitative Results To provide better understanding of the intermediate generated transitions, in addition to the animated videos, we also present generated images in Fig. 24Fig. 27, which correspond to the animated videos in the HTML file. Figure 24. Images with different semantics and different layouts. Figure 25. Images with similar semantics and similar layouts. Figure 26. Images with different semantics and similar layouts. Figure 27. Images with similar semantics and different layouts. D. Visualization of Morph4Data We present range of visualizations from our collected Morph4Data to enhance understanding of the dataset and the distinctions among its different classes. Figure 28. Examples of 4 classes in Morph4Data. E. Applications We highlight that our FreeMorph method can be adapted for image editing tasks. Specifically, this is accomplished by (1) using the same image as both the \"input source\" and \"input target,\" and (2) employing different text prompts, where the first prompt describes the original image and subsequent prompts indicate the desired editing direction. An example is provided in Fig. 29. Notably, our method produces image editing results that align correctly with the text prompts, preserving the original identity while effectively generating smooth transitions throughout the editing process. Figure 29. Application of FreeMorph in image editing F. Limitations and Failure Cases While our method establishes new state-of-the-art, we acknowledge that it has certain limitations. We illustrate several failure cases in Fig. 30. Specifically: 1) Although our model can achieve reasonable results when processing images with no semantic or layout similarity, the generated transitions may not be smooth, potentially leading to abrupt changes. 2) Our method inherits biases from Stable Diffusion [39], resulting in difficulties in accurately transitioning images that model human limbs. Figure 30. Failure cases. G. Societal Impact Our research advances the image morphing task across range of semantics and layouts, establishing more versatile pipeline. However, there is risk of misuse, such as brands creating misleading advertisements that distort consumer perceptions and create unrealistic product expectations. This practice not only undermines consumer trust but also raises significant ethical concerns about the authenticity of marketing. Additionally, the complexities of copyright and consent are amplified, as manipulated images blur the lines of ownership and accountability. Therefore, we advocate for strict legal compliance and usage restrictions to regulate the application of image morphing techniques and derivative models."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "S-Lab, Nanyang Technological University",
        "The Chinese University of Hong Kong"
    ]
}