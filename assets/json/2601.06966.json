{
    "paper_title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "authors": [
        "Haonan Bian",
        "Zhiyuan Yao",
        "Sen Hu",
        "Zishan Xu",
        "Shaolei Zhang",
        "Yifu Guo",
        "Ziliang Yang",
        "Xueran Han",
        "Huacan Wang",
        "Ronghao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench)."
        },
        {
            "title": "Start",
            "content": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction Haonan Bian1,*, Zhiyuan Yao2,*, Sen Hu3,*,, Zishan Xu4, Shaolei Zhang5, Yifu Guo6, Ziliang Yang1, Xueran Han5, Huacan Wang7, Ronghao Chen3 2026-01-13 1Xidian University, 2Zhejiang University, 3Peking University, 4Shanghai Jiao Tong University, 5Renmin University of China, 6Sun Yat-sen University, 7University of the Chinese Academy of Sciences *Equal contribution. Corresponding author: husen@pku.edu.cn 6 2 0 2 J 1 1 ] . [ 1 6 6 9 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture long-term project-oriented interactions where agents must track evolving goals. To bridge this gap, we introduce RealMem, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at https://github.com/AvatarMemory/RealMemBench."
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have achieved remarkable progress across multiple dimensions (Du et al., 2025b,a; Guo et al., 2025; Luo et al., 2025). There is growing consensus that AI agents must evolve from turn-based chatbots into long-term companions capable of sustained, context-aware collaboration (Tran et al., 2025; Sapkota et al., 2025). In this paradigm shift, memory systems have emerged as foundational component (Hu et al., 2025). Robust memory is essential not only for practical applications such as personalized chatbots (Li et al., 2025a) and financial analysis (Zhang et al., 2024), but also as key enabler toward Artificial General Intelligence (AGI) (Fang et al., 2025a). Real-world memory-driven interaction empowers agents to maintain coherent, long-term collaboration (Zhang et al., 2025b). We posit that long-term project-oriented interactions is governed by four fundamental imperatives: (1) Endogenous Query Nature: Queries arise organically from task progression, rather than appearing as isolated fact-checking questions. (2) Interleaved Distribution: User inquiries are interwoven across fragmented sessions. (3) Dynamic State Evolution: The interaction environment is inherently non-stationary, demanding continuous synchronization of memory with constantly evolving information states (Majumder et al., 2023; Zhang et al., 2025a). (4) Proactive Contextual Alignment: Agents should proactively resolve ambiguous intents by leveraging memory details (e.g., schedule management), and simultaneously maintain granular state updates that capture situational transitions rather than simplistic factual overwrites. To contextualize these requirements, we categorize useragent interactions into three paradigms, as illustrated in Figure 1: casual conversation, task-oriented dialogue, and long-term project-oriented interactions. We argue that the third paradigm constitutes crucial component of real-world memorydriven interactions. For example, in fitness program spanning six months, an AI assistant can function as personal trainer, providing continuous guidance by leveraging accumulated memories from past 1 Figure 1: Comparison of three interaction paradigms in humanagent interactions: (a) casual conversation, (b) task-oriented dialogue, and (c) long-term project-oriented interactions spanning multiple sessions with interleaved projects and evolving context. interactions. Existing dialogue memory benchmarks primarily focus on the first two interaction paradigms. For example, LoCoMo (Maharana et al., 2024) is limited to simulating humanhuman social chit-chat, while LongMemEval (Wu et al., 2024) approximates task dialogues via artificial needle-in-a-haystack tests. Consequently, their information flow remains discrete and episodic, failing to reflect the coherence of real-world workflows. To bridge this gap, we introduce RealMem, benchmark explicitly constructed to evaluate Real-World Memory-Driven Interaction through the lens of long-term projects. Grounded in eleven representative scenarios requiring robust long-term memory, RealMem comprises over 2,000 cross-session dialogues, thereby shifting the evaluation paradigm from isolated fact retrieval to project-centric memory utilization. RealMem evaluates agents using natural user queries that are organically interwoven across fragmented sessions. Our core focus is to assess its capacity to leverage accumulated memory and maintain the coherent thread of project to fulfill user requests within realistic and evolving context. We design three-stage synthesis pipeline comprising Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management. This pipeline simulates the continuous evolution of long-term interactions, ensuring that memory is not predefined but dynamically emerges and evolves alongside the dialogue trajectory. Furthermore, we define diverse set of query types (as illustrated in Figure 2) to rigorously evaluate the agents memory system. Through extensive evaluation on RealMem, we demonstrate that existing agent memory systems are inadequate for the dynamic requirements of long-term project-oriented interactions. Our contributions are summarized as follows: 2 Benchmark RealMem HaluMem LongMemEval LoCoMo Dialogue Type QA Timing Long-term project-oriented Interleaved within sessions Persona Creation Task-oriented Casual conversation After each session After all sessions After all sessions Query Source Natural user queries External queries External queries External queries Proactive Alignment Project States Memory Max Memory Items Memory Content 20 9 6 19 Project States, Schedules, Personas Personas, Isolated Events, Relationships Personas, Isolated Events Personas, Isolated Events Table 1: Comparison of representative memory benchmarks. RealMem distinguishes itself by introducing proactive alignment and project state memory into long-term interactions, shifting the evaluation paradigm from isolated fact-checking to continuous, natural dialogue progression. We identify critical gap between existing memory benchmarks and real-world memory-driven interaction: long-term project-oriented scenarios. To address this gap, we introduce RealMem, which shifts the evaluation focus from post hoc fact retrieval to the proactive use of memory within ongoing conversations to generate more effective responses. We propose three-stage synthesis pipeline for constructing long-term project-oriented dialogues. This framework ensures global logical coherence across sessions and projects, while supporting the dynamic evolution of fine-grained memories. Extensive evaluations reveal that state-of-the-art agent memory systems struggle to maintain coherent project threads. Our findings expose critical gap in existing models ability to proactively align with evolving contexts in long-term interactions."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Long-Term Memory Benchmarks number of benchmarks (Wei et al., 2025; Jiang et al., 2025) have been proposed to evaluate the memory capabilities of LLM-based agents. In this work, we focus on representative benchmarks most relevant to dialogue-based settings, as summarized in Table 1. LoCoMo (Maharana et al., 2024) established an end-to-end evaluation paradigm for memory retention via question answering over ultra-long contexts, though it primarily assesses static information recall without explicit updates. LongMemEval (Wu et al., 2024) extended this framework by incorporating memory updating mechanisms to rigorously quantify retrieval precision and knowledge consistency across multi-year spans. HaluMem (Chen et al., 2025) utilizes injected persona profiles to assess hallucinations over extended timeframes, specifically measuring the consistency of generated responses against pre-defined facts. However, these methods predominantly rely on externally constructed QA tasks to evaluate isolated factual details, often overlooking the dynamic state consistency essential for practical applications. In contrast, RealMem targets long-term project-oriented interactions. It requires agents not only to recall static facts but to actively track evolving project states and handle interleaved queries that arise naturally across fragmented sessions. 3 Figure 2: Examples of four query types in REALMEM: (1) Temporal Reasoning resolves temporal constraints and schedule conflicts; (2) Static Retrieval ensures continuity by recalling accumulated context; (3) Dynamic Updating synchronizes memory with evolving project states; and (4) Proactive Alignment leverages user memory to anticipate implicit intents and goals. 2.2 Agent Memory Systems To support effective long-term interaction, existing approaches commonly adopt RAG-based mechanisms for memory retrieval (Gutiérrez et al., 2025; Thang et al., 2025), and variety of agent memory systems have been proposed. These systems typically distinguish between experiential memory (Tang et al., 2025b; Ouyang et al., 2025; Wang et al., 2024; Kim et al., 2025), derived from reflections on conversational history, and factual memory (Li et al., 2025b; Fang et al., 2025b; Wang and Chen, 2025; Rasmussen et al., 2025), which records knowledge acquired through agentuser and agentenvironment interactions. A-mem (Xu et al., 2025) proposes an agentic memory organization framework to improve memory effectiveness in long-term interactions. Mem0 (Chhikara et al., 2025) introduces scalable memorycentric architecture with graph-based representations to capture relational structures across dialogue elements. MemoryOS (Kang et al., 2025) adopts operating-system-inspired memory management to improve retrieval and update efficiency. Graph Memory (Hu et al., 2026) further enhances the knowledge graphs capacity to represent memory by allowing entity descriptions to record relevant events and remain up to date."
        },
        {
            "title": "3 Method",
            "content": "To evaluate memory systems in complex and evolving contexts, we introduce the RealMem benchmark. Inspired by recent advances in agentic frameworks (Figure 3) that leverage multi-agent collaboration for complex problem solving (Huang et al., 2024; Liu et al., 2024; Tang et al., 2025a), RealMem simulates realistic user behavior in which multiple long-term projects are managed concurrently within unified SuperApp environment. Its design is structured around three tightly coupled stagesstatic project foundation construction, dynamic interaction generation, and closed-loop memory feedbackwhich together enable systematic assessment of an agents ability to utilize, update, and maintain dynamic memory over extended interactions. 4 Figure 3: Overview of the data synthesis framework. The pipeline consists of three cascaded stages: (1) Project Foundation Construction, which initializes user personas and hierarchical project skeletons (i.e., blueprints, events, and sessions); (2) Multi-Agent Dialogue Generation, where the User Agent and Assistant Agent simulate interactions based on the session queue and dynamic context; and (3) Memory and Schedule Management, which iteratively retrieves, updates, and deduplicates memory points and schedule tables to ensure long-term consistency. 3.1 Project Foundation Construction The Project Foundation Construction phase initializes the static context and structural scaffolding required for subsequent dialogue generation. As long-horizon generation demands explicit planning to preserve global coherence (Xia et al., 2025), we adopt hierarchical strategy to construct Project Skeleton, following blueprint-first paradigm that has proven effective in recent multi-turn data generation frameworks (Prabhakar et al., 2025; Chen et al., 2025). Specifically, this phase establishes user Persona to encode demographics for behavioral consistency, and Project Goal to specify quantitative objective (e.g., losing 15 kg in six months) as clear long-term target. Crucially, we introduce Project Attributes as dynamic state variables that serve as the core mechanism for project modeling, tracking temporal evolution to capture changes in user progress and contextual state. To reflect realistic user needs and assess the generality of our framework, we curate eleven representative scenarios spanning four application domains. For each scenario, we identify set of core attributesranging from schedule adjustments to emotional regulationto capture the multifaceted nature of long-term humanAI interaction. The generation process proceeds in three stages. First, Project Blueprint is constructed to outline high-level milestones. Second, an Event List is generated to encode causal dependencies among milestones. Third, Session Summaries are produced to guide individual dialogue sessions. This hierarchical decomposition ensures that local interactions remain aligned with the global project narrative, mitigating fragmentation commonly observed in long-context generation (Xia et al., 2025; Prabhakar et al., 2025). Finally, to simulate concurrent task management, session summaries from multiple projects are aggregated and interleaved to form unified session queue. 3.2 Multi-Agent Dialogue Generation Inspired by recent advances in multi-agent data synthesis (Liu et al., 2024; Xia et al., 2025), we adopt dual-agent framework consisting of User Agent and an Assistant Agent to simulate realistic useragent interactions. Compared to one-shot generation of multi-turn dialogues, this simulation-based paradigm provides finer-grained control over interaction dynamics and policy constraints, as demonstrated in prior work such as IntellAgent (Levi and Kadar, 2025). 5 To ensure long-term consistency, each dialogue session is generated under structured context spanning three dimensions: (1) static project background, (2) memory points extracted from historical interactions, and (3) established schedules. Dialogue generation proceeds at the session level, with sessions sequentially drawn from an interleaved queue that simulates concurrent project management. For the User Agent, the context includes information about the current event together with summary of the immediately preceding event, enabling coherent progression awareness without exposing future plans. Importantly, we restrict the User Agents input to session-relevant summaries only, thereby enforcing explicit task boundaries and preventing premature reasoning over future tasks. In contrast, the Assistant Agent is provided with all memory points relevant to the current session to support informed decision making. To further mitigate temporal inconsistencies frequently observed in LLM-based generation (e.g., scheduling conflicts across projects), we incorporate Global Schedule into the Assistant Agents context. This design substantially improves temporal alignment and logical coherence across long-term, multi-project interactions. 3.3 Memory and Schedule Management This stage processes the generated dialogue data to form closed feedback loop. To this end, our framework employs set of specialized agents to validate, consolidate, and update the system state, ensuring that memory evolves accurately and consistently over time. Specifically, Memory Extraction Agent parses raw dialogues to identify salient facts and converts them into structured memory points. In parallel, Schedule Agent detects tasks with explicit temporal attributes (e.g., scheduled meetings or deadlines) and updates the global schedule accordingly. To improve storage efficiency and reduce noise, we introduce Deduplication Agent that performs semantic-level deduplication over the memory base, removing redundant or overlapping entries. In subsequent dialogue generation cycles, the Session Summary is used as query to retrieve relevant information from the optimized memory base, which is then provided as contextual input to the Assistant Agent. This closed-loop design enables continuous memory refinement and supports coherent long-term interactions across sessions."
        },
        {
            "title": "4 Evaluation",
            "content": "To comprehensively assess retrieval quality, we adopt hybrid evaluation strategy that combines standard quantitative metrics with LLM-based semantic judgments (Liu et al., 2023). For retrieval performance, we report Recall@k and NDCG@k (k {10, 20}) to measure strict ranking accuracy. To capture semantic relevance beyond exact lexical matching, we further introduce LLM-based metrics: Mem Recall, which assesses the semantic coverage of relevant information, and Mem Helpful, which evaluates the practical usefulness of the retrieved context for answering the query. For generation quality, we employ the QA Score, which is derived from response consistency rubric (ranging from conflicting to fully state-aligned). This metric explicitly evaluates whether the agent correctly incorporates the users dynamic state, rather than merely producing fluent but context-agnostic responses."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experiment Setup We conduct systematic evaluation of representative state-of-the-art memory systems on our proposed RealMem benchmark, including Mem0 (Chhikara et al., 2025), A-mem (Xu et al., 2025), MemoryOS (Kang et al., 2025), and Graph Memory (Hu et al., 2026). For A-mem, MemoryOS, and Graph Memory, we strictly follow the embedding models and hyperparameter settings recommended in their original papers; for Mem0, we use text-embedding-3-small (OpenAI, 2024) as its default embedding model. We utilize GPT-4o-mini for memory extraction. For the answer generation phase, we employ both GPT-4o-mini and GPT-4o (Hurst et al., 2024) to assess performance across different backbones. Finally, to ensure consistency, we adopt GPT-4o as the LLM-as-a-Judge for all evaluations. 6 Method Graph Mem Mem0 A-mem MemoryOS Oracle Value = Memory Value = Session QA-4o-Mini QA-4o Mem Rec. Mem Help. QA-4o-Mini QA-4o Mem Rec. Mem Help. 0.474 0.449 0.416 0.490 0.683 0.497 0.514 0.492 0. 0.804 0.490 0.529 0.455 0.532 0.993 0.598 0.634 0.513 0.606 0.922 0.539 0.526 0.504 0.580 0.567 0.609 0.606 0.696 0.608 0.594 0.590 0.767 0.708 0.666 0.655 0.845 Table 2: Generation performance comparison. We report QA Score (on GPT-4o-mini and GPT-4o), Mem Recall, and Mem Helpful scores. Memory denotes memory-only context, while Session includes full session context. Bold indicates the best performance among non-oracle methods. Note that Value = session metrics are not applicable to MemoryOS due to its inherent mechanism. Method R@10 R@20 NDCG@10 NDCG@20 Graph Mem 0.6344 0.6055 Mem0 0.5760 A-mem 0.7017 0.6515 0. 0.5654 0.5251 0.5211 0.5826 0.5398 0.5560 Table 3: Retrieval performance evaluated on session-level context. Metrics include Recall@K (R@K) and NDCG@K. Bold indicates the best performance. During the answer generation stage, we evaluate each method under two independent context construction settings. In the memory-only setting ( Value = memory ), the model is provided solely with the Top-20 memory entries retrieved by the memory system. In the session-based setting ( Value = session ), memory entries are not directly used; instead, the model receives the corresponding Top-5 original dialogue sessions associated with the retrieval results, in order to assess generation performance based on session-level context. We note that MemoryOS internally maintains memory at the page level and does not support alignment with original dialogue sessions; as result, it cannot be evaluated under the session-based setting. 5.2 Overall Evaluation The results on generation tasks offer critical insights into the architectural requirements for long-term project agents. First, under the Memory-only setting, the superior performance of MemoryOS validates the effectiveness of hierarchical memory architectures (e.g., STM, MTM, and LTM). It demonstrates that effective compression and indexing of key information allow agents to maintain high accuracy even without full session history. Second, when Session context is available, Graph Memory achieves the highest scores. This indicates that the RealMem benchmark involves intricate entity relationships, where graph structures excel at capturing complex dependencies, thereby providing more precise context than linear history alone. Furthermore, cross-model comparison reveals the challenging nature of realistic project scenarios. The QA accuracy of GPT-4o is consistently higher than that of GPT-4o-mini. Crucially, even with Oracle memory, GPT-4o-mini exhibits limited performance. This suggests that solving the complex queries in RealMem necessitates not only accurate retrieval but also high-performance foundation models with strong reasoning capabilities. Finally, the substantial gap between all methods and the Oracle (e.g., Oracle Recall reaches 0.993) highlights significant room for improvement in capturing long-term context dependencies. This underscores key finding: memory can effectively replace raw session history only when retrieval quality is sufficiently close to the theoretical upper bound. The retrieval results further clarify the correlation between retrieval metrics and downstream generation quality in the RealMem benchmark. While A-mem achieves the highest Recall@20 (0.7235), its lower NDCG scores suggest it retrieves broader but noisier set of information. Conversely, Graph Memory dominates in NDCG metrics (0.5654 @10), aligning with its superior generation performance in Table 3. 7 Method Dyn. Inc. Dyn. Upd. Proac. Align. Temp. Reas. Single Sess. Multi Sess. Memory Category Session Type QA Rec. QA Rec. QA Rec. QA Rec. QA Rec. QA Rec. 0.404 0.465 0.453 0.427 0.483 0.436 0.278 0.361 0.421 0.454 0.409 0.457 A-mem 0.436 0.529 0.462 0.553 0.538 0.506 0.347 0.539 0.457 0.542 0.437 0.509 Mem0 MemoryOS 0.478 0.536 0.521 0.557 0.571 0.490 0.319 0.448 0.505 0.565 0.468 0.482 Graph Mem 0.470 0.500 0.483 0.512 0.513 0.392 0.375 0.559 0.483 0.516 0.461 0.451 Table 4: Overall performance comparison (QA Score and Recall). Methods are listed in rows, while memory categories and session types are arranged in columns. Abbreviations used in the header: Dyn. Inc. (Dynamic Incremental), Dyn. Upd. (Dynamic Updating), Proac. Align. (Proactive Alignment), and Temp. Reas. (Temporal Reasoning). This discrepancy highlights critical characteristic of the benchmark: for long-term project dialogues, precision and ranking quality (NDCG) are more decisive than mere coverage (Recall). High recall with low precision introduces noise that distracts the LLM, whereas high NDCG ensures that the most relevant context is prioritized, directly translating to better response quality. 5.3 Performance on Different Question Types Table 4 presents fine-grained performance analysis across different memory categories and session types. We summarize the key observations as follows: Our results highlight distinct architectural strengths across different complexity dimensions. MemoryOS demonstrates robust superiority in handling dynamic information, achieving the highest scores in both Static Retrieval and Dynamic Updating categories (e.g., 0.521 QA). This validates the effectiveness of its hierarchical architecture in tracking evolving user states and managing fragmented updates. Conversely, Graph Memory dominates the Temporal Reasoning category (QA 0.375, Recall 0.559). Regarding session types, performance across all models generally declines in Multi Session settings compared to Single Session, highlighting the inherent difficulty of maintaining long-range context. Nevertheless, MemoryOS maintains leading QA performance in both settings (Single: 0.505, Multi: 0.468), demonstrating its robustness in handling diverse interaction spans. Additionally, in the Proactive Alignment category, MemoryOS achieves the highest QA score (0.571), indicating superior capability in anticipating future steps and aligning with the users long-term objectives. 5.4 Scenario-Based Performance Analysis In this section, we present the memory system performance across various topics. To visualize trends, we select MemoryOS, the most robust model, as the representative. Notably, other baselines exhibit similar tendencies; detailed results are provided in the Appendix. As illustrated in Figure 4, the efficacy of MemoryOS reveals significant domain-dependent variations. The system exhibits its strongest performance in consultative and creative domains, such as Mental Health Support, Health Consultation, and Literary Creation. In these dimensions, the Helpful Score (indicated by the orange trajectory) consistently outperforms other metrics, approaching or exceeding the 0.6 to 0.8 range. This trend suggests that for openended, human-centric tasks, the system is capable of providing highly valuable feedback and maintaining coherent narrative flow, even when precise Recall is not optimal. Figure 4: Average performance scores of MemoryOS across various topics. Conversely, the system encounters severe challenges in highly technical and rigid domains. This is most 8 distinct in Code Architecture, where all three metricsQA Score, Recall, and Helpful Scoreshow sharp contraction towards the center of the chart, dropping significantly below 0.4. Unlike soft-constraint tasks like Financial Planning, the rigorous dependency tracking and strict logical consistency required for complex engineering tasks expose structural limitation in the current architectures ability to handle hard constraints effectively. 5.5 Efficiency and Cost Analysis of Long-Term Memory Systems Method Avg. Add Memory Time (s) Avg. Retrieve Memory Time (s) Cost Token Graph Mem Mem0 A-mem MemoryOS 26.898 11.751 8.663 14.755 0.18803 1.261 0.013 0. 1224.02 4178.18 500.32 9201.01 Table 5: Comparison of efficiency and cost across different memory systems. We report the average latency for memory addition and retrieval, alongside total token consumption. Table 5 reveals critical bottleneck: memory incorporation latency consistently exceeds retrieval speeds across all systems. This underscores the urgent need to optimize memory ingestion mechanisms for scalable deployment. Our evaluation of system latency and token consumption reveals significant trade-off between operational overhead and model performance. Specifically, while MemoryOS achieves competitive retrieval latency (0.151 s), it incurs the highest token cost (9,201), indicating that its superior performance necessitates extensive context maintenance. Conversely, A-mem demonstrates the lowest latency and resource consumption, yet as noted previously, this efficiency comes at the expense of retrieval accuracy and response quality. Graph Memory offers viable compromise by delivering retrieval speeds comparable to MemoryOS (0.188 vs. 0.151 s) with significantly reduced token costs, although it requires longer processing times for memory addition. 5.6 Cross-Validation by Humans System Avg. QA Score QA Rank Avg. Human Rank MemoryOS Graph Mem A-mem Mem0 1.40 1.10 1.00 0.80 1 2 3 4 1.60 2.20 2.60 3.60 Table 6: Comparison of automated QA scores and human evaluation. The results demonstrate strong alignment between our QA metrics and human judgments, with MemoryOS ranking first in both settings. Table 6 presents the comparative results of average QA scores and human rankings. MemoryOS demonstrates superior performance, achieving the highest QA score (1.40) and the best average human ranking (1.60). Crucially, the ranking order remains consistent across all evaluated systems (MemoryOS > Graph Memory > A-mem > Mem0), where decrease in QA score strictly corresponds to decline in human ranking (e.g., Mem0 scores lowest at 0.80 and ranks last at 3.60). This perfect alignment in system-level ranking further validates that our automated QA Score serves as reliable indicator of model performance consistent with human preference."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce RealMem, benchmark for evaluating long-term memory in realistic, project-oriented interactions, where agents must track evolving goals and maintain coherent project states across sessions. 9 RealMem departs from prior benchmarks by emphasizing sustained, multi-session projects and by providing synthesis pipeline that enables controlled simulation of dynamic memory evolution. Experiments on RealMem show that existing memory systems remain fundamentally limited in handling long-term project dependencies. The substantial gap between all evaluated methods and the Oracle upper bound further highlights that effective long-term memory integration remains core bottleneck for project-oriented agents. We believe RealMem provides valuable diagnostic testbed for studying long-term agent memory and hope it will facilitate future research toward more robust, scalable, and reliable memory systems for real-world autonomous agents."
        },
        {
            "title": "Limitations",
            "content": "Our data construction process relies significantly on the Gemini 2.5 series models for data collection, alongside human annotation for label verification. While this reliance may raise concerns regarding reproducibility and associated costs, the Gemini 2.5 models demonstrate superior capability to simulate realistic humancomputer interactions with highly controllable memory utilization. In contrast, our experiments with the GPT series revealed inferior format adherence, leading us to ultimately select the Gemini models. In terms of evaluation scope, RealMem currently focuses specifically on memory-centric challenges within long-term project-oriented interactions. Consequently, the benchmark does not yet involve the assessment of tool use capabilities, which we plan to incorporate in future work to further extend its applicability in complex task processing."
        },
        {
            "title": "References",
            "content": "Ding Chen, Simin Niu, Kehang Li, Peng Liu, Xiangping Zheng, Bo Tang, Xinchi Li, Feiyu Xiong, and Zhiyu Li. 2025. Halumem: Evaluating hallucinations in memory systems of agents. arXiv preprint arXiv:2511.03506. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413. Enjun Du, Xunkai Li, Tian Jin, Zhihan Zhang, Rong-Hua Li, and Guoren Wang. 2025a. Graphmaster: Automated graph synthesis via LLM agents in data-limited environments. In Advances in Neural Information Processing Systems 39 (NeurIPS 2025). Enjun Du, Siyu Liu, and Yongqi Zhang. 2025b. Graphoracle: foundation model for knowledge graph reasoning. arXiv preprint arXiv:2505.11125. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, and 1 others. 2025a. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407. Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, and 1 others. 2025b. Lightmem: Lightweight and efficient memory-augmented generation. arXiv preprint arXiv:2510.18866. Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Huacan Wang, and Ronghao Chen. 2025. Octopus: Agentic multimodal reasoning with six-capability orchestration. Preprint, arXiv:2511.15351. Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From rag to memory: Nonparametric continual learning for large language models. arXiv preprint arXiv:2502.14802. Sen Hu, Yuxiang Wei, Jiaxin Ran, Zhiyuan Yao, and Lei Zou. 2026. Does memory need graphs? unified framework and empirical analysis for long-term dialog memory. Preprint, arXiv:2601.01280. Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, and 1 others. 2025. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564. Huang, JM Zhang, Luck, Bu, Qing, and Cui. 2024. Agentcoder: Multi-agent code generation with effective testing and self-optimization. University of Hong Kong, Kings College London, University of Sussex, Shanghai Jiao Tong University. 10 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Bowen Jiang, Yuan Yuan, Maohao Shen, Zhuoqun Hao, Zhangchen Xu, Zichen Chen, Ziyi Liu, Anvesh Rao Vijjini, Jiashu He, Hanchao Yu, and 1 others. 2025. Personamem-v2: Towards personalized intelligence via learning implicit user personas and agentic memory. arXiv preprint arXiv:2512.06688. Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. 2025. Memory os of ai agent. Preprint, arXiv:2506.06326. Namyoung Kim, Kai Tzu iunn Ong, Yeonjun Hwang, Minseok Kang, Iiseo Jihn, Gayoung Kim, Minju Kim, and Jinyoung Yeo. 2025. Principles: Synthetic strategy memory for proactive dialogue agents. Preprint, arXiv:2509.17459. Elad Levi and Ilan Kadar. 2025. Intellagent: multi-agent framework for evaluating conversational ai systems. arXiv preprint arXiv:2501.11067. Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2025a. Hello again! llmpowered personalized agent for long-term dialogue. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 52595276. Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li, Qingchen Yu, and 1 others. 2025b. Memos: memory os for ai system. arXiv preprint arXiv:2507.03724. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, and 1 others. 2024. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Xianzhen Luo, Jinyang Huang, Wenzhen Zheng, Qingfu Zhu, Mingzheng Xu, Yiheng Xu, Yuantao Fan, Libo Qin, and Wanxiang Che. 2025. How many code and test cases are enough? evaluating test cases generation from binary-matrix perspective. Preprint, arXiv:2510.08720. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. Clin: continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134. OpenAI. 2024. text-embedding-3-small (model documentation). Siru Ouyang, Jun Yan, Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long Le, Samira Daruki, Xiangru Tang, and 1 others. 2025. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, and 1 others. 2025. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956. Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. 2025. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenges. arXiv preprint arXiv:2505.10468. Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Tian Jin, Xiaowen Dong, Yanfeng Wang, and Siheng Chen. 2025a. Synthesizing post-training data for llms through multi-agent simulation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2330623335. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, and 1 others. 2025b. Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint arXiv:2507.06229. Nguyen Thang, Peter Chin, Thang Nguyen, Yu-Wing Tai, Peter Chin, and Yu-Wing Tai. 2025. Ma-rag: Multi-agent retrieval-augmented generation via collaborative chain-of-thought reasoning. http://arxiv.org/abs/2505.20096, abs/2505.20096. Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang Nguyen. 2025. Multi-agent collaboration mechanisms: survey of llms. arXiv preprint arXiv:2501.06322. Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Yanbin Lu, Xiaojiang Huang, and Yingzhen Yang. 2024. Recmind: Large language model powered agent for recommendation. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 43514364. Yu Wang and Xi Chen. 2025. Mirix: Multi-agent memory system for llm-based agents. Preprint, arXiv:2507.07957. Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed Chi, and 1 others. 2025. Evo-memory: Benchmarking llm agent test-time learning with self-evolving memory. arXiv preprint arXiv:2511.20857. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2024. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813. Haotian Xia, Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2025. Storywriter: multi-agent framework for long story generation. arXiv preprint arXiv:2506.16445. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110. Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, and 1 others. 2025a. Agentic context engineering: Evolving contexts for self-improving language models. arXiv preprint arXiv:2510.04618. Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, and 1 others. 2024. multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. In Proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining, pages 43144325. Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2025b. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):147."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Supplementary Details of RealMem This appendix provides additional statistical information and key definitions of the RealMem dataset to support more detailed understanding of its data composition and task taxonomy. Statistic Dialogue Scale Avg. Context Length (tokens / user) Avg. Session Number (per user) Avg. Turns per Session Total Dialogue Turns Memory Composition Total Memories Project-centric Event Persona Schedule Used Memory (Ground Truth) Question Distribution Total Questions Static Retrieval Dynamic Updating Implicit Preference Temporal Reasoning Value 269,190 205 6.8 14,028 5,072 4,135 850 87 1, 1,415 1,075 156 160 24 Table 7: Statistical overview of the RealMem dataset. A.1.1 Definition of Question Types To systematically evaluate the capabilities of our memory mechanism, we categorize user queries into four distinct types based on their retrieval requirements and interaction logic: Static Retrieval: Queries that seek to advance the project state linearly based on confirmed context. In these scenarios, the user accepts the current progress (e.g., \"The suggestions are great\") and explicitly requests the subsequent step (e.g., \"What do we need to do next?\"). The agent must retrieve the latest stable state to build upon it without altering previous decisions. Dynamic Updating: Queries involving the modification, deletion, or conflict resolution of existing plans. Unlike incremental queries, the user introduces new constraints (e.g., \"maintain 12 days but add West Coast\") that conflict with established memories. The agent must retrieve relevant constraints and perform trade-off analysis to generate revised state. Proactive Alignment: Queries lacking explicit instructions, often manifested as emotional feedback or vague statements (e.g., \"This is fantastic!\"). Here, the agent cannot rely on current turn commands but must retrieve long-term user priorities or historical preferences to proactively propose the next logical action, transforming latent intent into execution. Temporal-reasoning: Queries requiring the processing of time-sensitive information and scheduling logic. These tasks involve validating proposed slots against existing commitments (e.g., checking for overlaps between new workout plan and existing study sessions) or sequencing events chronologically. The agent must retrieve structured schedule data to perform constraint satisfaction checks. 13 Academic Writing Topic ideation, literature synthesis, structural outlining, argumentation refinement, and stylistic editing. Career Development CV/Resume polishing, mock interview simulation, behavioral question strategy, and career narrative refinement. Code Architecture Design System modularization, design pattern application, API interface definition, technical stack selection, and scalability planning. Financial Planning Long-term goal formulation, asset allocation, risk assessment, and financial literacy education. Fitness Personalized workout routines, dietary tracking, biometric monitoring, and progressive plan adaptation. Health Consultation Symptom triage, medical report interpretation, lifestyle health advisory, and chronic condition management. Knowledge Learning Curriculum scaffolding, progress tracking, knowledge reinforcement, and adaptive learning path design. Literary Creation Narrative ideation, world-building coherence, character development, and stylistic enhancement. Mental Health Support Emotional support, cognitive reframing strategies, interpersonal conflict resolution, and stress management. Project Management Task decomposition, resource scheduling, documentation maintenance, and retrospective analysis. Travel Planning Multi-day itinerary scheduling, POI recommendations, culinary integration, and dynamic constraint-based adjustments. Table 8: Overview of the 11 evaluation scenarios and their core task attributes in REALMEM. A.1.2 Definition of Memory Types To support effective retrieval and distinct reasoning tasks, we structure the agents memory into three specific categories: Persona: This category stores static and semi-static user attributes, including personal profiles, long-term goals, and specific preferences (e.g., dietary restrictions, preferred writing styles). It serves as the foundation for personalization, ensuring that the agents responses consistently align with the users identity and historical habits across different sessions. Project States: This category records the evolving state of the specific task or project. It encapsulates the core content generated during the collaboration, such as finalized itinerary details, code architecture decisions, or plot outlines. These memories act as the \"knowledge base\" for the project, allowing the agent to track progress and maintain context continuity without redundant inquiries. Schedule: This category contains structured, time-sensitive data representing the users global timeline. It includes explicit appointments, deadlines, and recurring routines (e.g., \"Gym every Tuesday at 6 PM\"). The primary function of this memory type is to facilitate temporal reasoning and conflict detection, ensuring that proposed project plans do not violate existing temporal constraints. A.1.3 Definition of Topic Types To ensure the dataset reflects the diversity and complexity of real-world human-AI interactions, we curated eleven high-frequency user scenarios. As detailed in Table 8, these scenarios span multiple distinct domains: Life Planning & Management: Scenarios like Travel Planning, Financial Planning, and Project Management that require the agent to handle dynamic scheduling, resource allocation, and hierarchical goal decomposition. 14 Professional & Skill Development: Tasks such as Academic Writing, Literary Creation, Career Development, Knowledge Learning, and Code Architecture Design, focusing on iterative content refinement, technical precision, and long-term learning trajectories. Personal Well-being: Sensitive domains including Fitness, Mental Health Support, and Health Consultation, which demand high personalization, empathy, and strict adherence to context-specific constraints. For each scenario, we explicitly modeled core attributesranging from budget constraints to emotional coping mechanismsto establish clear evaluation boundaries."
        },
        {
            "title": "Topic",
            "content": "A-Mem Mem"
        },
        {
            "title": "Graph Memory",
            "content": "QA"
        },
        {
            "title": "Recall",
            "content": "QA"
        },
        {
            "title": "Recall",
            "content": "QA"
        },
        {
            "title": "Recall",
            "content": "QA"
        },
        {
            "title": "Academic Writing\nCareer Development\nCode Architecture Design\nFinancial Planning\nFitness\nHealth Consultation\nKnowledge Learning\nLiterary Creation\nMental Health Support\nProject Management\nTravel Planning",
            "content": "0.383 0.485 0.322 0.453 0.418 0.556 0.432 0.437 0.465 0.420 0.368 0.504 0.517 0.291 0.532 0.423 0.470 0.438 0.516 0.441 0.506 0.462 0.517 0.515 0.380 0.453 0.482 0.681 0.445 0.498 0.511 0.447 0.385 0.554 0.502 0.446 0.540 0.548 0.569 0.519 0.581 0.526 0.488 0.534 0.550 0.553 0.363 0.522 0.476 0.694 0.484 0.517 0.606 0.533 0.442 0.552 0.593 0.381 0.563 0.510 0.655 0.511 0.633 0.584 0.526 0. 0.450 0.492 0.363 0.497 0.503 0.681 0.482 0.502 0.569 0.453 0.424 0.458 0.467 0.345 0.577 0.489 0.651 0.481 0.532 0.552 0.438 0.483 Table 9: Performance variability across topics. For each method, the best-performing topic is highlighted in bold, and the worst-performing topic is underlined. Comparisons are made column-wise (per method). A.2 Comparison of QA Score and Recall across different topics for four methods. Table 9 reveals significant domain sensitivity across all evaluated architectures, characterized by sharp performance stratification between rigid and entity-centric tasks. Specifically, Code Architecture Design consistently yields the lowest scores across all methods (e.g., MemoryOS QA: 0.363), indicating that current retrieval-based memory systems struggle with engineering tasks demanding strict logical consistency and precise dependency tracking, where fuzzy semantic matching is often insufficient. In contrast, Health Consultation emerges as the most robust scenario (e.g., MemoryOS QA: 0.694), likely because medical dialogues revolve around distinct named entities (e.g., symptoms, medications) that serve as strong semantic anchors for retrieval. This divergence suggests that while current long-term memory paradigms are effective for open-ended, consultative interactions, they remain insufficient for problem-solving scenarios that require high-precision state maintenance. A.3 Case Studies To provide clearer illustration of our defined problem types, we present the following case studies. These examples serve to concretize the abstract definitions, demonstrating how the agent must leverage distinct memory operationsranging from incremental reasoning to conflict resolutionto address specific challenges in long-term interactions. As illustrated in the case study below, the user acknowledges prior context (attractions and food suggestions) and explicitly initiates state transition toward itinerary planning. The system leverages memory to confirm these preferences and proposes logical continuation. This exemplifies Dynamic 15 Incremental (Explicit Progression), where the dialogue builds constructively upon established history to advance the specific task goal. Dynamic Incremental User: Hi! Ive had look at the updated attractions list. The foodie suggestions are great! Im excited to start mapping out the actual day-by-day itinerary now. What do we need to do next? Assistant: Thats fantastic to hear! Im glad you liked the foodie suggestions. Since we have the overall Classic Loop itinerary and now great list of local food experiences, the next logical step is exactly what you said: mapping out the actual day-by-day itinerary, making sure to weave in those delicious foodie stops. Would you like to start by reviewing Day 1 and Day 2, or perhaps focus on specific regions first, like Queenstown or the West Coast, to integrate the food elements? Memory Used: [Food Scheme] [Application]: Acknowledge users positive feedback on foodie suggestions and propose integrating them into the day-by-day itinerary. As illustrated in the case study below, the users input is predominantly emotional and affirmative, lacking explicit directives or queries. To maintain dialogue momentum, the assistant proactively retrieves stored long-term priority (booking flights and RV rental) to propose the next logical step. This exemplifies Proactive Alignment, where the system utilizes memory to bridge the conversational gap and guide the planning flow when immediate user intent is ambiguous. Proactive Alignment User: Wow, this is fantastic! Youve really outdone yourself. The foodie additions are exactly what was hoping for, and youve hit so many great spots. Fergburger is definitely going straight to the top of my list, and local salmon in Tekapo sounds absolutely amazing. Im already imagining all the delicious food! This enhanced guide sounds absolutely perfect, and yes, my taste buds are definitely ready to be tantalized! Assistant: Thats wonderful to hear! Im so glad the foodie additions hit the mark and youre excited about all the delicious experiences awaiting you in New Zealand. Now that we have fantastic baseline for attractions and mouth-watering foodie guide, we can move on to the next crucial step. You previously mentioned that booking flights and the RV rental would be the most important next priority to lock in. Shall we dive into those details now? Memory Used: [Priority: Flights & RV] [Application]: Propose the next step in trip planning, which is booking flights and RV rental, as confirmed by the user previously. As illustrated in the case study below, the user introduces constraint conflict: maintaining fixed 12-day duration while integrating substantial new activities (West Coast). This necessitates structural modification of the existing itinerary rather than simple addition. The system retrieves specific plan details (e.g., Queenstown commitments) to evaluate the impact of resource reallocation. This exemplifies Dynamic Updating, where the agent leverages memory to negotiate trade-offs and alter the plans state to accommodate conflicting requirements. Dynamic Updating User: That sounds amazing, really want to do it! Hmm, extending the trip might be tough with my work schedule, so Id really prefer to stick to 12 days if possible. But also dont want the whole trip to feel super rushed and stressful, you know? Can you show me what it would look like if we tried to fit the two West Coast days into the existing 12 days? want to see how much more rushed it would actually be before decide. Assistant: understand completely! Sticking to 12 days while adding two full West Coast days is challenge, but we can definitely 16 map it out so you can see the trade-offs. To incorporate two full days on the West Coast for your heli-hike and exploration, wed need to allocate three nights in the Franz Josef/Fox Glacier area instead of the original one night. This means we need to free up two days from elsewhere in the itinerary. The most practical way to do this without drastically altering the core... Memory Used: [Plan Detail: Queenstown Day 4] [Application]: Identify the existing commitments in Queenstown that would be impacted by reducing the time allocated there. As illustrated in the case study below, the user proposes recurring workout schedule for specific days. To validate feasibility, the system retrieves existing temporal constraints (Study Sessions) from memory. By cross-referencing the proposed days with occupied time slots to ensure no overlap, the agent performs constraint satisfaction. This exemplifies Temporal Reasoning, where the system processes chronological relationships to maintain schedule consistency. Temporal Reasoning User: Hello, want to arrange workout schedule for Tuesdays, Thursdays, and Saturdays. How about that? Assistant: Yes, that sounds excellent! Ive checked your proposed times against your study sessions, and they fit perfectly without any overlaps. So, to confirm, your fitness schedule will be: Tuesdays: 6:00 PM - 6:45 PM Thursdays: 6:00 PM - 6:45 PM Saturdays: 9:00 AM - 9:45 AM Now that we have the times locked in, the next step is to outline the specific exercises... Memory Used: [Constraints: Study Sessions (Tue/Thu 20:00-21:30, Sat 10:00-12:00)] [Validation]: Check for potential time conflicts with newly proposed workout days. A.4 Prompts This section presents some of the important prompt templates involved in the paper. A.4.1 Evaluation Prompts Figures 5 and 6 illustrate the specific prompt templates used in our evaluation framework for assessing response consistency and memory retrieval quality, respectively. 17 Prompt for Response Consistency Evaluation Instruction: Your task is to evaluate the consistency between the [candidate answer] and the [user-related memory]. Input Data: Users current query User-related memory (representing the latest valid user state) Reference answer (based on the relevant memory) Candidate answer (to be evaluated) Evaluation Rules: Focus only on whether facts, constraints, preferences, and confirmed states are correctly used. Do NOT evaluate language style, tone, politeness, empathy, or fluency. Do NOT give high score just because the answer sounds reasonable. The reference answer is only to help understand how relevant memory should ideally be used; candidate answer does not need to exactly match the reference answer to receive full score. Scoring Criteria: Score 0 (Poor): The candidate answer conflicts with the user-related memory. Score 1 (Fair): The candidate answer does not conflict with the relevant memory but is generic and not based on user memory. Score 2 (Good): The candidate answer uses part of the user-related memory. Score 3 (Very good): The candidate answer (like the reference answer) uses all of the user-related memory. Output Format: { } \"score\": int, \"reason\": str # Briefly explain the reason for the score Figure 5: The specific prompt used for evaluating the consistency between the generated response and user memory. Prompt for Memory Retrieval Quality Evaluation Instruction: Your task is to evaluate the consistency between the [retrieved memory] and the [ground-truth memory], and whether the retrieved memory is helpful. Input Data: <question>: The users current query. <groundtruth_memory>: The true memory that is helpful for answering the question. <retrieved_memory>: The retrieved memory. Evaluation Dimensions: 1. Memory Recall (01): Semantics-aware memory recall calculation. step1: For each groundtruth_memory, check in sequence whether its semantics are contained in any retrieved_memory. step2: Count how many groundtruth_memory items are covered (hits_cnt). step3: Compute the final recall score as hits_cnt / total number of groundtruth_memory items. 2. Memory Helpfulness (02): The helpfulness of the retrieved memory. Score 0: retrieved_memory contains mutually conflicting or contradictory memories, which not only fail to help answer the question but may also cause confusion. Score 1: retrieved_memory is somewhat helpful for answering the question (can provide partial supporting evidence). Score 2: retrieved_memory is very helpful for answering the question (can provide comprehensive supporting evidence). Output Format: Please provide your evaluation results using the following structure: { } \"Mem_recall\": float, \"Mem_helpful_score\": int, \"Mem_hits\": list[str], \"Mem_helpful_reason\": str # List the matched groundtruth_memory items # Explain the reason for the score Figure 6: The specific prompt used for evaluating the quality of memory retrieval, focusing on semantic recall and helpfulness metrics."
        }
    ],
    "affiliations": [
        "Peking University",
        "Renmin University of China",
        "Shanghai Jiao Tong University",
        "Sun Yat-sen University",
        "University of the Chinese Academy of Sciences",
        "Xidian University",
        "Zhejiang University"
    ]
}