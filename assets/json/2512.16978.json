{
    "paper_title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "authors": [
        "Mohammed Irfan Kurpath",
        "Jaseel Muhammad Kaithakkodan",
        "Jinxing Zhou",
        "Sahal Shaji Mullappilly",
        "Mohammad Almansoori",
        "Noor Ahsan",
        "Beknur Kalmakhanbet",
        "Sambal Shikhar",
        "Rishabh Lalla",
        "Jean Lahoud",
        "Mariette Awad",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Rao Muhammad Anwer",
        "Hisham Cholakkal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 7 9 6 1 . 2 1 5 2 : r Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos Mohammed Irfan Kurpath1 Jaseel Muhammad Kaithakkodan1 Jinxing Zhou1 Sahal Shaji Mullappilly1 Mohammad Almansoori1 Noor Ahsan1 Beknur Kalmakhanbet1 Sambal Shikhar1 Rishabh Lalla1 Jean Lahoud1 Mariette Awad2 Fahad Shahbaz Khan1,3 Salman Khan1 Rao Muhammad Anwer1 Hisham Cholakkal1 1Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI) 2American University of Beirut 3Linkoping University"
        },
        {
            "title": "Abstract",
            "content": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, diagnostic benchmark with open-ended, intent-driven questions; singleand multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are humanverified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world longform video understanding. LongShOTBench provides practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot. 1. Introduction Large language models (LLMs) have gradually expanded from text-only reasoning to handling increasingly diverse input modalities [1, 3, 13, 34, 36, 37]. In parallel, benchmarks have been developed to reflect this trajectory: from text-only tasks, to vision-language datasets, and now to omni-modal audio-visual-language evaluations. Within this landscape, video stands out as both natural and challenging. It requires models to jointly process visual, speech, and ambient audio signals, while maintaining temporal coherence across extended sequences. Prior benchmarks on video understanding have typically focused on short, trimmed clips [6, 10, 18, 19, 23, 24, 26]. With the rapid improvement of multimodal large language models (MLLMs), however, such short-form clips are no longer sufficient to reveal their strengths and limitations. As result, recent work has turned to long-form videos, sometimes lasting an hour or more. Yet despite advances in model architectures and training corpora [4, 5, 16], evaluations of long-form video understanding remain fragmented, shallow, and difficult to interpret. Existing benchmarks often trade off between temporal length and modality coverage. Some handle long videos but ignore key modalities such as audio or speech [2, 33, 41], while others address multi-modality but focus on short clips or narrow tasks like video captioning or temporal grounding [8, 10, 11, 21]. Furthermore, most existing benchmarks rely on multiple-choice questions, and even those with openended answers typically reduce evaluation to single score score obtained by using LLM such as GPT as judge. Such simplifed appraoches often hides model failures in perception, cross-modal integration, and reasoning [24, 32, 40], limiting diagnostic insights and opportunities for improvement. To address the aforementioned limitations in existing benchmarks, we present new benchmark, LongShOTBench, along with an agentic framework, LongShOTAgent, for Omni-modal reaSoning and Tool use in Long videos. Our LongShOTBench goes beyond typical visioncentric benchmarks by explicitly incorporating speech, audio, and complex reasoning and tool calling in long-video"
        },
        {
            "title": "Speech",
            "content": "MV-Bench [20] EgoSchema [24] LongVideoBench [35] Moviechat [30] MLVU [43] SVBench [38] LVBench [33] LvBench [41] Video-Holmes [7] InfiniBench [2] Video-MME [8] LongVALE [11] TriSense-2M [21] DailyOmni [44] LongShOTBench (ours) Open-Ended Q&A Multi-Turn Q&A Intent-Driven Q&A ToolUsage CustomRubrics Table 1. Comparison between LongShOTBench and existing video understanding benchmarks. LongShOTBench balances multiple modalities (Visual, Audio, and Speech) and supports intent-driven and tool-augmented Q&A, multi-turn interactions, and rubric-based explainable scoring. Subtitle aided. contexts. Our contributions are fivefold: (i) Holistic Omni-Modal integration. LongShOTBench aligns audio, speech, and vision into temporally consistent representation, capturing cross-modal interactions such as spoken references to off-screen events or sounds that help explain what is happening visually. By contrast, earlier long-video benchmarks (e.g., those primarily focused on visual tasks [24]) often omit raw audio [2, 41], or rely only on transcripts. However, newer multimodal datasets like Daily-Omni [44], LongVALE [11], and TriSense [21] have begun to explicitly integrate raw audio, speech, and vision, though often focusing on shorter clip durations or specific retrieval/captioning tasks. (ii) Intent-driven questioning. LongShOTBench generates intent-driven, scenario-based questions that mirror the diverse ways people watch and engage with videossuch as seeking facts, understanding causes, planning actions, or using tools. These questions appear in diagnostic, multi-turn Q&A sequences designed to evaluate models multimodal reasoning and agentic tool-use capabilities, rather than simply its ability to sustain conversation. The dialogues mimic natural interactions where models must actively gather and apply information through specialized tools, instead of relying solely on memorized knowledge. Unlike prior multi-turn benchmarks (e.g., MovieChat [30]), which emphasize conversational flow or static comprehension, LongShOTBenchs multi-turn structure explicitly probes reasoning depth, intent alignment, and adaptive problem-solving across varied scenarios. [11, 22, 24]. (iii) Diagnostic, interpretable scoring. Instead of relying on single score, LongShOTBench pairs each question with weighted rubric that breaks success into clear criteria, such as factual completeness, temporal localization, modality grounding, and tool use. Independent evaluation of these rubrics provides fine-grained diagnostics, highlighting both strengths and weaknesses, and enables partial credit when models demonstrate partial understanding. (iv) Practical scale with human rigor. LongShOTBench scales to hour-level videos through combination of automated, chunk-based dense caption, Q&A generation, and systematic human validation. This ensures both wide coverage and reliability, and the full pipeline will be released to support reproducibility and community-driven expansion. (v) Agentic method. We introduce LongShOTAgent, fully integrated agentic pipeline designed to reason over long videos using pre-processing, search, and refinement tools. LongShOTAgent is able to seamlessly integrate information across time, leverage multimodal inputs, and execute tool-based strategies, setting new standard for practical, real-world video understanding. By pairing LongShOTBench with LongShOTAgent, we provide both diagnostic benchmark and an actionable pipeline, demonstrating how MLLMs can move beyond static evaluation to tackle complex, temporally extended tasks. We evaluate LongShOTAgent alongside both representative closed-source and open-source omni-modal MLLMs, such as Gemini-2.5-Flash [13] and Qwen2.5-Omni [36]. To minimize evaluator-induced variance particularly from frame sampling choices known to affect long-video QA we adopt native video-only evaluation protocol and defer sampling to each models documented defaults. Empirically, LongShOTAgent surpasses open-source baselines and achieves performance comparable to Gemini on agentic tasks. Nevertheless, all approaches continue to struggle with long-form video reasoning, particularly on hour-long videos, where overall performance remains far below expectations. 2 2. Related Work 2.1. Benchmarks for Long Video Understanding To better probe the capabilities and limitations of current MLLMs, recent studies have shifted towards long-form videos, including hour-level durations. Several benchmarks have been proposed for this purpose. For example, LVBench [33] contains 103 curated high-quality videos with 1,549 manually generated question-answer pairs, targeting six core capabilities such as summarization and temporal grounding. However, LVBench primarily emphasizes visual modalities (i.e., video frames) and excludes audio signals. InfiniBench [2] and LvBench [41] incorporate long-duration video and subtitle contexts, but audio understanding remains underexplored. More recent efforts have sought to establish omni-modal benchmarks for evaluating MLLMs. Videos are typically sampled from large-scale corpora such as VAST27M [5], VALOR [4], and ACAV-100M [16], which provide rich audio-visual annotations. Specifically, LongVALE [11] and TriSense-2M [21] incorporate audio, visual, and speech modalities. However, LongVALE only features videos averaging around 4 minutes, while TriSense-2M extends to approximately 15 minutes. Moreover, although these datasets broaden multimodal coverage, their Q&A tasks remain largely confined to segment captioning and temporal retrieval, limiting evaluation of advanced MLLM reasoning and understanding. Table 1 presents an overall comparison between LongShOTBench and existing benchmarks. In summary, LongShOTBench achieves better balance between video duration and multimodal understanding, and also stands out for its unique question and rubric design. 2.2. Unique Characteristics of LongShOTBench Question Design in Video Q&A. Existing benchmarks adopt human-authored or template-driven queries. EgoSchema [24] emphasizes long-horizon reasoning by designing diagnostic questions validated by human curation to enforce long temporal certificate lengths, but its approach centers on measuring this objective duration capability rather than diverse user intent scenarios. ALLVB [31] rely on predefined task categories or prompt taxonomies, while GAIA [25] and GTA [32] focus on real-world grounding using human-crafted questions and diversified expansion strategies to ensure scenario diversity, their required output is often limited to unambiguous, factoid answers or predetermined tool sequences, which restricts the evaluation of flexible, conversational agent behavior. LongVALE [11] and TriSense [21] focus on the temporal video grounding and segment caption. These designs risk constraining model behavior to narrow expectations. LongShOTBench addresses this limitation with scenario-driven, intent-aware questioning that covers singleand multi-turn interactions. Tool-Augmented Video Understanding. While toolaugmented language models have been studied [27, 29], their extension to multimodal video understanding remains largely unexplored. LongShOTBench enables evaluation of this capability by requiring models to actively gather information via tools in video Q&A tasks. Evaluation Methodologies. Most prior benchmarks rely on coarse-grained metrics, such as accuracy or exact-match scores, offering limited diagnostic insight. MoVQA [40] and GTA [32] propose more fine-grained metrics like cosine similarity or step-level evaluation, but interpretability remains limited. LongShOTBench differs by incorporating weighted, rubric-based scoring for more granular analysis of model performance across multiple criteria. 3. Constructing LongShOTBench LongShOTBench is designed to evaluate the multimodal understanding capabilities of MLLMs in long-form videos, integrating information from vision, audio, and speech. To this end, we build five-stage construction pipeline that includes multimodal caption generation, question design, answer generation, hierarchical rubric generation, and human validation. As illustrated in Fig. 1, the pipeline converts raw multi-source videos into structured evaluation samples. Questions are derived from diverse user-intent scenarios, while the rubric-based evaluation framework ensures interpretable and traceable model scoring. The LongShOTBench dataset contains 157 long-form videos, averaging about 45 minutes in duration. These videos are segmented into 2,083 samples, each paired with singleturn or multi-turn questions, for total of 3,092 questionanswer instances. On average, each video contributes around 13 samples and 20 questions, capturing wide range of narrative complexity and interaction styles. We describe each stage of the pipeline in the following subsections. 3.1. Multimodal Caption Generation Video Collection. We collect long videos from VideoMME [8], sampling total of 92 videos. In addition, we manually curate 65 videos from YouTube to support the design of questions involving more complex agentic tool invocation. Overall, LongShOTBench contains 157 long videos with an average length of 44.7 minutes, totaling 117.46 hours. In comparison, the average video lengths in previous multimodal benchmarks, LongVALE [11] and TriSense-2M [21], are 4 and 15 minutes, respectively. Each video comes with high-quality audio track, which is used for speech and audio analysis. Modality-specific Captioning. To efficiently process long videos, we split each video into segments based on speech activity, treating silent intervals or gaps between speech as natural segment boundaries. Speech content is then transcribed into time-stamped captions using Whisper-large-v3 [28]. For visual content, represen3 Figure 1. Construction pipeline of LongShOTBench. The pipeline begins with raw video data where speech, visuals, and audio cues are extracted. These are passed into multimodal processing to generate segment-wise aligned and fused metadata. Only the distilled information flows to question design, where scenarios and question types are mapped, followed by the generation of questions and conversational answers. Next, verifiable rubrics are created to evaluate correctness and difficulty. Finally, the core dataset, comprising Q&A pairs and tailored evaluation rubrics, is manually reviewed and corrected by human validators, ensuring clean, reliable benchmark. tative frames from each segment are passed to the visionlanguage model Qwen-2.5-VL-32B [3] to generate dense, segment-level scene descriptions. Audio events such as music, applause, and environmental sounds are detected using Audio-Flamingo-3 [12]. This segmentation strategy allows us to capture rich, modality-specific information while preserving temporal granularity, even in segments with little or no speech. Cross-modal Summarization. The captions from all three modalities are integrated using Qwen3-30B-A3B-2507Instruct [37] to produce coherent, high-level narratives. This model reconciles potential conflicts across modalities, abstracts complex interactions (e.g., the presenter gestures toward diagram while explaining concept), and aligns multimodal captions temporally. 3.2. Question Design Scenario Framing. Prior benchmarks prompt models to write questions directly, limiting diversity and control. We instead add Scenario Framing before question creation. scenario is plausible viewing context (e.g., in phone review, buyers ask about battery life while enthusiasts compare cameras), which captures intent and steers questions toward perceptual and reasoning challenges. Using Qwen3-30B-A3B-2507-Instruct [37], we analyze video metadata to extract key entities, actions, and temporo-spatial relations, then generate up to five diverse scenarios per video. The relevant prompt is provided in Appendix. Task Mapping. To balance questions across the scenarios, we add Task Mapping step that maps each question onto structured taxonomy. As shown in Table 2, the taxonomy comprises five major categories: Core Perception, Information, Multimodal, Reasoning, and Agentic tasks. Each category further decomposes into multiple subtasks. In total, our LongShOTBench encompasses 32 distinct tasks, providing broad and systematic coverage across perceptual, cognitive, and reasoning dimensions. Question Generation. To probe genuine multimodal video understanding, we generate questions per scenario using Qwen3-30B-A3B-2507-Instruct. We elicit (i) single-turn, self-contained queries and (ii) multi-turn dialogues with follow-ups. Difficulty is controlled on five-level scale with Levels 12 for recall and recognition, Level 3 for moderate reasoning, and Levels 45 for temporal, causal, and contextual inference, with targeted mix of up to 15% at Levels 12, 2530% at Level 3, 3540% at Level 4, and 2530% at Level 5. All questions are explicitly grounded in the video metadata, referencing observable speech, visual frames, or audio cues."
        },
        {
            "title": "Task Category",
            "content": "# Sub-Tasks Description"
        },
        {
            "title": "Reasoning Tasks",
            "content": "4 4 4 4 Comprehensive perceptual understanding covering entity, event, temporal, and audio comprehensionevaluating the models ability to interpret visual and auditory information over time. Tasks for extracting and organizing information, assessing the models ability to retrieve, summarize, and interpret content from text and video. Tasks integrating and aligning multiple modalities, assessing the models ability to reason and generate insights across visual, auditory, and textual inputs. Higher-order cognitive tasks assessing the models ability to infer, calculate, and reason across causal, quantitative, compositional, and comparative scenarios."
        },
        {
            "title": "Agentic Tasks",
            "content": "16 Tool-augmented reasoning tasks using perceptual, computational, and retrieval tools to assess the models ability to gather information and solve problems actively. Table 2. Task taxonomy of LongShOTBench for comprehensive video understanding evaluation. 3.3. Answer Generation LongShOTBench adopts an open-ended questionanswering framework that better aligns with the flexible conversational capabilities of MLLMs. Given the multimodal captions and generated questions, the Qwen3-30B-A3B-2507-Instruct model is prompted to produce corresponding answers. To ensure high-quality responses, we establish several guiding principles. First, simple questions receive concise replies, while complex ones are addressed with more detailed explanations. Answers never introduce or assume information that is not supported by the video metadata. Second, responses should integrate evidence from multiple modalities when relevantlinking actions, dialogue, and sounds to form coherent explanations. Third, all answers remain grounded and succinct, avoiding over-analysis, technical jargon, or speculation about internal states unless such details are explicitly observable. When question cannot be answered from the available metadata, the system explicitly indicates this rather than providing uncertain or fabricated information. 3.4. Hierarchical Rubrics Generation key distinction of LongShOTBench is its use of unique, verifiable rubrics for each Q&A pair. Criterion-based Scoring. Each rubric consists of multiple criteria, designed via LLM prompting, that target specific facts or events from the video and can be verified by humans or LLM judges. For agentic tasks, the criteria assess query decomposition, tool invocation, parameter extraction, and integration of outputs (see Appendix A.2). The rubrics emphasize factual correctness over phrasing, focusing on entities, actions, numbers, and events. Unlike direct numeric scoring, which is often inconsistent, we query whether each criterion is satisfied, then compute the score automatically, leveraging LLMs strength in text understanding while avoiding instability in numeric judgments [9]. Priority Levels. Criteria are weighted on 10-point scale: high-priority (7-10) facts are essential, medium-priority (4-6) capture supporting details, and low-priority (1-3) add contextual enrichment. Factual errors incur negative weights. Final scores are calculated as the weighted sum of satisfied (+1) or violated (1) criteria. This system allows partial credit, rewarding models that capture the most critical facts while offering bonuses for completeness. Beyond grading accuracy, it provides transparency by linking scores to explicit criteria, enabling interpretable and fine-grained evaluation of reasoning quality. 3.5. Human Validation To ensure the reliability and consistency of LongShOTBench, we conducted rigorous human validation process following automatic filtering. Ten trained annotators with backgrounds in linguistics and computer vision participated in this stage, each experienced in multimodal annotation tasks. Before annotation, they completed structured training program that included tutorials on the task taxonomy and rubric definitions, calibration sessions with sample videos, and group discussions to resolve ambiguities. This preparation ensured shared understanding of multimodal cues and consistent annotation practices across annotators. Revision and Removal. During validation, samples were discarded if the video was corrupted, overly ambiguous, or lacked sufficient multimodal information. Question phrasing was refined to remove ambiguity or unintended bias, while answers were corrected when they omitted key details, contained hallucinated content, or used unnecessarily technical language. Rubrics were also revised to reduce redundancy and clarify evaluation criteria. Through this iterative refinement process, the final dataset achieves strong balance between naturalness, clarity, and factual accuracy. Figure 2. LongShOTAgent Pipeline. The orchestrator agent (Qwen3-4B) receives user query and video input, then calls the Preprocessor to extract multimodal signals, including Whisper-small speech transcription, scene-based frame sampling, SigLIP embeddings, OCR, and audio analysis. These features populate vector database, which the Search tool queries to retrieve top-k relevant segments via semantic similarity. For deeper analysis, the orchestrator invokes Refiner tools such as Whisper-large-v3 for high-quality speech transcription, Audio-Flamingo-3 for detailed audio understanding, and Video Refiner for dense caption generation. Beyond these core modules, LongShOTAgent can access external tools including activity detection, web search, and other APIs to expand reasoning and retrieve additional context when needed. By flexibly sequencing preprocessing, search, refinement, and external tool calls, the orchestrator integrates multimodal evidence and auxiliary knowledge to generate coherent final answer, demonstrating adaptive and agentic coordination across heterogeneous capabilities. 4. LongShOTAgent In addition to the LongShOTBench, we introduce LongShOTAgent, training-free, agentic framework  (Fig. 2)  that orchestrates suite of specialized models and external tools for advanced multimodal understanding. At its core, LongShOTAgent uses Qwen3 LLM [37] as compact orchestrator that dynamically coordinates expert modules including Qwen2.5-VL [3] for vision-language reasoning, Whisper-v3 [28] for refined speech transcription, and AudioFlamingo-3 [12] for audio understanding alongside external tools such as activity detection, web search, and other taskspecific tools. The pipeline is intentionally modular: LLM, ASR, and VLM backends can be replaced with smaller or larger models from different families. The orchestrator composes these specialists on demand routing queries through preprocessing, targeted retrieval, and refinement without any task-specific finetuning. The preprocessing module performs frame sampling via scene detection, background noise and non speech audio analysis, SigLIP-based visual embedding extraction [39], OCR, and lightweight speech transcription [28]. The refinement module then applies more powerful models for nuanced transcription, complex vision-language reasoning, and detailed audio interpretation. During task execution, LongShOTAgent adaptively decides which modules and tools to invoke: broad exploratory questions may trigger external search and cross-modal preprocessing, whereas targeted queries over specific temporal segments can bypass search and go directly to refinement. Sequential chaining is supported (e.g., preprocessing to identify candidate regions followed by focused refinement), and the orchestrator maintains contextual memory of prior outputs to avoid redundancy and ensure coherent multimodal reasoning. By flexibly coordinating these capabilities, LongShOTAgent exhibits agentic behavior dynamically selecting the optimal combination of models and tools for each query rather than following rigid pipeline. Despite being training-free, this design effectively integrates and organizes existing multimodal LLMs, exhibiting superior performance than relying on independent model. LongShOTAgent delivers competitive results on LongShOTBench, which surpasses open source baselines by large margin, highlighting the strength of agentic coordination even without additional training. Together, these findings motivate training-free, modular agent as practical alternative to monolithic end-to-end models: LongShOTAgent 6 attains high performance on long-video, multimodal tasks; de-risks deployment via drop-in upgrades or budget-aware downsizing of individual components; and remains futureproof as stronger specialists and tools emerge. 5. Benchmarking LongShOTBench 5.1. Evaluation Pipeline We evaluate candidate MLLMs on LongShOTBench using two-stage pipeline: Answer Generation. The raw videos and corresponding questions are provided to the model for answer generation, which dynamically processes visual frames, audio, and speech (when supported). Notably, we do not pre-sample fixed number of frames from videos. Instead, we pass each video to models that natively accept video input and let them apply their own default video processing (e.g., frame rate, tokenization). For consistency in text decoding, all open models are served through vLLM [15] using each models deafult generation configuration, unless explicitly overridden, the maximum context length is taken from the models default configuration. We evaluate only models with documented native video APIs to avoid injecting evaluator-chosen frame heuristics. For example, Gemini exposes first-class video API with documented sampling defaults, so we delegate sampling to Gemini. By contrast, APIs that do not accept raw video (e.g., OpenAI and Anthropic models) and require client-side frame extraction would force us to pick frame policy for them but not for others. This native videoonly protocol eliminates evaluator-induced variance while preserving each models intended video ingestion behavior. Answer Evaluation. The models answers are compared against ground truth using the LLM-as-judge [14] framework. We employ Qwen3-14B [37], strong open-source LLM for reproducibility. Specifically, the evaluation is conducted by separately checking each predefined rubric criterion, with the judge LLM constrained to only consider the relevant fact or event. This reduces ambiguity, improves reliability, and ensures verifiable scoring. After checking all criteria, we can obtain an overall score for question-answer pair. Notably, for the multi-turn dialogue evaluation, to avoid error accumulation across multi-turns, we adopt an ideal trajectory setting. At each turn, the candidate model generates response, which is stored for later evaluation. However, the running context for the next turn is updated with the ideal response rather than the models output. This ensures that every turn is assessed independently under the same optimal history, isolating intrinsic response quality from conversational drift. Finally, we report the task-level scores by summarizing evaluation results of all samples according to the question task types. Compute and Resource Budget (Practical Considerations). Evaluating long-form videos in LongShOTBench poses substantial computational challenges. Even with inference acceleration using the vLLM [15] engine, processing extended video sequences and multimodal contexts remains major bottleneck. To ensure feasibility under constrained resource budget, all primary evaluations were conducted on 4 NVIDIA RTX A6000 (48 GB) GPUs. On average, complete evaluation cycle for single model including video multimodal response generation, and LLM-as-judge evaluation and scoring required approximately 7.5 hours, amounting to total of 30 GPU-hours per model. Despite inference parallelization and acceleration, the overall pipeline remains computation-bound due to the long-form nature of the benchmark. Consequently, we restrict the main leaderboard to models with fewer than 10 billion parameters, maintaining practical reproducibility while providing configuration scripts to facilitate larger-model evaluations when sufficient computational resources are available. 5.2. Results and Analysis Following the above evaluation pipeline, we assess the zeroshot performance of several representative closed-source and open-source models, spanning both omni-modal and vision-language paradigms. The models include Gemini 2.5 Flash [13], Qwen2.5-Omni-7B [36], LLaVA-OneVision7B [17], LLaVA-NeXT-Video-7B [42], Qwen2.5-VL-7B [3], InternVL3.5-8B [34], and Qwen3-VL-8B-Instruct [37]. Note that scores reflect each models own video-ingestion defaults under our native video-only protocol, we did not enforce fixed frame count or FPS across models to avoid biasing models toward particular sampling policy. Main Results on LongShOTBench. As shown in Table 3, we report task-wise performance across the four major task categories. For agentic tasks, we primarily compare against Gemini due to its strong multimodal tool-calling capabilities. Although models such as GPT-4o are highly capable, their current APIs can only process pre-extracted frames rather than raw videos with audio, and are therefore excluded from our comparison. Overall, omni-models demonstrate stronger performance than traditional vision-language models (VLMs). Gemini2.5-Flash achieves the highest overall score of 52.95%, substantially outperforming all open-source models and highlighting the advantage of jointly leveraging visual, auditory, and linguistic modalities. However, its parameter count is not publicly disclosed. Among open-source models, Qwen3-VL achieves the best overall performance, likely benefiting from its larger parameter size and enhanced vision capabilities. When comparing models built upon the same or similar base LLM, Qwen2.5Omni consistently outperforms Qwen2.5-VL on audioand speech-related tasks such as Audio Understanding and Sentiment Analysis, underscoring the benefits of incorporating audio signals into multimodal reasoning. The overall perforTable 3. Performance on LongShOTBench (%). Gemini: Gemini-2.5-Flash, LLaVA-OV: LLaVA-OneVision-Qwen2-7B-ov, LLaVA-NV: LLaVA-NeXT-Video-7B-hf, Qwen-VL: Qwen2.5-VL-7B-Instruct, InternVL: InternVL3.5-8B, Qwen-Omni: Qwen2.5-Omni-7B, Qwen3VL: Qwen3-VL-8B-Instruct. Gemini [13] LLaVA-OV [17] LLaVA-NV [42] Qwen-VL [3] InternVL [34] Qwen-Omni [36] Qwen3-VL [37] LongShOTAgent (Ours)"
        },
        {
            "title": "Entity Recognition\nEvent Understanding\nTemporal Understanding\nAudio Understanding",
            "content": "Avg."
        },
        {
            "title": "Causal Reasoning\nQuantitative Reasoning\nCompositional Reasoning\nComparative Analysis",
            "content": "Avg."
        },
        {
            "title": "Information Retrieval\nSummarization\nInstruction Extraction\nSentiment Analysis",
            "content": "Avg."
        },
        {
            "title": "Multimodal Synthesis\nCross Modal Verification\nAudio Visual Alignment\nMotion Analysis",
            "content": "Avg."
        },
        {
            "title": "Agentic Tasks",
            "content": "43.62 41.84 41.23 37.46 41.04 68.41 49.56 57.37 71.24 61.65 61.02 58.86 46.53 52.18 54.65 55.38 50.45 50.89 61.22 54.49 52.95 40.27 8.12 6.66 7.50 6.08 7. 9.01 1.79 11.70 9.73 8.06 9.14 12.92 8.33 5.31 8.92 8.92 4.60 9.33 16.98 9.96 8.51 - 11.16 9.60 10.34 9.53 10.16 14.43 2.92 14.60 13.61 11. 13.42 18.22 9.90 7.86 12.35 11.34 10.91 15.19 47.17 13.82 13.76 - 20.54 14.21 14.11 9.07 14.48 24.76 13.34 19.56 20.72 19.59 18.78 18.61 14.62 13.10 16. 19.14 10.81 21.05 40.57 22.89 18.39 - 19.95 14.47 15.88 12.36 15.66 23.98 14.64 22.14 20.05 20.20 21.41 22.72 15.91 14.03 18.52 18.99 12.31 24.23 54.72 27. 20.49 - 17.03 13.95 14.17 16.20 15.34 23.73 12.92 19.53 17.93 18.53 18.60 19.16 13.04 15.23 16.51 16.59 11.21 22.41 42.45 23.17 18.39 - 27.30 22.19 23.08 26.22 24.70 32.58 20.42 33.13 30.53 29.16 29.58 28.84 23.50 22.59 26.13 27.95 16.82 29.58 71.70 36.51 29.12 - 42.84 35.41 31.35 35.51 36. 54.26 45.16 45.52 52.87 49.45 48.87 60.17 38.47 33.70 45.30 44.15 37.79 44.26 64.15 47.59 44.66 38.25 Duration Gemini LLaVA-OV LLaVA-NV Qwen-VL Qwen3-VL InternVL Qwen-Omni Ours 0-30 mins 30-60 mins >60 mins 55.6 48.6 47.2 19.4 9.5 5.7 19.4 10.0 16.7 34.6 16.1 13.7 25.4 28.2 32. 32.9 21.8 16.5 32.5 15.5 16.4 45.9 41.7 40.5 Table 4. Performance on various video durations. mance of open-source models remains below 30%, underscoring the current limitations of state-of-the-art MLLMs in multimodal understanding and reasoning, and highlighting the challenges posed by our benchmark. By contrast, our LongShOTAgent is more close to the superior Gemini and achieves comparable performance on the agentic task, while significantly surpassing open-source models across all types of tasks. These results demonstrate the effectiveness of our method in retrieving useful multimodal information with relevant tool invocation. Ablation on Video Duration. Table 4 reports the results across different video durations. We divide the benchmark into three ranges: short (0-30 mins), medium (30-60 mins), and long (>60 mins). Overall, Gemini-2.5-Flash consistently achieves the highest performance across all durations, with our proposed LongShOTAgent pipeline ranking second. Moreover, clear trend emerges: most MLLMs perform best on shorter videos, and their performance degrades as the video length increases. For instance, Gemini-2.5-Flash drops notably from 55.6% on short videos to 47.2% on hourlong ones. These findings suggest that existing MLLMs still struggle with long-horizon multimodal understanding and reasoning. This highlights the urgent need for future research to design models with improved long-context handling, efficient memory mechanisms, and scalable multimodal reasoning capabilities. 6. Conclusion We present LongShOTBench, comprehensive diagnostic benchmark for evaluating MLLMs on long-form, multimodal video understanding, integrating vision, speech, and audio across hour-long contexts. Its open-ended, intentdriven questions and rubric-based evaluation provide finegrained, interpretable diagnostics across perception, reasoning, and agentic tool-use tasks. Our experiments reveal substantial gaps in current state-of-the-art models, under8 scoring the persistent challenges of coherent multimodal reasoning over extended temporal contexts. By releasing both LongShOTBench and LongShOTAgent, our agentic pipeline for structured long-video analysis, along with scalable, human-validated annotation framework, we enable reproducible research and provide practical foundation for future model development. Collectively, these contributions aim to guide the next generation of MLLMs toward robust, real-world video understanding and advance progress on complex, multimodal reasoning tasks. 7. Ethics Statement We use publicly available or permissioned long-form videos for research; we exclude sensitive data; and all annotations were performed by adult annotators who agreed to participate. We carefully filtered the videos for sensitive content including violence, explicit material, and personally identifiable information, implementing multiple review stages to ensure compliance with ethical guidelines. Any videos containing potentially harmful or inappropriate content were systematically removed from our dataset before annotation began."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is partially supported by the Meta Regional Research Grant, Project OMER, the Google Gift Research Award, and the NVIDIA Academic Grant."
        },
        {
            "title": "References",
            "content": "[1] Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, and Zhengyu He. Ming-omni: unified multimodal model for perception and generation. arXiv preprint arXiv:2506.09344, 2025. 1 [2] Kirolos Ataallah, Eslam Abdelrahman, Mahmoud Ahmed, Chenhui Gou, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. Infinibench: benchmark for large multi-modal models in long-form movies and tv shows. arXiv preprint arXiv:2406.19875, 2025. 1, 2, 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 4, 6, 7, 8 [4] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu. Valor: Vision-audiolanguage omni-perception pretraining model and dataset. arXiv preprint arXiv:2304.08345, 2023. 1, 3 [5] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitletext omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36:7284272866, 2023. 1, 3 [6] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answerIn European Conference on Computer Vision, pages ing. 179195, 2024. 1 [7] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. [8] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2, 3 [9] Tairan Fu, Raquel Ferrando, Javier Conde, Carlos Arriaga, and Pedro Reviriego. Why do large language models (llms) struggle to count letters? arXiv preprint arXiv:2412.18626, 2024. 5 [10] Tiantian Geng, Teng Wang, Jinming Duan, Runmin Cong, and Feng Zheng. Dense-localizing audio-visual events in untrimmed videos: large-scale benchmark and baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2294222951, 2023. 1 [11] Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audiolanguage-event benchmark towards time-aware omni-modal perception of long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18959 18969, 2025. 1, 2, 3 [12] Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. In Forty-second International Conference on Machine Learning, 2025. 4, 6 [13] Google. Gemini 1.5: Unlocking multimodal underhttps:// gemini/gemstanding across millions of tokens of context. storage.googleapis.com/deepmindmedia/ ini v1 5 report.pdf, 2024. 1, 2, 7, 8 [14] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. 7 [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Pro9 ceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 7 [16] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10274 10284, 2021. 1, [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7, 8 [18] Guangyao li, Yake Wei, Chenliang Xu Yapeng Tian, Ji-Rong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1 [19] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, 2023. 1 [20] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2 [21] Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, Farid Boussaid, Girish Dwivedi, Luqi Gong, and Qiuhong Ke. Watch and listen: Understanding audiovisual-speech moments with multimodal llm. arXiv preprint arXiv:2505.18110, 2025. 1, 2, 3 [22] Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, et al. Unleashing hour-scale video training for long videolanguage understanding. arXiv preprint arXiv:2506.05332, 2025. 2 [23] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [24] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 1, 2, 3 [25] Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general In The Twelfth International Conference on ai assistants. Learning Representations, 2023. 3 [26] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 1 [27] Yujia Qin et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 3 [28] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. 3, [29] Timo Schick et al. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 3 [30] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 2 [31] Xichen Tan, Yuanjing Luo, Yunfan Ye, Fang Liu, and Zhiping Cai. Allvb: All-in-one long video understanding benchmark. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 72117219, 2025. 3 [32] Jize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: benchmark for general tool agents. Advances in Neural Information Processing Systems, 37:7574975790, 2024. 1, 3 [33] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark. ArXiv, abs/2406.08035, 2024. 1, 2, 3 [34] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 7, [35] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interarXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 2 [36] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5omni technical report. arXiv preprint arXiv:2503.20215, 2025. 1, 2, 7, 8 [37] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1, 4, 6, 7, 8 [38] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. SVBench: benchmark with temporal multi-turn dialogues for streaming video understanding. In The Thirteenth International Conference on Learning Representations, 2025. 2 [39] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 6 [40] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: 10 benchmark of versatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817, 2023. 1, 3 [41] Hongjie Zhang, Lu Dong, Yi Liu, Yifei Huang, Yali Wang, Limin Wang, and Yu Qiao. Lvbench: benchmark for longform video understanding with versatile multi-modal question answering. International Journal of Computer Vision, pages 122, 2025. 1, 2, 3 [42] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 7, 8 [43] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2 [44] Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities, 2025. 11 Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "with 891 total tool invocations across 16 distinct tools. Each tool is designed to extract specific information from video content or perform computational operations on extracted data. Table 5 provides comprehensive overview of each tools functionality, parameters, and expected outputs. The tools span multiple categories of functionality: (1) speech and audio processing for transcription and sound detection, (2) visual understanding for text extraction, object detection, and activity recognition, (3) translation services for multi-lingual content, (4) computational tools for calculations and code execution, and (5) information retrieval through web search and memory operations. This diverse tool set enables agents to solve complex real-world video understanding tasks that require coordinated use of multiple modalities and external knowledge sources. A. Details of LongShOTBench A.1. Dataset Statistics The LongShOTBench dataset is composed of 157 longform videos, specifically curated to evaluate the long-context understanding capabilities of Multimodal Large Language Models (MLLMs). The benchmark dataset consists of 3,092 Q&A samples. As illustrated in Fig. 3, the benchmark has distinct focus on videos of substantial length to rigorously test model performance on extended temporal sequences. The video durations are heavily concentrated around central peak, with mean of 44.7 minutes and median of 45.6 minutes. The close proximity of these two metrics indicates relatively symmetric distribution. The histogram clearly shows that the vast majority of samples are clustered between 40 and 60 minutes, confirming that the benchmark is primarily built with near-hour-long videos. This characteristic makes LongShOTBench challenging testbed for evaluating multimodal integration, temporal reasoning, and information retention over significant durations. Fig. 4 visualizes detailed video category distribution and Fig. 5 provides detailed example of Q&A and criterion rubrics. Figure 4. Video category distribution. Each video may belong to multiple categories. B. Additional Details on the Agentic Pipeline Motivation. The proposed agentic pipeline is not intended as baseline for comparison, but rather as demonstration of what can be achieved through principled modularization and structured orchestration. Its purpose is to highlight how systematic composition of open models and tools can achieve performance approaching large proprietary systems without additional training or private data. This section provides detailed overview of its architecture and operational principles, and clarifies its intended scope within the study. System Overview. The LongShOT Agent employs modular, hierarchical architecture in which compact controller Figure 3. Distribution of video durations (minutes) in our validated sample set (n = 157). A.2. Details of the Agentic Tasks This section presents the complete inventory of tools available for agentic video understanding tasks in our benchmark. These tools enable multi-modal reasoning across visual, auditory, and textual modalities, supporting complex queries that require tool composition and sequential reasoning. The benchmark comprises 193 multi-turn conversation scenarios 12 Figure 5. Evaluation Examples of LongShOTBench. The data samples illustrate how we construct scenario context, model users thought process, generate diverse questions (singleand multi-turn), and apply criterion-weighted evaluation rubrics for interpretable scoring. 13 Table 5. Complete list of tools for agentic video understanding tasks. Optional parameters are marked with (?)."
        },
        {
            "title": "Description",
            "content": "transcribe speech timestamp, language(?) {transcript, timestamp} Extract spoken words from video segment with temporal alignment extract scene text timestamp, language(?) {texts[]} Extract visible text overlays, captions, or written content from frames count objects object detection timestamp, object query timestamp, categories[] {object, count}"
        },
        {
            "title": "Count specific objects within video segment",
            "content": "{timestamp, object, bounding box} Detect objects, logos, and brands with spatial localization extract math exp timestamp, language(?) {expressions[]} calculator expression {result}"
        },
        {
            "title": "Detect and extract mathematical expressions from\nvideo frames",
            "content": "Perform mathematical calculations on extracted values web search query, num results(?) {title, snippet, url(?)} Query external web sources for supplementary information detect audio events timestamp {description}"
        },
        {
            "title": "Identify environmental sounds and audio events",
            "content": "execute code code, language, inputs(?) {stdout, stderr}"
        },
        {
            "title": "Execute code snippets in sandboxed environment",
            "content": "detect activities timestamp {activities[]}"
        },
        {
            "title": "Recognize human actions and activities in video",
            "content": "summarize segment timestamp, text, length detect faces timestamp {summary}"
        },
        {
            "title": "Generate concise summaries of video segments",
            "content": "{faces[{name, role(?), confidence}]}"
        },
        {
            "title": "Identify and recognize known individuals",
            "content": "cross modal search query, modalities, {timestamp, results} memory tool func=[adddeletesearch], data {successfail, output} Find query occurrences across modalities (default k=5) Store, retrieve, or delete intermediate results translate speech timestamp, from lang, to lang translate text text, from lang, to lang {translated transcript} Translate spoken content between languages {translated text}"
        },
        {
            "title": "Translate written text between languages",
            "content": "language model (Qwen3-4B) dynamically coordinates collection of specialized modules. Each module corresponds to well-defined sub-capability essential for long-horizon video understanding: VisionLanguage Module: frozen VLM (Qwen2.5VL-7B) performs clip-level semantic grounding, dense captioning, and temporal localization of visual events. Speech and Audio Module: Whisper-large-v3 is used for automatic speech recognition, complemented by AudioFlamingo for non-speech acoustic event recognition. The controller selects the appropriate branch based on the inferred dominant modality of the query. Text and Symbolic Modules: PaddleOCR handles framelevel text extraction, while lightweight code execution sandbox enables symbolic and numerical reasoning. Retrieval and Memory Module: cross-modal retriever built on SigLIP embeddings indexes visual and textual descriptors at one frame per second, maintaining an episodic memory of multimodal features for query-time access. Planner and Fusion Layer: The controller transforms the user query into structured task graph, retrieves relevant temporal spans through similarity search, and issues structured function calls to specialized modules. The returned representations are fused through late-binding weighted attention to produce the final reasoning trace. Execution Dynamics. The agent follows three-stage process. First, during preprocessing and indexing, the video is segmented using shot-boundary detection. Frame-level embeddings, ASR transcripts, and audio descriptors are computed and stored in unified multimodal vector database. Second, during adaptive retrieval, the orchestrator identifies the most relevant temporal spans for given query, forwarding only those segments to specialized high-quality modules. 14 This selective routing ensures that computation is focused where it contributes most to understanding. Finally, in iterative refinement and synthesis, the orchestrator incrementally refines hypotheses by invoking modules conditioned on intermediate outputs. For example, when an object reference is detected, the orchestrator may trigger localized ASR analysis to verify dialog context. The resulting outputs are aggregated and summarized into coherent response through structured reasoning process. Fairness and Scope. The agentic pipeline is not part of the primary model benchmarking. It serves as system-level demonstration of compositional reasoning under the same video-only input protocol and within the same computational constraints as the benchmarked models. Because modules are invoked conditionally and never run concurrently, parameter summation across components is not meaningful fairness metric. The key measure of efficiency lies in selective activation and modular coordination rather than in total parameter count. All components are frozen and publicly available, ensuring reproducibility and transparency. The improvement over single end-to-end models arises solely from efficient orchestration, not from additional data or tuning. Interpretation. The results demonstrate that agentic modularity can effectively compensate for model scale. By decomposing reasoning into targeted, verifiable sub-steps, the system achieves competence close to large proprietary models while remaining interpretable and resource-efficient. This suggests that progress in multimodal understanding can emerge not only from larger architectures but also from advances in structured coordination and agentic design. C. Prompts for Dataset Generation Pipeline We provide the complete prompts used in our five-stage LongShOTBench generation pipeline. Each stage takes structured outputs from the previous stage as input, ensuring consistency and preventing hallucination through careful information flow control. These prompts guide the automated generation of scenarios, questions, answers, and evaluation criteria from video metadata. Each prompt is carefully designed to ensure high-quality, grounded, and realistic benchmark data while avoiding hallucination and maintaining consistency across the pipeline stages. C.1. Scenario Analysis and Task Assignment"
        },
        {
            "title": "Prompt",
            "content": "The following prompt is used in Stage 2 of our pipeline to analyze video metadata and generate realistic viewing scenarios with associated evaluation tasks. This prompt emphasizes strict grounding in the video content to avoid hallucination while generating diverse perspectives for comprehensive evaluation."
        },
        {
            "title": "Scenario Analysis and Task Assignment Prompt",
            "content": "You are analyzing video metadata to identify natural contexts and perspectives from which people would be curious about this video content. Think about realistic situations where someone would want to understand what happened in the video. CRITICAL ANTI-HALLUCINATION REQUIREMENTS ======================================== - NEVER add details, objects, people, or events not explicitly mentioned in the video metadata - NEVER assume content based on typical expectations for video types - NEVER infer activities, locations, or contexts beyond what is directly described - ONLY reference what is explicitly documented in the provided metadata - If metadata is sparse, create fewer but more grounded scenarios - Every scenario element must be verifiable against the source metadata"
        },
        {
            "title": "Your Task",
            "content": "Your job is to: 1. **CAREFULLY** analyze the metadata to understand ONLY what is explicitly described in the video. 2. Generate diverse, realistic scenarios representing different natural human perspectives for viewing this content. 3. For each scenario, identify what kinds of questions person in that context would naturally ask. 4. Map those natural curiosities to evaluation task categories (to ensure comprehensive testing). Requirements 1. **Think like real people**: Consider realistic situations where someone would watch this video and be curious. 2. **Natural multimodal interest**: People naturally connect what they see, hear, and observe - scenarios should reflect this. 3. **Diverse perspectives**: Cover different reasons why people might watch (learning, entertainment, analysis, etc.). 4. **Distinct contexts**: Each scenario should represent meaningfully different viewing situation. 5. **STRICT METADATA GROUNDING**: Every scenario must be completely supportable by the provided metadata. Input: Structured or free-form metadata describing the visual, auditory, and speech content of video. {video_metadata} Output format (strict JSON): { \"scenarios\": [ { \"scenario\": \"<concise scenario description>\", \"tasks\": [\"<task_1>\", \"<task_2>\", \"...\"] } 15 ] } Each scenario must: - Represent realistic context where someone would watch this video - Be 1 2 sentences describing the viewing context and natural curiosity - **STRICTLY GROUNDED**: Use ONLY information explicitly present in the video metadata - **NO HALLUCINATION**: Do not add ANY details, objects, people, locations, or activities not mentioned in metadata - **VERIFIABLE**: Every element must be checkable against the provided metadata - Lead to questions people would naturally ask based on whats actually documented - Map to evaluation task categories that test the required capabilities Task Categories to Evaluate: Core Perception Tasks: entity_recognition (people, objects, brands, animals, landmarks, products, vehicles) ocr_text_extraction (documents, signs, handwriting, screens, multiple languages) event_understanding (action detection, event sequences, event boundaries, state changes) temporal_reasoning (duration, ordering, timestamps, frequency, temporal grounding) spatial_reasoning (object locations, navigation, distance, spatial relationships, 3D understanding) audio_understanding (speech transcription, speaker identification, sound recognition, music analysis) visual_scene_understanding (scene classification, lighting conditions, weather, indoor/outdoor) motion_analysis (tracking, trajectory, speed, direction changes) Reasoning Tasks: causal_reasoning (cause-effect, predictions, consequences) commonsense_reasoning (social norms, physics understanding, everyday logic) mathematical_reasoning (counting, calculations, measurements, statistics) compositional_reasoning (multi-step logic, combining information) comparative_analysis (differences, similarities, changes over time) pattern_recognition (recurring events, anomalies, trends) counterfactual_reasoning (\"what if\" scenarios, hypothetical outcomes, forecasting) Information Tasks: information_retrieval (facts, instructions, contact info, prices, specifications) (1-2 sentences) summarization (key points, highlights, action items, narrative summary) question_answering (factual, inferential, hypothetical) instruction_extraction (procedures, recipes, tutorials, guidelines) (detailed answers) Multimodal Tasks: multimodal_synthesis (combining visual+audio+text information) cross_modal_verification (checking consistency across modalities) audio_visual_alignment (lip sync, sound source localization) multimodal_translation (describing visual in text, audio in text) Specialized Tasks: sentiment_analysis (emotion, mood, tone, stress detection) accessibility_support (scene description, caption generation) privacy_security_reasoning (PII detection, sensitive content) safety_monitoring (hazard detection, emergency situations) ethical_reasoning (detecting sensitivities in context of ethical) METADATA GROUNDING EXAMPLES: **CORRECT - Grounded in metadata:** If metadata mentions \"a person cooking pasta in kitchen with visible recipe book\": - \"Someone following the recipe wants to understand the cooking steps shown and what ingredients are visible\" instruction_extraction, entity_recognition, event_understanding **INCORRECT - Hallucinated content:** \"Someone wants to learn Italian cooking techniques\" (assumes Italian cuisine not mentioned in metadata) \"A chef wants to understand advanced pasta-making skills\" (assumes skill level not mentioned) \"Someone planning dinner party wants to know cooking times\" (assumes dinner party context not present) **CORRECT - Only whats documented:** If metadata states \"two people discussing charts on whiteboard with financial data\": - \"Someone reviewing the meeting wants to understand what financial information was presented and what conclusions were reached\" information_retrieval, summarization ocr_text_extraction, **INCORRECT - Adding assumptions:** \"Business students learning about market analysis\" (assumes educational context) \"Executives making quarterly decisions\" (assumes corporate context and timing) VALIDATION CHECKLIST: - Can every detail in my scenario be found in the metadata? - CORRECT - Did add any context not explicitly mentioned? - WRONG - Would someone reading only my scenario know whats actually in the video? - CORRECT IMPORTANT CONSTRAINTS: - Generate MAXIMUM of 3 scenarios per video - Each scenario must have MAXIMUM of 2 tasks assigned to it - Focus on realistic viewing contexts that lead to natural questions - Ensure comprehensive evaluation coverage through natural curiosity **Scenario Guidelines:** 1. **Different viewing contexts**: Each scenario should represent distinct reason someone would watch 2. **Natural curiosity**: Focus on what people would genuinely want to know 3. **Realistic situations**: Avoid academic or artificial viewing contexts 16 4. **Diverse capabilities**: Ensure different scenarios test different multimodal capabilities 5. **Quality over quantity**: Better to have fewer, more realistic scenarios 6. **METADATA FIDELITY**: Every scenario must be completely derivable from the provided metadata **Task Selection Strategy:** - Choose tasks based on what questions would naturally arise in each viewing context - Multimodal tasks should emerge naturally from the scenario, not be forced - Prioritize tasks that test capabilities people actually need in real situations - **GROUND IN METADATA**: Tasks must test understanding of content explicitly present in metadata **FINAL VALIDATION:** Before finalizing scenarios, ask yourself: 1. Is every detail in my scenario explicitly mentioned in the metadata? 2. Can someone verify each scenario element by checking the metadata? 3. Did avoid adding any assumed context, locations, activities, or details? 4. Would my scenarios still make sense if the metadata were different? **Remember:** These scenarios drive the entire downstream pipeline. Natural, realistic scenarios that are strictly grounded in metadata lead to conversational questions that create more useful and realistic video benchmark without hallucinated content. C.2. Question Type Mapping Prompt This prompt maps evaluation scenarios and tasks to specific question types, ensuring comprehensive coverage of video understanding capabilities across different cognitive demands and reasoning levels."
        },
        {
            "title": "Question Type Generation Prompt",
            "content": "You are an expert in video understanding evaluation. Your task is to analyze evaluation scenarios and map each scenario-task combination to the most appropriate question types for comprehensive testing. Your Task: 1. Review the provided video metadata for context 2. Analyze the given scenario and ALL its assigned tasks 3. For each task, select the most relevant question type IDs from the comprehensive list below 4. Generate MAXIMUM of 3 question types per task 5. Ensure diversity of question types across all tasks in the scenario 6. Return only the question type IDs (e.g., \"direct_fact\", \"causal_explanation\") - no descriptions needed 17 Input: Video Metadata (for context): {video_metadata} Scenario with Tasks: {scenario_with_tasks} COMPREHENSIVE QUESTION TYPE CATALOG Factual Retrieval Questions: - direct_fact: Extract specific information (\"What did the coach tell them to focus on?\") - numerical_extraction: Count or measure (\"How many people entered the room during the meeting?\") - frequency_counting: Count event repetitions (\"How many times did they try the experiment?\") - text_extraction: Read text from video (\"What does the sign on the door say?\") - attribute_identification: Identify properties (\"What equipment were they using for the demonstration?\") Temporal Questions: - timestamp_identification: Locate events in time (\"At what time does the alarm start ringing?\") - duration_calculation: Measure time spans (\"How long does the cooking process take?\") - sequence_ordering: Order events based on occurrence (\"In what order did the three customers arrive?\") - temporal_relationship: Relate events in time (\"What happens immediately after the door closes?\") Spatial Questions: - location_identification: Identify object/person location (\"Where is the red book placed in the room?\") - spatial_relationship: Describe positional relationships (\"What is to the left of the window?\") - movement_tracking: Track motion over time (\"Where does the person go after leaving the kitchen?\") - navigation_description: Describe movement routes (\"What route does the delivery person take?\") Descriptive Questions: - scene_description: Describe the environment (\"What does the room look like at the beginning?\") - action_description: Describe activities (\"What is the person doing with the tools?\") - visual_appearance: Describe physical attributes (\"What is the speaker wearing?\") - audio_description: Describe audio cues (\"What sounds can be heard in the background?\") Reasoning Questions: - causal_explanation: Explain why something occurred (\"Why did the alarm go off?\") - inference_making: Draw logical conclusions (\"What can you infer about the speakers expertise?\") - prediction: Anticipate likely outcomes (\"What will likely happen next based on the setup?\") - intention_analysis: Identify motives/goals (\"What is the person trying to achieve?\") - problem_identification: Detect issues/errors (\"What mistake does the instructor make?\") - counterfactual_reasoning: Hypothetical scenario reasoning (\"If the ingredient were missing, what would happen in this context?\") Comparative Questions: - difference_identification: Identify contrasts (\"How do the two methods demonstrated differ?\") - similarity_identification: Identify commonalities (\"What do all three examples have in common?\") - change_analysis: Track meaningful differences over time (\"How does the speakers tone change throughout?\") Procedural Questions: - step_extraction: List observed or described steps (\"What are the steps to complete the recipe?\") - instruction_clarification: Explain an observed process (\"How does the instructor say to hold the tool?\") - missing_step_identification: Identify missing parts of process (\"What step did the presenter skip?\") Synthesis Questions: - summary_generation: Summarize main content (\"What are the main points of the presentation?\") - key_information_extraction: Extract the essentials (\"What are the three key takeaways mentioned?\") - multi_hop_reasoning: Link multiple pieces of information (\"Based on what the first and third speakers say, what can you conclude?\") - contextual_interpretation: Use broader situational context (\"Given the setting and tone, what is the real message?\") - implicit_information: Extract meaning not directly stated (\"What is implied but not directly stated?\") - holistic_understanding: Provide global interpretation (\"What is the overall purpose of this video?\") - ambiguity_resolution: Resolve unclear scenarios using evidence (\"Who is more likely addressing the audience when two people speak at once?\") MAPPING GUIDELINES: 1. Match question types to task requirements: - Entity recognition tasks direct_fact, attribute_identification, visual_appearance - Temporal reasoning tasks timestamp_identification, duration_calculation, sequence_ordering - Spatial reasoning tasks location_identification, spatial_relationship, movement_tracking - Audio understanding tasks audio_description, cross_modal_information - Reasoning tasks causal_explanation, inference_making, multi_hop_reasoning 2. Selection criteria: - narrative_construction: Construct coherent - Choose exactly 1-3 most relevant question story from events (\"What is the overall story being told?\") - pattern_identification: Identify recurring themes or behaviors (\"What pattern emerges in the customer interactions?\") - long_horizon_integration: Connect events across distant timestamps (\"How does the presenters attitude evolve across the entire session?\") Multimodal Questions: - audio_visual_alignment: Verify audio-video consistency (\"Does what the speaker says match what is shown?\") types per task - Focus on question types that directly test the task capability - Ensure diversity across all tasks in the scenario - avoid repeating question types - Include mix of basic and complex types where appropriate - Ensure multimodal integration where applicable to the task Output format (strict JSON): { \"scenario\": \"scenario description\", \"tasks\": [ - cross_modal_information: Combine modalities to { answer (\"What does the narrator say about the object shown at 1:23?\") - modality_comparison: Compare different modality content (\"How does the written instruction differ from the verbal explanation?\") - complementary_information: Integrate information from multiple modalities (\"Combining visual and audio cues, what is happening?\") Analytical Questions: - error_detection: Identify mistakes or inconsistencies (\"What error occurs during the demonstration?\") - quality_assessment: Evaluate quality or clarity (\"What aspects of the presentation could be improved?\") - consistency_check: Check for logical/visual consistency (\"What inconsistency appears in the explanation?\") - completeness_evaluation: Identify missing but necessary details (\"What important information is missing?\") Complex Understanding Questions: \"task\": \"task_name\", \"question_types\": [ \"direct_fact\", \"causal_explanation\", \"inference_making\" ] } ] } Your goal is to create comprehensive question type mappings that will enable systematic and thorough evaluation of AI video understanding capabilities across all relevant dimensions for each scenario-task combination. C.3. Question Generation Prompt This prompt guides the generation of natural, human-like questions that reflect genuine curiosity about video content. The prompt emphasizes conversational language and realistic question patterns while ensuring proper multimodal integration."
        },
        {
            "title": "Question Generation Prompt",
            "content": "You are generating questions that real person would naturally ask after watching video. Think like someone who watched the video and is genuinely curious about what they observed. CORE PRINCIPLE: HUMAN CURIOSITY Generate questions that reflect natural human interest and curiosity about what happened in the video. These should be the kinds of questions people actually ask when discussing videos theyve watched. INPUTS - Video Metadata: {video_metadata} (what was seen, heard, and said) - Scenario: {scenario} (context for viewing the video) - Expected Question Types: {enriched_question_types} (guidance on question styles) WHAT REAL PEOPLE ASK ABOUT Focus on questions people naturally have when watching videos: **What happened and why?** - \"What was the person trying to do when they...\" - \"Why did they decide to...\" - \"What caused them to change their approach?\" **How things worked or were done** - \"How did they manage to...\" - \"What technique did they use to...\" - \"How did they solve the problem when...\" **Understanding the situation** - \"What was going on when...\" - \"Who was involved in...\" - \"What was the point of...\" **Outcomes and consequences** - \"What happened as result of...\" - \"How did it turn out when they...\" - \"What was the final outcome...\" **Meaning and purpose** - \"What was the main message about...\" - \"What were they trying to demonstrate...\" - \"What lesson was being taught...\" MULTIMODAL INTEGRATION REQUIREMENTS As difficulty increases, questions should naturally require multiple modalities: **Difficulty 1-2**: May focus on single modality (visual OR audio OR speech) **Difficulty 3-5**: Must integrate multiple modalities naturally, such as: - Questions where spoken instructions relate to visual actions - Understanding reactions (visual) to what was said (speech) - How background sounds (audio) affected what people did (visual) - Connecting what someone explained (speech) with what they demonstrated (visual) - How tone of voice (audio) revealed feelings about what was happening (visual) **CRITICAL: Only connect modalities when theres REAL relationship** - Dont force connections between unrelated simultaneous events 19 - Just because audio and visual happen at the same time doesnt mean theyre related - For compilation videos, different segments are usually unrelated - dont connect them AVOID ARTIFICIAL QUESTIONS Do NOT create questions that: - Sound like test questions or academic exercises - Use overly technical language or jargon (\"synchronization\", \"visual cues\", \"audio-visual alignment\") - Focus on minute details that dont matter to the story - Ask about production aspects (camera work, editing, etc.) - Require precise timing or measurements unless naturally relevant - Force unnatural combinations just to be \"multimodal\" - Try to find deep meaning in random coincidences - Connect unrelated events just because they happen simultaneously - Use film analysis language (\"visual metaphor\", \"symbolic representation\") **THE COMMON SENSE TEST**: Would normal person watching this video with friends actually ask this question? If not, dont generate it."
        },
        {
            "title": "NATURAL CONVERSATION PATTERNS\nThink about how people discuss videos in real",
            "content": "conversations: **Simple curiosity (most common)** - Direct questions about what they saw happen - Questions about motivations and reasons - Questions about outcomes and results **Follow-up questions (natural flow)** - Building on previous answers to go deeper - Asking for clarification or more detail - Connecting different parts of the video **Conversational roles in multi-turn:** - **Opener**: Initial curiosity about something interesting - **Deepener**: Wants to understand more about what was just discussed - **Challenger**: Questions or probes an assumption or claim - **Synthesizer**: Tries to put pieces together or see the bigger picture QUESTION GENERATION REQUIREMENTS - Generate **1 single questions** that someone might ask after watching - Generate **1 conversation sets** (2-3 questions each) that flow naturally - Use conversational language, not formal or academic tone - **CRITICAL: Ground questions in what actually happened in the video metadata** - **CRITICAL: Only reference entities, people, products, or events that exist in the video** - **CRITICAL: Dont create questions about fabricated elements not in the metadata** - **CRITICAL: Dont create similar questions in both single-turn questions and multi-turn conversations** - Make sure questions require watching the video to answer DIFFICULTY SCALE (1-5, keep natural) - **1**: Simple \"what happened\" questions about obvious actions (single modality OK) - **2**: Basic \"why\" or \"how\" questions about single events (single modality OK) - **3**: Questions requiring context or relationships (should use 2+ modalities naturally) - **4**: Questions connecting multiple events or requiring reasoning (must use 2+ modalities) - **5**: Questions about deeper meaning, lessons, or complex patterns (must use 2+ modalities) **IMPORTANT**: If the video content doesnt naturally support higher difficulty multimodal questions, its better to generate more difficulty 1-2 questions than to force artificial complexity. TIMESTAMP USAGE - Use actual time references from the video metadata, not segment numbers - Example: \"What happened around the 2-minute mark?\" instead of \"What happened in segment 4?\" - Only reference specific times when the timing is actually important to the question - Most questions should be timeless - about events that happened without needing precise timing OUTPUT FORMAT Valid JSON only: { \"single_turn_questions\": [ { \"question\": \"What was the person trying to accomplish when they started mixing those ingredients?\", \"difficulty\": 2, \"modalities\": [\"visual\", \"speech\"], \"key_segments\": [0, 2, 5] } ], \"multi_turn_questions\": [ [ { \"question\": \"What went wrong with their first attempt?\", \"difficulty\": 2, \"conversation_role\": \"opener\", \"modalities\": [\"visual\"], \"key_segments\": [1, 3] }, { \"question\": \"How did they figure out how to fix it?\", \"difficulty\": 3, \"conversation_role\": \"deepener\", \"modalities\": [\"visual\", \"speech\"], \"key_segments\": [4, 6] } ] ] } GOOD vs BAD EXAMPLES **Examples of BAD artificial questions to NEVER generate:** - \"What specific action does the presenter perform while confirming the heavy thundery downpours?\" ask this Over-specific, nobody would - \"How does the visual detail of balloons relate to the audios layered musical arrangement?\" unrelated elements"
        },
        {
            "title": "Forced connection of",
            "content": "- \"What does the synchronization between ripple effects and musical cues reveal?\" Academic film analysis language - \"Why does the gesture coincide with the rhythmic beat intensifying?\" correlation Random timing - \"How does the static cityscape reflect this resolution?\" background visuals to speech Connecting unrelated **Examples of GOOD natural questions:** - \"What was the weather forecast for the southwest region?\" (speech/visual - actual content people care about) - \"How did the kids react when the balloons fell in the pool?\" (visual - genuine human interest) - \"What did the coach tell them to do differently?\" (speech - practical question) - \"Why did the person look frustrated after trying that?\" (visual + context - real human curiosity) - \"What was the main point they were trying to make?\" (speech/visual - natural question about meaning) FINAL GUIDELINES - Use natural, conversational language that sounds like real people talking - Ask questions people would actually want to know the answer to - Focus on the story and content, not technical or production details - Make questions that need the full video context to answer - Keep genuine human curiosity at the center - **NEVER mention \"segment X\" or technical identifiers in questions** - Use actual timestamps only when timing matters (e.g., \"around 3:15\" not \"segment 7\") - **Always assign difficulty 1-5** based on how much thinking the question requires - **For difficulty 3-5: naturally integrate multiple modalities** - but only when theyre actually related - **Apply the friend test**: Would you ask this question if watching the video with friend? If no, dont generate it - **Dont force complexity**: Better to have good simple questions than bad complex ones - **Avoid coincidence questions**: Dont connect things just because they happen at the same time **ABSOLUTE REQUIREMENTS FOR ACCURACY:** - **Before generating questions, carefully scan the video metadata to identify whats actually there** - **Only ask about products, people, or events mentioned in the speech or visual descriptions** - **Dont assume details not explicitly stated** - stick to whats clearly present - **Be specific but accurate** - reference actual quotes, actions, or elements from the metadata - **Double-check: Does every entity referenced in your questions exist in the metadata?** - **Avoid generic questions that could apply to any video** - ground them in specific content C.4. Answer Generation Prompt This prompt ensures that answers are conversational, accurate, and properly grounded in video content. It emphasizes 20 natural language while maintaining strict adherence to what is explicitly shown or stated in the video. - **Lengthy explanations** when simple answer would do"
        },
        {
            "title": "Answer Generation Prompt",
            "content": "You are answering questions about video in natural, conversational way. Think of yourself as someone who watched the video and is now explaining what happened to friend who asked. CORE APPROACH: HELPFUL FRIEND Answer like youre having conversation with someone whos genuinely curious about the video. Be accurate, but sound natural and human. KEY PRINCIPLES: 1. **Be factually accurate** Only say what you can actually see or hear in the video metadata. NEVER invent or assume details not explicitly present. 2. **Sound conversational** Use natural language that flows like normal speech 3. **Be helpful** Fully answer what they asked, dont leave them hanging 4. **Stay grounded** Base everything on whats actually in the video metadata. If something isnt mentioned, DONT make it up. 5. **Be appropriately detailed**"
        },
        {
            "title": "Simple",
            "content": "questions get simple answers, complex questions get more detail 6. **Verify before stating** If unsure about detail, dont include it. Better to be incomplete than wrong. Inputs - Video Metadata: {video_metadata} - Scenario: {scenario} - Task Context: {task_context} - Question Context: {question_context} - Conversation: {conversation} HOW TO ANSWER NATURALLY: **What to include:** - What you actually see happening in the video (only whats explicitly described) - What people say or sounds you hear (only whats in the speech transcript) - Natural inferences that any viewer would make (like \"they looked frustrated\" if someones face shows it) - Connections between whats said and whats shown when relevant - **LIMIT TO WHATS EXPLICITLY STATED** - dont extrapolate beyond the metadata **What to avoid:** - Technical jargon or academic language - Overly formal or robotic phrasing - Mentioning \"segments\" or technical video terms - **CRITICAL: Speculating about things not shown in the video - this causes hallucinations** - **CRITICAL: Adding details not in the metadata - stick to whats actually there** - **CRITICAL: Assuming what people think or feel unless explicitly described** - Being unnecessarily precise with timing unless it matters - **Repetitive sentence starters** - dont start every answer the same way - **Over-analysis and commentary** - stick to what happened, not extensive interpretation 21 - **Making up quotes or dialogue not present in the speech transcript** - **Describing technology features not mentioned in the video** **Natural language examples:** - Instead of: \"The subject exhibits forward weight displacement\" - Say: \"They lean forward\" - Instead of: \"Auditory and visual modalities align to indicate...\" - Say: \"What theyre saying matches what theyre doing\" OUTPUT FORMAT Return answers in JSON only, no extra text: [ \"question\": \"<exact question text>\", \"answer\": \"<natural, conversational answer based on video content>\" { } ] **CRITICAL: QUESTION-ANSWER MATCHING** - **Read each question carefully** - understand what is actually being asked - **Answer ONLY what is asked** - dont provide answers to different questions - **If you cant find information to answer the specific question, say so** - dont substitute with unrelated information - **Each answer must directly address its paired question** - verify the connection before responding GOOD ANSWER EXAMPLES: **Natural conversational answers:** { \"question\": \"What was the coach trying to teach them?\", \"answer\": \"The coach was showing them how to position themselves better when defending. He kept telling them to stay low and watch the opponents hips instead of the ball, because thats what tells you which way theyre really going to move.\" } { } \"question\": \"How did the kids react when the experiment didnt work?\", \"answer\": \"You could see they were pretty disappointed - couple of them had their shoulders slumped and one kid actually said aw man, that stinks. But then the teacher encouraged them to try again with different approach.\" **BAD - overly technical:** \"The subjects exhibited postural indicators of negative affect following experimental failure, with observable biomechanical changes in shoulder elevation consistent with disappointment.\" **GOOD - natural but accurate:** \"They looked bummed out when it didnt work - you could see it in their body language.\" FINAL REMINDERS: - Sound like helpful person, not textbook - Be accurate but dont be robotic - Answer what they actually want to know directly - Use timing details only when they help explain what happened - **Vary your sentence starters** - dont always begin with the same phrases - **Focus on facts, not analysis** - tell them what happened rather than interpreting why - **Keep it concise** - give them what they need without unnecessary commentary - Connect speech and actions naturally when they go together **ABSOLUTE REQUIREMENTS FOR FACTUAL ACCURACY:** - **ONLY describe whats explicitly in the video - **Dont elaborate or add details not directly metadata** stated** - **Dont describe specific gestures, movements, or expressions unless explicitly mentioned** - **Dont create detailed choreography or specific interactions not in the metadata** - **If you cant find specific details in the metadata, say so rather than guessing** - **Cross-check: Does everything in your answer have direct source in the metadata?** - **When in doubt, be less specific rather than more specific** C.5. Evaluation Criteria Generation Prompt This prompt generates weighted evaluation rubrics for each question-answer pair, focusing on essential factual content while allowing flexibility in expression style. The criteria emphasize content accuracy over linguistic formality."
        },
        {
            "title": "Criteria Generation Prompt",
            "content": "You are creating evaluation criteria to assess how well AI models answer questions about videos. The reference answers are conversational, so your criteria should focus on whether the model captured the key factual content, regardless of whether they use formal or conversational language. Task: Given Q&A pair, generate criteria that identify the essential factual elements any correct answer must include, while allowing flexibility in how those facts are expressed. Evaluation Framework: - **High-Priority** (Weight: 5): Essential facts - if missing, the answer fails to help the questioner - **Medium-Priority** (Weight: 3): Important details that add value and show understanding - **Low-Priority** (Weight: 1): Additional context that enriches the answer - **Penalties** (Negative Weights): Factual errors or misleading information that would confuse the questioner FOCUS ON CONTENT, NOT STYLE: - Accept both \"They looked frustrated\" and \"The subjects exhibited negative affect\" - Accept both \"around 2:30\" and \"at the 2-minute 30-second mark\" - Value completeness of information over formal language - Dont penalize conversational tone or natural expressions Question Type Adaptation: - **Factual Questions**: Did they get the key facts right? Did they miss important information? - **Reasoning Questions**: Is their logic sound? Did they explain why something happened? - **Procedural Questions**: Did they cover the main steps? Are they in the right order? - **\"What happened\" Questions**: Did they capture the main events and their significance? Criteria Requirements: - **One focus per criterion**: Each criterion should check one specific fact or element - **Be specific**: Include actual entities, actions, or events from the video - **Be verifiable**: Someone should be able to clearly judge pass/fail from the video content - **Be essential**: Focus on information that genuinely matters for answering the question - **Allow natural expression**: Dont require specific wording, just the factual content Input: {Question_answer_pair} Output JSON: [ \"name\": \"factual_correctness\", \"description\": \"Must mention that the coach told players to watch the opponents hips\", \"category\": \"high_priority\", \"is_penalty\": false \"name\": \"hallucination\", \"description\": \"Must not include information not present or supported in the video\", \"category\": \"penalty\", \"is_penalty\": true { }, { } ] Guidelines: - **Maximum 5 criteria total** - focus on what really matters - **Use different category names** - never repeat the same name (e.g., dont have multiple \"factual_correctness\") - **Distribute facts across categories** - assign different types of facts to different category names - **Be extremely specific** - include exact facts, names, numbers, actions from the video - **Focus on essential facts only** - ignore commentary, analysis, or interpretive observations - **Make it judge-friendly** - an LLM should be able to scan the answer and check if the specific fact is there - **Be literal** - the judge needs to know exactly what words/facts to look for **EXAMPLE: Good criteria with proper name distribution:** json [ { 22 - **Focus on facts, not commentary** - ignore analytical observations in the reference answer - **Essential facts only** - what does the questioner actually need to know? - **Judge-friendly descriptions** - an LLM should easily verify if the fact is present - **Content over style** - accept any phrasing as long as the fact is there \"name\": \"factual_correctness\", \"description\": \"Must mention that the AI-powered dog washer is real machine shown working\", \"category\": \"high_priority\", \"is_penalty\": false \"name\": \"key_details\", \"description\": \"Must mention that it has sensors and cameras that adjust water pressure\", \"category\": \"high_priority\", \"is_penalty\": false \"name\": \"completeness\", \"description\": \"Must mention that it was demonstrated on real dog\", \"category\": \"medium_priority\", \"is_penalty\": false \"name\": \"hallucination\", \"description\": \"Must not include information not present or supported in the video\", \"category\": \"penalty\", \"is_penalty\": true }, { }, { }, { } ] **Bad criteria examples:** - Multiple \"factual_correctness\" entries (violates no-repeat rule) - \"Must demonstrate understanding of the absurdity\" (captures commentary, not facts) - \"Must explain why the speaker found it funny\" (too vague, interpretive) **CATEGORY NAME DISTRIBUTION STRATEGY:** Use different names for different types of facts - NEVER repeat the same name: - **\"factual_correctness\"** - for the most important core fact (use only once) - **\"key_details\"** - for important specific details (use only once) - **\"completeness\"** - for coverage of main elements (use only once) - **\"accuracy\"** - for precision of numbers, names, or specifics (use only once) - **\"essential_information\"** - for critical context needed to answer (use only once) **Penalty criteria names:** - \"hallucination\", \"contradiction\", \"temporal_error\", \"entity_error\", \"factual_error\" **Standard Penalty Descriptions (use these exact descriptions):** - \"hallucination\": \"Must not include information not present or supported in the video\" - \"contradiction\": \"Must not contain self-contradictory statements\" - \"temporal_error\": \"Must not provide incorrect timing or sequence of events\" - \"entity_error\": \"Must not misidentify people, objects, or locations\" - \"factual_error\": \"Must not state facts that contradict what actually happened in the video\" **FINAL REMINDERS:** - **Never use the same category name twice** - each criterion must have unique name"
        }
    ],
    "affiliations": [
        "American University of Beirut",
        "Linkoping University",
        "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)"
    ]
}