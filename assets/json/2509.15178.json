{
    "paper_title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding",
    "authors": [
        "Zaiquan Yang",
        "Yuhao Liu",
        "Gerhard Hancke",
        "Rynson W. H. Lau"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks. The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 7 1 5 1 . 9 0 5 2 : r Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W.H. Lau Department of Computer Science City University of Hong Kong {zaiquyang2-c, yuhliu9-c}@my.cityu.edu.hk {gp.hancke, Rynson.Lau}@cityu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporalaugmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each subquery. These prompts highlight attribute and action cues, respectively, directing the models attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks. The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
        },
        {
            "title": "Introduction",
            "content": "Spatio-Temporal Video Grounding (STVG) aims to localize target object in video both spatially and temporally, given an input text query. This task is fundamental to many different applications (e.g., video surveillance and autonomous driving [53] ). However, it is also very challenging as it requires the model to be able to distinguish the target from distractors over time and identify the precise temporal boundary of the action. While existing methods handle the STVG task mainly in fully-supervised setting [11; 47; 44], which often relies on costly frame-level annotations, several works [24; 3; 48; 19; 20] attempt to introduce the weakly-supervised or zero-shot setting to alleviate the burden of dense annotations. For example, E3M [3] integrates CLIP [34] and an expectation maximization strategy to optimize spatio-temporal localization in zero-shot manner. However, CLIP is known to be weak in localization [61; 12] as it simply aligns the global representation of image-text pairs. Joint Corresponding authors. Preprint. Under review. Figure 1: (a) The visual attention maps shows that some special tokens (marked as ) can precisely attend to the target region of the input query. However, these special tokens, referred to as the grounding tokens in our work, underperform on complex STVG, where they often focus on part cues and ignore other cues (marked in red within the input text prompts). Examples (b) and (c) illustrate spatial/temporal grounding errors caused by ignoring the discriminative attribute/action cues. Red and green bounding boxes denote the ground truths and predictions, respectively. Considering the strong capability of multi-modal large language models (MLLMs) [29; 22; 25; 7; 43; 28] in cross-modality alignment, several works explore the application of MLLMs in the visual grounding task. However, they typically require explicit fine-tuning [21; 35; 31] of MLLMs with additional grounding datasets and specific model modifications, which can be difficult to scale and generalize to novel visual data [4]. Although some recent works [4; 55] have investigated the attention maps in language models as done in previous ViT and CNN architectures [60; 42; 10], they only focus on the generated tokens while neglecting other token components input in the language model (e.g., system tokens and special tokens). In particular, we observe that the special tokens play an important role in structuring the communication between the user input and the language model, and help guide the generation of coherent responses in dialogues. With the above observation in mind, we delve deeper and find that the special tokens following the input instruction have outstanding grounding ability. In particular, few special tokens are characterized by high visual activation and can attend to the region of interest well. Considering the left sample of Fig. 1(a) as an example, the special token _A provides tangible attention to the boy referred to by the given query, while in the right sample of Fig. 1(a), the token IST is assigned to ground the elephant. These special tokens are referred to as grounding tokens in our work. Despite the strong comprehension ability, the grounding tokens cannot optimally adapt to STVG as they tend to ignore some important cues (e.g., attribute or action) in complex video query. As shown in Fig. 1(b), the grounding token fails in the spatial grounding by ignoring the attributes. Similarly, in Fig. 1(c), it leads to the failure of temporal grounding by neglecting the action cues of the target. In this work, we first conduct systematic analysis to probe the special tokens of various MLLMs (Sec. 3.2), which demonstrates our aforementioned empirical findings and enables zero-shot spatiotemporal video grounding. To alleviate the problem of neglecting discriminative cues, we propose novel decomposed spatio-temporal highlighting (DSTH) strategy (Sec. 3.3). Specifically, the language query is decomposed into attribute and action sub-queries, which are utilized as the text input of the MLLM for inquiring the targets existence spatially and temporally, respectively. We then propose novel logit-guided re-attention (LRA) module to highlight the cues in attribute and action sub-queries. For each sub-query, LRA optimizes the learnable latent variable as the (spatial/temporal) prompts via enhancing the positive response generation while suppressing the negative response. As result, the DSTH strategy can well adapt the model to mine faithful visual context and concentrate on spatially/temporally relevant regions. In addition, to further enhance the temporal consistency during spatial grounding by the attribute sub-query, we develop temporal-augmented assembling (TAS) strategy (Sec. 3.4). The TAS strategy utilizes temporally perturbed frames as input to assemble different predictions into final spatial grounding result. In summary, our contributions are as follows: 2 We reveal that MLLMs dynamically assign the special tokens for precisely grounding text-related regions. We identify the special tokens characterized by the salient visual activation for deriving novel zero-shot STVG framework. We propose novel test-time tuning strategy named decomposed spatio-temporal highlighting (DSTH). It introduces an innovative logit-guided re-attention (LRA) module to adapt the grounding token for thorough spatio-temporal localization. We also develop temporal-augmented assembling (TAS) strategy to further improve the robustness of spatial localization. We conduct extensive experiments on various MLLMs to demonstrate our empirical findings and validate the effectiveness of the proposed method. Our method outperforms existing methods by remarkable margin on three STVG benchmarks."
        },
        {
            "title": "2 Related work",
            "content": "Spatio-Temporal Video Grounding (STVG) aims to localize spatio-temporal tube in video corresponding to the text query. Unlike image-based visual grounding [8; 9; 50; 27; 49], STVG [59; 41; 47] presents significant challenges, requiring models to distinguish targets from distractors both spatially and temporally. Fully-supervised STVG approaches [41; 44; 59; 39] have achieved promising results. However, these methods heavily depend on an extensive collection of laborintensive annotations. Several recent works [24; 19; 20] tackle STVG in more efficient manner, which only uses coarse video-level descriptions for training. E3M [3] leverages pre-trained visionlanguage models and proposes an expectation maximization framework to optimize spatio-temporal localization. However, contrastive objective based multimodal models are known to be weak in localization [61; 12] as they simply align the global representations of image-text pairs. Multimodal Large Language Models (MLLMs) are capable of handling diverse language and vision tasks. To equip MLLMs with the grounding ability, current works [32; 21; 23; 52; 57] construct grounding-oriented supervision data for instruction tuning and propose novel architectural modifications. LLaVA-ST [23] achieves spatio-temporal understanding by introducing progressive training strategy consisting of three sequential stages. However, the large amount of grounding data needed for instruction-tuning imposes high labeling costs. In addition, changing the focus of MLLMs to grounding tasks can degrade the original dialog capabilities due to catastrophic forgetting [54]. Recent works [4; 17; 55] reveal the inherent perception ability of MLLMs obtained by general instruction-tuning. Unlike these works that only focus on the generated tokens or in our work, we delve deeper into the more token components and reveal that the MLLMs always dynamically assign the special tokens following the instruction prompt for attending to the regions of interest. Test-time Tuning (TTT) aims to optimize the inference of test samples online. With the progress of multimodal foundation models [34; 5]), TTT has attracted more attention [62; 16; 38; 51] as it can learn effective prompts for test samples and well adapt the foundation model for zero-shot applications. TPT [38] pioneers the study on TTT by minimizing the prediction entropy between each test sample and its augmented views. HisTPT [56] explores memory learning for test-time prompt tuning by introducing the memory bank. However, the test prompt optimization for MLLMs is barely explored. The most relevant work to our work is ControlMLLM [45], which optimizes the attention map by taking the referring regions as supervision. With the lack of supervision, we propose to learn visual prompts by regularizing the token-level response to instruction input. Our work shows that we can rectify where text prompts attend to by altering the outputs of MLLMs."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Task Formulation. Given an untrimmed video = {ft}Tv t=1 composed of Tv image frames and sentence query Q, the goal of the STVG task is to localize the spatio-temporal tube of target = {bt}te t=ts described by Q. Here bt represents the bounding box of the target in the t-th frame, ts and te specify the starting and ending boundaries of the target tubelet, respectively. The STVG task can be solved through two sub-tasks: spatial grounding and temporal grounding. In our work, we first derive zero-shot solution for the STVG task by unleashing the strong cross-modal comprehension ability of MLLMs. 3 The Setup of MLLMs. Current MLLMs typically consist of visual encoder, projector, and large language model. Specifically, given an image-question pair (I, Q) as input, the image is first projected into text-aligned visual tokens Tv = {v1, . . . , vM} by visual encoder and projector, and the question is converted into text tokens Tq = {t1, . . . , tNq } by text tokenizer and embedding layer. Here, and Nq denote the numbers of visual tokens and question tokens, respectively. In practice, MLLMs also introduce system tokens Tsys and some special tokens Trole for instructionfollowing ability. In particular, the special tokens play an important role in structuring the wellorganized conversation framework. In our work, the special tokens are positioned subsequent to the instruction prompts by the user. They help guide the generation of coherent responses. The number of special tokens Nrole often differs among different MLLMs. As result, the language model receives concatenated tokens, = {t1, . . . , tNsys ; v1, . . . , vM; t1, . . . , tNq ; t1, . . . , tNrole }, as input. Appendix ?? provides visual illustration about the input tokens in MLLMs. Text-to-visual Attention. The LLM in MLLMs typically processes the input tokens through transformer blocks [29] with the multi-head attention (MHA) for interactions of different tokens. Particularly, the text-to-visual attention represents the relationships between the visual and the textual tokens. We can derive the text-to-visual attention matrix RLHNN, where = Nsys + + Nq + Nrole. and denote the numbers of layers and heads in the transformer. Our empirical observation from Fig. 1 is that the special tokens Trole show outstanding grounding ability with global comprehension. For the simplicity of the following analysis, we omit the affect of different layers and heads by the mean operation. We can then obtain the text-to-visual attention matrix, Arole = [Nsys + + Nq : N, Nsys : Nsys + M]. 3.2 Grounding Token Identification In this section, we conduct pilot study to quantitatively analyze the grounding ability of the special tokens. Without loss of generality, we randomly select subset of 1,000 imagetext pairs from the RefCOCOg [15] validation set for image MLLMs analysis, and subset of 1,000 videotext pairs from the HC-STVGv2 [41] dataset for video MLLMs analysis. Particularly, we choose three typical MLLMs (i.e., LLaVA-1.5, the Qwen-VL, Deepseek-VL) study on image input and three MLLMs (i.e., LLaVA-Next-Video, Qwen2-VL, LLaVA-OneVision) for the study on video input. We have two key findings from our studies. for Figure 2: In (a), the results show the frequency with which different special tokens (such as _A, .) have superior grounding ability than other tokens, i.e., hit_ratio. In (b), the results represent the grounding accuracy of tokens ranked by their visual activation degrees. For each MLLM, we select four tokens for visualization. MLLMs dynamically assign the special tokens to attend to the text-related regions. To demonstrate the finding, we first define the attention ratio of each special token as the ratio of maximum attention within the ground-truth bounding box bgt to that outside it. For example, the attention ratio of the special token _A can be computed as: max(A_A R_A A_A att = max (cid:0)A_A role R1M, role fB2M (bgt)) role (1 fB2M (bgt))(cid:1) , where fB2M denotes the function that transforms bounding box into its corresponding binary mask. Here, higher ratio indicates better target grounding ability. Given test sample, we can identify the token yielding the highest attention ratio as the superior token for grounding. The hit ratio of token is then defined as the frequency of being the superior token for grounding across all test samples. Fig. 2(a) shows the hit ratio of four special tokens in each MLLM. Notably, the fixed token does not consistently exhibit the best grounding ability for different samples. For example, the highest hit ratio achieved by token . in LLaVA-1.5 is not more than 50%. In addition, for different MLLMs, the token at fixed position does not always yield the best localization performance. For example, the (1) 4 Figure 3: Overview of the proposed approach for zero-shot STVG. Given video-text pair, we first decompose the text into spatially and temporally related sub-queries, Qs and Qt. The text prompt tokens converted from Qs and Qt are then concatenated with visual tokens Tv for spatial and temporal inferences, respectively. In addition, we introduce learnable variables as visual prompts and optimize them by the logit-guided re-attention (LRA) module. For spatial grounding, we also develop temporal-augmented assembling (TAS) strategy by reversing the frames to enhance temporal consistency. After optimization, we obtain the object track score Sobj and frame score Sframe based on the grounding token identification. The final prediction is derived by joining Sobj and Sframe. last special token :, which is adopted in previous work [55], obtains high hit ratio in Qwen-VL, but its hit ratio is quite low in LLaVA-1.5. This observation holds for both image and video inputs. The special token with higher visual activation tends to show superior grounding performance. Since the superior grounding tokens vary across different samples and MLLMs, identifying them for grounding is problem that needs to be resolved when ground truth is unavailable as prior. With further analysis, we reveal that the superior token for grounding tends to show higher visual activation. For each sample, we rank the special tokens according to the maximum value of visual attention and then evaluate their grounding accuracy by selecting the proposal with the highest attention value. Following the paradigm in previous works [40; 14], we extract box proposals using detector and evaluate the grounding accuracy by the Acc@0.5 metric. Fig. 2(b) shows the results. We can see that the grounding accuracy decreases as the rank of visual activation reduces (from left to right). This supports our hypothesis that the special token with higher visual activation tends to show superior grounding ability. We have also observed that models with better comprehension possess better grounding ability overall. For example, the special tokens in LLaVA-OneVision achieve better grounding performance than those of the other MLLMs. straightforward solution for zero-shot STVG. Inspired by the grounding ability of special tokens subsequent to the text prompt, we refer to them as grounding tokens in our work. By identifying the special tokens characterized by high visual activation, we derive strong training-free framework for STVG. Specifically, given the video-text pair (V, Q), we first extract the object track proposals Opro = {O1, . . . , OP } from the video , as done in previous works [3; 20], where denotes the number of proposals, and Op = {b is the set of bounding boxes of the p-th proposal in Tv frames. Based on the foregoing findings, we then select the token with the highest attention value for locating the target object, and denote the text-to-image attention matrix of the selected token as Ag R1M . As result, we obtain each object track score by computing the maximum attention value inside each object track as: 1, so = max(Ag fB2M (Op)), }, with so Sobj = {so 2, , so t}t=Tv t=1 (2) where denotes element-wise multiplication. We choose the track with the highest score as spatial prediction pred. In similar manner, we can compute the frame score Sframe = {st }. By selecting the top-K frames with the highest scores from the temporal prediction Sframe, we obtain t}te the final spatio-temporal prediction t=ts , where ts and te denote the starting and ending boundary predictions. Though simple, this solution has achieved comparable or even superior performance than current zero-shot methods. For example, based on LLaVA-OneVision model, this solution achieves 23.3% on the m_vIoU metric on HC-STVGv1 dataset, which outperforms previous SOTA result (19.1%) by E3M [3]. 2, , st Tv pred = {b 1, st 5 3.3 Decomposed Spatio-Temporal Highlighting Despite strong performance by the above solution, these grounding tokens often neglect some important cues during spatio-temporal localization especially when processing complex video queries. As shown in Fig. 1(b), the attribute cue in red clothes is overlooked during inference, which causes the incorrect spatial localization. We observe that the language query often contains attribute and action descriptions of the target object, which are beneficial for spatial and temporal localization, respectively. To this end, we propose novel decomposed spatio-temporal highlighting (DSTH) strategy in our framework, which aims at highlighting the attributes/ actions cues in language query and enhances the spatial/temporal reasoning, respectively. Generation of target-related cues. For comprehensive spatial and temporal reasoning, it is essential to extract the attribute and action descriptions from the original query as textual cues of the target. Here, we leverage the strong in-context capability of the LLM [1] to extract attribute and action descriptions. Following previous works [49; 14], we construct prompt with general instructions and in-context task examples. As shown in Fig. 3, we feed the prompt into LLM, and then these related descriptions Qs and Qt are generated through the LLM completion. More implementation details can be found in Appendix ??. Spatio-temporal prompt learning. With the decomposed attribute and action descriptions as cues for spatial and temporal reasoning, the next question is how to efficiently direct the model to focus on the corresponding visual regions. Reliable responses in visual question answering (VQA) necessitate careful attention to the relevant visual context as pointed out by previous studies [46; 33]. Inspired by this, we innovatively propose regularizing the response to the questions constructed from the attribute and action descriptions for adjusting the visual attention of MLLMs. Specifically, we first transform the descriptions into interrogative queries by fixed template to inquire the existence of the target. As shown in Fig. 3, from the original text input Q, we can obtain the attribute description man on the left of the man in the orange shirt, which is further transformed into interrogative sub-query Qs: Is there man on the left of the man in the orange shirt in this video? for spatial inquiry. In similar way, we can obtain the interrogative sub-query Qt for temporal inquiry. These interrogative sub-queries will be taken as input instructions of MLLMs for response generation. Logit-guided re-attention. With extracted sub-queries above, we then propose novel logit-guided re-attention (LRA) module to regularize the token prediction during response generation. Taking the spatial sub-query as an example, we first initialize learnable variable Vs with the same shape as the visual tokens Tv, and then add it to Tv as the visual input of the language model. Sub-query Qs is converted as text prompt tokens Ts by the text tokenizer and embedding layer. Given the input token sequence, the next token probability prediction over the vocabulary set is formulated as: py = exp (cid:0)logitπθ (3) where πθ denotes the parameter of the language model and is frozen in our work. logitπθ is the log probability of the generated token at time step i. y<t denotes the text tokens sequence prior to prediction time step i. We define the optimization objective by contrasting the probabilities of positive token yes and negative token no: (yi(Tv + Vs, Ts q, y<i))(cid:1) , (yyes Ls = 1 exp (cid:0)logitπθ (Tv + Vs, Ts (4) During the inference process, we conduct backpropagation to optimize the learnable variable Vs as the spatial prompt. The process is iterated Nep times by test-time tuning paradigm. By enhancing the positive response towards the sub-query Qs, we can prompt the MLLM to effectively mine target-related contextual information during VQA, which in turn highlights the attribute cues in the original text. Similarly, we can obtain the temporal prompt Vt by optimizing the temporal inference. q, y<i)) logitπθ (Tv + Vs, Ts q, y<i))(cid:1) . (yno and AT Joint inference. Based on the spatial and temporal visual prompts, we derive the attention maps RTvhw of the special token with high visual activation for spatial and temporal AS predictions, where and denote the token numbers of the height and width. From Eq. 2, we obtain the object track score Sobj and the temporal score Sframe based on AS and AT , respectively. Finally, pred = {b we integrate the predictions t=ts as the spatio-temporal grounding result. t}te 3.4 Temporal-augmented Assembling The attribute sub-query, which provides static state description, exhibits temporal independence for spatial grounding. In other words, the spatial grounding by the attribute sub-query should be 6 Table 1: Quantitative comparison on HCSTVG (v1&v2) and VidSTG (Declarative) benchmarks. Sup Method HCSTVG-v1 HCSTVG-v2 m_vIoU vIoU@0.3 vIoU@0.5 m_vIoU vIoU@0.3 vIoU@0.5 m_vIoU vIoU@0.3 vIoU@0.5 VidSTG (Declarative) TubeDETR [47] [CVPR2022] STCAT [18] [NeurIPS2022] CSDVL [26] [CVPR2023] CG-STVG [13] [CVPR2024] Full Weak ZS WINNER [24] [CVPR2023] VEM [19] [ECCV2024] CoSPaL [20] [ICLR2025] STPro [11] [CVPR2025] RedCircle [37] [CVPR2023] ReCLIP [40] [ACL2022] E3M [3] [ECCV2024] Ours LLaVA-Next-Video-7B Ours ShareGPT4Video-8B Ours Qwen2-VL-7B Ours LLaVA-OneVision-7B 32.4 35.0 36.9 38.4 14.2 14.6 22.1 17.6 9.2 14.4 19.1 20.4 20.0 23.6 24. 49.8 57.7 62.2 61.5 17.2 18.6 31.8 27.0 7.8 18.3 29.4 33.6 32.2 39.0 41.5 23.5 30.0 34.8 36.3 6.1 5.8 19.6 12.9 1.6 4.9 10.6 12.4 10.9 14.4 16. 36.4 38.7 39.5 22.2 20.0 23.6 24.4 25.6 27.7 58.8 65.5 64.5 31.4 31.1 36.8 38.9 40.5 44. 30.6 33.8 36.3 18.9 14.6 15.5 15.4 17.1 19.5 30.4 33.1 33.7 34.0 11.6 14.5 16.0 15.5 8.6 14.2 16.2 16.6 17.1 17.0 18. 42.5 46.2 47.2 47.7 14.1 18.6 20.1 19.4 7.6 17.5 20.5 26.8 27.8 27.4 29.8 28.2 32.6 32.8 33.1 7.4 8.8 13.1 12.7 0.9 7.9 11.9 11.1 11.6 11.4 12. temporally consistent for temporal augmentation (e.g., reversing the order of video frames). However, there exists temporal inconsistency when introducing temporal augmentation in current MLLMs. Here, we propose metric to measure the temporal consistency. By denoting the spatial attention maps before and after reversing the order of input frames as AS , the temporal consistency can be measured as: and AS Scons = max{s1, , sP }, sp = max (cid:16) (AS fB2M (Op)) ( AS fB2M (Op)) (cid:17) , (5) where higher Scons indicates better temporal consistency. We then divide the testing samples into ten groups in descending order of temporal consistency. Fig. 4 shows the average grounding accuracy of each group samples. The results demonstrate pronounced association between temporal consistency and spatial grounding performance. The temporal inconsistency tends to cause worse spatial localization. To this end, we integrate temporal-augmented assembling (TAS) strategy in our framework. As shown in Fig. 3, we perform frame-level reversion operation on the visual tokens and the spatial prompt simultaneously, and then optimize the spatial prompts for the input of the original frames and the temporal-augmented input, respectively. During inference, we derive the spatial prediction by assembling the attention maps of temporalaugmented input frames. The proposed TAS strategy alleviates the effect of temporal inconsistency and improves the robustness of spatial grounding. Figure 4: Spatial grounding accuracy of different groups of samples on the HC-STVGv1 dataset. These groups are ranked by descending temporal consistency."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings Datasets. We evaluate on three video benchmark datasets: HCSTVG-v1, HCSTVG-v2 [41], and VidSTG [59]. We provide detailed introduction about them in Appendx ??. Implementation Details. We adopt G-DINO [30] and SAM2 [36] for detection and tracking tubelet generation, and use GPT-4o to decompose the original query sentence into spatial and temporal sub-queries. We consider four widely-used video MLLMs: LlaVA-Next-Video-7B [22], Qwen2-VL-7B [43], ShareGPT4Video-8B [6], LlaVa-OneVision-7B [22] for demonstrating the efficiency of our method. Refer to Appendix ?? for more details. 7 (cid:80) Evaluation Metrics. We follow the standard evaluation protocol [47; 20] and use m_vIoU, and vIoU@R to assess the performance of spatio-temporal grounding. Specifically, let Si, Su denote the intersection and union between the predicted and ground-truth frames. The vIoU is computed by 1 and bt denote the detected and ground-truth bounding box at frame Su t, respectively. The m_vIoU represents the vIoU averaged over all testing samples, and vIoU@R denotes the proportion of data samples in the testing subset with vIoU greater than the threshold {0.3, 0.5}. t, bt), where IoU(b tSi 4.2 Performance Comparison Quantitative Comparison. Tab. 1 presents comparison of our method against 11 methods from three categories, including zero-shot, weakly-supervised, and fully-supervised methods. Specifically, we compare our approach with existing zero-shot SOTA approaches (e.g., E3M [3], ReCLIP [40], RedCircle [37]). Our method consistently outperforms these methods by remarkable margin on all benchmarks. Based on LLaVA-Next-Video-7B, our method outperforms E3M by 4.2% on vIoU@0.3 and 1.8% on vIoU@0.5. When integrated into the better LLaVA-OneVision-7B model, the corresponding improvements reach 12.1% and 5.7%. This shows that our framework can adapt to various MLLMs well, and better performances can be achieved by using better MLLMs. Even on the VidSTG dataset, which contains fewer action cues related to the target and thus makes temporal grounding particularly challenging, our framework still outperforms the previous SOTA methods overall. This demonstrates the strong generalization capability of our method. Besides, our method even surpasses current SOTA weakly-supervised methods on most metrics. For example, on the HCTSVG-v2 benchmark, our method outperforms CoSPaL [20] by margin of 5.5% on the m_vIoU metric. Furthermore, compared to the fully-supervised methods, our method can still achieve comparable results, which further validates the superiority of our approach. We provide quantitative comparison on the VidSTG (Interrogative) and image benchmarks in Appendix ?? Figure 5: Qualitative results on the HC-STVGv1 test set. Better spatio-temporal grounding results (green) are obtained when the DSTH strategy is being used for optimization. Qualitative Comparison. We present qualitative results in Fig. 5. In the example below, before introducing the DSTH optimization strategy, the model neglects the attribute cues in the language query and suffers from the spatial grounding error. By highlighting the attribute cues of the target, our method can direct the MLLMs toward reliable visual context and improve spatial localization. 4.3 Ablation Study In this section, based on HC-STVGv1 dataset, we analyse the effect of different proposed components when integrated into LlaVa-Next-Video and LlaVa-OneVision. We also conduct extensive ablation experiments with the hyper-parameters based on LlaVa-Next-Video. Component analysis. In Tab. 2, we first average the attention maps of all special tokens (1st row) as the baseline. This solution has achieved outstanding performance. Next, in the 2nd row, we integrate the selection of the superior token introduced in grounding token identification (GTI). It brings consistent improvements on different MLLMs. The results show that identifying the superior special token can effectively unleash the powerful comprehension ability of MLLMs. We them validate the efficiency of the decomposed spatio-temporal highlighting strategy (3rd and 4th rows). Stune and Ttune denote adopting the prompt learning in spatial and temporal inferences, respectively. It is demonstrated that the MLLMs can be directed to focus on the spatial/temporal related regions better 8 Figure 6: (a) Comparison on the number of frames Nf as input, using vIoU@0.3. (b) Ablation on the number of selected frames during temporal prediction. (c) Ablation on different trackers. when highlighting the attribute/action cues with proper prompts. When utilizing prompt learning on the two sub-tasks, the final performance can be further refined (5th row). In addition, we also find that the DSTH strategy can improve the grounding performance especially for the less efficient MLLMs (e.g., LLaVA-Next-Video). Even for the MLLMs (e.g., LLaVA-OneVision) with strong temporal comprehension ability, performance improvement can still be achieved by the test-time optimization. Finally, we introduce the temporal-augmented assembling (TAS) strategy (6th row). The overall performance can be further improved by refining the spatial localization. Component ablation on LLaVA-Next-Video and Table 2: LLaVA-OneVision. m_vIoU vIoU@0.3 vIoU@0.5 m_vIoU vIoU@0.3 vIoU@0.5 GTI Sp Tp TAS LLaVA-Next-Video LLaVA-OneVision 15.2 16.3 18.0 18.4 19.9 20.4 25.1 26.6 30.1 29.5 32.1 33.6 8.5 9.3 10.1 10.1 11.9 12. 21.3 23.3 24.1 23.8 24.3 24.8 36.1 38.8 40.3 39.7 40.7 41.5 12.6 15.1 16.0 15.5 16.0 16.3 Effect of input frames Nf . Fig. 6(a) analyzes the effect of using different numbers of video frames Nf as input. We can see that more input frames provide richer visual context for model inference and lead to better overall performance. Besides, the performance becomes slightly peaked as the number of input frames increases. To balance between performance and efficiency during inference, we uniformly sample 20 frames as the visual input of MLLMs. In Fig. 6, we also evaluateVanilla-Tuning, solution that directly applies test-time optimization to the entire text query instead of decomposing it into attribute/action sub-queries. We observe that the performance of Vanilla-Tuning is inferior to our method. It demonstrates that decomposing the text query into attribute/action sub-queries can help facilitate the intensive spatio-temporal comprehension of MLLMs. Ablation on the number of predicted frames. Fig. 6(b) analyzes the effect of predicted frame numbers for temporal grounding based on the HCSTVG-v1 dataset. Here, we consider selecting the optimal frame number from the set {3, 5, 7, 9, 11}. In general, when is set as 7, the optimal result can be obtained. Thus, we select top-7 frames as the prediction during temporal grounding. Trackers ablation. Fig. 6(c) analyzes the effect of different trackers. Besides SAM2 [36], foundation model for tracking, we also consider two other tracking models (i.e., ByteTrack [58] and BoTSort [2]) for analysis. It is reasonable that when stronger tracking model (e.g., SAM2) is used for generating spatio-temporal tubelets, we can obtain better STVG performances. Besides, when suboptimal tracking models are used, our method can still achieve comparable or better performances than the current SOTA methods, which shows the generalization of our approach."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we have presented novel MLLM-based zero-shot framework for the spatio-temporal video grounding (STVG) task. Our approach is initiated by identifying the grounding capability of special tokens in widely used MLLMs during response generation. To leverage and unleash the comprehension ability of MLLMs, we have proposed the decomposed spatio-temporal highlighting (DSTH) strategy. It first decomposes the text query into attributes and actions sub-queries. It then employs logit-guided re-attention (LRA) module to sharpen the spatial/temporal visual context comprehension. We have also proposed the temporal-augmented assembling (TAS) strategy to alleviate the effect of temporal inconsistency. Extensive experiments conducted on three STVG benchmarks demonstrate the effectiveness of our proposed framework. 9 Our approach does have limitations. For example, it may struggle to process long videos well due to high computational consumption caused by MLLMs. As future work, we would like to consider incorporating token pruning and key frame selection techniques into the model design."
        },
        {
            "title": "References",
            "content": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv:2303.08774 (2023) [2] Aharon, N., Orfaig, R., Bobrovsky, B.Z.: Bot-sort: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651 (2022) [3] Bao, P., Shao, Z., Yang, W., Ng, B.P., Kot, A.C.: E3m: zero-shot spatio-temporal video grounding with expectation-maximization multimodal modulation. In: ECCV. pp. 227243. Springer (2024) [4] Cao, S., Gui, L.Y., Wang, Y.X.: Emerging pixel grounding in large multimodal models without grounding supervision. arXiv preprint arXiv:2410.08209 (2024) [5] Chen, D., Wu, Z., Liu, F., Yang, Z., Zheng, S., Tan, Y., Zhou, E.: Protoclip: Prototypical contrastive language image pretraining. IEEE Transactions on Neural Networks and Learning Systems (2023) [6] Chen, L., Wei, X., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Lin, B., Tang, Z., et al.: Sharegpt4video: Improving video understanding and generation with better captions. In: The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track [7] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In: CVPR. pp. 2418524198 (2024) [8] Deng, J., Yang, Z., Chen, T., Zhou, W., Li, H.: Transvg: End-to-end visual grounding with transformers. In: ICCV. pp. 17691779 (2021) [9] Ding, H., Liu, C., Wang, S., Jiang, X.: Vision-language transformer and query generation for referring segmentation. In: ICCV. pp. 1632116330 (2021) [10] Gao, W., Wan, F., Pan, X., Peng, Z., Tian, Q., Han, Z., Zhou, B., Ye, Q.: Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In: ICCV. pp. 28862895 (2021) [11] Garg, A., Kumar, A., Rawat, Y.S.: Stpro: Spatial and temporal progressive learning for weakly supervised spatio-temporal grounding. arXiv preprint arXiv:2502.20678 (2025) [12] Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.: Scaling open-vocabulary image segmentation with image-level labels. In: ECCV. pp. 540557. Springer (2022) [13] Gu, X., Fan, H., Huang, Y., Luo, T., Zhang, L.: Context-guided spatio-temporal video grounding. In: CVPR. pp. 1833018339 (2024) [14] Han, Z., Zhu, F., Lao, Q., Jiang, H.: Zero-shot referring expression comprehension via structural similarity between images and captions. In: CVPR. pp. 1436414374 (2024) [15] Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expressions. In: Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14. pp. 108124. Springer (2016) [16] Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.: Visual prompt tuning. In: ECCV. pp. 709727. Springer (2022) [17] Jiang, Z., Chen, J., Zhu, B., Luo, T., Shen, Y., Yang, X.: Devils in middle layers of large visionlanguage models: Interpreting, detecting and mitigating object hallucinations via attention lens. arXiv preprint arXiv:2411.16724 (2024) [18] Jin, Y., Li, Y., Yuan, Z., Mu, Y.: Embracing consistency: one-stage approach for spatiotemporal video grounding. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. pp. 2919229204 (2022) 11 [19] Jin, Y., Mu, Y.: Weakly-supervised spatio-temporal video grounding with variational crossmodal alignment. In: ECCV. pp. 412429. Springer (2024) [20] Kumar, A., Kira, Z., Rawat, Y.S.: Contextual self-paced learning for weakly supervised spatiotemporal video grounding. arXiv preprint arXiv:2501.17053 (2025) [21] Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning segmentation via large language model. In: CVPR. pp. 95799589 (2024) [22] Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al.: Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024) [23] Li, H., Chen, J., Wei, Z., Huang, S., Hui, T., Gao, J., Wei, X., Liu, S.: Llava-st: multimodal large language model for fine-grained spatial-temporal understanding. arXiv preprint arXiv:2501.08282 (2025) [24] Li, M., Wang, H., Zhang, W., Miao, J., Zhao, Z., Zhang, S., Ji, W., Wu, F.: Winner: Weaklysupervised hierarchical decomposition and alignment for spatio-temporal video grounding. In: CVPR. pp. 2309023099 (2023) [25] Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. In: EMNLP. pp. 59715984 (2024) [26] Lin, Z., Tan, C., Hu, J.F., Jin, Z., Ye, T., Zheng, W.S.: Collaborative static and dynamic vision-language streams for spatio-temporal video grounding. In: CVPR. pp. 2310023109 (2023) [27] Liu, C., Ding, H., Jiang, X.: Gres: Generalized referring expression segmentation. In: CVPR. pp. 2359223601 (2023) [28] Liu, F., Liu, Y., Xu, K., Ye, S., Hancke, G.P., Lau, R.W.: Language-guided salient object ranking. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 2980329813 (2025) [29] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems [30] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In: ECCV. pp. 3855. Springer (2024) [31] Ma, C., Jiang, Y., Wu, J., Yuan, Z., Qi, X.: Groma: Localized visual tokenization for grounding multimodal large language models. In: ECCV. pp. 417435. Springer (2024) [32] Munasinghe, S., Thushara, R., Maaz, M., Rasheed, H.A., Khan, S., Shah, M., Khan, F.: Pgvideo-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435 (2023) [33] Prasad, A., Stengel-Eskin, E., Bansal, M.: Rephrase, augment, reason: Visual grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861 (2023) [34] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML. pp. 87488763. PmLR (2021) [35] Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R.M., Xing, E., Yang, M.H., Khan, F.S.: Glamm: Pixel grounding large multimodal model. In: CVPR. pp. 1300913018 (2024) [36] Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) [37] Shtedritski, A., Rupprecht, C., Vedaldi, A.: What does clip know about red circle? visual prompt engineering for vlms. In: ICCV. pp. 1198711997 (2023) 12 [38] Shu, M., Nie, W., Huang, D.A., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.: Test-time prompt tuning for zero-shot generalization in vision-language models. In: Advances in Neural Information Processing Systems (2022) [39] Su, R., Yu, Q., Xu, D.: Stvgbert: visual-linguistic transformer based framework for spatiotemporal video grounding. In: ICCV. pp. 15331542 (2021) [40] Subramanian, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.: Reclip: strong zero-shot baseline for referring expression comprehension. In: ACL. pp. 51985215 (2022) [41] Tang, Z., Liao, Y., Liu, S., Li, G., Jin, X., Jiang, H., Yu, Q., Xu, D.: Human-centric spatiotemporal video grounding with visual transformers. TCSVT 32(12), 82388249 (2021) [42] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. pp. 1034710357. PMLR (2021) [43] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al.: Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024) [44] Wasim, S.T., Naseer, M., Khan, S., Yang, M.H., Khan, F.S.: Videogrounding-dino: Towards open-vocabulary spatio-temporal video grounding. In: CVPR. pp. 1890918918 (2024) [45] Wu, M., Cai, X., Ji, J., Li, J., Huang, O., Luo, G., Fei, H., JIANG, G., Sun, X., Ji, R.: Controlmllm: Training-free visual prompt learning for multimodal large language models. In: NeurIPS [46] Xiao, J., Yao, A., Li, Y., Chua, T.S.: Can trust your answer? visually grounded video question answering. In: CVPR. pp. 1320413214 (2024) [47] Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Tubedetr: Spatio-temporal video grounding with transformers. In: CVPR. pp. 1644216453 (2022) [48] Yang, Z., Liu, Y., Xu, W., Huang, C., Zhou, L., Tong, C.: Learning prototype via placeholder for zero-shot recognition. arXiv preprint arXiv:2207.14581 (2022) [49] Yang, Z., Liu, Y., Lin, J., Hancke, G., Lau, R.W.: Boosting weakly-supervised referring image segmentation via progressive comprehension. arXiv preprint arXiv:2410.01544 (2024) [50] Yang, Z., Wang, J., Tang, Y., Chen, K., Zhao, H., Torr, P.H.: Lavt: Language-aware vision transformer for referring image segmentation. In: CVPR. pp. 1815518165 (2022) [51] Yoon, H.S., Yoon, E., Tee, J.T.J., Hasegawa-Johnson, M.A., Li, Y., Yoo, C.D.: C-tpt: Calibrated test-time prompt tuning for vision-language models via text feature dispersion. In: ICLR [52] Yuan, Y., Li, W., Liu, J., Tang, D., Luo, X., Qin, C., Zhang, L., Zhu, J.: Osprey: Pixel understanding with visual instruction tuning. In: CVPR. pp. 2820228211 (2024) [53] Zeng, S., Chang, X., Xie, M., Liu, X., Bai, Y., Pan, Z., Xu, M., Wei, X.: Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. arXiv preprint arXiv:2505.17685 (2025) [54] Zhai, Y., Tong, S., Li, X., Cai, M., Qu, Q., Lee, Y.J., Ma, Y.: Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313 (2023) [55] Zhang, J., Khayatkhoei, M., Chhikara, P., Ilievski, F.: Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422 (2025) [56] Zhang, J., Huang, J., Zhang, X., Shao, L., Lu, S.: Historical test-time prompt tuning for vision foundation models. In: NeurIPS [57] Zhang, T., Li, X., Fei, H., Yuan, H., Wu, S., Ji, S., Loy, C.C., Shuicheng, Y.: Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In: The Thirtyeighth Annual Conference on Neural Information Processing Systems 13 [58] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating every detection box. In: ECCV. pp. 121. Springer (2022) [59] Zhang, Z., Zhao, Z., Zhao, Y., Wang, Q., Liu, H., Gao, L.: Where does it exist: Spatio-temporal video grounding for multi-form sentences. In: CVPR. pp. 1066810677 (2020) [60] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discriminative localization. In: CVPR. pp. 29212929 (2016) [61] Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels from clip. In: ECCV. pp. 696712. Springer (2022) [62] Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. IJCV 130(9), 23372348 (2022)"
        }
    ],
    "affiliations": [
        "Department of Computer Science, City University of Hong Kong"
    ]
}