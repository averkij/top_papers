{
    "paper_title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence",
    "authors": [
        "Yijiong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality."
        },
        {
            "title": "Start",
            "content": "Yijiong Yua,b aTsinghua University bOpenCSG 5 2 0 2 2 ] . [ 2 3 3 5 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and timeconsuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using specialized attention mask, processing them within single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality. Our code is available in github."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs), capable of performing complex reasoning processes, excel across diverse array of tasks. However, their autoregressive decoding structure renders them inefficient for parallelizable tasks. Parallelizable tasks are those that, while requiring multiple steps, involve many steps that can be executed concurrently due to the absence of priority relationship. In the decoding phase, LLMs generate tokens sequentially, one at time. This sequential nature forces parallel steps to be processed sequentially, imposing considerable computational overhead on the attention layer and hindering the full utilization of the hardwares parallel computing capabilities. While methods like skeleton-of-thoughts (Ning et al., 2024) aim to address such inefficiencies, they suffer from notable drawbacks. Firstly, they rely on batch decoding or multiple API calls, which, although reducing inference time, exponentially increase computational load and memory requirements. These constraints force reduction in batch 1 size when GPU memory is limited. Secondly, these methods require distinct prompt templates for different generation stages, preventing reuse of the key-value (KV) cache. Lastly, skeleton-of-thoughts treat every point in their reasoning skeleton as independent, potentially neglecting causal relationships between different points. To overcome these limitations, we propose novel decoding method called \"Parallel Decoding in One Sequence,\" specifically designed to address parallelizable tasks in LLM reasoning. This method operates in three stages: (1) identifying parallelizable steps in the reasoning process, (2) parallel decoding of these steps, and (3) concatenating the results and continuing generation. In the first stage, the model generates the reasoning process, but for parallelizable steps, special token is used to mark them. For these steps, only the initial few words are generated, while the remaining portion is omitted with ellipses, reducing the number of output tokens. In the second stage, we decode each steps subsequent tokens in parallel, using their initial tokens as prefixes. We modify the attention mask and the position IDs so the model can generate multiple tokens simultaneously in single forward pass, significantly accelerating the generation process. Finally, in the third stage, we concatenate the full reasoning outputs from each parallel step and append them to the sequence, allowing the model to resume its reasoning process. The overall process is illustrated in Figure 1. Unlike previous methods, our approach eliminates the need for additional memory usage and KV cache recomputation, as we do not create multiple sequences. Moreover, our method relies on the LLM itself to identify parallelizable steps but not always manually treats every step as independent, avoiding the loss of information caused by the inappropriate use of parallel processing for steps with causal relationships, enabling greater flexibility and adaptability. Figure 1: Comparison between our method and traditional decoding for case with four parallel branches. Blue and green blocks in the attention mask indicate \"can see,\" while white blocks indicate \"cannot see.\" Experiments demonstrate that our method significantly enhances decoding speed, especially for tasks with large number of parallelizable steps. Moreover, it achieves this acceleration with only minor impact on generation quality. Since our approach does not require additional training or modules, it is generally applicable across different types of LLMs."
        },
        {
            "title": "2 Related Works",
            "content": "The computational demands of LLMs have inspired the development of various techniques aimed at accelerating the decoding process. One widely recognized method is speculative decoding (Leviathan et al., 2023), which employs smaller assistant model to generate tokens first, allowing the main model to verify thema process that leverages parallelizable verification for faster inference. Jacobi decoding (Santilli et al., 2023) constructs initial sequences using pad tokens, enabling iterative updates to parallelize token generation within each sequence. Medusa (Cai et al., 2024) enhances the target LLM with auxiliary guess heads to facilitate self-speculation, achieving up to threefold speedup on diverse tasks. Skeleton-of-thoughts (Ning et al., 2024) prompts LLMs to generate skeleton containing multiple points and completes each point in parallel by employing batch decoding or multiple API calls."
        },
        {
            "title": "3 Method",
            "content": "Our method includes 3 stages: branch title generation, parallel decoding, concatenating and continuation. Stage 1. First, as normal, we give the model the task description. After it, we append an additional prompt (detailed in Appendix A), by which the model is prompted to mark parallelizable steps with ####, generating only titles followed by colone and an ellipses (e.g., \"#### Step 1: ......\"). To ensure the correct format is generated, we manually manipulate the logits to force it to immediately generate an ellipsis after colone, and #### after an ellipsis. We group this bunch of consecutive parallel steps as \"parallel block\", and we prompt it that terminator (%%%%) should be generated to signify the end of the parallel block. This yields compact skeleton while ensuring correct parallelism detection. The specific prompt we use in this stage is in Appendix A. Stage 2. We use each parallel steps title generated in stage 1 as each branchs prefix tokens. We signify the number of parallel steps as n. For parallel branches: We reuse KV cache from pre-parallel steps (the part before the parallel block). We process tokens per forward pass using belt-like attention mask (as shown in Figure 1), which ensures that the branches are isolated from one another while sharing pre-parallel tokens. 2 The position ids of the tokens processed in one forward pass are all the same. The position ID is incremented by one after every forward pass. We terminate branches upon generating ####, and pad shorter branches until all branches are completed. Stage 3. We sequentially concatenate the fully decoded parallel steps from Stage 2 with the original reasoning sequence. The model then resumes decoding as normal, and the KV cache of prior steps and the prompt can be reused. If \"####\" is detected again, the process loops back to Stage 1, initiating the next parallel block."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details The key innovation of our method lies in the modified causal attention mask, termed the \"belt-like mask.\" While FlashAttention-2 (Dao, 2023) accelerates attention computation and saves memory, it does not support custom masks. Thus, we modified its source code to track two additional parameters: the number of branches (n) and the position where parallel decoding begins. They both derived from content generated in Stage 1. Specifically, where the first special mark \"####\" appears determines the position where parallel decoding begins, and the number of special marks \"####\" determines the number of branches. For experimentation, we use Qwen2.5 (Team, 2024) implemented via the HuggingFace Transformers (Wolf et al., 2020), coupled with our customized FlashAttention-2 package. All models are run on an single A100 GPU with bfloat16 precision. We set temperature to 0 for all the generation experiments. In order to make baseline and ensure that the length and format of the answers are basically consistent, we provide the model with the task description and the additional prompt used in stage 1 of our method, and let it generate answers using standard decoding methods."
        },
        {
            "title": "4.2 Data",
            "content": "Our method targets parallelizable tasks, for which we select retrieval task, multi-document QA task and planning task requiring reasoning. In all the tasks, the reasoning process involves sequential analysis of individual items (e.g., multiple students, documents or aspects), which is inherently parallelizable. The retrieval task and the multi-document QA task belongs to long-context tasks, with long reference text in the prompt on which the model must answer based. While for the planning task, our prompt is relatively short, letting the model answer based on its internal knowledge. The retrieval task utilizes the student resume retrieval dataset from difficult-retrieval (Yu et al., 2024), containing 100 samples per setting. The model retrieves students whose GPA falls within specified range from among 10 or 30 students, requiring parallel GPA analysis. The multi-document QA task involves \"2WikiMultihopQA\" from LongBench (Bai et al., 2023), comprising 200 samples. The model must answer questions based on 10 documents, with only one of them containing relevant information, necessitating parallel document analysis. The planning task is multi-branch task, which requires the model to first analyze multiple aspects in parallel and summarize them to give final plan. We design 100 planning tasks using QwQ-32B (Team, 2025), which covers topics from various fields such as technology, politics, and business. The number of branches for the task ranges from 2 to 10, with an average of 4.4 branches. We use exact-match to assess the retrieval task and use GPT-4o-2024-11-20 (OpenAI, 2024) to rate (giving score from 1 to 5) the answer quality of the QA task and planning task. The prompt for GPT-4o rating is shown in Appendix C. For each sample, we recorded the total time (including both prefilling and decoding) it took for the model to complete the inference. The decoding speed, measured in average number of tokens generated per second during the decoding phase, is also recorded."
        },
        {
            "title": "4.3 Results",
            "content": "Table 1 presents retrieval task results. For contexts with 10 items, our method nearly doubles decoding speed for both models, and for 30 items, the improvement is even greater. Accuracy loss is observed but remains under 10%, representing an acceptable trade-off. Appendix offers generated example with our parallel decoding method of the student retrieval task. Table 2 displays results of the QA task. Our method improves the decoding speed by about 60% for Qwen2.5-14b and about 45% for Qwen2.5-7b. The answer quality is basically maintained, as evi3 Model Num items Method Accuracy (%) Time (s) Speed (tokens/s) Qwen2.5-14b-Instruct Qwen2.5-7b-Instruct 10 10 30 30 10 10 30 30 normal parallel normal parallel normal parallel normal parallel 92 89 56 47 93 95 59 62 15.8 9.3 36.8 17.1 10.5 4.9 17.7 14.4 21.2 40.5 21.3 49.2 37.2 65.7 35.6 71. Table 1: Accuracy, inference time and decoding speed on the retrieval task. Model Method Score Time (s) Speed (tokens/s) Qwen2.5-14b-Instruct Qwen2.5-7b-Instruct normal parallel normal parallel 3.77 3.67 3.57 3.49 15.6 11.4 7.2 8. 17.5 27.5 33.6 49.0 Table 2: Evaluation scores, inference time and decoding speed on the multi-documents QA task."
        },
        {
            "title": "Method",
            "content": "Score Time (s) Speed (tokens/s) Qwen2.5-14b-Instruct Qwen2.5-7b-Instruct normal parallel normal parallel 3.56 3. 3.69 3.30 24.5 17.6 17.6 11.9 21.1 35.8 36.3 49.8 Table 3: Evaluation scores, inference time and decoding speed on the multi-branch planning task. denced by minor decrease in GPT-4o rating scores. We also find our method results in the model outputting longer reasoning process, making the inference time of Qwen2.5-7b even increased little. Table 3 displays results of the planning task. For Qwen2.5-14b, our method improves the decoding speed by approximately 70% and saves the total inference time by nearly 30%. Moreover, the answer quality is perfectly maintained, even improved little. For Qwen2.5-7b, our method improves the decoding speed by nearly 40%, but the answer quality is slightly decreased. This indicates that our method may perform better for larger models, as larger models can better strictly follow our prompts."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces \"Parallel Decoding in One Sequence,\" novel method to accelerate the reasoning process of LLMs for parallelizable tasks. By optimizing parallel step decoding, our method substantially reduces inference time while basically maintaining model flexibility and quality. Critically, it achieves these advantages without additional memory usage or recomputing the KV caches of the prefilling stage. Future work could explore extending this approach to more complex and diverse tasks. Investigating its applicability across different model architectures and sizes may also provide insights into scalability and generalizability. Overall, our method constitutes an important advancement in efficient LLM reasoning for practical applications."
        },
        {
            "title": "6 Limitations",
            "content": "For smaller models, Stage 1 may fail to produce the required format, preventing subsequent stages from functioning. Specifically, non-standard format impedes recognition of branch titles and the determination of the number of parallel branches. 4 Furthermore, our method has been tested only on synthetic datasets. For more complex tasks, where reasoning processes exhibit greater uncertainty, extending our approach may present challenges."
        },
        {
            "title": "A Prompt",
            "content": "The additional prompt we use in stage 1 is:"
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. Preprint, arxiv:2401.10774 [cs]. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. Preprint, arxiv:2211.17192 [cs]. Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. 2024. Skeletonof-thought: Prompting LLMs for efficient parallel generation. Preprint, arxiv:2307.15337 [cs]. OpenAI. 2024. GPT-4o System Card. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. 2023. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1233612355. Qwen Team. 2024. Qwen2.5: party of foundation models! Section: blog. Qwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Yijiong Yu, Ma Xiufa, Fang Jianwei, Zhi Xu, Su Guangyao, Wang Jiancheng, Yongfeng Huang, Zhixiao Qi, Wei Wang, Weifeng Liu, Ran Chen, and Ji Pei. 2024. Hyper-multi-step: The Truth Behind Difficult Long-context Tasks. 5 When you need to sequentially handle multiple parallel steps (the steps are individual, for example, analyzing multiple individual documents, planning multiple branches, evaluating multiple aspects) during the reasoning process, you must strictly adhere to the following format: You need to prefix each step with ####, followed by the steps title, and then colon : (an English colon). After all the steps are completed, you need to output ####%%%%, and only then can you proceed with the subsequent reasoning process. Example 1: Question: [Resumes of A, B, C, D] Please analyze which of the four individuals best meets the requirements. Answer: Let us analyze the strengths of each person based on their resumes. ####Strengths of A:...... ####Strengths of B:...... ####Strengths of C:...... ####Strengths of D:...... ####%%%%Therefore, believe that As resume best meets the requirements. Example 2: Question: [document 1,2,3,4] Based on the information in the documents, what is the birthday of Jack? Answer: Let us analyze each documents. ####document 1:...... ####document 2:...... ####document 3:...... ####document 4:...... ####%%%%Therefore, Jacks birthday is 5th, May, 2000. Example 3: Question: Please analyze the development status of China from the aspects of economy, politics, culture, and society. Answer: Let us analyze from four aspects. ####Economy:......####Politics:......####Culture:......####Society:...... ####%%%%Therefore, we can conclude that the development status of China is...... Note that the examples is only used to illustrate the format; the specific content should closely revolve around the specific task requirements. If you see that the part after the colon in certain step is replaced with an ellipsis, it means that the specific content does not need to be provided for this reasoning step, only the title is required, and you should directly proceed to the next parallel step. Otherwise, give complete and clear analysis for this step. Please be careful, do not forget any necessary steps. Note that only branch step should start with ####. If it is stem or general step, you should not add ####."
        },
        {
            "title": "B Example of the content generated in",
            "content": "our method Here we show an example of what the model generated when using our parallel decoding method, in the student retrieval task. We omit some content in the prompt. As we can see, in stage 1, the model only generate the skeleton, where the specific analysis process of each branch is represented by ellipses. Then in stage 2, each branchs specific reasoning process is completed by parallel decoding. Finally, in stage 3, all the reasoning processes are concatenated, and the model generate the final conclusion. Prompt: Here are 10 students resumes: The student named Nitin Sharma is 29 years old, graduated from IIM Ahmedabad with GPA of 4.09. He/She is interested in Finance, Cooking, Travel and his/her self-introduction is: Finance enthusiast who loves cooking and exploring diverse cuisines. ...... The student named Lily Wilson is 23 years old, graduated from University of Brighton with GPA of 2.45. He/She is interested in Political Science, Activism, Public Speaking and his/her self-introduction is: am dedicated to activism and often channel my thoughts into public speaking. ...... Question: Which student has GPA between 2.36 and 2.75? Please give your final answer (the students name) in the format of \"name: answer\" You should check every student to judge whether he meets the requirement in your reasoning process. Stage 1: Let us analyze each students GPA to find the one between 2.36 and 2.75. ####Nitin Sharma: ...... ####Hallie Turner: ...... ####Hikaru Sawai: ...... ####Abigail Clark: ...... ####Charlotte Phillips: ...... ####Lily Wilson: ...... ####Cao Ling: ...... ####William Stewart: ...... ####Sienna Torres: ...... ####Eli White: ...... ####%%%% Stage 2: ####Nitin Sharma: GPA is 4.09, which is not between 2.36 and 2.75. ####Hallie Turner: GPA is 3.68, which is not between 2.36 and 2.75. ####Hikaru Sawai: GPA is 2.98, which is higher than 2.75. ####Abigail Clark: GPA is 1.83, which is below the range. ####Charlotte Phillips: GPA is 1.54, which is below the range. ####Lily Wilson: GPA is 2.45, which falls within the range of 2.36 to 2.75. ####Cao Ling: Cao Lings GPA is 2.82, which is above 2.75. ####William Stewart: GPA is 4.04, which is not between 2.36 and 2.75. ####Sienna Torres: GPA is 4.48, which is not between 2.36 and 2.75. ####Eli White: GPA is 4.61, which is not between 2.36 and 2.75. Stage 3: Based on the analysis, Lily Wilson is the student whose GPA falls within the specified range. Name: Lily Wilson"
        },
        {
            "title": "C Prompt for GPT rating",
            "content": "The prompt used for GPT-4o to rate the answer quality is as follows. For rating, we set the temperature to 0 and max generated tokens to 4k. You are an assistant skilled in evaluating the quality of responses. Please act as an impartial judge to assess the accuracy, usefulness, and clarity of an AI assistants response to users question. will provide the question, reference answer, and the AI assistants response. You need to evaluate the quality of the AI assistants response and give score from 1 to 5. 1 point: The response is completely incorrect, unhelpful to the question, or poorly formatted with low readability. 2 points: The response is basically incorrect, somewhat helpful to the question, or somewhat poorly formatted with low readability. 3 points: The response is basically correct, moderately helpful to the question, or moderately formatted with average readability. 4 points: The response is correct, helpful to the question, or well-formatted with good readability. 5 points: The response is completely correct, highly helpful to the question, and neatly formatted with excellent readability. Please provide your evaluation in the format: \"Reason: ... Score: ...\". Users question: {question} Reference answer: {reference} AI assistants answer: {answer}"
        }
    ],
    "affiliations": [
        "OpenCSG",
        "Tsinghua University"
    ]
}