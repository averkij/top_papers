{
    "paper_title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
    "authors": [
        "Yi-Cheng Lin",
        "Yu-Hua Chen",
        "Jia-Kai Dong",
        "Yueh-Hsuan Huang",
        "Szu-Chi Chen",
        "Yu-Chen Chen",
        "Chih-Yao Chen",
        "Yu-Jung Lin",
        "Yu-Ling Chen",
        "Zih-Yu Chen",
        "I-Ning Tsai",
        "Hsiu-Hsuan Wang",
        "Ho-Lam Chung",
        "Ke-Han Lu",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream."
        },
        {
            "title": "Start",
            "content": "TAU: BENCHMARK FOR CULTURAL SOUND UNDERSTANDING BEYOND SEMANTICS Yi-Cheng Lin1, Yu-Hua Chen2, Jia-Kai Dong1, Yueh-Hsuan Huang1, Szu-Chi Chen1, Yu-Chen Chen1, Chih-Yao Chen1, Yu-Jung Lin1, Yu-Ling Chen1, Zih-Yu Chen1, I-Ning Tsai1, Hsiu-Hsuan Wang1, Ho-Lam Chung1, Ke-Han Lu1, Hung-yi Lee1 1National Taiwan University 2University of Toronto 5 2 0 2 0 3 ] . e [ 1 9 2 3 6 2 . 9 0 5 2 : r ABSTRACT Large audiolanguage models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), benchmark of everyday Taiwanese soundmarks. TAU is built through pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream. Index Terms culture understanding, localization, large audiolanguage model, benchmark 1. INTRODUCTION Understanding the sounds around us often depends more on what is heard than on what is said. Everyday acoustic cues carry cultural meaning that is independent of language. Metro chimes, scooter beepers, and convenience-store jingles are recognized by locals because of exposure, not because of words. Contemporary audio evaluation already includes environmental sound [1, 2]. However, strong performance on many benchmarks can be achieved by matching generic categories that appear worldwide and are heavily represented in web-scale sources, such as siren, dog bark, doorbell, rainfall, and engine noise. This leaves open key question: can current models recognize localized, non-semantic audio that communities instantly identify but that outsiders rarely encounter? At the same time, Large AudioLanguage Models (LALMs) [36] are recently becoming important backbones for multimodal assistants, accessibility tools, and embodied agents. Evaluation of modern LALMs has surged, but current benchmarks still emphasize speech semantics or generic, globally sourced environmental audio. LALM benchmarks, such as Dynamic-SUPERB [7, 8], AIR-Bench [9], and MMAU [10], have advanced coverage across speech, paralinguistics, and some non-speech sounds; yet they were not designed to probe localized cultural knowledge in non-speech audio. They use other datasets sampled from global platforms (e.g., Freesound, YouTube). While this breadth is valuable, the taxonomies and sampling strategies seldom encode community-specific distinctiveness. This gap carries significant implications, both for scientific methodology and societal equity. Scientifically, failing to account for localized audio compromises the validity of evaluation benchmarks. Soundscape scholarship has long argued that communities possess characteristic soundmarks, distinctive auditory cues that anchor identity and place [11]. Recognizing these depends on cultural exposure, not linguistic comprehension. Consequently, benchmarks that ignore such soundmarks create distorted view of model performance, overestimating real-world competence while underestimating the generalization challenges across different geographic and cultural contexts. These scientific oversights have direct societal consequences. Studies show that skewed or unrepresentative training data can perpetuate inequities and harm underrepresented communities. [1214] Without deliberate localization, models trained on globally aggregated data will inevitably be deaf to community-specific signals. This leads to technologies that underperform for and marginalize populations outside the cultural and geographic mainstream. We propose pathway toward building localized audio benchmarks and present Taiwan as case study. Our benchmark, TAU (Taiwan Audio Understanding) 1, addresses this cultural localization gap. TAU curates everyday, locally distinctive Taiwanese nonspeech sounds and evaluates models with multiple-choice questions (MCQs) that cannot be answered by semantic reasoning alone, steering evaluation toward timbre, rhythm, and iconic acoustic patterns. Beyond Taiwan, TAU illustrates how localized benchmarks can highlight cultural blind spots in audiolanguage models, include underrepresented communities in multimodal evaluation, and guide the design of more equitable and robust multimodal systems. 2. RELATED WORKS LALMs increasingly unify audio analysis with open-ended language outputs. Qwen2-Audio [4] reports strong instruction-following and audio analysis; DeSTA2.5-Audio [3] targets robust auditory perception and instruction following with general-purpose LALM design, reporting broad improvements without task-specific audio instruction tuning; Audio-Reasoner [15] focuses on long-context reasoning over auditory input; Gemma-3n [16] serves as lightweight yet competitive model for audiolanguage tasks. Together, these LALMs illustrate the diversity of design choices and capabilities in the current landscape. In parallel, LALM evaluation has branched into three broad categories. (i) Chat-based audio understanding benchmarks assess open-ended comprehension and dialogue grounded in audio, e.g. SD-Eval [17] for spoken dialogue beyond words, ADU-Bench [18] for open-ended audio dialogue, and VoiceBench [19] for LLM- (ii) General-purpose multi-task audio unbased voice assistants. derstanding and reasoning suites provide wide coverage across 1https://dlion168.github.io/TAU demo/ Fig. 1. Construction workflow of our localized audio benchmark TAU. speech, non-speech, and paralinguistics, such as AudioBench [20], MMAU [10], and AIR-Bench [9] for acoustic-aspect sensitivity. (iii) Domain-focused stress tests target specific modalities or skills, including Audio Entailment [21] for deductive reasoning, MuChoMusic [22] for music understanding, SAKURA [23] for audio muti-hop reasoning, ToxicTone [24] for toxic speech detection, and Speech-IFEval [25] for instruction-following skills. While these efforts broaden coverage, most draw on globally sourced, generic corpora and rarely target locale-specific, non-semantic cues, which motivates our culturally localized TAU benchmark. Recent benchmarks have begun probing countryor regionspecific knowledge. BLEnD [26] evaluates locally grounded knowledge across 16 countries/regions and 13 languages, showing large culture-dependent gaps even for frontier models. CulturalBench [27] provides 1,227 human-written, human-verified questions that span 45 global regions and 17 cultural topics. ThaiCLI [28] targets Thai cultural competence alongside core capabilities, illustrating how national-culture evaluation complements language proficiency assessments. TaiwanVQA [29] and VisTW [30] focus on Taiwanese culture through visionlanguage QA. However, these resources remain textor image-centric. None evaluates whether models can recognize locale-specific, non-semantic acoustic cues that locals identify from sound alone. TAU complements culture-specific text or vision benchmarks by testing audio cultural grounding, capability that current suites do not measure. 3. DATASET COLLECTION 3.1. Design Principles Local identifiability TAU targets sounds that Taiwanese listeners recognize immediately, while non-locals are unlikely to identify them without prior exposure. Curators prioritize everyday soundmarks (e.g., public signals, transportation cues, consumer-device prompts) rather than globally ubiquitous sounds. Semantic independence Items must be answerable from the acoustic signature itself, using timbre, rhythm, envelope, or characteristic patterns, rather than lexical content. When speech fragments are present, they are trimmed or counter-balanced so that transcripts alone cannot reveal the answer. Diverse yet accessible We encourage breadth across venues, devices, and activities, but avoid overly rare or expert-only phenomena. The aim is creative coverage of culturally salient sounds that typical residents encounter without specialized knowledge. 3.2. Roles Annotators are 10 native Taiwanese recruited from different regions across Taiwan with gender-balanced composition to ensure both regional and demographic diversity in practices and soundscapes. Editors nominate culturally distinctive sounds and gather candidate audio sources. Checkers verify that the proposed sounds meet the design principles in Sec. 3.1, performing quality control on timing, audibility, and semantic independence. Editors also refine question stems, options, and labels to ensure clarity and cultural plausibility. Finally, Reviewers conduct human performance validation to confirm that items are both solvable and appropriately challenging. 3.3. Workflow The proposed dataset collection workflow is summarized in Fig. 1. 3.3.1. Concept collection Editors first compiled pool of 550 Taiwan-specific soundmarks, each described with short rationale for why it is instantly recognizable to locals but not to non-locals. The collection emphasizes diversity across venues, devices, and activities while avoiding rare or expert-only edge cases. Checkers check if these sounds follow the design principles in Sec. 3.1, producing list of candidate sounds with provisional categories and concise descriptors. 3.3.2. Audio Collection Approved targets are sourced from two channels: permissively licensed Creative Commons repositories (YouTube and aporee map) and self-recordings contributed by the team. Annotators provided metadata including source url, start time, and end Fig. 2. Example TAU multiple-choice items in three categories Media, Transit, and Retail, showing hop-type labels and culturally grounded distractors. 385 400 300 u 100 69 261 288 149 104 154 107 36 o 60 40 20"
        },
        {
            "title": "Payment",
            "content": "0 5 10 15 Duration (s) 20 30 Fig. 4. Histogram of audio durations in TAU benchmark. Fig. 3. Distribution of question types in TAU benchmark. 3.3.4. Question filtering time. To promote robustness beyond single context, we attach up to three variants per target sound that differ in location, background conditions, or device/version (e.g., crowded vs. quiet carriage, different terminal stations). During scouting, segments can be up to 30 to facilitate selection. Each candidate must satisfy continuity (no abrupt cuts or dropouts), audibility of the intended source, and acceptable perceptual SNR as judged by human listeners. Every sourced clip is reviewed by checker distinct from its proposer. The checker verifies timing format and boundary accuracy, confirms that the target sound dominates the excerpt, and performs semantic-leakage screen to ensure the eventual item cannot be solved by lexical content alone. Descriptions are edited for specificity so that culturally grounded identity is unambiguous. For example, explicitly naming the soundscape from specific metro line rather than simply metro chime. We collected 943 audios at this stage. 3.3.3. Question Generation Quality-controlled clips are converted into four-option MCQs using human-in-the-loop process. Gemini 2.5 Flash automatically drafts four question stems, answer options, and tentative categories from minimal clip descriptors to avoid text-only shortcuts. Editors refine wording for clarity, calibrate distractors toward plausible near-miss cultural confusions, delete unsuitable items, and diversify what each item probes so that the four questions attached to clip assess different facets (e.g., place, source object, activity, cultural practice) rather than repeating single skill. Each item is labeled as Single-hop when single acoustic cue suffices, or Multi-hop when the answer requires combining acoustic evidence with background knowledge. After editor revisions, we employ an automatic item filtering pipeline. strong ASR model (Whisper large v3 [31]) first transcribes any spoken content in the clip. Then, text-only LLM (LLaMA-3.1 8B [32]) attempts to answer the multiple-choice questions using only the transcript, without access to the audio. This procedure identifies items that can be solved through lexical cues alone. For each item, we sampled five responses and computed the models success rate. We then perform one-tailed t-test under the null hypothesis H0: the models success rate does not exceed random guessing (25%). If the null is rejected at < 0.05, the item is considered vulnerable to transcript-only shortcuts and is discarded. This ensures that the retained questions genuinely require acoustic or cultural understanding rather than textual leakage. 3.4. Dataset Statistic Fig. 3 and 4 summarize the scale and composition of TAU. It contains 702 audio clips across 10 culturally distinctive categories. This diversity ensures that the dataset covers both highly frequent urban soundmarks (e.g., transit chimes, store jingles) and less common but socially important cues (e.g., emergency alarms, religious chants). The category distribution is intentionally imbalanced to reflect the natural frequency of sounds in everyday Taiwanese soundscapes, rather than enforcing artificial uniformity. Each clip is paired with up to four MCQs, resulting in 1794 evaluation items in total. The median clip length is 9.43 seconds, with maximum of 30 seconds by design. This design balances realism with usability, allowing evaluation without excessive cognitive load. On average, each soundmark has 2.1 recording variants that differ by location, time, or background conditions, which increases robustness and reduces overfitting to specific contexts. Table 1. Model performance on TAU benchmark using default system prompt, separated by Single-hop and Multi-hop items (Accuracy %). Parameter counts (Params) are reported in billions. Best results in bold. Category Model Params Single-hop Multi-hop Random Random LALM Gemini 2.5 Pro Gemini 2.5 Flash Qwen2-Audio-Instruct Qwen2.5-Omni-7B DeSTA2.5-Audio Gemma-3n-E2B-it Gemma-3n-E4B-it ASR+LLM LLaMA-3.1 LLM only Qwen2.5-7B-Instruct LLaMA-3.1 Human Topline 8.2B 7.6B 8.8B 4.4B 6.8B 9.6B 7.0B 8.0B 25. 72.4 61.3 30.3 46.4 43.3 29.6 29.0 34.9 38.5 37.6 84.0 25.0 73.9 63.2 27.8 46.1 41.7 25.8 25. 34.1 35.5 41.4 83.3 Table 2. Model performance on TAU benchmark using culturally grounded prompt (Accuracy %). Best results in bold. Category Model Single-hop Multi-hop Random Random LALM Gemini 2.5 Pro Gemini 2.5 Flash Qwen2-Audio-Instruct Qwen2.5-Omni-7B DeSTA2.5-Audio Gemma-3n-E2B-it Gemma-3n-E4B-it ASR+LLM LLaMA-3. LLM only Qwen2.5-7B-Instruct LLaMA-3.1 25.0 70.6 62.8 29.0 43.6 38.2 29.7 34.0 34.7 38.4 37. 25.0 71.8 62.2 27.1 42.3 38.9 29.4 33.4 31.8 34.3 35.8 4. EXPERIMENTS 4.1. Evaluation protocol We evaluate model performance on TAU using several LALMs. Each model is tested under two system prompts: (i) its default prompt, and (ii) culturally grounded prompt, You are Taiwanese person. Always respond with the perspective, cultural background, and knowledge of someone from Taiwan. We parse the output of all models to 4 options via Gemini-2.0 flash. For comparison, we include several baselines: Random, which selects an answer uniformly at random; ASR+LLM, which feeds Whisper-large-v3 transcriptions of the clips to text-only LLM; and LLM only, which asks the same questions directly without any audio or transcript input. Finally, we report Human topline: nine annotators each answer the benchmark, with every question evaluated by two independent annotators. 4.2. Performance with Default System Prompt Table 1 presents results with each models default prompt. Overall, humans remain far ahead of all systems, achieving 84.0% on Single-hop and 83.3% on Multi-hop questions. Among LALMs, Gemini 2.5 Pro clearly leads (72.4% / 73.9%), followed by Gemini 2.5 Flash (61.3% / 63.2%). Other audiolanguage models such as Qwen2.5-Omni-7B, DeSTA2.5-Audio, and Qwen2-Audio-Instruct show moderate performance (3046%), while lightweight models such as Gemma-3n variants struggle to surpass random baselines by large margin. Interestingly, text-only baselines reveal limitations of transcript reasoning. The ASR+LLM setup with LLaMA-3.1 achieves 34.9% / 34.1%, and LLM only baselines (Qwen-2.5, LLaMA-3.1) stay around 3541%. This confirms that most items cannot be solved by lexical information alone, validating the design of TAU. We also observe that Multi-hop accuracy lags behind Single-hop across weaker systems, suggesting that models find it harder to integrate cultural background knowledge with acoustic evidence. 4.3. Performance with Local Specific Prompt We next examine results under the culturally grounded prompt. Table 2 shows that this prompt does not universally improve performance, but produces nuanced shifts. For high-performing models such as Gemini 2.5 Pro, accuracy slightly decreases compared to the default setting (70.6% / 71.8% vs. 72.4% / 73.9%), suggesting that localization cues may not help frontier systems already capable of strong acoustic reasoning. By contrast, Gemma-3n variants benefit more noticeably, improving from 29% to 3334% on Multi-hop items, narrowing the gap with midtier LALMs. Some models, such as DeSTA2.5-Audio and Qwen2.5Omni-7B, show mixed changes, indicating that cultural priming can alter reasoning strategies but does not consistently enhance accuracy. Overall, the culturally specific prompt highlights that prompting alone is insufficient for closing the gap between models and humans. Nevertheless, its selective gains for certain models suggest pathway for aligning LALMs with localized knowledge, especially for weaker or instruction-tuned models. Future work could explore integrating explicit cultural grounding into training rather than relying solely on prompt engineering. 5. LIMITATIONS First, TAU is intentionally centered on Taiwan-specific soundmarks. Strong performance on TAU does not guarantee competence in other locales; weak performance may reflect cultural unfamiliarity rather than general auditory shortcomings. Secondly, our pool of sounds, venues, devices, and times of day is finite. Urban scenes may be overrepresented relative to rural contexts, and some cultural practices may be undersampled. Variation in devices and microphones also introduces covariate shifts that are not fully controlled. Lastly, soundmarks change over time (e.g., updated transit chimes or jingles). Models evaluated today may face distribution shift as the physical environment evolves. We would adopt versioned benchmark philosophy, release snapshots with explicit collection dates, and encourage lightweight incremental updates rather than one-off frozen datasets. 6. CONCLUSION We presented pathway for constructing localized audio benchmarks and instantiated it with culturally grounded, non-semantic evaluation suite. Our five-stage pipeline, covering concept curation, licensed sourcing, quality control, LLM-assisted item generation, and leakage filtering, offers reproducible template that others can adapt to different regions and communities. Empirically, current audiolanguage models fall well short of human performance on localized items, and culturally specific prompt provides, at best, selective gains, indicating that prompt engineering alone is insufficient. These findings underscore the need for culturally informed data, tasks, and metrics to assess real-world robustness, mitigate region-dependent failure modes, and support equitable deployment of multimodal systems. 7. REFERENCES [1] Jort F. Gemmeke et al., Audio set: An ontology and humanin 2017 IEEE Internalabeled dataset for audio events, tional Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017. [2] Eduardo Fonseca et al., Fsd50k: An open dataset of humanIEEE/ACM Transactions on Audio, labeled sound events, Speech, and Language Processing, 2022. [3] Ke-Han Lu et al., Desta2. 5-audio: Toward general-purpose large audio language model with self-generated cross-modal alignment, arXiv preprint arXiv:2507.02768, 2025. [4] Yunfei Chu et al., Qwen2-audio technical report, arXiv preprint arXiv:2407.10759, 2024. [5] Ke-Han Lu et al., Developing instruction-following speech language model without speech instruction-tuning data, in ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. [6] Chih-Kai Yang et al., spoken language model: first attempt, arXiv:2411.07111, 2024. Building taiwanese mandarin arXiv preprint [7] Chien-yu Huang et al., Dynamic-superb: Towards dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. [8] Chien-yu Huang et al., Dynamic-SUPERB phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks, in The Thirteenth International Conference on Learning Representations, 2025. [9] Qian Yang et al., AIR-bench: Benchmarking large audiolanguage models via generative comprehension, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [10] Sakshi et al., MMAU: massive multi-task audio understanding and reasoning benchmark, in The Thirteenth International Conference on Learning Representations, 2025. [11] Margret Engel and Andre Fiebig, Detection and classification of soundmarks ans special features in urban areas, in 24th International Congress on Acoustics ICA 2022, 2022. [12] Yi-Cheng Lin et al., Emo-bias: Large Scale Evaluation of Social Bias on Speech Emotion Recognition, in Interspeech 2024, 2024. [13] Yi-Cheng Lin et al., Emo-debias: Benchmarking gender debiasing techniques in multi-label speech emotion recognition, arXiv preprint arXiv:2506.04652, 2025. [14] Yi-Cheng Lin et al., Listen and speak fairly: study on semantic gender bias in speech integrated large language models, in 2024 IEEE Spoken Language Technology Workshop (SLT), 2024. [15] Zhifei Xie et al., Audio-reasoner: Improving reasoning caarXiv preprint pability in large audio language models, arXiv:2503.02318, 2025. [16] Gemma Team et al., Gemma 3 technical report, arXiv preprint arXiv:2503.19786, 2025. [17] Junyi Ao et al., SD-eval: benchmark dataset for spoken dialogue understanding beyond words, in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [18] Kuofeng Gao et al., Benchmarking open-ended audio dialogue understanding for large audio-language models, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. [19] Yiming Chen et al., Voicebench: Benchmarking llm-based voice assistants, arXiv preprint arXiv:2410.17196, 2024. [20] Bin Wang et al., AudioBench: universal benchmark for audio large language models, in Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025. [21] Soham Deshmukh et al., Audio entailment: Assessing deductive reasoning for audio understanding, in Proceedings of the AAAI Conference on Artificial Intelligence, 2025. [22] Benno Weck et al., Muchomusic: Evaluating music understanding in multimodal audio-language models, in Proceedings of the 25th International Society for Music Information Retrieval Conference (ISMIR), 2024. [23] Chih-Kai Yang et al., SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information, in Interspeech 2025, 2025. [24] Yu-Xiang Luo et al., Toxictone: mandarin audio dataset annotated for toxicity and toxic utterance tonality, arXiv preprint arXiv:2505.15773, 2025. [25] Ke-Han Lu et al., Speech-IFEval: Evaluating instructionfollowing and quantifying catastrophic forgetting in speechaware language models, in Interspeech 2025, 2025. [26] Junho Myung et al., BLEnd: benchmark for LLMs on everyday knowledge in diverse cultures and languages, in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [27] Yu Ying Chiu et al., CulturalBench: robust, diverse and challenging benchmark for measuring LMs cultural knowledge through human-AI red-teaming, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. [28] Dahyun Kim et al., Representing the under-represented: Cultural and core capability benchmarks for developing Thai large language models, in Proceedings of the 31st International Conference on Computational Linguistics, 2025. [29] Hsin-Yi Hsieh et al., TaiwanVQA: benchmark for visual question answering for Taiwanese daily life, in Proceedings of the First Workshop of Evaluation of Multi-Modal Generation, 2025. [30] Zhi Rui Tam et al., Vistw: Benchmarking vision-language arXiv preprint models for traditional chinese in taiwan, arXiv:2503.10427, 2025. [31] Alec Radford et al., Robust speech recognition via large-scale weak supervision, in International conference on machine learning. PMLR, 2023. [32] Aaron Grattafiori et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024."
        }
    ],
    "affiliations": [
        "National Taiwan University",
        "University of Toronto"
    ]
}