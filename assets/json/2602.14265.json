{
    "paper_title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts",
    "authors": [
        "Zachary Bamberger",
        "Till R. Saenger",
        "Gilad Morad",
        "Ofra Amir",
        "Brandon M. Stewart",
        "Amir Feder"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 5 6 2 4 1 . 2 0 6 2 : r STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts Zachary Bamberger1 Till R. Saenger2 Gilad Morad3 Ofra Amir1 Brandon M. Stewart2 Amir Feder 1Technion Israel Institute of Technology 2Princeton University 3Independent 4Hebrew University of Jerusalem Equal contribution February 17, Abstract Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of hightemperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: controller selects actions encoding high-level reasoning choices; generator produces reasoning steps conditioned on those choices; and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in case study on argument generation, STATes explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at state-of-thoughts."
        },
        {
            "title": "Introduction",
            "content": "Many applications of LLMs require more than generating high-quality responses: they need systematic and interpretable control over how text is produced [42, 35, 87]. For example, in subjective tasks like persuasive writing, researchers vary the rhetorical structure and content themes of arguments to study the features that drive belief change [102, 28, 29, 87, 88, 43, 20]. Similarly, in creative writing, researchers are concerned with generating diverse yet high-quality outputs that satisfy the preferences of the audience [26, 64, 115, 122]. In both settings, the challenge is to produce text that varies systematically along dimensions of interest while maintaining coherence and quality. ITC methods address part of this challenge by allocating additional compute for LLM reasoning [114, 60, 75, 22, 81] and for producing multiple candidate responses [11, 99, 112, 16, 103]. Tree-based methods [6, 46, 45] like Tree of Thoughts (ToT by Yao et al. [117]) further enhance quality by branching on intermediate thoughts and pruning less-promising reasoning trajectories. However, The work described in this manuscript is subject to pending patent application. 1 Figure 1: STATe for argument generation. Tasked with generating persuasive arguments in favor of banning single-use plastics, STATes workflow involves the following steps: (1) Define action templates that control output features of interest, such as structural prefixes and content themes. (2) Generate outputs via tree search (Grey nodes indicate pruned branches; the rightmost path illustrates early stopping after single step). (3) Evaluate outputs on downstream metric, and study associations between action choices and performance. these methods rely primarily on temperature-based sampling for diversity, which yields limited meaningful variation [122, 53]. Moreover, since ITC methods sample at the token-level, decisions about what to say and how to say it remain implicit in the decoding process [72]. As result, they provide limited control over which high-level decisions are explored and limited insight into which decision patterns drive success or failure. To induce interpretable yet diverse sampling, we prepend diverse prefixes to each LLM completion. Specifically, we define discrete action templates that encode high-level reasoning choices (such as which rhetorical structure to employ, which content theme to develop, or which writing operation to perform). We use intervention-based sampling to build STATe-of-Thoughts (STATe), an inference-time compute framework that searches over sequences of high-level reasoning actions. STATes controller selects which actions to explore at each reasoning step, and then its generator produces reasoning steps conditioned on the selected actions.1 An evaluator scores both intermediate and final states to guide beam search [6, 46]. We illustrate STATes complete three-step workflow in Figure 1 through the lens of argument generation. We compare STATe to existing ITC methods in both creative writing and argument generation. On NoveltyBench [122] (Section 4.1), we found that STATe produces up to twice as many semantically distinct outputs as the best baseline across seven LLMs from three model families (Qwen3 [116], Nemotron-3 [74], and Ministral-3 [67], Section 5.1). In our case study on argument generation (Section 4.2), we found that sequential action features are highly predictive of argument quality on held-out data (Section 5.2.1), and that model-guided trajectory selection allows for generating 1Unlike latent interventions [3, 32, 4], interventions in STATe are explicit text prefixes and thus directly auditable. 2 high-quality outputs from promising yet unexplored regions of the action space (Section 5.2.2). Our contributions, therefore, are the following: 1. controllable and interpretable ITC framework based on explicit action-space search. 2. mechanism for diverse generation which outperforms high-temperature sampling. 3. An attribution framework that uses action traces to predict output quality, identify promising unexplored regions of the action space, and steer generation toward them."
        },
        {
            "title": "2 Background",
            "content": "2.1 Inference-Time Compute The conventional approach to using LLMs involves direct input-to-output generation, where model maps an input sequence directly to an output sequence y: GI/O(x) y. While effective for many tasks, this approach exhibits limited robustness to common failure modes such as hallucinations [94, 78, 95], sycophancy [91], and other biases [52, 77]. These limitations are compounded in tasks that require complex reasoning or multi-step solutions [114, 97]. Building on the intuition that human reasoning benefits from more time to think [54], Inference-Time-Compute methods provide LLMs with additional reasoning tokens (reasoning depth [97, 75, 81, 73]2), and permit parallel reasoning attempts (reasoning breadth [11, 99, 112, 16, 103]). Chain-of-Thought (CoT) reasoning [114, 60] scales depth by enabling models to generate intermediate reasoning steps before arriving at final answer. Formally, we define CoT as GCoT (x) Z, y, where represents the chain of reasoning steps and the final answer. CoT enhances performance on complex reasoning tasks [97, 22, 75], and models specifically optimized for reasoning demonstrate even greater improvements [75, 22, 63]. However, standard CoT implementations remain brittle: errors can propagate through the reasoning chain, and there is no principled mechanism to revisit decisions or explore alternative strategies. Rather than scaling depth, Best-of-n methods [11, 99] scale breadth by generating independent candidate outputs and selecting the best according to some scoring criterion. This enhances robustness by reducing the impact of individual generation failures.3 Self-Consistency [112, 16, 103] addresses CoTs brittleness by sampling multiple CoT reasoning paths and selecting the most consistent answer through majority voting. When sampling more than one completion from an LLM, we refer to this operation as branching [117]. Formally, we express branching as single operation that produces multiple complete reasoning chains with their associated answers: GCoT (x; n, temp) {(Z1, y1), . . . , (Zn, yn)}, where each Zj represents complete reasoning chain and yj is its corresponding final answer. Best-of-N methods, typically branch only at the initial reasoning step, affording limited control over which intermediate choices are explored. Moreover, inducing diversity across branches through high-temperature sampling often yields homogeneous outputs or degrades quality [72, 53, 122]."
        },
        {
            "title": "2.2 Tree of Thoughts",
            "content": "Tree of Thoughts (ToT) [117] combines both strategies: scaling depth through multi-step reasoning (improving reasoning quality) and scaling breadth through branching at intermediate points (enhancing robustness). ToT reframes generation as search over tree of partial reasoning steps, enabling branching at multiple intermediate points, evaluation of progress, and pruning of unpromising paths. 2Reasoning effort for LLMs manifests as more output tokens [81]. 3Generation failures include LLM refusals, exceeding the context limit, or parsing errors over LLM outputs. 3 Each node represents state si := [x, z1, . . . , zi] = [x, Zi],4 capturing partial solution with the input and reasoning steps so far. leaf node sd+1 represents complete solution [x, Zd, y] where is the final answer and is the predefined maximum reasoning depth. Formally, when sampling thoughts at step with language model pθ: pθ(zi) := pθ(zi x, Zi1; n, temp), {z , . . . , zn } pθ(zi). where Zi1 = [z1, . . . , zi1] represents the reasoning steps so far. ToT provides enhanced robustness through two key mechanisms. First, intermediate evaluation and pruning identify and eliminate unproductive branches early, preventing error propagation. Second, systematic exploration of different trajectories increases the likelihood of discovering valid solutions even when some branches fail.5 To evaluate intermediate and final reasoning steps, ToT methods use LLM-as-a-Judge [123, 65]. Process Reward Models (PRMs) [117, 66, 110], which score intermediate reasoning, are defined as (Zi x) [0, 1] where d. Outcome Reward Models (ORMs) [123, 57, 58], are defined as (y x) [0, 1]. Traditional ToT implementations face two primary limitations. First, despite sampling at high temperatures to promote diversity, branches tend to cluster around similar content with only minor variations [117, 45, 53, 122]. Second, ToT implementations lack adaptive termination mechanisms, executing for predetermined number of steps regardless of reasoning progress, leading either to overthinking after convergence [73, 51] or to premature termination."
        },
        {
            "title": "3 Methods",
            "content": "STATe extends ToT [117, 6, 46] with three methodological contributions that improve both output diversity and controllability while retaining quality. First, STATe replaces stochastic temperature sampling with discrete action templates that diversify branches in tree search. This allows each branch to explore fundamentally different reasoning strategies from its neighbors. Second, STATe supports both verifiable [63] and task-specific LLM-as-a-Judge evaluators to reliably score and select among diverse candidates. Combining both diverse generation and reliable evaluator (for rejection sampling) helps mitigate the quality-diversity trade-off [72]. Third, STATe tracks actions along trajectory, which enables systematic analysis of which textual features and reasoning patterns drive performance. This kind of analysis allows researchers to study associations between controllable, concept-level interventions and downstream outcomes [41, 1]."
        },
        {
            "title": "3.1 Tree of Thoughts Components",
            "content": "STATe instantiates ToT as modular Plan Generate Evaluate Select loop (Figure 2). At each layer i, STATe starts with list of states, each of the form si = [x, Zi]. The controller selects interventions for each state in the frontier. The generator then produces completions that extend each of these interventions. Finally, the evaluator scores the resulting trajectories, and retains the top-k states for the next layer (beam selection). We present the full process in Algorithm 1. 4For ease of notation, we denote the reasoning steps z1, . . . , zi by Zi, but treat si as flat vector of inputs (x), reasoning steps (z1, . . . , zi), and optionally final outputs (y), not nested vector. 5For example, the model may fail to create valid generation due to refusal, exceeding the context limit, or failing to adhere to structured output schema. Figure 2: Stylized example of STATes PlanGenerateEvaluateSelect loop. The controller plans which actions to explore, the generator expands candidate trajectories, the evaluator scores them, and beam selection retains the top-k states. 3.1.1 Controller We treat each action as tool call. Selecting an action corresponds to choosing tool name from fixed set of action templates (Appendix D) and providing values for the tools arguments (if any). We discuss how to design action templates for new problems in Appendix D.3. Each argument in tool has finite domain, which naturally encodes structured action-space dimensions. In our argument generation example (Figure 2), the actions encode structure and content dimensions, per Wachsmuth et al. [109]. Executing the tool returns structured intervention, i.e., prefix and internal reasoning guidance,6 that is injected into the next generation step (see Table 1). This mirrors the iterative tool-use paradigm in ReAct [118], with two key differences: (i) we allow branching by selecting multiple tools per input state, and (ii) our tools are lightweight prefix interventions rather than external capabilities such as retrieval or code execution. Given parent state si1 = [x, Zi1] representing the input and reasoning so far, the controller must choose up to actions from the action space to explore in parallel branches.7 Formally, we define the controller output as: Implicitly, the controller implements scoring function Q(si1, ai) that estimates the value of taking action ai from state si1, so we can formalize action selection as: {a1 , . . . , an } = C(si1, A, n) (1) {a1 , . . . , an } = arg max AA,A=n (cid:88) aiA Q(si1, ai) (2) We implement two kinds of controllers: one that uses generative LLM to produce tool calls (Appendix B.1.1), and another that uses reranker LLM [121] to pick among all possible tool 6When an action combines multiple dimensions (e.g., both structure choice and content choice), only one dimension can introduce prefix, since the prefix sets the beginning of the next reasoning step. However, all dimensions can contribute to the internal reasoning guidance (by concatenating claims like should ... or will ... ). 7Branches from parent nodes lead to children nodes as described in Algorithm 1. 5 calls (Appendix B.1.2). Additionally, we introduce early stopping. If the controller determines that reasoning is sufficient, it can select dedicated FINISH action, allowing the state to proceed directly to final answer generation. This is represented in our controller as an additional tool that takes no arguments. If selected, we pre-fill the next generation with delimiters signifying the end of the reasoning process and the beginning of the answer generation process. This mechanism helps prevent overthinking where additional steps become degenerate after the model has effectively converged [69, 85, 100, 51, 73]. 3.1.2 Generator , . . . , an Once the controller selects the next actions {a1 } for given parent state si1, we generate reasoning steps for its children conditioned on these interventions. For each action aj , . . . , an }, we execute the corresponding tool to obtain text guidance aj (), typically consisting of internal reasoning to guide the next step and prefix fixing the beginning of the next step. We append these to the LLMs assistant message as prefill (see Table 1 for examples). Formally, given the parent state si1 = [x, Zi1], where Zi1 = [z1, . . . , zi1] represents the reasoning steps so far, we sample continuation zj {a1 for each action aj as: zj pθ (cid:0)z x, prefill(Zi1, aj ()); temp(cid:1) [:stop_token] (3) The prefill operation ensures that the models generation begins with the intervention text, biasing the direction of reasoning along the desired dimension. Each generated thought zj is combined with the current state to create child state sj ] that can be expanded further. We determine when to stop generation through stop tokens: </step> if generating reasoning step, and </answer> if generating final answer. = [si1, zj Once the maximum reasoning depth is reached or the controller selects the FINISH action, STATe reaches the synthesis step that transforms these traces into cohesive final outputs. This step conditions on the input and the complete reasoning trace Zi1, generating final output yj that integrates the intermediate claims into coherent whole: yj pθ(y x, prefill(Zi1); temp) [:stop_token] (4) We define four synthesis modes that vary in how strictly the output preserves the intermediate reasoning. Strict synthesis concatenates reasoning steps verbatim with minimal connectives. Faithful synthesis permits rephrasing while preserving order and structure. Restructured synthesis allows free reorganization using the trace as source material. Conclusion synthesis treats the trace as internal guidance only, with no constraints on the final output. The choice of synthesis mode reflects trade-off between action attribution and output quality; full specifications appear in Appendix E.3. 3.1.3 Evaluator After generating child states from each parent, we evaluate their quality using either score-based LLM-as-a-Judge models [123, 57, 58, 13, 68], or verifiable rewards [63, 39, 104]. We employ two types of evaluators corresponding to the two types of states in our search tree. For intermediate states si where d, we use Process Reward Model (PRM) defined as VP RM (si) := (Zix) [0, 1]. The PRM scores the reasoning chain based on its current coherence (backward compatibility) and projected final answer quality (forward compatibility), enabling early pruning of unpromising paths. On the other hand, for complete solution states si = [x, Zi1, y] (where d) we use an Outcome Reward Model (ORM) defined as VORM (si) := (yx) [0, 1]. 6 First step Intermediate step Final step <thinking> <step> ## internal_reasoning should identify risks, unintended outcomes, cascading effects, and potential for escalation. ## claim If current levels of plastic waste continue, they will cause permanent harm to marine ecosystems... <thinking> <step> ## internal_reasoning should identify risks... ## claim If current levels of plastic waste continue... </step> ... <step> ## internal_reasoning should evaluate historical precedents, long-term vs short-term tradeoffs, and obligations to future generations. ## claim For example, Canadas existing single-use plastic bans are expected to reduce total waste by 5%... <thinking> <step> ## internal_reasoning should identify risks... ## claim If current levels of plastic waste continue... </step> ... <step> ## internal_reasoning should evaluate... ## claim For example, Canadas existing bans... </step> ... </thinking> <answer> Table 1: Illustrative interventions for argument generation example in favor of single-use plastics ban. Templates are in black, internal reasoning in teal, prefixes in blue, and the model continuation in orange. Each column shows the generation state at different stages: first step (single claim), intermediate (multiple claims), and final (complete reasoning with answer delimiter). The action space used here is presented in Appendix D.2. This scores the quality of the final answer, considering task-specific criteria such as instruction adherence, persuasiveness, coherence, and stylistic appropriateness. We support three evaluator implementations: generative LLM-as-a-Judge which scores candidates against rubric (Appendix B.2.1), re-ranker [121] LLM-as-a-Judge, which assigns latent relevance scores for candidates (Appendix B.2.2), and deterministic verifier [63] (Appendix B.2.3)."
        },
        {
            "title": "3.2 Beam Search Algorithm",
            "content": "We present the complete STATe-of-Thoughts beam search in Algorithm 1.8 The algorithm initializes with the input and iteratively expands states through controller-guided interventions. The algorithm terminates when all states have produced final answers or their trajectory has been pruned, at which point the highest-scoring final states according to the ORM are returned. Each of STATes components9, the Generator (G), Controller (C), PRM (VP RM ), and ORM (VORM ), include dedicated prompts, which we present in Appendix E. Notably, the synthesis type used to generate final outputs is included in the system prompt of (Appendix E.2). 8Algorithm 1 is simplified relative to our implementation. In practice, we parallelize LLM calls from the Controller, Generator, and Evaluator across all nodes in layer with vLLM [61]. 9STATes components correspond to DSPy [56] Modules, which convert Signatures (input/output and instruction specifications) and inputs to prompts (via Adapters), call the LLM, and parse the response according to the signature. 7 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: Algorithm 1 STATe-Beam-Search(x, G, C, VPRM, VORM, A, n, k, d, temp) Require: Problem input x, generator G, controller C, process evaluator VPRM, outcome evaluator VORM, action space A, branching factor n, beam width k, maximum depth d, temperature temp Initial layer with just the input Collection of final states with answers 1: Initialize L0 {x} 2: Initialize 3: for = 1 to + 1 do 4: for each state si1 Li1 do Candidate states for layer Final layer: force termination Controller selects up to actions if = + 1 then {a } {FINISH} else , . . . , an {a1 end if for each action aj do if aj } C(si1, A, n) is FINISH then yj G(si1, prefill(Zi1, aj si [si1, yj] {si} else zj G(si1, prefill(Zi1, aj si [si1, zj ] L {si} end if ()); temp)[:stop_token] Generate response Create final state Add to collection of final states ()); temp)[:stop_token] Generate thought Append thought to existing state Add to next layers candidates end for end for if = then break All branches terminated via early stopping end if Score all candidates: vsi VPRM(si) for all si Li arg maxLL 27: 28: end for 29: Score all final states: vs VORM(s) for all 30: return arg maxsF vs i,L=min(k,L siL vsi (cid:80) i) Select top-k states for layer Return highest-scoring final state"
        },
        {
            "title": "3.3 Attributing Outcomes to Controller Actions",
            "content": "A key advantage of STATe-of-Thoughts is its ability to attribute differences in outcomes to specific controller actions. As each branch in the reasoning tree is associated with logged sequence of actions, we can analyze which actions lead to higher-quality outputs. However, estimating the causal effect of action choices on output quality is complicated by the sequential nature of decision-making. Actions are selected and executed conditional on prior actions of given trace, confounding simple correlation-based analysis. Hence, for now, we focus on association rather than causation, aiming to identify action patterns that consistently correlate with better or worse outcomes. 8 3.3.1 Problem Formulation Let τ = (a1, a2, . . . , an, y) denote complete action trace leading to the final output y. We are interested in predicting final output quality from these action trajectories. One central question this allows us to explore is whether the sequential structure of actions matters beyond their mere presence. If ordering is unimportant, simple bag-of-actions representation suffices. This is analogous to what topic model could capture about argument composition [87]. However, if the timing of an action affects outcomes (e.g., starting with concession versus ending with one), then sequential features that encode positions and transitions should explain additional variance in and allow for better predictions. 3.3.2 Presence-Based Model (Bag-of-Actions) The presence-based model represents each trajectory using binary indicators for whether each action type appeared anywhere in τ : 1a(τ ) = (cid:40) 1 if action type appears in τ 0 otherwise (5) This produces feature vector xpresence {0, 1}A1 that ignores the order and frequency of actions. We fit linear model: Yi = α + xpresence β + ϵi (6) The coefficient β captures the average difference in outcome quality when action is present versus absent, relative to the reference category and controlling for other actions. This kind of model serves as baseline representing what could be learned from action composition alone, without information on action order. 3.3.3 Sequential Model The sequential model extends beyond the presence-based representation with features that capture the sequential ordering of actions. We consider two core feature types: Position Features For each action a, we create indicators for occurring at each reasoning step position: (cid:40) 1a,k(τ ) = 1 if action occurs at step 0 otherwise (7) This captures whether an actions effect depends on when it occurs, for instance, whether deploying action early versus late in the trajectory produces different outcomes. Transition Features We create indicators for action sequences between consecutive steps: 1kk+1 aa (τ ) = 1a,k(τ ) 1a,k+1(τ ) (8) This captures sequential patterns: the extent to which action followed by action affects outcomes. In principle, longer n-gram transitions or other complicated interactions could be included, but we focus on bi-grams for tractability. 9 The sequential model combines these feature types in xsequential such that: Yi = xsequential β + ϵi (9) When the action space contains multiple dimensions (e.g., both what to say and how to say it), additional feature types arise, including cross-dimension interactions at each step. We describe our specific experiment instantiation in Section 4.2. 3.3.4 Model Comparison via Regularized Regression The sequential model contains substantially more features than the presence-based model, creating risk of overfitting and numeric instability. Moreover, because the controller and evaluator may focus on subset of the action space, some features or feature combinations may be irrelevant. We address these issues using LASSO regression [106], which performs automatic feature selection by driving irrelevant coefficients to zero: ˆβ = argminβ (cid:40) 1 2N (cid:88) i=1 (Yi xsequential β)2 + αβ1 (cid:41) (10) This allows us to identify relevant action patterns while controlling model complexity. In practice, one might want to design broad action spaces that capture many possible strategies, then explore how these actions affect perceived argument quality for different target audiences. When it is unclear priori which actions or sequences will be associated with differences in outcomes, this approach allows data-driven selection of the most relevant patterns."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate STATe in two complementary settings that probe its capacity for diversity, interpretability, and controllability. First, we compare STATe to existing ITC methods on NoveltyBench [122] to test whether structured interventions can induce semantic diversity without degrading quality (Section 4.1). Next, in our Argument Quality experiment, we test whether STATes interpretable action sequences provide sufficient signal to predict the quality of persuasive arguments. We further evaluate whether learned associations can guide controllable targeted generation toward promising regions of the action space (Section 4.2)."
        },
        {
            "title": "4.1 Generating Diverse Responses",
            "content": "NoveltyBench [122] measures whether methods can increase diversity without collapsing quality. The curated set includes 100 prompts across four categories: randomness (e.g., Roll make-believe 20-sided die), factual knowledge with underspecified queries (e.g., List capital city in Africa), creative writing (e.g., Write short story about time traveler), and subjectivity (e.g., Whats the best car to get in 2023?). For each prompt, models generate 10 responses, which are then partitioned into functional equivalence classes using fine-tuned DeBERTa-v3-large classifier [47] trained on 1, 000 human-annotated pairs. Two generations are considered functionally equivalent if user who has seen one would gain little additional value from seeing the other. Mean Distinct counts the number of unique equivalence classes across the 10 generations, quantifying semantic diversity.10 We compare the diversity of outputs from best-of-n (with I/O and CoT prompting) against ToT [6, 46] and STATe, using open-source models from three families: Qwen3 [116], Nemotron-3 10NoveltyBench also includes Mean Utility metric, which we include in Appendix C.1. 10 [74], and Ministral-3 [67]. When using STATe, we utilize Qwen3-8B-Reranker [121] for both Re-Ranker Controller (Appendix B.1.2) and Re-Ranker Evaluator (Appendix B.2.2).11 We use depth-1 trees with wide branching (n = 10, = 10), which produces single reasoning step and then final answer in each candidate trajectory. Each configuration is repeated across 10 random seeds to measure variance from sampling randomness. We also include baselines that explicitly expose action spaces in prompts (as input fields of DSPy signatures [56]) to isolate the benefit of controller-guided interventions from simply providing additional task structure. As NoveltyBench includes only single test split, we perform initial experiments on train subset of 10 examples to refine our system and its prompts and report results on the remaining 90 examples. Notably, this study examines the diversity of branching operations as function of (1) whether we use ToT template, and (2) whether we include the action space as an input, and (3) whether the controller interventions guide generations. Diversity as function of tree depth is outside the scope of this experiment. STATes internal evaluator does not affect search or final candidate selection in this configuration (all 10 outputs are returned). The action space we use here combines two dimensions with 5 choices each, yielding 25 possible action combinations per step (Appendix D.1). The first dimension, personality traits, follows the Big Five model [40]. The second dimension, target audience, specifies the demographic age to appeal to. Each action provides internal reasoning guidance (Section 3.1.2) that steers generation toward the specified persona or audience. We sweep three temperature regimes per model: low, medium, and high."
        },
        {
            "title": "4.2 Argument Quality",
            "content": "We also evaluate STATe on argument generation, using the single-use plastic ban proposition introduced in Figures 1 and 2. This setting provides convenient testing ground for analyzing how different action choices affect GPT-5-minis [96] perception of an arguments quality. Argument generation is particularly well-suited for attribution analysis because each action (selecting what claim to develop or how to structure it) can manifest directly and sequentially in the final output. The resulting argument generally comprises the generated parts in the order they were produced, creating clear association from controller decisions to output features. We use strict synthesis, faithful synthesis, and restructured synthesis (Section 3.1.2) in this experiment. Following Wachsmuth et al. [108], we operationalize content interventions as stock-topics to discuss, and structure interventions as discourse-relations to use [82, 113]. We define ten dimensions each for the content and structure action spaces, giving us total of 100 action choices at each branching point. The full action space templates are provided in Appendix D.2. We use STATe with Qwen3-30B-A3B-Instruct to generate 5,000 arguments in favor of banning single-use plastic per synthesis method. Specifically, we use STATe trees with depth 3 and beam width 250, yielding 250 unique trajectories and corresponding final arguments per tree. We generate 20 such trees initialized with different random seeds to achieve multiple, different realizations of some overlapping trajectories, rather than single tree with very large beam width. To measure argument quality, we prompt our LLM-judge to evaluate 50,000 random pairwise comparisons per argument collection, indicating which argument it finds more effective. We then fit Bradley-Terry (BT) model [9] to these judgments and use the resulting standardized ranks as the outcome variable in our regression analyses. We split the dataset into training (60%) and test (40%) sets for model fitting and evaluation. We 11In our experiments, STATe uses 2 GPUs, since we load the generative model on one, and the reranker model on the other. However, when STATe uses generative evaluator, it requires only single GPU. 12T {0.5, 0.7, 1.0} for most models; {0.1, 0.3, 0.5} for Ministral-3, which Mistral recommends using with lower temperatures. 11 instantiate the sequential model (M2) with two action dimensions: structure (discourse-relations) and content (topical focus). For this multi-dimensional setting, we consider three feature types: 1. Position per dimension features: whether particular structure or content choice at step affects quality. 2. Within-step interactions: whether specific structurecontent pairings at each position are particularly effective. 3. Within-dimension transitions: whether sequential patterns within each dimension (e.g., contrastive causal structure, or feasibility risk & consequences) affect outcomes. We restrict transitions to length two and to within-dimension pairs for interpretability and to prevent extremely sparse features. For the presence-based models (M1ac), we use ordinary least squares regression. For the sequential model (M2), we use LASSO, with the regularization parameter α selected via line search and 10-fold cross-validation on the training set, to handle the larger and sparser feature space. Note that the features of M2 can linearly replicate all presence-based features used in M1ac; that is, M2 has access to strict superset of the M1 feature space. Bootstrap resampling (1,000 iterations) provides 95% confidence intervals for test-set R2 values. We report an ablation controlling for argument length in Appendix C.2.2."
        },
        {
            "title": "5.1 Structured Textual Interventions Improve Diversity",
            "content": "STATe consistently achieves the highest diversity (Mean Distinct) across all model families and temperature regimes. Our results, depicted in Table 2 (and more comprehensively in Appendix C.1, Table 4), demonstrate that action-guided interventions outperform temperature-based stochastic sampling for inducing semantic diversity. For Qwen3-30B-A3B-Instruct at the recommended temperature (T = 0.7), STATe produces 5.02 0.09 distinct generations out of 9, compared to 3.36 0.12 for the next-best method (Baseline CoT with Action Space) and just 2.44 0.07 for standard CoT. In Appendix C.1 we show that STATes wins on diversity generalize not only across model sizes, but also across model families. Notably, simply exposing the action space in prompts (I/O w/ Action Space and CoT w/ Action Space) provides moderate diversity improvements over vanilla baselines, confirming that explicit diversity dimensions help, but controller-guided selection is necessary to fully realize the benefit. Method T=0.5 T=0. T=1.0 U I/O CoT I/O w/ Action Space CoT w/ Action Space ToT ToT w/ Action Space STATe of Thoughts 1.48 0.03 2.15 0.07 1.74 0.05 3.07 0.11 2.51 0.09 3.16 0.08 4.87 0.1 2.28 0.03 2.65 0.07 2.23 0.05 3.15 0.06 2.87 0.05 3.29 0.05 3.77 0.12 1.67 0.04 2.44 0.07 2.01 0.05 3.36 0.12 2.81 0.08 3.25 0.08 5.02 0.09 2.45 0.05 2.85 0.07 2.41 0.07 3.28 0.1 3.07 0.06 3.36 0.07 3.83 0.11 2.01 0.05 2.8 0.08 2.44 0.07 3.74 0.09 3.16 0.12 3.61 0.1 5.39 0.11 2.64 0.04 3.07 0.06 2.71 0.04 3.49 0.11 3.32 0.08 3.61 0.11 4.01 0. Table 2: NoveltyBench diversity (D) and utility (U) for Qwen3-30B-A3B across ITC methods and temperatures (T). We report meanstd over 10 seeds. See Appendix C.1.2 for additional results. The diversity gains from STATe generally benefit the utility metric that NoveltyBench measures (Appendix C.1.1). Table 2 shows that at temperature = 0.7 Qwen3-30B, STATe achieves 3.830.11 12 Mean Utility versus 3.36 0.07 for ToT with Action Space, indicating that the additional diversity does not degrade quality. In Appendix C.1.2 we show that STATes wins in utility on Qwen3-30B generalize to 3 of 5 models as well.13 Despite the two outliers, the general trend is clear: STATe achieves superior diversityutility trade-offs across model families, temperatures, and model scales, validating the hypothesis that discrete action-space search enables controllable exploration without sacrificing output quality."
        },
        {
            "title": "5.2 Action Sequences Help Predict Argument Quality",
            "content": "5.2.1 Predictability We apply the attribution framework from Section 3.3 to the argument generation trajectories described in Section 4.2. Figure 3 compares the baselines based on presence (M1a-c) with the sequential model (M2) in three synthesis settings. Figure 3: Predictability of argument quality from controller actions. (A) Cross-validation R2 across model variants: M1a (structure presence only), M1b (content presence only), M1c (both), and M2 (full sequential model with position effects and transitions). (B) Held-out test set performance with 95% bootstrap confidence intervals. The sequential model consistently outperforms presence-based baselines, with the largest absolute predictability under strict synthesis. Across all synthesis modes, the sequential model (M2) substantially outperforms the presencebased baseline (M1a-c): gains range from +25% (faithful) to +88% (restructured) relative improvement in R2 on the test set. This demonstrates that the timing and order of actions matter beyond simply the presence of actions. In other words, the temporal structure of controller decisions carries predictive information about output quality. The full comparison, including confidence intervals and feature counts, is shown in Table 5 of Appendix C.2. Further, the results reveal clear predictability gradient across synthesis modes. Under strict synthesis, where the final argument should preserve exact wording from reasoning steps, the sequential model (M2) achieves R2 = 0.57 on test data; the controllers action choices strongly predict argument quality as judged by pairwise comparisons.14 Under faithful synthesis, which permits light rephrasing, predictability decreases to R2 = 0.38. Under restructured synthesis, which allows free reorganization, 13The two models where STATe didnt achieve the highest diversity were Qwen3-4B and Nemotron-3-30B, where Baseline CoT w/ Action Space outperformed STATe by low margin (as little as 0.01-0.12). See Appendix C.1.2 for additional discussion on this. 14We removed 6 of the 5000 strict synthesis arguments that were exact duplicates of other arguments in the dataset. 13 predictability is R2 = 0.37. The more faithful the argument is to the reasoning steps that precede it, the better the predictability based on action features. 5.2.2 Targeted Trajectory Exploration The attribution analysis establishes that M2s sequential features explain significant variance in argument quality. natural extension is testing whether these estimates generalize beyond the observed feature combinations. M2 was trained on 5,000 arguments per synthesis type, but the full trajectory space contains 1003 = 1,000,000 possible 3-step sequences, the vast majority of which If M2s learned coefficients generalize, we can use them to identify promising are unobserved. unexplored regions of the trajectory space and generate targeted arguments with those features. This corresponds to the final step of our proposed workflow in Figure 1: using attribution estimates to guide targeted generation. Using M2s fitted coefficients, we score all possible trajectories and rank them by predicted quality. We select the top 50 trajectories per synthesis type, all of which were never observed in the training data, and use STATes forced controller mechanism15 to generate arguments following each trajectory exactly. For each trajectory, we generate 5 samples, yielding 250 targeted arguments per synthesis type. We evaluate these targeted arguments against three baselines that test different aspects of M2s contribution. First, the Random baseline samples trajectories uniformly from the unobserved trajectory space; if M2s selection provides no value, targeted arguments should perform at chance (50%) against random exploration. Second, the M1b (Topic Presence) baseline tests whether simply knowing which content topics correlate with quality is sufficient. This emulates what simpler topic modelling approach might discover: which topics matter, but without M2s sequential and structural information [87, 36]. Specifically, we identify the top-3 topics based on the M1b model and filter for trajectories that contain only these topics, then sample randomly from this filtered set. Third, the Original Top 5% baseline compares targeted arguments against the best 5% of arguments from the original pairwise evaluation (by BT score), testing whether M2-guided generation can match or exceed the quality of the best observed arguments. Argument length correlates strongly with performance in this setting, as detailed in the ablation in Appendix C.2.2. In particular, the original top 5% arguments tend to be disproportionately long, creating confounder that would affect any direct comparison [27]. We construct length-matched evaluation sets using greedy pairing of arguments of similar length. 16 This yields balanced datasets ranging from 172 to 438 arguments, depending on synthesis type and baseline. We evaluate these datasets by running 5,000 random pairwise comparisons within each and calculating new BT scores. Table 3 shows that targeted arguments substantially outperform both the random baseline (7781% win rate) and the topic-presence baseline (6477% win rate) across synthesis types. This confirms that M2s trajectory rankings identify genuinely promising regions of the action space, more so than simpler topic-based approach might do. Against the original top 5%, targeted arguments remain competitive (1952% win rate), substantially exceeding the win rate of less than 5% that we would expect if M2s rankings failed to generalize beyond the observed samples. Here too, the familiar predictability gradient emerges: strict synthesis shows the strongest performance, while restructured synthesis exhibits greater variability. We also report the share of targeted arguments among the top-10 and top-100 of the performance-ranked, length-matched datasets. This highlights 15Our implementation allows for forcing controller choices rather than using the controller module to choose the next action. For details, please refer to the repository. 16For each targeted argument, we find the closest-length baseline argument within 5 characters, using each baseline argument at most once. This effectively leaves us with the intersection of the length histograms shown in Figures 10, 11, and 12. 14 Baseline Synthesis Comparisons Win (T) Top-10 Top-100 Strict Random M1b (Topic Presence) Strict Strict Original Top 5% Faithful Random M1b (Topic Presence) Faithful Faithful Original Top 5% 310 342 242 408 398 Random Restructured 438 M1b (Topic Presence) Restructured 428 Restructured 186 Original Top 5% 5000 5000 5000 5000 5000 5000 5000 5000 5000 77.8% 67.6% 51.7% 81.2% 77.3% 39.6% 77.4% 63.5% 19.3% 9/10 7/10 6/10 9/10 9/10 7/10 9/10 6/10 2/10 78/100 66/100 54/100 87/100 81/100 38/ 80/100 68/100 23/100 Table 3: Targeted Trajectory Exploration: Evaluating new, targeted vs. new baseline explorations. is the total number of arguments in the length-matched dataset (balanced: N/2 targeted, N/2 baseline). Comparisons count is the number of random pairwise comparisons evaluated. Win (T) is the share of comparisons between targeted and baseline arguments that the targeted argument won. Top-10 and Top-100 counts show the number of targeted arguments in the top-n arguments of the length-matched dataset when sorting by Bradley-Terry score based on the pairwise comparisons. that when the goal is to find the very best arguments, M2-guided trajectory selection offers promising approach."
        },
        {
            "title": "6 Discussion",
            "content": "We developed STATe-of-Thoughts (STATe) as controllable inference-time compute framework that makes step-level decisions explicit and auditable. On NoveltyBench, STATe produces substantially higher semantic diversity while maintaining good output quality, demonstrating that interventionbased branching can yield diverse candidates without the typical quality degradation associated with high-temperature sampling. Further, STATe enables using ITC as tool to explore what makes open-ended writing effective or ineffective. In argument generation, we show that action sequences (not just action presence) help predict downstream judgments, and that preserving tighter coupling between actions and final outputs improves predictability. Crucially, we also show that these learned associations can be operationalized: by scoring and targeting previously unseen trajectories, STATe can systematically explore under-visited regions of the controllable feature space and surface strong candidates, rather than repeatedly sampling near-duplicates. Taken together, these results position STATe as practical method to (1) generate diverse yet high-quality texts, (2) understand which writing strategies drive quality, and (3) discover and target promising new strategies."
        },
        {
            "title": "7 Limitations",
            "content": "STATe relies on prefilling text prefixes to implement intervention-based branching. As result, the method is currently only straightforward to deploy with open-source models, which are still less capable than cutting-edge closed-source models. In addition, our analysis of actionoutcome relationships is observational: while we find that action sequences are predictive of downstream judgments and that targeted, previously unseen trajectories can perform well, we do not make causal claims about how any particular action choice affects the final text or the downstream outcome. Our 15 current setup violates sequential ignorability, as we choose actions conditional on existing reasoning. second limitation is rigidity in the action and prefix design. In our implementation, each action is realized by fixed textual prefix, but many interventions admit multiple natural surface forms (e.g., synonymous discourse markers for causal reasoning include Because, Therefore, and As result). Representing these variants naively would require expanding the action space substantially, while many actions would share identical definitions. Moreover, some prefixes are well-formed as mid-document transitions but can sound unnatural as the first step of an answer (e.g., Therefore or However), which can create stylistic artifacts unless one conditions the action space on position or introduces context-aware prefix variants. These issues point to broader limitation: action spaces require careful, task-specific engineering, and the best granularity of actions (coarse vs. fine) may vary across domains. Relatedly, the synthesis step that converts reasoning traces into final outputs introduces tradeoff between control and quality. Strict synthesis preserves tight coupling between action sequences and output text, enabling high predictability in the argument quality experiment, but potentially producing stilted prose that mechanically concatenates reasoning steps. More flexible synthesis modes allow the model to smooth transitions and improve eloquence, but this freedom attenuates the mapping from actions to output properties. This trade-off has practical implications: when the goal is to study how specific rhetorical choices affect perceived quality, stricter synthesis provides cleaner attribution, whereas producing high-quality arguments for deployment may favor more flexible synthesis despite reduced interpretability. Moreover, our synthesis modes do not always behave as intended. Under strict synthesis, outputs are typically simple concatenations of reasoning steps, but occasionally the synthesis step adds concluding sentences, producing unexpectedly longer arguments. This elaboration behavior becomes increasingly common under faithful synthesis, causing the observed bimodality. Under restructured synthesis, elaboration is the default behavior. Task-specific tuning of synthesis prompts and more explicit length control would likely improve consistency. Additionally, our current approach to generating multiple realizations of the same trajectory relies on random seeds and achieves only limited diversity. Future work could introduce variation at the tree level, such as including personas [79, 80] in the input or using different language models, to enable better estimation of trajectory-level effects. Finally, our framework makes several scope assumptions. STATe currently focuses on single-turn, multi-step generation and does not explicitly model multi-turn conversational dynamics. 17 We also treat actions as textual interventions and do not support general tool calling beyond what is encoded in the action templates; integrating retrieval (for additional diversity) and tool-based verification (e.g., for numerical or algorithmic claims) could improve both generation and evaluation."
        },
        {
            "title": "8 Future Work",
            "content": "The associations we identify between controller actions and output performance (Section 5.2) as well as STATes ability to balance diversity and quality (Section 5.1) suggest several promising directions for future research. Causal inference for sequential action choices: Our current attribution analysis is focused on associations and predictability rather than causal claims. natural extension is to formalize action sequences as sequential treatments, where at each depth 1, . . . , the controller selects ai conditional on previous actions (ai, . . . , ai1) and current state (si). This framework is directly 17We do not support tool calls and tool call outputs as conversation turns, instead opting to directly include them as part of the thinking process of assistant messages. 16 related to marginal structural models and sequential ignorability [49, 48]. In this framework, gcomputation or inverse probability weighting could estimate the per-step causal effects of ai on the quality of the final output [48]. Mediation analysis [107] could further distinguish whether an action affects the outcome directly or through its influence on subsequent action choices. Crucially, because STATes controller can randomize action selections at each step, it enables experimental designs that eliminate the need for sequential ignorability assumptions. Human evaluation and behavioral outcomes: Our argument evaluation currently relies on GPT-5-mini as scalable judge (Section 4.2). Although LLM-as-judge offers repeated access to stable preferences and is broadly correlated with human judgments, it does not substitute for rigorous human experimentation. The preferences of groups and individuals are complex, context-dependent, and shaped by heterogeneous prior beliefs that are difficult to simulate with language models. Future work should therefore conduct controlled human subject experiments with preand post-intervention measurements of beliefs or behaviors. STATes sequential and multi-dimensional action traces provide uniquely informative design space for such studies, enabling systematic manipulation of rhetorical structure, topical framing, and ordering effects that would be difficult to isolate using other methods. Search and optimization over action spaces: We currently explore action spaces using fixed beam search and show that regression-based estimates can identify promising, previously unobserved trajectories (Section 5.2.2). natural extension is to employ principled tree search algorithms such as Monte Carlo Tree Search (MCTS) [59, 21, 12, 92, 93, 45]. Such approaches could iteratively generate arguments, update effect estimates, and adapt exploration toward high-performing regions of the action space under constrained evaluation budgets. Importantly, tree search methods also extend naturally to multi-turn and adversarial settings. This opens the possibility of integrating STATe with Multi-Agent Debate (MAD) frameworks to identify optimal multi-turn conversational or adversarial strategies, rather than optimizing single-turn outputs alone. Integration with RL and prompt optimization: Group-wise policy optimization methods for LLMs [90, 70] often suffer from mode collapse, where multiple sampled trajectories converge to near-identical completions. Prior work highlights this as central limitation of RLHF-style training regimes [14, 38]. By sampling across discrete and interpretable action sequences rather than relying solely on token-level stochasticity, STATe increases semantic diversity while preserving quality (Section 5.1), potentially mitigating collapse in group-based rollout sets. Conversely, prompt optimization methods [56, 76, 2, 119] could improve STATes controller, generator, and evaluator modules. At the same time, STATes structured action spaces offer mechanism to diversify prompt-search strategies within reflective prompt evolution frameworks [2]."
        },
        {
            "title": "9 Ethical Implications",
            "content": "Argument generation systems can be misused for persuasion at scale by generating misleading, manipulative, or otherwise harmful messages. Prior work shows that LLM-generated arguments can affect human beliefs and preferences in consequential domains such as public policy [5, 43] and can be used to support harmful narratives (e.g., conspiratorial content) [19]. Similarly, persuasive arguments act as tool for Red-Teaming LLMs, and can coerce them into either revealing hidden information or performing harmful requests [120]. These risks are amplified when systems are optimized not 17 merely to produce single response, but to search over many alternatives and select or refine the most effective one. STATe introduces additional ethical considerations because it enables search for optimal decision sequences, and can target those decisions toward particular LLM or human audience [80, 88]. Personalization can increase persuasive effectiveness [88] on human audience, and STATes actionlevel control makes it natural to operationalize personalization as discrete search problem (e.g., selecting content lenses, discourse moves, and framing styles conditioned on an inferred audience). This capability can be beneficial for benign applications (e.g., charity [111], education, or improved patient compliance), but also increases the risk of targeted manipulation. In practice, such microtargeting [88] could be used to persuade individuals to vote against their interests, purchase products that do not match their needs, or adopt beliefs that serve an adversarys goals. LLMs can already be used to draft convincing phishing messages [83], manipulate humans through blackmail, or commit espionage [71]. Combining these capabilities with controllable search over rhetorical strategies could further improve attack success by selecting audience-tailored arguments and calls to action. Though STATe can be source for highly effective manipulation, it can also serve as the basis for defending against malicious arguments. Researchers can use STATe to identify argumentative strategies that are emotionally abusive, or highly associated with misuse, and steer LLMs away from using them. Moreover, STATes arguments can be used for good, like advocating for donating to charities [111], improving pedagogy in the Education System, and introducing greater equality to legal representation."
        },
        {
            "title": "10 Acknowledgments",
            "content": "Funded by the European Union (ERC, Convey, 101078158). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. This work was supported in part by the Israel Science Foundation (grant 3123/25). We thank the Princeton Laboratory for Artificial Intelligence for providing computational resources and the Princeton Data-Driven Social Science Initiative for feedback and support. We also thank OpenAI for providing additional resources through the Researcher Access Program. We are grateful to Omar Khattab for his ongoing support of this project and for reviewing our paper at its early stages. We also thank Justin Grimmer, Yamil Velez, and Matthew Salganik for providing helpful comments and feedback. Finally, we appreciate the discussions, support, and feedback of our colleagues at Princeton, Technion, and HUJI."
        },
        {
            "title": "References",
            "content": "[1] Eldar Abraham, Karel DOosterlinck, Amir Feder, Yair Gat, Atticus Geiger, Christopher Potts, Roi Reichart, and Zhengxuan Wu. Cebab: Estimating the causal effects of real-world concepts on nlp model behavior. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 1758217596. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_ files/paper/2022/file/701ec28790b29a5bc33832b7bdc4c3b6-Paper-Conference.pdf. [2] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista OpsahlOng, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and Omar Khattab. Gepa: Reflective prompt evolution can outperform reinforcement learning, 2025. URL https://arxiv.org/abs/2507.19457. [3] Anthropic. Golden gate claude. Anthropic News, 2024. URL https://www.anthropic.com/ news/golden-gate-claude. Research demo of feature activation steering; accessed 2025-10-17. [4] Anthropic. Tracing the thoughts of large language model. Anthropic Research, 2025. URL https://www.anthropic.com/research/tracing-thoughts-language-model. [5] Hui Bai, Jan Voelkel, Shane Muldowney, johannes Eichstaedt, and Robb Willer. Aigenerated messages can be used to persuade humans on policy issues, Mar 2025. URL osf.io/stakv_v5. [6] Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute URL https://huggingface.co/spaces/HuggingFaceH4/ with open models, blogpost-scaling-test-time-compute. 2024. [7] David M. Blei. Probabilistic topic models. Commun. ACM, 55(4):7784, April 2012. ISSN 00010782. doi: 10.1145/2133806.2133826. URL https://doi.org/10.1145/2133806.2133826. [8] Esther Boissin, Thomas Costello, Daniel Spinoza-Martín, David Rand, and Gordon Pennycook. Dialogues with large language models reduce conspiracy beliefs even when the ISSN 2752-6542. doi: ai is perceived as human. PNAS Nexus, 4(11):pgaf325, 10 2025. 10.1093/pnasnexus/pgaf325. URL https://doi.org/10.1093/pnasnexus/pgaf325. [9] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [10] Simon Martin Breum, Daniel Vædele Egdal, Victor Gram Mortensen, Anders Giovanni Møller, and Luca Maria Aiello. The persuasive power of large language models. Proceedings of the International AAAI Conference on Web and Social Media, 18(1):152163, May 2024. doi: 10.1609/icwsm.v18i1.31304. URL https://ojs.aaai.org/index.php/ICWSM/article/view/ 31304. [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot 19 learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [12] Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):143, 2012. doi: 10.1109/TCIAIG.2012.2186810. [13] Nitay Calderon, Roi Reichart, and Rotem Dror. The alternative annotator test for llmas-a-judge: How to statistically justify replacing human annotators with llms, 2025. URL https://arxiv.org/abs/2501.10970. [14] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomek Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip J.K. Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan HadfieldMenell. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id=bx24KpJ4Eb. Survey Certification, Featured Certification. [15] Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan, Kathy McKeown, and Alyssa Hwang. AMPERSAND: Argument mining for PERSuAsive oNline discussions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 29332943, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1291. URL https://aclanthology.org/D19-1291. [16] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language models. In ICML 2024 Workshop on In-Context Learning, 2024. URL https: //openreview.net/forum?id=LjsjHF7nAN. [17] Robert Cialdini. The science of persuasion. Scientific American, 284(2):7681, 2001. [18] Thomas Costello and Gordon Pennycook. Just the facts: How dialogues with ai reduce conspiracy beliefs, 2025. [19] Thomas H. Costello, Gordon Pennycook, and David G. Rand. Durably reducing conspiracy beliefs through dialogues with ai. Science, 385(6714):eadq1814, 2024. doi: 10.1126/science. adq1814. URL https://www.science.org/doi/abs/10.1126/science.adq1814. [20] Thomas Costello, Kellin Pelrine, Matthew Kowal, Antonio Arechar, Jean-François Godbout, Adam Gleave, David Rand, and Gordon Pennycook. Large language models can effectively convince people to believe conspiracies. arXiv preprint arXiv:2601.05050, 2026. 20 [21] Rémi Coulom. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In Paolo Ciancarini and H. Jaap van den Herik, editors, 5th International Conference on Computer and Games, Turin, Italy, May 2006. URL https://inria.hal.science/inria-00116992. [22] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [23] Sebastian Deri, Jeremie Rappaz, Luca Maria Aiello, and Daniele Quercia. Coloring in the links: Capturing social ties as they are perceived. Proc. ACM Hum.-Comput. Interact., 2 (CSCW), November 2018. doi: 10.1145/3274312. URL https://doi.org/10.1145/3274312. [24] Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. Metacognitive capabilities of llms: an exploration in mathematical problem solving. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9798331314385. [25] Kefan Dong, Arvind V. Mahankali, and Tengyu Ma. Formal theorem proving by rewarding LLMs to decompose proofs hierarchically. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview.net/forum?id=D83tiHiNfF. 21 [26] Anil R. Doshi and Oliver P. Hauser. Generative ai enhances individual creativity but reduces the collective diversity of novel content. Science Advances, 10(28):eadn5290, 2024. doi: 10. 1126/sciadv.adn5290. URL https://www.science.org/doi/abs/10.1126/sciadv.adn5290. [27] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [28] Esin Durmus and Claire Cardie. Exploring the role of prior beliefs for argument persuasion. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 10351045, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1094. URL https://aclanthology.org/N18-1094/. [29] Esin Durmus and Claire Cardie. corpus for modeling user and language effects in argumentation on online debating. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 602607, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1057. URL https://aclanthology.org/P19-1057. [30] Esin Durmus, Faisal Ladhak, and Claire Cardie. The role of pragmatic and discourse context in determining argument impact. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 56685678, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1568. URL https://aclanthology.org/D19-1568. [31] Esin Durmus, Liane Lovitt, Alex Tamkin, Stuart Ritchie, Jack Clark, and Deep Ganguli. Measuring the persuasiveness of language models, 2024. URL https://www.anthropic.com/ news/measuring-model-persuasiveness. [32] Esin Durmus, Alex Tamkin, Jack Clark, Jerry Wei, Jonathan Marcus, Joshua Batson, Kunal Handa, Liane Lovitt, Meg Tong, Miles McCain, et al. Evaluating feature steering: case study in mitigating social biases. Anthropic Research, 2024. URL https://www.anthropic. com/research/evaluating-feature-steering. [33] Naoki Egami, Christian J. Fong, Justin Grimmer, Margaret E. Roberts, and Brandon M. Stewart. How to make causal inferences using texts. Science Advances, 8(42):eabg2652, 2022. doi: 10.1126/sciadv.abg2652. URL https://www.science.org/doi/abs/10.1126/sciadv. abg2652. [34] Roxanne El Baff, Khalid Al Khatib, Milad Alshomary, Kai Konen, Benno Stein, and Henning Wachsmuth. Improving argument effectiveness across ideologies using instruction-tuned large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 46044622, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-emnlp.265. URL https://aclanthology.org/2024.findings-emnlp.265/. [35] Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Bran22 don M. Stewart, Victor Veitch, and Diyi Yang. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Transactions of the Association for Computational Linguistics, 10:11381158, 2022. doi: 10.1162/tacl_a_00511. URL https://aclanthology.org/2022.tacl-1.66/. [36] Christian Fong and Justin Grimmer. Discovery of treatments from text corpora. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16001609, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1151. URL https://aclanthology.org/P16-1151/. [37] G. Ray Funkhouser and Richard Parker. An action-based theory of persuasion in marketing. Journal of Marketing Theory and Practice, 7(3):2740, 1999. doi: 10.1080/10696679.1999. 11501838. URL https://doi.org/10.1080/10696679.1999.11501838. [38] Jingchu Gai, Guanning Zeng, Huaqing Zhang, and Aditi Raghunathan. Differential smoothing mitigates sharpening and improves llm reasoning, 2025. URL https://arxiv.org/abs/2511. 19942. [39] Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and Yi Wu. On designing effective rl reward at training time for llm reasoning, 2024. URL https://arxiv.org/abs/2410.15115. [40] Lewis Goldberg. An alternative \"description of personality\": The big-five factor structure. Journal of Personality and Social Psychology, 59(6):12161229, 1990. [41] Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. Explaining classifiers with causal concept effect (cace). arXiv preprint arXiv:1907.07165, 2019. [42] Justin Grimmer, Margaret Roberts, and Brandon Stewart. Text as data: new framework for machine learning and the social sciences. Princeton University Press, 2022. [43] Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, and Christopher Summerfield. The levers of political persuasion with conversational artificial intelligence. Science, 390(6777):eaea3884, 2025. doi: 10.1126/science.aea3884. URL https://www.science.org/doi/abs/10.1126/science. aea3884. [44] Kobi Hackenburg, Ben M. Tappin, Paul Röttger, Scott A. Hale, Jonathan Bright, and Helen Margetts. Scaling language model size yields diminishing returns for single-message political persuasion. Proceedings of the National Academy of Sciences, 122(10):e2413443122, 2025. doi: 10. 1073/pnas.2413443122. URL https://www.pnas.org/doi/abs/10.1073/pnas.2413443122. [45] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https:// aclanthology.org/2023.emnlp-main.507/. [46] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, and Zhiting Hu. LLM reasoners: 23 New evaluation, library, and analysis of step-by-step reasoning with large language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= b0y6fbSUG0. [47] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. {DEBERTA}: {DECODING}- {enhanced} {bert} {with} {disentangled} {attention}. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=XPZIaotutsD. [48] Miguel A. Hernan. Causal Inference: What If. Taylor & Francis, Boca Raton, 2024. [49] Miguel Ángel Hernán, Babette Brumback, and James Robins. Marginal structural models to estimate the causal effect of zidovudine on the survival of hiv-positive men, 2000. [50] Christopher Hidey, Elena Musi, Alyssa Hwang, Smaranda Muresan, and Kathy McKeown. Analyzing the semantic types of claims and premises in an online persuasive forum. In Ivan Habernal, Iryna Gurevych, Kevin Ashley, Claire Cardie, Nancy Green, Diane Litman, Georgios Petasis, Chris Reed, Noam Slonim, and Vern Walker, editors, Proceedings of the 4th Workshop on Argument Mining, pages 1121, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5102. URL https://aclanthology.org/ W17-5102/. [51] Kelly Hong, Anton Troynikov, and Jeff Huber. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma, July 2025. URL https://research. trychroma.com/context-rot. [52] Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, and Yonatan Belinkov. Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias. Transactions of the Association for Computational Linguistics, 12:771785, 2024. doi: 10.1162/tacl_a_00673. URL https://aclanthology.org/2024.tacl-1.43/. [53] Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, and Yejin Choi. Artificial hivemind: The open-ended homogeneity of language models (and beyond). In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview.net/ forum?id=saDOrrnNTz. [54] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision Under Risk, chapter Chapter 6, pages 99127. 2013. doi: 10.1142/9789814417358_0006. URL https://www.worldscientific.com/doi/abs/10.1142/9789814417358_0006. [55] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tenenholtz. Mrkl systems: modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning, 2022. URL https://arxiv.org/abs/2205.00445. [56] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=sY5N0zY5Od. 24 [57] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=8euJaTveKw. [58] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.248. URL https://aclanthology.org/2024.emnlp-main.248/. [59] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In Johannes Fürnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006, pages 282293, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-46056-5. Large language models are zero-shot reasoners. [60] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Curran Associates, URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ Inc., 2022. 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf. [61] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [62] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Revisiting NLI-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163177, 2022. doi: 10.1162/tacl_a_00453. URL https://aclanthology.org/2022.tacl-1.10/. [63] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. [64] Byung Cheol Lee and Jaeyeon Jae Chung. An empirical investigation of the impact of chatgpt on creativity. Nature Human Behaviour, 8:1906 1914, 2024. URL https://api. semanticscholar.org/CorpusID:271857922. [65] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instructionfollowing models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. [66] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. 25 In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=v8L0pN6EOi. [67] Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sadé, Alan Jeffares, Albert Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Amélie Héliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Clémence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Gaëtan Ecrepont, Gauthier Guinet, Georgii Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, Joachim Studnia, Jonas Amar, Joséphine Delas, Josselin Somerville Roberts, Karmesh Yadav, Khyathi Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poirée, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, Nikhil Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, Sagar Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, Vincent Maladière, Virgile Richard, Wassim Bouaziz, WenDing Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, and Zaccharie Ramzi. Ministral 3, 2026. URL https://arxiv.org/abs/2601.08584. [68] Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, and Yahui Zhou. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025. [69] Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas L. Griffiths. Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=J3gzdbYZxS. [70] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https://openreview.net/forum?id=jLpC1zavzn. [71] Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart J. Ritchie, Soren Mindermann, Evan Hubinger, Ethan Perez, and Kevin Troy. Agentic misalignment: How llms could be insider threats, 2025. URL https://arxiv.org/abs/2510.05179. [72] Nguyen Nhat Minh, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid Shwartz-Ziv. Turning up the heat: Min-p sampling for creative and coherent LLM outputs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=FBkpCyujtS. [73] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [74] NVIDIA, :, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frank Sun, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt, Gargi Prasad, George Armstrong, Gerald Shen, Gorkem Batmaz, Grigor Nalbandyan, Haifeng Qian, Harsh Sharma, Hayley Ross, Helen Ngo, Herbert Hum, Herman Sahota, Hexin Wang, Himanshu Soni, Hiren Upadhyay, Huizi Mao, Huy Nguyen, Huy Nguyen, Iain Cunningham, Ido Galil, Ido Shahaf, Igor Gitman, Ilya Loshchilov, Itamar Schen, Itay Levy, Ivan Moshkov, Izik Golan, Izzy Putterman, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jatin Mitra, Jeffrey Glick, Jenny Chen, Jesse Oliver, Jian Zhang, Jiaqi Zeng, Jie Lou, Jimmy Zhang, Jinhang Choi, Jining Huang, Joey Conway, Joey Guman, John Kamalu, Johnny Greco, Jonathan Cohen, Joseph Jennings, Joyjit Daw, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kai Xu, Kan Zhu, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kevin Shih, Kezhi Kong, Khushi Bhardwaj, Kirthi Shankar, Krishna C. Puvvada, Krzysztof Pawelec, Kumar Anik, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Li Ding, Lizzie Wei, Lucas Liebenwein, Luis Vega, Maanu Grover, Maarten Van Segbroeck, Maer Rodrigues de Melo, Mahdi Nazemi, Makesh Narsimhan Sreedhar, Manoj Kilaru, Maor Ashkenazi, Marc Romeijn, Marcin Chochowski, Mark Cai, Markus Kliegl, Maryam Moosaei, Matt Kulka, Matvei Novikov, Mehrzad Samadi, Melissa Corpuz, Mengru Wang, Meredith Price, Michael Andersch, Michael Boone, Michael Evans, Miguel Martinez, Mikail Khona, Mike Chrzanowski, Minseok Lee, Mohammad Dabbah, Mohammad Shoeybi, Mostofa Patwary, Nabin Mulepati, Najeeb Nabwani, Natalie Hereth, Nave Assaf, Negar Habibi, Neta Zmora, Netanel Haber, Nicola Sessions, Nidhi Bhatia, Nikhil Jukar, Nikki Pope, Nikolai Ludwig, Nima Tajbakhsh, Nir Ailon, Nirmal Juluru, Nishant Sharma, Oleksii Hrinchuk, Oleksii Kuchaiev, Olivier Delalleau, Oluwatobi Olabiyi, Omer Ullman Argov, Omri Puny, Oren Tropp, Ouye Xie, Parth Chadha, Pasha Shamis, Paul Gibbons, Pavlo Molchanov, Pawel Morkisz, Peter Dykas, Peter Jin, Pinky Xu, Piotr Januszewski, Pranav Prashant Thombre, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Qing Miao, Qiyu Wan, Rabeeh Karimi Mahabadi, Rachit Garg, Ran El-Yaniv, Ran Zilberstein, Rasoul Shafipour, Rich Harang, Rick 27 Izzo, Rima Shahbazyan, Rishabh Garg, Ritika Borkar, Ritu Gala, Riyad Islam, Robert Hesse, Roger Waleffe, Rohit Watve, Roi Koren, Ruoxi Zhang, Russell Hewett, Russell J. Hewett, Ryan Prenger, Ryan Timbrook, Sadegh Mahdavi, Sahil Modi, Samuel Kriman, Sangkug Lim, Sanjay Kariyappa, Sanjeev Satheesh, Saori Kaji, Satish Pasumarthi, Saurav Muralidharan, Sean Narentharen, Sean Narenthiran, Seonmyeong Bak, Sergey Kashirsky, Seth Poulos, Shahar Mor, Shanmugam Ramasamy, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shiqing Fan, Shreya Gopal, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Simeng Sun, Smita Ithape, Somshubra Majumdar, Soumye Singhal, Stas Sergienko, Stefania Alborghetti, Stephen Ge, Sugam Dipak Devare, Sumeet Kumar Barua, Suseella Panguluri, Suyog Gupta, Sweta Priyadarshi, Syeda Nahida Akter, Tan Bui, Teodor-Dumitru Ene, Terry Kong, Thanh Do, Tijmen Blankevoort, Tim Moon, Tom Balough, Tomer Asida, Tomer Bar Natan, Tomer Ronen, Tugrul Konuk, Twinkle Vashishth, Udi Karpas, Ushnish De, Vahid Noorozi, Vahid Noroozi, Venkat Srinivasan, Venmugil Elango, Victor Cui, Vijay Korthikanti, Vinay Rao, Vitaly Kurin, Vitaly Lavrukhin, Vladimir Anisimov, Wanli Jiang, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenfei Zhou, Will Jennings, William Zhang, Wojciech Prazuch, Xiaowei Ren, Yashaswi Karnati, Yejin Choi, Yev Meyer, Yi-Fu Wu, Yian Zhang, Yigong Qin, Ying Lin, Yonatan Geifman, Yonggan Fu, Yoshi Subara, Yoshi Suhara, Yubo Gao, Zach Moshe, Zhen Dong, Zhongbo Zhu, Zihan Liu, Zijia Chen, and Zijie Yan. Nvidia nemotron 3: Efficient and open intelligence, 2025. URL https://arxiv.org/abs/2512.20856. [75] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [76] Krista Opsahl-Ong, Michael Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 93409366, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.525. URL https://aclanthology.org/2024. emnlp-main.525/. [77] Hadas Orgad and Yonatan Belinkov. BLIND: Bias removal with no demographics. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 88018821, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.490. URL https://aclanthology.org/2023.acl-long.490. [78] Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. Llms know more than they show: On the intrinsic representation of llm hallucinations, 2024. URL https://arxiv.org/abs/2410.02707. [79] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763. URL https://doi.org/10.1145/3586183.3606763. [80] Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, and Michael S. Bernstein. Generative agent simulations of 1,000 people, 2024. URL https://arxiv.org/abs/2411.10109. [81] Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=NikbrdtYvG. [82] Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. The Penn Discourse TreeBank 2.0. In Nicoletta Calzolari, Khalid Choukri, 29 Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08), Marrakech, Morocco, May 2008. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2008/pdf/754_paper.pdf. [83] Qinglin Qi, Yun Luo, Yijia Xu, Wenbo Guo, and Yong Fang. Spearbot: Leveraging large language models in generative-critique framework for spear-phishing email generation. Inf. Fusion, 122(C), October 2025. ISSN 1566-2535. doi: 10.1016/j.inffus.2025.103176. URL https://doi.org/10.1016/j.inffus.2025.103176. [84] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410/. [85] Liran Ringel, Elad Tolochinsky, and Yaniv Romano. Learning continue-thinking token In Kentaro Inui, Sakriani Sakti, Haofen Wang, Derek F. for enhanced test-time scaling. Wong, Pushpak Bhattacharyya, Biplab Banerjee, Asif Ekbal, Tanmoy Chakraborty, and Dhirendra Pratap Singh, editors, Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, pages 33243345, Mumbai, India, December 2025. The Asian Federation of Natural Language Processing and The Association for Computational Linguistics. ISBN 979-8-89176-298-5. URL https://aclanthology.org/2025.ijcnlp-long. 177/. [86] Margaret E. Roberts, Brandon M. Stewart, Dustin Tingley, Christopher Lucas, Jetson LederLuis, Shana Kushner Gadarian, Bethany Albertson, and David G. Rand. Structural topic models for open-ended survey responses. American Journal of Political Science, 58(4):1064 1082, 2014. ISSN 00925853, 15405907. URL http://www.jstor.org/stable/24363543. [87] Till Raphael Saenger, Musashi Hinck, Justin Grimmer, and Brandon M. Stewart. AutoPersuade: framework for evaluating and explaining persuasive arguments. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1632516342, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.913. URL https://aclanthology.org/2024.emnlp-main.913/. [88] Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. On the conversational persuasiveness of gpt-4. Nature Human Behaviour, 9(8):16451653, May 2025. ISSN 2397-3374. doi: 10.1038/s41562-025-02194-6. URL http://dx.doi.org/10.1038/ s41562-025-02194-6. [89] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. In Marine ColBERTv2: Effective and efficient retrieval via lightweight late interaction. Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 37153734, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.272. URL https://aclanthology.org/2022.naacl-main.272/. [90] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. [91] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Esin DURMUS, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language modIn The Twelfth International Conference on Learning Representations, 2024. URL els. https://openreview.net/forum?id=tvhaxkMKAn. [92] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587): 484489, 2016. [93] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018. doi: 10.1126/ science.aar6404. URL https://www.science.org/doi/abs/10.1126/science.aar6404. [94] Adi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Distinguishing ignorance from error in llm hallucinations. arXiv preprint arXiv:2410.22071, 2024. [95] Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, and Yonatan Belinkov. Trust me, Im wrong: LLMs hallucinate with certainty despite knowing the answer. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1466514688, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176335-7. doi: 10.18653/v1/2025.findings-emnlp.792. URL https://aclanthology.org/2025. findings-emnlp.792/. [96] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. [97] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning, 2024. URL https: //arxiv.org/abs/2409.12183. [98] Christian Stab and Iryna Gurevych. Annotating argument components and relations in persuasive essays. In Junichi Tsujii and Jan Hajic, editors, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1501 1510, Dublin, Ireland, August 2014. Dublin City University and Association for Computational Linguistics. URL https://aclanthology.org/C14-1142. [99] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Adback. 31 vances in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf. [100] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=HvoG8SxggZ. [101] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1491814937, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.923. URL https://aclanthology.org/2023.emnlp-main.923/. [102] Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. In Proceedings of the 25th International Conference on World Wide Web, WWW 16, page 613624, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering Committee. ISBN 9781450341431. doi: 10.1145/2872427.2883081. URL https: //doi.org/10.1145/2872427.2883081. [103] Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. Confidence improves self-consistency in LLMs. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 2009020111, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025. findings-acl.1030. URL https://aclanthology.org/2025.findings-acl.1030/. [104] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. [105] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=wCu6T5xFjeJ. 32 [106] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267288, 1996. [107] Tyler VanderWeele. Explanation in causal inference: methods for mediation and interaction. Oxford University Press, 2015. [108] Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar Prabhakaran, Tim Alberdingk Thijm, Graeme Hirst, and Benno Stein. Computational argumentation quality assessment in natural language. In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 176187, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://aclanthology.org/E17-1017/. [109] Henning Wachsmuth, Manfred Stede, Roxanne El Baff, Khalid Al-Khatib, Maria Skeppstedt, and Benno Stein. Argumentation synthesis following rhetorical strategies. In Emily M. Bender, Leon Derczynski, and Pierre Isabelle, editors, Proceedings of the 27th International Conference on Computational Linguistics, pages 37533765, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://aclanthology.org/C18-1318/. [110] Kaiwen Wang, Jin Peng Zhou, Jonathan Daniel Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, and Wen Sun. Value-guided search for efficient chain-of-thought reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=jOsuKwiCL0. [111] Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu. Persuasion for good: Towards personalized persuasive dialogue system for social good. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 56355649, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1566. URL https://aclanthology.org/P19-1566/. [112] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. [113] Bonnie Webber, Rashmi Prasad, Alan Lee, and Aravind Joshi. The penn discourse treebank 3.0 annotation manual. Philadelphia, University of Pennsylvania, 35:108, 2019. [114] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J. [115] Weijia Xu, Nebojsa Jojic, Sudha Rao, Chris Brockett, and Bill Dolan. Echoes in ai: Quantifying lack of plot diversity in llm outputs. Proceedings of the National Academy of Sciences, 122(35): e2504966122, 2025. doi: 10.1073/pnas.2504966122. URL https://www.pnas.org/doi/abs/ 10.1073/pnas.2504966122. 33 [116] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [117] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf. [118] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=WE_vluYUL-X. [119] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639:609616, 2025. [120] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1432214350, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.773. URL https://aclanthology.org/2024. acl-long.773/. [121] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. [122] Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, and Daphne Ippolito. Noveltybench: Evaluating creativity and diversity in language models. In Second Conference on Language Modeling, 2025. URL https://openreview.net/ forum?id=XZm1ekzERf. [123] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Social Science Experiments with Text Persuasion is central to human communication, spanning political discourse [5, 44, 43], humanAI interaction [88, 20, 31], and misinformation correction [18, 8]. Computational social science increasingly formalizes persuasion research by treating text as treatment variable to study how linguistic features causally affect downstream behaviors [42, 35]. Traditional approaches focus on identifying content themes across document corpora and assessing how these themes affect outcomes [36, 86]. For example, Saenger et al. [87] use topic modeling to discover persuasive themes in argument collections, while Egami et al. [33] analyze how different framings affect bureaucratic responsiveness. Recently, researchers have examined how conversations with LLMs affect beliefs [19, 20, 88], identifying consistent patterns in effective messaging, such as emphasizing facts and evidence [18]. However, existing methods face limitations in studying fine-grained textual features. Topic modeling approaches [7, 42, 87] naturally capture content themes but struggle with structural and stylistic variation. Such text-as-treatment experiments ideally manipulate specific featuresrhetorical structure [98, 50, 15, 109] (whether arguments begin with concessions or lead with strong claims), stylistic choices [23, 109, 10, 34] (formality, tone, pragmatic objective), and content themeswhile maintaining coherence [30] and logical soundness. Yet these features are difficult to control systematically. Moreover, most prior work examines feature presence (whether theme appears) rather than sequential ordering (when in message feature appears), limiting insights into how narrative structure affects argument quality."
        },
        {
            "title": "B STATe Modules",
            "content": "B.1 Controller B.1.1 Generative controller The generative controller prompts an LLM to propose tool call [118, 55] from conditioned on the current state: it chooses which tool to use (which action template) and provides values for the tools arguments (permitted choices are specified through Literal type). In this setup, the same generator model that produces thoughts can also decide what to do next. key advantage is that the controller can utilize ITC methods like CoT, and produce natural-language rationales (internal reasoning) for why tool call is appropriate given the chain so far. 18 key limitation is low action diversity: because tool-call generation is itself sampled from an LLM, the controller can collapse to repeatedly predicting the same high-probability action, reducing the benefit of branching. B.1.2 Reranker controller To introduce reliable diversity, we instead use discriminative reranker controller that scores all candidate action-argument combinations and selects the top-n. We formulate action selection by measuring the relevance of query and document using cross-encoder [84, 105, 89, 62, 101, 121]: the query contains the input and the reasoning chain Zi, while each document is description of the effect of the given tool and parameter (e.g., introduce new claim that expands on financial impacts). reranker assigns relevance score to each document, yielding distribution over actions, and we take the top-n scoring actions for expansion. This design supports diverse branching (by selecting different high-scoring actions) and enables efficient enumeration when is finite and structured. B.2 Evaluator B.2.1 Generative Evaluator The generative evaluator prompts an LLM to act as judge and score the reasoning (PRM) or output (ORM) at hand. With this setup, the same generator model that produces thoughts and outputs is used to score them. In practice, our evaluators follow multi-item rubric: list of domaindependent criteria 19 the judge should check (e.g., constraint satisfaction, correctness, coherence, style). Importantly, we allow rubric items to carry different importance weights, so that violations of high-priority requirements dominate the score. This is implemented via weighted, multi-dimensional scoring (each rubric item has weight, and the final score is weighted aggregate). B.2.2 Reranker Evaluator Analogous to the reranker controller (Section 3.1.2), we can use cross-encoder to score candidate states rather than candidate actions. Here, the query contains the input and the evaluation criteria (e.g., coherence, correctness, constraint satisfaction), while each document is candidate reasoning chain Zi (for intermediate evaluation) or final output (for outcome evaluation). The reranker assigns relevance score to each candidate, which we interpret as quality estimate. This approach is particularly efficient when the number of candidates is large, as cross-encoders can 18Preceding tool call outputs with reasoning enables more informed decision-making and easier debugging. 19For example, in argument generation tasks, the criteria should be centered on the arguments persuasiveness, logical rigor, and effective structure [108]. 36 score all candidates in single batched forward pass without requiring the longer generations that LLM-as-a-Judge evaluators produce. Further, this resolves the sycophancy issues of the generative evaluator, which tends to award perfect scores to all good candidates [91]. B.2.3 Programmatic Evaluator In some domains, intermediate and final states admit programmatic evaluation: deterministic procedure can verify correctness, constraint satisfaction, or structural validity without invoking an LLM [63, 39, 104]. This setting crucially assumes an additive action space, where each reasoning step produces text that is concatenated to all previous steps, so that partial trajectory Zi = [z1, . . . , zi] represents prefix of well-formed candidate solution. Many instruction-following and mathematical tasks satisfy this property, as successive steps monotonically construct single output whose validity can be checked incrementally (e.g., JSON well-formedness, exact string constraints, section counts, or arithmetic consistency). This stands in contrast to metacognitive action spacessuch as selfreflection, critique, or targeted editing of an existing draftwhere actions do not compose into single executable artifact, and intermediate states cannot be interpreted as partial answers. As result, programmatic evaluators are inherently task-dependent and cannot be assumed to exist for all domains. Formally, we define Programmatic Evaluator as deterministic scoring function conditioned on the input x:20 PRM(concat(Zi) x) [0, 1] ORM(y x) [0, 1] (11) (12) which evaluates whether the concatenated reasoning (concat(Zi)) or answer (y) satisfies all constraints induced by the task, x.21 In tasks where constraint satisfaction is prefix-monotonic, can be used interchangeably as both Process Reward Model and an Outcome Reward Model, i.e., PRM ORM , allowing invalid trajectories to be pruned immediately upon violation. When available, programmatic evaluators eliminate judge variance, avoid sycophancy effects, and provide exact credit assignment over the action space. However, their applicability fundamentally relies on additive action spaces and reliable prefix-level validation; extending programmatic evaluation to non-additive or revision-based action spaces remains an open challenge. 20It is also possible to condition the ORM on concat(x, Zi) as opposed to just conditioning on x. 21In practice, the concatenation operation ignores intermediate or internal reasoning fields (e.g., chain-of-thought or planning annotations), retaining only the externally visible output fields. Including internal reasoning in the concatenation would often invalidate otherwise correct partial outputs and interfere with reliable programmatic grading."
        },
        {
            "title": "C Complete Experiment Results",
            "content": "We conduct all runs with vLLM [61] in offline mode to enable efficient layer-wise batching across tree expansions. C.1 NoveltyBench C.1.1 Experiment Details Following the NoveltyBenchs standard three-stage evaluation pipeline, we first partition the 10 generations per prompt using an embedding model (selecting the first matching equivalence class based on embedding distance, or creating new one), then score the top clusters using reward model to obtain per-generation utility scores, and finally aggregate these into Mean Distinct and Mean Utility metrics. The partition algorithm iterates through generations sequentially, comparing each against random representative from existing equivalence classes; if the classifier predicts functional equivalence (threshold > 0.102), the generation joins that class, otherwise new class is created. The Mean Utility metric, on the other hand, combines novelty and quality through patience-weighted metric: utilityk = 1 1 pk (cid:88) i=1 pi1 1[ci = cj, < i] ui (13) where = 0.8 models user patience, ci denotes equivalence class, and ui is the quality score from the Skywork-Reward-Gemma-2-27B [68] reward model. This formulation penalizes redundant generations geometrically while rewarding high-quality novel outputs. Table 4 includes both metrics across the different model and temperature configurations. We discuss results in Section 5.1. C.1.2 Full NoveltyBench Results Diversity STATe outperforms best-of-n and ToT across all models under consideration: for Qwen3-4B, STATe achieves 3.91 0.1 distinct generations versus 3.52 0.13 for CoT with Action Space, while for Qwen3-8B, STATe reaches 4.65 0.11 versus 3.33 0.1 for ToT with Action Space. We also show that gains hold on Non-Qwen3 models  (Table 4)  : Nemotron-3-30B shows STATe at 5.92 0.16 distinct generations compared to 5.41 0.16 for Baseline ToT with Action Space. Utility STATes performance in utility also generalize across most configurations  (Table 4)  : Qwen38B shows STATe at 3.43 0.07 versus 3.05 0.09 for CoT with Action Space, and Ministral-3-14B demonstrates STATe at 5.39 0.11 versus 4.98 0.12 for the best baseline. However, two models exhibit exceptions where non-STATe methods achieve competitive or slightly higher utility scores. For Qwen3-4B at = 0.7, CoT with Action Space reaches 3.26 0.07 utility compared to STATes 3.18 0.1, and Nemotron-3-30B shows CoT with Action Space at 4.34 0.12 versus STATe at 4.44 0.12. These small differences in relative utility performance are likely driven by STATes response ordering. Because the Controller Re-Ranker [121] selects actions according to learned preferences, early responses tend to share similar choices along one dimension (e.g., target audience) and vary only along others (e.g., personality). This effectively orders STATe responses by similarity to the controllers top preference, whereas alternative methods return responses in arbitrary order. The generally high utility scores confirm that STATe produces high-quality, diverse responses, but the ordering effect means that utility, which also depends on order, may slightly understate STATes quality relative to randomly ordered baselines. 38 Model Method Low Medium High D U Ministral 3 14B Qwen3 4B Qwen3 8B Qwen3 30B Nemotron 3 30B Baseline Baseline CoT Baseline w/ Action Space Baseline CoT w/ Action Space Baseline ToT Baseline ToT w/ Action Space STATe of Thoughts Baseline Baseline CoT Baseline w/ Action Space Baseline CoT w/ Action Space Baseline ToT Baseline ToT w/ Action Space STATe of Thoughts Baseline Baseline CoT Baseline w/ Action Space Baseline CoT w/ Action Space Baseline ToT Baseline ToT w/ Action Space STATe of Thoughts Baseline Baseline CoT Baseline w/ Action Space Baseline CoT w/ Action Space Baseline ToT Baseline ToT w/ Action Space STATe of Thoughts Baseline Baseline CoT Baseline w/ Action Space Baseline CoT w/ Action Space Baseline ToT Baseline ToT w/ Action Space STATe of Thoughts 2.02 0.09 3.97 0.07 3.15 0.09 4.92 0.09 4.27 0.08 5.39 0.18 5.74 0.08 1.71 0.04 2.69 0.08 1.59 0.04 3.36 0.1 2.54 0.07 3.1 0.1 3.66 0.09 1.6 0.04 2.56 0.07 1.72 0.04 3.03 0.07 2.33 0.06 3.05 0.07 4.39 0.07 1.48 0.03 2.15 0.07 1.74 0.05 3.07 0.11 2.51 0.09 3.16 0.08 4.87 0.1 3.25 0.07 3.74 0.1 4.08 0.14 4.57 0.14 3.74 0.08 4.87 0.15 5.37 0. 2.91 0.07 4.26 0.06 2.99 0.12 4.55 0.07 4.23 0.07 4.75 0.14 5.0 0.07 2.23 0.04 2.85 0.06 1.96 0.06 3.14 0.08 2.67 0.08 3.04 0.12 3.02 0.05 2.26 0.02 2.77 0.07 1.49 0.05 2.9 0.09 2.49 0.05 2.67 0.05 3.29 0.07 2.28 0.03 2.65 0.07 2.23 0.05 3.15 0.06 2.87 0.05 3.29 0.05 3.77 0.12 3.41 0.06 3.76 0.07 3.92 0.17 4.16 0.09 3.43 0.06 3.55 0.12 4.15 0.07 3.23 0.1 4.69 0.07 4.55 0.11 5.6 0.14 4.8 0.09 5.71 0.15 6.27 0. 1.95 0.04 3.01 0.06 1.82 0.06 3.52 0.13 2.79 0.07 3.29 0.1 3.91 0.1 1.86 0.05 2.81 0.05 2.07 0.03 3.26 0.11 2.67 0.1 3.33 0.1 4.65 0.11 1.67 0.04 2.44 0.07 2.01 0.05 3.36 0.12 2.81 0.08 3.25 0.08 5.02 0.09 3.46 0.09 3.96 0.1 4.13 0.06 4.79 0.12 4.36 0.13 5.41 0.16 5.92 0.16 3.9 0.08 4.78 0.08 3.83 0.1 4.98 0.12 4.65 0.1 4.94 0.12 5.39 0.11 2.41 0.05 3.02 0.07 2.09 0.05 3.26 0.07 2.85 0.05 3.15 0.09 3.18 0. 2.44 0.05 2.92 0.05 1.7 0.07 3.05 0.09 2.67 0.07 2.83 0.09 3.43 0.07 2.45 0.05 2.85 0.07 2.41 0.07 3.28 0.1 3.07 0.06 3.36 0.07 3.83 0.11 3.56 0.09 3.87 0.11 3.99 0.08 4.34 0.12 3.8 0.08 3.73 0.09 4.44 0.12 4.21 0.1 5.32 0.15 5.49 0.06 6.08 0.16 5.36 0.11 6.29 0.14 6.87 0.09 2.39 0.06 3.32 0.07 2.27 0.06 3.75 0.09 3.16 0.12 3.56 0.12 4.3 0.1 2.29 0.1 3.27 0.08 2.64 0.04 3.7 0.13 3.21 0.09 3.77 0.05 5.07 0. 2.01 0.05 2.8 0.08 2.44 0.07 3.74 0.09 3.16 0.12 3.61 0.1 5.39 0.11 3.95 0.09 4.51 0.09 4.5 0.11 5.26 0.13 5.61 0.11 6.6 0.11 7.11 0.14 4.62 0.09 5.15 0.12 4.31 0.1 5.24 0.11 5.01 0.09 5.25 0.17 5.75 0.12 2.65 0.07 3.22 0.05 2.34 0.05 3.35 0.08 3.11 0.08 3.33 0.09 3.38 0.09 2.74 0.07 3.22 0.07 1.97 0.06 3.29 0.07 3.0 0.04 3.01 0.06 3.65 0.07 2.64 0.04 3.07 0.06 2.71 0.04 3.49 0.11 3.32 0.08 3.61 0.11 4.01 0. 3.9 0.11 4.25 0.09 4.26 0.09 4.57 0.12 4.29 0.07 3.85 0.15 4.67 0.11 Table 4: NoveltyBench diversity (D) and utility (U) (meanstd over seeds). Low, medium, and high temperature correspond to sampling temperature = 0.1, 0.3, and 0.5 for Ministral 3 14B, and to = 0.5, 0.7, and 1.0 for all other models. Best and second best per model per column are bolded and underlined, respectively. C.2 Predictability of Argument Quality from Actions C.2.1 Argument Evaluation Argument quality measures the extent to which an argument is preferred over alternatives or rated as effective, whereas persuasiveness measures how effective an argument is in changing beliefs or behaviors [17, 37, 43]. We are not claiming to discover generalizable insights into argument quality; rather, this setup allows for efficient argument evaluation based on the presumably constant preferences of our LLM judge. Future work, discussed in Section 8, might explore similar approach but survey human target audience. C.2.2 Controlling for Argument Length When we include argument length (character count) as feature in all models, we observe substantial increases in predictability across all synthesis modes (Figure 4). The relative performance ordering, with M2 performing best, is preserved across all settings. However, the magnitude of the improvement varies substantially across synthesis types. The most striking result is for faithful synthesis: M2 R2 increases from 0.38 to 0.82. In contrast, strict synthesis improves from 0.57 to 0.71 (+25%), and restructured synthesis from 0.37 to 0.57 (+54%). This disproportionate effect on faithful synthesis breaks the apparent predictability ordering: with length controlled, faithful synthesis is the most predictable (R2 = 0.82), followed by strict 39 Synthesis M0 M1a R2 M1b R2 M1c R2 M2 R2 Strict Faithful Restructured 2996/1998 3000/2000 3000/ + Argument length control 0.284 0.032 0.254 0.032 0.108 0.028 0.100 0.026 0.131 0.027 0.092 0.022 0.366 0.036 0.308 0.034 0.196 0.032 0.569 0.032 0.383 0.033 0.368 0. Strict Faithful Restructured 2996/1998 3000/2000 3000/2000 0.293 0.027 0.720 0.017 0.286 0.035 0.550 0.024 0.774 0.015 0.385 0.034 0.376 0.029 0.755 0.017 0.375 0.034 0.614 0.023 0.801 0.014 0.459 0. 0.708 0.020 0.823 0.013 0.567 0.027 Table 5: Model comparison across synthesis types. R2 values on held-out test set with 95% bootstrap CI ( half-width) where shows the number of train/test samples. M0 models only include argument length (characters) as regression feature. M1a arguments include structure presence features, M1b content presence features, and M1c includes bot structure and content presence features. M2 LASSO models select from more granular, sequential structure and content features as detailed in Section 4.2. (R2 = 0.71) and restructured (R2 = 0.57). We attribute this pattern to the bimodal length distribution under faithful synthesis. Inspection of the length histograms (Figure 5) reveals that faithful synthesis exhibits high variance: arguments cluster into two modes: one resembling the compact outputs of strict synthesis, the other including one or two additional concluding sentences. This behavioral inconsistency introduces substantial length variation that strongly correlates with quality judgments. In contrast, strict and restructured synthesis produce more unimodal length distributions centered on their respective means. Notably, the length-only baseline (M0) achieves R2 = 0.72 for faithful synthesis, in contrast to approximately 0.3 for both strict and restructured synthesis. This highlights the importance of controlling for length when developing causal estimates in future work. C.2.3 Supporting Figures Figure 6 shows the cross-validation curve used to select the LASSO regularization parameter. Figure 7 shows which feature categories the LASSO model selects across synthesis types. Figure 8 and Figure 9 display the top 20 features by coefficient magnitude. 40 Figure 4: Predictability of argument quality from controller actions and argument length. (A) Cross-validation R2 across model variants: M0 (argument length), M1a (structure presence only + argument length), M1b (content presence only + argument length), M1c (both + argument length), and M2 (full sequential model with position effects and transitions + argument length). (B) Held-out test set performance with 95% bootstrap confidence intervals. The sequential model consistently outperforms presence-based baselines, with the largest absolute predictability under strict synthesis. Figure 5: Distribution of argument lengths (characters) across synthesis modes. Strict and restructured synthesis produce relatively unimodal distributions, while faithful synthesis exhibits higher variance with bimodal tendencysome arguments remain compact while others include additional concluding material. We removed exact duplicates, reducing the number of samples to N=4,994 for strict synthesis. 41 Figure 6: Cross-validation R2 as function of LASSO regularization parameter α. Stars indicate the selected α for each synthesis type. Figure 7: Distribution of LASSO-selected features by category. Position effects and transitions within each dimension (structure, content) contribute to predictions across all synthesis types. 42 Figure 8: Top 20 features by absolute LASSO coefficient for each synthesis type. Positive coefficients indicate patterns associated with higher persuasiveness scores; negative coefficients indicate patterns associated with lower persuasiveness scores. 43 Figure 9: Top 20 features by absolute LASSO coefficient for each synthesis type for M2 models including argument length. Positive coefficients indicate patterns associated with higher persuasiveness scores; negative coefficients indicate patterns associated with lower persuasiveness scores. 44 Figure 10: Length distributions for targeted trajectory evaluation for strict synthesis. Left: All arguments before length matching, including 250 targeted arguments (generated from M2s top-50 predicted trajectories) and the 250 arguments of each baseline method. The original top-5% arguments skew longer, reflecting the correlation between length and judged quality.Right: Lengthmatched subset used for evaluation. Greedy pairing within 5 characters produces groups with comparable length distributions, enabling fair comparison. Dashed lines indicate group means. 45 Figure 11: Length distributions for targeted trajectory evaluation for faithful synthesis. Left: All arguments before length matching, including 250 targeted arguments (generated from M2s top-50 predicted trajectories) and the 250 arguments of each baseline method. The original top-5% arguments skew longer, reflecting the correlation between length and judged quality.Right: Lengthmatched subset used for evaluation. Greedy pairing within 5 characters produces groups with comparable length distributions, enabling fair comparison. Dashed lines indicate group means. 46 Figure 12: Length distributions for targeted trajectory evaluation for restructured synthesis. Left: All arguments before length matching, including 250 targeted arguments (generated from M2s top-50 predicted trajectories) and the 250 arguments of each baseline method. The original top-5% arguments skew longer, reflecting the correlation between length and judged quality.Right: Length-matched subset used for evaluation. Greedy pairing within 5 characters produces groups with comparable length distributions, enabling fair comparison. Dashed lines indicate group means."
        },
        {
            "title": "D Action Spaces",
            "content": "This appendix lists the structured action templates used by STATe in each experiment suite. D.1 NoveltyBench action spaces NoveltyBench ToT sweeps use one or both of the following action spaces: experiments/noveltybench/ action_space/personalities.json and experiments/noveltybench/action_space/target_audiences. json. The both setting concatenates these spaces; uncontrolled uses no actions. Table 6: NoveltyBench action space: Personality (Big-5 traits). Name openness Definition Emphasizes creativity, intellectual curiosity, and preference for novelty over tradition. conscientiousness Emphasizes organization, self-discipline, reliability, and goal-oriented achievement. Prefix (none) (none) extraversion Emphasizes enthusiasm, assertiveness, sociability, and high energy. (none) agreeableness Emphasizes empathy, cooperation, warmth, and concern for others. (none) Internal reasoning Approach with curiosity and creativity; seek novel ideas; think abstractly and imaginatively. Be methodical and detail-oriented; work systematically toward clear goals; be thorough. Be energetic and confident; engage boldly; express thoughts with passion and optimism. Be empathetic and collaborative; consider stakeholders; seek harmony; show sincere concern. neuroticism Emphasizes caution, risk awareness, and sensitivity to potential problems. (none) Be cautious and attentive to risks; examine uncertainties; consider worst-case scenarios. Table 7: NoveltyBench action space: Target Audience (age demographics). Name children teenagers young_adults middle_aged Definition Writes for children ages 512 using simple language, examples, and enthusiasm. Writes for teenagers ages 1319 using relatable language, current trends, and engaging tone. Prefix (none) (none) Internal reasoning Use very simple words and short sentences; cheerful tone; fun, concrete examples. Use casual, relatable language; energetic tone; socially current examples. Writes for young adults ages 2035 using modern, direct language with practical examples. (none) Use clear, modern language; practical examples; confident, approachable tone. Writes for adults ages 3655 using professional, balanced tone with real-world applications. (none) Use professional, balanced tone; grounded examples; pragmatic framing. 48 Name seniors Definition Prefix Internal reasoning Writes for seniors (ages 56+) using clear, respectful, and warm language. (none) Use clear, respectful language; gentle pacing; thoughtfully explained examples. D.2 Argument generation action spaces Table 8: Argument generation action space: Subtopics (topical lenses). Name Definition Prefix Internal reasoning cost_benefit_and_ impact_analysis Weighs economic, social, and practical consequences systematically. (none) should quantify and compare costs, benefits, and real-world impacts across economic, social, and environmental dimensions. rights_and_ liberties justice_and_ fairness ethical_ principles Protects fundamental rights, freedoms, privacy, and individual autonomy. (none) should consider inalienable human rights, civil liberties, privacy protections, and the freedom to make ones own choices. Ensures equitable treatment, fair distribution, and equal opportunity. (none) should analyze whether outcomes, processes, and distributions are fair to all parties involved. Applies moral frameworks including duties, virtues, and care for others. (none) should evaluate actions based on moral rules, character virtues, relationships, and ethical obligations. governance_and_ accountability Examines rule of law, transparency, legitimacy, and institutional responsibility. (none) should consider legal frameworks, accountability mechanisms, democratic principles, and proper authority. risk_and_ unintended_ consequences Anticipates harms, unforeseen effects, and slippery slopes. (none) should identify risks, unintended outcomes, cascading effects, and potential for escalation. feasibility_and_ implementation Assesses practical workability, technical constraints, and enforcement challenges. incentives_and_ power_dynamics Analyzes how rewards, penalties, and power structures shape behavior. (none) should evaluate whether the proposal can actually be implemented and enforced effectively. (none) should examine what behaviors are encouraged, who holds power, and how interests align or conflict. precedent_and_ long_term_effects Considers precedents and future implications across generations. (none) stakeholder_ responsibility Clarifies duties of government, individuals, and institutions. (none) should evaluate historical precedents, long-term vs short-term tradeoffs, and obligations to future generations. should analyze who bears responsibilitywhether government, individuals, corporations, or other institutions. 49 Table 9: Argument generation action space: Structures (discourse moves). Name Definition Prefix Internal reasoning causal_reasoning States causes, effects, consequences, or logical implications. Therefore (none) conditional Introduces conditional, hypothetical, or counterfactual scenarios. If (none) concession_and_ contrast Acknowledges counterpoints or highlights opposing perspectives. However (none) addition_and_ elaboration evidence_and_ authority exemplification Adds supporting information, elaborates, or strengthens point. Moreover (none) Cites evidence, data, or authoritative sources. Evidence shows that (none) Provides concrete examples, illustrations, or case studies. For example (none) clarification_ and_specification Restates, clarifies, defines, or narrows down to specifics. In other words (none) emphasis_and_ evaluation sequence_and_ transition Stresses importance or offers evaluative judgment. Importantly (none) Signals progression through steps or shifts to new topic. Next (none) conclusion_and_ summary Summarizes, concludes, or states the practical takeaway. In conclusion (none) D.3 Practitioner Guidance for Action Space Design Designing an effective action space is one of the most consequential choices when applying STATe to new domain. Below, we outline key decision points, trade-offs, and recommendations based on our experience across the experiments in this paper. 1. Identify controllable dimensions. Begin by enumerating the aspects of generation that can be meaningfully controlled at each step. These typically fall into categories such as content (what to say), structure (how to organize it), style (tone, register, formality), and strategy (rhetorical or reasoning approach). Where possible, ground dimensions in existing domain taxonomiesfor example, Wachsmuth et al. [109] for argumentation structure, Dong et al. [25] and Didolkar et al. [24] for math, or other established reasoning taxonomies for reasoning-intensive tasks. 2. Sequential vs. single-step: Should dimensions vary per step? If the task involves multi-step generation (e.g., constructing an argument claim-by-claim), the action space should allow different choices at each step. For example, in argument generation, content strategy and structural choices naturally vary per claim. In contrast, some features of interest, such as target audience, argument topic, and stance, could be varied at the tree level rather than at the per-step level for trees with reasoning depth greater than one. 3. Prefix vs. internal reasoning. STATe supports two mechanisms for injecting action guidance into the generator: prefix (pre-filled text that begins the generation) and internal reasoning (guidance 50 injected into the system prompt or context). Only one dimension can use prefix, since the prefix occupies fixed position in the generated text. All dimensions can use internal reasoning. If more than one dimension includes internal reasoning, we concatenate them one after the other. In practice, the structural or discourse dimension benefits most from prefix control, since it directly shapes the opening of each generation step (e.g., First, will present counterexample...). 4. Early stopping (finish action). Including finish action allows the controller to terminate generation early, preventing overthinking or redundant steps. However, variable-length trajectories complicate attribution: trajectories of different lengths have different numbers of positional features, and shorter trajectories may systematically differ from longer ones for reasons unrelated to the actions chosen. Application-specific tuning. Both the choice of synthesis mode and the action space itself may require domain-specific exploration. The recommendations above provide starting points, but practitioners should expect to iterate on action definitions and synthesis settings based on early experimental results."
        },
        {
            "title": "E Prompt Templates",
            "content": "This appendix provides the exact prompt templates used in our implementation for the generator component. E.1 Generator System Prompt (Vanilla) The following template is used when generating reasoning steps without controller-guided internal reasoning: Listing 1: Generator System Prompt (Vanilla Mode) # Instructions { task _ instructions } { field _ descriptions } When solving this problem , you must break down your solution into series of reasoning steps , followed by final answer . Each step towards the answer should be encased within < step >... </ step > tags , and contain { reasoning _ field _ name } that advances the solution towards producing { output _ fields }. { final _ output _ description } Your reasoning process should follow the rules below : - Each { reasoning _ field _ name } ( of type { reasoning _ field _ type } ) entails { reasoning _ field _ description }.{ thought _ length _ instruction } { response _ length _ instruction } ## Response Format Once user provides { input _ fields } , your response must follow this template : < thinking > < step > ## { reasoning _ field _ name } The first reasoning step towards producing { output _ fields } </ step > < step > ## { reasoning _ field _ name } The second reasoning step towards producing { output _ fields } </ step > ... < step > ## { reasoning _ field _ name } The final reasoning step towards producing { output _ fields } </ step > </ thinking > < answer > { output _ field _ sections } </ answer > E.2 Generator System Prompt (With Internal Reasoning) When the controller provides internal reasoning guidance, we use an enhanced template: Listing 2: Generator System Prompt (Internal Reasoning Mode) # Instructions { task _ instructions } { field _ descriptions } When solving this problem , you must break down your solution into series of reasoning steps , followed by final answer . Each step towards the answer should be encased within < step >... </ step > tags , and contain { reasoning _ field _ name } that advances the solution towards producing { output _ fields }. { final _ output _ description } Your reasoning process should follow the rules below : - Each { reasoning _ field _ name } ( of type { reasoning _ field _ type } ) entails { reasoning _ field _ description }. - Before writing new { reasoning _ field _ name } , start with some internal reasoning which discusses and guides what to do with the next { reasoning _ field _ name } .{ thought _ length _ instruction } { response _ length _ instruction } ## Response Format Once user provides { input _ fields } , your response must follow this template : < thinking > < step > ## internal _ reasoning Your internal reasoning about the first { reasoning _ field _ name } ## { reasoning _ field _ name } The first reasoning step towards producing { output _ fields } </ step > < step > ## internal _ reasoning Your internal reasoning about the second { reasoning _ field _ name } ## { reasoning _ field _ name } The second reasoning step towards producing { output _ fields } 52 </ step > ... </ thinking > < answer > { output _ field _ sections } </ answer > E.3 Final Output Synthesis Modes We support multiple modes for synthesizing the final answer from reasoning steps: Strict Synthesis Preserves exact wording from reasoning steps: Your final answer must include the full text from all reasoning steps , copied nearly word - for - word and in sequential order . Listing 3: Final Output Instruction (Strict) - Preserve the exact wording , phrasing , structure , and examples . - Maintain the original order and logical flow exactly as provided . - You may add only : brief introduction / conclusion , short transitions . - Do NOT rewrite , paraphrase , summarize , or restructure . - Do NOT add new ideas , arguments , facts , or examples . Faithful Synthesis Allows light rephrasing while preserving meaning: Your final answer must remain highly faithful to the reasoning steps . Listing 4: Final Output Instruction (Faithful) - Preserve the full set of reasoning steps and their original order . - You may lightly rephrase for clarity , but meaning must remain unchanged . - Structure and sequence should closely follow the original . - Do NOT introduce new ideas or significantly alter existing reasoning . Restructured Synthesis Allows light restructuring to produce the best final answer: Listing 5: Final Output Synthesis (Restructured) Your final answer should preserve the same core ideas and reasoning from the steps provided , while improving clarity and coherence . - Maintain the essential arguments and logical intent . - You may rephrase , reorganize , and restructure the content for better flow and readability . - The overall set of ideas should remain the same , but the presentation may differ . - Do NOT introduce new ideas or factual content beyond what appears in the reasoning steps . Your goal is to produce well - structured synthesis that faithfully reflects the original reasoning while optimizing expression and organization . 53 Conclusion Synthesis Treats reasoning as internal guidance only, producing standalone answer: Listing 6: Final Output Instruction (Conclusion) Your final answer must be standalone response to the user task and instructions . - Focus on producing clear , logically consistent , and high - quality final answer . - You are not required to preserve the structure , wording , or order of the reasoning steps ( between < thinking >... </ thinking > tags ) . - Use the reasoning steps only as internal guidance ; do NOT mention them or refer to them . - The user will * not * have access to the reasoning steps you wrote , so referencing them is confusing and unhelpful . The user will only see what you write between < answer >... </ answer > tags . - Do NOT explain what you are going to do ; just produce the final deliverable . While reasoning was meant for planning , the final output should be standalone response to the user task and instructions . - If the task requires strict formatting ( math , formatting specifications for text , code , etc .) , follow those requirements exactly in the final output . Your goal is to output only the final answer content that satisfies the user instructions ."
        }
    ],
    "affiliations": [
        "Hebrew University of Jerusalem",
        "Princeton University",
        "Technion Israel Institute of Technology"
    ]
}