{
    "paper_title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
    "authors": [
        "Seanie Lee",
        "Sangwoo Park",
        "Yumin Choi",
        "Gyeongman Kim",
        "Minki Kang",
        "Jihun Yun",
        "Dongmin Park",
        "Jongho Park",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git."
        },
        {
            "title": "Start",
            "content": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Seanie Lee * 1 Sangwoo Park * 1 Yumin Choi 1 Gyeongman Kim 2 Minki Kang 1 Jihun Yun 2 Dongmin Park 2 Jongho Park 3 Sung Ju Hwang 1 4 1KAIST AI 2KRAFTON 3UC Berkeley {lsnfamily02, swgger}@kaist.ac.kr 4DeepAuto.ai 6 2 0 J 0 3 ] . [ 1 3 4 1 3 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces distributional discrepancy that degrades native reasoning. We propose THINKSAFE, self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. THINKSAFE unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Finetuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1Distill and Qwen3 show THINKSAFE significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git. 1. Introduction By scaling test-time compute (Snell et al., 2025) and leveraging chain-of-thought (CoT; Wei et al., 2022), large reasoning models (LRMs) demonstrate exceptional proficiency in solving complex tasks, from mathematical problem solving to code generation. Recent advancements have further amplified these capabilities by effectively post-training models *Equal contribution 1 Figure 1. Safety & reasoning performance of the Qwen3 family. via reinforcement learning (RL), such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). By utilizing verifiable rewards, these approaches enable models to generate long reasoning traces and tackle intricate logical challenges. However, this excessive optimization for reasoning often incurs safety tax (Huang et al., 2025), term describing the trade-off where enhanced reasoning capabilities come at the expense of safety alignment. While modern large language models (LLMs) typically undergo initial safety training, subsequent post-training stages focused on maximizing reasoning performance, such as RL on mathematical or coding benchmarks, can actively degrade these established guardrails (Qi et al., 2024). For instance, Li et al. (2025a) demonstrate negative correlation between reasoning capabilities and safety, suggesting that models optimized purely for complex problem solving may learn to bypass safety constraints to maximize reward signals. Consequently, the critical challenge is to mitigate this regression: how can we effectively restore safety alignment in reasoning-intensive models without sacrificing the advanced problem-solving capabilities gained during post-training? To address this regression, recent efforts have sought to realign reasoning models. Approaches such as SafeChain (Jiang et al., 2025) and STAR-1 (Wang et al., 2025) typically rely on external supervision by distilling safe responses, along with reasoning traces, from larger teacher models to override unsafe behaviors. However, these methods introduce fundamental limitation: distributional discrepancy. When student model is forced to mimic THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Figure 2. THINKSAFE employs refusal steering to guide the student model. This mechanism unlocks the students latent safety capabilities to generate valid reasoning traces, resulting in responses that are both safe and in-distribution. the reasoning style of an external teacher, the training data inevitably deviates from the students internal distribution. As result, these approaches often face severe trade-off where they either struggle to effectively internalize safety constraints or suffer from degradation in their native reasoning capabilities constructed during post-training. natural alternative to avoid this distribution shift is selfdistillation (Furlanello et al., 2018), where the model generates its own training data. While this approach keeps the data in-distribution, it faces hurdle illustrated in the middle row of Fig. 2. The models strong optimization for instruction compliance often suppresses its safety mechanisms. Consequently, the model struggles to generate valid refusal traces for harmful prompts, instead yielding responses that are technically in-distribution but fundamentally unsafe. One potential alternative to mitigate this distributional discrepancy is online RL. By actively sampling from the models current policy and updating based on safety reward, online RL ensures that training data remains strictly on-policy, thereby avoiding the shift introduced by external teachers. Furthermore, recent work (Shenfeld et al., 2025) suggests that this on-policy RL better retains prior knowledge than supervised fine-tuning when adapting to new task. However, the requirement for continuous online sampling incurs prohibitive computational cost. In this work, we propose THINKSAFE, framework for self-generated safety alignment that serves as computationally efficient middle ground. It restores safety using student-generated data without the overhead of online RL. Our key insight is that although the models intense focus on instruction compliance suppresses its safety mechanism, we hypothesize that it frequently preserves the latent knowledge required to identify harm. Specifically, as illustrated in the bottom row of Fig. 2, we prepend specific refusaloriented instruction (e.g., The following prompt is harmful. You should refuse to answer the prompt.) to steer the student model to generate its own safety reasoning traces for harmful prompts. For benign prompts, we utilize direct sampling without additional instructions to preserve the models native helpfulness. Because these training samples are generated by the student model itself, rather than an external teacher, they significantly minimize the distribution shift, allowing us to realign the model for safety while preserving the structural integrity of its reasoning capabilities. As shown in Fig. 1, we validate THINKSAFE across the Qwen3 and DeepSeek-R1-Distill families, where it consistently yields the most favorable safety-reasoning balance. Unlike teacher-distillation baselines, which often incur sharp trade-off between ensuring safety and maintaining reasoning capabilities, our method preserves native capabilities while robustly mitigating harmfulness. Moreover, THINKSAFE significantly outperforms naive self-distillation via rejection sampling, which struggles to generate safe responses. Crucially, our approach achieves superior safety compared to the online RL baseline (GRPO) and maintains comparable reasoning performance, while reducing the computational cost by an order of magnitude. Our contribution can be summarized as follows: We show that external teacher supervision induces distribution shift that degrades the reasoning capabilities of student models when re-aligning them. We propose THINKSAFE, framework that steers models to generate their own safety alignment data, bypassing the need for external supervision. We demonstrate across diverse benchmarks that THINKSAFE consistently achieves the most favorable safety-reasoning trade-off, improving safety while maintaining near-original performance on reasoning tasks. 2. Related Works Safety risk of LRMs. CoT (Wei et al., 2022) improves performance by prompting models to generate explicit intermediate steps. Building on this foundation, recent advancements have scaled this capability by training LRMs via RL to generate long reasoning traces. However, excessive optimization in reasoning tasks compromises the safety alignment of these models. Li et al. (2025a) observe negTHINKSAFE: Self-Generated Safety Alignment for Reasoning Models ative correlation between reasoning capabilities and safety alignment, finding that models optimized purely for reasoning often bypass safety constraints. Furthermore, Huang et al. (2025) characterize this trade-off as safety tax, where safety alignment degrades the models reasoning capability. Safety-alignment of LRMs. To address the trade-off between safety and reasoning, recent efforts have moved beyond standard refusal training, such as DirectRefusal (Huang et al., 2025), which bypasses reasoning entirely, to focus on preserving the reasoning capabilities of LRMs. One primary direction focuses on the refinement of safety datasets to better match the reasoning style of LRMs. Specifically, SafeChain (Jiang et al., 2025) integrates structured reasoning steps into safety responses. Similarly, STAR-1 (Wang et al., 2025) employs larger teacher model to generate policy-guided reasoning traces and strictly filters for the top 1,000 examples. Alternatively, model-centric strategies seek to mitigate the degradation of reasoning by modifying training objectives to strengthen early safety signals (Zhou et al., 2025) or injecting lightweight safety cues to guide the reasoning trajectory (Jeung et al., 2025). However, these approaches largely rely on external supervision or teacherdistillation, which introduces distributional discrepancy that degrades the students general capabilities. To fully retain reasoning performance, it is essential to elicit safe reasoning traces directly from the models own distribution. Self-distillation. Self-distillation, where student trains on its own output, enhances generalization (Furlanello et al., 2018) via implicit regularization (Mobahi et al., 2020) and feature consolidation (Allen-Zhu & Li, 2023). Crucially, it mitigates catastrophic forgetting (Lee et al., 2023) and bridges fine-tuning distribution gaps by having the student model rewrite reference responses (Yang et al., 2024). However, applying this to safety alignment presents distinct challenges. The rewriting strategy of Yang et al. (2024) requires safe ground-truth references, which are unavailable for harmful queries without external supervision. Moreover, naive self-distillation fails as strong helpfulness priors suppress the generation of safe responses. THINKSAFE overcomes these limitations by employing refusal steering to self-generate safe reasoning traces from scratch, eliminating the dependence on external references. 3. Preliminaries Problem setup. Let pθ be language model capable of generating responses with long reasoning chains. We assume the model is post-trained to possess both general reasoning capabilities and safety alignment. However, it still fails to generate safe responses to harmful prompts. Let Dh = {x(i) i=1 be set of harmful prompts that bypass the models safety guardrails and elicit unsafe responses. Following previous works (Jiang et al., 2025), we define safe }n response as one that the safety guard model (Llama Team, 2024; Han et al., 2024; Lee et al., 2025) classifies as safe. Our goal is to improve the reasoning models robustness against harmful prompts while retaining its general reasoning capabilities. To achieve this, we generate safe response y(i) for each harmful prompt x(i) Dh. Additionally, we use set of benign instructions, Db = {x(i) i=1, and generate corresponding benign response y(i) for each prompt x(i) Db. We then train the model on the union of these prompt-response pairs. As is common practice in prior safety studies (Bianchi et al., 2024; Jiang et al., 2025), incorporating this benign data is crucial for mitigating the degradation of general instruction-following capabilities. }m Safety fine-tuning. Many previous works (Jiang et al., 2025; Wang et al., 2025; Zhou et al., 2025) primarily rely on larger teacher model pT to train the student model pθ. Specifically, given the union of harmful and benign prompt sets, = Dh Db, the teacher model generates response for each prompt D. The parameters θ are then optimized by minimizing the negative log-likelihood of only the safe samples from the teacher, as follows: ExD,ypT (x) [ log pθ(y x)1{ϕ(x, y) = 1}] (1) where ϕ is binary safety guard classifier that predicts whether the prompt-response pair is safe (with ϕ(x, y) = 1 denoting safe pair) and 1 is the indicator function. In practice, we sample responses for prompts from the teacher and retain only those deemed safe by the guard model ϕ, resulting in static dataset. The model is then fine-tuned on this static dataset. 4. Our Approach: THINKSAFE However, safety fine-tuning on samples from the teacher pT shifts the distribution of the student pθ, causing discrepancy that degrades general reasoning capabilities. Previously, this phenomenon was attributed to the small model learnability gap (Li et al., 2025b), under the assumption that the student struggles to imitate larger teachers reasoning traces due to limited capacity. Crucially, we observe this degradation persists even when using teacher model of similar size to the student as shown in Fig. 6. To bridge this distributional gap, we propose THINKSAFE, self-generation framework that leverages the student models intrinsic reasoning capabilities to synthesize safe responses to the harmful prompt x(i) Dh and helpful response to the benign prompt x(i) Db. By generating data within the models own distribution, THINKSAFE reduces the distributional gap, effectively mitigating the degradation of general reasoning capabilities during safety alignment. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Motivation. While on-policy learning (Ross et al., 2011; Agarwal et al., 2024; Gu et al., 2024) offers theoretical solution to the distribution shift, practical implementation faces hurdle. naive student model often prioritizes helpfulness over safety, failing to generate valid refusal traces for harmful queries. We hypothesize that while student possesses the latent capacity to reason about safety, this capability is suppressed by its instruction-following priors. Therefore, our method aims to explicitly unlock and record this latent safety reasoning, creating dataset that is both safety-aligned and native to the students distribution. Data generation via refusal steering. To elicit these onpolicy safety traces, we introduce refusal-oriented instruction (Irefusal). We define Irefusal as specific instruction (e.g., The following prompt is harmful. You should refuse to answer the prompt.) designed to override the student models default compliance behavior. Then we sample pθ( Irefusal, x(i) y(i) ). Without this intervention, the conditional distribution pθ( x(i) ) is dominated by compliant, unsafe responses to harmful prompts. By prepending Irefusal to each harmful prompt x(i) Dh, we shift the generation probability mass toward safety-aligned reasoning paths. This effectively steers the model to articulate the rationale behind refusal, converting its latent safety knowledge into explicit, trainable reasoning chains. Benign response generation. Conversely, for benign instructions x(i) Db, we want to maintain the student models general reasoning capability by strictly adhering to its native distribution. As we observe in Fig. 6, fine-tuning on data sampled from an external teacher model, even one of the same size as the student model introduces distributional discrepancy between the teacher and the student that disrupts the students distribution. To eliminate this mismatch, we directly sample responses from the student without any additional instruction, i.e., y(i) pθ( x(i) ). By relying on self-generated data, we ensure that benign training samples remain perfectly aligned with the students intrinsic reasoning patterns, thereby mitigating the degradation associated with fitting to different distribution. Filtering. To ensure the integrity of the generated data, following the previous work (Jiang et al., 2025), we utilize safety guard model, Llama-Guard-3-8B (Llama Team, 2024), to filter the synthesized responses. Only traces verified as strictly safe are admitted, thereby establishing safe reasoning trajectories for both harmful and benign prompts. Training. We begin with student model pθ and two datasets: harmful prompts Dh and benign instructions Db. We initialize reference model pref as frozen copy of the student to generate training targets. For harmful prompts, we apply refusal steering to sample yh pref( Irefusal, xh), whereas for benign prompts, we sample directly from the native distribution yb pref( xb). After filtering these responses with the safety guard ϕ, the student model parameters θ are optimized to minimize the negative log-likelihood of the valid traces: ExhDh yhπh [ℓsafe(xh, yh)] + ExbDb ybπb [ℓsafe(xb, yb)] , (2) where πh = pref( Irefusal, xh), πb = pref( xb), and ℓsafe(x, y) = log pθ(y x)1{ϕ(x, y) = 1}. In practice, rather than performing online updates, we approximate this objective by merging the filtered prompt-response pairs into single static dataset and fine-tuning the model on it. 5. Experiment 5.1. Experimental Setup Implementation details. Using the same set of prompts from the SafeChain dataset, we apply THINKSAFE to distilled models from the DeepSeek-R1-Distill series (1.5B to 8B) (Shao et al., 2024) and the Qwen3 family (0.6B to 8B) (Yang et al., 2025). Details regarding our generated dataset are provided in Sec. A.2. Based on the findings that LoRA (Hu et al., 2022) effectively preserves models intrinsic capabilities after fine-tuning (Biderman et al., 2024; Xue & Mirzasoleiman, 2025), we adopt LoRA with rank = 32, scaling factor α = 16 and dropout rate (Srivastava et al., 2014) of 0.05 to the query and value projections for all experimental configurations. For optimization, we use AdamW (Loshchilov & Hutter, 2019) with base learning rate of 1 105 and cosine scheduler with linear warmup over the first 10% of training steps. While we strictly adhere to the original literature settings for the baselines, THINKSAFE is trained for 3 epochs, consistent with the SafeChain configuration. All experiments are conducted with total batch size of 8 and executed on 2 NVIDIA H100 GPUs. Datasets. We use four challenging benchmarks to assess the extent to which the models retain their reasoning proficiency: GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2024), AIME24 (Zhang & Math-AI, 2024), and GPQA (Rein et al., 2024). We sample 8 responses for each prompt and compute average pass@1 using SkyThought (Team, 2025). More details are in Sec. B. For safety, we evaluate safety alignment of models across four benchmarks: StrongReject (Souly et al., 2024), HarmBench (Mazeika et al., 2024), WildJailbreak (Jiang et al., 2024), and XSTest (Rottger et al., 2024). For StrongReject, HarmBench, and WildJailbreak, we sample single response for each prompt, evaluate the harmfulness of the response with Llama-Guard-3, and report the ratio of harmful responses. For XSTest, we specifically evaluate models on the safe prompt subset to monitor over-refusal, assessing the refusal rate using WildGuard (Han et al., 2024). 4 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Table 1. Results on Qwen3 models. We evaluate safety across three benchmarks (HarmBench, StrongReject, WildJailbreak) by reporting the ratio of harmful responses (). Over-refusal is measured by the refusal rate () on benign XSTest prompts. For reasoning tasks, we sample 8 trajectories per prompt and report the average pass@1 (). Best results are bolded; second best are underlined. Safety () Harmfulness Over-refusal Size Method Harm Bench Strong Reject Wild Jailbreak XSTest Reasoning (Avg pass@1, ) Safety Average AIME 2024 GSM8k MATH GPQA Reasoning Average 0.6B 1.7B 4B 8B Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE 68.44 43.85 58.64 56.64 67.61 60.96 40.37 52.66 38.54 47.34 37.38 54.15 46.84 28. 38.21 33.06 43.69 33.72 37.71 32.39 9.63 35.05 24.42 41.20 24.42 35.22 26.91 9.14 66.45 11.82 72.84 38.02 60.06 48.88 33.87 36.10 5.75 57.51 7.67 36.42 18.21 9.58 8.31 3.19 41.21 5.75 7.35 3.19 0.32 4.47 1.92 38.95 1.28 6.71 4.79 0. 52.80 36.30 49.60 50.60 52.80 52.75 37.95 51.10 35.75 43.85 46.60 49.30 48.85 29.20 43.00 36.20 39.65 35.15 42.45 32.95 7.45 38.35 28.05 36.42 29.25 39.45 28.80 7.35 5.20 83.60 0.00 22.40 4.40 18.40 6.40 1.20 61.60 1.60 10.80 1.20 8.80 2. 0.80 32.00 2.00 6.80 1.60 0.80 2.80 0.40 37.60 1.20 6.80 1.20 8.80 1.20 48.22 43.89 45.20 41.92 46.22 45.25 29.65 35.27 35.41 37.58 25.61 35.27 30.68 17.38 22.58 29.80 31.64 20.36 22.28 17.33 5.05 19.57 23.00 29.44 15.44 20.64 17.33 4. 10.42 5.83 4.58 6.25 7.92 5.42 9.58 44.58 43.75 34.58 46.25 43.33 38.33 44.17 67.50 68.33 62.08 62.50 72.08 67.08 73.33 74.17 74.58 70.00 72.50 74.58 70.00 72.92 72.51 64.30 68.68 68.15 71.26 71.58 72.36 84.31 82.78 85.29 84.38 84.33 84.31 83. 84.69 82.58 89.59 90.97 84.45 91.79 88.06 85.28 84.31 92.98 90.29 84.89 92.44 88.00 71.73 67.53 62.42 68.17 71.77 66.17 70.65 88.85 88.10 85.72 88.30 88.32 88.12 89.05 93.43 93.20 93.03 93.05 93.33 92.87 93.53 94.18 93.63 93.53 93.73 93.85 93.30 93. 25.13 24.81 23.74 24.18 26.07 24.94 23.30 41.73 41.29 38.13 41.16 42.42 40.03 40.53 52.27 53.03 51.01 51.96 53.54 51.83 53.79 50.69 59.41 58.21 57.83 61.24 59.91 59.67 44.95 40.62 39.86 41.69 44.26 42.03 43.97 64.87 63.98 60.93 65.02 64.60 62.70 64. 74.47 74.29 73.93 74.62 75.85 75.89 77.18 76.08 77.98 78.68 78.59 78.64 78.91 78.50 Baselines. We compare THINKSAFE against the following competitive fine-tuning based baselines, following their original training settings; detailed baseline-specific configurations are provided in Sec. C. DirectRefusal (Huang et al., 2025) adds fixed thinking trajectory (I should not answer this question!) into existing refusal responses to harmful prompts. Training models on this dataset enforces immediate refusals to harmful prompts, bypassing extended reasoning. SafeChain (Jiang et al., 2025) distills both the intermediate reasoning chain and the final response from larger teacher model, DeepSeek-R1. STAR-1 (Wang et al., 2025) leverages larger teacher model to generate policy-guided reasoning traces. It employs an LLM-as-a-judge to select the top 1,000 examples, which are then used for training. SafePath (Jeung et al., 2025) injects safety cue (Lets think about safety first) at the beginning of reasoning, leaving the remainder of the generation unsupervised. SafeKey (Zhou et al., 2025) uses the same dataset as STAR-1 but employs auxiliary loss to enhance safety generalization. Specifically, it introduces objectives such as dual-path safety head to strengthen safety signals in internal representations early in the reasoning process. 5.2. Main Results Superiority of THINKSAFE. Our main results, summarized in Table 1 and Table 2, demonstrate that THINKSAFE consistently achieves the most favorable trade-off between safety and reasoning. Ranging from the Qwen3 family to DeepSeek-R1-Distill models, our method significantly enhances robustness while retaining, and often improving, native reasoning capabilities. For instance, on Qwen3-4B, THINKSAFE drastically reduces harmfulness on the HarmBench score from 38.21 to 9.63 while simultaneously boosting average reasoning accuracy from 74.47 to 77.18. We observe similar trend in the DeepSeek-R1-Distill family. On the 1.5B model, our method improves the overall reasoning score from 53.77 to 57.30 while reducing average 5 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Table 2. Results on DeepSeek-R1-Distill models. We evaluate safety across three benchmarks (HarmBench, StrongReject, WildJailbreak) by reporting the ratio of harmful responses (). Over-refusal is measured by the refusal rate () on benign XSTest prompts. For reasoning tasks, we sample 8 trajectories per prompt and report the average pass@1 (). Best results are bolded; second best are underlined. Safety () Harmfulness Over-refusal Size Method Harm Bench Strong Reject Wild Jailbreak XSTest Reasoning (Avg pass@1, ) Safety Average AIME 2024 GSM8k MATH 500 GPQA Reasoning Average 1.5B 7B 8B Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE 67.28 66.11 59.30 62.79 65.28 58.80 52. 56.98 52.33 51.00 52.99 55.15 45.35 40.20 52.33 32.39 44.52 21.26 47.51 32.72 27.08 82.11 82.75 76.68 77.00 82.43 73.16 74.12 63.58 33.55 54.63 47.92 64.86 33.87 41.85 53.99 0.64 46.33 3.51 51.44 11.82 26.52 51.55 50.45 46.95 49.65 51.80 47.65 40. 53.15 50.20 45.85 48.75 52.65 45.75 35.40 49.70 32.60 42.45 17.60 50.20 30.85 21.15 0.00 8.40 0.40 1.20 0.40 3.60 1.20 1.20 43.60 0.40 2.40 0.00 7.20 0.40 0.40 50.00 1.60 12.00 0.40 8.00 1.60 50.23 51.93 45.73 47.66 49.98 45.80 42. 43.73 44.92 37.97 38.02 43.16 33.04 29.46 39.10 28.91 33.72 13.59 37.39 20.85 19.09 21.25 19.17 24.17 17.08 24.17 19.58 32.92 49.58 47.50 49.17 45.83 52.08 43.75 51.25 47.50 40.00 41.67 40.42 43.33 35.83 48.75 82.42 81.06 80.47 81.38 82.37 81.25 82. 90.32 88.27 89.75 90.32 89.71 90.58 90.10 87.74 83.26 86.06 87.28 87.45 87.41 87.55 79.45 78.55 81.25 79.07 79.57 78.02 82.50 90.18 89.82 91.50 90.58 90.62 89.90 91.90 87.38 85.00 86.50 86.65 87.43 85.80 87.70 31.94 32.70 28.28 31.25 32.51 28.72 31. 46.65 44.95 46.78 46.02 46.15 47.29 45.20 48.11 43.50 42.05 43.69 47.41 42.49 46.28 53.77 52.87 53.54 52.20 54.66 51.89 57.30 69.18 67.64 69.30 68.19 69.64 67.88 69.61 67.68 62.94 64.07 64.51 66.41 62.88 67.47 harmfulness from 50.23 to 42.20. Even on larger models like Qwen3-8B and DeepSeek-R1-Distill-8B, THINKSAFE cuts average harmfulness scores by more than half (e.g., 19.57 4.50 on Qwen3-8B) without the reasoning penalties. This validates that training on self-generated samples allows models to internalize safety constraints as part of their native problem-solving process. Failure of teacher-distillation methods. Baselines that rely on external teacher models, specifically SafeChain, STAR-1, and SafeKey, exhibit inconsistent performance and frequently degrade general reasoning capabilities. This confirms our hypothesis that forcing student to imitate external reasoning traces creates harmful distribution shift. This degradation is most severe in smaller or distilled models. For example, on Qwen3-0.6B, SafeChain causes the average reasoning score to drop to 39.86 compared to the initial 44.95. Similarly, on Qwen3-1.7B, SafeChain drops reasoning performance to 60.93 from an initial 64.87. The DeepSeek-R1-Distill-8B model further highlights this vulnerability. While the initial model achieves reasoning average of 67.68, teacher-based methods cause significant regression, with SafeKey dropping to 62.88, SafeChain to 64.07, and STAR-1 to 64.51. These results suggest that while external supervision can enforce safety, it disrupts the students fragile chain-of-thought capabilities, whereas THINKSAFEs self-generated approach preserves them. Limitations of superficial alignment. Approaches that bypass or loosely constrain the reasoning process also fail to yield optimal results. As noted above, DirectRefusal suffers from severe over-refusal and significant reasoning penalties. For instance, on Qwen3-0.6B, it degrades average reasoning to 40.62 while exhibiting an extreme refusal rate of 83.6 on benign XSTest prompts. By short-circuiting the reasoning process, DirectRefusal prevents the model from leveraging its latent capacity to think through safety constraints. Similarly, SafePath often struggles to achieve robust safety alignment despite being less destructive to reasoning. On Qwen3-1.7B, SafePath achieves an average harmfulness score of only 46.62, failing to reduce harmfulness as effectively as THINKSAFE (22.51). This suggests that mere cues are insufficient to override the models compliance priors. Explicit reasoning traces are necessary to robustly steer generation toward safety. 5.3. Comparison with RL GRPO baseline. We compare THINKSAFE against an online RL baseline trained via GRPO (Shao et al., 2024). In this setup, the student model pθ, initialized from Qwen30.6B, generates rollouts pθ( x) for prompts and is optimized using combined objective: safety reward rsafety(x, y) [0, 1] derived from the safety guard model ϕ and format reward rformat(x, y) {0, 1}. Further 6 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Figure 3. Comparison of THINKSAFE with online RL; GRPO. Figure 5. Ratio of reasoning gain to safety gain for student models trained on data generated by teachers from the same model family. Figure 4. Ablation of safety reasoning in R1 model series. details on the GRPO objective, reward mechanisms, and hyperparameters are provided in Sec. E. KL divergence and THINKSAFE + DKL. To enable fair comparison with GRPOs backward KL regularization, we introduce THINKSAFE + DKL. Recognizing that standard cross-entropy minimizes forward KL, we replace the standard loss for benign responses only with token-wise, full-vocabulary forward KL divergence between the reference and student models. This allocates the KL computation specifically to preserving the models native distribution on safe queries, offering closer structural analogue to GRPO within our self-generation framework. Results. As shown in Fig. 3, THINKSAFE delivers superior balance of safety and efficiency compared to the online RL baseline, GRPO. While GRPO achieves slight improvement in reasoning performance, it incurs prohibitive computational cost, requiring over 21 hours of training time, approximately 8 times slower than our method. Although our reported time includes data generation for fair comparison with online RL methods, the additional cost remains marginal. Thus THINKSAFE retains substantial efficiency advantage. Moreover, THINKSAFE significantly outperforms GRPO in safety by reducing harmfulness to 29.6% compared to 37.0% with only negligible drop in reasoning capabilities. Furthermore, the introduction of THINKSAFE + DKL effectively bridges this gap. It further suppresses harmfulness to 26.4% while recovering reasoning performance to 45.5%, matching GRPO under much lower training cost. These results demonstrate that our offline dataset with self-generated refusal steering enables robust alignment more effectively and efficiently than computationally intensive online RL. Figure 6. Safety and reasoning performance gain using different family of teacher model with similar size. 5.4. Ablation Studies Necessity of safety reasoning. Previous work such as SafeChain observed that suppressing reasoning during inference can enhance safety by preventing the model from generating unsafe thoughts that lead to harmful outputs. To investigate whether this observation translates to model training, we conduct an ablation study on the DeepSeek-R1Distill families (See Qwen3 results in Fig. 11 from Sec. D). We construct w/o reasoning dataset where reasoning traces are stripped from refusal responses yh, while benign responses yb retain their full CoT. Contrary to the inference-time findings of SafeChain, our experiments in Fig. 4 demonstrate that stripping safety reasoning significantly degrades both safety and reasoning performance. Removing reasoning leads to sharp increase in harmful responses (e.g., 7B: 29.5 44.4, 8B: 19.1 33.7). This suggests that training model to bypass reasoning prevents it from internalizing robust refusal mechanisms. Furthermore, general reasoning capabilities are also compromised when safety reasoning is removed. For instance, the DeepSeek-R1-Distill-8B model exhibits degradation in reasoning proficiency, where the average pass@1 score declines from 67.5 to 64.1. We attribute this to the inconsistent optimization objective: forcing the model to switch between thinking (for benign tasks) and not thinking (for safety tasks) destabilizes the models intrinsic chain-ofthought patterns. Thus, explicit safety reasoning is essential not just for safety alignment, but for preserving the models overall reasoning capabilities Refusal steering with external teachers. To demonstrate the necessity of self-generated data, we investigate the impact of using external teacher models for refusal steering. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Figure 7. We evaluate models trained on safety data generated via standard rejection sampling versus THINKSAFE. Figure 8. Perplexity of generated safety dataset measured by the initial student models. We measure the trade-off between reasoning capability and safety alignment as ratio of reasoning gain to safety gain after training. Using Qwen3-0.6B and DeepSeek-R1-Distill1.5B as student models, we generate safety data using larger teachers within the same model family (Qwen3-1.7B/4B/8B and DeepSeek-R1-Distill-7B/8B, respectively). As illustrated in Fig. 5, relying on larger teachers frequently improves safety while degrading reasoning capability. In the case of the Qwen3-0.6B student, using external teachers results in significant reasoning loss, whereas the selfgenerated THINKSAFE data exhibits the least reasoning degradation. Furthermore, for the DeepSeek-R1-Distill1.5B student, while the 8B teacher yields marginal positive reasoning gain, it is significantly outperformed by the self-generated approach, which demonstrates substantial improvement in reasoning capabilities. To isolate the effect of distributional discrepancy from model capacity, we conduct cross-model distillation experiment using teachers of similar size but different architectures. We employ Qwen3-1.7B and DeepSeek-R1-Distill1.5B to generate safety data via refusal steering and use these datasets to train each other. As shown in Fig. 6, while cross-model training occasionally improves safety (e.g., DeepSeek-R1-Distill achieves 14.3% safety gain when trained on Qwen3-1.7B data), it consistently degrades reasoning performance (e.g., Qwen3-1.7B suffers 22.6% drop in reasoning when trained on DeepSeek-R1-Distill data). This confirms that even when the teacher model is of comparable size, the distributional shift introduced by an external model disrupts the students native reasoning capabilities, validating the superiority of THINKSAFEs selfgenerated approach. Necessity of refusal steering. To validate the role of refusal steering, we compare THINKSAFE against standard rejection sampling baseline where responses are sampled directly from the student model without the refusal-oriented instruction (Irefusal). Specifically, following the protocol of SafeChain (Jiang et al., 2025), we sample 5 responses for each prompt and retain the instruction only if all five generated responses are verified as safe, subsequently selecting one response at random. We then train the Qwen3 student models on these rejection-sampled datasets. As shown in Fig. 7, removing refusal steering renders the safety alignment ineffective. For example, on the Qwen38B model, naive rejection sampling results in harmful response ratio of 21.3%, which is statistically indistinguishable from the initial model (19.6%) and significantly worse than THINKSAFE (4.5%). We attribute this failure to the models strong instruction-following priors overshadowing its latent safety knowledge. Since the reasoning model Qwen3 tends to comply with harmful instruction, this strict filtering criterion likely discards the vast majority of useful training signals, leaving the model with only the easy safety examples it had already mastered. This confirms that refusal steering is critical for explicitly shifting the generation probability mass to elicit valid refusal responses with reasoning for difficult prompts. 5.5. Quantifying Distributional Discrepancy To quantify the distributional discrepancy, we evaluate the perplexity of the safety training datasets generated by each method using the initial, frozen student model as the evaluator. As illustrated in Fig. 8, THINKSAFE consistently achieves the lowest perplexity across all model sizes, significantly outperforming teacher-distilled baselines such as STAR-1 and SafeChain. For instance, on the Qwen3-1.7B model, THINKSAFE yields perplexity of 1.55 compared to 7.35 for STAR-1. This result suggests that our method effectively mitigates distributional discrepancy, as the selfgenerated safety data aligns closely with the students intrinsic distribution, whereas external teachers introduce highperplexity data that significantly deviates from the students distribution. 6. Conclusion In this work, we presented THINKSAFE, framework that reconciles the tension between reasoning capabilities and safety alignment by addressing the distributional discrepancy inherent in external teacher supervision. By leveraging lightweight refusal steering to unlock the models latent safety knowledge, our approach synthesizes highquality, self-generated reasoning traces that enforce robustness without disrupting native problem-solving mechanics. This ensures the training data remains aligned with the stuTHINKSAFE: Self-Generated Safety Alignment for Reasoning Models dents distribution, consistently achieving the most favorable safety-reasoning trade-off across the Qwen3 and DeepSeekR1-Distill families. Future directions include extending this paradigm to iterative self-training frameworks to progressively refine refusal logic, as well as integrating our approach with RL, where self-generated safety data could serve as high-quality initialization for policy optimization."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. International Conference on Learning Representations (ICLR), 2024. Allen-Zhu, Z. and Li, Y. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. International Conference on Learning Representations (ICLR), 2023. Bianchi, F., Suzgun, M., Attanasio, G., Rottger, P., Jurafsky, D., Hashimoto, T., and Zou, J. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. International Conference on Learning Representations (ICLR), 2024. Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., Blakeney, C., and Cunningham, J. P. LoRA learns less and forgets less. Transactions on Machine Learning Research (TMLR), 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., and InterAnandkumar, A. Born again neural networks. national conference on machine learning (ICML), 2018. Gu, Y., Dong, L., Wei, F., and Huang, M. MiniLLM: Knowledge distillation of large language models. International Conference on Learning Representations (ICLR), 2024. Han, S., Rao, K., Ettinger, A., Jiang, L., Lin, B. Y., Lambert, N., Choi, Y., and Dziri, N. WildGuard: Open onestop moderation tools for safety risks, jailbreaks, and refusals of LLMs. Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 2022. Huang, T., Hu, S., Ilhan, F., Tekin, S. F., Yahn, Z., Xu, Y., and Liu, L. Safety tax: Safety alignment makes your large reasoning models less reasonable. arXiv preprint arXiv:2503.00555, 2025. Jeung, W., Yoon, S., Kahng, M., and No, A. SAFEPATH: Preventing harmful reasoning in chain-of-thought via early alignment. Advances in neural information processing systems (NeurIPS), 2025. Jiang, F., Xu, Z., Li, Y., Niu, L., Xiang, Z., Li, B., Lin, B. Y., and Poovendran, R. SafeChain: Safety of language models with long chain-of-thought reasoning capabilities. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 2330323320, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025. findings-acl.1197. URL https://aclanthology. org/2025.findings-acl.1197/. Jiang, L., Rao, K., Han, S., Ettinger, A., Brahman, F., Kumar, S., Mireshghallah, N., Lu, X., Sap, M., Choi, Y., et al. WildTeaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models. Advances in Neural Information Processing Systems (NeurIPS), 2024. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Lee, S., Kang, M., Lee, J., Hwang, S. J., and Kawaguchi, K. Self-distillation for further pre-training of transformers. International Conference on Learning Representations (ICLR), 2023. Lee, S., Seong, H., Lee, D. B., Kang, M., Chen, X., Wagner, D., Bengio, Y., Lee, J., and Hwang, S. J. HarmAug: Effective data augmentation for knowledge distillation International Conference on of safety guard models. Learning Representations (ICLR), 2025. Li, A., Mo, Y., Li, M., Wang, Y., and Wang, Y. Are smarter llms safer? exploring safety-reasoning tradearXiv preprint offs in prompting and fine-tuning. arXiv:2502.09673, 2025a. Li, Y., Yue, X., Xu, Z., Jiang, F., Niu, L., Lin, B. Y., Ramasubramanian, B., and Poovendran, R. Small models struggle to learn from strong reasoners. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 2536625394, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 9 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 1301. URL https://aclanthology.org/2025. findings-acl.1301/. for Computational Linguistics. doi: 10.18653/v1/2024. naacl-long.301. URL https://aclanthology. org/2024.naacl-long.301/. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. International Conference on Learning Representations (ICLR), 2024. . M. Llama Team, A. The llama 3 family of modhttps://github.com/meta-llama/ els. PurpleLlama/blob/main/Llama-Guard3/ 1B/MODEL_CARD.md, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. International Conference on Learning Representations (ICLR), 2019. Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., et al. HarmBench: standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. Mobahi, H., Farajtabar, M., and Bartlett, P. Self-distillation amplifies regularization in hilbert space. Advances in Neural Information Processing Systems (NeurIPS), 2020. Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! International Conference on Learning Representations (ICLR), 2024. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. Conference on Language Modeling (COLM), 2024. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to no-regret online In Gordon, G., Dunson, D., and Dudık, M. learning. (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 627 635, Fort Lauderdale, FL, USA, 1113 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/ ross11a.html. Rottger, P., Kirk, H., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D. XSTest: test suite for identifying exaggerated safety behaviours in large language models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 5377 5400, Mexico City, Mexico, June 2024. Association 10 Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shenfeld, I., Pari, J., and Agrawal, P. RLs razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. Snell, C. V., Lee, J., Xu, K., and Kumar, A. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. International Conference on Learning Representations (ICLR), 2025. Souly, A., Lu, Q., Bowen, D., Trinh, T., Hsieh, E., Pandey, S., Abbeel, P., Svegliato, J., Emmons, S., Watkins, O., et al. strongreject for empty jailbreaks. Advances in Neural Information Processing Systems (NeurIPS), 2024. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 2014. Team, N. Sky-T1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouedec, Q. TRL: Transformers Reinforcement Learning, 2020. URL https://github.com/ huggingface/trl. Wang, Z., Tu, H., Wang, Y., Wu, J., Mei, J., Bartoldson, B. R., Kailkhura, B., and Xie, C. Star-1: Safer alignment of reasoning llms with 1k data. arXiv preprint arXiv:2504.01903, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems (NeurIPS), 35: 2482424837, 2022. Xue, Y. and Mirzasoleiman, B. LoRA is all you need for safety alignment of reasoning LLMs. arXiv preprint arXiv:2507.17075, 2025. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yang, Z., Pang, T., Feng, H., Wang, H., Chen, W., Zhu, M., and Liu, Q. Self-distillation bridges distribution gap in language model fine-tuning. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1028 1043, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. acl-long.58. URL https://aclanthology.org/ 2024.acl-long.58/. Zhang, Y. and Math-AI, T. American invitational mathematics examination (aime) 2024, 2024. insights for safety reasoning. Zhou, K., Zhao, X., Srinivasa, J., Liu, G., Feng, A., SafeKey: AmplifySong, D., and Wang, X. E. ing aha-moment In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2539625412, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979doi: 10.18653/v1/2025.emnlp-main. 8-89176-332-6. 1291. URL https://aclanthology.org/2025. emnlp-main.1291/. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models A. THINKSAFE A.1. Sampling details All prompts used in THINKSAFE are from the SafeChain dataset and processed by each model using our THINKSAFE framework. For Qwen3 model family, we sample one response for each prompt with top-p 0.95, top-k 20, temperature 0.6, and maximum token limit 16,384. For DeepSeek-R1-Distill family, we use the same hyperparameter except that top-k is set to 0. Responses that are classified as unsafe by the Llama-Guard-3, denoted as ϕ, are excluded from the analysis. Table 3 shows the ratio of filtered samples per model, which indicates that most Qwen3 models retain over 99% of both benign and harmful samples, while the R1-Distill-1.5B model exhibits substantially higher filtering rates. In general, larger models tend to preserve greater portion of the original data, suggesting more stable and consistent response distributions after filtering. Table 3. Filtered ratio (%) per model. Qwen DeepSeek-R1-Distill Sample category Benign Harmful 0.6B 1.7B 4B 1.07 4. 0.93 3.29 0.63 0.35 8B 0.74 0.16 1.5B 11.01 12. 7B 2.69 2.93 8B 0.81 0.48 A.2. Statistics We report the statistics of the responses generated by THINKSAFE in Figs. 9 and 10. Here, Nh and Nb denote the numbers of harmful and benign prompts, respectively, while µh and µb represent the average response lengths (in tokens) for harmful and benign queries. Across both the Qwen3 and R1-distilled model series, benign responses consistently exhibit longer generation lengths than harmful ones, reflecting the presence of more detailed reasoning traces. Moreover, as model size increases, both harmful and benign responses tend to become longer and more stable in distribution. Figure 9. Statistics of THINKSAFE in Qwen3 model series. Top 1% outliers by length are excluded for better interpretability. Figure 10. Statistics of THINKSAFE in DeepSeek-R1-Distill model series. Top 1% outliers by length are excluded for better interpretability. B. Experimental Details For the AIME 2024, GSM8K, MATH500, and GPQA datasets, we use the SkyThought (Team, 2025) library to evaluate models. We sample 8 responses for each prompt using dataset-specific prompts and report the average pass@1. For the Qwen model family, we use temperature of 0.6, top-p of 0.95, and top-k of 20, with maximum token limit of 32,768. For the DeepSeek-R1-Distill family, we use temperature of 0.6 and top-p of 0.95, with maximum token limit of 32,768. THINKSAFE: Self-Generated Safety Alignment for Reasoning Models AIME 2024 Please reason step by step, and put your final answer within boxed{{}}. {problem} GSM8K Given the following problem, reason and give final answer to the problem. Problem: {question} Your response should end with The final answer is [answer] where [answer] is the response to the problem. GPQA Return your final response within boxed{{}} and only include the letter choice (A, B, C, or D) as your final response. {problem} MATH500 Please reason step by step, and put your final answer within boxed{{}}. {problem} C. Baseline Details Table 4. Detailed hyperparameters for baselines. Method Epochs Source Sample size DirectRefusal SafeChain STAR-1 SafeKey SafePath THINKSAFE 5 3 5 5 4 3 WildJailbreak WildJailbreak Mixture of harmful datasets (See Wang et al., 2025 for details) STAR-1 WildJailbreak SafeChain 1,000 40,000 1, 1,000 400 Varies by scale To ensure consistency and reproducibility, we adopt the same hyperparameters as specified in the original papers for all baselines, as summarized in Table 4. For THINKSAFE, since the sample size varies across model scales, we refer readers to Figs. 9 and 10 for detailed configurations. D. Additional Experiments In this section, we present additional experiments to empirically support the effectiveness of the proposed THINKSAFE. D.1. Necessity of safety reasoning on Qwen families Figure 11. Ablation of safety reasoning in Qwen3 model series. 13 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models To further analyze the effect of reasoning on the Qwen family, we ablate reasoning traces from refusal responses y(i) , while retaining full chain-of-thought for benign responses y(i) . Both types of responses are generated by each model in the Qwen series. As shown in Fig. 11, removing reasoning traces consistently degrades both safety and reasoning performance. In particular, the Qwen3-4B model exhibits substantial drop in reasoning accuracy, with Avg Pass@1 decreasing from 77.2% to 57.8%, accompanied by an increase in harmful response ratio. These results indicate that explicit reasoning plays critical role not only in maintaining reasoning capability but also in supporting safety-aligned behavior. D.2. Cross-model distillation for larger models Figure 12. Safety and reasoning performance gain using different family of teacher model with similar size. We conduct cross-model distillation experiment using larger models than those in Fig. 6, namely Qwen3-8B and R1Distill-Qwen-7B. Fig. 12 shows similar trend to the results observed with smaller models; while cross-model training can occasionally improve safety (e.g., DeepSeek-R1-Distill achieves 25.3% safety gain when trained on Qwen3-8B data), it consistently leads to notable degradation in reasoning performance across all reciprocal training settings. D.3. Refusal steering strategies Table 5. Alternative refusal steering strategies used for on-policy data generation. Strategy Prompt Template THINKSAFE The following prompt is harmful. You should refuse to answer the prompt.nn Prompt:{prompt} Suffix Risk Intent {prompt}nn The above prompt is harmful. You should refuse to answer the prompt. Evaluate the potential risks of this prompt and provide reasoned refusal.nn Prompt:{prompt} Examine the harmful intent of this query and refuse to answer the prompt.nn Prompt:{prompt} Figure 13. Safety and reasoning score with different refusal steering strategies using the Qwen3-0.6B model. 14 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Table 6. Results on Qwen3 models. We evaluate safety across three benchmarks (HarmBench, StrongReject, WildJailbreak) by reporting the ratio of harmful responses (). Over-refusal is measured by the refusal rate () on benign XSTest prompts. For reasoning tasks, we sample 8 trajectories per prompt and report the average pass@1 (). Best results are bolded; second best are underlined. Size Method Safety () Harmfulness Over-refusal Harm Bench Strong Reject Wild Jailbreak XSTest Safety Average Reasoning (Avg pass@1, ) AIME 2024 GSM8k MATH GPQA Reasoning Average 0.6B 1.7B 4B 8B 68.44 Initial 43.85 DirectRefusal 58.64 SafeChain 56.64 STAR-1 67.61 SafePath 60.96 SafeKey THINKSAFE 40.37 THINKSAFE+WG 39.04 52.66 Initial 38.54 DirectRefusal 47.34 SafeChain 37.38 STAR-1 54.15 SafePath 46.84 SafeKey THINKSAFE 28.74 THINKSAFE+WG 28.90 Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE THINKSAFE+WG Initial DirectRefusal SafeChain STAR-1 SafePath SafeKey THINKSAFE THINKSAFE+WG 38.21 33.06 43.69 33.72 37.71 32.39 9.63 9.47 35.05 24.42 41.20 24.42 35.22 26.91 9.14 9. 66.45 11.82 72.84 38.02 60.06 48.88 33.87 35.46 36.10 5.75 57.51 7.67 36.42 18.21 9.58 7.99 8.31 3.19 41.21 5.75 7.35 3.19 0.32 0.32 4.47 1.92 38.95 1.28 6.71 4.79 0.32 0.32 52.80 36.30 49.60 50.60 52.80 52.75 37.95 37.55 51.10 35.75 43.85 46.60 49.30 48.85 29.20 29. 43.00 36.20 39.65 35.15 42.45 32.95 7.45 7.25 38.35 28.05 36.42 29.25 39.45 28.80 7.35 7.05 5.20 83.60 0.00 22.40 4.40 18.40 6.40 7.20 1.20 61.60 1.60 10.80 1.20 8.80 2.00 2.00 0.80 32.00 2.00 6.80 1.60 0.80 2.80 2.40 0.40 37.60 1.20 6.80 1.20 8.80 1.20 3. 48.22 43.89 45.20 41.92 46.22 45.25 29.65 29.81 35.27 35.41 37.58 25.61 35.27 30.68 17.38 17.17 22.58 29.80 31.64 20.36 22.28 17.33 5.05 5.68 19.57 23.00 29.44 15.44 20.64 17.33 4.50 5.05 10.42 5.83 4.58 6.25 7.92 5.42 9.58 9.58 44.58 43.75 34.58 46.25 43.33 38.33 44.17 45. 67.50 68.33 62.08 62.50 72.08 67.08 73.33 75.42 74.17 74.58 70.00 72.50 74.58 70.00 72.92 73.33 72.51 64.30 68.68 68.15 71.26 71.58 72.36 72.82 84.31 82.78 85.29 84.38 84.33 84.31 83.80 83.43 84.69 82.58 89.59 90.97 84.45 91.79 88.06 88.15 85.28 84.31 92.98 90.29 84.89 92.44 88.00 87. 71.73 67.53 62.42 68.17 71.77 66.17 70.65 72.00 88.85 88.10 85.72 88.30 88.32 88.12 89.05 88.98 93.43 93.20 93.03 93.05 93.33 92.87 93.53 93.55 94.18 93.63 93.53 93.73 93.85 93.30 93.10 93.65 25.13 24.81 23.74 24.18 26.07 24.94 23.30 24.81 41.73 41.29 38.13 41.16 42.42 40.03 40.53 41. 52.27 53.03 51.01 51.96 53.54 51.83 53.79 53.60 50.69 59.41 58.21 57.83 61.24 59.91 59.67 60.29 44.95 40.62 39.86 41.69 44.26 42.03 43.97 44.80 64.87 63.98 60.93 65.02 64.60 62.70 64.39 64.61 74.47 74.29 73.93 74.62 75.85 75.89 77.18 77.68 76.08 77.98 78.68 78.59 78.64 78.91 78.50 78. We further investigate the robustness of THINKSAFE on the Qwen3-0.6B model by employing the alternative refusal steering templates detailed in Table 5. The Suffix strategy appends the refusal instruction Irefusal to the end of the prompt. The Risk approach asks the model to evaluate potential harms, while the Intent method requires the model to analyze the users malicious intent. As shown in Fig. 13, the Suffix strategy produces harmful response ratio similar to that of the default prefix-based THINKSAFE. We compute this metric by averaging results over HarmBench, StrongReject, and WildJailbreak. This similarity indicates that where the refusal instruction appears is not especially important, as straightforward refusal instructions work well regardless of placement. In contrast, the Risk and Intent strategies lead to noticeably worse safety outcomes, with higher harmful response ratios. We attribute this gap to the added complexity of these instructions. By asking the model to carry out extra reasoning steps instead of issuing direct refusal, these prompts may weaken the strength of the safety constraint. Importantly, overall reasoning performance remains stable across all four strategies, as measured by average pass@1 on AIME2024, GSM8K, MATH500, and GPQA. This stability supports our central claim. Because all variants depend on self-generated outputs from the model itself rather than learning from an external model, the models core reasoning ability remains intact even when the safety approach changes. D.4. Filtering with WildGuard To assess whether the effectiveness of THINKSAFE depends on the specific characteristics of the filtering model, we conduct an ablation study using WildGuard instead of Llama-Guard-3. We denote this variant as THINKSAFE + WG, which 15 THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Table 7. Results on DeepSeek-R1-Distill models. We evaluate safety across three benchmarks (HarmBench, StrongReject, WildJailbreak) by reporting the ratio of harmful responses (). Over-refusal is measured by the refusal rate () on benign XSTest prompts. For reasoning tasks, we sample 8 trajectories per prompt and report the average pass@1 (). Best results are bolded; second best are underlined. Size Method Safety () Harmfulness Over-refusal Harm Bench Strong Reject Wild Jailbreak XSTest Safety Average Reasoning (Avg pass@1, ) AIME 2024 GSM8k MATH 500 GPQA Reasoning Average 1.5B 7B 8B 67.28 Initial 66.11 DirectRefusal 59.30 SafeChain 62.79 STAR-1 65.28 SafePath 58.80 SafeKey THINKSAFE 52.99 THINKSAFE + WG 54.82 56.98 Initial 52.33 DirectRefusal 51.00 SafeChain 52.99 STAR-1 55.15 SafePath 45.35 SafeKey THINKSAFE 40.20 THINKSAFE + WG 40.03 52.33 Initial 32.39 DirectRefusal 44.52 SafeChain 21.26 STAR-1 47.51 SafePath 32.72 SafeKey THINKSAFE 27.08 THINKSAFE + WG 27.24 82.11 82.75 76.68 77.00 82.43 73.16 74.12 76. 63.58 33.55 54.63 47.92 64.86 33.87 41.85 38.98 53.99 0.64 46.33 3.51 51.44 11.82 26.52 26.20 51.55 50.45 46.95 49.65 51.80 47.65 40.50 39.95 53.15 50.20 45.85 48.75 52.65 45.75 35.40 34.25 49.70 32.60 42.45 17.60 50.20 30.85 21.15 22.00 0.00 8.40 0.40 1.20 0.40 3.60 1.20 1. 1.20 43.60 0.40 2.40 0.00 7.20 0.40 0.80 0.40 50.00 1.60 12.00 0.40 8.00 1.60 1.60 50.23 51.93 45.73 47.66 49.98 45.80 42.20 43.16 43.73 44.92 37.97 38.02 43.16 33.04 29.46 28.52 39.10 28.91 33.72 13.59 37.39 20.85 19.09 19.26 21.25 19.17 24.17 17.08 24.17 19.58 32.92 30. 49.58 47.50 49.17 45.83 52.08 43.75 51.25 52.50 47.50 40.00 41.67 40.42 43.33 35.83 48.75 45.42 82.42 81.06 80.47 81.38 82.37 81.25 82.58 82.36 90.32 88.27 89.75 90.32 89.71 90.58 90.10 90.98 87.74 83.26 86.06 87.28 87.45 87.41 87.55 87.62 79.45 78.55 81.25 79.07 79.57 78.02 82.50 82. 90.18 89.82 91.50 90.58 90.62 89.90 91.90 92.05 87.38 85.00 86.50 86.65 87.43 85.80 87.70 86.82 31.94 32.70 28.28 31.25 32.51 28.72 31.19 32.89 46.65 44.95 46.78 46.02 46.15 47.29 45.20 46.97 48.11 43.50 42.05 43.69 47.41 42.49 46.28 45.14 53.77 52.87 53.54 52.20 54.66 51.89 57.30 56. 69.18 67.64 69.30 68.19 69.64 67.88 69.61 70.63 67.68 62.94 64.07 64.51 66.41 62.88 67.47 66.25 employs WildGuard to filter the self-generated safety data. The results in Table 6 and Table 7 demonstrate remarkable stability in performance and confirm that our framework is robust to the choice of safety classifier. For example, the WildGuard variant of the Qwen3-4B model (THINKSAFE + WG) achieves safety scores nearly identical to the baseline while preserving superior reasoning capabilities. This consistency reinforces our core hypothesis that the success of THINKSAFE arises from the refusal steering mechanism itself rather than from overfitting to specific reward model. By successfully filtering self-generated traces with completely different guard model, we demonstrate that the elicited safety behaviors are generalized and transferable. E. GRPO Baseline Details E.1. GRPO Objective The objective function for GRPO is formulated to optimize the policy without separate value function, instead estimating advantages in group-relative manner as introduced in Shao et al., 2024. Given dataset of prompts x, GRPO samples group of candidate responses {yi}G i=1 from behavior policy πθold , while constraining updates to remain close to fixed reference policy πref. Then, the advantage ˆAi for each response yi in group of size is computed by normalizing the rewards ri against the groups mean and standard deviation: These advantages are used in clipped objective with importance sampling, together with DKL penalty term: ˆAi = ri mean({r1, . . . , rG}) std({r1, . . . , rG}) (θ) = xD,{yi}G i=1πθold (cid:34) 1 (cid:88) (cid:18) min i= (cid:18) πθ(yix) πθold (yix) ˆAi, clip (cid:18) πθ(yix) πθold (yix) , 1 ϵ, 1 + ϵ (cid:19) (cid:19) ˆAi 16 βDKL(πθπref) (4) (3) (cid:19)(cid:35) THINKSAFE: Self-Generated Safety Alignment for Reasoning Models E.2. Safety Reward The safety reward rsafety is derived from the output logits of the safety guard model. Specifically, given prompt and sampled response y, we extract the log probabilities of the tokens corresponding to safe (c = 1) and unsafe (c = 0) from the guard models prediction. The final reward is computed by applying softmax function over these two token probabilities to represent the likelihood of the response being safe: rsafety(x, y) = pϕ(c = 1 x, y) pϕ(c = 1 x, y) + pϕ(c = 0 x, y) (5) The rsafety ensures continuous reward signal between 0 and 1, reflecting the guard models confidence in the safety of the generated trace. Subsequently, the rsafety is combined with the rformat to constitute the total reward r, which is then utilized in Eq. 3. E.3. Format Reward The format reward, rformat, strictly enforces the structural integrity of the reasoning traces. For standard models, we assign reward of 1 if the response contains exactly one pair of <think> and </think> tags in the correct order, and 0 otherwise. For the DeepSeek-R1-Distill family, which omits the opening tag by default, we adapt this criterion to require exactly one occurrence of the closing </think> tag and no opening <think> tag. This ensures that the model maintains consistent chain-of-thought structure during the online RL process. E.4. Experimental Setup For the GRPO baseline, we utilize the TRL (von Werra et al., 2020) library integrated with vLLM (Kwon et al., 2023) for efficient online rollout generation. We generate = 8 rollouts per prompt to estimate the group-relative advantage. The DKL coefficient is fixed at β = 0.04 and the clipping parameter ϵ is set to 0.2. To ensure consistent and fair comparison with THINKSAFE, all other experimental details including the optimizer, batch size, and hardware configuration are maintained identical to the primary experimental setup described in Sec. 5."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST AI",
        "KRAFTON",
        "UC Berkeley"
    ]
}