{
    "paper_title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "authors": [
        "Sunghyun Ahn",
        "Youngwan Jo",
        "Kijung Lee",
        "Sein Kwon",
        "Inpyo Hong",
        "Sanghyun Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly."
        },
        {
            "title": "Start",
            "content": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM Sunghyun Ahn*, Youngwan Jo*, Kijung Lee, Sein Kwon, Inpyo Hong, Sanghyun Park Yonsei University, Seoul, Korea {skd, jyy1551, rlwjd4177, seinkwon97, hip9863, sanghyun}@yonsei.ac.kr 5 2 0 2 6 ] . [ 1 4 0 5 4 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers userdefined text as an abnormal event and detects frames containing specified event in video. We effectively implemented AnyAnomaly using context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly. 1. Introduction Video anomaly detection (VAD) aims to detect abnormal events in video streams. Abnormal events include the actions of objects that are inappropriate for the environment (e.g., climbing over fence) or objects with unusual appearances (e.g., bicycle on walkway). However, abnormal events are rare and diverse, making it difficult to construct large-scale datasets for VAD. Therefore, the VAD is recognized as highly challenging problem. To overcome these limitations, previous studies primarily used one-class classification (OCC) methods that learn only from normal data. In the OCC approach, the model *Equal contribution Corresponding author Figure 1. Comparison of traditional video Anomaly Detection (VAD) and customizable video anomaly detection (C-VAD). Traditional VAD models struggle with generalization, making them hard to apply in diverse environments, while C-VAD can handle various video environments. learns normal patterns and classifies the cases that deviate from them as abnormal. Representative OCC-based VAD methods include classification-[14, 27, 35], distance- [2, 24, 26], and prediction-based methods [10, 15, 33], all of which have demonstrated excellent performance in VAD tasks. However, because normal and abnormal classes can be defined differently depending on the environment, OCC methods cannot always guarantee generalized performance. For example, as shown on the left side of Figure 1, model trained in campus environment learns the characteristics of person as normal pattern and classifies car as abnormal. However, when this model is applied to road environment, car is still detected as abnormal, which can increase the number of false positives. Therefore, OCC methods require the retraining of normal patterns for each new environment, which entails additional costs, such as data collection, expert intervention, and high-performance equipment. Because of these limitations, the application of VAD models to real-world scenarios is challenging. To address this issue, we propose novel technique called customizable video anomaly detection (C-VAD). CVAD considers user-defined text as abnormal events and detects the frames containing these events in the video. For instance, in campus videos, car can be set as an abnormal 1 event, while in road videos, person can be set as an abnormal event. In contrast to existing VAD models, which judge abnormalities based on learned normal patterns, CVAD dynamically detects abnormal patterns based on the text provided. This implies that as the generalizability of visual text analysis improves, anomaly detection becomes more effective in various environments. Consequently, we introduce zero-shot capable C-VAD approach, as shown on the right side of Figure 1, and propose the AnyAnomaly model, which allows for VAD in various environments without the need for additional training. An effective method to implement zero-shot C-VAD is to leverage large vision language models (LVLMs). Recently, LVLMs have demonstrated outstanding generalization performance in visual text analysis. By leveraging this capability, C-VAD can be performed effectively across various environments. The most intuitive method involves performing visual question answering (VQA) [4] on each frame to estimate the anomaly score. For instance, one could provide the model with the prompt: Return value between 0 (no) and 1 (yes) indicating how well the input image represents the text provided by the user. This was used as the baseline model. However, through experiments, we observed the following limitations of the baseline model: 1 Due to the large computational cost of LVLMs, the latency is high. 2 Difficulty in analyzing specific objects due to the characteristics of surveillance videos, such as foreground-background imbalance and object congestion. 3 Difficulty in detecting action-related anomalies because of the inability to utilize temporal information. To overcome these limitations, we designed an AnyAnomaly model with the structure shown in Figure 2. First, to reduce the latency, we adopted segment-level approach that groups consecutive frames into single segment for processing. For this purpose, we introduced key frames selection module (KSM) that selects key frames representing the segment and performed VQA per segment. Second, instead of performing simple image-text matching, we introduced context-aware VQA approach to enable deeper understanding of the scene. To this end, we additionally utilized two types of information: position context, and temporal context, C. is context that emphasizes important locations within frame, enhancing the object analysis capability of the LVLM. is context that structures scene changes over time into grid format, improving the action analysis capability of the LVLM. Notably, the proposed KSM and context generation modules operate in training-free manner, allowing for easy application of CVAD without additional data training. To evaluate the performance of C-VAD, we classified existing VAD benchmark datasets based on anomaly types to create the C-VAD datasets. Through this process, we demonstrated the superiority of AnyAnomaly. Furthermore, Figure 2. The architecture of AnyAnomaly despite being zero-shot approach, AnyAnomaly achieved competitive performance on VAD datasets compared to traditional OCC-based VAD models. It achieved state-of-theart (SOTA) results on the UBnormal dataset [1] and showed superior generalization across all datasets. The proposed approach is expected to be an effective solution for deploying VAD technology in real-world applications. The contributions of this study are as follows: We propose the C-VAD technique for anomaly detection in diverse environments. To the best of our knowledge, it is the first to perform VAD based on user-defined anomalies. We develop the AnyAnomaly model, which applies context-aware VQA to perform C-VAD effectively. To evaluate the performance of C-VAD, we construct new C-VAD datasets and experimentally verify the superiority of AnyAnomaly. AnyAnomaly achieves SOTA performance on UBnormal dataset and outperforms other methods in generalization across all datasets. 2. Related Work Video Anomaly Detection. Most VAD models adopt the OCC approach to detect anomalies by learning normal patterns. Among them, prediction-based methods train models to predict future or past frames based on normal frames, assuming that abnormal frames exhibit larger prediction errors. Liu [15] proposed method that utilizes 2 FlowNet [11] and GANs [20] to predict the + 1-th frame given input frames. Yang [33] introduced an approach that selects key frames from input frames to predict the entire sequence. However, because the definitions of normal and abnormal patterns may vary depending on the environment, the OCC approach, which relies on learned normal patterns, has limited generalization performance. To mitigate this limitation, recent studies have explored cross-domain VAD (xVAD). Notably, zxVAD [3] enhances the adaptability to new environments by synthesizing abnormal patterns using the cut mix technique on images from auxiliary datasets. However, these approaches depend on fixed data transformations, making it difficult to fully capture the diverse abnormal patterns that may occur in real-world scenarios. Therefore, we propose novel VAD method that uses textual information to dynamically detect abnormal patterns that vary depending on the environment. Large Vision Language Models. Large language models have primarily been used in natural language processing; however, they have recently been applied to multimodal tasks such as image captioning and VQA. For example, MiniGPT-4 [38] processes multimodal inputs by connecting pre-trained vision encoder to the Vicuna [7] model through linear layer. Recent LVLMs have employed novel visual encoding techniques to better understand images. Chat-UniVi [13] generates dynamic tokens for images, thereby reducing unnecessary information and effectively extracting key visual features. This model enables flexible analysis by applying dynamic tokens across various resolutions. MiniCPM-V [34] applies the best partition technique according to the image resolution and generates tokens optimized for each segment, thereby improving the memory efficiency. However, despite the advancements in LVLMs, they are trained for general purposes, making their direct application in VAD challenging. To handle VAD effectively, it is essential to consider the characteristics of surveillance videos and leverage temporal information. Therefore, we propose training-free approach to minimize the domain gap between the LVLMs and VAD tasks. 3. Method 3.1. Overview Figure 2 illustrates the structure of the AnyAnomaly model, which performs context-aware VQA. The input is video segment = {s0, . . . , sN 1} comprising frames, where is multiple of 4. The KSM selects key frames = {k0, . . . , k3} from S. Among the selected key frames, the representative frame ˆk is used to generate PC, whereas is used to create C. Subsequently, ˆk, C, and are utilized as image inputs for the LVLM, whereas the userprovided text is combined with prompt and used as the text input. Finally, the LVLMs response results are integrated to compute an anomaly score. 3.2. Key frames Selection Module Figure 3a shows KSM, key component of the segmentlevel approach. For this purpose, we selected four frames representing the segment as and utilized the CLIP [22] model, which was trained to match the images and text. Specifically, and are inputs to the image encoder EI and text encoder ET , respectively, and the similarity is calculated using the dot product of image and text embeddings. The frame with the highest similarity is selected as the representative frame ˆk. ˆk = arg max siS (EI (si) ET (X)) (1) The index of the representative frame ˆk, denoted as ˆi, is used to select the other key frames. We divide the segment into four groups of equal size and select the ˆi mod 4 -th frame from each group. For example, when = 8 and ˆi = 4, the 0-th frame from each group is selected and the final set is = {s0, s2, s4, s6}. This process is defined as follows: ki = sx, = (i 4 ) + (ˆi mod ) (2) Using the KSM, is generated by considering both text alignment and temporal uniformity, thereby enabling effective context generation. comparative analysis of the key frames selection method is presented in Section 4.5. 3.3. Context Generation and are key elements of context-aware VQA, serving as additional information that complements the input image. enhances the object analysis capability of the LVLM and is generated through WinCLIP-based attention (WA). strengthens the action analysis ability of the LVLM and is created through grid image generation (GIG). WinCLIP-based Attention. Figure 3b illustrates the WA method. We emphasize the regions related to at ˆk based on WinCLIP, as proposed by Jeong [12]. First, ˆk is divided into multiple windows, and individual embeddings are generated from each window using EI . For example, when the image size is 240240, it is divided into 25 windows of size 4848, and the embeddings of each window are collected to create small-scale window embedding map R25D. By adjusting the window size, middle-scale window embedding map and large-scale window embedding map are also generated, and the similarity between these embedding maps and the text embedding RD is calculated. 3 (a) Key frames Selection Module (KSM) (b) WinCLIP-based Attention (WA) (c) Grid Image Generation (GIG) Figure 3. Architecture of the proposed modules. KSM is essential for the segment-level approach, and WA and GIG are crucial for context generation. The final similarity map is generated by averaging the similarities calculated on three scales: = 1 3 (z(W s)T + z(W m)T + z(W l)T ) (3) We combined the template proposed by Jeong [12] with and passed it through ET to generate z. Finally, we multiplied and ˆk to create C: = fnorm(M ) ˆk (4) Here, fnorm represents min-max normalization, and denotes element-wise multiplication. was used after interpolation and reshaping to match the resolution of ˆk. Because is created by integrating similarities from multiple scales, it is robust to object size and location, and operates effectively even in situations with multiple objects. Grid Image Generation. Figure 3c illustrates the GIG method, which comprises two stages. In the multiscale grid generation stage, is used to create grid images at different scales. Similar to the process described in WA, each frame of is divided into multiple windows, and the windows at the same position are connected in 22 grid format to create single grid image. This process is defined as follows: gi = (cid:21) (cid:20)ui 0 ui 1 2 ui ui (5) Here, ui refers to the i-th window created from kj, and gi refers to the i-th grid image. We defined the sets of grid images generated using small-, middle-, and large-scale windows as Gs, Gm, and Gl, respectively. In the grid image selection stage, the previously created sets are aggregated to generate Gall. Then, using the same method as in KSM to select ˆk, the grid image with the highest similarity to the text is chosen to generate C: Gall = Gs Gm Gl = arg max giGall (cid:0)EI (gi) ET (X)(cid:1) (6) (7) The generated through this process represents the object movement over time within the same background, making it advantageous for action analysis and robust to various object sizes. An analysis of the window sizes used in the WA and GIG is presented in Section 4.5. 3.4. Anomaly Detection Instead of tuning the LVLM, we propose new prompt and context for performing context-aware VQA. The VQA results were used as anomaly scores to enable training-free zero-shot anomaly detection. Prompt Design. Figure 4 illustrates the proposed prompt . The prompt comprises three main components: task, consideration, and output. First, task defines the operation the LVLM should perform, specifically evaluating whether is present in the image. Next, consideration specifies factors to be taken into account during evaluation, while output defines the format for presenting the evaluation results. To leverage the chain-of-thought [30] effect, we instructed the model to provide brief reasoning along with the anomaly score, rounded to one decimal place. When conducting VQA using C, an additional element, 4 Figure 4. Proposed prompt for VQA context, is inserted between task and consideration in the prompt. This context element conveys the meaning of the rows and columns of to the LVLM. We define the modified prompt as . comparative analysis of prompts is presented in supplementary materials. Anomaly Scoring. The context serves as supplementary information to the image. However, because the LVLM accepts only single image as input, it is challenging to utilize both the original and additional information simultaneously. To address this issue, we adopt late fusion approach. Specifically, ˆk, C, and were used as the image inputs for the LVLM. The LVLM returns an anomaly score for each input, and these three scores are combined to compute the final ascore: ascore = γ1 ΦLVLM(ˆk, ) + γ2 ΦLVLM(P C, ) + γ3 ΦLVLM(T C, ) (8) Here, γ is hyperparameter that adjusts the proportion of context reflected in ascore. performance comparison experiment based on the hyperparameter tuning is presented in supplementary materials. Consequently, even if the abnormal frame ˆk receives low score, the final anomaly score will be high if the additional information, or C, is assigned high score. This enabled accurate anomaly detection. 4. Experiments 4.1. Datasets Figure 5 illustrates the composition of the VAD and proposed C-VAD datasets. In conventional VAD datasets, videos are not categorized by an abnormal class type. In contrast, the proposed C-VAD datasets are organized by abnormal event type, with videos classified as positive or negative based on the presence of each abnormality. This categorization enables precise evaluation of detection perfor5 Figure 5. Comparison between the VAD and C-VAD datasets mance for specific types of abnormalities (e.g., bicycle). In this study, we validated the effectiveness of the proposed method on three VAD datasets: CUHK Avenue (Ave) [16], ShanghaiTech Campus (ShT) [18], and UBnormal (UB) [1] as well as two C-VAD datasets: Customizable-ShT (CShT) and Customizable-Ave (C-Ave). Further details of the datasets are provided in supplementary materials. 4.2. Evaluation Criteria To ensure consistency with previous VAD studies, the performance of the proposed model was evaluated using the micro-averaged area under the receiver operating characteristic curve (micro AUROC) metric. Specifically, the anomaly scores of all the frames in the dataset were aggregated, and the threshold of the anomaly score was progressively adjusted to compute the final evaluation. 4.3. Results Tables 1 and 2 present the evaluation results on the C-VAD datasets. The baseline, as described in Section 1, performs VQA at the frame level to compute anomaly scores. The proposed model achieved performance improvements of 9.88% and 13.65% compared to the baseline on the C-ShT and C-Ave datasets, respectively. Specifically, it showed improvements of 14.34% and 8.2% in the action class, and 3.25% and 21.98% in the appearance class, respectively. When only KSM was applied to the baseline, the execution time decreased in proportion to the segment length, whereas the average performance remained similar to that of the baseline. This is because the CLIP effectively selects representative frames for each segment, thereby compensating for the loss of temporal information. However, because it does not fully capture fine-grained spatio-temporal details, its performance significantly decreases for certain classes. Therefore, we address these issues using the proposed contextual information. First, using resulted in performance improvements of 5.64% and 3.62% compared to the KSM, as the LVLM focused on analyzing objects related Table 1. Performance comparison on C-ShT dataset Category Class Baseline +KSM +KSM/PC +KSM/TC Proposed Improvement (%) Skateboarding Throwing Running Loitering Jumping Falling Fighting Average Car Hand truck Bicycle Motorcycle Average 61.30 91.41 53.13 61.98 82.84 78.31 84.48 73.35 88.72 95.50 72.36 88.04 86.16 78. 57.06 72.82 51.93 51.96 92.89 78.95 91.18 72.00 90.96 98.20 72.46 86.72 87.09 77.48 57.79 88.74 53.68 81.27 92.91 79.24 91. 77.83 91.46 98.91 78.47 86.72 88.89 81.85 Action Appearance Overall Average 73.66 82.53 59.77 76.94 95.31 88.01 98.06 82.04 90.96 99.20 72.46 86.72 87.34 83. 73.66 90.67 60.11 81.27 95.31 88.01 98.06 83.87 91.46 99.20 78.47 86.72 88.95 85.72 +20.16 -0.81 +13.14 +31.12 +15.05 +12.39 +16. +14.34 +3.09 +3.87 +8.44 -1.50 +3.25 +9.88 Table 2. Performance comparison on C-Ave dataset"
        },
        {
            "title": "Baseline",
            "content": "+KSM +KSM/PC +KSM/TC Proposed Improvement (%)"
        },
        {
            "title": "Average",
            "content": "78.44 75.82 85.65 79.97 57.23 99.99 78.61 79.43 80.13 77.67 72. 76.69 61.48 99.84 80.66 78.28 89.77 77.67 76.64 81. 61.48 99.99 80.74 81.11 82.40 77.90 91.92 84.07 91.78 99. 95.86 88.79 89.77 77.90 91.92 86.53 91.78 100.00 95. 90.27 +14.44 +2.74 +7.32 +8.2 +60.37 +0.01 +21.98 +13. to X. Additionally, applying led to performance improvements of 8.38% and 14.43% over the KSM, respectively, with particularly notable enhancements observed in the action class. This indicates that utilizing the temporal information provided in the grid image is essential for action analysis. 4.4. Qualitative Analysis To analyze the effect of context-aware VQA, we present the visualization results for the anomaly scores and input frames in Figure 6. When was not applied, the bicycle object appeared smaller then the other objects, leading to lower detection performance. Once is applied, the bicycle region is emphasized, thereby enhancing the object recognition capability of the LVLM. Similarly, without C, the model misinterpreted fighting as standing, resulting in lower detection performance. Incorporating temporal information through improves the action recognition capability of the LVLM. These results demonstrate that contextaware VQA is more effective than the conventional VQA. 4.5. Ablation Study length was set Segment length and FPS. Table 3 presents the performance comparison and FPS based on different segment lengths. The baseline segment to 1. It was observed that deriving anomaly scores at the segment level yields superior performance compared to the baseline, which relies on single frame. The highest AUC performance was achieved when the segment length is set to 24, reaching 85.72% and 90.27% for C-ShT and C-Ave, respectively. However, excessively long segment length introduces irrelevant information into the temporal context, leading to decrease in accuracy. Furthermore, performing VAD at the segment level resulted in 594% improvement in the FPS 6 Figure 6. Anomaly score comparison and context visualization Table 3. Comparison on segment length Segment length C-ShT C-Ave FPS 0.96 78.01 2.67 83.83 4.49 83.45 85.72 6.67 8.45 82.50 Baseline 8 16 24 79.43 83.96 87.45 90.27 85.94 compared with the baseline. Table 4. Comparison on key frames selection method. RD, CP and Gr. indicate random, CLIP and grouping, respectively. * indicates testing without context. Act. and App. indicate action and appearance class, respectively. Key frames RD* CP* RD CP Gr. CP CP Gr. C-ShT C-Ave Act. App. Total Act. App. Total 69.9 72.0 80.0 81.2 82.2 83.9 84.0 87.1 89.1 88.9 88.8 89.0 75.0 77.5 83.3 84.0 84.7 85.7 66.4 76.7 79.1 84.3 83.9 86. 78.8 80.7 92.3 81.2 92.2 95.9 71.3 78.3 84.4 83.1 87.2 90.3 Key frames Selection. We conducted an ablation study on 7 key frames selection from two perspectives: temporal uniformity and text alignment. The random method considers neither of these aspects, whereas the CLIP-based approach considers only text alignment. Selecting key frames using CLIP after grouping ensures text alignment but does not guarantee temporal uniformity. Applying grouping after CLIP resulted in evenly distributed key frames, thereby considering both temporal uniformity and text alignment. As shown in Table 4, incorporating both factors yielded the best performance for C-VAD, highlighting the critical role of temporal uniformity in action recognition. Furthermore, RD* and CP*, which do not utilize the contextual information, perform worse than the random method, which disregards both temporal uniformity and text alignment. This demonstrates the importance of leveraging the contextual information. Window Size. Table 5 presents the experimental results based on the window sizes used in and C. For the action classes, the best performance was achieved with the large window size in C-ShT and the middle window size in C-Ave. This indicates that middle or large window sizes are more effective in capturing temporal movements and interactions between multiple objects. For appearance classes, the optimal performance was observed with the small window size in C-ShT and the middle window size in C-Ave, suggesting that the appropriate window size varies depending on the dataset owing to differences in object sizes. To Table 5. Comparison on window size. 4.6. Comparison with SOTA Window Size C-ShT C-Ave Act. App. Total Act. App. Total small middle large all 78.8 81.2 82.1 83.9 90.6 89.0 89.7 89.0 83.1 84.1 84.9 85.7 84.7 87.5 86.8 86.5 87.1 92.0 86.4 95. 85.7 89.3 86.6 90.3 Table 6. Comparison with state-of-the-art VAD methods. * indicates testing without context. Method AMMC-Net[6] STEAL-Net[5] MPN[19] Zhong et al.[36] DLAN-AC[32] UBnormal[1] FPDM[31] SLM[28] Venue AAAI 21 ICCV 21 CVPR 21 PR 22 ECCV 22 CVPR 22 ICCV 23 ICCV 23 USTN-DSC[33] CVPR 23 CVPR 24 CVPR 24 AAAI 25 TMLR 25 - - MULDE[21] AED-MAE[25] MA-PDM[37] AccI-VAD[24] AnyAnomaly* AnyAnomaly Training Ave 86.6 87.1 89.5 88.9 89.9 - 90.1 90.9 89.9 - 91.3 91.3 - 81.4 87. ShT UB 73.7 73.7 73.8 70.7 74.7 - 78.6 78.8 73.8 81.3 79.1 79.2 - 77.2 79.7 - - - - - 68.5 62.7 - - 72.8 58.5 63.4 66.8 73.1 74.5 enhance the generalization performance of the model, we adopted an approach that utilized all three window sizes and found that incorporating them yielded the best overall performance. Table 7. Generalization performance comparison. Tr.: crossdomain training where models trained on one VAD dataset are evaluated on another. Few.: methods that adapt to the target domain using only few training samples, Aux.: methods that utilize auxiliary datasets, *: since competitors did not perform crossdomain evaluations on ShT, we present their same-domain results instead. Method STEAL-Net[5] Jigsaw[29] rGAN[17] MPN[19] zxVAD[3] Shibao et al.[8] ZS CLIP[22] ZS ImageBind[9] AnyAnomaly Tr. Few. Aux. Ave 54.3 62.9 76.6 78.9 82.2 86.2 62.3 64.5 87.3 ShT 51.7 59.3 77.9* 73.8* 71.6* 78.7 60.9 61.3 79.7 8 To assess the effectiveness of AnyAnomaly in handling multiple text inputs, we conducted experiments on the VAD benchmark datasets. For performance evaluation, each anomaly class in the dataset was treated as X, and the maximum anomaly score among all computed scores was assigned to the corresponding segment. Table 6 presents performance comparison with frame-centric VAD methods. Despite not being trained on VAD datasets, AnyAnomaly demonstrated performance comparable to that of SOTA methods. Notably, it achieved new SOTA performance of 74.5% on the UB dataset and the second-best performance of 79.7% on the ShT dataset. These results highlight the effectiveness of the proposed model, which leverages the LVLM and context-aware VQA, achieving performance on par with that of fully supervised methods. 4.7. Generalization Performance Comparison Table 7 presents comparison of the generalization performance of AnyAnomaly. Although STEAL-Net [5] and Jigsaw [29] achieved high accuracy in same-domain testing, their performance was significantly degraded in crossdomain settings. Specifically, on the Ave dataset, the performances of STEAL-Net and Jigsaw decreased as 87.1% 54.3% and 92.2% 62.9%, respectively. Similarly, on the ShT dataset, their performance decreased as 73.7% 51.7% and 84.3% 59.3%, respectively. This suggests that the existing OCC-based VAD models tend to overfit the training data, making them less effective when applied to new environments. For instance, Too close where an object is in close proximity to the camera is considered anomalous in the Ave dataset but normal in the ShT dataset. Consequently, OCC-based models trained on ShT struggle to detect such anomalies. The zeroand few-shot VAD models designed for xVAD exhibited better generalization performance than the OCCbased models. However, few-shot models depend heavily on the number of K-shot samples, whereas zero-shot models require auxiliary datasets. Training-free methods such as ZS-CLIP and ZS-ImageBind leverage strong image understanding capabilities and outperform some VAD models. Nevertheless, their performance remains limited by domain gaps. In contrast, AnyAnomaly effectively overcomes the domain gap by incorporating contextual information, achieving superior performance. 5. Conclusion We propose AnyAnomaly, novel approach that leverages the LVLM for universal VAD. AnyAnomaly effectively performs the C-VAD by incorporating segmentlevel approach and context-aware VQA. This design reduces latency when processing large videos and minimizes the domain gap between the LVLM and VAD task. Despite being zero-shot method, AnyAnomaly demonstrates competitive performance on benchmark datasets and holds promise for real-world VAD. Furthermore, because it operates without any training and enables anomaly detection in any video, it significantly improves accessibility in the VAD domain. We anticipate that AnyAnomaly will contribute substantially to VAD research and practical deployment."
        },
        {
            "title": "References",
            "content": "[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, and Mubarak Shah. Ubnormal: New benchmark for supervised open-set the video anomaly detection. IEEE/CVF conference on computer vision and pattern recognition, pages 2014320153, 2022. 2, 5, 8, 11 [2] Sunghyun Ahn, Youngwan Jo, Kijung Lee, and Sanghyun Park. Videopatchcore: An effective method to memorize normality for video anomaly detection. In Proceedings of the Asian Conference on Computer Vision, pages 21792195, 2024."
        },
        {
            "title": "In Proceedings of",
            "content": "[3] Abhishek Aich, Kuan-Chuan Peng, and Amit RoyChowdhury. Cross-domain video anomaly detection without target domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 25792591, 2023. 3, 8 [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. 2 [5] Marcella Astrid, Muhammad Zaigham Zaheer, and Seung-Ik Lee. Synthetic temporal anomaly guided end-to-end video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 207214, 2021. 8 [6] Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. Appearance-motion memory consisIn Protency network for video anomaly detection. ceedings of the AAAI conference on artificial intelligence, pages 938946, 2021. 8 [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. impressing gpt-4 Vicuna: An open-source chatbot with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 3 [8] Shibo Gao, Peipei Yang, and Linlin Huang. Sceneadaptive svad based on multi-modal action-based feature extraction. In Proceedings of the Asian Conference on Computer Vision, pages 24712488, 2024. 8 [9] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Imagebind: One embedJoulin, and Ishan Misra. In Proceedings of the ding space to bind them all. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 8 [10] Seungkyun Hong, Sunghyun Ahn, Youngwan Jo, and Sanghyun Park. Making anomalies more anomalous: Video anomaly detection using novel generator and destroyer. IEEE Access, 2024. [11] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with In Proceedings of the IEEE condeep networks. ference on computer vision and pattern recognition, pages 24622470, 2017. 3 [12] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1960619616, 2023. 3, 4 [13] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with imIn Proceedings of the age and video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370013710, 2024. 3, 11, 12, 14 [14] Dongha Lee, Sehun Yu, and Hwanjo Yu. Multi-class data description for out-of-distribution detection. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 13621370, 2020. 1 [15] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection In Proceedings of the IEEE cona new baseline. ference on computer vision and pattern recognition, pages 65366545, 2018. 1, 2 [16] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event In Proceedings of detection at 150 fps in matlab. the IEEE international conference on computer vision, pages 27202727, 2013. 5, [17] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly In Computer VisionECCV 2020: 16th detection. European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part 16, pages 125141. Springer, 2020. 8 [18] Weixin Luo, Wen Liu, and Shenghua Gao. revisit of sparse coding based anomaly detection in stacked rnn framework. In Proceedings of the IEEE international 9 conference on computer vision, pages 341349, 2017. 5, 11 IEEE/CVF International Conference on Computer Vision, pages 1033010340, 2023. 8 [19] Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang. Learning normal dynamics in videos with meta prototype network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1542515434, 2021. [20] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 27942802, 2017. 3 [21] Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, and Mateusz Kozinski. Mulde: Multiscale log-density estimation via denoising score matching for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1886818877, 2024. 8 [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 8 [23] Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. 12, 14 [24] Tal Reiss and Yedid Hoshen. An attribute-based method for video anomaly detection. Transactions on Machine Learning Research. 1, 8 [25] Nicolae-C Ristea, Florinel-Alin Croitoru, Radu Tudor Ionescu, Marius Popescu, Fahad Shahbaz Khan, Self-distilled masked autoMubarak Shah, et al. encoders are efficient video anomaly detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1598415995, 2024. 8 [26] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14318 14328, 2022. 1 [27] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep In International conference one-class classification. on machine learning, pages 43934402. PMLR, 2018. [28] Chenrui Shi, Che Sun, Yuwei Wu, and Yunde Jia. Video anomaly detection via sequentially learning the multiple pretext In Proceedings of tasks. [29] Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, and Di Huang. Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles. In European Conference on Computer Vision, pages 494511. Springer, 2022. 8 [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 4, 11 [31] Cheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, and Wenjun Wang. Feature prediction diffusion model In Proceedings of the for video anomaly detection. IEEE/CVF International Conference on Computer Vision, pages 55275537, 2023. 8 [32] Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu. Dynamic local aggregation network with adaptive clusterer for anomaly detection. In European Conference on Computer Vision, pages 404421. Springer, 2022. [33] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. Video event restoration based on keyframes for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1459214601, 2023. 1, 3, 8 [34] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: arXiv preprint gpt-4v level mllm on your phone. arXiv:2408.01800, 2024. 3, 11, 12, 14 [35] Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In Proceedings of the Asian conference on computer vision, 2020. 1 [36] Yuanhong Zhong, Xia Chen, Jinyang Jiang, and Fan Ren. cascade reconstruction model with generalization ability evaluation for anomaly detection in videos. Pattern Recognition, 122:108336, 2022. 8 [37] Hang Zhou, Jiale Cai, Yuteng Ye, Yonghui Feng, Chenxing Gao, Junqing Yu, Zikai Song, and Wei Yang. Video anomaly detection with motion and appearance guided patch diffusion model. arXiv preprint arXiv:2412.09026, 2024. 8 [38] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3, 12, 10 AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Experiment Details A.1. Dataset Details VAD Dataset. We used the CUHK Avenue (Ave) [16], ShanghaiTech Campus (ShT) [18], and UBnormal (UB) [1] datasets. Ave comprises of videos captured by single camera on university campus, containing five types of abnormal events; throwing paper, running, dancing, approaching the camera (Too close) and bicycle. ShT is campus CCTV dataset includes 13 different background scenes and 11 types of abnormal events; such as bicycles, cars, fighting, and jumping. UB is synthetic dataset generated using the Cinema4D software, encompassing 29 diverse background scenes, including indoor environments, sidewalks, and etc. It provides total of 22 abnormal events, including not only challenging-to-detect events such as smoking and stealing but also complex scenarios such as driving outside the lane and people-car accidents. that C-VAD Dataset. We constructed the Customizable-ShT (CShT) and Customizable-Ave (C-Ave) datasets. C-ShT reorganizes the test data of ShT into 11 abnormal event types and assigns new labels to each type. For example, in the bicycle category, videos containing bicycles were assigned to positive, whereas all other videos were assigned to negative. The frame-level labels were set to 1 only for frames in which bicycle appeared in the positive videos. C-Ave was constructed by reorganizing the test data of Ave into 5 abnormal event types, following the same labeling methodology as C-ShT. A.2. Implementation Details In key experiment using the C-VAD datasets, we employed an efficient Chat-UniVi [13] 7B model, considering the balance between performance and speed. For the VAD dataset experiment, we utilized the effective MiniCPM-V [34] 8B model to achieve optimal performance and compared it with state-of-the-art (SOTA) models. The CLIP model used for key frames selection and context generation was ViT-B/32. For context generation, we adopted large, middle, and small window sizes of (120,120), (80,80), and (48,48), respectively. For C-Ave and Ave, the large window size was set to (240,240). All the experiments were conducted on single NVIDIA GeForce RTX 3090 GPU. A.3. Prompt Details Figure S1 shows the detailed prompts used in the experiments. First, reasoning prompt is designed to obtain the Table S1. Comparison on prompt tuning Prompt Tuning Baseline (simple) Baseline (+reasoning) Baseline (+reasoning, consideration) Proposed (simple) Proposed (+reasoning) Proposed (+reasoning, consideration) C-ShT C-Ave 67.58 70.38 72.79 71.58 79.43 78.01 74.01 79.29 82.09 79.79 90.27 85. chain-of-thought [30] effect by requiring simple reason along with the anomaly score. This helps to break down the problem step-by-step, guiding the model to resolve complex issues more systematically. For example, the question Does the image include jumping? can be divided into two steps: 1. Is there an object related to jumping (e.g., person)? and 2. Is the object performing jumping action? This allows object-level image analysis, leading to more refined predictions. The consideration prompt encourages the assignment of high score even when is not central within the image. This prompt was introduced to address the issue where low scores are assigned simply because exists but is not the central element. The effectiveness of this prompt tuning is compared and analyzed in Table S1. The simple prompt instructs the LVLM to output only the anomaly score, while adding reasoning prompt the model to perform reasoning during the score calculation process, and applying consideration prompt encourages the model to focus on the given text. Experimental results showed that using both reasoning and consideration prompt achieved the best performance, suggesting that when the LVLM includes reasoning in the process, it produces more accurate results and can respond more precisely to user instructions through consideration prompt. B. Additional Quantitative Evaluation B.1. Hyperparameter Tuning We tuned the three hyperparameters γ1, γ2, andγ3 used for the final anomaly score calculation for each VAD dataset. Each hyperparameter controls the influence of the anomaly score derived from the frame, position, and temporal contexts. As shown in Table S2, the optimal hyperparameter values vary across datasets owing to differences in object sizes and abnormal events. Additionally, comparing w/o context, which does not utilize context information, and w/o tuning, where all hyperparameters were set to the same value, we observed performance improvements of 3.0%, Figure S1. Prompt details. The content written in the simple version is not utilized when applying reasoning. Table S2. Comparison of different methods on various datasets Dataset Method Value AUC Ave ShT UB w/o context w/o tuning w/ tuning w/o context w/o tuning w/ tuning w/o context w/o tuning w/ tuning - 1.0, 1.0, 1.0 0.6, 0.3, 0.1 - 1.0, 1.0, 1.0 0.5, 0.3, 0.2 - 1.0, 1.0, 1.0 0.6, 0.1, 0.3 81.4 84.4 87.3 77.2 79.4 79.7 73.1 73.8 74. 2.2%, and 0.7%, even without hyperparameter tuning. In contrast, the performance differences owing to hyperparameter tuning were 2.9%, 0.3%, and 0.7%, respectively. This demonstrates the effectiveness of our proposed approach in utilizing context information in VAD and proves that it achieves strong generalization performance even without hyperparameter tuning. B.2. Diverse LVLM Comparison Table S3 presents the results for C-ShT and C-Ave when using various LVLMs. We evaluated the performances of four SOTA LVLMs: Chat-UniVi [13], MiniGPT-4 [38], MiniCPM-V [34], and LLAVA++ [23]. All experiments were conducted using the default settings, and Pre-trained refers to the names of the pre-trained model weights. The experimental results demonstrate that incorporating the proposed context-aware VQA improves the performance of all LVLMs. Specifically, the use context-aware VQA leads to improvements ranging from 2.6% to 24.8%. Notably, even MiniCPM, which achieved the best performance without context-aware VQA, and showed additional improvements of 2.7% and 5.4% for C-ShT and C-Ave, respectively, when context-aware VQA was applied. This confirms that leveraging the proposed context-aware VQA is effective for C-VAD. Additionally, we observed that Chat-UniVi, with an FPS of 6.67, was the most efficient model, whereas MiniCPM-V achieved the highest performance on both datasets, scoring 90.1% and 91.0%, respectively. Therefore, as mentioned in Appendix A.1, Chat-UniVi was used for the C-VAD experiments and MiniCPM-V was used for the VAD dataset experiments. C. Additional Qualitative Evaluation C.1. Context Complementarity In this section, we explain the complementarity between and in context-aware VQA. Figure S2 visualizes the key frame of specific segment along with the images generated using WA and GIG for of and C. We also present the results of context-aware VQA that utilizes these contexts. 12 (key frame, position context, and temporal context) and involves reasoning process, which makes real-time analysis more challenging. Furthermore, when multiple abnormal events occur simultaneously, each event must be processed independently, which leads to substantial increase in latency. Hence, our future studies will aim to enhance to the efficiency of the C-VAD in handling multiple abnormal events simultaneously. In the first row, when the text input was bicycle, successfully identified the bicycle via WA, yielding score of 1.0. However, the temporal context suffers from cropping effect due to motion over time, resulting in lower score of 0.5. In the second row, when the text input is jumping, the attention result from WA fails to accurately locate the jumping person. Additionally, because of the lack of temporal information, was unable to recognize the jumping action, resulting in score of 0.0. In contrast, captured the entire jumping action over time, achieving score of 0.9. These results demonstrate that the proposed C, which focuses on the object appearance, and C, which leverages temporal information, are complementary. By integrating both approaches, we enable an effective generalization of the VAD. C.2. Anomaly Detection in Diverse scenarios Figure S3 visualizes the results of VAD performed on videos containing multiple abnormal classes. The captions in each figure indicate the abnormal classes used in the corresponding video. We input the user-defined abnormal keywords as text individually to obtain the scores, and assigned the highest score as the anomaly score for the corresponding segment. As shown in the visualization results, the proposed AnyAnomaly enables VAD across various types of abnormal events. This demonstrates that AnyAnomaly can be effectively utilized even when the user aims to simultaneously detect multiple abnormal types. C.3. Anomaly Detection in Complex scenarios Figure S4 presents the visualization results of AnyAnomaly on complex scenarios. Key Frame, Position Context, and Temporal Context visualize ˆk, C, and C, respectively. The text below each figure represents the LVLM output. These visualization results demonstrate that the proposed context-aware VQA, which utilizes and C, is effective and contributes to improving VAD performance. Additionally, in Figure S4d, we observe that the model can detect certain frames of walking drunk even without utilizing context information. This suggests that the strong visual reasoning capabilities of the LVLM enable VAD in complex scenarios. However, as shown in Figure S4aS4c, relying solely on individual frames is insufficient for fully leveraging these reasoning abilities. Therefore, the proposed context-aware VQA approach is essential for effective VAD. D. Limitations Efficiency is crucial in VAD; therefore, we utilized the most lightweight model among the SOTA LVLMs and adopted segment-level approach to significantly reduce the latency. However, our method still requires three inputs per segment 13 Table S3. Comparison of diverse LVLMs. The model highlighted in blue represents the most efficient model for the C-VAD task, while the one highlighted in purple indicates the most effective model. LVLM Pre-trained C-ShT C-Ave w/o context Proposed w/o context"
        },
        {
            "title": "Proposed",
            "content": "Chat-UniVi[13] MiniGPT-4[38] MiniCPM-V[34] LLAVA++[23] Chat-UniVi-7B LLaMA-2 Chat 7B MiniCPM-Llama3-V-2 5 (8B) LLaVA-Meta-Llama-3-8B-Instruct-FT 77.5 54.0 87.7 73.3 85.7 67.4 90.1 82.8 78.3 53.9 86.3 59.0 90.3 55.3 91.0 69. FPS 6.67 1.26 1.36 7.25 Figure S2. Example of complementarity between position and temporal context. The first example highlights the importance of position context and the second example emphasizes the importance of temporal context. (a) jumping-falling-pickup (b) bicycle-running (c) bicycle-stroller Figure S3. Anomaly detection in diverse scenarios. Various abnormal events can emerge over time. 14 (a) Anomaly event: jaywalking (b) Anomaly event: driving outside lane (c) Anomaly event: people and car accident (d) Anomaly event: walking drunk Figure S4. Anomaly detection in complex scenarios. Results with and without the inclusion of context are presented."
        }
    ],
    "affiliations": [
        "Yonsei University, Seoul, Korea"
    ]
}