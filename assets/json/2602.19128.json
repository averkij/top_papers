{
    "paper_title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
    "authors": [
        "Shiyi Cao",
        "Ziming Mao",
        "Joseph E. Gonzalez",
        "Ion Stoica"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 8 2 1 9 1 . 2 0 6 2 : r K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model Shiyi Cao, Ziming Mao, Joseph E. Gonzalez, Ion Stoica UC Berkeley Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with co-evolving world model, our framework leverages LLMs prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10 improvement and up to 14.3 gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030 µs and surpassing both prior evolution and human-designed solutions. Code: https://github.com/caoshiyi/K-Search"
        },
        {
            "title": "1 Introduction",
            "content": "High-performance GPU kernels are fundamental to modern machine learning systems for training and serving large models, as reflected in widely used optimized libraries such as FlashInfer (Ye et al., 2025) for highthroughput LLM serving (Kwon et al., 2023; Zheng et al., 2024), FlashAttention (Dao et al., 2022; Dao, 2023; Shah et al., 2024) for memory-efficient attention, and FlashLinearAttention (fla org, 2024) for emerging model architecture variants (Gu and Dao, 2024; Peng et al., 2023). Unfortunately, GPU kernels are notoriously challenging to manually optimize and tune. First, achieving near-peak performance on modern GPUs requires navigating large design space of tiling, memory layout, synchronization, and architecture-specific primitives (NVIDIA, 2025). Second, fast hardware evolution adds additional optimization complexity; new architectures (e.g., transitioning from NVIDIA Hopper to Blackwell) introduce new instructions and architectural characteristics that fundamentally alter performance trade-offs, rendering previously optimized kernels sub-optimal. Third, testing kernel optimization might require significant implementation effort with many manual trials and errors (Ouyang et al., 2025). Compiling and profiling generated kernels is also computationally expensive, normally mandating strictly limited testing budgets (Zheng et al., 2020). Therefore, automated kernel generation methods that can adapt efficiently to new workloads and hardware with low search budgets become increasingly important. Existing LLM evolutionary approaches, such as OpenEvolve (Superintelligence, 2025), couple language models with genetic algorithms or quality-diversity search. These methods typically treat LLMs purely as stochastic code generators, relying on heuristic mechanisms such as MAP-Elites (Mouret and Clune, 2015) to select and mutate candidates directly in the program space. However, high-performance kernels often require coordinated, multi-step structural transformations, such as refactoring memory layout before applying vectorization, where intermediate steps may not yield immediate performance gains. By treating the LLM merely as code 1 generator without an explicit planning mechanism, existing evolutionary methods typically cannot plan multi-step optimization sequences in which intermediate edits fail to improve the objective, and they often prematurely discard theoretically sound strategies due to temporary compilation errors, limiting their ability to discover deep structural optimizations necessary for achieving state-of-the-art performance. To address this, we propose Search via Co-Evolving World Model and build the K-Search framework, inspired by recent works (Fang et al., 2025; Hao et al., 2023) on using large language models as world models to guide planning and decision making. We formulate kernel generation as planning problem over structured search tree, governed by World Model instantiated from an LLM to leverage its prior domain knowledge for efficient search. In this setup, the World Model is responsible for maintaining the search frontier and estimating the priority scores of high-level optimization intents. Crucially, this model is co-evolving with the search process: it continuously refines its transition dynamics by assimilating execution feedback via in-context learning, allowing it to dynamically update its prior belief and calibrate the search strategy. This approach explicitly decouples high-level planning from low-level program instantiation, enabling the system to navigate complex, non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse set of complex workloads from FlashInfer (Ye et al., 2025), including GQA (Yang et al., 2025), MLA (flashinfer-ai, 2026), and MoE (Liu et al., 2024) kernels. Our results demonstrate that K-Search significantly outperforms state-of-the-art baselines, achieving an average 2.10 improvement over OpenEvolve and an average 2.21 improvement over ShinkaEvolve. Notably, on the challenging MoE kernel, K-Search delivers 14.3 improvement over OpenEvolve. We also test K-Search on the GPUMode TriMul task, and it achieves state-of-the-art performance (1030 µs on H100), surpassing both prior automated and human-designed solutions. Collectively, these results validate the efficacy of disentangling high-level intent from low-level implementation to enable deep structural optimization."
        },
        {
            "title": "2 Related Work",
            "content": "Fast iteration and specialized kernel libraries. Recent years have seen substantial engineering effort devoted to highly optimized, workload-specific GPU kernel libraries. Examples include FlashAttention (Dao et al., 2022; Dao, 2023; Shah et al., 2024) for dense attention, FlashLinearAttention (fla org, 2024) for wide range of linear and state-space attention variants such as Mamba (Gu and Dao, 2024) and RWKV (Peng et al., 2023), and FlashInfer (Ye et al., 2025) for high-throughput LLM serving with paged KV-cache and dynamic batching. Although each library targets relatively narrow class of operators, achieving near-peak performance requires careful architecture-specific tuning. As model architectures continue to diversify and introduce new attention mechanisms and custom sequence operators, manually developing and maintaining specialized kernels becomes increasingly costly, motivating automated LLM-based kernel generation methods that can rapidly adapt to new workloads and hardware. Compiler autotuning, DSLs, and kernel optimization. Automated kernel optimization has long history in compilers and high-performance systems. TVM (Chen et al., 2018) and its associated auto-schedulers, such as Ansor (Zheng et al., 2020), optimize tensor programs by employing learned cost models to search large scheduling spaces. Parallel to these search-based approaches, domain-specific frameworks like Triton (Tillet et al., 2019) and layout abstractions like CuTe (NVIDIA, 2023) enable higher-level expression of GPU kernels, allowing developers to explicitly manage architecture-aware tiling and memory hierarchies. Collectively, these systems illustrate the immense complexity of GPU kernel optimization, highlighting the necessity for methods capable of coordinating tiling, memory layouts, and specialized instructions to maximize performance. LLMs for GPU kernel generation. Most existing LLM-based GPU kernel generation systems (Wei et al., 2025; Zhang et al., 2025; Lange et al., 2025b; Liao et al., 2025; Li et al., 2025a) employ simple iterative search or refinement pipelines, where LLMs generate kernel variants based on compilation results, execution, and profiling feedback. Recent work augments this paradigm with evolutionary strategies to improve exploration, such as EvoEngineer (Guo et al., 2025), which maintains and manages population of candidate kernels during search. In parallel, several works (Li et al., 2025c,b; Baronio et al., 2025) leverage reinforcement learning to train models to generate optimized CUDA or Triton kernels, primarily focusing on enhancing the models one-shot generation or local refinement capabilities. 2 LLM-guided evolutionary and population-based program search. Recent work has explored coupling large language models with execution-based evaluation and evolutionary search to discover high-quality programs. FunSearch (Romera-Paredes et al., 2024) pairs an LLM with an evaluator in an evolutionary loop to improve solutions in mathematical and combinatorial domains. AlphaEvolve (Novikov et al., 2025) generalizes this paradigm to codebase evolution using LLM-generated edits and program database that seeds subsequent generations. OpenEvolve (Superintelligence, 2025), the open-source realization of these ideas, instantiates archive-based evolution with explicit island models and quality-diversity mechanisms such as MAP-Elites (Mouret and Clune, 2015). ShinkaEvolve (Lange et al., 2025a) proposes population-based evolutionary framework that combines performance-driven selection with novelty-aware rejection to improve sample efficiency. Critically, these methods fundamentally treat the LLM merely as stochastic code generator. They search directly in the space of program implementations, relying on evolutionary heuristics to drive progress rather than leveraging the LLMs capacity for high-level planning or reasoning. Large Language Models as World Models Recent work suggests that LLMs can function as implicit or explicit world models for planning and decision making. RAP (Hao et al., 2023) frames reasoning as planning with an LLM world model. Complementary approaches externalize dynamics by inducing structured domain models (e.g., PDDL) from language and refining them for classical planning (Guan et al., 2023). Recent works (Fang et al., 2025; Gu et al., 2024) further demonstrate that LLMs can act as world models to guide agentic planning tasks by simulating action outcomes and evaluating candidate trajectories. Together, these studies highlight an emerging perspective of treating LLMs as structured world models that support search and model-based planning."
        },
        {
            "title": "3.1 Problem Setup",
            "content": "We formulate GPU kernel synthesis as an optimization problem under fixed evaluation budget. We list the core notations in Table 1. For given kernel program x, the evaluator returns an observation tuple: Table 1 Notation and Definitions = (s, p, m) = E(x), Symbol where {0, 1} indicates correctness, R+ is the performance metric (i.e., latency), and contains metadata (e.g., compiler logs, profiler output). We define the maximization objective J(x) as the speedup relative to reference SoTA baseline (pref): J(x) = pref 100. Definition Kernel program implementation Observation tuple from execution Evaluator function Scalar objective score History {(xi, oi)}t Search state Frontier of actions E : : Ht St A(St) (a St) [0, 1] Model-estimated priority score i= The optimization goal is to identify program = arg maxxX J(x) utilizing fixed budget of evaluations."
        },
        {
            "title": "3.2 Search via Co-Evolving World Model",
            "content": "Large Language Models possess rich intrinsic prior knowledge regarding optimization heuristics and strong planning capabilities (Zhao et al., 2023; Bohnet et al., 2025). However, existing evolution methods often underutilize these capabilities by treating the LLM merely as code generator. We show that effective search requires disentangling algorithmic planning (leveraging intrinsic and evolving understanding) from implementation. 3 Figure 1 Overview of K-Search. The framework operates on Search State St structured as search tree. The tree consists of Closed nodes (blue, visited states with attached program like x12) and Frontier of Open nodes (orange, pending hypotheses like u13). The workflow iterates through three phases: (1) Action Selection, where the most promising action node is retrieved from the frontier based on world model estimated priority score ; (2) Local Refinement, where stochastic policy πcode samples concrete implementations until stagnation; and (3) World Model Update, where the LLM reasons over the trajectory to update the search tree via Insert (adding new actions), Update (adjusting , e.g., u11 dropping from 0.9 to 0.6), and Prune (removing less promising nodes like u10). Baseline: Heuristic Search in Program Space. Existing approaches search directly in the space of the program. At step t, they select context subset Ct Ht using evolutionary heuristics such as MAPElites (Mouret and Clune, 2015; Romera-Paredes et al., 2024; Superintelligence, 2025)) and generate new candidate by conditioning on the raw text of previous programs and their associated execution feedback: xt+1 πLLM (cid:16) (cid:12) (cid:12) (cid:12) {(xk, ok)}(xk,ok)Ct (cid:17) . Here, the observation ok is serialized into textual prompt (e.g., compiler error messages or profiler logs). This formulation inherently couples high-level optimization intent with low-level implementation. Lacking an explicit planning mechanism, theoretically sound strategy may be discarded simply because of transient syntax error in xt+1. Ours: Search via Co-Evolving World Model. We structure code generation as search process guided by an LLM repurposed as world model with an intrinsic understanding of the design space. The World Model estimates the next state of the search process after applying an action to the current state (Ha and Schmidhuber, 2018; Hao et al., 2023; Matsuo et al., 2022). Formally, the LLM world model represents search state (St) transition distribution Pmodel(St+1 St, at). search state St encapsulates the current snapshot of the models understanding on the search process, including the history of explored actions and their performance, the frontier A(St) actions (i.e., the set of pending, unexplored actions available), and the estimated priority score over the frontier actions. Crucially, this model is co-evolving with the search process: it continuously refines its understanding and beliefs based on past trials and accumulated experience via in-context learning. The search proceeds through three iterative phases: 1. Action Selection. The hypothesis with the highest priority score from the search states current frontier A(St) is selected as the next action: at = arg max aA(St) (a St). Here, an action at = (xparent, δ) represents specific intent δ (e.g., Resolve bank conflicts via padding) applied to specific parent program xparent. is scalar priority score predicted by the world model when 4 the action is created (or updated). It represents the models intrinsic assessment of the potential of the actions. 2. Program Instantiation. concrete program is then synthesized by applying the selected plan to the parent implementation found in St: xt πcode(x at), ot = E(xt). 3. World Model Co-Evolution. Upon observing the result ot of xt (i.e., the outcome of action at), the world model then performs the search state transition to obtain St+1 conditioned on the newly accumulated experience, which updates and calibrates : St+1 Pmodel(S St, at; xt, ot). This co-evolution allows the world model to update its prior assumptions and beliefs effectively, and perform the state transition to progressively sharpen the search policy."
        },
        {
            "title": "3.3 System Design",
            "content": "We build K-Search based on the concept of Search via Co-Evolving World Model. In our design, the LLM functions as an intrinsic world model that maintains and evolves tree-structured state of the search process. The system operates by iteratively expanding, updating, and pruning an explicit search tree, decoupling high-level planning from low-level code generation. Figure 1 and Algorithm 1 illustrate the complete workflow of K-Search. Search State. The search state St is maintained as an explicit search tree partitioned into Closed and Open nodes. Closed nodes (blue boxes in Figure 1, e.g., x12) represent visited states where the local refinement process has concluded and the best-found programs are attached. In contrast, Open nodes (orange dashed boxes, e.g., u13) form the frontier A(St) of pending actions waiting to be realized. Each Open node encapsulates Proposed Optimization tuple (xparent, δ), linking parent program to specific natural language intent, and Priority Score [0, 1]. This score is dynamically updated by the world model (e.g., u11 in the figure shows dropping from 0.9 0.6) to guide the selection of the next action. Algorithm 1 K-Search: Search via Co-Evolving World Models 1: Input: Spec , Evaluator E, Budget B, Stagnation Limit 2: Init: Init(T ) 3: while > 0 do 4: 5: 6: 7: at arg maxaA(S) (a S) 0 xbest , obest 1. Selection: Retrieve best action from frontier Stagnation counter 2. Instantiation: Local refinement loop while > 0 and < do πcode( at) E(x); 1 if J(x) > J(xbest) then xbest x; 0 obest Reset counter on improvement else + 1 Increment on failure end if end while Pmodel(S, at, xbest, obest) 19: 20: 21: end while 22: return Best found 3. Evolution: Insert, Update, and Prune 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Execution: Local Refinement. To isolate logical reasoning from implementation details, we treat program instantiation as local refinement task. Upon selecting an action at (Step 1 in Figure 1), the system employs the LLM as stochastic policy πcode. We repeatedly sample implementations πcode( at) and evaluate them (o = E(x)) until stagnation condition is met (K consecutive attempts without improvement). This ensures that valid actions are not discarded due to transient syntax errors or minor bugs. 5 Evolution: World Model Update. Upon concluding local refinement, the LLM analyzes the execution trajectory to update the state St through three kinds of Tree Edit Operations (Step 3 in Figure 1). It performs Insert to propose new child nodes extending the current state (e.g., adding action u13 with intent δ = fuse head) and Update to re-evaluate the priority of existing frontier nodes based on new evidence. Additionally, the model applies Prune to identify and permanently remove infeasible or redundant branches (e.g., node u10), thereby concentrating search resources on promising directions. Note that in the current version of K-Search, the world model evolution is merely performed by in-context learning of past observations."
        },
        {
            "title": "3.4 Case Study: MLA Paged Decode",
            "content": "Figure 2 K-Search Search Trace Visualization. It tracks the evolution of the Search State across search rounds on the MLA Paged Decode kernel (refer to Section 4 for setup details). round corresponds to one candidate program evaluation. Nodes represent actions (blue=Closed, orange=Open), annotated with their instantiated program performance (closed nodes) or priority scores (open nodes). The timeline highlights how the kernel is improved and how the LLM dynamically Inserts new hypotheses, Updates beliefs, and Prunes less promising branches based on evolved understanding. We illustrate the co-evolutionary search process using the MLA Paged Decode kernel (refer to Table 2 for kernel details). Figure 2 visualizes how the World Model co-evolves with the kernel implementation. We track the progress in rounds, where each round corresponds to single invocation of the evaluator E(x) (i.e., one compiled and benchmarked kernel, consuming one unit of the budget). r1: Initialization and Hypothesis Selection. The search initializes S0 with three high-level actions in the frontier: fused_multi_head, split_k_decoding, and independent_heads. The World Model assigns the highest value (V ) to fused_multi_head, hypothesizing that processing shared CKV heads together will reduce global memory traffic by 16 compared to independent processing, and selects it for the first round of program instantiation. r14-r34: Tree Evolution via Topological Edits. Following the local refinement of fused_multi_head (score = 34), the model evolves the topology of St to reflect this feedback. It first Inserts refinement actions such as register_resident_rescaling and occupancy_tuned_chunk32 to deepen the successful branch. Simultaneously, it Updates the belief state by downgrading the sibling independent_heads, reasoning that the proven efficacy of head fusion renders independent processing less promising. By round 34, as evidence 6 accumulates, the model permanently Prunes the independent_heads branch, reallocating search resources entirely to the register_resident subtree. r42-r102: Structural Insight and Experience Accumulation. At round 42, the World Model exhibits structural shift in reasoning. It deletes the initial root-level split_k action but re-Inserts targeted variant, low_overhead_split_k, deep within the register_resident branch. This edit reflects learned insight: split-K is ineffective as an isolated baseline but highly effective as composable optimization atop strong fusion kernel. Notably, after chunk32_vectorized succeeds, the model proposes chunk32_prescale_vectorized to apply sm_scale immediately upon loading Q. This specific refinement eventually yields the global optimum (star marker) at r102. Takeaway. This trace highlights the efficiency of K-Search. By starting with high-level intent rather than raw code, the system avoids enumerating massive sparse search space. It relies on local refinement to filter out transient coding noise and allows the World Models understanding to co-evolve with the kernels optimization progress. Such evolution enables the system to prune dead ends and dynamically reposition strategies, guided by intrinsic reasoning over accumulated experience."
        },
        {
            "title": "4 Experiments",
            "content": "Table 2 Representative kernels used for evaluation. Detailed configurations (e.g., head counts) are omitted for brevity. We provide example optimization challenges based on FlashInfer (flashinfer-ai, 2026) implementations. Kernel MLA Paged Prefill Arch. Hopper Model Context DeepSeek-V3 (Liu et al., 2024) MLA Paged Decode Hopper DeepSeek-V3 Characteristics & Challenges Implements attention with paged KV cache and split ckv (compressed, no RoPE) and kpe (RoPE). Bandwidth-bound in long contexts; requires matrix absorption and Split-K optimization. Targets large dynamic batches. Latency-bound at low batch sizes. Uses Persistent Data Layout and specialized Hopper-instructions to minimize memory movement. GQA Paged Decode Hopper Qwen3-A3B-30B (Yang et al., 2025) Memory bound. Optimizations include fusing multiple query heads per KV head to reuse KV loads, and using vectorized/async paged-KV gather with pipelined global to shared memory prefetch. FP8 MoE Blackwell DeepSeek-V3 Challenges include irregular data-dependent routing, load balancing, and managing FP8 packing/scaling overhead."
        },
        {
            "title": "4.1 Setup",
            "content": "Our main evaluation is conducted on FlashInfer (Ye et al., 2025) kernels, since these kernels are highlyoptimized by experienced human engineers. For each target kernel, we run each system for fixed budget of 120 iterations (we define one iteration as one evaluation of single candidate kernel on the benchmark workloads), and report the best-so-far score (i.e., J(x) averaged over workloads in Section 3.1) achieved at each iteration, using FlashInfer (Ye et al., 2025) kernels as reference SoTA. We repeat each method three times and report the mean curve with shaded minmax band to indicate the range of scores achieved across repeated experiments. Implementation details. K-Search exposes unified Task interface for plugging in optimization problems. Each task is defined by (i) task specification and (ii) an evaluator. task specification contains the PyTorch reference implementation, the optimization objective, and any task-specific instructions. The evaluator is responsible for compiling candidate implementations, validating functional correctness against the reference code, and measuring performance under standardized benchmarking environment. For FlashInfer kernels, K-Search integrates FlashInfer-Bench (Xing et al., 2026) as the task evaluator. All candidate implementations must be written in CUDA and pass functional correctness tests before receiving non-zero score. We adopt the same compilation toolchain, correctness suite, and benchmark harness across all 7 compared methods to ensure fairness. Experiments are conducted on NVIDIA H100 and B200 GPUs using CUDA 12.8, FlashInfer 0.5.3, and PyTorch 2.8.0. ((a)) Search Process. ((b)) Best Kernel per-Workload Performance. ((c)) Best Kernel Fastp Plot. Figure 3 Main Results (3 runs each). (a) compares the kernels best-so-far scores generated by the three systems across 120 iterations. (b) provides per-workload analysis for all compared systems. (c) shows the fraction of workloads for which the best kernel from each system achieves the specified speedup over the FlashInfer baseline."
        },
        {
            "title": "4.2 Baselines",
            "content": "We evaluate 3 automated kernel-optimization methods: OpenEvolve (Superintelligence, 2025), ShinkaEvolve (Lange et al., 2025a), and K-Search. We used gemini-3-pro-preview and the same initial program for each kernel. For ShinkaEvolve, we used Qwen3-8B as the embedding model. Across all experiments, we standardized the evaluation by using an identical set of input workloads for all methods on each kernel, and adhered to the default configurations for each baseline with the prompt template shown in Section A.2. K-Search sets budget value = 120 and stagnation value = 7."
        },
        {
            "title": "4.3 Kernels",
            "content": "We focus on four representative kernels (i.e., paged attention and MoE) from FlashInfer-Bench (Xing et al., 2026) that are widely used in modern LLM serving. We summarize these kernels in Table 2. Each kernel includes fixed set of test traces used for correctness and benchmarking, captured in real traffic (Xing et al., 2026). We use identical traces for OpenEvolve, ShinkaEvolve, and K-Search to ensure fair comparison. We provide an initial CUDA program for MLA decode kernel generation, as baselines struggle to write working kernels without an initial program."
        },
        {
            "title": "4.4 Evaluation Results\nOverall Performance. Figure 3(a) compares the three systems across 120 iterations for GQA decode, MLA\ndecode, MLA prefill, and MoE kernels (Section 4.3). For each method and kernel, we plot the scores\nover iterations across repeated runs. We find that K-Search significantly outperforms OpenEvolve and\nShinkaEvolve. Across all kernels, K-Search achieves an overall average final score of 56.13, representing a\n2.10 × improvement over OpenEvolve (with score 26.68) and a 2.21× improvement over ShinkaEvolve (with\nscore 25.37). The performance gains vary across kernels. For the MoE kernel, K-Search achieves a final\nscore of 44.1, representing a 14.3× improvement over OpenEvolve (3.09) and a 1.58× improvement over\nShinkaEvolve (27.9). Similarly, for MLA prefill, K-Search achieves a score of 57.4 compared to OpenEvolve’s\n19.5 and ShinkaEvolve’s 11.3, or 2.95× and 5.10× improvements, respectively. For GQA decode, K-Search\nreaches a score of 76.0, outperforming OpenEvolve (44.2) and ShinkaEvolve (27.7) by 1.72× and 2.74×. On\nthe MLA decode, K-Search maintains a final score of 47.1 versus 39.9 and 34.7, representing 18% and 36%\nimprovements.",
            "content": "Best Kernel per-Workload Performance. Figure 3(b) provides per-workload analysis for all compared methods. Each dot represents the kernels performance on particular workload instance. Across 4 kernel types and 152 total workload traces, K-Search achieves higher performance than baselines on the vast majority of workloads. Interestingly, on some workloads for GQA decode, K-Search underperforms OpenEvolve and ShinkaEvolve, specifically those with small batch sizes: 16 of which have batch_size=1, and 4 with batch_size=16. KSearch does not underperform on workload with larger batch_size. This occurs because K-Searchs kernel employs split-K parallelism strategy that divides the key-value sequence across multiple thread blocks to maximize GPU utilization for large batches (described more in Section 4.6). While this approach excels when there is sufficient batch-level parallelism to amortize the coordination overhead, it introduces unnecessary synchronization costs for small batches. In contrast, OpenEvolve and ShinkaEvolve use simpler single-block-per-batch design that processes the entire sequence within one thread block. While this simpler approach proves more efficient for batch_size=1 (no coordination), it leads to bad performance with larger batch size. Best Kernel Fastp Analysis. Figure 3(c) shows the fraction of workloads for which the best kernel from each system achieves the specified speedup over the FlashInfer baseline. We observe that the generated kernels rarely exceed the expert-optimized FlashInfer kernel; however, K-Search significantly outperforms OpenEvolve and ShinkaEvolve. For GQA decode, K-Search attains speedup 0.36 on 100% of workloads, compared to 50% for ShinkaEvolve. At higher thresholds, the gap widens: at speedup 0.50, K-Search succeeds on 87.5% of workloads versus 50.0% and 39.6% for OpenEvolve and ShinkaEvolve, representing 1.75 and 2.21 improvements. For MLA prefill, at speedup 0.40, K-Search reaches this threshold on 57.9% of workloads, while none of the baseline solutions achieve speedup 0.40. Key observations. OpenEvolve and ShinkaEvolve both evolve over an archive of programs. The crucial difference from K-Search is that both directly search in program space and neither system explicitly tracks which optimizations are likely to be correct or to improve performance. Consequently, ShinkaEvolve suffers from low yield of correct programs: in our GQA logs, the vast majority of generations receive score zero (incorrect or failed programs). Search budget is heavily spent mostly on extending invalid or low-performing candidates. OpenEvolve similarly exhibits high per-iteration variance: Many iterations yield programs that underperform the currently best program. On harder tasks such as MoE, OpenEvolves search struggles to escape the low-accuracy regime: mean final score stays near 3 versus K-Searchs 44. We hypothesize that this advantage stems from K-Searchs ability to maintain and co-evolve persistent search state. By conditioning proposals on past attempts, the system generates more targeted hypotheses, significantly improving search efficiency."
        },
        {
            "title": "4.5 Kernel Analysis: FP8 MoE Kernel (Blackwell)\nWe first discuss the FP8 MoE kernel: for each token, the top-k experts are chosen from 256 candidate experts.\nThe kernel then runs an “up” and “gate” projection, combined via SiLU, then a down-projection. All systems",
            "content": "9 generate CUDA kernels for the same specification (DeepSeek-V3 style, top-8 routing, 32 local experts, hidden size 7168). We compare kernels generated by K-Search, OpenEvolve, and ShinkaEvolve. Routing Each token is assigned scores for 256 experts; the kernel then picks the top 8 experts. K-Searchs kernel dedicates one GPU thread block per token (256 threads) and uses warp-level cooperation: threads in warp exchange values (__shfl_down_sync) to find the the global top-8 experts. This keeps work parallel and avoids serialization. OpenEvolve uses persistent kernel where each thread block runs in loop: take the next tile index from global counter (atomicAdd ), identify the (expert, token batch) it corresponds to, then perform computation for that tile. This incurs prohibitive overhead as it requires an atomic operation to process the next tile inside while loop. In ShinkaEvolve, each thread loads all 256 scores for the experts and does plain for-loops to find the top experts, similarly leading to poor performance. Expert FFN computation After routing, tokens must be processed by chosen experts. K-Search uses simple pipeline: (1) routing, (2) sort-scatter that reorders tokens by expert to place them in contiguous memory, (3) computation (gate + up, then SiLU). K-Search uses tensor cores (with WMMA) on small 16 16 blocks and double-buffering so that loading the next block of data overlaps with computing the current one. K-Searchs kernel also skips experts that receive zero tokens. As discussed, OpenEvolve adopts single persistent kernel, which reduces kernel launches but needs more shared memory with lower GPU occupancy. ShinkaEvolves kernel does not use tensor cores; each block performs dot-product-style computation, thereby suffering from low performance."
        },
        {
            "title": "4.6 Kernel Analysis: GQA Paged Decode (Hopper)\nWe next discuss the GQA paged decode kernel. In decode, each batch item contributes a single new query\ntoken, and the kernel attends over the keys and values already stored in the paged KV cache. All three\nsystems—K-Search, OpenEvolve, and ShinkaEvolve—generate CUDA kernels for the same specification.",
            "content": "Parallelism over the sequence In decode, the costly part is sweeping over the full key-value sequence. K-Searchs kernel splits the sequence across multiple blocks: each block is assigned contiguous chunk of keys and values, computes partial attention result for that chunk, and writes it to temporary buffer. When all chunks for given (batch, key-value head) are done, one block detects that it is last (via lightweight counter) and merges the partial results into the final output. Thus, for long sequences, many blocks work in parallel on different segments. OpenEvolve and ShinkaEvolve, however, use single block per (batch, key-value head): that block loops over the entire key-value sequence itself. Therefore, for long sequences, they cannot exploit the same parallelism as K-Search. Overlapping memory and compute K-Search loads keys and values in double-buffered chunks: while the current chunk is used for computation, the next chunk is being fetched. ShinkaEvolve does not doublebuffer: it loads one chunk of keys and values, performs all attention work for that chunk, then loads the next. Memory load and compute are largely serialized, leading to lower performance. We defer discussion of other generated kernels to Section A.1."
        },
        {
            "title": "4.7 GPUMODE TriMul\nTask Overview. GPUMODE is a public kernel optimization competition platform that hosts leaderboard-\nbased challenges on real-world GPU workloads. Participants submit Triton or CUDA implementations that\nmust pass strict correctness checks before being evaluated under standardized benchmarking conditions.",
            "content": "The Triangle Multiplicative Update (TriMul) is core module in AlphaFold3 (Abramson et al., 2024) and other protein structure prediction models. It operates on 4D pair representation RBN C and involves LayerNorm, five gated linear projections with optional masking, pairwise contraction with O(N 3) complexity, and final gated output projection. 10 Configuration. We run K-Search with K=5, reduced from K=7 used on FlashInfer CUDA tasks, as Tritons implementation is simpler than CUDA. The search budget is 300 iterations: 150 steps with GPT-5.2, followed by 150 continuation steps with Gemini-3-Pro starting from the best solution found by GPT-5.2, without seed Triton programs provided. Submission ID Table 3 Top GPUMODE TriMul submissions on NVIDIA H100. Latency is the geometric mean across fixed set of benchmark cases. Results. We report leaderboard results from the official GPUMODE rankings1 for competing submissions. Since the submission period has closed, we evaluate our kernel locally using the official GPUMODE evaluator2 on NVIDIA H100 80 GB HBM3 GPUs. As shown in Table 3, K-Search achieves state-of-the-art performance with geometric-mean latency of 1030 µs, surpassing prior solutions, where the TTT submission is from recent work TTT-Discover (Yuksekgonul et al., 2026) that combines Reinforcement Learning (RL) with evolution methods. CUDA Triton Triton GPT-OSS-20B w/ RL K-Search (Ours) Triton GPT-5.2 + Gemini-3-Pro shiyegao Zeyu Shen TTT 25,600 Latency (µs, ) 1074 1140 Model 1030 Lang. 300 Iter."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce K-Search, demonstrating that LLMs are not merely strong code generators but also possess latent planning capabilities that allow them to function as effective intrinsic world models. By replacing static search heuristics with co-evolving world model, our framework enables the LLM to actively reason over the search space, distinguishing valid strategies from implementation noise. The empirical results, including 2.1 average improvement across diverse complicated kernels, up to 14.3 gains on MoE over state-of-the-art evolutionary baselines, and SoTA performance on GPUMODE TriMul competition, validate this paradigm shift. These findings indicate that LLMs can serve as the core planning engine for complex optimization problems, moving beyond simple task implementation to autonomously driving the search strategy itself."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by the kind compute support from Databricks, Amazon, Anyscale, and gifts from Google, Lambda, AMD, Mayfield, Accenture, Broadcom, Cisco, IBM, Intel, Intesa Sanpaolo, Lightspeed, Mibura, Microsoft, NVIDIA, Samsung SDS, and SAP. We also thank Dacheng Li, Mayank Mishra, Andy Yang, Parth Asawa, Fangzhou Zhao, and Tian Xia for helpful discussion and feedback."
        },
        {
            "title": "References",
            "content": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630(8016):493500, 2024. Carlo Baronio, Pietro Marsella, Ben Pan, Simon Guo, and Silas Alberti. Kevin: Multi-turn rl for generating cuda kernels. arXiv preprint arXiv:2507.11948, 2025. Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, and Noah Fiedel. Enhancing llm planning capabilities through intrinsic self-critique. arXiv preprint arXiv:2512.24103, 2025. 1https://www.gpumode.com/leaderboard/496?tab=rankings 2https://github.com/gpu-mode/reference-kernels/blob/main/problems/bioml/trimul/eval.py Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 578594, 2018. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with co-evolving world model. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 89708986, 2025. fla org. Flashlinearattention. https://github.com/fla-org/flash-linear-attention, 2024. note = Last indexed: 24 January 2026. Accessed: 28 January 2026 flashinfer-ai, howpublished = https://deepwiki. com/flashinfer-ai/flashinfer/2.5-multi-level-attention-%28mla%29. Multi-level attention (mla), 2026. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling, 2024. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:7908179094, 2023. Ping Guo, Chenyu Zhu, Siyuan Chen, Fei Liu, Xi Lin, Zhichao Lu, and Qingfu Zhang. Evoengineer: Mastering automated cuda kernel code evolution with large language models. arXiv preprint arXiv:2510.03760, 2025. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3):440, 2018. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and sample-efficient program evolution. arXiv preprint arXiv:2509.19349, 2025a. Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha. Towards robust agentic cuda kernel benchmarking, verification, and optimization. arXiv preprint arXiv:2509.14279, 2025b. Haonan Li, Keyu Man, Partha Kanuparthy, Hanning Chen, Wei Sun, Sreen Tallam, Chenguang Zhu, Kevin Zhu, and Zhiyun Qian. Tritonforge: Profiling-guided framework for automated triton kernel optimization. arXiv preprint arXiv:2512.09196, 2025a. Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, et al. Autotriton: Automatic triton programming with reinforcement learning in llms. arXiv preprint arXiv:2507.05687, 2025b. Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, and Chris Shum. Cuda-l1: Improving cuda optimization via contrastive reinforcement learning. arXiv preprint arXiv:2507.14111, 2025c. Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, et al. Kernelevolve: Scaling agentic kernel coding for heterogeneous ai accelerators at meta. arXiv preprint arXiv:2512.23236, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Yutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina Precup, David Silver, Masashi Sugiyama, Eiji Uchibe, and Jun Morimoto. Deep learning, reinforcement learning, and world models. Neural Networks, 152:267275, 2022. 12 Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. NVIDIA. Cuda templates and python dsls for high-performance linear algebra. https://github.com/NVIDIA/cutlass, 2023. NVIDIA. Parallel Thread Execution ISA Version 9.1, 2025. https://docs.nvidia.com/cuda/parallel-thread-execution/ index.html. Accessed: 2025-02-28. Anne Ouyang, Simon Guo, Simran Arora, Alex Zhang, William Hu, Christopher Ré, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37: 6865868685, 2024. Algorithmic Superintelligence. Openevolve: Teaching llms to discover algorithms through quality-diversity search. Blog post, 2025. https://algorithmicsuperintelligence.ai/blog/openevolve-overview/. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, and Alex Aiken. Astra: multi-agent system for gpu kernel performance optimization. arXiv preprint arXiv:2509.07506, 2025. Shanli Xing, Yiyan Zhai, Alexander Jiang, Yixin Dong, Yong Wu, Zihao Ye, Charlie Ruan, Yingyi Huang, Yineng Zhang, Liangsheng Yin, et al. Flashinfer-bench: Building the virtuous cycle for ai-driven llm systems. arXiv preprint arXiv:2601.00227, 2026. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, et al. Learning to discover at test time. arXiv preprint arXiv:2601.16175, 2026. Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, and Caiwen Ding. Cudaforge: An agent framework with hardware feedback for cuda kernel optimization. arXiv preprint arXiv:2511.01884, 2025. Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in neural information processing systems, 36:3196731987, 2023. Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. Ansor: Generating {High-Performance} tensor programs for deep learning. In 14th USENIX symposium on operating systems design and implementation (OSDI 20), pages 863879, 2020. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Other generated kernels analysis In this kernel, each query is 576-dimensional vector (512 CKV plus 64 MLA Paged Prefill (Hopper): ROPE). The kernel performs causal attention: it computes attention scores between queries and keys, applies softmax to get attention weights , then forms the output as weighted sum of values, . All three systemsK-Search, OpenEvolve, and ShinkaEvolvegenerate CUDA kernels for the same specification (16 heads, causal masking, paged layout). Handling variable-length batches: Each input batch has sequences of different lengths. natural approach is to partition work into tiles of 16 query rows (one row of the attention computation for single query vector). Because sequence boundaries do not align with tiles, single tile can contain rows from more than one batch. K-Search handles this entirely on the GPU: each thread block is assigned contiguous tile of 16 rows. When 16-row tile straddles the end of one sequence and the start of the next, the thread block resolves the split on the fly. It uses the prefix-sum array of sequence boundaries to determine which subset of rows in the tile belongs to each sequence, then fetches the corresponding KV-cache range for that sequence and computes attention for that contiguous segment of rows. It then moves to the next segment within the same tile, repeating until the tile is done. OpenEvolve and ShinkaEvolve instead precompute list of tile to batch mapping on the CPU and pass it to the GPU. However, that adds separate CPU pass and extra memory to store the tile metadata. Score computation and softmax : The expensive part of attention is computing attention scores (the equivalent of QK ) and then softmax. K-Search keeps all threads in block busy during this phase: they jointly compute the score matrix in small blocks, combine partial results in SRAM, then run the softmax per row. OpenEvolve effectively restricts the score-and-softmax stage to one small group of threads (a single warp); the rest of the block sits idle during that time, so large fraction of the GPUs capacity in that block is unused. In summary, K-Search is faster because it (1) resolves batch boundaries on the GPU without precomputed tile list or extra host-side pass; (2) keeps all thread groups busy during computation. MLA Paged Decode (Hopper): Lastly, we discuss the MLA paged decode kernel. In decode, each batch item has single new query (one token). It computes attention scores QK (over both CKV and KPE), applies softmax to get weights , then forms the output as the weighted sum . All three systemsK-Search (best solution), OpenEvolve, and ShinkaEvolvegenerate CUDA kernels for the same specification (16 heads, CKV dim 512, KPE dim 64). Sequence split and chunk size. All three systems split the key-value sequence across multiple blocks: each block processes contiguous chunk of the sequence, writes partial results to temporary buffer, and separate reduce step merges them into the final output and log-sum-exp. K-Search chooses when to split adaptively: for short sequences it uses single block per (batch, head) and writes directly to the output, avoiding the reduce pass and extra memory. OpenEvolve and ShinkaEvolve use larger chunks (64 tokens) and fix minimum number of splits, so they do not adapt well to short sequences. Avoiding shared-memory staging of the per-token query vectors (Q). In MLA decode, the per-token query vectors (Q) are small (16 heads 576 dims) but are reused across every chunk processed by thread block. K-Search loads into register-resident fragments and reuses these fragments for all chunks in the block, without materializing the full in shared memory. In contrast, OpenEvolve and ShinkaEvolve stage the full matrix in shared memory at block entry. By keeping in registers, K-Search reduces per-block shared-memory pressure and achieves more speedups. Overlapping memory and compute. K-Search uses deeper prefetch pipeline than standard double-buffering by loading two chunks ahead: while the thread block is working on chunk i, it has already started loading chunk + 1 and issues the load for chunk + 2. That keeps the pipeline fuller and hides more memory 14 latency. OpenEvolve and ShinkaEvolve use standard double-buffering (load chunk + 1 while using chunk i). K-Searchs deeper pipeline helps especially when the key-value sequence is long and many chunks are processed. K-Search is faster because it (1) keeps the query in registers instead of SRAM. (2) adopts deeper prefetch pipeline and overlaps memory and compute more aggressively by loading two chunks ahead, not just one; and (3) adapts the number of splits to sequence length. A.2 Prompt template In this section, we show the prompt template we used for baselines for generation. Listing 1 Prompt used for code generation (cid:7) 1 You are code generator. Generate CUDA kernel implementation optimized for {GPU} for the following specification. 2 3 Specification: 4 {specification} 5 6 Requirements: 7 8 9 10 11 13 14 15 16 - Write clean, efficient CUDA C++ code optimized for {GPU} architecture - Use proper CUDA syntax and memory management optimized for {GPU} - Implement the exact functionality described in the specification - The reference code provides the mathematical specification but is unoptimized - your CUDA implementation should match its computational accuracy while delivering high performance - Use the definitions tensor shapes, dtypes, and axes information to guide memory access patterns and optimization strategies - Optimize for {GPU} GPU characteristics (memory hierarchy, compute units, etc.) - For fixed axis values, optimize specifically for those constants rather than general cases - You may use 3rd party libraries (cuBLAS, cuDNN, CUTLASS) when beneficial, but custom implementations often perform better for specialized kernels with known axis constraints IMPORTANT: Generate code in XML format with exactly 3 files with these strict names: 17 18 <header_file name=\"kernel.h\"> ... 19 20 </header_file> 21 22 <cuda_file name=\"kernel.cu\"> ... 23 24 </cuda_file> 25 26 <cpp_file name=\"main.cpp\"> ... 27 28 </cpp_file> 29 30 Performance targets (lower is better): 31 {workloads} (cid:6) 15 (cid:4) (cid:5)"
        }
    ],
    "affiliations": [
        "UC Berkeley"
    ]
}