{
    "paper_title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query",
    "authors": [
        "Wei Chow",
        "Yuan Gao",
        "Linfeng Li",
        "Xian Wang",
        "Qi Xu",
        "Hang Song",
        "Lingdong Kong",
        "Ran Zhou",
        "Yi Zeng",
        "Yidong Cai",
        "Botian Jiang",
        "Shilin Xu",
        "Jiajun Zhang",
        "Minghui Qiu",
        "Xiangtai Li",
        "Tianshu Yang",
        "Siliang Tang",
        "Juncheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 4 1 3 0 . 6 0 5 2 : r MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query Wei Chow1, Yuan Gao1, Linfeng Li1, Xian Wang1, Qi Xu1, Hang Song1, Lingdong Kong1, Ran Zhou1, Yi Zeng1, Yidong Cai1, Botian Jiang1, Shilin Xu1, Jiajun Zhang1, Minghui Qiu1, Xiangtai Li1, Tianshu Yang1, Siliang Tang2, Juncheng Li2 1ByteDance Inc. 2Zhejiang University Equal Contributions Data & Code: MERIT-2025.github.io Figure 1: Illustrative examples of interleaved multi-condition semantic retrieval. MERIT enables the first multilingual semantic retrieval with composite multi-condition queries that interleave textual descriptions and visual references, reflecting real-world product search scenarios where users specify multiple attributes through both text and images."
        },
        {
            "title": "Abstract",
            "content": "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information, as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify the existing models critical limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose CORAL, novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that CORAL achieves 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework establish foundation for future research in interleaved multi-condition semantic retrieval."
        },
        {
            "title": "Introduction",
            "content": "Semantic retrieval is pivotal task that involves sourcing relevant information from vast data collections to meet specific user requirements [77, 61, 25, 44]. This task has become increasingly important with the advent of AI, as it not only enables precise user recall [96, 84, 71] but also mitigates the risk of inaccuracies in the generated content of Multimodal Large Language Models (MLLM) [2, 70]. However, semantic retrieval remains confined to narrow research scopes, which are limited to single languages [98, 82], single images [28, 10, 100], or employing only singular retrieval condition [63, 85], as illustrated in the left part of Fig. 2. Furthermore, many existing works [84, 105, 97] fail to fully exploit the expressive capacity of images, as evidenced by their maintained performance when images are replaced with corresponding captions (Vision Unnecessarity in Fig. 2). Moreover, in practical applications, product retrieval tasks frequently involve interleaved multi-condition queries (e.g., specific patterns and particular texture), with many aspects requiring visual representation through images [75, 76, 8], as demonstrated in the right part of Fig. 2. To further investigate this issue, we pose two fundamental research questions: 1) How can we comprehensively measure the capability of existing models in the interleaved multicondition semantic retrieval task? To address this question and comprehensively assess the performance gap in interleaved multi-condition semantic retrieval tasks, we introduce MERIT, the first multilingual semantic retrieval dataset with composite multi-condition queries. Our dataset comprises 135,000 products, forming 320,000 retrieval pairs in 5 languages, covering 7 distinct product retrieval scenarios. Given the challenges in acquiring such data, we employed open-set attribute annotation to increase diversity, closed-set product annotation to improve precision and recall, and designed three sampling algorithms to enhance richness and distributional uniformity. After multiple rounds of filtering, we finalized the dataset, investing total of 10,000 labor hours in the annotation process. 2) What are the important factors that limit their performance, and how can we enhance the retrieval effectiveness for such challenging task? To address this question, we evaluate 9 existing retrieval models on MERIT and demonstrate that recall rates remain substantially below expectation, despite these methods effectively solving established semantic retrieval tasks [40, 98]. Through in-depth analysis, we identify that these methods neglect specific conditional elements in queries, failing to correctly extract targeted attributes and misinterpreting visual content. This limitation stems primarily from existing retrieval models [5, 37, 40] that typically fine-tune pre-trained MLLMs through contrastive learning with supervision applied exclusively at the [EOS] token [90], thereby prioritizing Figure 2: Comparisons among and existing datasets [58, 84, 97]. Left: Previous works are limited to single-condition, single-image, single-language scenarios. Right: Our benchmark enables multilingual semantic retrieval, featuring composite multi-condition queries. 2 global semantic information while inadequately addressing specific conditional elements [45], such as material attributes in product descriptions or distinctive visual textures in images. To address this limitation, we propose Contrastive-reconstruction for multimodal retrieval (CORAL), novel fine-tuning framework to adapt pre-trained MLLMs into multimodal retrieval models. CORAL simultaneously preserves detailed conditional elements through multi-modal embedding reconstruction while effectively extracting global semantics via contrastive learning. Experimental results demonstrate that our method achieves 45.9% performance improvement compared to conventional approaches on MERIT, with efficacy further validated across 8 established retrieval benchmarks. Interestingly, we discover that existing MLLM-based retrieval models achieve performance approximately 16 times higher in R@1 when multiple images are concatenated into single input image, compared to an interleaved input of multiple images. This occurs despite the fact that pre-trained MLLMs support interleaved image inputs, which contradicts established MLLM behavior on visual comprehension tasks [88, 46] and zero-shot performance. We hypothesize that this discrepancy may stem from existing retrieval datasets containing at most one image, potentially causing MLLMs to lose their capability to process interleaved inputs effectively. After training on MERIT, sequence input performance improved by 14.3%, further validating our hypothesis. These findings underscore the significance of our dataset as the first interleaved semantic retrieval dataset. In summary, this paper makes three contributions to the retrieval research community: We introduce MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, and provide insightful observations based on it. We identify critical limitations of existing methods: focusing solely on global semantic information while neglecting conditional query elements, failing to extract specific attributes, and misinterpreting visual content. We propose CORAL, which combines embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics, demonstrating strong performance across our dataset and eight standard benchmarks."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large Language Models (MLLMs) are large-scale models that integrate visual modalities with language understanding [32, 53, 4, 65, 12, 9]. Real-world multimodal documents [104] often contain interleaved image-text pairs, and recent research [49, 46] has begun to extend MLLMs toward processing such interleaved inputs, resulting in the development of several relevant benchmarks [22, 42, 23]. Prior to the emergence of interleaved MLLMs, models supporting only singleimage inputs [54, 103] typically processed multiple images by concatenating them into single image. However, this approach results in reduced resolution and loss of sequential information, leading to inferior performance compared to sequential input of multiple images [71, 87]. Contrary to these findings, for fine-tuned retrieval models, concatenating image conditions into single input image significantly outperforms sequential image input. Interestingly, on MERIT, models fine-tuned with sequential inputs demonstrate superior performance. This suggests that fine-tuning with previous datasets, which predominantly contain single-image examples, compromises the models ability to maintain information across sequential inputs, further emphasizing the uniqueness of our dataset. Semantic Retrieval is not only crucial application in real-world scenarios, such as product search [84, 23, 46] and webpage retrieval [7], but also facilitates content generation (retrievalaugmented generation)[41, 86, 30, 29] and training for reasoning tasks[70]. However, existing semantic retrieval datasets are limited to single languages [98, 82], single images [28, 10, 100], or singular retrieval condition [63, 85], often failing to fully exploit [13] the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions as shown in Fig. 6. MERIT is the first multilingual dataset for interleaved multi-condition semantic retrieval. Comparison of related works can be seen in Tab. 1. Multimodal Retrieval Models have primarily focused on cross-modal retrieval [57, 82, 101, 102], typically leveraging models such as CLIP [66, 28] or BLIP [82, 47] for multimodal embeddings. However, these approaches exhibit limited instruction comprehension capabilities. Subsequent research has adapted MLLMs to function as embedding models for retrieval tasks, capitalizing on their robust instruction comprehension capabilities [50, 55, 37, 98, 38, 33]. These methods [5, 3 Table 1: Summary of multi-modal query retrieval datasets. We compare existing works from aspects including: 1semantics, 2multilingual data, 3multiple types, 4interleaved queries, 5multiattributes queries, and 6whether manual annotations and filtering are applied. Note that this table does not include datasets [82, 98] that are collated and summarized but not yet newly marked. Benchmark Venue Sem. Multi Multi Type Lingual Inter Multi Manual Anno. Leaved Attri. Fashion200K [26] CIRR [58] Fashion-IQ [84] DTIN [68] OVEN [28] InfoSeek [10] CIRCO [6] INSTRUCTIR [63] SciMMIR [85] Magiclens [97] MIRACLE [62] ICCV17 ICCV21 CVPR21 CVPR23 ICCV23 EMNLP23 ICCV23 arXiv24 ACL24 ICML24 CVPR MERIT Ours #Queries 200,000 36,554 20,090 10,000 139,000 1,350,000 800 16,072 530,975 36,700,000 26,221 320, 000 Statistic Total Queries - Two Conditions - Three Conditions - Four Conditions Unique Attributes - Unique Values Number 320,000 319,600 300 100 116 2,594 Products Number - Maximum Product Title Length - Average Product Length 135,000 190 95.83 Figure 3: Dataset statistics. Figure 4: Summary of product categories and language distributions. 37, 40] typically fine-tune existing MLLMs through contrastive learning, utilizing only the [EOS] token [90] for supervision. This approach results in an over-reliance on contrastive learning to supervise global information [90], while neglecting detailed semantic information, which often leads to semantic misunderstandings [76, 72]. To overcome these shortcomings, CORAL preserves original detailed information by integrating embedding reconstruction and contrastive learning as finetuning framework to adapt pre-trained MLLMs into multimodal retrieval models. Our experiments demonstrate strong performance across MERIT and eight standard benchmarks."
        },
        {
            "title": "3 MERIT: A Multi-Condition Smantic Retrieval Benchmark",
            "content": "To evaluate the effectiveness of existing retrieval models in addressing the interleaved multi-condition semantic retrieval task, we introduce MERIT in Sec. 3.1. Subsequently, we provide comprehensive description of the data collection methodology in Sec. 3.2. Leveraging our data, we conduct extensive experiments on 9 state-of-the-art retrieval models and derive insights regarding visual conditioning necessity, interleaving support, and out-of-distribution scenarios in Sec. 3.3. Finally, in Sec. 3.4, we present an in-depth analysis of the factors potentially contributing to suboptimal performance. 3.1 Benchmark Overview In practice, product retrieval tasks frequently encompass multiple simultaneous conditions (e.g., specific patterns, precise colors, and particular styles), with many attributes necessitating visual representation through images [8]. However, existing semantic retrieval datasets are limited to single languages [98, 82], single images [28, 10, 100], or singular retrieval conditions [63, 85], often failing to fully exploit [13] the expressive capacity of visual informationa limitation evidenced by maintained performance when images are replaced with textual captions. 4 Figure 5: The data annotation pipeline for MERIT. We ensure data diversity and quality through open-set deduplication and multi-round filtering procedures in 4 steps. We first select high-quality products and annotate their attributes, then combine them into query pairs before performing data cleaning to produce MERIT. Details can be found in Appendix A. To bridge this gap, we present MERIT, which encompasses 135, 000 products, resulting in 320, 000 retrieval pairs across 5 languages (English, Malay, Indonesian, Vietnamese, Thai), encompassing 7 distinct product retrieval scenarios. Our dataset constitutes structured query dataset, where each fundamental unit is product comprising an image and its corresponding title generated by GPT-4o [32], as illustrated in Fig. 2. Tab. 3 presents the key statistical characteristics of our MERIT, while Fig. 4 displays the category and language distribution of the product pool. Each search query contains at least one positive sample. For convenience, the dataset is partitioned into training and test sets, containing 310, 000 and 10, 000 entries respectively. Additional statistical results and examples are provided in Appendix and Appendix D, respectively. 3.2 Data Collection Process To ensure data quality, all data underwent manual filtering by annotators proficient in all five languages, complemented by multiple rounds of automated filtering during the collection process. Specifically, our dataset collection pipeline comprises the following four steps: 1) High-Quality Product Selections. While maintaining diversity, we carefully selected popular products from our internal dataset across 6 Southeast Asian countries in 5 languages with each product title is generated by GPT-4o [32]. Each product was further filtered based on popularity and aesthetic scores [69, 91] to form our product inventory used in the following steps. 2) Product Annotations. To accommodate diverse real-world search requirements, we needed to obtain variety of fine-grained product attributes for combination. However, attribute information in real-world E-commerce data is often insufficient, resulting in suboptimal retrieval for specific user needs. This gap arises from the limited attribute richness constrained by operational attribute structure versus the need for fine-grained, precise product attribute information in search relevance systems. Consequently, we adopted an open annotation approach followed by statistical analysis for Attribute Delineation, and subsequently tagged products based on these derived attributes. 3) Search Query Compositions. To simultaneously enhance dataset quality and diversity, we implemented composite sampling approach for constructing retrieval pairs. This approach integrates three distinct methods: Conventional Uniform Sampling (Appendix A.3.1), Attribute Uniform Sampling (Appendix A.3.2), and High-Similarity Product Prioritization Sampling (Appendix A.3.3). Furthermore, our pipeline supports cold-start expansion, enabling the extension of our dataset to previously unseen product classes, as detailed in Appendix A.3.4. 4) Filtering & Refinement. Finally, we introduce two-stage filtering process, encompassing automatic filtering and manual curation, respectively. The automatic filtering stage employs rulebased systems and statistical methods to eliminate obvious inconsistencies and low-quality samples, while the manual filtering stage involves expert annotators who apply nuanced judgment to ensure 5 Table 2: Comparative study of retrieval performance on MERIT. Details about the baselines can be found in Appendix E.1. Seq, Cat, and Avg denote sequential multi-image input, concatenated images as single image input, and averaged embeddings, respectively. Type Method Size Venue Type R@1 R@5 R@10 MRR Zero-Shot MLLM Embedding MLLM InternVL2.5 [11] InternVL2.5-MPO [81] Qwen2.5-VL [5] InternVL2.5 [11] InternVL2.5-MPO [81] Qwen2.5-VL [5] E5-V [37] LLaVE [40] GME-Qwen2VL [98] LLaVE [40] LamRA-Qwen2.5VL [55] LLaVE [40] BGE-VL [100] LLaVE [40] GME-Qwen2VL [98] LLaVE [40] VLM2Vec [38] LamRA-Qwen2.5VL [55] LLaVE [40] 1B 1B 3B 1B 1B 3B 8B 0.5B 2B 2B 7B 7B 7B 0.5B 2B 2B 4B 7B 7B arXiv24 arXiv24 arXiv25 arXiv24 arXiv24 arXiv25 Cat Cat Cat Seq Seq Seq arXiv24 Avg CVPR25 arXiv24 CVPR25 arXiv24 CVPR25 arXiv25 CVPR25 arXiv24 CVPR25 arXiv24 arXiv24 CVPR25 Cat Cat Cat Cat Cat Cat Seq Seq Seq Seq Seq Seq 0.20 0.24 0.05 0.27 0.41 0.09 3.10 4.89 8.47 5.80 12.05 8.03 11.55 0.38 5.29 0.12 0.43 3.26 0.39 0.98 1.04 0. 1.24 1.37 0.39 7.54 33.11 47.13 43.62 39.13 45.34 38.01 1.17 24.18 1.03 1.86 13.10 1.76 1.72 1.81 0.40 1.94 2.28 0. 9.90 41.98 56.18 53.51 48.03 55.32 46.26 1.79 30.66 1.67 2.97 19.03 2.77 0.56 0.60 0.14 0.69 0.87 0.21 5. 16.95 25.02 21.78 23.80 24.25 23.00 0.71 13.42 0.51 1.04 7.57 1.03 Figure 6: Comparisons on (a) Visual Necessity Test, and (b) Out-of-Distribution Scenarios. semantic coherence and practical relevance. This rigorous quality control process results in high-fidelity dataset that meets stringent academic standards. Due to space limits, we provide more detailed description of our annotation process and the rationale behind our design choices in Appendix A. 3.3 How Far to MERIT To evaluate the effectiveness of existing retrieval models in addressing the interleaved multi-condition semantic retrieval task, we conduct experiments on 9 state-of-the-art retrieval models. The principal results are presented in Tab. 2. Detailed information regarding the experimental settings and datasets can be found in Appendix E. MERIT is divided into training and test sets, consisting of 310,000 and 10,000 queries respectively as mentioned in Sec. 3.1. Main Results. Existing retrieval methods struggle to address interleaved multi-condition semantic tasks, with even the best Recall@1 being only 12.05%. Additionally, we identify several key insights: Visual Conditioning Necessity. To verify the necessity of visual information, we conducted experiments using BGE-VL [100] on CIRR [58], FashionIQ [84], and MERIT. We report R@1 for CIRR, R@10 for FashionIQ, and our dataset. As shown in Fig. 6(a), when replacing images with their corresponding captions for retrieval, the performance on FashionIQ and CIRR does not significantly deteriorate. In contrast, we exhibit substantial performance degradation when either replacing images with their corresponding captions (w/o image) or removing product titles (w/o title), with image removal resulting in particularly severe decline of 73.9%. This demonstrates the effectiveness of our dataset, indicating that both images and product titles are indispensable components. Interleaving Support. As shown in Tab. 2, concatenating multiple images into single image significantly outperforms sequential input such as GME-Qwen2VL [98], with concatenation achieving 119.7% improvement in R@5 over its sequential version. This occurs despite the fact that 6 Figure 7: (a) Different Languages Performance on MERIT (R@1). (b) Distribution of Error Types. pre-trained MLLMs support interleaved image inputs [5], which contradicts established MLLM behavior on visual comprehension tasks [88, 46] and zero-shot performance on MERIT, where sequential processing typically excels by preserving more image information [39, 87, 43, 1, 24]. We hypothesize that this discrepancy may stem from existing retrieval datasets containing at most one image, potentially causing MLLMs to lose their capability to process interleaved inputs. After training, sequence input performance improved by 14.3% in Tab. 3, further validating our hypothesis. This underscores the significance of MERIT as the first interleaved semantic retrieval dataset. Out-of-Distribution Scenarios. We evaluated Qwen2.5-VL [5] on three types of OOD scenarios (Class OOD, Language OOD, and Attribute OOD), with results illustrated in Fig. 6(b). Detailed numeric results can be seen in Tab. 7,8,9 in the Appendix E.4. Specifically, performance in the Language OOD scenario shows notable gap compared to full training (Mixed); however, it still demonstrates substantial improvement over zero-shot performance due to the activation of the MLLMs multilingual capabilities. In both Class and Attribute OOD scenarios, the performance gap between OOD and full training is relatively small, reflecting the diversity of our dataset. 3.4 Error Analysis To investigate the poor performance of retrieval models on MERIT, we first analyzed whether success rates correlate with specific languages. As shown in Fig. 7(a), the statistical results reveal minimal variation across different languages, with no observable advantage for English despite its predominance in the initial training data of MLLMs. We then randomly selected 500 queries and obtained explanations from Qwen2.5-VL and InternVL 2.5, both of which underwent full-parameter contrastive learning training. Expert annotators classified the root causes of mispredictions into five categories (details can be seen in Appendix E.5). The distribution of these error types, shown in Fig. 7(b), reveals that attribute and visual understanding errors constitute the largest proportion of failures. This analysis reveals these methods neglect conditional query elements, failing to extract specific attributes and misinterpreting visual content. This likely stems from retrieval-oriented fine-tuning, where MLLMs prioritize global over specific semantic information. Furthermore, since current retrieval datasets are predominantly single-image based, existing methods fail to leverage the image sequence understanding capabilities of interleaved MLLMs as analyzed in Sec. 3.3. This limitation likely leads to failures in understanding precise semantics, resulting in attribute extraction errors (causing Attribute Errors) and incorrect interpretation of visual features such as patterns (causing Visual Understanding Errors)."
        },
        {
            "title": "4 CORAL: Contrastive Reconstruction for Multimodal Retrieval",
            "content": "Recognizing neglecting specific conditional elements in queries as primary source of error highlighted in Sec. 3.4, we introduce CORAL in Sec. 4.1 to enhance MLLM-based retriever performance in addressing interleaved multi-condition semantic retrieval tasks through the integration of visual reconstruction during the fine-tuning process of the MLLM-to-retrieval model adaptation. Subsequently, we validate the effectiveness of our approach in Sec. 4.2. 4.1 Preliminaries Prerained MLLM. For common MLLM [54, 60, 39], it has image and text input ximg and xtxt. We assume as the hidden state dimension of the language model. We first process ximg 7 Figure 8: Overview for CORAL. he loss function of CORAL consists of three components: Contrastive Learning Loss Lcl, Vision Reconstruction Loss Lmse, and Masked Language Modeling Loss Lmlm. During training, we reconstruct both the query and its corresponding positive sample. subject to visual representation backbone [66, 94] Vω that outputs sequence of features pimg = Vω(ximg) RNV dv . Next, we map pimg to sequence of embeddings via learned projector Fψ, where eimg = Fψ(pimg) RNV d. Finally, we concatenate the sequence eimg with the text prompt embeddings etxt = embed(xtxt) RNLd, passing the result to the language model. Generally, we have the interleaved image-text input xinput by concatenating all the etxt and eimg. The language model generates output hidden state hgen = LMθ(eimg; etxt). In particular, we denote the hidden layer representation of the [EOS] position as heos. Finally, hgen can be transferred into text output ugen. The composition LMθ(Fψ(pimg); embed(xtxt)) then defines the MLLM. Given triple (ximg, xtxt, ˆugen) during training, MLLM minimizes that loss = logp(ˆugenximg, xtxt). Masked Embedding Reconstruction. During training, we apply masking to the attention maps of individual modalities. We define the functions MASKv(q, e) and MASKl(q, e) to mask the visual and linguistic portions [89, 78] of the input multi-modal embedding = [eimg; etxt] at fixed ratio δ (we set δ = 0.5 in our experiments). Taking MASKv as an example, for given multi-modal input embedding, we retain all textual attention while randomly masking the visual self-attention and cross-attention from the complete text to the image. We set = [e, heos] and = [heos, heos, ...] with position embedding [17], which has the same length with e. This process is illustrated in Fig. 8 (a), and the reconstructed multi-modal embedding is then obtained using: = qWQ, = eWK, Mij = (cid:26)0, attended, , masked; = eWV , QT erec = softmax( + M)V. (1) CORAL. We introduce fine-tuning method designed to adapt pretrained MLLMs into multimodal retrieval models. It enhances visual understanding capabilities while preserving the models original linguistic comprehension. Specifically, for pretrained MLLM, we perform fine-tuning as follows: Contrastive Learning Loss Lcl. We employ the InfoNCE Loss [83] for supervised contrastive learning. Given batch of samples, where τ denotes the temperature coefficient, qi represents the query sample, and ki+ is the encoded vector of the positive sample corresponding to query i, the contrastive loss is computed as: Lcl = log 1 (cid:88) i= exp (cid:17) (cid:16) qiki+ τ (cid:16) qikj τ (cid:80)N j=1 exp . (cid:17) (2) Vision Reconstruction Loss Lmse. We employ decoder θ , randomly initialized as BERT layer [89, 16]. Using the full input representation heos as the query, we compute the MSE loss 8 Table 3: Ablation results of existing methods and CORAL on MERIT using Qwen2.5-VL [5]. Ablation Factor Method LoRA Type R@1 R@5 R@10 MRR Baseline Input Type Partial Reconstruct Final Version CL CL Zero-Shot Zero-Shot +CORAL (Ours) +Vison +Language +Vision +Language +CORAL (Ours) +CORAL (Ours) - - Seq Seq Seq Cat Cat Seq Seq Seq Seq Seq Seq 48.52 47.76 0.09 0.05 60.94 58.18 58.38 59.46 59. 73.11 73.97 0.39 0.27 85.60 83.19 83.01 85.46 86.01 59.40 69.68 82.80 89.26 77.93 80. 0.56 0.40 90.40 88.02 88.26 90.81 90.72 87.94 93.08 59.48 59.06 0.21 0.14 71.70 69.13 69.35 70.89 71. 69.74 78.33 Figure 9: Comparisons of our method with other methods on eight established retrieval tasks. We take zero-shot Qwen2-VL [79] as our baseline. CL denotes contrast learning. between the original unmasked embedding and the reconstructed embedding from θ as follows: Lmse = 1 N (cid:88) i=1 (cid:13) ˆE (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 , where ˆE = θ [ MASKv(E); heos]. (3) Masked Language Modeling Loss Lmlm. Similar to vision reconstruction, we use decoder reconstruction. To reduce trainable parameters, of the MLLM. The masked language modeling loss is computed as: θ for θ shares weights with the language modeling head Lmlm = 1 (cid:88) i=1 log (ˆxi X) , where ˆxi = [F θ[ MASKl(E); heos]](i). The overall training objective of CORAL is formulated as: max θ,θv,θl = Lcl + λ1Lreg + λ2Lrec . (4) (5) Here, Lreg and Lrec represent the reconstructions of the retrieval target using the conditions [EOS] token and the targets own [EOS] token as attention queries, respectively. For both terms, the attention keys and values referenced in Eq. 1 are derived from the embeddings of the retrieval target. Each reconstruction component encompasses both image reconstruction and language reconstruction. 4.2 Experiments To validate the effectiveness of CORAL, we conducted experiments on MERIT and 8 established retrieval tasks. Due to space constraints, the implementation details are placed in the Appendix. Main Results on MERIT. Results lead to the following conclusions: (i) Embedding reconstruction contributes significantly to retrieval performance. Both partial feature reconstruction (Tab. 3, rows 6-11) enhance model performance, with multimodal reconstruction yielding 45.9% improvement compared to contrastive learning alone. (ii) Multi-modal reconstruction outperforms partial reconstruction. Comparing Tab. 3, rows 6-9 and 10-11 reveal superior performance when reconstructing 9 both modalities simultaneously. (iii) Sequential input surpasses image concatenation. Based on rows 3-5 and 11, we observe that sequential inputs achieve higher performance. We hypothesize that sequential representation preserves more information than image concatenation [36, 93, 23, 46], which aligns with our findings in Sec. 3.3. (iv) Full parameter fine-tuning yields optimal results. Due to the substantial divergence between retrieval tasks and pre-training objectives, full parameter fine-tuning generally produces better outcomes, consistent with conclusions from previous work [40, 38]. Results on Eight Retrieval Tasks. To further validate the efficacy of CORAL, we conducted evaluations on 8 retrieval benchmarks with experimental configurations following the methodology described in [38]. The results are illustrated in Fig. 9. Comparative analyses between our approach and other foundational models, such as CLIP [66] and E5-V [37], are presented in Appendix E.4. Experimental results demonstrate that our method achieves consistent improvements across these eight retrieval tasks, with particularly notable performance on VisDial [15], where our approach exhibits 181% enhancement over the baseline."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval. Extensive experiments identify existing critical limitations of existing models: focusing solely on global semantic information while neglecting conditional query elements, failing to extract specific attributes, and misinterpreting visual content. To address this limitation, we propose CORAL, novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction for preserving detailed conditional elements with contrastive learning for extracting global semantics. Ablation study across benchmarks demonstrates the effectiveness of CORAL."
        },
        {
            "title": "Contents",
            "content": "A Detailed Dataset Collection Process A.1 Product Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Attribute Delineation and Annotation . . . . . . . . . . . . . . . . . . . . . . . . . A.2.1 Attribute Delineation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.2 Attribute Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Retrieve Data Build . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Conventional Uniform Sampling . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Attribute Uniform Sampling . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 High-Similarity Product Prioritization Sampling . . . . . . . . . . . . . . A.3.4 Cold start expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Auto and Human Filter A.4.1 Auto Filter . . A.4.2 Human Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Annotation Protocol B.1 General Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Data Format and Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Quality Control and Validation Protocol . . . . . . . . . . . . . . . . . . . . . . . B.4 Handling Ambiguities . B.5 Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Data Contamination Considerations . . . . . . . . . . . . . . . . . . . . . . . . . 11 12 12 16 18 18 18 19 22 22 23 24 24 24 25 25 10 B.7 Annotation Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.8 Data Preparation and Release . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Dataset Analysis C.1 Global Statics . . . . C.2 Attribute and Product Dataset Examples Experiments Details E.1 Baselines . . . . . . E.2 Other Dataset Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Main Experiments Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 More Experiments Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Error Analysis Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Broader Impact F.1 Impact . . . F.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 27 27 27 34 41 41 42 43 43 46"
        },
        {
            "title": "A Detailed Dataset Collection Process",
            "content": "The complete data generation pipeline is illustrated in Fig. 5. Below, we provide more detailed description of our annotation process and the rationale behind our design choices than in the main text. We confirm that all data has been anonymized and complies with open-source guidelines. Specifically, in Appendix A.1, we describe the sources of our product data, and in Appendix A.2, we elaborate on our methodology for labeling products to obtain diverse attributes. Subsequently, in Appendix A.3, we describe our approach for generating rich set of retrieval pairs (which were later manually filtered and curated). Specifically, to enhance dataset quality and diversity, we employed composite sampling approach integrating three methods: Conventional Uniform Sampling (Appendix A.3.1), Attribute Uniform Sampling (Appendix A.3.2), and High-similarity Product Prioritization Sampling (Appendix A.3.3). Additionally, our pipeline supports cold-start expansion as detailed in Appendix A.3.4. Finally, we introduce our two-stage filtering process in Appendix A.4.1 and Appendix A.4.2, encompassing automatic filtering and manual curation, respectively. The automatic filtering stage employs rule-based systems and statistical methods to eliminate obvious inconsistencies and low-quality samples, while the manual filtering stage involves expert annotators who apply nuanced judgment to ensure semantic coherence and practical relevance. This rigorous quality control process results in high-fidelity dataset that meets stringent academic standards. A.1 Product Source The MERIT dataset comprises high-quality, anonymized items from our internal dataset. Specifically, we curated popular products across six Southeast Asian countries, covering five languages. The items were sampled from both category-specific listings (as the primary source) and high-quality open-category offerings to enhance data diversity. Each item underwent aesthetic scoring [69, 91] before inclusion in the final repository. The distribution of categories and their respective quantities is detailed in Tab. 4. Note that these categories are simplified for annotation purposes and do not reflect the full hierarchical taxonomy. All sensitive information has been removed to comply with open-source data standards. Table 4: Product Categories and Countings in MERIT."
        },
        {
            "title": "Total",
            "content": "Fruits Snacks Beverages Tops Pants Shoes Phones Headphones Laptops Other Screens Backpacks Handbags Suitcases Gold Silver Diamonds Tables Chairs 1, 001 3, 077 3, 481 32, 100 26, 398 24, 11, 671 4, 670 842 351 5, 881 4, 5, 540 405 608 590 3, 104 6, 158 135, 000 However, among these well-performing products, some exhibit suboptimal visual presentations that diminish their search augmentation potential. The imagery associated with these products lacks visual efficacy and fails to fulfill the objective of facilitating retrieval through visual cues. Fig. 10 illustrates several examples of such products. A.2 Attribute Delineation and Annotation A.2.1 Attribute Delineation To support diverse real-world retrieval needs, it is essential to obtain fine-grained and varied product attributes for effective combinatorial search. However, existing product retrieval data often suffers from insufficient attribute richness, leading to suboptimal recall for specific user queries. This limitation stems from fundamental gap: operational attribute schemas (typically rigid and sparse) fail to meet the demands of search relevance tasks, which require precise, granular attribute information. Directly predefining attributes and values for each product category whether through manual annotation or MLLMs [32, 74, 20] introduces two challenges: (1) restricted coverage due to finite predefined options, and (2) biases (human or LLM-induced) that may overlook critical product features. To address this, we propose an open-ended attribute delineation approach, where attributes and candidate values are first generated openly and then refined. As illustrated in Fig. 11, our pipeline begins with raw platform properties, employs LLM verification to ensure diversity, and finally derives structured attribute-value pairs through frequency inversion and label refinement. 12 Figure 10: Examples of low-quality products. The associated images fail to accurately represent the visual characteristics of the products, instead containing irrelevant content. Figure 11: An Overview of our Attribute Delineation Pipeline. The process begins with extracting original platform properties and diverse product attributes, followed by LLM-based attribute verification. We then perform attribute frequency inversion and generate refined labels, resulting in structured product information with open-domain attribute-value pairs. Specifically, the LLM-based attribute extraction module takes products image and textual information as input and generates set of open-ended attributes. Figs. 12, 13, 14, 15, and 16 provide examples of annotated outputs. However, such open-set annotations frequently generate excessive irrelevant attributes (i.e., attributes that do not align with practical product retrieval needs). For instance, in Fig. 12, attributes such as \"Isbn/Issn\": [\"9780226264219\"], \"Language\": [\"English\"], \"Version\": [\"First Edition\"], \"Cover Type\": [\"Paperback\"], \"Number Of Pages\": [\"464\"], \"Features\": [\"27 photographs\", \"6 maps\", \"25 illustrations/diagrams\"], \"Publication Date\": [\"May 15, 2018\"], and \"Publisher\": [\"Basic Books\"] are rarely used for product retrieval queries and should therefore be excluded. Similarly, in Fig. 14, the attribute \"Batteries Included\": [\"no\"] is ambiguous and unsuitable for retrieval purposes. The LLM attribute verification step evaluates whether the extracted attributes are reasonable for the given product category, retaining only those that are valid to narrow down the candidate set. During this process, we also standardize the format of attributes and their corresponding values by ensuring consistent formatting, avoiding capitalization of first letters, and removing underscores. Next, attribute frequency inversion and filtering organizes the verified attributes by product category, as outlined in Tab. 4. This step accounts for the distinct characteristics of different products for instance, clothing may include attributes such as pattern, neckline style, zipper type, and sleeve length, whereas electronics like smartphones may feature RAM, storage capacity, and battery size. Considering these factors, we employed the following methodology to define closed set of attributes: Rule-based filtering: We initially identified 100 attributes using inverted indexing (employing normalized strings: converting all text to lowercase and removing extraneous spaces and punctuation). Selecting frequently occurring instances helps avoid annotating niche attributes that are unlikely to be used in retrieval tasks. 13 Human-LLM collaboration: Based on the 100 identified attributes and our domain expertise, we curated distinctive set of 20 attributes for each product category. The prompt used for attribute selection via LLM is as follows: You are an assistant who communicates only in JSON and is an expert in physics. Do not am working with dataset for attribute retrieval in clothing categories. Below is list of extracted attributes. Your goal is to help me select the top 20 attributes that best meet the following criteria: Relevance: The attribute should be common across multiple clothing categories (e.g., dresses, shirts, pants). Searchability: The attribute should be easily combinable with other attributes in queries (e.g., \"long sleeve + striped\"). User Demand: The attribute should reflect features users frequently search for or filter by. Visual Detectability: The attribute should be identifiable from product image (e.g., \"color\", not subjective traits). Please prioritize attributes that satisfy all four criteria. If needed, suggest rephrasing attributes for clarity or consistency (e.g., \"sleeve length\" instead of \"long sleeve\"). Exclusions: Exclude overly niche or rarely used attributes. No overlap: Remove synonymous terms You should return your answer in Python list: list: [tape here] The following is an example of the list of attribute values returned by this operation (products of cloth type): { \"color\": [\"red\", \"green\", \"yellow\", \"orange\", \"purple\"], \"size\": [\"small\", \"medium\", \"large\", \"extra large\"], \"flavor\": [\"sweet\", \"sour\", \"tart\", \"bland\", \"tangy\"], \"fruit type\": [\"apple\", \"berry\", \"citrus\", \"melon\", \"tropical\"], \"organic\": [\"organic\", \"non-organic\"], \"quantity per pack\": [\"1 piece\", \"3-pack\", \"5 lbs\", \"10 kg\"], \"ripeness\": [\"unripe\", \"ripe\", \"overripe\"], \"texture\": [\"smooth\", \"bumpy\", \"fuzzy\", \"rough\"], \"shape\": [\"round\", \"oval\", \"elongated\", \"lobed\"], \"weight\": [\"100g\", \"500g\", \"1kg\", \"2kg\"], \"pattern\": [\"striped\", \"spotted\", \"solid\", \"mottled\"], \"seed presence\": [\"seedless\", \"with seeds\"], \"skin thickness\": [\"thin-skinned\", \"thick-skinned\"], \"juiciness\": [\"juicy\", \"dry\", \"moderate\"], \"sweetness\": [\"low sugar\", \"medium sweet\", \"very sweet\"], \"storage type\": [\"refrigerate\", \"room temperature\", \"cool dry place\"], \"region of origin\": [\"Florida\", \"California\", \"Chile\", \"Spain\"], \"material\": [\"plastic clamshell\", \"mesh bag\", \"paper bag\", \"loose\"], \"height\": [\"short\", \"medium\", \"tall\"], \"allergen information\": [\"citrus-free\", \"latex-free\", \"sulfite-free\"] } After obtaining the attributes, we expanded each attribute to its corresponding values. Specifically, similar to the previous step, we employed the following methodology: Rule-based approach: We retrieved the top 100 values for each of the 20 attributes through an inverted index (ensuring that attributes were clearly specified and excluding cases where attribute values contained multiple values). The returned content was consistent with the previous step. Human + LLM approach: From the 100 values for each attribute, we identified 20 distinctive values. The prompt used for attribute selection via LLM is as follows: 14 Figure 12: Open-ended Attributes Annotated Product Case. Figure 13: Open-ended Attributes Annotated Product Case. am working on attribute retrieval for clothing categories and need to select the top 10 to 20 values per attribute for labeling. The chosen values should optimize for: Relevance Common across multiple clothing categories (e.g., dresses, shirts, pants). Searchability Easily combinable in queries (e.g., \"long sleeve + striped\"). User Demand Frequently searched or filtered by users. Visual Detectability Clearly identifiable from product images. For example, the specific numerical value of weight and size cannot be seen and should not appear in your answer). Universality Collectively covers all scenarios without redundancy. Requirements: No overlap: Remove synonymous terms (e.g., \"round neck\" \"crew neck\"). Prioritization: Favor attributes that meet all five criteria. Clarity: Suggest rephrasing for consistency (e.g., \"button-front\" vs. \"buttoned\"). Exclusions: Omit niche, ambiguous, or rarely used terms (e.g., \"peasant sleeve\"). Please return the attribute (dicts key) with its choice values (dicts value) as JSON. [tape here] 15 Figure 14: Open-ended Attributes Annotated Product Case. Figure 15: Open-ended Attributes Annotated Product Case. Our approach yields attributes with two key advantages: 1. Diversity and Reduced Bias: The extracted attributes are both comprehensive and varied, demonstrating greater resistance to biases from either the LLM or human annotators [3, 31], while simultaneously improving recall of long-tail attributes. 2. Product-Centric Representation: Since attributes are first annotated at the product level before being aggregated, they more accurately reflect real-world product characteristics. A.2.2 Attribute Annotation Through the methodology described in Appendix A.2.1, we obtain rich set of attributes and their corresponding values for each product category. This attribute set forms closed set that is suitable for retrieval tasks. Utilizing these closed attribute tables, we can annotate each product with its corresponding attributes. Specifically, we employ the following prompt: Figure 16: Open-ended Attributes Annotated Product Case. Objective: Annotate clothing product attributes based on the provided inputs. Your annotations must align with the given attribute options and labeling rules. Inputs: List of Attributes and Options: predefined list of attributes (e.g., \"color,\" \"sleeve length\") and their valid options. Product Image: The primary reference for annotation. Image Attribute Table (Optional): Existing attribute descriptions for reference (may be incomplete or empty). Product Description (Optional): Textual description of the product (may be inaccurate verify against the image). Labeling Rules: Select the most accurate option for each attribute based on visible/verifiable details. Skip an attribute if it cannot be determined from the inputs. Do not add new attributes. Leverage the Image Attribute Table (if available): If an attribute (or semantically similar one) exists in the table, summarize its content as one of the predefined options. If your summarized value isnt in the options, ensure it adheres to: Exclusions: Omit niche/rare attributes. No Overlap: Remove synonymous terms. Your Output Format: Please output your answer in JSON format. The answer is dict, where the key is the attribute and the value is the value of the attribute The list of Attributes and Options is [paste here] Product Image is [paste here] Image Attribute Table is [paste here] Product Description is [paste here] Following annotation completion, automatic filtering is performed to: eliminate newly constructed attributes; remove attributes with values designated as none or skip; exclude the most frequent value within each attribute category to avoid generic descriptors such as all ages; and discard terms that appear only once in the dataset. 17 A.3 Retrieve Data Build To enhance dataset quality and diversity, we employed composite sampling approach integrating three methods: Conventional Uniform Sampling (Appendix A.3.1), Attribute Uniform Sampling (Appendix A.3.2), and High-similarity Product Prioritization Sampling (Appendix A.3.3). Additionally, our pipeline supports cold-start expansion as detailed in Appendix A.3.4. It is worth emphasizing that all data generated by our automatic combination algorithm subsequently underwent manual review and refinement. Furthermore, due to the inherent limitations of closed-set attributes, many matches lacked precision (e.g., products labeled as Bohemian style might not exhibit particularly similar stylistic elements, thus being filtered out). Consequently, significant proportion of data was eliminated during post-processing, reflecting the high quality standards of our final dataset. A.3.1 Conventional Uniform Sampling Conventional Uniform Sampling is our most frequently utilized sampling algorithm. Specifically, we randomly select two products and randomly choose product attributes, then retrieve all products from the product pool that simultaneously satisfy these two conditions. We apply certain constraints to these recalled products: The value of attribute for product and product cannot be identical, as this would render the use of two products for retrieval unnecessary When randomly selecting values, we employ probability distribution that suppresses frequently occurring keys, such as color Our dataset contains 2-4 conditions per query. When forming combinations, we set the probability distribution as follows: 75 for 2 conditions, 10 for 3 conditions, and 15 for 4 conditions (the higher proportion for 4 conditions accounts for potential filtering during subsequent annotation) At this stage, numerous products still remain. To enhance the efficiency of subsequent manual annotation, we use CLIP Similarity to rank all qualifying samples according to their average similarity with the two products in descending order. This arrangement prioritizes similar products, facilitating the annotation process. The CLIP Similarity Ranking of Recalled Products can be seen in Figs. 17, 18, and 19. These figures demonstrate that CLIP similarity ranking serves meaningful purpose. However, this sampling method suffers from long-tail distribution of attributes (uncommon attributes rarely appear in the combined retrieval entries). Furthermore, due to the extensive size of the product repository, we do not process all qualifying products in each iteration but instead conduct selective sampling, which may result in the absence of products that genuinely match the conditions in the recalled set. To address these issues, two novel sampling algorithms are introduced in Appendix A.3.3 and Appendix A.3.4. A.3.2 Attribute Uniform Sampling Relying solely on Conventional Uniform Sampling strategies can be limited by the long-tail distribution of attributes, as shown in Fig. 20. For instance, the color\" attribute appears with significantly higher frequency, while other attributes occur much less frequently [19]. To address this issue, we introduce the Attribute Uniform Sampling algorithm. Specifically, we first uniformly select particular value of given attribute, and then identify corresponding items to form combinations. This approach effectively increases the occurrence of less common keys and introduces long-tail scenarios, thereby enhancing data diversity. As illustrated in Fig. 20, after implementing this sampling method, the attribute distribution becomes more balanced, and wider range of attributes emerges. A.3.3 High-Similarity Product Prioritization Sampling Due to the vast size of the product catalog, we do not process all qualifying products at once, but instead employ sampling, which may result in the retrieved products not truly matching the specified conditions. To address this issue, we introduce the High-similarity Product Prioritization Sampling algorithm, which identifies product and one of its similar products (manually pre-annotated). The algorithm then derives several conditions that would transform product 1 into product 2, and Figure 17: The CLIP Similarity [34, 94] textbfRanking of Recalled Products Case 1. The first row is the two condition products, and the second row is the recalled products. The CLIP similarity with the condition products decreases from left to right. Figure 18: The CLIP Similarity Ranking of Recalled Products Case 2. The first row is the two-condition products, and the second row is the recalled products. The CLIP similarity with the condition products decreases from left to right. subsequently retrieves candidate products from the catalog based on these conditions. Finally, human annotators perform additional filtering to construct the retrieval query. Additionally, the High-similarity Product Prioritization Sampling algorithm accounts for long-tail distributions by probabilistically reducing the frequency of the most common attributes (such as color-based differences between product pairs). A.3.4 Cold start expansion Furthermore, our pipeline demonstrates extensibility; upon establishing comprehensive attribute list at scale, we can perform cold-start initialization for previously unseen categories of data. Specifically, we leverage LLMs for automatic expansion through the following systematic methodology. First, for any given product, we employ similarity matching approaches [48, 64, 66, 18] to retrieve analogous products as query conditions and corresponding target images. Subsequently, we utilize the following LLM prompt to characterize the distinctions between the two products. Where { 19 Figure 19: The CLIP Similarity Ranking of Recalled Products Case 3. The first row is the two-condition products, and the second row is the recalled products. The CLIP similarity with the condition products decreases from left to right. Figure 20: Frequency distribution of conditional attributes before and after applying the Attribute Uniform Sampling strategy. The left panel shows the distribution before applying the Attribute Uniform Sampling algorithm, while the right panel demonstrates the distribution after implementing 10 Attribute Uniform Sampling. Notably, the attribute distribution becomes more balanced after applying the Attribute Uniform Sampling algorithm. all_attribute } represents the distinguishing attributes previously identified from our retrieval dataset, thereby enabling newly integrated data to seamlessly adapt to the existing retrieval paradigm. Task: Identify the Most Significant Attribute Difference Between Two Products Instructions: 1. Examine the two product images carefully. 2. From the provided attribute list, select ONLY ONE attribute that represents the most significant difference between these two products. By modifying this attribute, you can convert picture 1 to picture 2 3. Even though multiple differences may exist, focus on identifying the single most visually distinct and important attribute difference. Try not to answer vague attributes like style. 4. Your response should contain ONLY the attribute name - no explanations, no quotes, no additional text. Image 1: <image> Image 2: <image> Available attributes to choose from: { all_attribute } Remember: - Your response must be exactly one attribute from the provided list - Do not add quotation marks or any additional text - Select the most visually distinctive difference - If multiple differences exist, choose the most significant one Response (attribute name only): Upon obtaining the differential product attributes, analogous statistical analysis of the original retrieval data enables the extraction of the various differential values for each attribute. Given that our preceding data generation methodology prioritizes comprehensive diversity, the differential value options are nearly certain to encompass the most salient distinguishing characteristics within product pairs. Specifically, the annotation process utilizes the following prompt: Task: Identify the Specific Value of Differing Attribute Between Two Products Instructions: 1. Carefully examine the two product images. 2. Focus ONLY on the following differing attribute: {diff_attribute} 3. From the provided list of possible values, select EXACTLY ONE value that best describes the {diff_attribute} that can make the first product (Image 1) into the product (Image 2). 4. Your response must contain ONLY the selected value - no explanations, no quotes, no additional text. 5. If none of the values in the provided list accurately describe the difference, respond with ONLY the word: no Image 1: <image> Image 2: <image> Differing attribute: {diff_attribute} Available values to choose from: {all_values} Remember: - Your response must be exactly one value from the provided list - Do not add quotation marks or any additional text - Focus exclusively on the {diff_attribute} difference between the products Response (value only): 21 Task: Generate Enhanced Product Search Queries Based on Visual Comparison Instructions: 1. Examine both the reference product image (Product 1) and the target product image carefully. 2. Create natural, concise search query that helps retrieve the target product by: - Maintaining the original statement structure - Adding ONE new distinctive attribute unique to the target image - Including the target products category/type in your search query Key Requirements: - Focus only on visually detectable attributes of the specific product in both images - Highlight meaningful differences or similarities between products - Use natural, conversational language - Incorporate the new attribute seamlessly without explicitly stating \"this product has attribute\" - Preserve all Product tag formatting exactly as shown: <Product 1> image </Product 1> and <Product 2>product description</Product 2> Note: Product 1 will be represented by an image, while Product 2 will be represented by text attributes. Example: Product 1 Image: [IMAGE] Target Image: [IMAGE] Search Statement: Find product with the same color as in <Product 1> image </Product 1> and the same brand as in <Product 2> product with brand Nike</Product 2>. Your Return is: Find T-shirt with the same color as in <Product 1> image </Product 1> and the same brand as in <Product 2> product with brand: Nike</Product 2> with small logo. Product 1 Image: <image> Target Image: <image> Search Statement: Find product that has the same material as <Product 1> image </Product 1> and the same {diff_attr} as <Product 2> product with {diff_attr}: {diff_value}</Product 2>. Your Return is: Upon obtaining the differentiating attributes and their corresponding values, we can retrieve additional products through attribute-based lookup and assemble them together with the initial conditions to form complete search query. It should be noted that the example presented above illustrates cases with single differentiating factor, resulting in query formations with two conditions retrieving one product. We can further prompt the model to identify additional existing differentiating factors (utilizing the aforementioned prompts), thereby expanding the number of conditions and enhancing the precision of both search statements and retrieved samples. Once we have acquired the conditions and target positive sample products, we can employ the following prompt to generate the retrieval instruction through the LLM. As shown in the black block above the text. Finally, this augmented data, like the data generated by the original pipeline, is fed into the filtering pipeline described in Appendix A.4. A.4 Auto and Human Filter A.4.1 Auto Filter To enhance efficiency, we implement an initial LLM filtering stage prior to manual annotation. Specifically, when employing machine-based annotation, we require adherence to the following three fundamental principles: Non-omission requirement: Essential information must be present in the text. For instance, material attributes must be explicitly mentioned within the textual content when required for retrieval. 22 Vision-centric approach: Attributes that can be effectively communicated through visual representation should rely on visual information rather than textual description. For example, product color attributes should be primarily identified through visual features rather than written descriptions. Accuracy criterion: The characteristics of positive samples must correspond precisely with the specified retrieval features. For instance, when color consistency is required as search criterion, the positive samples must exhibit color attributes that exactly match those of the corresponding conditional products. This multi-modal evaluation framework ensures that the automated filtering process maintains semantic consistency across both textual and visual dimensions while maximizing retrieval relevance. The LLM prompt employed in our implementation follows structured guidelines that prioritize these three principles, resulting in higher-quality candidate samples for subsequent manual verification. For the LLM implementation, the specific prompt utilized is: A.4.2 Human Filter Following the automated filtering process, all data undergoes rigorous manual refinement to ensure quality and comprehensiveness. This human verification stage consists of the following key components: 1. Data Curation: Annotators critically review and eliminate data entries deemed inappropriate for the dataset. Furthermore, they refine imprecise search queries to enhance their accuracy and relevance to the target products. This process ensures that the search statements precisely capture the intended product characteristics and filtering criteria. 2. False Negative Correction and Positive Sample Expansion: Recognizing that search queries may retrieve multiple valid products beyond those initially annotated, annotators systematically identify and include all products satisfying the established search criteria as positive samples. This step addresses potential false negatives from the automated process and creates more comprehensive and representative set of positive examples, thereby improving the overall quality and completeness of the dataset. The detailed annotation protocol, platform specifications, and comprehensive documentation for this manual verification process are provided in Appendix B. These resources offer complete guidance on annotation standards, quality control measures, and platform-specific instructions for the human curation phase."
        },
        {
            "title": "B Data Annotation Protocol",
            "content": "B.1 General Guidelines As previously discussed, there is significant gap in existing benchmarks [84, 62], which primarily focus on homogeneous retrieval scenarios with predefined formats, limiting themselves to single domains, single languages, or employing only singular conditions. To bridge this gap, our benchmark, MERIT, is designed to provide comprehensive dataset for interleaved multi-condition semantic retrieval, integrating multilingual understanding with the assessment of composite multi-condition queries across diverse product categories. Our dataset follows the guidelines outlined below for data collection: General Principles: Annotations must be accurate, consistent, and adhere to high standard of academic rigor. It covers multiple product categories and languages to mirror real-world applications across Southeast Asia. It incorporates diverse visual contexts and attribute combinations to foster wellrounded evaluation. It provides robust evaluation settings for deterministic assessments. Specific Instructions: All retrieval pairs must contain one or more images expressing specific properties. All queries should be available in five languages spanning six Southeast Asian countries. All queries should meet real-world e-commerce search complexity. Queries should not be ambiguous and must be answerable with the products in the dataset. Clearly categorize each query across seven distinct product retrieval scenarios. Annotate all fields, including attribute tags, visual descriptors, and other elements that follow the format requirement. Review Process: Ensure that every annotation undergoes peer review to maintain high standards and minimize errors or inaccuracy. Annotations such as product attributes, language, country, category are also collected, providing detailed examples that demonstrate the semantic retrieval capabilities of the models for further analysis and usage. B.2 Data Format and Structure Detailed examples of annotated retrieval pairs are provided in the guidance to serve as reference for the annotators. JSON File Format: The structured JSON format will include fields for product identifiers, attribute types, attribute values, query conditions, target products, language identifiers, and region information. Naming Conventions: Each collected sample will be stored in separate JSON file following standard naming rule: product_category_{Number}.json Image Files: image_{ProductID}_{ImageNum}.png B.3 Quality Control and Validation Protocol The integrity and reliability of the MERIT dataset are maintained through comprehensive quality assurance framework that encompasses multilingual expertise and systematic validation procedures. The following protocol has been established and rigorously implemented throughout the data collection process: 24 (1) Product Selection and Authentication Protocol. All candidate products must (i) look authentic. (ii) demonstrate substantial commercial viability as evidenced by popularity, and (iii) meet or exceed the predetermined aesthetic quality threshold as quantified by established metrics [69, 91]. Each product undergoes verification by authorized annotators before inclusion in the product inventory. (2) Attribute Standardization and Verification Protocol. Given the inherent insufficiency of attribute richness in commercial e-commerce data, all products must undergo attribute enrichment via our prescribed open annotation methodology. Annotators shall (i) document all observable attributes using standardized lexicon, (ii) participate in statistical analysis for Attribute Delineation, and (iii) verify attribute consistency across similar products. All attribute assignments must receive secondary validation before entering the final database. (3) Query Formulation Compliance Protocol. To ensure statistical robustness and ecological validity, every search query must be generated through our tri-modal sampling methodology: (i) Conventional Uniform Sampling, (ii) Attribute Uniform Sampling, and (iii) High-similarity Product Prioritization Sampling, as detailed in Appendices A.3.1-A.3.3. Cold-start expansion procedures (Appendix A.3.4) shall be employed only after primary sampling methods have been exhausted. Each query composition undergoes verification against pre-established diversity metrics. (4) Multi-stage Validation Protocol. All dataset entries are subject to mandatory dualphase validation: (i) Automated Validation Phaseemploying deterministic rule-based systems and statistical outlier detection methods to identify and eliminate non-conforming samples; followed by (ii) Expert Validation Phaserequiring assessment by annotators with demonstrated proficiency in the relevant languages and product domains. Samples must achieve unanimous approval through both validation phases to MERIT inclusion in the final dataset. Non-compliant samples shall be documented for methodological refinement purposes. B.4 Handling Ambiguities Instances of ambiguity or unclear data are flagged for detailed review. These instances are collaboratively examined during team meetings to establish standardized approach for annotation. Particular attention is paid to visual attribute interpretation across different cultural and linguistic contexts. B.5 Ethical Considerations Copyright and Licensing: Adherence to copyright and licensing regulations is strictly enforced. Data from sources that prohibit copying or redistribution is explicitly avoided. Data Privacy: Compliance with privacy laws and ethical standards in data handling is paramount. Annotators must avoid collecting product information that contains any private information. Ethical Data Usage: All data collection and usage must respect ethical guidelines. This includes avoiding biased or harmful content and ensuring that the datasets promote fairness and inclusivity across diverse cultural contexts. B.6 Data Contamination Considerations The risk of data contamination is mitigated by assigning annotators to carefully select products and attributes that extend beyond straightforward queries with easily accessible answers. It is essential that retrieval tasks rely on provided images for attribute identification rather than the common knowledge of large language models. This approach is beneficial for creating benchmarks that genuinely test the models ability to comprehend and synthesize information from diverse visual sources across multiple languages and cultural contexts. B.7 Annotation Platform We developed GUI-based annotation platform [3, 13], as illustrated in Fig. 22, specifically engineered to facilitate the data annotation process for human experts. This system enables specialists Figure 21: Flowchart of manual review of labeled data. to efficiently visualize images, while performing annotations and modifications directly within an intuitive interface. The platforms streamlined layout significantly enhances user experience, enabling experts to execute annotation tasks with increased precision and efficiency, thus elevating both the quality and productivity of the annotation process. The primary objective of this tool is to optimize the complex annotation workflow, minimize manual intervention, and substantially improve the overall efficiency of data annotation procedures. Figure 22: Annotation Platform. 26 B.8 Data Preparation and Release For evaluation purposes, we selected 10, 000 queries from total of 320, 000 queries in MERIT as the test set. To ensure equitable representation of each source dataset within the test split and maintain distributional consistency of language and product categories between the test set and the complete dataset, we implemented stratified sampling methodology: 1. Random sampling of queries to achieve proportional representation of languages and product categories in alignment with the full dataset distribution. 2. Supplementary random sampling of remaining queries from individual source datasets according to their respective volumetric contributions to the complete corpus. After dividing the dataset into training and test sets, the test set underwent two additional rounds of manual annotation to further enhance quality. Our initial pre-annotation test set consisted of approximately 20, 000 queries, which was subsequently refined to 10, 000 queries through careful curation."
        },
        {
            "title": "C More Dataset Analysis",
            "content": "C.1 Global Statics Tab. 4 delineates the taxonomic structure and distribution of product categories within the MERIT dataset. The hierarchical organization comprises seven primary product types (Food, Clothing, Electronics, Bags, Jewelry, Furniture, and Others), further subdivided into specific secondary types to enable fine-grained analysis. This comprehensive categorization encompasses essential consumer products ranging from perishables such as fruits and beverages to durable items, including electronics and furniture. The diverse product spectrum reflects real-world product retrieval inventory diversity, providing robust foundation for evaluating retrieval models across various product domains. With approximately 160,000 total products, MERIT represents one of the most extensive multimodal semantic retrieval benchmarks in the literature, facilitating meaningful performance assessments across different product categories and their associated attributes. Tab. 23 and Tab. 24 present the geographical distribution of samples within the MERIT dataset across six Southeast Asian countries. Indonesia contributes the largest portion with samples, reflecting its significant market presence in the region. The Philippines follows with 569 samples, while Thailand, Malaysia, and Vietnam contribute X, and samples respectively. Singapore, with its smaller market size but strategic importance, accounts for samples. This diverse geographical representation ensures that the benchmark captures the linguistic and cultural nuances essential for evaluating retrieval systems in multilingual Southeast Asian product retrieval context. Language Number ID EN TH MS VN 2281 597 366 284 Country Number Indonesia Philippines Thailand Malaysia Vietnam Singapore 2281 569 366 284 268 28 Figure 23: Language distribution. Figure 24: Country distribution. Fig. 25 illustrates the cross-linguistic patterns in the retrieval process within the MERIT dataset. This Sankey diagram visualizes the flow between source query languages and target product languages, demonstrating the prevalence of both intra-linguistic searches (where source and target languages match) and cross-linguistic retrieval scenarios. The diagram reveals that Indonesian (id) serves as the predominant source language, with significant flows to other Southeast Asian languages, including Thai (th), Vietnamese (vi), and Malay (ms), as well as English (en). The thickness of each connection represents the frequency of language transitions, highlighting the multilingual nature of product retrieval interactions in the region. This visualization underscores the importance of robust cross-lingual retrieval capabilities in real-world applications, particularly in linguistically diverse markets. 27 Figure 25: Language Flow Diagram. The flow chart shows the flow of conditional products to target products. Fig. 26 presents the geographic distribution of retrieval patterns across six Southeast Asian countries within the MERIT benchmark. The Sankey visualization maps the flow of queries originating from one country to products associated with potentially different countries. Indonesia (ID) emerges as the primary source of queries, with substantial connections to other regional markets including the Philippines (PH), Thailand (TH), Malaysia (MY), and Vietnam (VN). The diagram illustrates both intra-national retrieval (where source and target countries match) and cross-border retrieval scenarios that reflect the interconnected nature of product retrieval in Southeast Asia. The varying widths of the flow connections quantify the frequency of these cross-market interactions, providing valuable insights into regional commerce patterns and highlighting the necessity for retrieval systems to effectively operate across national boundaries. Figure 26: Country Flow Diagram. The flow chart shows the flow of conditional products to target products. Fig. 27 delineates the inter-category retrieval dynamics within the MERIT dataset through comprehensive Sankey diagram. This visualization maps the relationships between query source product classes and retrieved target product classes, revealing both intra-category retrieval (where queries and results belong to the same product category) and cross-category retrieval scenarios. The diagram demonstrates how users searching within one product domain (e.g., clothing) may retrieve items not only within that same category but also from related categories (e.g., accessories or footwear). The varying thickness of connecting flows represents the frequency of these category transitions, providing quantitative insights into product category relationships. This visualization is particularly valuable for understanding the complex cross-categorical nature of product retrieval search behavior and for developing retrieval systems capable of accommodating diverse user intentions that span multiple product domains. Figure 27: Product Class Type Flow Diagram. The flow chart shows the flow of conditional products to target products. Fig. 28 presents comprehensive visualization of product distribution across linguistic and geographical dimensions in the MERIT dataset. The left sunburst chart illustrates the distribution by language, with the inner ring representing five languages (id, en, th, ms, vi) and the outer ring depicting the corresponding product categories. Indonesian (id) dominates the linguistic landscape, constituting the largest portion, followed by English (en), Thai (th), Malay (ms), and Vietnamese (vi). The right sunburst chart demonstrates the geographical distribution, with the inner ring representing six Southeast Asian countries (ID, PH, TH, MY, VN) and the outer ring showing product categories within each country. Indonesia (ID) accounts for the most substantial proportion, followed by the Philippines (PH), Thailand (TH), Malaysia (MY), and Vietnam (VN). Across both dimensions, clothing items (particularly pants and clothes) and electronics (predominantly phones) emerge as the most prevalent product categories. This dual visualization effectively captures the multilingual and multicultural nature of the dataset, highlighting the proportional distribution of product types across different languages and markets in Southeast Asia. C.2 Attribute and Product Attribute Distribution. Fig. 29 presents bar chart of the top 30 most frequent attributes within the MERIT dataset. The chart, ordered by descending frequency, highlights the predominant attribute categories such as color and material, providing quantitative overview of attribute distribution. This visualization aids in understanding the key characteristics emphasized in the dataset. Attribute Word Cloud. Fig. 30 displays word cloud of the top 30 attributes from the MERIT dataset. Larger font sizes indicate higher frequency, with color and material standing out, offering visual summary of the most common attributes. This representation enhances the perception of attribute prominence within the benchmark. Value Distribution. Fig. 31 illustrates bar chart of the top 30 most frequent values associated with attributes in the MERIT dataset. Ordered by frequency, it showcases dominant values like gray and pink, providing detailed view of value distribution. This chart supports the analysis of specific attribute values prevalent in the dataset. 29 Figure 28: Sunburst visualization of product distribution in MERIT. The left chart shows the hierarchical relationship between languages (inner ring) and product categories (outer ring), while the right chart illustrates the distribution between countries (inner ring) and product categories (outer ring). Figure 29: bar chart of the top 30 most frequent attributes in MERIT, ordered by frequency, highlighting key attribute categories. Value Word Cloud. Fig. 32 presents word cloud of the top 30 values from the MERIT dataset. Larger font sizes reflect higher frequency, with gray and pink being prominent, offering visual insight into the most common attribute values. This visualization facilitates the exploration of value diversity within the benchmark. Product Word Cloud Fig. 33 presents word cloud visualization of the query instruction strings from the MERIT dataset. The word cloud highlights the most frequent terms, with larger font sizes indicating higher frequency, providing clear overview of the key themes and vocabulary used in user queries. This visualization underscores the diversity and focus of query instructions within the benchmark, offering insights into user search behavior. Title Fig. 34 displays word cloud representation of the product titles within the MERIT dataset. The visualization emphasizes frequently occurring words with larger font sizes, revealing common 30 Figure 30: word cloud of the top 30 attributes in MERIT, with larger fonts indicating higher frequency. Figure 31: bar chart of the top 30 most frequent values in MERIT, ordered by frequency, highlighting dominant attribute values. descriptors and product characteristics in product retrieval titles. This word cloud provides snapshot of the linguistic patterns and descriptive focus of product titles, shedding light on the nature of product representations in the benchmark. Product Length Fig. 35 illustrates the distribution of character counts in product titles within MERIT. The visualization reveals significant variation in title lengths across the dataset, reflecting the diverse nature of e-commerce product descriptions. For visualization simplicity, titles exceeding 190 characters in length have been consolidated into single category. This distribution provides insight into the complexity and descriptive depth of product representations within the benchmark. Query Length Fig. 36 presents the distribution of character counts in query instructions across MERIT. The distribution demonstrates considerable variation in query complexity, reflecting the multi-condition and multilingual nature of the benchmark. For visualization clarity, query instructions exceeding 500 characters have been aggregated into single category. This analysis provides valuable context regarding the linguistic complexity of retrieval instructions that models must process to accurately identify relevant products. 31 Figure 32: word cloud of the top 30 values in MERIT, with larger fonts indicating higher frequency. Figure 33: word cloud of the query instruction strings in MERIT. Larger font sizes represent higher frequency of terms, illustrating the key themes in user queries. Figure 34: word cloud of the product titles in MERIT. Larger font sizes indicate higher frequency of terms, highlighting common descriptors in product titles. Figure 35: The distribution of the string length of product title in MERIT. Titles with length greater than 190 are categorized as 190 for visualization simplicity. Figure 36: The distribution of the string length of query instruction in MERIT. Query instructions with length greater than 500 are categorized as 500 for visualization simplicity."
        },
        {
            "title": "D Dataset Examples",
            "content": "Table 5: Table index of case study figures by meta-task with associated error categories. If there are more than 2 conditions, we only show the first 2. Case Figure Attribute Attribute II Target Product Type Target Product Language Figure 37 Figure 38 Figure 39 Figure 40 Figure 41 Figure 42 Figure 43 Figure 44 Figure 45 Figure 46 Figure 47 Figure 48 Figure 49 Figure 50 Figure 51 Figure 52 Figure 53 Figure 54 Figure 55 Figure 56 Figure 57 Figure 58 Figure 59 Figure 60 Figure 61 Figure 62 Figure 63 Figure 64 Figure 65 Style Toe Style Heel Height Design Style Brand Insole Material Material Tightness Movement Type Fit Closure Color Color Brand Wheel Type Camera Neck shape Material Collar type Embellishment Brand Material Occasion Lace style Brand Pockets Capacity Color Shoe type Color Color Color Size Fashion Style Color Color Pattern Color Color Color Material Material Color Color Color Color Pattern Color Sleeve type Storage capacity Closure type Material Color Color Color Brand Shoe type Closure type Pants Shoes Shoes Handbag Backpack Shoes Pants Pants Chair Pants Shoes Cloth Pants Headphone Suitcase Phone Cloth Pants Cloth Cloth Phone Shoes Pants Shoes Headphones Pants Suitcase Shoes Shoes Vietnamese Thai Thai Thai English Indonesian Vietnamese Thai English Thai Thai English English English Thai English Indonesian Thai Thai Thai Thai English English Thai Thai English English English English Figure 37: sample case. Back to List of Figures. 34 Figure 38: sample case. Back to List of Figures. Figure 39: sample case. Back to List of Figures. Figure 40: sample case. Back to List of Figures. Figure 41: sample case. Back to List of Figures. Figure 42: sample case. Back to List of Figures. 35 Figure 43: sample case. Back to List of Figures. Figure 44: sample case. Back to List of Figures. Figure 45: sample case. Back to List of Figures. Figure 46: sample case. Back to List of Figures. Figure 47: sample case. Back to List of Figures. 36 Figure 48: sample case. Back to List of Figures. Figure 49: sample case. Back to List of Figures. Figure 50: sample case. Back to List of Figures. Figure 51: sample case. Back to List of Figures. Figure 52: sample case. Back to List of Figures. 37 Figure 53: sample case. Back to List of Figures. Figure 54: sample case. Back to List of Figures. Figure 55: sample case. Back to List of Figures. Figure 56: sample case. Back to List of Figures. Figure 57: sample case. Back to List of Figures. 38 Figure 58: sample case. Back to List of Figures. Figure 59: sample case. Back to List of Figures. Figure 60: sample case. Back to List of Figures. Figure 61: sample case. Back to List of Figures. Figure 62: sample case. Back to List of Figures. Figure 63: sample case. Back to List of Figures. Figure 64: sample case. Back to List of Figures. Figure 65: sample case. Back to List of Figures."
        },
        {
            "title": "E Experiments Details",
            "content": "E.1 Baselines For most MLLM-based models, we adhered to the standard evaluation protocol delineated in their respective original configurations. In instances where such protocols were not specified, we followed the established conventions outlined in VLMEvalKit [14, 67], consistently setting the temperature parameter to 0. Specifically, we categorized the evaluated models into two distinct types: ZeroShot MLLMs, which have not undergone training on dedicated retrieval datasets, and Embedding MLLMs, which have been specifically trained on existing retrieval datasets through particular methodologies for retrieval purposes. Zero-Shot MLLM: InternVL2.5-VL [11] is an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. Through extensive evaluations on wide range of benchmarks, including multi-discipline reasoning, document understanding, interleaved multi-image understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, InternVL 2.5 is the first open-source MLLM to achieve over 70% on the MMMU benchmark. For our experiments, we utilized the 1B variant, as well as the version enhanced through MPO training. Both models were evaluated with the maximum number of tiles constrained to one. Qwen2.5-VL [5]. Qwen2.5-VL represents the latest flagship model in the Qwen vision-language series, achieving significant advancements in foundational multimodal capabilities including enhanced visual recognition, precise object localization via bounding boxes, robust document parsing, and long-video comprehension with second-level event localization. The models innovative architecture incorporates dynamic resolution processing and absolute time encoding, enabling it to natively perceive spatial scales and temporal dynamics without traditional normalization techniques, while its native dynamic-resolution Vision Transformer with Window Attention [56] maintains resolution integrity with reduced computational overhead. For our experiments, we employed the 3B parameter variant of this model, configuring the max_pixels parameter to 576 28 28. Embedding MLLM: E5-V [37]. E5-V adapts Multimodal Large Language Models (MLLMs) to generate universal multimodal embeddings, effectively bridging the modality gap between different input types. Employing an innovative single-modality training approach using exclusively text pairs, E5-V demonstrates superior performance compared to traditional multimodal training methods while reducing training costs by approximately 95% and eliminating the need for expensive multimodal data collection. Following the original paper, we obtain the global token through the prompt Summary above image in one word: . LLaVE [40]. LLaVE represents groundbreaking framework for universal multimodal embeddings that effectively addresses the challenge of distinguishing hard negative pairs in image-text retrieval tasks through dynamic representation learning based on discriminative difficulty. We evaluated three model sizes: 0.5B, 2B, and 7B parameter versions. GME-Qwen2VL [98]. General Multimodal Embedder (GME) functions as an MLLM-based dense retriever capable of processing queries and candidates across text, images, or multimodal combinations. Developed using novel training data synthesis pipeline that addresses modality imbalance issues, GME overcomes the limitations of previous approaches that relied solely on text data for training. In our experiments, we utilized the GME implementation based on Qwen2VL-2B. LamRA-Qwen2.5VL [55]. LamRA is versatile framework that repurposes MLLMs for comprehensive retrieval tasks, eliminating the need for task-specific fine-tuning. Employing two-stage training methodologylanguage-only pre-training followed by multimodal instruction tuningand joint training for both pointwise and listwise reranking, LamRA demonstrates exceptional performance across more than ten retrieval tasks in both supervised and zero-shot settings. Our evaluation utilized the LamRA implementation based on Qwen2.5VL-7B. 41 BGE-VL [100]. We evaluated the BGE-VL-MLLM-S1 model, which was obtained from https: //huggingface.co/BAAI/BGE-VL-base. BGE-VL-MLLM-S1 is trained exclusively on the MegaPairs dataset, achieving outstanding performance in composed image retrieval tasks. VLM2Vec [38]. VLM2Vec is novel multimodal embedding framework designed to encode sequences of images and text into unified representation space for diverse downstream applications. Unlike conventional CLIP or BLIP embeddings that operate under constraints of fixed image resolutions and text lengths, VLM2Vec accommodates inputs of arbitrary dimensions and sequence lengths, significantly enhancing its versatility across multimodal tasks. Our experiments utilized the 4B parameter version of VLM2Vec. E.2 Other Dataset Usage In this section, we provide detailed introduction to the additional datasets utilized in our paper. To validate the effectiveness of our proposed method, CORAL, we conducted comprehensive evaluations on several widely-used retrieval benchmarks. VisDial [15]: This dataset presents interactive visual dialogues generated through controlled collaboration between two annotators on Amazon Mechanical Turk. In this conversational framework, one participant assumes the role of the \"questioner,\" with access only to textual caption of an image, while the other serves as the \"answerer,\" with complete visual access to the image itself. They engage in structured 10-round question-and-answer exchanges regarding image content. We repurpose this dialogically rich dataset as cross-modal retrieval task, where the objective is to identify and retrieve the precise image that corresponds to the given conversational dialogue, thereby testing models abilities to construct visual representations from textual discourse. CIRR [58]: This dataset is meticulously designed for composed image retrieval tasks, focusing on natural language modifications of visual content. It comprises pairs of real-world reference and target images, accompanied by linguistically nuanced modification descriptions that articulate the transformative differences between the source and target images. This configuration presents particularly challenging evaluation scenario that requires models to understand both visual foundations and linguistic modifications in compositional manner. VisualNews [52]: This corpus encompasses substantial collection of publicly available news images paired with professionally written captions. The dataset features content from major news organizations and represents diverse range of visual journalism across various domains, providing real-world test of multimodal understanding in the context of current events and factual reporting. The caption-image pairs exhibit complex relationships that often require world knowledge and contextual understanding of news content. MSCOCO [51]: This comprehensive benchmark dataset features over 330,000 images, each meticulously annotated with multiple human-generated captions. Originally designed for object detection, segmentation, and captioning tasks, MSCOCO has become fundamental standard for evaluating multimodal capabilities. Its diversity spans 91 object categories captured in everyday contexts with multiple objects per image. The datasets rich annotations facilitate cross-modal retrieval in both textto-image and image-to-text directions, providing robust assessment of bidirectional understanding between visual and linguistic modalities. NIGHTS [21]: This dataset introduces perceptually calibrated human similarity judgments on image pairs that exhibit diverse forms of visual correspondences. The corpus consists of carefully constructed triplets: reference image and two systematically perturbed variations, accompanied by human perceptual judgments indicating which variation maintains greater similarity to the reference. Following the methodology established in M-BEIR [82], we reconfigure this dataset into retrieval framework for image-to-image matching, where the reference image functions as the query, and the perturbed version that aligns with human perceptual judgment serves as the target. This transformation provides rigorous test of models ability to replicate human perceptual similarity assessments. WebQA [7]: This dataset presents multihop, multimodal question-answering framework that necessitates the retrieval and integration of information from Wikipedia pages to formulate responses to given queries. The datasets complexity emerges from its requirement for models to navigate both textual and visual information across multiple reasoning steps. In our experimental context, we utilize the Wikipedia pages images and accompanying textual descriptions as candidate elements 42 Table 6: Comparision with other method across 8 retrieval tasks. Qwen2.5-VL [5] InternVL2.5-VL [11] BGE-VL [100] EN 48.73 55.38 13.76 ID 56.23 60.75 14.02 TH 55.34 58.97 14.83 VN 55.13 62.82 20.51 MS 47.98 52.47 14.08 for retrieval, thereby evaluating models capacities to identify relevant multimodal content based on query specifications. E.3 Main Experiments Settings All experiments were conducted on computing node equipped with 8H100 GPUs. Experiments were conducted for single epoch with the following training configuration: perdevice batch size of 4 was employed with gradient accumulation steps set to 2, resulting in an effective global batch size of 64. The InfoNCE contrastive loss temperature parameter (τ ) was fixed at 0.02. For negative sampling, we implemented in-batch negatives combined with cross-device negative sample gathering, achieving final positive-to-negative ratio of 1 : 63. For full-parameter fine-tuning, we adopted learning rate of 1e 5 with weight decay of 0.0005 and linear warmup ratio of 0.01. The LoRA [27] configuration employed the following parameters: learning rate of 1e 4 (10 times higher than full fine-tuning), identical weight decay (0.0005) and warmup ratio (0.01), with LoRA-specific hyperparameters set to = 8, α = 16, no bias terms, and dropout rate of 0.05 between LoRA layers. The CORAL framework was configured with the following hyperparameters: the loss weighting coefficients λ1 andλ2 were both set to 0.1 to balance the objective components, while maintaining uniform masking probabilities of 0.5 for both visual and linguistic modalities. This symmetric configuration ensures equal contribution from both vision and language streams during the masked reconstruction tasks. Across all training regimes, we kept the vision tower completely frozen to preserve its pretrained representations. For LoRA-based adaptation, we specifically applied low-rank adaptation only to the LLM backbone components, while maintaining standard full-parameter training for all BERT decoder layers. This hybrid approach allowed us to efficiently adapt the language model while preserving the decoders complete expressive capacity. E.4 More Experiments Results Different Languages Performance In Section 3.4, we present the accuracy distribution across different languages, with specific values shown in Table 6. We define query as belonging to particular language only when all products in both the query statement and the positive samples are in that language. Out of Distribution Scenarios. As mentioned in Sec. 3.3, we tested several out-of-distribution (OOD) scenarios. Tables 8, 7, and 9 correspond to the Zero-shot, OOD, and Mixed results depicted in Figure 6(b), respectively. Zero-shot refers to direct inference using Qwen2.5-VLs [EOS] token. OOD indicates that for each row in Table 7, we excluded the specified OOD data (e.g., for the first row concerning Language ID OOD, we removed all queries containing the ID language from the training set, while for the test set, we only selected data where all languages were ID). Mixed refers to training conducted on the complete training dataset. Results on 8 Retrieval Tasks. To further validate the efficacy of CORAL, we conducted evaluations on several retrieval datasets. The experimental results are presented in Tab. 10, with experimental configurations following the methodology described in [38]. E.5 Error Analysis Details In this section, we present case study analysis of the error types made by Qwen2.5-VL-3B [5] and GME-Qwen2VL-2B [98] across various tasks. The errors are classified into the following five categories. Other less frequent error types are not included in this analysis. For the analysis, as 43 Table 7: Out-of-distribution test results for Qwen2.5-VL trained on in-domain data. AVG represents the average value, and # indicates the total number of entries in the test set. Language Attribute Class # 513 434 1020 1967 889 1002 89 1980 475 1958 422 2855 MRR R@1 R@5 R@ 44.26 42.82 45.00 44.03 60.72 54.93 44.29 53.31 51.14 43.47 48.57 47.73 15.98 17.97 19.80 17.92 48.03 37.23 19.10 34.79 33.47 31.77 35.07 33. 79.34 74.65 75.29 76.43 78.07 77.74 78.65 78.15 73.47 58.63 64.69 65.60 87.91 84.33 84.22 85.49 85.60 84.53 86.52 85.55 81.05 67.72 74.64 74. ID MS TH AVG Brand Pattern Region AVG Drink Phone Table AVG Table 8: Out-of-distribution test results for zero-shot Qwen2.5-VL. AVG represents the average value, and # indicates the total number of entries in the test set. Language Attribute Class # 513 434 1020 1967 889 1002 89 1980 475 1958 422 2855 MRR R@1 R@ R@10 4.28 6.47 2.67 4.47 5.46 0.95 7.46 4.62 6.68 2.18 2.82 3.89 2.34 3.92 0.69 2.32 2.81 0.50 4.49 2. 3.58 1.02 1.18 1.93 7.02 9.22 5.20 7.15 8.44 1.50 12.36 7.43 10.95 3.47 4.98 6.47 9.55 12.44 7.94 9.98 13.05 2.40 14.61 10. 13.89 5.72 7.35 8.99 ID MS TH AVG Brand Pattern Region AVG Drink Phone Table AVG mentioned in Sec. 3.4, we selected 500 samples for each model, but due to space limitations, we present only some of them here, as shown in the following Figures. Attribute Error: Retrieval models often misidentify or incorrectly select attributes in product retrieval tasks, resulting in recommendations that fail to meet the specified attribute criteria. As illustrated in Fig. 66, while the recalled product matches the pattern of Product 1, its color does not align with the requirement of Product 2 (which shares the same condition). This discrepancy highlights the models misinterpretation of attribute-based constraints. Visual Understanding Error: Retrieval models accurately identify the requested attributes but fail to generate corresponding and consistent visual outputs. In such cases, while the model correctly interprets the textual instructions, it struggles to align them with appropriate visual representations. As illustrated in Fig. 67, although the language command is comprehended correctly, the retrieved product image is incorrect [80]. Category Error: Retrieval models retrieve products from incorrect categories, indicating failure to accurately classify items within the prescribed product taxonomy. As illustrated in Fig. 68, the model is tasked with identifying mobile phone bag but instead retrieves generic bag, demonstrating misclassification. Detail Error: Retrieval models accurately identify the primary product conditions but often overlook specific secondary requirements or finer details. Consequently, their recommendations satisfy the main criteria but fail to capture critical nuances specified in the query. As illustrated in Fig. 69, 44 Table 9: Out-of-distribution test results for Qwen2.5-VL trained on the whole train set of 310, 000 queries. AVG represents the average value, and # indicates the total number of entries in the test set. Language Attribute Class # 513 434 1020 1967 889 1002 89 1980 475 1958 422 MRR R@1 R@5 R@10 71.34 67.51 69.79 69.55 71.87 63.50 57.81 64.39 53.54 63.64 52.40 56. 57.31 53.23 55.29 55.28 59.62 50.30 38.20 49.37 33.26 51.58 38.63 41.16 88.89 85.48 87.65 87.34 88.98 80.44 83.15 84.19 78.53 79.78 69.19 75. 94.93 91.94 91.57 92.81 93.59 87.03 93.26 91.29 86.74 86.57 77.96 83.76 ID MS TH AVG Brand Pattern Region AVG Drink Phone Table AVG Table 10: Comparision with other method across 8 retrieval tasks. Model VisDial CIRR VisualNewsT 2I VisualNewsI2T COCOT 2I COCOI2T NIGHTS WebQA CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V GME-Qwen2-VL-2B Qwen2-VL-2B Qwen2-VL-2B-CL Qwen2-VL-2B+CORAL 30.70 25.40 21.50 18.00 24.80 9.20 26.00 13.00 51.00 73.00 12.60 15.40 15.10 9.80 39.10 6.10 38.00 20.00 39.00 50.00 78.90 74.00 51.00 48.10 50.70 13.50 66.00 40.00 56.00 67.00 79.60 78.00 52.40 13.50 21.10 8.10 71.00 43.00 52.00 72.00 59.50 63.60 58.30 53.70 54.10 20.70 62.00 49.00 56.00 68.00 57.70 62.10 55.00 20.30 40.00 14.00 56.00 39.00 45.00 64. 60.40 66.10 62.90 56.50 58.10 4.20 64.00 59.00 58.00 65.00 67.50 62.10 58.10 55.40 43.00 17.70 83.00 20.00 67.00 84.00 Figure 66: case for Attribute Error. In this example, the same condition as product 2 is color. The recalled product meets the pattern of product 1, but the color does not meet the requirements of product 2. This is misunderstanding of the attribute content. although the retrieved product fulfills the two primary conditions, it does not meet the detailed requirement of \"having visibly displayed brand logo.\" Annotation Error: Inaccuracies in dataset annotations may result in cases where the models response appears incorrect when evaluated against imprecise ground truth. Such errors are rare, as our data undergoes multiple rounds of manual review. However, given the vast scale of the candidate set, we cannot entirely rule out the possibility of small number of positive samples remaining unlabeled. Figure 67: case for Visual Understanding Error. In this example, the language instruction was understood correctly, but the visual image of the recalled product was wrong. Figure 68: case for Category Error. In this example, the goal is to find mobile phone bag, but bag is recalled, so it is wrong."
        },
        {
            "title": "F Broader Impact",
            "content": "F.1 Impact The broader impact of MERIT carries both potential benefits and risks upon deployment and release. Some considerations are unique due to the multimodal nature of our dataset, while others reflect challenges common to retrieval systems in product retrieval environments. Built upon multilingual semantic understanding across Southeast Asian markets, MERIT inherits issues associated with cross-cultural product retrieval and multi-condition query interpretation. Below, we outline risks and mitigation strategies for its release. Hallucination. On one hand, since the product titles in our dataset were generated by GPT-4o, there exists potential for hallucination issues [35, 92]. On the other hand, similar to other retrieval datasets [48, 99, 73], models trained on MERIT may generate outputs that are disconnected from user intentions or input conditions. This raises concerns, particularly in product retrieval applications where purchase decisions depend on accurate results, as user requirements and their modes of expression are inherently variable. Biases. Bias in training data can propagate to models employing MERIT, arising from both visual feature extraction and linguistic interpretation. This may result in biased retrieval outcomes or unfair 46 Figure 69: case for Detail Error. In this example, although the recalled product meets both conditions, the detail condition \"with visible brand logo\" is not met. representations across diverse cultural contexts. Additionally, multilingual processing can introduce further biases in language alignment, as noted by [59]. Ethical Impacts. This research does not present substantial ethical concerns. Furthermore, we affirm that our open-source data and model distribution comply with all corporate guidelines and industry regulations governing intellectual property and data sharing practices. Expected Societal Implications. significant societal benefit lies in enhancing cross-cultural product retrieval experiences through improved interleaved multi-condition semantic retrieval. However, challenges remain in ensuring fairness across linguistic and cultural boundaries. Strong ethical standards and ongoing evaluation are essential for maximizing positive impact. These issues arent unique to our method but are prevalent across different techniques for multi-condition retrieval. Despite the challenges, we believe the benefits significantly outweigh the potential limitations, allowing ongoing investigation and improvement of retrieval models while engaging the community in developing better approaches [98, 82, 61, 95]. Moreover, the release of MERIT can foster new applications and research directions, contributing to the progress and responsible deployment of retrieval systems in multilingual product retrieval environments. F.2 Limitations (i) Our dataset, while comprehensive, may inherit limitations from real-world product retrieval data, such as imbalances in product categories and potential biases in attribute distributions across different Southeast Asian markets. (ii) Despite rigorous filtering, the dataset might inevitably contain some inconsistencies between visual attributes and textual descriptions, which could adversely affect model training and evaluation."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. [2] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 4146, 2023. [3] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. [4] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In The Thirteenth International Conference on Learning Representations, 2024. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot composed image retrieval with textual inversion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1533815347, 2023. [7] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504, 2022. [8] Haibin Chen, Kangtao Lv, Chengwei Hu, Yanshi Li, Yujin Yuan, Yancheng He, Xingyao Zhang, Langming Liu, Shilei Liu, Wenbo Su, et al. Chineseecomqa: scalable e-commerce concept evaluation benchmark for large language models. arXiv preprint arXiv:2502.20196, 2025. [9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as unified interface for vision-language multi-task learning. arXiv:2310.09478, 2023. [10] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713, 2023. [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [12] Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, and Qianru Sun. Unified generative and discriminative training for multi-modal large language models. arXiv preprint arXiv:2411.00304, 2024. [13] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. [14] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. GitHub repository, 2023. [15] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326335, 2017. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [17] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32, 2019. [18] Taoran Fang, Tianhong Gao, Chunping Wang, Yihao Shang, Wei Chow, Lei Chen, and Yang Yang. Kaa: Kolmogorov-arnold attention for enhancing attentive graph neural networks. arXiv preprint arXiv:2501.13456, 2025. [19] Taoran Fang, Wei Zhou, Yifei Sun, Kaiqiao Han, Lvbin Ma, and Yang Yang. Exploring correlations of self-supervised tasks for graphs. arXiv preprint arXiv:2405.04245, 2024. 48 [20] Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, and Hanwang Zhang. On path to multimodal generalist: General-level and general-bench, 2025. [21] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. [22] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [23] Zhiqi Ge, Juncheng Li, Qifan Yu, Wei Zhou, Siliang Tang, and Yueting Zhuang. Demon24: Acm mm24 demonstrative instruction following challenge. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1142611428, 2024. [24] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [25] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. Semantic models for the first-stage retrieval: comprehensive review. ACM Transactions on Information Systems (TOIS), 40(4):142, 2022. [26] Xintong Han, Zuxuan Wu, Phoenix Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and In Proceedings of the IEEE Larry Davis. Automatic spatially-aware fashion concept discovery. international conference on computer vision, pages 14631471, 2017. [27] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [28] Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12065 12075, 2023. [29] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. Mrag-bench: Vision-centric evaluation for retrieval-augmented multimodal models. arXiv preprint arXiv:2410.08182, 2024. [30] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2336923379, 2023. [31] Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, and Yang Yang. One graph model for cross-domain dynamic link prediction. arXiv preprint arXiv:2402.02168, 2024. [32] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [33] Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul Chilimbi, and Abhinav Shrivastava. Collm: large language model for composed image retrieval. arXiv preprint arXiv:2503.19910, 2025. [34] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below. [35] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138, 2023. [36] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. Transactions on Machine Learning Research, 2024, 2024. [37] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. [38] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. 49 [39] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. [40] Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812, 2025. [41] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [42] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [43] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [44] Hui Li, Tsz Nam Chan, Man Lung Yiu, and Nikos Mamoulis. Fexipro: fast and exact inner product In Proceedings of the 2017 ACM International Conference on retrieval in recommender systems. Management of Data, pages 835850, 2017. [45] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in neural information processing systems, 35:72907303, 2022. [46] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. arXiv preprint arXiv:2308.04152, 2023. [47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training In International conference on machine for unified vision-language understanding and generation. learning, pages 1288812900. PMLR, 2022. [48] Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng, Xiao-Ming Wu, and Qianli Ma. Embedding-based product retrieval in taobao search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 31813189, 2021. [49] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. [50] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571, 2024. [51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [52] Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. arXiv preprint arXiv:2010.03743, 2020. [53] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [54] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [55] Yikun Liu, Pingan Chen, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. Lamra: Large multimodal model as your advanced retrieval assistant. arXiv preprint arXiv:2412.01720, 2024. [56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [57] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. arXiv preprint arXiv:2209.00179, 2022. [58] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21252134, 2021. 50 [59] Hanqing Lu, Youna Hu, Tong Zhao, Tony Wu, Yiwei Song, and Bing Yin. Graph-based multilingual product retrieval in e-commerce search. arXiv preprint arXiv:2105.02978, 2021. [60] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [61] Alessandro Magnani, Feng Liu, Min Xie, and Somnath Banerjee. Neural product retrieval at walmart. com. In Companion Proceedings of The 2019 World Wide Web Conference, pages 367372, 2019. [62] Md Messal Monem Miah, Agent Chatterjee, Arindam Mitra, Ruihong Huang, and Man Luo. Miracle: Multimodal image-text retrieval and analysis for contextual long-form evaluation. [63] Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, and Minjoon Seo. Instructir: benchmark for instruction following of information retrieval models. arXiv preprint arXiv:2402.14334, 2024. [64] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [65] Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv preprint arXiv:2405.01926, 2024. [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [67] Tim Rädsch, Leon Mayer, Simon Pavicic, Emre Kavur, Marcel Knopp, Barıs Öztürk, Klaus Maier-Hein, Paul Jaeger, Fabian Isensee, Annika Reinke, et al. Bridging vision language model (vlm) evaluation gaps with framework for scalable and cost-effective benchmark generation. arXiv preprint arXiv:2502.15563, 2025. [68] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1930519314, 2023. [69] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [70] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. Reasonir: Training retrievers for reasoning tasks. arXiv preprint arXiv:2504.20595, 2025. [71] Qi She, Junwen Pan, Xin Wan, Rui Zhang, Dawei Lu, and Kai Huang. Mammothmoda: Multi-modal large language model. arXiv preprint arXiv:2406.18193, 2024. [72] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [73] Manish Sihag. From videos to requirement: data-driven approach for finding requirements relevant feedback from TikTok and YouTube. PhD thesis, 2023. [74] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [75] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [76] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [77] Anthony Wagner, Juliana Paré-Blagoev, Jill Clark, and Russell Poldrack. Recovering meaning: left prefrontal cortex guides controlled semantic retrieval. Neuron, 31(2):329338, 2001. [78] Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen. On position embeddings in bert. In International conference on learning representations, 2020. 51 [79] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [80] Wei Wang, Zhaowei Li, Qi Xu, Linfeng Li, YiQing Cai, Botian Jiang, Hang Song, Xingcan Hu, Pengyu Wang, and Li Xiao. Advancing fine-grained visual understanding with multi-scale alignment in multimodal models. arXiv preprint arXiv:2411.09691, 2024. [81] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [82] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision, pages 387404. Springer, 2024. [83] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. Rethinking infonce: How many negative samples do you need? arXiv preprint arXiv:2105.13003, 2021. [84] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1130711317, 2021. [85] Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, et al. Scimmir: Benchmarking scientific multi-modal information retrieval. arXiv preprint arXiv:2401.13478, 2024. [86] Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, and Wenya Wang. Visual-rag: Benchmarking text-to-image retrieval augmented generation for visual knowledge intensive queries. arXiv preprint arXiv:2502.16636, 2025. [87] Ziheng Wu, Zhenghao Chen, Ruipu Luo, Can Zhang, Yuan Gao, Zhentao He, Xian Wang, Haoran Lin, and Minghui Qiu. Valley2: Exploring multimodal models with scalable vision-language design. arXiv preprint arXiv:2501.05901, 2025. [88] Robert Wyer Jr, Iris Hung, and Yuwei Jiang. Visual and verbal processing strategies in comprehension and judgment. Journal of Consumer Psychology, 18(4):244257, 2008. [89] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. arXiv preprint arXiv:2205.12035, 2022. [90] Brent Xu, Dhruv Luthra, Zak Cole, and Nate Blakely. Eos: An architectural, performance, and economic analysis. Retrieved June, 11(2019):41, 2018. [91] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [92] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1294412953, 2024. [93] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [94] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [95] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1178211791, 2021. [96] Haotian Zhang, Mustafa Abualsaud, Nimesh Ghelani, Mark Smucker, Gordon Cormack, and Maura Grossman. Effective user interaction for high-recall retrieval: Less is more. In Proceedings of the 27th ACM international conference on information and knowledge management, pages 187196, 2018. [97] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. arXiv preprint arXiv:2403.19651, 2024. 52 [98] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855, 2024. [99] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. Make: Vision-language pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference 2023, pages 356360, 2023. [100] Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024. [101] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. Vista: visualized text embedding for universal multi-modal retrieval. arXiv preprint arXiv:2406.04292, 2024. [102] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. Marvel: unlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037, 2023. [103] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [104] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. [105] Jian Zou, Tianyu Huang, Guanglei Yang, Zhenhua Guo, Tao Luo, Chun-Mei Feng, and Wangmeng Zuo. Unim 2 ae: Multi-modal masked autoencoders with unified 3d representation for 3d perception in autonomous driving. In European Conference on Computer Vision, pages 296313. Springer, 2024."
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "Zhejiang University"
    ]
}