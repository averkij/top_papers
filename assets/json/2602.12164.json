{
    "paper_title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "authors": [
        "Xiaohan He",
        "Shiyang Feng",
        "Songtao Huang",
        "Lei Bai",
        "Bin Wang",
        "Bo Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE."
        },
        {
            "title": "Start",
            "content": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Xiaohan He * 1 2 Shiyang Feng * 1 Songtao Huang 1 2 Lei Bai 1 Bin Wang 2 Bo Zhang 1 6 2 0 2 2 1 ] . [ 1 4 6 1 2 1 . 2 0 6 2 : r Abstract Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, twostage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through transition from sparse supervision to unsupervised learning. In the first stage, the model uses small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github. com/InternScience/Sci-CoE. 1. Introduction Self-evolving Reinforcement Learning (RL) has emerged as transformative paradigm for Large Language Models (LLMs) to refine their reasoning trajectories through feedback. notable milestone is the Zero RL paradigm introduced by DeepSeek-R1 (Guo et al., 2025), which elicits sophisticated reasoning behaviors without prior supervised imitation learning. However, this approach remains fundamentally dependent on annotated datasets for reward calculation. To mitigate this reliance, self-play mechanisms have been adopted to facilitate autonomous evolution. In these 1Shanghai Artificial Intelligence Laboratory 2Fudan University. Correspondence to: Bo Zhang <zhangbo@pjlab.org.cn>. Preprint. February 13, 2026. 1 Figure 1. Examples of Scientific Question, Generated Solution and Verification Strategies. settings, the LLM concurrently assumes multiple roles, such as challenger and solver (Huang et al., 2025a; Zhao et al., 2025) or solver and verifier (Wang et al., 2025b), to drive co-evolution through mutual interaction. These self-play methods significantly enhance the autonomy of reasoning training by reducing the need for external supervision. However, existing self-evolution paradigms are primarily confined to domains such as coding and mathematics, where task quality can be assessed through clear verification signals. In these domains, correctness can either be judged directly using ground-truth solutions or indirectly through explicit verification methods such as unit tests. In contrast, scientific reasoning tasks rarely provide such clear verification signals, which makes self-evolution in this domain far more challenging. Specifically, these tasks unfold in an open-world regime with multiple valid solution pathways and heterogeneous verification criteria. Furthermore, scientific reasoning tasks spans diverse disciplines where verification requires expert assessment of complex intermediate logic rather than simple answer matching. The prohibitive cost of curating such specialized supervision renders largescale and data-intensive training approaches impractical. Motivated by these challenges, we raise pivotal question: Can we develop self-evolving RL framework for scientific reasoning tasks under limited supervision? In this work, we introduce Sci-CoE, scientific co-evolving framework that consists of Solver and Verifier, both implemented within single LLM. The Solver generates Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision candidate solutions, and the Verifier constructs strategies to evaluate their correctness. These two roles are optimized jointly through interactive reinforcement learning. Furthermore, to maintain stability without ground-truth supervision, we propose geometric reward mechanism that represents verification strategies in latent geometric space. This mechanism encourages strategies to remain both reliable and diverse, which prevents consensus collapse and supports sustained self-evolution in open-ended scientific domains. Comprehensive experimental results demonstrate that Sci-CoE remains effective even when explicit verification signals are absent. Across diverse scientific domains, the framework achieves strong reasoning performance under limited supervision. Furthermore, the proposed reward mechanism enhances both the reliability and the diversity of verification strategies, leading to consistent performance gains. Our contributions are summarized as follows: We proposed co-evolving framework called Sci-CoE for scientific reasoning that integrates Solver and Verifier within single LLM. This design enables the acquisition of both solution-generation and solution-verification capabilities, supporting self-evolution without ground-truth solutions or predefined verification procedures. geometric reward mechanism is developed to model verification strategies in latent geometric space. By encouraging both reliability and diversity, this mechanism prevents consensus collapse and enables stable unsupervised evolution. Comprehensive experiments show that Sci-CoE framework not only improves reasoning accuracy and robustness but also cultivates effective verification behaviors capable of multi-perspective evaluation of scientific questions. 2. Related Work 2.1. Scientific Large Language Models Recent advancements in scientific LLMs span generalist architectures and domain-specific adaptations (Hu et al., 2025; Team et al., 2025; Fallahpour et al., 2025; Tan et al., 2025; Zhang et al., 2024). Intern-S1 utilizes multimodal Mixtureof-Experts (MoE) architecture with Mixture-of-Rewards reinforcement learning to outperform closed-source models in complex scientific tasks (Bai et al., 2025). SciReasoner aligns natural language with heterogeneous scientific representations to establish reasoning foundation (Wang et al., 2025a), while SCI-Verifier introduces unified reasoningaugmented framework for robust equivalence judgment in verification (Wang et al., 2025a). Regarding domainspecific innovations, ChemVLM integrates visual encoders to bridge molecular structures with text (Li et al., 2025). Med-R1 applies reinforcement learning for medical visionlanguage reasoning (Lai et al., 2025), and MindLLM employs subject-agnostic framework to decode fMRI signals directly into text (Qiu et al., 2025). AstroMLab 3 leverages high-quality data curation to enable compact 8B-parameter model to match GPT-4o performance in astronomy (de Haan et al., 2024). 2.2. Self-Evolving Large Language Models Early research in self-evolution LLMs explored the selfplay between generation and verification, particularly within code domains. Sol-Ver introduces self-play framework where models iteratively refine both code implementations and test cases (Lin et al., 2025). CURE leverages reinforcement learning to mutually enhance the LLM coder and unit tester through dynamic interaction (Wang et al., 2025b). Pushing autonomy further, subsequent frameworks learn to generate their own problems and adaptive curricula from scratch or minimal seeds. Absolute Zero demonstrates that reasoning capabilities can emerge purely through reinforced self-play without human priors (Zhao et al., 2025), whereas SERL bootstraps robust policies from limited data via iterative selection (Fang et al., 2025). Scaling this verification paradigm, Loong introduces an agent-environment loop that synthesizes large-scale training data with executable code-based ground truth, enabling Reinforcement Learning with Verifiable Rewards (RLVR) across diverse disciplines (Huang et al., 2025b). R-Zero employs internal consistency as reward signal to facilitate self-evolution in general reasoning domains where external verifiers are absent (Huang et al., 2025a). Addressing the instability of such open-ended exploration, R-Few introduces guided self-play mechanism to explicitly mitigate concept drift and diversity collapse (Yu et al., 2025). Distinct from these approaches, our work explores self-evolution for general scientific reasoning with limited data. We orchestrate the co-evolution of reasoning and rigorous verification strategies to eliminate dependencies on external verifiers while establishing closed-loop reinforcement of scientific logic. 3. Methodology 3.1. Overview We propose Sci-CoE, Scientific Co-Evolving Framework designed to systematically improve scientific reasoning capabilities under minimal supervision, featuring Solver and Verifier. Training proceeds in two stages. In the first anchored learning stage, Sci-CoE leverages small amount of labeled data to establish anchored notions of correctness and verification reliability, providing stable initialization for both roles (Section 3.3). In the second unsupervised co-evolution stage, the framework scales to large unlabeled 2 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Figure 2. The overall pipeline of Sci-CoE data, where Solver and Verifier mutually supervise each other through consensus and geometric reward mechanism, enabling fully unsupervised co-evolution (Section 3.4). 3.2. The Solver-Verifier Co-Evolving Mechanism The key challenge we address is that, in scientific domains, solution generation and answer verification are both difficult and mutually dependent, especially when reliable groundtruth annotations are scarce. Sci-CoE is built upon central insight: robust scientific reasoning emerges from the coevolution of solution generation and verification capability. Instead of treating solving and evaluation as separate components, Sci-CoE trains single model to simultaneously assume two complementary roles: (1) Solver, which generates candidate solutions for scientific questions; (2) Verifier, which generates verification strategies to assess the correctness of the solutions. As illustrated in Figure 2, given scientific question q, the model concurrently generates multiple solutions and multiple verification strategies: tion and outputs binary result: Eval(si, vj) {0, 1} (2) These results form verification matrix {0, 1}N , which serves as the core feedback signal for reinforcement learning. The Solver and Verifier share the same set of model parameters and are jointly optimized using Proximal Policy Optimization (PPO). This framework establishes closed-loop co-evolving process: higher-quality solutions facilitate the learning of more discriminative verification strategies, while stronger verification strategies provide more reliable reward signals for solution generation in turn. 3.3. Anchored Learning with Sparse Supervision We initiate the training process using very small subset (1%-10%) of scientific questions annotated with groundtruth answers. The objective of this stage is not to maximize performance under supervision, but to establish stable reference anchors for both problem solving and strategy generation. S(q) = {s1, . . . , sN }, (q) = {v1, . . . , vM } (1) Solver Reward. For questions with ground-truth answers, the Solver is rewarded based on exact correctness: The Solver produces solutions si that contain explicit reasoning steps and final answer, while the Verifier generates natural-language verification strategies vj that evaluate solution correctness from diverse perspectives, such as logical consistency checks, physical constraints, or inverse derivations. Each solutionverification pair (si, vj) is evaluated by an external LLM acting as judging model, which strictly follows the specified verification strategy to assess the solursol = G(si) {0, 1} (3) If generated solution is consistent with the reference answer, it is regarded as correct and receives reward of 1; otherwise, it receives 0. This binary reward formulation encourages the Solver to generate solutions fully consistent with the reference. This supervision does not aim to exhaustively teach domain knowledge, but instead provides 3 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision minimal alignment signal that calibrates the models reasoning trajectories scientifically. Verifier Reward. The goal of the Verifier is to generate verification strategies that are both discriminative and reliable: an optimal verification strategy should pass all correct solutions while rejecting incorrect ones. First, we define the set of correct solutions as: S+(q) = {si S(q) G(si) = 1} (4) If verification strategy passes all correct solutions, it is considered positively aligned. The sign of the verification reward is defined as: (cid:40) sign(vj) = +1, si +(q), Eval(si, vj) = 1, 1, otherwise. (5) The final reward function for verification is: = sign(vj) EsS(q) [1 Eval(s, vj)] rver where S(q) = S(q) S+(q) is the set of incorrect solutions. This formulation assigns positive rewards to verification strategies that pass all correct solutions while rejecting larger proportion of incorrect ones, and penalizes strategies that incorrectly reject correct solutions. (6) Sequential Optimization. Since the Solver and Verifier share parameters, directly optimizing both objectives jointly at this stage may lead to unstable dynamics. We therefore adopt sequential optimization scheme after collecting rollout samples and their corresponding rewards for both solutions and strategies. Specifically, within each PPO iteration, we first update the model parameters using solution data, and then update the same parameters using strategy data. This ensures that the shared model parameters θ alternately integrate feedback signals from solving accuracy and strategy discriminability. By aligning the Solver with ground-truth correctness and training the Verifier to maximally distinguish correct solutions from incorrect ones, this stage establishes stable and reliable foundation for the subsequent unsupervised co-evolving stage. 3.4. Unsupervised Co-evolution via Geometric Consensus. After anchored learning in Stage 1, the model acquires basic capabilities for generating candidate solutions and verification strategies. In Stage 2, we further scale training to large corpus of unlabeled scientific questions, where no groundtruth answers are available. The key challenge in this stage is to provide reliable training signals for both the Solver and the Verifier without relying on external annotations. To address this challenge, we design fully unsupervised co-evolving mechanism that leverages mutual consistency Algorithm 1 Stage 1: Anchored Learning Input: Sparse labeled dataset DGT = {(q, a)}; shared policy πθ; number of solutions ; number of strategies . Initialize: Policy parameters θ. for each training iteration do for each rollout sampled labeled question (q, a) DGT do Generate solutions and verification strategies. Evaluate each (si, vj) using an external Judge Model: Eij = Eval(si, vj) {0, 1}. Compute solver reward using ground-truth: rsol G(si, a). Identify correct set: +(q) = {si rsol Compute verifier reward: = sign(vj) EsS(q) rver end for Sequentially update θ using PPO with solution samples )} and strategy samples {(q, vj, rver {(q, si, rsol (cid:2)1 Eval(s, vj) = 1}. = )}. end for Output: Updated policy πθ. between solutions and verification strategies. The core idea is to replace absolute correctness signals with relative agreement and structural consensus, enabling the Solver and Verifier to supervise each other. Solution Reward via Strategy Consensus. Without reference answers, the Solver may reinforce incorrect reasoning through self-confirmation. To address this, we replace absolute correctness with relative consensus as the learning signal. Specifically, the quality of solution is measured by its pass rate across various verification strategies. The solution reward is defined as: rsol = 1 M (cid:88) j=1 Eval(si, vj) (7) This relative formulation mitigates noise from individual verification errors, encouraging the model to generate solutions that remain consistent across multiple verification perspectives. In practice, we introduce threshold τ (e.g., τ =0.8) and treat solutions whose passing rate exceeds τ as high-consensus solutions. These solutions are collected into the set S+(q). The consistency-based component rcons of the verification reward, is then computed based on this set via Eq.(6). Verification Reward via Geometric Modeling. To prevent the Verifier from maximizing consensus reward by generating homogeneous or trivial verification strategies, we propose reward framework based on geometric modeling, 4 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision evaluating verification strategies along three dimensions: consistency, reliability, and diversity. The consistency reward rcons is derived from the highconsensus solution set S+(q) via Eq.(6), encouraging strategies that accept high-consensus solutions while rejecting others. For the other two, we decouple the verification assessment from generated solutions and instead model the rewards based on the geometric structure of strategies in the latent representation space. Each natural-language verification strategy vj is mapped into high-dimensional semantic vector zj = ϕ(vj) Rd using pretrained embedding model Qwen3-Embedding-8B (Zhang et al., 2025). We then perform K-means clustering over these embeddings {zj}, obtaining clusters {C1, . . . , CK} with corresponding centers {µ1, . . . , µk}. Reliability Reward. Strategies that lie closer to the cluster center are assumed to less likely to hallucinations or topic-drifting, representing more stable and trustworthy verification logic. We therefore define the reliability reward: rrel = 1 dj maxk dk + ϵ (8) where dj = zj µc(j)2 is the Euclidean distance between verification strategy zj and its cluster center, strategies closer to the center receive higher reliability rewards. Diversity Reward. Furthermore, to encourage the coverage of diverse verification perspectives, we introduce diversity reward modeled in polar coordinates. Specifically, we use Principal Component Analysis (PCA) to project the decentralized vectors uj = zj µk into 2D subspace, as uj = (xj, yj) R2 Then compute the polar angle θj = atan2(xj, yj) for each strategy. In the ideal state, strategies should be uniformly distributed around the center from geometric viewpoint. We promote this by rewarding samples with significant angular deviations from others, strategies that are too close to others receive lower rewards: rdiv = 1 Ck 1 (cid:88) j=j (1 cos(θj θj)) (9) The final verification reward is weighted sum of consistency, reliability, and diversity: rver = α rcon + β rrel + γ rdiv (10) In our experiments, we set the clustering coefficient = 1, and α = 1.0, β = 0.5, γ = 0.5. Joint Optimization. Unlike the sequential training in Anchored Learning, this stage employs joint optimization. Specifically, solution samples and strategy samples with their respective rewards are mixed within the same training 5 Algorithm 2 Stage 2: Unsupervised Co-evolution Input: Unlabeled dataset DU = {q}; shared policy πθ; embedding model ϕ(); number of solutions ; number of strategies ; consensus threshold τ . for each training iteration do for each rollout sampled question DU do (cid:80)M j=1 Eij Generate solutions and verification strategies. Evaluate each (si, vj) using an external Judge Model: Eij = Eval(si, vj) {0, 1}. = 1 Compute solution reward: rsol Identify high-consensus solutions: +(q) = {si ci τ }. Perform K-means clustering on {zj = ϕ(vj)} and obtain centers {µk}. Compute reliability reward and diversity reward (see Equation(8) and (9)). Compute final verifier reward: rver βrrel end for θ Update )} {(q, vj, rver {(q, si, rsol using PPO with mixed = αrcon + γrdiv samples + )}. end for Output: Updated policy πθ. batch. Consequently, during each PPO iteration, the model parameters θ receive gradient updates simultaneously from both the Solver and Verifier tasks. This enables the model to dynamically adjust its verification criteria while exploring new solution spaces, allowing solutions and verification strategies to mutually reinforce each other and progressively improve without ground-truth supervision. 4. Experiments 4.1. Experimental Setup Model and Optimization Configuration. We employ Qwen2.5-7B-Instruct and Qwen3-8B (Yang et al., 2025) as the base policy models to concurrently perform Solver and Verifier tasks. To provide high-quality verification feedback signals during training , that is to judge whether Solvers solution passes the Verifiers generated strategy, we utilize Qwen3-235B-A22B (Yang et al., 2025) as the external judging model. Data Construction. We integrate datasets including MegaScience (Fan et al., 2025), Numinamath (Li et al., 2024), ScienceQA (Saikh et al., 2022), and CaseHold (Zheng et al., 2021), covering diverse scientific domains such as mathematics, physics, chemistry and biology. In Anchored Learning Stage, we sample 4k data from MegaScience and Numinamath annotated with ground-truth reference answers. In Unsupervised Co-evolution Stage, we Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Table 1. Main Results on MMLU-Pro. We report the accuracy (%) on MMLU-Pro and its subsets. The best results within each column are highlighted in bold, and underline indicates the second best. Model Overall Bio. Bus. Che. C.S. Eco. Eng. Hea. His. Law Math Phi. Phy. Psy. Oth. Comparable Scale Model Llama-3.1-8B-Instruct Ministral-8B-Instruct Mathstrao-7B-v0.1 Yi-1.5-9B-Chat Mistral-Small-Instruct Qwen2.5-7B-Instruct Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2-18k Sci-CoE-Stage 2-30k Qwen3-8B Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2-18k Sci-CoE-Stage 2-30k 44.25 37.93 42.00 45.95 48.40 57.39 57.68 58.05 58.51 63.19 63.27 63.56 64.34 63.04 49.30 37.63 48.29 55.09 29.72 50.73 42.26 27.25 43.82 44.49 40.26 60.03 44.81 59.00 39.42 26.41 44.88 49.29 23.12 43.28 43.83 25.98 41.15 36.47 29.18 51.63 40.15 63.46 42.08 38.78 45.61 51.78 38.39 39.24 35.96 22.43 47.67 38.28 38.03 52.63 40.91 66.67 54.25 39.49 50.00 60.19 33.23 43.52 40.94 26.61 52.48 40.08 41.42 59.40 44.91 71.69 52.72 36.84 53.66 60.07 30.55 53.79 50.13 31.97 50.85 48.10 40.34 63.91 55. 72.11 64.89 57.16 60.49 68.84 39.94 56.85 48.29 32.52 71.87 49.90 58.35 65.91 54.33 74.34 67.17 56.89 60.73 68.72 40.04 56.60 47.77 33.33 72.09 48.50 59.05 65.54 53.90 73.50 68.69 57.16 61.22 68.01 40.04 58.07 49.08 32.61 72.54 50.50 58.97 66.67 54.65 73.92 68.19 55.39 61.71 70.62 42.31 58.19 48.82 34.06 72.76 50.50 59.35 67.17 54.87 78.80 69.71 68.02 66.10 72.27 53.04 62.47 51.97 31.52 78.53 51.50 67.67 69.30 55.95 78.94 69.20 66.78 65.85 72.63 53.77 63.08 50.92 32.61 78.09 52.51 68.44 69.42 55.41 79.22 70.85 68.02 66.34 72.87 53.35 62.71 51.44 32.52 79.42 52.91 67.74 69.17 55.19 80.20 70.72 68.20 68.05 73.93 54.59 63.33 54.07 33.42 79.79 53.51 68.36 70.30 56.06 further construct two unlabeled training sets of different scales of 18k and 30k to test scalability. Detailed dataset compositions and statistics for each training stage are provided in the Appendix A.1. Benchmarks. To test scientific reasoning ability, we evaluate our approach on the following benchmarks: MMLU-Pro (Wang et al., 2024), GPQA-Diamond (Rein et al., 2024), and UGPhysics (Xu et al., 2025). These benchmarks collectively cover wide range of scientific disciplines with challenging tasks, offering stricter evaluation of complex reasoning abilities. For evaluation, we strictly employ the official evaluation scripts provided for each dataset to ensure accuracy and comparability. 4.2. Main Results Performance on Scientific Reasoning Benchmarks. As shown in Table 1, Table 2 and Table 3, Sci-CoE consistently outperforms the corresponding base models on both general and domain-specific reasoning benchmarks. To highlight, our framework Sci-CoE with the Qwen3-8B outperforms the baseline by 4.04% on GPQA-Diamond dataset, raising the accuracy from 36.87 to 40.91. On the larger and broader MMLU-Pro benchmark, Sci-CoE also achieves 1.15% improvement, from 63.19 to 64.34, demonstrating the general applicability of the learned reasoning and verification capabilities across scientific domains. For UGPhysics which focuses on undergraduate-level physics reasoning, Sci-CoE does not exhibit monotonic improvements across all sub-disciplines, likely due to domain distribution mismatches between training data and specific physics topics. Nevertheless, Sci-CoE consistently Figure 3. Performance of the model at different stages of the training process on GPQA-D evaluation data, the three broken lines represent the average accuracy of the generated solutions, the average accuracy of the generated validation strategies and Best-of-N(BoN) accuracy, using 16 generated solutions and 16 generated strategies. The baseline is Qwen2.5-7B-Instruct, and the rest of the model names represent the number of training steps in that stage. improves the overall average accuracy, achieving increases of 1.97% and 1.34% repectively on the 7B and 8B base model. This suggests that Sci-CoE primarily learns general reasoning and verification patterns, rather than overfitting to specific subfields, enabling stable performance gains even when domain-specific supervision is extremely sparse. We compare Sci-CoE with several baselines of comparable model scale, including Llama-3.1-8B-Instruct (Dubey et al., 2024), Ministral-8B-Instruct-2410 (MistralAI, 2024), Mathstral-7B-v0.1 (Mistral, 2023), Yi-1.5-9B-Chat (Young et al., 2024) and Mistral-Small-Instruct-2409 (Jiang et al., 2023). As shown in in Table 2, Sci-CoE consistently outperforms all same-scale baselines on both the general reasoning benchmark MMLU-Pro and the domain-specific benchmark UGPhysics. Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Table 2. Main Results on UGPhysics. We report the accuracy (%) on the English subset of UGPhysics. The best results within each column are highlighted in bold, and underline indicates the second best. In case of ties, all tied results are marked. Mec., Elec. and Modern stand for Mechanics & Thermodynamics, Electromagnetism, and Modern Physics subsets of UGPhysics. Model Data Scale Overall Acc Mec. and Ther. Elec. Modern Physics Comparable Scale Model Llama-3.1-8B-Instruct Ministral-8B-Instruct-2410 Mathstral-7B-v0.1 Yi-1.5-9B-Chat Mistral-Small-InstructQwen2.5-7B-Instruct Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2 Sci-CoE-Stage 2 Qwen3-8B Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2 Sci-CoE-Stage 2 4k 18k 30k 4k 18k 30k 14.66 16.39 17.45 17.61 25.72 20.67 21.07 21.92 22.64 31.76 32.03 32.46 33.10 12.64 13.95 14.82 16.00 22.71 18.88 20.14 20.92 21. 30.73 30.25 30.21 30.51 14.35 15.52 17.77 15.85 22.70 18.52 19.81 21.31 23.13 29.98 30.62 33.30 34.80 16.80 19.20 19.94 19.94 29.97 23.34 22.51 23.17 24. 33.51 34.38 34.38 34.99 Table 3. Main Results on GPQA-Diamond. We report the accuracy (%) on GPQA-Diamond and its subsets. The best results within each column are highlighted in bold, and underline indicates the second best. Model Data Scale Overall Acc Physics Chemistry Biology Qwen2.5-7B-Instruct Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2 Sci-CoE-Stage 2 Qwen3-8B Base Model Sci-CoE-Stage 1 Sci-CoE-Stage 2 Sci-CoE-Stage 2 4k 18k 30k 4k 18k 30k 30.81 31.31 33.33 35.35 36.87 37.88 38.89 40.91 33.73 34.88 41.86 41.86 39.53 45.35 41.86 43.02 24.73 24.73 23.66 26. 33.33 29.03 33.33 35.48 47.37 47.37 42.11 47.37 42.11 47.37 52.63 57.89 Scalability on Unlabeled Data. After introducing largescale unlabeled data, Sci-CoE demonstrates strong scalability. As the scale of unlabeled data in Stage 2 increases from 18k to 30k, we observe continuous improvements in reasoning accuracy, without evident performance saturation. This indicates that increasing the diversity and quantity of unlabeled scientific problems enables Sci-CoE to discover more robust reasoning patterns. Crucially, our framework effectively bypasses the performance plateau commonly encountered in self-training, maintaining strong Scaling Law in the absence of ground-truth supervision. Evolutionary Trends and Co-evolving Iterations. The performance improvements achieved by Sci-CoE are progressive, as illustrated in Figure 3, reflecting stable and promising co-evolutionary process. The final model substantially outperforms the baseline in solution, verification strategy, and Best-of-N (BoN) performance. In Stage 1, the rapid improvement in verification strategy accuracy equips the model with initial evaluation capabilities for reasoning processes, providing higher-quality feedback for the Solver in subsequent unsupervised co-evolution. Furthermore, the continuous improvement in BoN accuracy underscores the practical value of our designed Verifier in inference-time. Sci-CoE not only enhances the ability of generated candidate solutions but also constructs reliable internal reward signal, allowing the model to accurately identify correct reasoning trajectories among multiple candidates during inference. 4.3. Ablation Study and Analysis Impact of Anchored Learning. We investigated the necessity of the Anchored Learning by skipping Stage 1 and training directly on Stage 2 using unlabeled data. As shown in Table 4 Index 1, the model significantly lags behind Sci-CoE. In several benchmarks, it even underperforms the baseline. This confirms that although Stage 1 uses relatively small amount of data, it provides essential training 7 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Figure 4. Visualization of geometric reward and quantitative analysis of verification strategies. (a)-(d) display PCA projections of strategy embeddings in polar coordinate system across different training stages, respectively corresponding to Baseline Model, Stage 1 only, Stage 2 with Naive Consensus Reward, and Stage 2 with Geometric Reward. The angular distribution of points indicates diversity, while the radial distance to the cluster center represents strategy reliability, with closer points indicating more stable. And the color represents the consistency score, with closer to green indicating higher scores. (e)-(g) illustrate mean consistency, reliability, and diversity reward scores of different models. Table 4. Ablation study on Scientific Reasoning benchmarks. Index Baseline 1 2 3 4 Anchored Learning Stage1 Data Scale Geometric Reward GPQA-D MMLU-Pro UGPhysics Avg Acc 0.4k 4k 4k 36.87 35.86 37.37 37.88 38.89 63.19 63.00 63.36 63.27 63.53 31.76 31.61 31.79 32.07 32.46 43.94 43.49 44.17 44.41 44.96 anchors that help the model establish an initial notion of correctness and verification reliability. Notably, in Stage 1, utilizing only 0.4k annotated samples, the model achieves overall performance improvement (Index 2). This confirms that minimal set of high-quality anchor data is sufficient to successfully bootstrap the fundamental capabilities of both the Solver and Verifier, establishing solid foundation for subsequent evolution. Furthermore, comparison between Stage 1-0.4k and Stage 1-4k reveals that while more data yields better starting point, the system can be successfully bootstrapped with as few as 0.4k samples, demonstrating the frameworks robustness. Effectiveness of Geometric Reward. We compared the efficacy of the Naive Consensus Reward against our proposed Geometric Reward during Stage 2. As shown in Table 4 Index 3-4, the model utilizing the Geometric Reward significantly outperforms the Raw Reward version across all benchmarks. To provide more intuitive explanation, we visualize the verification strategies by projecting their embedding vectors into 2D space using PCA and representing them in polar coordinates as Figure 4 (a-d). The strategy points of the baseline model are mostly close to red in color, indicating low consensus scores. Meanwhile, they are unevenly distributed and located far from the cluster center, suggesting that the generated strategies are neither reliable nor diverse. Compared to the baseline, the strategy points after Stage 1 exhibit an overall improvement in consensus. However, their angular distribution remains concentrated, indicating limited diversity. For Stage 2 with the Naive Reward, most strategy points are close to green and form several highly dense clusters that cover only small angular range. This 8 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision reflects that the model repeatedly generates homogenized and overly simplistic strategies (e.g., simple format checks) to maximize consensus scores, which leads to significant loss of diversity. In contrast, under the Geometric Reward, strategy points are uniformly distributed along the polar angle while maintaining high reliability (i.e., smaller radial distances). This demonstrates that the geometric reward successfully encourages the Verifier to explore orthogonal verification perspectives, thereby constructing more robust evaluation system. Quantitative Dynamics of Reward Components. Beyond qualitative visualization, we analyze the quantitative metrics in Figure 4(e-g). The Naive Reward model achieves high consistency (rcon) but at the severe cost of diversity (rdiv). In contrast, the Geometric Reward mechanism achieves high level of diversity and reliability while also maintaining decent consistency score. This reveals that our geometric reward acts as structural regularizer. By penalizing angular redundancy in the latent space, it prevents the model from falling into local optima where the Solver and Verifier prefer simple reasoning trajectories. The balanced improvement across consistency, reliability, and diversity is the key driver behind Sci-CoEs superior generalization on complex scientific questions. 5. Conclusion We present Sci-CoE, scientific co-evolving framework that improves LLMs scientific reasoning under minimal supervision. key insight is that verification strategies form structured and learnable space, whose reliability and diversity can be encouraged through geometric modeling. Experimental results demonstrate that Sci-CoE improves reasoning accuracy and robustness, and scales effectively to large unlabeled data. We acknowledge the following limitations. Due to limited budget, we only trained models with up to eight-billion parameters. Additionally, Sci-CoE currently relies on an external judging model to execute verification strategies, which introduces additional computational cost and potential bias. We believe Sci-CoE represents meaningful step toward self-evolving scientific reasoning systems and opens new directions for learning reliable reasoning without supervision."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, L., Cai, Z., Cao, Y., Cao, M., Cao, W., Chen, C., Chen, H., Chen, K., Chen, P., Chen, Y., et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. de Haan, T., Ting, Y.-S., Ghosal, T., Nguyen, T. D., Accomazzi, A., Wells, A., Ramachandra, N., Pan, R., and Sun, Z. Astromlab 3: achieving gpt-4o level performance in astronomy with specialized 8b-parameter large language model. arXiv preprint arXiv:2411.09012, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Fallahpour, A., Magnuson, A., Gupta, P., Ma, S., Naimer, J., Shah, A., Duan, H., Ibrahim, O., Goodarzi, H., Maddison, C. J., et al. Bioreason: Incentivizing multimodal biological reasoning within dna-llm model. arXiv preprint arXiv:2505.23579, 2025. Fan, R.-Z., Wang, Z., and Liu, P. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Fang, W., Liu, S., Zhou, Y., Zhang, K., Zheng, T., Chen, K., Song, M., and Tao, D. Serl: Self-play reinforcement learning for large language models with limited data. arXiv preprint arXiv:2505.20347, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hu, M., Ma, C., Li, W., Xu, W., Wu, J., Hu, J., Li, T., Zhuang, G., Liu, J., Lu, Y., et al. survey of scientific large language models: From data foundations to agent frontiers. arXiv preprint arXiv:2508.21148, 2025. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Selfevolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025a. Huang, X., Franke, G., Yang, Z., Bai, J., Bai, W., Bi, J., Ding, Z., Duan, Y., Fan, C., Fan, W., et al. Loong: Synthesize long chain-of-thoughts at scale through verifiers. arXiv preprint arXiv:2509.03059, 2025b. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Lai, Y., Zhong, J., Li, M., Zhao, S., Li, Y., Psounis, K., and Yang, X. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Li, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou, C., Liu, W., Yang, Y., Xiong, X., et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 415423, 2025. Lin, Z., Shen, S., Shang, J., Weston, J., and Nie, Y. Learning to solve and verify: self-play framework for code and test generation. arXiv preprint arXiv:2502.14948, 2025. Mistral. Mathstral. https://mistral.ai/news/ mathstral/, 2023. Accessed: 2024-09-23. MistralAI. Ministral model card, 2024. URL https://huggingface.co/mistralai/ Ministral-8B-Instruct-2410. Qiu, W., Huang, Z., Hu, H., Feng, A., Yan, Y., and Ying, R. Mindllm: subject-agnostic and versatile model for fmri-to-text decoding. arXiv preprint arXiv:2502.15786, 2025. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Saikh, T., Ghosal, T., Mittal, A., Ekbal, A., and Bhattacharyya, P. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. Tan, Q., Zhou, D., Xia, P., Liu, W., Ouyang, W., Bai, L., Li, Y., and Fu, T. Chemmllm: Chemical multimodal large language model. arXiv preprint arXiv:2505.16326, 2025. Team, N., Zhang, B., Feng, S., Yan, X., Yuan, J., Yu, Z., He, X., Huang, S., Hou, S., Nie, Z., et al. Novelseek: When agent becomes the scientistbuilding closed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938, 2025. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Wang, Y., Tang, C., Deng, H., Xiao, J., Liu, J., Wu, J., Yao, J., Li, P., Su, E., Wang, L., et al. Scireasoner: Laying the scientific reasoning ground across disciplines. arXiv preprint arXiv:2509.21320, 2025a. Wang, Y., Yang, L., Tian, Y., Shen, K., and Wang, M. Coevolving llm coder and unit tester via reinforcement learning. arXiv preprint arXiv:2506.03136, 2025b. Xu, X., Xu, Q., Xiao, T., Chen, T., Yan, Y., Zhang, J., Diao, S., Yang, C., and Wang, Y. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Wang, G., Li, H., Zhu, J., Chen, J., et al. Yi: arXiv preprint Open foundation models by 01. ai. arXiv:2403.04652, 2024. Yu, W., Liang, Z., Huang, C., Panaganti, K., Fang, T., Mi, H., and Yu, D. Guided self-evolving llms with minimal human supervision. arXiv preprint arXiv:2512.02472, 2025. Zhang, D., Liu, W., Tan, Q., Chen, J., Yan, H., Yan, Y., Li, J., Huang, W., Yue, X., Ouyang, W., et al. Chemllm: chemical large language model. arXiv preprint arXiv:2402.06852, 2024. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Zheng, L., Guha, N., Anderson, B. R., Henderson, P., and Ho, D. E. When does pretraining help? assessing selfsupervised learning for law and the casehold dataset of 53,000+ legal holdings. In Proceedings of the eighteenth international conference on artificial intelligence and law, pp. 159168, 2021. 10 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision A. Appendix A.1. Training Data Table 5. Training Data Composition of Different Scales. The third column, Disciplines, represents the subset composition of disciplines in MegaScience. Scale MegaScience Disciplines NuminaMath Other 4k 18k 30k 3k 13k 23k Phy 1k, Bio 1k, Chem 1k Phy 5k, Bio 2k, Chem 2k, Med 1k, Math 1k, CS 1k, Eco 1k Phy 5k, Bio 4k, Chem 4k, Med 4k, Math 2k, CS 2k, Eco 2k 1k 5k 5k ScienceQA 1k, CaseHold 1k A.2. Experiment Details At each sampling step during reinforcement learning, we generate rollouts for solutions and verification strategies using vLLM (Kwon et al., 2023) . Training Stages. Sci-CoE is trained in two stages: Anchored Learning: trained for 300 optimization steps using sparse labeled data. Unsupervised Co-evolution: 18k-scale data setting: trained for 300 optimization steps. 30k-scale data setting: trained for 500 optimization steps. Optimization. We adopt Proximal Policy Optimization (PPO) for joint SolverVerifier training with the following settings: Optimizer learning rate: 1 106 PPO updates per step: 1 Training epochs per update: 1 KL regularization enabled with coefficient 0.01 KL estimator: K3 estimator Sampling Configuration. At each optimization step, we sample scientific questions and generate multiple Solver and Verifier trajectories: Number of sampled questions per step: 100 Number of Solver rollouts per question: 10 Number of Verifier rollouts per question: 10 Sampling temperature: 1.0 A.3. Prompt This is the prompt for solution generation: Solver Prompt You are helpful assistant help user solve problems. Please reason step by step, and put your final answer within boxed{}. 11 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision This is the problem you need to solve:{{problem}} This is the prompt for verification strategy generation:"
        },
        {
            "title": "Verifier Prompt",
            "content": "You are an intelligent assistant specialized in designing an effective verification strategy for various scientific problems. Given problem, your task is NOT to solve the problem yourself or provide the final answer, but to generate ONE high-level verification strategy to check the correctness and quality of the provided solution. This is the problem:{{problem}} The strategy should aim to be: 1. Specific: Clearly define the input and expected output for one test scenario. 2. Actionable: Clearly describe how to perform the verification. 3. Discriminating: Capable of identifying subtle errors or confirming robust correctness. Before providing the strategy, you MUST think step-by-step about why the strategy is useful and how it can reveal potential flaws or confirm correctness. Finally, after generating the strategy and thinking thoroughly, you MUST output the strategy in the following format: **Strategy Type:**n```(strategy type here)```nn**Strategy Description:**nn(A detailed, natural language description of the strategy design here.)n The structure of the strategy requires the following: Consider reverse calculations, alternative solution methods, step-by-step logical checks, simplification, or specific mathematical property validations, etc. Think about checking final answer, checking units, applying fundamental laws, verifying against known principles, or consistency with expected experimental outcomes, etc. The plan should describe the logic for checking the solution, for example: 1. How to parse the input solution content. 2. What specific property, calculation, or logic to check./What specific theorem is used... 3. What the expected outcome of the check is. - The strategy type examples: boundary_test/core_functionality_test/answer_check/re verse_calculation/step_check/unit_check/property_validation/... Crucially, your description must be in natural language only. DO NOT include any Python code. This is the prompt for ground-truth test judgment: Ground-truth Test Prompt You are teacher specialized in evaluating solutions for scientific problems. need you to judge whether the student's answer is correct given the ground truth answer. This is the problem:{{problem}} This is the reference correct solution of this problem: {{reference_solution}} This is generated answer of this problem: {{solution}} Your task is to assess whether the student's answer captures the same meaning as the reference answer, even when expressed with different wording or format. Your tasks include: A. Identify Mathematical or Notational Equivalence: Pay special attention to any LaTeX expressions in both answers. Confirm that the mathematical relationships, variables, and operations conveyed are equivalent. 12 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision B. Consider Physiccal Equivalence: Pay special attention to transferring the units of both answers and equivalent variables given in the problem description. Feel free to ignore some physical constants appropriately. **Output ONLY \"True\" if the solution is correct, otherwise output \"False\".** Do not include any other text or explanation. This is the prompt for solution-strategy judgment:"
        },
        {
            "title": "Juding Model Prompt",
            "content": "You are an expert reviewer specialized in evaluating solutions for scientific problems. This is the problem: {{problem}} This is generated solution of this problem: {{solution}} This is verification strategy design for this problem: strategy type: {{strategy_type}} strategy description: {{strategy_details}} Your task is to carefully review the solution based on the provided verification strategy and determine if the solution is correct. You must follow the steps or criteria of the strategy, instead of verifier the solution by yourself. Output ONLY \"True\" if the solution is correct according to the verification strategy, otherwise output \"False\". Do not include any other text or explanation. A.4. Case Study We show examples for Sci-CoE-7B as follows: Scientific Problem benzene-toluene mixture with 40 mole% benzene is to be separated in distillation column to produce top product with 90 mole% benzene and bottom product with 10 mole% benzene. The feed enters at its boiling point, and the column operates with reflux ratio of 3 kmol/kmol product. Determine the number of theoretical plates required and the feed plate location. Generated verification strategy 1 Strategy type: Step Check Strategy description: To effectively verify the correctness of the solution for the benzene-toluene distillation column problem, we break down the solution into several key steps. Here is detailed approach: 1. HOW TO PARSE THE INPUT SOLUTION CONTENT: Verify input details: mole fractions in feed (zF = 0.40), top product (xD = 0.90), bottom product (xW 0.10), and reflux ratio (R = 3). 2. WHAT SPECIFIC PROPERTY, CALCULATION, OR LOGIC TO CHECK: Step 1: Feed Composition: Verify the benzene mole fraction is 0.40. Step 2: Product Specifications: Ensure xD = 0.90 and xW 0.10. Step 3: Reflux Ratio: Confirm = 3 kmol/kmol product. Step 4: Number of Theoretical Plates: Use the Fenske-Underwood-Gilliland (FUG) method or McCabe-Thiele plot to verify the count. Step 5: Feed Plate Location: Verify consistency using the Kirkbride equation or graphical McCabe-Thiele analysis. 3. WHAT THE EXPECTED OUTCOME OF THE CHECK IS: Expected Mole Fractions: 0.90 (Top), 0.10 (Bottom). Expected Reflux Ratio: = 3. 13 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Expected Number of Plates: Should fall within the calculated range for the given specifications. Expected Feed Plate Location: Must optimize the balance between vapor and liquid flow rates. This detailed step-check strategy ensures that all aspects of the distillation column design are thoroughly verified against fundamental principles. Generated verification strategy 2 Strategy type: Property Validation Strategy description: To validate the correctness of the solution for the benzene-toluene separation problem, we will perform property validation check focusing on the theoretical plate calculations and feed plate location. This strategy aims to ensure that the solution adheres to the principles of distillation tower design and separation throughput. 1. HOW TO PARSE THE INPUT SOLUTION CONTENT: The problem states that the feed contains 40 mole% benzene, and the column operates with reflux ratio of 3 kmol/kmol product. The top product should have 90 mole% benzene, and the bottom product should have 10 mole% benzene. The feed enters at its boiling point. 2. WHAT SPECIFIC PROPERTY, CALCULATION, OR LOGIC TO CHECK: Verify the theoretical plate calculations by checking the number of theoretical plates required. Ensure the feed plate location meets the given operating conditions for the distillation column. 3. EXPECTED OUTCOME OF THE CHECK: The number of theoretical plates required should be consistent with the McCabe-Thiele method or balance equations. The location of the feed plate should be consistent with the feed rate and the composition of the feed. Step 1. Determine the Number of Theoretical Plates: Use the McCabe-Thiele method or more advanced simulation tool to calculate the number of theoretical plates required. Check if the calculated number of plates matches the expected value based on the given reflux ratio and specified product compositions. Ensure that the top product has 90 mole% benzene and the bottom product has 10 mole% benzene. Step 2. Verify the Feed Plate Location: Identify the feed plate by checking how the feed composition changes as it is distributed throughout the column. Ensure that the feed plate is positioned such that the top product is rich in benzene and the bottom product is rich in toluene. Verify that the feed plate location matches the feed composition of 40 mole% benzene. Step 3. Boundary Test: Double-check the case where the feed percentage of benzene is at the lower limit (40 mole%). Verify that the column operates correctly with the specified reflux ratio and product specifications. Step 4. Cross-Check with Alternative Methods: Use different thermodynamic model or software (e.g., Aspen Plus, Distemod) to verify the theoretical plates and feed plate location. Compare the results from the alternative method with the solution provided to ensure consistency. By following this strategy, any potential flaws or discrepancies in the solution can be identified, ensuring that the final answer is robust and correct."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
    ]
}