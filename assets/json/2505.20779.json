{
    "paper_title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
    "authors": [
        "Noy Sternlicht",
        "Tom Hope"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https://github.com/noy-sternlicht/CHIMERA-KB"
        },
        {
            "title": "Start",
            "content": "CHIMERA: Knowledge Base of Idea Recombination in Scientific Literature Noy Sternlicht1 and Tom Hope1,2 1School of Computer Science and Engineering, The Hebrew University of Jerusalem 2The Allen Institute for AI (AI2) https://noy-sternlicht.github.io/CHIMERA-Web 5 2 0 2 8 2 ] . [ 2 9 7 7 0 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A hallmark of human innovation is the process of recombinationcreating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present novel information extraction task of extracting recombination from scientific paper abstracts, collect high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to large corpus of papers in the AI domain, yielding KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https:// github.com/noy-sternlicht/CHIMERA-KB"
        },
        {
            "title": "Introduction",
            "content": "Recombinationcreating original conceptual and physical combinations of existing mechanisms, methods, perspectives and approaches to address problemsis common form of ideation (Uzzi et al., 2013; Youn et al., 2015; Shi and Evans, 2023). Recombination involves re-representing past ideas by decomposing them into conceptual chunks and then blending them into new solutions (Knoblich et al., 1999; McCaffrey, 2012), and also often involves forming abstract structural connections across domains (Gentner et al., 1997; Gentner and Markman, 1997; Gentner and Kurtz, 2005; Figure 1: We automatically extract thousands of recombination examples describing how scientists connect ideas in novel ways. Chan et al., 2011; Frich et al., 2019)for example, nature-inspired optimization algorithms that recombine concepts from nature and optimization. In this work, we automatically mine CHIMERA, knowledge base of recombination examples from across the scientific literature. We focus on two recombination types, which we name blends and inspirations. Blends combine multiple concepts to create new approaches (e.g., boosting classical machine learning algorithms using quantum computing), while inspirations involve adapting ideas from existing concepts to spark insight (e.g., applying bird flock behavior to coordinate drone swarms). CHIMERA includes examples of blends of concepts within and across domains, and also inspirations in the form of analogies, reductions, and abstractions. Unlike simpler concept co-occurrence approaches (Krenn et al., 2022) or more generic scientific extraction schema (Luan et al., 2018), Figure 2: 1) We start by collecting human-annotated recombination examples and use them to finetune an LLM for information extraction. 2) Next, we apply the fine-tuned LLM on the arXiv corpus and build knowledge base of recombination examples. 3) Given context string and query concerning the recombination of certain graph node, our recombination model suggests directions based on knowledge learned from the KB. CHIMERA contains examples in which the authors explicitly mention recombination as one of the core contributions of their work. Figure 1 presents recombination example automatically extracted using our methods. Our methods and collected data have broad potential applications in the field of Science of Science (Shi and Evans, 2019). The collected data enables empirical studies of innovation in novel waysfor example, examining how fields draw inspiration from one another and investigating how the blending of mechanisms across different domains emerges and evolves. Using our knowledge base, we present an analysis of sources of inspiration in the AI and NLP communities. We also demonstrate how to use this data to build supervised learning framework for recombinant ideation. While existing work in the human-computer interaction (HCI) community provides researchers with tools for exploring idea recombinations (Radensky et al., 2024a; Kang et al., 2022), CHIMERA allows us to take different approach: training supervised models that learn from past examples how to recombine concepts for generating new scientific ideas. Figure 2 presents an overview of this process."
        },
        {
            "title": "2 Related Work",
            "content": "Recombinant creativity Concept blending and finding inspiration through analogies are key way to create new ideas (McKeown, 2014; 201, 2019; Holyoak and Thagard, 1994). Recent research explores how idea recombination can enhance LLM-powered ideation tools. For example, CreativeConnect (Choi et al., 2023) enables users to recombine keywords to generate graphic sketches, while Luminate (Suh et al., 2023) facilitates the recombination of dimensional values to produce diverse LLM responses. Scideator (Radensky et al., 2024b) is another recent work that allows researchers to explore new ideas by interactively recombining scientific concepts. Researchers have also investigated combining elements from input and analogous artifacts to create new ideas (Srinivasan and Chan, 2024; Chilton et al., 2019). In our work, we aim to extract knowledge base Recombination extraction examples Abstract: \"...Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose learning-based approach to integrate advanced deep learning techniques and archaeological knowledge...\" Blend: \"advanced deep learning techniques\" \"archaeological knowledge\" Abstract: \"...Traditional approaches to enhance dialogue planning in LLMs, ... either face efficiency issues or deliver suboptimal performance. Inspired by the dual-process theory in psychology, which identifies two distinct modes of thinking - intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework...\" Inspiration: \"the dual-process theory in psychology\" \"enhance dialogue planning in LLMs\". Table 1: Example blend and inspiration. Note that blend is symmetric relation, while inspiration is not. of real recombinations in scientific papers, which can be used to facilitate research on recombination. For example, we train models that learn from past examples of recombination how to predict new recombinations (Figure 2). Scientific information extraction Much prior work has focused on information extraction (IE) from scientific papers. notable example is the SciERC dataset (Luan et al., 2018), which includes annotations of scientific entities (e.g., methods, tasks, metrics) and relations (e.g., conjunction) for 500 abstracts. Other, more recent examples are SciREX (Jain et al., 2020), and SciDMTAL (Pan et al., 2024), introducing document-level scientific IE datasets covering similar entity types. However, existing extraction approaches do not focus on recombination relationships, as we demonstrate in Appendix L, Figure 18. In our work we design simple information extraction schema that is focused on idea recombination, to enable exploration of this important form of scientific innovation. For example, as part of our knowledge base we are able to extract many examples of analogical inspirations used by the AI community (Figure 1), which was not possible using existing scientific IE schema."
        },
        {
            "title": "3.1 Problem Definition",
            "content": "We focus on cases where paper abstracts clearly say that contributed idea is rooted in an original combination of concepts or explicitly mention source of inspiration. As discussed in the introduction, we define two coarse-grained relation types to capture the notion of recombination: blend and inspiration. Blend refers to joining multiple concepts (e.g., methods, models, theories). Note that we use the terms concept blend and concept combination interchangeably. Inspiration refers to using knowledge from source entity and implementing it in different target entity. This could involve, for example, using an analogy or an abstraction as source of inspiration, or more generally being influenced by another line of work. Relations are defined between free-form spans of text representing scientific concepts (see Figure 1, and additional examples in Table 1). We refer to the entities in blend relation as combination-elements and those in an inspiration relation as inspiration-source and inspiration-target."
        },
        {
            "title": "3.2 Recombination Mining",
            "content": "Our approach to mining recombination examples begins with building curated dataset of annotated examples. We then use this dataset to train an information extraction model. Finally, we apply the trained model to collect recombination examples at scale. This process is illustrated in Figure 2. Data sourcing We use AI-related papers from the unarXive corpus (Saier and Färber, 2020) as source of annotation examples1. The data undergo an initial keyword-based filtering to identify works that are more likely to specify idea recombination. Table 7 in Appendix lists the keywords used in this process. We then assign the remaining abstracts to annotators. 1We focus on the following arXiv categories: cs.AI, cs.CL, cs.CV, cs.CY, cs.HC, cs.IR, cs.LG, cs.RO, cs.SI Example type # Train # Test # Total Category # Interdisciplinary # Total blend inspiration not-present All 124 45 195 364 76 24 116 216 100 69 311 Inspiration Edges Blend Edges Edges (total) Nodes (total) 5,182 (54.1%) 1,792 (9.6%) 6,974 (24.8%) n/a 9,578 18,586 28,164 43, Table 2: Human-annotated corpus. We use examples with and without recombinations (\"not-present\"), simulating extraction over real-world data. Table 3: The CHIMERA knowledge base summary. Our knowledge base encompasses over 28K recombination examples, quarter of which are interdisciplinary. Annotation process After thorough screening session, we recruit two annotators with scientific PhDs from Upwork2. The screening includes annotating examples following detailed guidelines document3. Annotators who passed the screening had personal one-hour training session to discuss additional examples. The annotators conduct the annotation through LightTag (Perry, 2021), web-based text annotation platform (no longer operational). We collect total of 580 annotated abstracts in this manner, as presented in Table 2. To ensure annotation quality, we assign 10% of the examples to both annotators and review the shared section at the end of each batch. During the review, we discuss disagreements and provide the annotators with feedback, after which they revise their work. The annotations undergo an additional review by an NLP expert. The expert verifies correctness, adjusts span boundaries, and merges annotations from different annotators. Automatic recombination mining We next use the collected data to fine-tune an LLM-based extraction model. Given the text and annotation schema, we instruct the model to extract the most salient recombination from the text, if one exists. The model must determine whether the text discusses recombination, infer its type, and identify entities in single query. We devise the test set from examples where at least two annotators (out of three) agree on the recombination type (or lack of recombination). This provides high-quality test data with fewer ambiguities. Table 2 presents the distribution of example types in both the train and test sets. Additional implementation details related to the extraction baselines are discussed in Appendix B. 2https://www.upwork.com/ 3https://tinyurl.com/4mfdrx2f 3.3 The CHIMERA Knowledge Base This section describes building the CHIMERA knowledge base. We first use our extraction method to mine recombination examples, categorize them, and build KB in which nodes represent scientific concepts, and edges represent recombination relations between them. Large-scale mining We source abstracts from the arXiv dataset4. This dataset updates monthly and includes newer examples than unarXive (Saier and Färber, 2020). We apply our fine-tuned extraction model over publications from 2019 to 2024 within the same CS categories used for the annotation task. After applying the model we filter predictions that fail to comply with the data schema or could not be properly parsed. Categorization In addition to extracting the relations, we apply GPT-4o to identify the scientific domain of each extracted entity given the abstract. This enables analyses we perform in Section 4.2. The model is instructed to select the most appropriate arXiv category from either the arXiv taxonomy list or supplementary list of non-arXiv domains (e.g., \"psychology\"). In cases where no given label captures the entitys scientific taxonomy, we classify the nodes domain as Other. The analysis in Section 4.2 omits nodes from this domain, as we find they might be too noisy, too general or miscellaneous. Examples of such nodes, along with the used prompts and additional technical information about this step, are available in Appendix D. We further assign the graph nodes higher-level scientific discipline. The discipline is either the arXiv group name if available (\"computer-science\" for cs.AI), or the relevant non-arXiv domain. KB building We normalize the knowledge base entities by clustering semantically similar ones, and 4https://tinyurl.com/mrzksbky Task Baseline F1 Abstract classification: Does it discusses recombination? Human-agreement E2EM istral7BInstructv0.3 E2ELlama3.18BInstruct E2EGoLLIE13B E2EGP 4o Abstract-classifierM istral7BInstructv0.3 Abstract-classifier-CoTM istral7BInstructv0.3 Entity extraction: What are the relevant entities? Relation extraction: What is the recombination? Human-agreement E2EM istral7BInstructv0.3 E2ELlama3.18BInstruct E2EGoLLIE13B E2EGP 4o Entity-extractorGP 4o Entity-extractorSciBERT Entity-extractorP RESciBERT Human-agreement E2EM istral7BInstructv0.3 E2ELlama3.18BInstruct E2EGoLLIE13B E2EICLGP 4o 0.757 0.815 0.630 0.677 0.720 0.622 0.774 0.876 0.587 0.249 0.259 0.138 0.268 0.324 0. 0.805 0.598 0.264 0.301 0.223 0.765 0.762 0.628 0.667 0.580 0.607 0.748 0.591 0.352 0.259 0.187 0.293 0.263 0.248 0.536 0.581 0.366 0.294 0.219 0.385 0.760 0.763 0.620 0.667 0.572 0.602 0.749 0.675 0.440 0.252 0.217 0.217 0.247 0.276 0. 0.651 0.454 0.276 0.253 0.244 Table 4: Recombination extraction results. Bold text signifies the best result, while underlined text signifies the second-best. We observe that surprisingly large and capable models struggle with the extraction tasks. further enrich each edge in the graph with the publication date and arXiv categories of the paper citing it. For simplicity, we focus on binary relations when building the graph. Table 3 reports the summary of the final KB, including the number of interdisciplinary blends and inspirations."
        },
        {
            "title": "4.1 Recombination extraction evaluation",
            "content": "We evaluate the recombination extraction process in three different levels of increasing difficulty: abstract classification (whether the text discusses recombination), entity extraction (what entities are described in the text) and relation extraction (what the discussed relation is). To evaluate abstract classification, we use precision, recall and F1. We use soft evaluation metric for entity and relation extraction tasks, where two entities of the same type match if they refer to semantically similar concept. We utilize GPT-4o-mini as judge of content similarity (Figure 12 in Appendix presents our prompt). We select GPT-4o-mini over GPT-4o after conducting qualitative examination and finding only handful of cases in which the model judgment differs (3 span pairs in the entire test set). To avoid position bias, we run the judge twice for each pair, reversing the span order each time. We consider two spans equivalent only when the judge returns positive answer on both runs. Each predicted entity can match with exactly one gold entity and vice versa, with any additional matches being ignored. Under this definition of soft entity matching, we compute the entity extraction quality of model using standard precision, recall and F1. For recombination relations, we apply the same metrics using partial relation matching: predicted relations contribution to the true-positive score depends on how many entities match with ground-truth relation of the same type. Extraction results Table 4 reports results for different levels of recombination extraction (abstract classification, entity extraction and relation extraction). We experiment with end-to-end (E2E) extraction approaches, inferring whether the text discusses recombination, its type, and what entities are involved - all at once. Note that when handling E2E approaches, we regard any extracted relation as positive abstract classification. We also (a) Frequent inspiration source and target domains. (b) Frequent domains of blend nodes. (c) Common sources of inspiration for leading domains. Figure 3: Recombinations between areas. cs.*, q-bio.nc and math.oc are arXiv categories. Inspirational connections are often cross-domain (Figure 3a), while blends more often stay within-domain (Figure 3b). Figure 3c zooms in on few domains, for example revealing that robotics often draws inspiration from zoology). study models specialized in classification (Abstractclassifiers) or entity extraction (Entity-extractors). Appendix describes implementation details regarding extraction. Human agreement has F1 scores of 0.760, 0.675, and 0.651 for classification, entity extraction, and relation extraction, respectively. These values are comparable with other works measuring soft annotators agreement in complex extraction tasks (Naik et al., 2023; Sharif et al., 2024). Generally, fine-tuning Mistral-7B using our data obtains the best performance in all subtasks. We observe that entity and relation extraction appear more challenging than classification for both humans and SOTA LLMs. However, humans still significantly outperform automatic extraction approaches. We present error analysis in Appendix F. Our findings indicate that focusing on smaller portion of the recombination extraction task is not necessarily easier than performing it end-to-end, as seen in the lower performance of abstract classifiers, and discuss this point further in Appendix C."
        },
        {
            "title": "4.2 Knowledge base analysis",
            "content": "inspirations Figures 3a and 3a Blends vs. present the predominant domain pairs for inspiration and blend relations in CHIMERA (with frequency above the 0.9 quantile). Inspirations display larger selection of domains than blends. We also observe that blends connect the same or similar domains, while inspirations are often between different and further domains. Of note is the volume of inspiration drawn from brain-related sources, such as cognitive science and q-bio.nc. possible explanation might be that many of our arXiv categories of interest are related to machine learning, where the human brain historically serves as general source of inspiration. Table 14 in Appendix presents the same information in tabular format for better readability. Inspiration analysis We next analyze how different fields draw inspiration from each other. Figure 3c shows the top 10% cross-domain inspiration sources for three prevalent domains in the graph: cs.RO (Robotics), cs.CV (Computer ViFigure 2 shows an example of the inputs and outputs of this task: given context string (\"Recent advancements in video generation have struggled to model complex narratives...\") and query regarding the recombination of graph entity (\"What would be good source of inspiration for video generation?\") we aim to answer this question with an additional graph entity (\"The concept of storyboarding...\"). More formally, given context string (e.g., problem, experimental settings, goals), an entity and recombination type τ , we aim to find different graph entity such that (e, τ, e) is an edge in CHIMERA. 5.2 Recombination prediction Data preparation We start by converting edges to pairs of queries and answers. The queries describe the task inputs: single graph node, the edge recombination type, and context string, which we extract from the corresponding abstract using GPT-4o-mini. Note that this process might leak information regarding the answer (the other graph node) into the query. Therefore, we follow it by applying GPT-4o-mini again to the result to identify leakages in the generated queries (see examples and additional implementation details for this step in Appendix I). We discard bad query-answer pairs (approximately 22% of the pairs, mostly due to leaks) and divide the remaining pairs into data splits based on the publication year. Our test set includes all pairs associated with papers published after 2024. Table 5 shows summary of the resulting data splits. Prediction We experiment with zero-shot and finetuned retrievers based on encoders trained before the test set cutoff year (2024). We next explore applying GPT-4o-based reranker (Sun et al., 2023) to the top 20 retrieved results to improve our predictions further. The GPT-4o data cutoff is October 2023, meaning the reranker is also unfamiliar with our test set. Appendix provides additional implementation details for the prediction baselines."
        },
        {
            "title": "5.3 Prediction results",
            "content": "We present our results in Table 6. We observe that fine-tuning greatly helps to improve retrievers, decreasing the median rank of the gold answer by one order of magnitude. The last row of Table 6 shows results for applying RankGPT (Sun et al., 2023) with GPT-4o over the top-20 results of the bestperforming retriever (all-mpnet-base-v2f inetuned). Figure 4: Prevalent domains inspired by cs.CL concepts (NLP). Note the decrease in within-domain inspiration. sion) and cs.CL (Computation and Language). We observe that while some sources of inspiration (like cognitive-science) are commonly shared across related fields, domains may draw inspiration from unique sources (e.g., from zoology to cs.RO). Examples of these interdisciplinary inspirations can be found in Appendix G, Table 13. Figure 4 shows the percentage of target nodes in domains drawing inspiration from cs.CL (Computation and Language) over the past five years. We observe two trends: decrease in intra-domain inspiration (where cs.CL concepts inspire other cs.CL concepts), and an increase in cs.CV (Computer Vision) concepts drawing inspiration from cs.CL."
        },
        {
            "title": "5 Recombination Prediction",
            "content": "This section gives an example of possible use case of the CHIMERA knowledge base. Using this data, we train supervised models that learn how to recombine concepts for predicting new scientific ideas. This process is illustrated in Figure 2."
        },
        {
            "title": "Split",
            "content": "# Inspiration # Blend # Total"
        },
        {
            "title": "Train\nValidation\nTest",
            "content": "5,408 119 2,026 19,909 411 8,591 25,317 530 10,617 Table 5: Prediction data splits. We divide our data by the publication years associated with each query (training and validation sets < 2024, test set 2024) to avoid contamination. Baseline H@3 H@5 H@10 H@50 H@100 MRR MedR 0.033 all-mpnet-base-v2 0.041 bge-large-en-v1.5 0.024 e5-large-v2 0.110 all-mpnet-base-v2f inetuned 0.104 bge-large-en-v1.5f inetuned e5-large-v2f inetuned 0.107 all-mpnet-base-v2f inetuned + RankGPT 0.100 0.042 0.053 0.033 0.135 0.130 0.133 0.130 0.061 0.076 0.050 0.178 0.168 0.173 0.192 0.126 0.151 0.113 0.320 0.306 0.317 0.320 0.170 0.199 0.155 0.402 0.392 0.397 0.402 0.033 0.041 0.026 0.106 0.102 0.103 0. 1305 1135 1590 194 222 212 194 Table 6: Recombination prediction results. MedR stands for \"Median Rank\". Using CHIMERA data to fine-tune the models improves the median rank by factor of 10. Reranking the top-20 answers using RankGPT boosts the H@10 but slightly reduces other metrics (H@3,5 and MRR). While the reranker improves H@10, it degrades H@k for = 3, 5 and MRR values. We hypothesize the reranker might lower the gold answer rank if there are many possible answers. User study We present human evaluation study exploring our ideation approach compared to other baselines. We recruit three volunteers with verifiable research experience (the authors of at least one research paper) and assign them examples based on their area of expertise. The examples are inspiration queries from our test set, along with inspiration source suggestions from different baselines: (1) Ours: our method, including reranking (2) Gold: the gold answer, (3) Random: random test-set node, (4) GPT-4o: suggestion generated by GPT-4o, (5) ZS-CHIMERA: zero-shot prediction model using our test nodes as candidates, and (6) ZS-SciERC: zero-shot prediction model using candidates extracted from test set abstracts with the SciERC (Luan et al., 2018) schema. Note that we use the highest ranked answer (k=1) for baselines returning ranked list of candidates. We request the annotators to rank baseline suggestions based on their helpfulness in inspiring interesting ideas. Figure 5 presents each baselines median and average rank over 70 annotated examples. Since the most helpful suggestions are ranked first, lower median and average rank signifies more helpful baseline. Our approach receives similar median and average rank as the gold answer, and annotators prefer it to all other baselines. This gives as an additional, complementary signal to the automatic evaluation, showing that our recombination prediction approach learns to create helpful recombinations. Appendix presents additional details regarding the task and interface we used to conduct the study. Figure 5: Researchers find our recombination suggestions almost as helpful as the gold answer in inspiring ideas, providing additional verification of our automated evaluation."
        },
        {
            "title": "Conclusions",
            "content": "We automatically mine the scientific literature to create CHIMERA, novel knowledge base spanning over 28K examples of how scientists blend concepts and draw inspiration from different areas. This knowledge base offers broad applications, and we demonstrate how it can be used to empirically study idea recombination across domains and to fine-tune models that predict new recombination directions that researchers find inspiring."
        },
        {
            "title": "Limitations",
            "content": "Extraction quality While our information extraction model improves the quality of mined recombinations, it is still far from being perfect. Our qualitative error analysis shows the extraction model struggles to identify and extract more subtle recombination descriptions, and it still falls significantly short of human performance on the same task. Improving the extraction model remains challenging and interesting direction for future work. Lydia B. Chilton, S. Petridis, and Maneesh Agrawala. 2019. Visiblends: flexible workflow for visual blends. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Recombination prediction evaluation One particularly challenging aspect of the recombination prediction task is the lack of single correct response. Given problem description, there are numerous ways to blend ideas and take inspiration that can lead to novel, recombinant solution. This could lead to many false negatives, resulting in an overly pessimistic estimate of the models. We partially handle this through an additional human evaluation. However, given the high level of expertise required from the participants, the scope and thoroughness of such an evaluation are limited. Experimenting with additional models We use models from the GPT-4o series for evaluation (judging entity span similarity), analysis (identifying entitys scientific domain), and to enrich our data (generating context string for the extracted recombinations). Since this work prioritises the extraction and prediction of recombinations, we only experiment with those models. We leave experimenting with larger range of models for these tasks for future work."
        },
        {
            "title": "Ethics Statement",
            "content": "To collect human-annotated recombination examples, we recruited crowdworkers via Upwork. Annotators were informed of the nature and purpose of the task and were compensated at an hourly rate of $26$30. Additionally, three volunteers participated in our human study. No personal information about the annotators or volunteers is disclosed. To promote transparency and reproducibility, we release our code and model checkpoints. The collected data is shared under an open license to facilitate further research. We used AI assistants for grammatical corrections and code writing (e.g., GitHub Copilot)."
        },
        {
            "title": "References",
            "content": "2019. The cambridge handbook of creativity. Joel Chan, Katherine Fu, Christian Schunn, Jonathan Cagan, Kristin Wood, and Kenneth Kotovsky. 2011. On the benefits and pitfalls of analogies for innovative design: Ideation performance based on analogical distance, commonness, and modality of examples. Journal of mechanical design, 133(8):081004. DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, and Juho Kim. 2023. Creativeconnect: Supporting reference recombination for graphic design ideation with generative ai. Proceedings of the CHI Conference on Human Factors in Computing Systems. Jonas Frich, Lindsay MacDonald Vermeulen, Christian Remy, Michael Mose Biskjaer, and Peter Dalsgaard. 2019. Mapping the landscape of creativity support tools in hci. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 118. Dedre Gentner, Sarah Brem, Ron Ferguson, Philip Wolff, Arthur Markman, and KD Forbus. 1997. Analogy and creativity in the works of johannes kepler. Creative thought: An investigation of conceptual structures and processes, pages 403459. Dedre Gentner and Kenneth J. Kurtz. 2005. Relational Categories. In Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin, APA decade of behavior series. American Psychological Association, Washington, DC, US. Dedre Gentner and Arthur Markman. 1997. Structure mapping in analogy and similarity. American psychologist, 52(1):45. Moritz Hennen, Florian Babl, and Michaela Geierhos. 2024. Iter: Iterative transformer-based entity recognition and relation extraction. In Conference on Empirical Methods in Natural Language Processing. Keith J. Holyoak and Paul Thagard. 1994. Mental leaps: Analogy in creative thought. J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685. Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020. Scirex: challenge dataset for document-level information extraction. In Annual Meeting of the Association for Computational Linguistics. Hyeonsu Kang, Xin Qian, Tom Hope, Dafna Shahaf, Joel Chan, and Aniket Kittur. 2022. Augmenting scientific creativity with an analogical search engine. ACM Transactions on Computer-Human Interaction, 29:1 36. G. Knoblich, S. Ohlsson, H. Haider, and D. Rhenius. 1999. Constraint relaxation and chunk decomposition in insight problem solving. Journal of Experimental Psychology: Learning, Memory, and Cognition, 25(6):15341555. 00691. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, João P. Moutinho, Nima Sanjabi, Rishi Sonthalia, Ngoc M. Tran, Francisco Valente, Yangxinyu Xie, Rose Yu, and Michael Kopp. 2022. Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Nature Machine Intelligence, 5:13261335. Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. ArXiv, abs/1808.09602. T. McCaffrey. 2012. Innovation Relies on the Obscure: Key to Overcoming the Classic Problem of Functional Fixedness. Psychological Science, 23(3):215 218. 00117. Céline McKeown. 2014. The cognitive science of science: explanation, discovery, and conceptual change. Ergonomics, 57:632 633. Feng Shi and James Evans. 2023. Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. Nature Communications, 14(1):1641. Feng Shi and James Allen Evans. 2019. Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. Nature Communications, 14. Arvind Srinivasan and Joel Chan. 2024. Improving selection of analogical inspirations through chunking and recombination. Proceedings of the 16th Conference on Creativity & Cognition. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, and Haijun Xia. 2023. Luminate: Structured generation and exploration of design space with large language models for human-ai co-creation. Proceedings of the CHI Conference on Human Factors in Computing Systems. Aakanksha Naik, Bailey Kuehl, Erin Bransom, Doug Downey, and Tom Hope. 2023. Care: Extracting experimental findings from clinical literature. ArXiv, abs/2311.09736. Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agent. ArXiv, abs/2304.09542. Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Constantin Dragut, and Longin Jan Latecki. 2024. Scidmt: large-scale corpus for detecting scientific mentions. In International Conference on Language Resources and Evaluation. You Can Teach, Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. 2020. You can teach an old dog new tricks! on training knowledge graph embeddings. In International Conference on Learning Representations. Tal Perry. 2021. Lighttag: Text annotation platform. In Conference on Empirical Methods in Natural Language Processing. Brian Uzzi, Satyam Mukherjee, Michael Stringer, and Ben Jones. 2013. Atypical combinations and scientific impact. Science, 342(6157):468472. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S. Weld. 2024a. Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. ArXiv, abs/2409.14634. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Daniel Weld, and Tom Hope. 2024b. Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. arXiv preprint arXiv:2409.14634. Tarek Saier and Michael Färber. 2020. unarxive: large scholarly data set with publications full-text, annotated in-text citations, and links to metadata. Scientometrics, 125:3085 3108. Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier López de Lacalle, German Rigau, and Eneko Agirre. 2023. Gollie: Annotation guidelines imArXiv, prove zero-shot abs/2310.03668. information-extraction. Omar Sharif, Joseph Gatto, Madhusudan Basak, and Sarah Masud Preum. 2024. Explicit, implicit, and scattered: Revisiting event extraction to capture complex arguments. In Conference on Empirical Methods in Natural Language Processing. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824 24837. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. Hyejin Youn, Deborah Strumsky, Luis MA Bettencourt, and José Lobo. 2015. Invention as combinatorial process: evidence from us patents. Journal of the Royal Society interface, 12(106):20150272. Zexuan Zhong and Danqi Chen. 2021. frustratingly easy approach for entity and relation extraction. In North American Chapter of the Association for Computational Linguistics. combines combined combine combination combinations combining mixing mixture mix mixed integrates integrating integrate integrated connection synergy fusion fuses unify aggregate aggregation alignment analogies equivalence equivalent reduction reframing reframe reformulating casting cast casts viewing viewed view inspire inspired inspiration inspires inspiring interconnect align reframing reframe Recombination keywords aggregate aggregation align alignment amalgamate amalgamation assemble assembling associate association bond bonding bridge bridging coalesce coalescence compose composition incorporation inspiring inspiration inspires unify intermingle unification intermingling weave join weaving joining juxtapose hybrid juxtaposition merge merges link merging linkage merged meld conflation melding couple mesh unite meshing unites perceive interplay perception interconnect relate harmonize relation harmony splice incorporate splicing reduction synthesis couple inspire fuse unite synthesis blending blends blend blends merge merges unites analogy analogize analogies equivalence equivalent correlate correlation envision envisioning harmonize harmony synthesis conjunction conjoin Table 7: Recombination keywords. We use predefined list of keywords to identify works that are more likely to discuss idea recombination."
        },
        {
            "title": "A Recombination keywords",
            "content": "We use keyword-based filtering to identify works that are more likely to discuss recombination before assigning papers to human annotators. Table 7 presents the list of keywords used for this step."
        },
        {
            "title": "B Extraction baselines implementation",
            "content": "Annotators agreement We calculate the annotators agreement by treating one annotators work as the ground truth and the other as predictions. In addition to entity-level and relation-level agreement, we also measure agreement on the recombination presence - whether the text presents recombination, regardless of type. We apply the soft entity and relation matching described in 4.1 to measure the relation and entity agreement. The agreement is computed over the 52 documents shared by both annotators (approximately 10% of all annotated data). We regard the annotators agreement as an approximation of human performance measurement over this task. tion extraction baseline. We fine-tune the model using mistral-finetune5 on single NVIDIA RTX A6000 48GB GPU over 500 steps. The training was conducted using the default learning rate of 6.e 5 and weight decay of 0.1. We use batch size of 1 and maximum sequence length of 4096 tokens. mistral-finetune implements Low-Rank Adaptation of LLM (LoRA), parameter efficient fine-tuning method (Hu et al., 2021), which we use with the default rank of 64. The evaluation uses the corresponding repository, mistral-inference6. We rerun the same experiment using Llama-3.1-8B as backbone, using an additional 500 warm-up steps, learning rate of 2e 5 and weight decay of 0.01. Figure 6 presents the prompt for these experiments. In addition to fine-tuning LLMs on our data, we experiment with GoLLIE (Sainz et al., 2023), general IE model fine-tuned to follow any annotation guidelines in zero-shot fashion. We apply GollIE-13B on our data, using single NVIDIA E2E recombination use Mistral-7B as the backbone for our recombinaextraction We 5https://github.com/mistralai/mistral-finetune 6https://github.com/mistralai/mistral-inference Figure 6: E2E extraction prompt. {TEXT} is the placeholder for the input abstract text. short CoT-style analysis string as well as the gold class. We construct the analysis string by incorporating the human entity annotations into predetermined templates (e.g., \"This paper discusses recombination since the authors take inspiration from [inspiration-source] and implement it in [inspiration-target]\"). To evaluate entity extraction, we compare our model against GPT-4o in few-shot settings and include 45 cases per example type, similarly to the E2E experiment. To account for variability due to example selection, we run each experiment 5 times, sampling new set of few-shot examples in each, and report the average. The total cost of this process sums up to 50$. The prompt template for this experiment is available on Figure 9. We experiment with non-generative approaches as well, and compare our model to SciBERT (Zhong and Chen, 2021) based token classifier. The encoder uses standard Hugging-Face implementation of SciBERT, which we train on single NVIDIA RTX A6000 48GB GPU over 500 steps. We use weight decay of 0.1, learning rate of 6.e 5 and batch size of 1. We also experiment with PURE (Zhong and Chen, 2021), well-known information extraction baseline. We finetune PURE over our train set using the default parameters, except for max_span_length, which we set to 40 to accommodate for the longer entities in our data. E2E vs Specialized extraction This section reflects on the results described in Section 4, drawing on implementation details of the baselines (described in Appendix B). In Section 4, we observe that narrowing the focus to smaller portion of the recombination extraction task does not always improve performance - in fact, it can lead to worse results. This pattern emerges across three Mistral-based classifiers: the end-to-end version (E2E), the specialized version (Abstract-classifier), and the specialized version trained with synthetic CoT strings (Abstractclassifier-CoT). We hypothesize that identifying recombination relations in text may be analogous to Chain-of-Thought prompting (CoT), technique known to enhance LLM performance across various tasks (Wei et al., 2022). This hypothesis is supported by the superior performance of Abstractclassifier-CoT compared to its non-CoT counterpart. Figure 7: GoLLIE guidelines. RTX A6000 48GB GPU, 1-beam search, and limit the new token number to 128. GoLLIE is finetuned from CODE-LLaMA2, and receives guidelines in the form of data classes describing what objects and properties the model should extract. Figure 7 depicts the guidelines we used to test GoLLIE as an E2E recombination extraction model. In the rare cases where the model returns more than single recombination type (< 10), we select the first. We also experiment with GPT-4o in few-shot settings. We select 45 examples for each example type (blend, inspiration, not-present) from the training data (a total of 135). As Table 2 describes, the training set only has 45 inspiration examples (as opposed to > 100 blend and not-present examples). 45 is, therefore, the maximal number of examples per class we can sample while keeping the ICL set balanced. We run each experiment 5 times, sampling new set of few-shot examples in each, and report the average. Figure 8 presents the prompt for this experiment. Specialized baselines The recombination extraction model has to execute multiple tasks at once (classifying the document, extracting entities, inferring relations), which might be more challenging than performing them separately. To explore this question, we examine our model classification and extraction abilities against designated models for each task. We use Mistral-7B as specialized classifier and experiment with two versions of the training data. The first includes binary responses (present, not-present), while the other contains Figure 8: E2E ICL prompt. {TEXT} is placeholder for the abstract text, and {EXAMPLES} for the ICL examples. Interestingly, some nodes in this domain describe nonacademic or niche concepts (see examples in Table 9). domain grouping To avoid sparsity, we group similar domains as displayed in Table 10. Table 11 presents the node distribution of common domains after applying this grouping process."
        },
        {
            "title": "E Span similarity prompt",
            "content": "We provide our span similarity prompt in Figure E. We use it in the extraction evaluation process as discussed in Section 4.1."
        },
        {
            "title": "F Error analysis",
            "content": "We perform analysis over the test set, revealing different sources of error which may inspire future improvements. Our focus is on understanding how different types of input texts can influence the result, specifically, in cases where the extraction model struggles. We use our best-performing fine-tuned E2E model for this analysis. Context dependent or implicit phrasing We observe that, unsurprisingly, cases in which the recombination is implied or subtle are more challenging for the model. For instance (see also Table 12, row 1), \"Kahneman & Tverskys prospect theory\" inspires the design of loss function that \"directly maximizes the utility of generations\", but this is not stated explicitly. Moreover, abstracts that explicitly express idea recombination while referencing previously mentioned entities are also harder to detect. Multiple recombinations Some papers present salient recombination along with other insignificant ones. We notice that in those cases, the model might extract non-salient recombination or mix multiple ones (see Table 12, row 2 for such case). Borderline cases The role of recombination as core element in the work is sometimes debatable. Table 12, row 3 presents an example of such case where the authors explicitly mention integrating \"embedding space comparison\" with \"computational notebook environment\", which may be interpreted as recombination (the usage of notebook in these environments is completely new and novel), or simply as way to present the tools environment. We notice that the extraction model tends to miss those cases. Figure 9: Entity extraction prompt. {TEXT} is placeholder for the input abstract."
        },
        {
            "title": "D Graph nodes domains",
            "content": "We identify entities scientific domain using GPT-4o in zero-shot settings. Given the abstract and recombination entities extracted from it, the model has to assign each an arXiv category and scientific branch. In case the model manages to assign the entity an arxive category, the scientific branch is the categorys full name (e.g., \"Artificial Intelligence\" for cs.AI). Otherwise, the models assign the branch from list of outer-arXiv domains described in Table 8. If the model can assign the entity standard arXiv category, we use it as the domain. Otherwise, we use the branch (an outerarXiv domain). Entities with neither are assigned to the Other domain. Figures 10 and 11 present our analysis prompts for blend and inspiration relations, respectively. The cost of running the analysis over the collected corpus was 250$. The Other domain We use the Other domain for nodes the model fails to analyze, and 2127 of the graph nodes are assigned to this category. We examine sample of 150 such nodes and observe that many are too noisy or overly general to classify. Non-arXiv scientific domains Anatomy Archaeology Bioinformatics Animal Science Behavioral Science Bioclimatology Biotechnology Chemical Engineering Cognitive Science Cytology Dermatology Ecotoxicology Electrical Engineering Energy Science Environmental Engineering Ethology Gastroenterology Geography Glaciology Hydrodynamics Immunogenetics Agricultural Science Anthropology Biochemistry Biomedical Engineering Biophysics Cardiology Botany Clinical Psychology Civil Engineering Cryosphere Science Criminology Dentistry Demography Ecology Developmental Biology Educational Psychology Economics Endocrinology Emergency Medicine Entomology Engineering Science Epidemiology Environmental Science Forestry Food Science Genomics Genetics Geophysics Geology Histopathology Health Informatics Hydrology Hydrogeology Industrial/Organizational Psychology Landscape Architecture Immunology Linguistics Marine Biology Mechanical Engineering Medical Microbiology Microbiology Mycology Neuroscience Obstetrics Ophthalmology Otology Pathobiology Pedagogy Pharmacology Political Science Psychology Pulmonology Seismology Surgery Toxicology Veterinary Science Wildlife Biology"
        },
        {
            "title": "Materials Science\nMeteorology\nMolecular Biology\nNeurology\nNutritional Science\nOncology\nOrthopedics\nPaleontology\nPediatric Medicine\nPharmacogenomics\nPhysiology\nPsychiatry\nPublic Health\nRheumatology\nSociology\nThermodynamics\nUrology\nVolcanology",
            "content": "Table 8: Non-arXiv scientific domains. We complement arXiv category taxonomy using broader list of scientific fields. Figure 10: blend domain analysis prompt. {ELEMENTS} is placeholder for the recombination entities extracted from {ABSTRACT}. {ARXIV} is placeholder for full arXiv category names and their descriptions. {BRANCHES} is placeholder for the list of non-arXiv domains given in Appendix D, Table 8. Figure 11: inspiration domain analysis prompt. {INSPIRATION_SOURCE} and {INSPIRATION_TARGET} are placeholders for the inspiration entities extracted from {ABSTRACT}. {ARXIV} is placeholder for full arXiv category names and their descriptions. {BRANCHES} is placeholder for the list of non-arXiv domains given in Appendix D, Table 8."
        },
        {
            "title": "Examples",
            "content": "Non-Academic \"the snap-through action of steel hairclip\", \"yoga\", \"origami, the traditional Japanese paper-folding technique, is powerful metaphor for design and fabrication of reconfigurable structures\", \"Tangram, game that requires replicating an abstract pattern from seven dissected shapes\""
        },
        {
            "title": "Noisy",
            "content": "\"a deep\", \"word-\", \"at the context level\", \"a neural part\", \"post\", \"textaudio\", \"end-toend multi-modal model only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLMs\", \"a users long-term\" Overly-general \"human experiences\", \"a styling method\", \"local search method\", \"a pipeline inspired by experts work\", \"a new modality\", \"feature based approaches\""
        },
        {
            "title": "Misclassified",
            "content": "\"Reinforcement learning, or RL\", \"Facial Expressions Recognition(FER)\", \"a Kullback-Liebler regularization function\", \"K-nearest neighbors algorithm\", \"Shapley values from game theory\", \"Gaussian Stochastic Weight Averaging\" Table 9: Examples of graph nodes in the \"other\" domain. We analyze sample of 150 nodes in this domain and identify groups with common traits, as shown in the table. Group Scientific domains Geosciences Geology, Geophysics, Petrology, Mineralogy, Hydrology, Hydrogeology, Seismology, Volcanology, Cryosphere Science, Glaciology, Geography Environmental Sciences Environmental Science, Environmental Engineering, Ecology, EcotoxiBiomedical Sciences Health and Medicine Zoology Agriculture Food Sciences Psychology Microbiology Humanities Social Sciences cology Biochemistry, Immunology, Immunogenetics, Neuroscience, Oncology, Pathology, Pathobiology, Pharmacology, Toxicology Cardiology, Neurology, Urology, Gastroenterology, Obstetrics, Pediatric Medicine, Rheumatology, Dermatology, Ophthalmology, Otology, Pulmonology, Emergency Medicine, Surgery, Radiology, Orthopedics, Psychiatry, Dentistry, Public Health, Epidemiology, Health Informatics, Clinical Psychology, Psychopathology Zoology, Entomology, Ornithology, Wildlife Biology, Animal Science, Veterinary Science, Ethology Agricultural Science, Forestry Nutritional Science, Food Science Educational Psychology, Social Psychology, Psychology, trial/Organizational Psychology Microbiology, Medical Microbiology Linguistics, Philosophy, Pedagogy Sociology, Anthropology, Political Science, Demography IndusTable 10: Scientific domains grouped by category. We group similar non-arXiv scientific domains (see Table 8) to thicken infrequent ones."
        },
        {
            "title": "G Extraction examples",
            "content": "Table 13 presents examples of interdisciplinary, automatically extracted inspiration recombinations. human annotator agrees with 87% of the models predictions (whether there is leak). Finally, we divide the remaining query-answer pairs into splits as described in Table 5 is Section 5.2."
        },
        {
            "title": "H Predominant inspiration and blend",
            "content": "relations We provide tabular version of Figure 3 in Section 4.2 on Table 14 for better readability."
        },
        {
            "title": "I Prediction data preprocessing",
            "content": "Context extraction and leakage filtering We use GPT-4o-mini to extract few sentences from each abstract describing the background or motivation of the authors using recombination (See prompt on Figure 13). Adding these contexts to the queries helps them be more specific and limits the search space. However, this might introduce leaks into the queries - cases where the extracted context reveals the answer. Table 15 presents leak examples. We utilize GPT-4o-mini again to filter out such cases from the data, using the prompt shown in Figure 14. In qualitative analysis of 50 randomly sampled query-answer pairs, we find that a"
        },
        {
            "title": "J Prediction baselines",
            "content": "We use bi-encoder architecture for recombination prediction and experiment with three popular encoders as backbones: all-mpnet-base-v2 (109M parameters), bge-large-en-v1.5 (Xiao et al., 2023) (335M parameters) and e5-large-v2 (Wang et al., 2022) (335M parameters). These models checkpoints predate 2024, meaning they are unfamiliar with our test set. The model receives query string composed of context description, graph entity, and relation type and returns ranked list of answers (other graph nodes). We perform HPO (random grid search of 10 trails) to select the number of training epochs, warmup ratio and learning rate for each model. We use contrastive loss and generate 30 negatives per positive example. Following the literature standard (Teach et al., 2020), we report metrics in the filtered settings to avoid false negatives. Given the difficulty of the task we focus on ranking only the 12751 test Domain Count Domain Count Domain Count cs.cv cs.ro cs.ir cs.hc cs.cg math.oc cs.db cs.ma cs.cr psychology zoology cs.dc eess.as cs.na physics.med-ph physics.bio-ph stat.th math.ds q-bio.pe math-ph math.dg econ.th physics.comp-ph cs.lg cs.ai cs.ne q-bio.nc cs.cy eess.iv eess.sp cs.ce stat.me eess.sy cs.it behavioral science nlin.ao cs.pl stat.ml cs.ni anatomy cs.fl cs.dl cond-mat.stat-mech physics.class-ph 12504 2241 884 645 382 356 254 203 164 116 101 89 79 66 60 52 43 39 32 27 22 21 math.ca 20 physics.optics cs.cl cognitive science cs.si cs.ds cs.gr cs.dm cs.lo cs.sy cs.gt cs.se 8440 2091 864 441 378 278 242 185 138 108 100 math.pr cs.mm cs.ar biomedical sciences health and medicine physics.ao-ph 88 79 65 56 48 41 math.na humanities 38 32 cs.sc 25 math.ap 22 cs.sd 21 math.mg cs.et 4697 936 655 409 367 269 204 177 132 104 96 82 74 63 56 44 40 38 30 24 22 20 20 Table 11: Node domains distribution. The table presents the number of graph nodes from each domain with above-median frequency. Bad extraction examples Abstract: \"...Kahneman & Tverskys prospect theory tells us that humans perceive random variables in biased but well-defined manner (1992) ... Using Kahneman-Tversky model of human utility, we propose HALO [Human Aware Loss Function] that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do...\" Gold = [Inspiration: \"Kahneman & Tverskys prospect theory\" \"a HALO\"] Pred = [] Abstract: \"...We address the problem by proposing Wasserstein GAN combined with new reverse mask operator, namely Reverse Masking Network (R-MNet), perceptual adversarial network for image inpainting ... Additionally, we propose new loss function computed in feature space to target only valid pixels combined with adversarial training...\" Gold = [Blend: \"a Wasserstein GAN\" \"...R-MNet\"] Pred = [Blend: \"a Wasserstein GAN\" \"...R-MNet\" \"a new loss function\"] Abstract: \"... In order to characterize model flaws and choose desirable representation, model builders often need to compare across multiple embedding spaces, challenging analytical task supported by few existing tools. We first interviewed nine embedding experts in variety of fields to characterize the diverse challenges they face and techniques they use when analyzing embedding spaces. Informed by these perspectives, we developed novel system called Emblaze that integrates embedding space comparison within computational notebook environment...\" Gold = [Blend: \"embedding space comparison\" \"...notebook environment\"] Pred = [] Table 12: In the first row, the extraction model misses an inspiration relation because of subtle phrasing. In the second row, when analyzing an abstract with multiple recombinations, the model fails to identify the most important one and confuses entities across different relations. In the third row, the model fails to detect weak recombination example. g l l t a a s m i e s - a f i o e i fi \" t f d l i r k e f i i - d e p T . . . t r i . d d h r a g r e h r i s t l s l m b t t r a e a r n g e y d i \" . . . d d p - u c s l v s t n o c fi a g l g h o H \" a b ] l [ \" d d f i e i h s \" i i I c D : u - t p ] . [ \" t l r n \" : u - t p I : r - t p d i - u , i o p s r t g d t . . . t g d a - m [ \" i o p c p c o i l r - u t \" - t o r c s i r a s t o e a y i a p a p t g r n - u t e s . . . i fi a e , a a u r l n i r e a r e w , i o p - o e s a l o d h i n , r n e fi , i \" . . . a a o t t n i r i . . . , L n a u a c n s a p n i T . . . \" y r n . a f l i b e e s s n fi c f d c s w fi e h w , l y i e s r u e P - D t o p , ) s ( i a n ) f ( t n - k t o y h h s o P . w r ) D ( n P o D l f f o i e c s a : t g n p a m m w f n e ) M ( a e l e M t b d a e c \" . . . r c v , p ] l y [ \" l y i e s r - d \" : u - t p I ] . [ \" t fi a g l g h o H \" ] . [ \" L n a u a c n \" : r - t p : r - t p ] n s c t o i t o p i t e ) ( a u T - l \" : u - t p f s d m u f i f n a r , t e c n o - o [ \" s o u s n o e s W o h \" i o y h a r a G y d p . . . . a fi i e e a c r e o b fi p l t s o i , s o c - u t t i d e y n a e t h e r r : r - t p ] e e \" . . . E m , i d R o i t u s d c T w n L d m m D b t t d T s r , i e ] . [ \" t e T f i b e a v e g r \" . r d o R H t r l x i i i m - n : 3 1 a Inspirations Blends Source Target Count Source Target Count cs.cv cs.lg cognitive science cs.lg cognitive science cs.cl cs.cl cognitive science cognitive science cs.cl cs.lg q-bio.nc q-bio.nc cognitive science cs.cv cs.lg cs.cl math.oc zoology cs.cv cs.cv cs.cv cs.lg cs.lg cs.cl cs.cv cs.ai cs.cl cs.ai cs.ai cs.cv cs.lg cs.ro cs.lg cs.cl cs.lg cs.lg cs.ro cs.cv cs.lg cs.cl cs.lg cs.cv cs.cl cs.cv cs.lg cs.ro cs.ro cs.cl cs.ai cs.ai cs.ai cs.lg cs.lg cs.ir cs.lg 334 300 278 254 211 190 188 184 142 141 118 114 102 100 94 84 84 83 76 cs.cv cs.lg cs.cl cs.cv cs.lg cs.cv cs.cl cs.cl cs.ro cs.cv cs.lg cs.cl cs.ai cs.lg cs.ai cs.ne cs.ir cs.ro 4329 2793 1049 992 470 422 391 363 299 218 197 174 161 151 146 133 132 124 Table 14: Predominant inspiration and blend relations. The above is tabular version of Figures 3b, 3a in Section 4.2. It presents edges with (source-domain, target-domain) pairs frequency above the 0.98 quantile."
        },
        {
            "title": "Query",
            "content": "Understanding the human brains processing capabilities can inspire advancements in machine learning algorithms and architectures. Previous methods in brain research were limited to identifying regions of interest for one subject at time, restricting their applicability and scalability across multiple subjects. What would be good source of inspiration for \"a highly efficient processing unit\"? Existing models for link prediction in knowledge graphs primarily focus on representing triplets in either distance or semantic space, which limits their ability to fully capture the information of head and tail entities and utilize hierarchical level information effectively. This indicates need for improved methods that can leverage both types of information for better representation learning in knowledge graphs. What could we blend with \"distance measurement space\" to address the described settings?"
        },
        {
            "title": "The human brain",
            "content": "Semantic measurement space Table 15: Leakages examples. Examples of leaks - queries that reveal or strongly imply the answer. set entities. full summary of our data splits is available on 5. The examples we use to train and evaluate our prediction models contain all collected nodes, including those classified as belonging to the \"other\" domain. We utilize RankGPT (Sun et al., 2023) as strong reranker and apply it to rerank the top-20 predicted results. We employ RankGPT with GPT4o, window size of 10 and step size of 5. Note the information cutoff of GPT-4o is October 2023 7, meaning it is unfamiliar with our test set as well. We use the implementation available in 8. However, we find that adjusting the default prompt works better for our task. Figure 15 shows the modified reranking prompt. The cost of applying the reranker to our data was 60$."
        },
        {
            "title": "K User study additional details",
            "content": "scientific domains We request each to fill out form asking in what they feel comfortable reading papers and short description of their research area. We then used granite-embedding-125m-english to retrieve semantically similar contexts to this description from the relevant arXiv categories. We manually verify that the retrieved contexts match the description and discard examples with poorly extracted information (e.g., the context begins with \"This study reviews the problem of...\" instead of directly describing the source study problem). In addition, we let the volunteers mark an example as \"ill-defined\", in which case we ignore their inputs. We conduct 10-minute training session with each volunteer, requesting them to read the instructions and explain the task. Figure 16 presents the instructions given to the participants in the study. Figure 17 presents the web interface of the annotation platform."
        },
        {
            "title": "L Comparison to other information",
            "content": "extraction methods Both general scientific extraction and concept cooccurrence struggle to capture concise and accurate recombination relations, as can be seen in Figure 18. Figure 18a presents how general scientific IE schemas lack relation types to model recombinations. The figure presents the results of our specialized extraction method besides transformerbased extraction model (Hennen et al., 2024) fine7As stated in https://platform.openai.com/docs/models/gpt4o 8https://github.com/sunnweiwei/RankGPT/tree/main Figure 12: Span similarity prompt. {ENTITY_TYPE} is either \"combination-element\", \"inspiration-source\" or \"inspiration-target\". {TEXT} is placeholder for the papers abstract. {SPAN1}, {SPAN2} are placeholders for the compared spans. Figure 13: Context extraction prompt. {{ABSTRACT}} is placeholder for the input abstract. {{METHODOLOGY_STATEMENT}} is sentence describing the recombination. We build it by filling one of the following templates with the extracted recombination entities: \"Combine <source-entity> and <target-entity>\" for blends and \"Take inspiration from <source-entity> and apply it to <target-entity>\" for inspirtions. Figure 14: Leak detection prompt. Figure 15: Adjusted RankGPT prompt. Figure 16: User study guidelines. Figure 17: User study interface. tuned on SciERC (Luan et al., 2018), general IE schema. While our new data schema easily models the recombinant connection between two techniques: \"BV-MAPP (Verbal Behavior Milestones Assessment and Placement Program)\", \"ChatGPT\" as concept blend, the SciERC extraction schema isnt equipped with proper relation types for this. As result, it captures mostly irrelevant information for our task (e.g background details as \"Early diagnosis\" or \"professional intervention\"). Figure 18b shows how recombination extraction using concept co-occurrence might be misleading. In this method, each pair of canonical scientific concepts (e.g, neural networks) that co-occur within the same abstract are considered recombination. The figure presents an example of using AI-related concepts curated by Krenn et al. (2022) for recombination extraction, alongside recombination extracted using our designated approach. Note that when using concept co-occurrence, the extracted recombinations are essentially {concepts}2, which might be imprecise, and capture meaningless recombinations (e.g., \"wide application\" recombined with \"final prediction\") or misleading recombinations (e.g., \"question answering\" with \"language models\", which explicitly presented by the authors as lacking approach for the task). In comparison, our new extraction schema neatly models the main recombiant relation presented in the text as taking inspiration from \"the step-by-step reasoning behavior of humans\" for \"temporal question answering.\" (a) Comparison to recombination extraction using general scientific IE schema (SciERC) (b) Comparison to recombination extraction using concept co-occurrence. Figure 18: Comparison of our designate recombination extraction method to alternative approaches. Figure 18a: General recombination extraction schemas lack fitting relation types to capture recombinations, which results in capturing plenty of irrelevant relations (\"Early diagnosis\" \"professional intervention\"). Figure 18b: Recombination extraction using concept co-occurrence might be nonsensical (\"wide application\" \"final prediction\") or even misleading (\"question answering\" \"language models\"))."
        }
    ],
    "affiliations": [
        "School of Computer Science and Engineering, The Hebrew University of Jerusalem",
        "The Allen Institute for AI (AI2)"
    ]
}