{
    "paper_title": "Progressive Language-guided Visual Learning for Multi-Task Visual Grounding",
    "authors": [
        "Jingchao Wang",
        "Hong Wang",
        "Wenlong Zhang",
        "Kunhua Ji",
        "Dingjiang Huang",
        "Yefeng Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. https://github.com/jcwang0602/PLVL"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 4 1 6 1 . 4 0 5 2 : r Progressive Language-guided Visual Learning for Multi-Task Visual Grounding Jingchao Wang East China Normal University Shanghai, China jcwang@stu.encu.edu.cn Kunhua Ji East China Normal University Shanghai, China 72285900034@stu.ecnu.edu.cn Hong Wang Xian Jiaotong University Xian, Shaanxi, China hongwang01@xjtu.edu.cn Dingjiang Huang* East China Normal University Shanghai, China djhuang@dase.ecnu.edu.cn Wenlong Zhang East China Normal University Shanghai, China zhangwenlong@pjlab.org.cn Yefeng Zheng* Westlake University Hangzhou, Zhejiang, China zhengyefeng@westlake.edu.cn Abstract Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. https://github.com/jcwang0602/PLVL Keywords Multi-modality, Multi-task visual grounding, Progressive learning"
        },
        {
            "title": "1 Introduction\nVisual Grounding seeks to identify a visual object in an image\nbased on a natural language expression. According to the granular-\nity of the prediction, it can be divided into two sub-tasks: Referring\nExpression Comprehension (REC) and Referring Expression Seg-\nmentation (RES). The former focuses on regional alignment with",
            "content": "Contributed Equally. * Corresponding Author. Figure 1: Comparison of different pipelines for multi-task visual grounding: (a) Visual and language features are extracted separately and then cross-modal fusion is performed; (b) Additional modules (marked by orange) are inserted after the original network layers to inject the language features into the visual backbone; (c) Our progressive language-guided visual learning framework with collaborative multi-task head, which directly adjusts the original network layer for progressively introducing the language guidance. the natural language representation to generate bounding box, and the latter aligns with the natural language representation at the pixel level, producing segmentation mask. Recently, some researchers [1, 2, 28] proposed multi-task collaborative learning framework to unify REC and RES, namely Multi-Task Visual Grounding (MTVG), which proved that multi-task collaboration has much greater potential. For visual grounding, prevalent research paradigm first utilizes visual backbone and linguistic backbone to extract features from image and text modalities, respectively, and then adopts Transformer encoder/decoder for cross-modal feature fusion, as shown in Fig. 1(a). While this approach has demonstrated promising performance [1, 10, 34], it is often hindered by potential limitation that the linguistic-information is not injected into the visual backbone. As result, the visual features extracted may not align well with the semantics of the natural language expression [38]. Against this issue, researchers [20, 38] have proposed different strategies to inject the language features into the visual backbone by introducing extra network modules in order to help the extracted visual features match referring expression as much as possible, as displayed in Fig. 1(b). However, an extra cross-modal interaction module still needs to be carefully designed for achieving better performance. Besides, as illustrated in Figs. 1(a)(b), common limitation of these two types of pipelines is that they use two independent heads to predict the bounding box and segmentation task, respectively, and do not take the prediction correlation between REC and RES into account, leaving room for performance improvement, especially for MTVG. To deal with these problems, very recently, state-of-art method, called VG-LAW [28], has been designed, which proposed language adaptive weight generation strategy to help achieve the language-related visual feature extraction without introducing any extra cross-modal interaction procedure. Albeit obtaining the promising performance, it needs to generate the weights for each layer of the visual backbones, causing certain amount of computational expense. Besides, it only utilized the class token to simultaneously guide the predictions of REC and RES tasks, but did not fully explore the cooperative relationship of these two sub-tasks from the perspective of prediction outputs. Against these aforementioned issues, in this paper, for multi-task visual grounding (MTVG), to achieve accurate predictions based on the referring expression, it is crucial to extract effective visual features that accurately represent the objects to be identified and align with the expression cues. Based on such understandings, inspired by ViTDet [13], we propose Progressive Language-guided Visual Learning (PLVL) backbone, which mainly consists of local and global blocks. Within the local block, we utilize self-attention mechanism to explore the relationships within the visual modality, facilitating the extraction of meaningful feature representations. Different from ViTDet, in the global block, we progressively incorporate the guidance of linguistic tokens into the visual backbone by inserting simple cross-attention interaction operation into the original network layer (see the gradient orange module in Fig. 1 (c)), which aids in learning effective visual features that align with the referring expression. By employing this progressive languageguided visual learning approach, our framework is designed to effectively capture valuable visual features, ultimately enhancing the accuracy of predictions. Furthermore, we recognize that the predictions of Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES) have the similar central position of to-be-identified objects. Instead of adhering to the traditional method of employing two independent heads, driven by the inherent bias priors of convolutional layers, we introduce pure convolution-based collaborative multi-task head that builds the bridge between REC and RES and accomplishes the joint prediction of different granularity information, thereby improving the prediction accuracy for both tasks. In summary, our contributions are mainly three-fold: We specifically propose progressive language-guided visual learning framework for multi-task visual grounding, which fully injects the linguistic content of referring expression into the entire visual backbone for helping more effective visual feature extraction. We deeply explore the relationship between REC and RES, and propose novel collaborative multi-task head to carefully couple REC and RES tasks to achieve the accurate joint prediction. Extensive experiments conducted on several benchmark datasets comprehensively demonstrate the effectiveness of our proposed PLVL, which always achieves superior performance on the REC task and the RES task."
        },
        {
            "title": "2 Related Work\n2.1 Referring Expression Comprehension (REC)\nThe traditional referring expression comprehension methods are\ngenerally divided into one-stage method and two-stage methods.\nThe core idea of the two-stage Visual Grounding method [5, 6, 40]\nis to first use a pre-trained object detector to generate candidate\nregion proposals and then select the region proposal that best\nmatches the natural language expression as the final result. One-\nstage methods[14, 35, 36] typically perform cross-modal informa-\ntion fusion after feature extraction and then directly predict bound-\ning boxes based on predefined anchors.",
            "content": "The emergence of the Transformer [18, 30] has introduced new paradigm for Visual Grounding. Recent works [1, 3] utilize feature extraction networks to extract visual and language features, then employ Transformer Encoder or Decoder to achieve crossmodal feature fusion. They introduce REG Token to aggregate multimodal information, which is fed into head (commonly using Multi-Layer Perceptron) to predict the information of objects. However, during our experiments, we found that this approach based on the REG Token has limitations that severely restrict the models performance. Therefore, in this paper, for the REC task, we employed classification and regression approach for prediction."
        },
        {
            "title": "2.2 Referring Expression Segmentation (RES)\nUnlike the REC task, which predicts a bounding box based on a re-\nferring expression, the RES task demands more refined results. The\nRES task predicts a mask, requiring the classification of each pixel\nin the image according to the referring expression. The existing\nwork [4, 8, 9, 37] has made great progress in cross-modal fusion\nmethods and in achieving more accurate segmentation.",
            "content": "Different from the previous multi-modal fusion of vision and language in the network decoding stage, EFN [4] innovatively proposed an encoder fusion network, which realizes the gradual refinement of multimodal features, and promotes the consistent representation of cross-modal information in semantic space. LTS [9] innovatively decouples it into \"locate-then-segment\" scheme. By 2 Figure 2: Flowchart of the proposed Progressive Language-guided Visual Learning (PLVL) framework which consists of three parts, i.e., linguistic backbone, language-guided visual backbone, and collaborative multi-task head. The detailed structures of local block, glocal block, and multi-task head are described in Fig. 3(a), Fig. 3(b), and Fig. 4, respectively. explicitly modeling the locations prior knowledge, LTS gets better segmentation performance than the previous best results. CGFormer [29] proposed new mask classification framework based on Transformer, which obtains object-level information through mark-based query and grouping strategy and realizes cross-modal and cross-hierarchy reasoning of object perception."
        },
        {
            "title": "2.3 Multi-task Visual Grounding (MTVG)\nIt is easy to find that, there is a certain correlation between REC and\nRES, they both need to identify the object in the image with the re-\nferring expression. Therefore, in recent years, some researchers [17,\n21, 28, 42] consider using the same Backbone and cross-modal in-\nteraction modules for visual and linguistic feature extraction and\nmulti-modal fusion between them, and then using different heads\nfor detection and segmentation.",
            "content": "MCN [21] proposes new multi-task collaborative network, which for the first time attempts the joint learning of REC and RES. MCN maximizes the collaborative learning benefits of both REC and RES tasks by leveraging the characteristics of both. SeqTr [42] presents novel approach for MTVG, defining it as point prediction problem and proposing an innovative, general-purpose network. Which unifies disparate visual base tasks under unified point prediction paradigm, demonstrating the potential for generalization across tasks without modification. Similar to SeqTr, PolyFormer [17] proposes sequence-to-sequence framework that naturally merges multi-modal features as input sequences and multitask predictions as output sequences. In addition, PVD [2] has developed Parallel Vertex Diffusion solution based on the parallelizability of diffusion models. This innovative approach allows for the accurate and efficient generation of vertices in parallel and scalable manner, offering competitive advantage in terms of both precision and efficiency."
        },
        {
            "title": "3.1 Overview\nFor MTVG, the two sub-tasks REC and RES share the same input,\nwhich consists of an image and a referring expression, but produce\ndifferent outputs, i.e., a bounding box and a segmentation mask,\nrespectively. Fig. 2 illustrates the proposed framework, which con-\nsists of three parts, i.e., linguistic backbone, language-guided visual\nbackbone, and a collaborative multi-task Head. Given a referring\nexpression and an input image, we first use the linguistic backbone\nto obtain the language tokens of the expression cue. Then, the lan-\nguage tokens and the input image are fed into the visual backbone\nto extract the useful visual tokens for the subsequent predictions\n(i.e., bounding box for REC and segmentation mask for RES).",
            "content": "Specifically, to achieve accurate predictions according to the referring expression, the key point is to extract effective visual features which represent to-be-identified objects and match the expression cues. Based on such understanding, we propose Progressive Language-guided Visual Learning backbone which not only finely mines the inherent feature expression of the visual modality itself but also progressively injects the language expression information to help learn language-related visual features. Besides, we analyze that the predictions of REC and RES share the same center for identifying the object. Unlike conventional approaches which use two independent heads, we propose collaborative multi-task head that serves as bridge between REC and RES, enhancing the prediction accuracy of both. Below we will provide the details. where ğ‘†ğ‘† () and ğ‘†ğ¶ () represent the split and concatenation at the spatial dimension, respectively. The local self-attention for every head in MHSA() is defined as: ğ‘Œ = Softmax( ğ‘„ = ğœ™ğ‘„ (ğ‘‡ ğ‘– ğ‘„ğ¾ )ğ‘‰ , ğ· ğ‘£ ), ğ¾ = ğœ™ğ¾ (ğ‘‡ ğ‘– ğ‘£ ), ğ‘‰ = ğœ™ğ‘‰ (ğ‘‡ ğ‘– ğ‘£ ), (2) where ğœ™ğ‘„ (), ğœ™ğ¾ (), and ğœ™ğ‘‰ () are linear projection operations for obtaining the query, key, and value, respectively. For global block, as illustrated in Fig. 3(b), unlike the local block, we do not divide the visual tokens spatially. The reason is that during the visual feature learning, it is very important to model the relationship among different division parts. Hence, we directly feed the entire feature Ë†ğ‘‡ğ‘£ in Eq. (1) output by local block into multi-head self-attention (MHSA) procedure. Then, we introduce multi-head cross-attention (MHCA) module to inject the linguistic information into the backbone for fully extracting the languagerelated visual information. The complete computation process is: ğ‘‡ğ‘£ = Ë†ğ‘‡ğ‘£ + MHSA( Ë†ğ‘‡ğ‘£), ğ‘‡ğ‘£ = ğ‘‡ğ‘£ + MHCA( ğ‘‡ğ‘£,ğ‘‡ğ‘™ ), ğ‘‡ğ‘£ = ğ‘‡ğ‘£ + FFN( ğ‘‡ğ‘£), (3) where the output ğ‘‡ğ‘£ is used as the input for the next local block and is fed into the computation procedure in Eq.(1). The attention operation for every head in MHCA() is: ğ‘Œ = Softmax( ğ‘„ğ¾ )ğ‘‰ , ğ· ğ‘„ = ğœ™ğ‘„ (ğ‘‡ğ‘£), ğ¾ = ğœ™ğ¾ (ğ‘‡ğ‘™ ), ğ‘‰ = ğœ™ğ‘‰ (ğ‘‡ğ‘™ ). (4) As clarified before, the proposed progressive language-guided visual learning backbone is indeed built based on ViTDet [13]. However, for the multi-task visual grounding task, we have specific and key contributions: 1) Through the progressive integration of the cross-attention mechanism into different global blocks in ViTDet, we carefully inject the linguistic information to help guide the visual feature learning. Such simple adjustment on the original structure of ViTDet brings obvious performance gains for MTVG, as validated in Table 4 below; 2) We specifically investigate the relationship among REC and RES and construct collaborative multi-task head, which will be described below."
        },
        {
            "title": "3.3 Collaborative Multi-task Head\nFor the multi-task visual grounding, the existing methods [1] gener-\nally adopt the multi-layer perception (MLP) to construct two inde-\npendent task-specific heads for separately predicting the bounding\nbox for the REC task and the segmentation mask for the RES task.\nAlthough there are some attempts to connect these two prediction\nprocedures via different strategies, such as vertex generation [2, 42],\nthere is still room for further performance improvement. In this\nsection, we explore the relationship between REC and RES, and\nconstruct a novel convolution-based collaborative multi-task head.\nSpecifically, we rethink the joint prediction of REC and RES,\nand analyze that the predictions for these two sub-tasks have the\nsimilar central position of to-be-identified objects. Since RES is a\npixel-wise classification task, if we can build the bridge between",
            "content": "Figure 3: The structures of local block and global block."
        },
        {
            "title": "Learning",
            "content": "It is easily understood that for accurate localization and segmentation prediction of to-be-identified objects, the effective feature learning on the image modality itself and the language-relevant feature learning are two key aspects we need to focus on for MTVG. To this end, driven by the excellent capability of ViTDet [13] in visual feature representation, we propose local-global-group-based Progressive Language-guided Visual Learning (PLVL) backbones. Please note that the key characteristic of our PLVL lies in that on the basis of the visual backbone of ViTDet, we carefully incorporate the guidance of language information for visual feature learning. Specifically, as displayed in Fig. 2, given the referring expression, we follow EEVG [1] and adopt the 12-layer Bert-Base as the linguistic backbone to extract language tokens ğ‘‡ğ‘™ Rğ‘ğ‘™ ğ· . For the input image ğ¼ Rğ» ğ‘Š 3, we use the patch Embedding layer to tokenize it as ğ‘‡ğ‘£ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ ğ· , where ğ‘ƒ is the Patch size. Then, we feed the visual token ğ‘‡ğ‘£ and language token ğ‘‡ğ‘™ into the visual backbone, which sequentially consists of four local-global groups and three global blocks. For every local-global group, it is composed of two local blocks and one global block. Through the local block, we employ the self-attention mechanism to mine the relationship among the visual modality itself for useful feature representation. Through the global block, we progressively inject the guidance of language tokens into the visual backbone via the cross-attention-based interaction mechanism to help learn the effective visual features which match the referring expression. In such progressive learning manner, it is expected that our framework can effectively learn useful visual features for boosting accurate predictions. Specifically, for the local block, as shown in Fig. 3(a), in order to reduce the computational complexity, we first split the vision toğ‘£ Rğ» /2ğ‘ƒ ğ‘Š /2ğ‘ƒ ğ· , kens ğ‘‡ğ‘£ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ ğ· into four parts as ğ‘‡ ğ‘– ğ‘– = 1, 2, 3, 4, and feed each part into the same computation procedure consisting of multi-head self-attention (MHSA) and FFN. Then by merging the four computed results along the spatial dimension, we can get the visual features output by the local block. Correspondingly, the entire computation process is formulated as: ğ‘£ ] = ğ‘†ğ‘† (ğ‘‡ğ‘£), [ğ‘‡ 1 ğ‘£ ,ğ‘‡ 2 ğ‘£ = ğ‘‡ ğ‘– ğ‘‡ ğ‘– ğ‘£ = ğ‘‡ ğ‘– ğ‘‡ ğ‘– Ë†ğ‘‡ğ‘£ = ğ‘†ğ¶ ([ ğ‘‡ 1 ğ‘£ ,ğ‘‡ 3 ğ‘£ ,ğ‘‡ 4 ğ‘£ + MHSA(ğ‘‡ ğ‘– ğ‘£ + FFN( ğ‘‡ ğ‘– ğ‘£ ), ğ‘£ , ğ‘‡ 3 ğ‘£ , ğ‘‡ 2 ğ‘£ , ğ‘‡ ğ‘£ ]), ğ‘£ ), ğ‘– = 1, 2, 3, 4, (1) 4 Lğ‘‘ğ‘’ğ‘¡ = Lğ‘“ ğ‘œğ‘ğ‘ğ‘™ ( Ë†ğ‘ƒ, ğ‘ƒ) + L1 ( Ë†ğµ, ğµ) + Lğ‘”ğ‘–ğ‘œğ‘¢ ( Ë†ğµ, ğµ)), Lğ‘ ğ‘’ğ‘” = Lğ‘“ ğ‘œğ‘ğ‘ğ‘™ ( Ë†ğ‘€, ğ‘€) + Lğ‘‘ğ‘–ğ‘ğ‘’ ( Ë†ğ‘€, ğ‘€), (6) where Lğ‘“ ğ‘œğ‘ğ‘ğ‘™ represents focal loss [15], L1 represents L1 Loss, Lğ‘”ğ‘–ğ‘œğ‘¢ represents GIoU loss [26], and Lğ‘‘ğ‘–ğ‘ğ‘’ represents dice loss [23]. ğµ and ğ‘€ are the ground-truth bounding box and segmentation mask, respectively. The label ğ‘ƒ is generated based on ground-truths. Finally, the total training loss for MTVG is adopted as: = ğœ†ğ‘‘ğ‘’ğ‘¡ Lğ‘‘ğ‘’ğ‘¡ + ğœ†ğ‘ ğ‘’ğ‘”Lğ‘ ğ‘’ğ‘”, (7) where ğœ†ğ‘‘ğ‘’ğ‘¡ and ğœ†ğ‘ ğ‘’ğ‘” are weights used to balance the two losses. Following EEVG [1], in this paper, we set ğœ†ğ‘‘ğ‘’ğ‘¡ = 0.1 and ğœ†ğ‘ ğ‘’ğ‘” = 1."
        },
        {
            "title": "4.1 Experimental Settings\nDatasets. We conduct experiments on three mainstream bench-\nmark datasets, including RefCOCO, RefCOCO+ [39], and RefCOCOg [22].\nThey are all collected from MS-COCO [16]. RefCOCO contains\n19,994 images and 142,210 reference expressions to represent 50,000\nobjects, which is divided into a validation set, a test set A, and a\ntest set B. RefCOCO+ contains 19,992 images and 141,564 reference\nexpressions to represent 49,856 objects, divided into a validation set,\na test set A, and a test set B. RefCOCOg contains 25,799 images and\n95,010 reference expressions to represent 49,822 objects, divided\ninto a validation set, and a test set.",
            "content": "Evaluation Metrics. Following most existing work, for REC, we adopt the classic intersection over union (IoU) as the evaluation metric with the threshold as 0.5. For RES, we use the mean intersection over union (mIoU) between the predicted result and the ground-truth mask as an evaluation metric. Implementation Details. Our experiments are implemented based on PyTorch by using 4 NVIDIA A100 GPUs. The model is end-toend optimized by AdamW [19] and the weight decay is 1 104. The resolution of the input image is resized to 448 448 and the referring expressions are padded or truncated to 40 tokens. The initial learning rate of the backbone encoder is 5 106, the initial learning rate of the self-attention and FFN in the first 12 layers of the decoder is 1 105, and the initial learning rate of the rest of the model is 2.5 105. We use the pre-trained BERT-base to initialize the linguistic backbone, the pre-trained ViTDet [13] to initialize the self-attention and FFN in the first 12 layers of the visual backbone, and use uniform distribution to initialize the parameters of the rest of the model. We use distributed training, with 20 pairs of samples on each GPU, total batch size of 80, and total epochs of 150. Figure 4: The structure of collaborative multi-task Head. the localization center of the REC task and the prediction mask of the RES task, it is expected to better boost the mutual predictions for these two sub-tasks. Besides, as we know, convolution-based network structures inherently introduce the bias prior, which is beneficial for capturing the locally consistent information of images and then helps the regression prediction. Motivated by these understandings, we build collaborative multi-task head on the basis of convolution layers, as illustrated in Fig. 4. For the REC task, three outputs are predicted, including the coarse-grained prediction of the to-be-identified objects center position, the offsets of the objects real center position from the center position; and the objects size (width and height). For the RES task, we need to classify each vision token and then expand each token into an image patch to accomplish the final mask prediction. Specifically, given the final visual tokens ğ‘‡ ğ‘“ ğ‘£ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ ğ· output by the visual backbone, we divide it into two elements ğ‘‡ ğ‘“1 ğ‘£ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ ğ·/2 and ğ‘‡ ğ‘“2 ğ‘£ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ ğ·/2 along the channel dimension. By separately feeding them into convolutional layer and then adopting different 3 3 convolutional layers for further information propagation, we can obtain the outputs as ğ‘‡ğ‘ ğ‘’ğ‘”, ğ‘‡ğ‘ğ‘¡ğ‘Ÿ , ğ‘‡ğ‘œ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ , and ğ‘‡ğ‘ ğ‘–ğ‘§ğ‘’ , respectively. For the RES task, we input ğ‘‡ğ‘ ğ‘’ğ‘” to convolution layer with convolution kernel of 5 5 and then obtain the segmentation mask Ë†ğ‘€ Rğ» ğ‘Š . Similarly, for the REC task, through the convolutional operations as shown in Fig. 4, we can correspondingly get the probability map Ë†ğ‘ƒ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ , offset Ë†ğ‘‚ Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ 2, and the size Ë†ğ‘† Rğ» /ğ‘ƒ ğ‘Š /ğ‘ƒ 2. Finally, the bounding box Ë†ğµ = ( Ë†ğ‘¥, Ë†ğ‘¦, Ë†ğ‘¤, Ë†â„) is predicted as: Ë†ğ‘¥ = ğ‘¥ğ‘‘ + Ë†ğ‘‚ [ğ‘¥ğ‘‘, ğ‘¦ğ‘‘, 1], Ë†ğ‘¦ = ğ‘¦ğ‘‘ + Ë†ğ‘‚ [ğ‘¥ğ‘‘, ğ‘¦ğ‘‘, 2], Ë†ğ‘¤ = Ë†ğ‘† [ğ‘¥ğ‘‘, ğ‘¦ğ‘‘, 1], Ë†â„ = Ë†ğ‘† [ğ‘¥ğ‘‘, ğ‘¦ğ‘‘, 2], (5) where (ğ‘¥ğ‘‘, ğ‘¦ğ‘‘ ) = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ (ğ‘¥,ğ‘¦) [ğ‘¥, ğ‘¦] represents the element index. Ë†ğ‘ƒ [ğ‘¥, ğ‘¦], ğ‘¥ = 1, . . . , ğ» /ğ‘ƒ, ğ‘¦ = 1, . . . ,ğ‘Š /ğ‘ƒ."
        },
        {
            "title": "4.2 Experimental Comparisons\nHere we conduct comprehensive comparisons based on traditional\nsetting and pre-trained setting where different methods are first\ntrained on a large corpus of visual grounding data which consists",
            "content": "5 Table 1: Comparison with state-of-the-art methods on RefCOCO [39], RefCOCO+ [39], and RefCOCOg [24] for REC task. We highlight the best results of traditional setting and pre-trained setting in blue and red. Method Venue Backbone Multi-task RefCOCO RefCOCO+ RefCOCOg val test test val test test val(U) test(U) Traditional setting MCN [21] LBYL [7] TransVG [3] TRAR [41] SeqTR [42] CLIP-VG [34] VG-LAW [28] ScanFormer [27] PVD [2] SegVG [10] HiVG [32] EEVG [1] PLVL CVPR2020 CVPR2021 ICCV2021 ICCV2021 CVPR2022 TMM2023 CVPR2023 CVPR2024 AAAI2024 ECCV2024 ACMMM 2024 ECCV2024 Ours Pre-trained setting RefTr [12] SeqTR [42] PolyFormer [17] EEVG [1] HiVG [32] OneRef [33] PLVL NeurIPS2021 CVPR2022 CVPR2023 ECCV2024 ACMMM 2024 NeurIPS2024 Ours DarkNet53 DarkNet53 ResNet101 DarkNet53 DarkNet53 ViT-B ViT-B ViLT Swin-B DETR-Res101 ViT-B ViT-B ViT-B ResNet101 DarkNet53 Swin-B ViT-B CLIP-B BEiT3-B ViT-B (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) 80.08 79.67 81.02 - 81.23 84.29 86.62 83.40 84.52 86.84 87.32 88.08 89.02 85.65 87.00 89.73 90.47 90.56 91.89 92.65 82.29 82.91 82.72 81.40 85.00 87.76 89.32 85.86 87.64 89.46 89.86 90.33 90.21 88.73 90.15 91.73 92.73 92.55 94.31 94.72 74.98 74.15 78.35 78.60 76.08 78.43 83.16 79.81 79.63 83.07 83.27 85.50 86.72 81.16 83.59 86.03 87.72 87.23 88.58 89. 67.16 68.64 64.82 - 68.82 69.55 76.37 72.96 73.89 77.18 78.06 77.97 79.19 77.55 78.69 83.73 81.79 83.08 86.38 87.36 72.86 73.38 70.70 69.10 75.37 77.33 81.04 77.57 78.41 82.63 84.81 82.44 84.97 82.26 84.51 88.60 87.80 89.21 90.38 91.04 57.31 59.49 56.94 56.10 58.78 57.62 67.50 62.50 64.25 67.59 68.11 69.15 71.55 68.99 71.87 76.38 74.94 76.68 79.47 81. 66.46 - 68.67 68.90 71.35 73.18 76.90 74.10 73.81 78.35 78.29 79.60 81.58 79.25 82.69 84.46 85.19 86.52 86.82 89.20 66.01 - 67.73 68.30 71.58 72.54 76.96 74.14 74.13 77.42 78.79 80.24 81.16 80.01 83.37 84.96 84.72 56.62 87.32 89.80 Table 2: Comparison with state-of-the-art methods on RefCOCO [39], RefCOCO+ [39], and RefCOCOg [24] for RES task. We highlight the best results of traditional setting and pre-trained setting in blue and red. Method Venue Backbone Multi-task RefCOCO RefCOCO+ RefCOCOg val test test val test test val(U) test(U) Traditional setting MCN [21] CRIS [31] SeqTR [42] LAVT [37] VG-LAW [28] PVD [2] EEVG [1] PLVL CVPR2020 CVPR2022 CVPR2022 CVPR2022 CVPR2023 AAAI2024 ECCV2024 Ours DarkNet53 CLIP-ResNet50 DarkNet53 Swin-B ViT-B Swin-B ViT-B ViT-B Pre-trained setting RefTr [12] SeqTR [42] PolyFormer [17] EEVG [1] OneRef [33] PLVL NeurIPS2021 CVPR2022 CVPR2023 ECCV2024 NeurIPS2024 Ours ResNet101 DarkNet53 Swin-B ViT-B BEiT3-B ViT-B (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 62.44 69.52 67.26 74.46 75.62 74.82 78.23 78.91 74.34 71.70 75.96 79.49 79.83 81.89 64.20 72.72 69.79 76.89 77.51 77.11 79.27 79. 76.77 73.31 77.09 80.87 81.86 82.95 59.71 64.70 64.12 70.94 72.89 69.52 76.58 77.61 70.87 69.82 73.22 77.39 76.99 80.04 50.62 61.39 54.14 65.81 66.63 63.38 69.04 69.79 66.75 63.04 70.65 71.86 74.68 76.68 54.99 67.10 58.93 70.97 70.38 68.60 72.65 74. 70.58 66.73 74.51 76.67 77.90 79.67 44.69 52.48 48.19 59.23 59.89 56.92 62.33 63.03 59.40 58.97 64.64 66.31 69.58 72.65 49.22 59.87 55.67 63.34 65.53 63.13 69.15 70.21 66.63 64.69 69.36 73.56 74.06 77.25 49.40 60.36 55.64 63.62 66.08 63.62 70.01 70. 67.39 65.74 69.88 73.47 74.92 77.67 of RefCOCO, RefCOCO+, RefCOCOg, Visual Genome [11] and Flickr30k [25] and then fine-tuned on the benchmark dataset. Evaluation under Traditional setting. For REC and RES tasks, the quantitative results under the conventional training setting are reported in the upper part of Table 1 and the upper part of Table 2, 6 Figure 5: Qualitative results on the RefCOCO. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task. Figure 6: Qualitative results on the RefCOCO+. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task. respectively. As seen, for these two different tasks, our proposed PLVL consistently achieves superior performance on these three benchmark datasets. These results finely substantiate the rationality of our proposed language-guided progressive visual learning mechanism and the multi-task collaborative prediction strategy. Fig. 5, Fig. 6, and Fig. 7 present the visual results on different samples which are randomly selected from these three benchmark datasets. As observed, our proposed method can detect and segment the referred objects accurately. Besides, we can see that the maximum response location in the score map output by the center branch finely aligns with the center of the mask, which complies with the design motivation of the multi-task head. Evaluation under Pre-trained setting. For comprehensive comparison, we provide the results of different methods on the REC and the RES tasks under the pre-trained setting, which are listed in the lower part of Table 1 and the lower part of Table 2, respectively. We can see that the pre-trained procedure assists different approaches in achieving better performance, and our PLVL consistently outperforms other baselines on different datasets. Table 3: Computational cost, inference time, and number of parameters of different state-of-the-art methods. means lower is better. The batch size is 1 and all experiments are conducted in an A100 GPU. PolyFormer [17] EEVG [1] Method GPLOPs Runtime (ms) #Param - 82.72 309.73M 75.22G 50.04 PLVL 75.04G 45.96 218.07M 218.02M Comparisons on Inference Time. Table 3 reports the comparison with existing state-of-the-art (SOTA) open-sourced methods in terms of inference time and computational cost. Especially, compared to the latest SOTA EEVG, we reduce the time overhead by 8% per image. As seen, our PLVL has much lower computational cost and fewer network parameters."
        },
        {
            "title": "4.3 Ablation Studies\nBased on RefCOCOg, we conduct a series of ablation studies to\nvalidate the effectiveness of our proposed method, including the",
            "content": "7 Figure 7: Qualitative results on the RefCOCOg. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task. Table 4: Performance on RefCOCOg validation set with adopting different numbers of global blocks. ğ‘ is the number of local blocks, ğ‘€ is the number of global blocks with language guidance, and Indexes represent the indexes where we insert the language information into the global blocks. ğ‘ ğ‘€ 12 11 10 9 8 3 4 5 6 Indexes 13,14,15 3,13,14,15 3,6,13,14,15 3,6,9,13,14,15 3,6,9,12,13,14,15 REC 80.15 80.02 80.78 81.50 81.58 RES 69.51 69.81 69.24 70.16 70.21 progressive injection of linguistic information, the cooperative potential of REC and RES, and the collaborative multi-head paradigm. Effectiveness of Language-guided Visual Learning. From Fig. 2, the visual backbone of our PLVL consists of 8 local blocks and 7 global blocks with progressively injecting the language guidance information through the cross-attention module. To verify the effectiveness of the proposed progressive visual learning, Table 4 reports the performance with adopting different numbers of global blocks. We can find that decrease in cross-attention modules before 12 layers leads to notable drop in performance for REC and RES. This finding supports the notion that the full use of language information can enhance the extraction of visual features. Effectiveness of Multi-Task Strategy. We compare the performance of the single-task networks with our proposed multitask strategy on the RefCOCOg validation set. Table 5 shows that our multitask strategy can bring significant performance gains of 1.43% on REC and 1.13% on RES. These results demonstrate that welldesigned multi-task approach can leverage the task relationship to further improve the performance of individual tasks. Effectiveness of Collaborative Task Grouping. We compare different collaborative task groupings for RES and REC. Table 6 indicates that grouping {ğ‘šğ‘ğ‘ ğ‘˜, ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ } in one group and {ğ‘œ ğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡, ğ‘ ğ‘–ğ‘§ğ‘’} in another leads to 0.21% performance gain for the REC task and 0.37% performance gain for the RES task. The experimental results Table 5: Performance comparison between multi-task and single-task for training on RefCOCOg validation set. Lğ‘‘ğ‘’ğ‘¡ Lğ‘ ğ‘’ğ‘” (cid:33) (cid:33) (cid:37) REC (cid:33) 81.58 (cid:37) 80.15 (cid:33) - RES 70.21 - 69.08 Table 6: Performance comparison between different head types on RefCOCOg validation set. Head type {mask}{center,offset,size} {mask,center}{offset,size} REC 81.37 81.58 RES 69.84 70.21 Table 7: Performance comparison between EEVG w. or w/o PLVL and CMTH. Head type EEVG EEVG w. CMTH EEVG w. PLVL REC 79.27 79.98 80.35 EEVG w. PLVL & CMTH 81.41 RES 68.16 68.66 69.48 69.78 also demonstrate that the design of multi-task grouping can further enhance the models performance. Flexibility of our proposed PLVL and CMTH. In order to further verify the advantages of our proposed core contribution points, we incorporate our proposed Progressive Language-guided Visual Learning (PLVL) and Collaborative Multi-task Head (CMTH) into EEVG. As shown in Table 7, our proposed PLVL and CMTH can both bring significant performance improvements on EEVG. These experimental results show that the two core contributions we propose have the potential to be applied to other architectures."
        },
        {
            "title": "5 Conclusion\nIn this paper, for multi-task visual grounding, we have proposed a\nnovel Progressive Language-guided Visual Learning (PLVL) frame-\nwork. Not only does it mine the inherent feature representations",
            "content": "8 of the visual modality itself, but it also progressively integrates linguistic information to enhance the learning of language-related visual features. Furthermore, we have carefully investigated the relationship between the two tasks of REC and RES and proposed collaborative multi-task head to enhance the performance of both tasks. Through extensive experiments on several benchmark datasets, the effectiveness of our proposed method has been validated, demonstrating its superiority over existing techniques in terms of both REC and RES. References [1] Wei Chen, Long Chen, and Yu Wu. 2024. An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding. In ECCV. [3] [2] Zesen Cheng, Kehan Li, Peng Jin, Siheng Li, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen. 2024. Parallel vertex diffusion for unified visual grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 13261334. Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. 2021. Transvg: End-to-end visual grounding with transformers. In ICCV. [4] Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. 2021. Encoder fusion network with co-attention embedding for referring image segmentation. In CVPR. [5] Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and Hanwang Zhang. 2019. Learning to compose and reason with language tree structures for visual grounding. TPAMI (2019). [6] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2017. Modeling relationships in referential expressions with compositional modular networks. In CVPR. [7] Binbin Huang, Dongze Lian, Weixin Luo, and Shenghua Gao. 2021. Look before you leap: Learning landmark features for one-stage visual grounding. In CVPR. [8] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. 2020. Referring image segmentation via cross-modal progressive comprehension. In CVPR. [9] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tieniu Tan. 2021. Locate then segment: strong pipeline for referring image segmentation. In CVPR. [10] Weitai Kang, Gaowen Liu, Mubarak Shah, and Yan Yan. 2024. Segvg: Transferring object bounding box to segmentation for visual grounding. In European Conference on Computer Vision. Springer, 5775. [11] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV (2017). [12] Muchen Li and Leonid Sigal. 2021. Referring transformer: one-step approach to multi-task visual grounding. NeurIPS (2021). [24] Varun Nagaraja, Vlad Morariu, and Larry Davis. 2016. Modeling context between objects for referring expression understanding. In ECCV. [25] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-tophrase correspondences for richer image-to-sentence models. In ICCV. [26] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. 2019. Generalized intersection over union: metric and loss for bounding box regression. In CVPR. [27] Wei Su, Peihan Miao, Huanzhang Dou, and Xi Li. 2024. ScanFormer: Referring Expression Comprehension by Iteratively Scanning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1344913458. [28] Wei Su, Peihan Miao, Huanzhang Dou, Gaoang Wang, Liang Qiao, Zheyang Li, and Xi Li. 2023. Language adaptive weight generation for multi-task visual grounding. In CVPR. Jiajin Tang, Ge Zheng, Cheng Shi, and Sibei Yang. 2023. Contrastive grouping with transformer for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2357023580. [29] [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NeurIPS (2017). [31] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. 2022. Cris: Clip-driven referring image segmentation. In CVPR. [32] Linhui Xiao, Xiaoshan Yang, Fang Peng, Yaowei Wang, and Changsheng Xu. 2024. HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding. In ACM Multimedia 2024. https://openreview.net/forum?id=NMMyGy1kKZ [33] Linhui Xiao, Xiaoshan Yang, Fang Peng, Yaowei Wang, and Changsheng Xu. 2024. OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [34] Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, and Changsheng Xu. 2023. CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding. IEEE Transactions on Multimedia (2023). [35] Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo Luo. 2020. Improving one-stage visual grounding by recursive sub-query construction. In ECCV. [36] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. 2019. fast and accurate one-stage approach to visual grounding. In ICCV. [38] [37] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. 2022. Lavt: Language-aware vision transformer for referring image segmentation. In CVPR. Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu Wang, Ji Zhang, Liang He, and Xin Lin. 2022. Shifting more attention to visual backbone: Querymodulated refinement networks for end-to-end visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15502 15512. [39] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. [13] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. 2022. Exploring plain 2016. Modeling context in referring expressions. In ECCV. vision transformer backbones for object detection. In ECCV. [40] Hanwang Zhang, Yulei Niu, and Shih-Fu Chang. 2018. Grounding referring expressions in images by variational context. In CVPR. [41] Yiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. 2021. Trar: Routing the attention spans in transformer for visual question answering. In ICCV. [42] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji. 2022. Seqtr: simple yet universal network for visual grounding. In ECCV. [14] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, and Bo Li. 2020. real-time cross-modality correlation filtering method for referring expression comprehension. In CVPR. [15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2017. Focal loss for dense object detection. In ICCV. [16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV. Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and Manmatha. 2023. PolyFormer: Referring image segmentation as sequential polygon generation. In CVPR. [17] [18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV. Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. [19] [20] Mingcong Lu, Ruifan Li, Fangxiang Feng, Zhanyu Ma, and Xiaojie Wang. 2024. LGR-NET: Language Guided Reasoning Network for Referring Expression Comprehension. IEEE Transactions on Circuits and Systems for Video Technology (2024). [21] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. 2020. Multi-task collaborative network for joint referring expression comprehension and segmentation. In CVPR. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. 2016. Generation and comprehension of unambiguous object descriptions. In CVPR. [22] [23] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV."
        }
    ],
    "affiliations": [
        "East China Normal University, Shanghai, China",
        "Westlake University, Hangzhou, Zhejiang, China",
        "Xian Jiaotong University, Xian, Shaanxi, China"
    ]
}