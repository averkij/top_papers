{
    "paper_title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions",
    "authors": [
        "Bimsara Pathiraja",
        "Maitreya Patel",
        "Shivam Singh",
        "Yezhou Yang",
        "Chitta Baral"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 4 4 3 0 . 6 0 5 2 : r RefEdit: Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions Bimsara Pathiraja* Maitreya Patel* Shivam Singh Yezhou Yang Chitta Baral Arizona State University {bpathir1, maitreya.patel, ssing631, yz.yang, chitta}@asu.edu http://refedit.vercel.app Figure 1. RefEdit is referring expression-based image editing benchmark and finetuned model. Our proposed RefEdit model can accurately identify the entity of interest and perform accurate edits."
        },
        {
            "title": "Abstract",
            "content": "state-of-the-art results comparable to closed-source methods. We release data & checkpoint for reproducibility. Despite recent advances in inversion and instructionbased image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving *These authors contributed equally to this work. 1. Introduction The field of AI for content creation (AICC) stands at the forefront of technological innovation, propelled by unprecedented advances in visual generative models [10, 30, 31, 35, 3739]. Techniques such as diffusion [9, 40] and flowbased models [20, 23, 33] have revolutionized image editing, enabling precise semantic transformations guided by user queries. Early efforts centered on inversion-based strategieslike Prompt-to-Prompt [12], Null-Text Inversion [28], and edit-friendly DDPM-inversion [14] which map reference image latents into noise and deterministically reconstruct the original. While these methods laid the critical groundwork, they are hindered by significant computational demands, prolonged processing times, and the need for extensive hyperparameter tuning. In response, instruction-based approaches, including InstructPix2Pix [5], UltraEdit [49], and OmniEdit [43], have gained prominence by fine-tuning diffusion models on millions of image editing triplets derived from inversion-based techniques, offering more efficient alternative despite inheriting some foundational limitations. These advancements, however, reveal persistent weakness: current methods struggle to maintain precision in complex scenarios, particularly when images feature multiple similar entities (i.e., two cats). Inversion-based strategies, though innovative, are computationally intensive and falter when tasked with isolating specific regions, flaw that propagates to instruction-based models trained on their outputs. This imprecision often results in failed edits or unintended modifications spilling into adjacent areas. Synthetic data has been leveraged extensively across various domains beyond image editing [24, 32]; however, existing pipelines within image editing rarely address multi-object complexity, further exacerbating their shortcomings. As result, the field faces an unresolved bottleneck that limits the practical utility of image editing technologies in real-world applications. We identify this bottleneck as the referring expression challengea problem well-explored in discriminative tasks like segmentation [44] but largely neglected in the generative domain of image editing. To confront this, we introduce RefEdit-Bench, rigorous benchmark designed to evaluate image editing under both straightforward and demanding conditions. By repurposing the RefCOCO dataset [44], we ensure real-world relevance, capturing intricate scenarios with multiple entities. As evidenced in Table 3, leading baselinessuch as MagicBrush [47], InstructDiffusion [11] , and UltraEdit [49] experience marked performance declines on our benchmark, highlighting its complexity. To address this challenge, we propose scalable synthetic data generation pipeline tailored to the referring expression challenge, harnessing GPT-4o [1] to craft sophisticated editing tasks and integrating Grounded Segment Anything [36] with primarily Flux-based inversion-free editing methodology (FlowChef [33]) for precise, controlled edits. Using this pipeline, we fine-tune diffusion modelsStable Diffusion v1.5 [37] and Stable Diffusion 3 [10]to create RefEdit, model that redefines the state-of-the-art in image editing. Extensive experiments, human evaluations, and detailed analyses confirm RefEdits superiority, with our Stable Diffusion v1.5 variant outperforming baselines trained on vastly larger datasets and more advanced architectures like SDXL [34] and SD3. This breakthrough not only overcomes the referring expression challenge but also sets new standard for efficiency and effectiveness in visual generative modeling. In summary, our contributions are: We present RefEdit-Bench, novel benchmark for realistic, referring expression-driven image editing. We introduce novel, FlowChef and Inpaint Anything [45]-powered pipeline for automated, scalable synthetic data generation. Through comprehensive evaluation, we establish RefEdit as the most human-preferred model, achieving exceptional success rates and outperforming existing methods. 2. Related works Inversion-based editing. Inversion-based image editing leverages generative models to modify existing images by mapping them into latent space, applying desired alterations, and then reconstructing the edited images. Promptto-Prompt [12] leverages cross attention maps to edit images by solely modifying the textual prompts. Negative-text Inversion [27] and Edict [42] uses DDIM/DDPM inversions to preserve the original image while editing the image. Other methods like LEdits [41], Sega [4], Null-text inversion [28], Direct Inversion [16] provides more efficient and reliable inversion based image editing methods. Still, the inversion based techniques suffer from leaking the editing to background areas and poor localization abilities. Notably we utilize FlowChef [33] for our pipeline as it provides gradientand inversion-free framework for image editing. Region-based controlled editing. Region-based editing approaches perform image modifications within userprovided masked regions guided by textual instructions. Blended Diffusion [2, 3] facilitates local inpainting based on user-supplied captions and masks. SDEdit [26] leverages pre-trained models to iteratively apply noise and denoising to real images guided by text prompts. DiffEdit [8] integrates DDIM inversion for automatic mask generation to preserve image backgrounds during editing. MAGEdit [25] optimizes latent noise features via mask-based crossattention constraints, progressively improving local alignment with desired textual prompts. Despite their effectiveness, region-based methods require additional user effort in mask annotation. Instruction-based editing. Instruction-based editing methods allow users to modify images solely through textual descriptions of intended changes. InstructPix2Pix [6] utilizes image prompts and GPT-3-generated editing instructions [7], alongside image synthesis techniques from Prompt2Prompt [13]. Subsequently, MagicBrush [47] introduced manually annotated datasets for InstructPix2Pix InstructDifcoupled with fine-tuned model checkpoints. fusion [11] extends beyond editing, incorporating broader computer vision tasks like segmentation and keypoint detection. OmniEdit [43] applies specialist-to-generalist training strategy, consolidating diverse specialized editing capabilities into single instruction-based model. UltraEdit [49] employs large-scale, automatically generated Figure 2. Three samples from each of Easy and Hard categories and the overlayed mask of the interested object are shown. Datasets InstructPix2Pix [6] MagicBrush [47] HQ-Edit [15] UtraEdit [49] InstructDiffusion [11] HIVE [48] OmniEdit [43] RefEdit-Data (ours) entities? generated? #Edits #Editing types Multiple similar Automatic 4 7 7 9+ 6 5+ 7 450K 10K 200K 4M 425K 1.1M 1.2M 20K 5 Use of referring expressions? Avg. #similar entities 1 1 1 1 1 1 1 3 Table 1. Comparison of different image editing datasets. dataset containing approximately 4 million samples for enhanced instruction-based image editing. Even though ReferDiffusion [21] addresses referring expressions based image editing, the training dataset is based on 400 initial images and its data generation pipeline is primarily manual, hindering scalability. Additionally, the work does not provide benchmark for the community to facilitate comparative evaluations. 3. Method This section introduces the RefEdit synthetic data creation framework to train the RefEdit model that can perform complex real-world edits. First, we will detail the preliminaries about diffusion-based instruction-guided image editing. Next, we will present our synthetic data creation pipeline. Finally, we will share our training details. 3.1. Preliminaries Diffusion models are generative models that create images by reversing diffusion process. In the forward process, Gaussian noise is incrementally added to an image across multiple time steps until it becomes pure noise. The reverse process, parameterized by neural network, begins with noise and iteratively denoises it to produce realistic image. The model is trained to predict the noise added at each step, minimizing the loss function: = E[ϵ ϵθ(zt, t, c)2], where zt represents the noisy latent at time t, ϵ is the actual noise, and denotes conditioning information, such as text prompt. (1) Instructpix2pix extends Stable Diffusion to enable textguided image editing by conditioning the diffusion process on both an input image cI and text instruction cT . This modification allows the model to generate edited images that Figure 3. Overview of our pipeline for generating controlled image edits. GPT-4o [29] produces the textual information. Based on that, FLUX [19] and Grounded Segment Anything [36] identify masks based on these expressions. At last, we use FlowChef [33], mask-based inversion-free technique, for editing the images. align with textual descriptions while retaining relevant aspects of the original image. The training objective is adapted to: (cid:3) = EE(x),E(cI ),cT ,ϵN (0,1),t (cid:2) ϵ ϵθ(zt, t, E(cI ), cT )2 2 (2) where E(cI ) is the encoded input image. By leveraging synthetic dataset comprising image pairs and corresponding edit instructions, Instructpix2pix learns to apply precise, instruction-driven modifications, offering an efficient and flexible approach to image editing without requiring perexample fine-tuning. 3.2. RefEdit-Data: pipeline"
        },
        {
            "title": "Synthetic data generation",
            "content": "The central challenge of instruction-based image editing is the data. Therefore, existing works rely on some form of synthetic data creation (see Table 1). One way to create such Figure 4. Two samples from our training dataset. From left to right: the initial image, the bounding box highlighting the object of interest, and the edited image. Figure 5. An overview of keywords in edit instructions. The inner circle depicts the types of edits and outer circle showcases the most frequent words used within each type. editing triplets is inversion-based editing methods, which do not require any training. However, inference-time editing is very ill-posed problem statement. It requires careful hyperparameter selection for each prompt, and inversion is not guaranteed to work. Despite these challenges, existing approaches utilize these methods to create million-scale diverse editing triplets. Furthermore, these inversion-based Editing Tasks Definition Instruction example Describes the new color of an object instance. The initial color may or may not be given. Can the horse which has blue color rein be white? Change the shirt color of the tennis player on right to red."
        },
        {
            "title": "Add object",
            "content": "Describes the object to be replaced with new object in its place Describes new object to add by the location the object should be added."
        },
        {
            "title": "Remove object",
            "content": "Describes the object instance that should be removed. Change texture Describes how to change the texture, pattern or material of an object instance Let the coffee drink be matcha drink. Replace the computer monitor with two green lines on it with computer monitor displaying vibrant beach scene. Put red cap on the elderly man with blue apron. Add bird to the bench next to the bench with the bird on it. Remove the half full mug of beer to the left of the pizza. Get rid of the woman in yellow shirt. Turn the man wearing surfing suit and holding surfboard into silver statue. Let the man with the red t-shirt who is playing on his phone wear stripped red shirt. Table 2. Task Definitions and examples from RefEdit-Bench. editing frameworks fail measurably in referring expressions (see Table 3). Hence, the challenges of inversion-based editing methods limit instruction-based editing and require lot of careful data creation process. Additionally, we found that existing referring expression datasets (e.g., RefMSCOCO) were very difficult for the existing method, and the failure rate was very high, which made them useless. Therefore, we propose new and simplified data generation framework. In Figure 3, we outline the four key steps of our framework. For training data generation we start with generating all the text data. We utilize GPT-4o [29] to generate the image prompt, edited prompt editing instruction, edited object, referring expression, and more expanded version of the same referring expression. We group the image editing task into five different categories, as detailed in Table 2. Importantly, each category focuses on the referring expression. We usually generate two sets of editing information per the same image prompt. In the template, we specify how to generate the image prompts, such as if there are two or more instances of the same object but with some differences. The difference could be due to the color (A white shirt\" or black shirt\"), separate item (person holding knife\" or person holding bag\"), or an activity like in this case (chef cutting vegetables\" or chef stirring the pot\"). We generate 1024 1024 image using FLUX [19]. The referring expression is used to generate the editing instruction, but we ask the LLM to generate descriptive referring expression for ease of mask generation. Grounded SAM [36] uses Segment Anything [17] and Grounding DINO [22] to generate accurate masks of objects using referring expressions. The initial image, the editing instructions, and the mask are used for generating the edited image. We utilized Inpaint Anything [46] for object removals and FlowChef [33] for other editing tasks. Adding object data were created by swapping the initial image and the edited image. With the help of this pipeline, we create 20000+ paired training dataset. Figure 4 shares the three examples of our training dataset. Each image contains multiple similar entities that require some form of referring expression to pinpoint. Figure 5 shows that our RefEdit data is very diverse and well distributed across the editing categories. 3.3. Training Details We use the finetuning configuration of MagicBrush [47] where the training starts with the pre-trained InstructPix2Pix model. To avoid overfitting on our RefEdit-Data and to ensure that our method can generalize to other tasks, we combine MagicBrush data with our own generated data during the training. This results in the 30K amount of combined training data. Specifically, we train the RefEdit Stable Diffusion v1.5 variant on this data following the Eq. (1) for 24 epochs on 2 80 GB NVIDIA A100 GPUs. Similarly, we train RefEdit-SD3 (UltraEdit) for 6000 iterations on 8 80 GB NVIDIA A100 GPUs. Method P2P [12] P2P (Direct Inversion) PnP InstructPix2Pix MagicBrush HIVE InstructDiffusion HQ-Edit OmniEdit* InstructDiffusion-HA RefEdit (ours) CosXLEdit FLUX-OmniEdit UltraEdit RefEdit-SD3 (ours) Easy SCavg PQavg Oavg Hard SCavg PQavg Oavg 2.7 2.96 4.25 3.52 4.18 3.15 4.16 2.58 4.18 5.01 5.47 3.2 1.95 3.68 5.46 5.46 5.84 5.59 5.66 6.10 5.26 4.71 5.39 6.15 5.88 5. 6.73 2.46 6.98 6.36 2.31 2.65 3.55 2.82 3.67 2.42 3.31 2.37 3.68 4.21 4.68 2.88 1.31 3.4 4.99 1.71 2.42 2.45 3.80 4.11 3.22 4.09 2.95 4.07 4.56 4. 3.44 1.55 2.93 4.47 5.3 5.9 5.62 6.02 6.16 5.52 4.94 5.21 6.28 6.1 6.48 6.65 1.92 6.23 6.21 1.66 2.3 2.27 3.22 3.56 2.47 3.16 2.6 3.53 3.75 3. 2.88 0.89 2.59 3.98 Table 3. Modified VIEScore evaluation results on RefEdit benchmark for both Easy and Hard categories. The best value is bolded and the second-best value is underlined. Oavg is the overall VIEScore. GPT-4o is used as the MLLM. 4. RefEdit-Bench 5.1. Experimental setup To evaluate the models performance on more complex and semantically rich image editing tasks, we introduce the RefEdit Benchmark, novel evaluation benchmark dataset designed to assess the capability of models in handling referring expression-based editing instructions. The benchmark leverages images sourced from the RefCOCO dataset [44], which provides rich collection of images annotated with referring expressions. Each image in RefEdit is carefully selected to ensure diversity in scenes, objects, and contexts. Additionally, we manually craft detailed and varied editing instructions for each image, encompassing tasks such as changing color, changing objects, adding objects, removing objects, and changing texture. The benchmark is divided into Easy and Hard categories, in which each category has 100 images. In the Easy category, the images had primarily one object. Even if there were multiple instances of the same object, mostly the interested object occupies significant portion of the image or can easily be identified. In the Hard category, there are multiple instances of the same object, and mostly those instances occupy the same areas of the image, making isolating the correct instance hard for the model. Some chosen examples from both Easy and Hard are shown in Figure 2. 5. Experiments In this section, we first discuss the baselines and evaluation setup. Later, we make extensive comparisons on RefEditBench and PIE-Bench [16] along with human evaluations."
        },
        {
            "title": "Model",
            "content": "SCavg PQavg Oavg InstructPix2Pix MagicBrush InstructDiffusion OmniEdit InstructDiffusion-HA RefEdit (ours) CosXLEdit FLUX-OmniEdit RefEdit-SD3 (ours) 4.00 5.02 4.49 5.18 4.69 5.65 5.12 2.37 6.04 6.62 6.80 5.51 7.22 6.86 6. 7.78 3.59 6.47 3.96 4.79 4.06 5.12 4.54 5.21 5.11 2.14 5.70 Table 4. VIEScore evaluation results on PIE-bench. The best value is bolded and the second-best value is underlined. GPT-4o is used as the MLLM. Baselines. We evaluate multiple instruction-based image editing models across the various base models at scale. We select pre-trained models built on top of Stable Diffusion v1.5, such as InstructPix2Pix, MagicBrush, InstructDiffusion, HQ-Edit, and HIVE. Most of these baselines use the InstructPix2Pix as the base model and finetune it using respective proposed data. Similarly, we train additional baseline OmniEdit on SDv1.5 using the released 1.2M editing dataset. Apart from this, we consider bigger baselines such as CosXLEdit (Stable Diffusion XL), UltraEdit (Stable DifFigure 6. Human preference analysis for image editing, RefEdit-SD3 vs. all. Here, win indicates the winning rate for RefEdit-SD3. fusion 3), and Flux-OmniEdit. As OmniEdit model is not public, we utilize the open-source replication*. We present two variants, RefEdit & RefEdit-SD3, trained on our proposed synthetic data. Evaluation Metrics. We perform extensive evaluations on our proposed RefEdit-Bench and PIE-Bench for referring expression and general tasks, respectively. We incorporate PIE-Bench as it is, which utilizes several different metrics focusing on background preservation and image editing. Importantly, it utilizes the ground truth mask to determine the region of interest. However, as CLIP is not good at referring expression, we instead utilize VIEScore [18] as an alternative evaluation metric for both benchmarks, which is trainingfree visual instruction-guided metric leveraging multimodal LLMs (MLLMs). It contains two metrics: Semantic Consistency (SC) and Perceptual Quality (P Q). SC determines the alignment of the edited image with the editing instruction and determines the authenticity and the naturalness of the image. Following prompting templates from OmniEdit [43], we prompt GPT-4o [29] on SC and Q. And we report the average performance. At last, we calculate the overall score as: = SC Q. To further improve the referring expression ability of GPT-4o, after PIE-Bench, we incorporate the ground truth mask to extract the region of interest. We call this metric Modified VIEScore. 5.2. Quantitative Evaluation RefEdit-Bench Evaluations. We provide the VIEScore based evaluations in Table 3 for easy and hard categories. It can be observed that the RefEdit (SDv1.5) variant consistently outperforms the baselines by significant margins on both categories. Interestingly, MagicBrush trained on small amount of human annotated data performs similar to OmniEdits 1.2 million synthetic data. While RefEdit consistently tops the benchmark. This shows the importance of small but high-quality datasets. Interestingly, bigger models *https://huggingface.co/sayakpaul/FLUX.1-dev-edit-v (CosXLEdit, UltraEdit, etc.) perform subpar even worse than the SDv1.5 based methods and observes the biggest drop. We attribute this behavior to potential overfitting. As existing training datasets do not contain complex image editing often requiring referring expression, larger models tend to overfit. Importantly, adding only 20000 of our synthetic data improves the performance over UltraEdit significantly across both categories. PIE-Bench Evaluations. Apart from RefEdit-Bench, we evaluate the methods on standard image editing tasks using PIE-Bench (see Table 4). It can be observed that RefEdit further improves the performance over the baselines despite Importantly, not containing any additional training data. OmniEdit achieves 5.12 score with the help of 1.2M dataset while RefEdit gets 5.21 with only 20000 additional training data. Our RefEdit-SD3 variant achieves the SOTA performance. Importantly, all methods drop performance on RefEdit-Bench compared to the PIE-Bench. That shows the difficulty of our RefEdit-Bench tasks. 5.3. Qualitative Evaluations. We perform human evaluations, where we ask the human annotators to pick the model that performs the task accurately or whether both models at hand lose or win. Specifically, randomly selected 400 pairs from RefEdit-Bench and asked the annotators to conduct A/B testing. Figure 6 shows the human evaluation results. It can be observed that our RefEdit-SD3 model significantly outperforms all baselines and is preferred consistently. Furthermore, Figure 7 shows the qualitative results from RefEdit-Bench in both Easy and Hard categories. While baselines either leak the edit to background regions or dont perform edits at all, our model RefEdit-SD3 can recognize the object of interest and consistently performs edits while maintaining the background information. We provide more examples in the appendix. Figure 7. Qualitative results on image editing. The top 3 samples are from the Easy category and the bottom 3 samples are from the Hard category. As illustrated, our method attains the SOTA performance on comparison of all the methods. 6. Conclusion In this paper, we introduce RefEdit-Bench, challenging benchmark designed specifically for evaluating image editing models on referring expressions, and RefEdit, novel model that significantly surpasses the existing state-of-theart in this domain. By leveraging carefully designed synthetic data generation pipeline, we demonstrate that training RefEdit on relatively small yet highly specialized dataset effectively outperforms existing baselines trained on considerably larger datasets. Our results underscore the potential of targeted, quality-focused synthetic data for improving model precision in complex editing scenarios. Future research could extend our pipeline to additional editing scenarios and explore further integration with multimodal AI frameworks."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by NSF RI grant #2132724. We thank the NSF NAIRR initiative, Research Computing (RC) at Arizona State University (ASU), and cr8dl.ai for their generous support in providing computing resources. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 1818718197. IEEE, 2022. 2 [3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Transactions on Graphics, 42(4):111, 2023. 2 [4] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. Advances in Neural Information Processing Systems, 36: 2536525389, 2023. 2 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. 2, 3 [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 2 [8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 2 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2 [11] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, and Baining Guo. Instructdiffusion: generalist modeling interface for vision tasks, 2023. 2, 3 [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 1, 2, 6 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. 2 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [15] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. [16] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. 2, 6 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 5 [18] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2024. 7 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. 4, 5 [20] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 1 [21] Chang Liu, Xiangtai Li, and Henghui Ding. Referring image editing: Object-level image editing via referring expressions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1312813138, 2024. 3 [22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 5 [23] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. [24] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: survey. arXiv preprint arXiv:2406.15126, 2024. 2 [25] Qi Mao, Lan Chen, Yuchao Gu, Zhen Fang, and Mike Zheng Shou. Mag-edit: Localized image editing in complex scenarios via mask-based attention-adjusted guidance. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 68426850, 2024. 2 [26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [27] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. 2 [28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 1, 2 [29] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. 4, 5, 7 [30] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. λ-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. ArXiv, abs/2402.05195, 2024. 1 [31] Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: resource-efficient text-to-image prior for image generations. In Proceedings of the IEEE/CVF [45] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Inpaint anything: SegarXiv preprint Wenjun Zeng, and Zhibo Chen. ment anything meets image inpainting. arXiv:2304.06790, 2023. 2 [46] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting, 2023. [47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing, 2024. 2, 3, 5 [48] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. 3 [49] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2025. 2, 3 Conference on Computer Vision and Pattern Recognition, pages 90699078, 2024. 1 [32] Maitreya Patel, Naga Sai Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, et al. Tripletclip: Improving compositional reasoning of clip via synthetic vision-language negatives. Advances in neural information processing systems, 37:3273132760, 2024. 2 [33] Maitreya Patel, Song Wen, Dimitris N. Metaxas, and Yezhou Yang. Steering rectified flow models in the vector field for controlled image generation, 2024. 1, 2, 4, [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2 [35] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1 [36] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 2, 4, 5 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [39] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. 1 [40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 1 [41] Linoy Tsaban and Apolinário Passos. Ledits: Real image editing with ddpm inversion and semantic guidance. arXiv preprint arXiv:2307.00522, 2023. [42] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2253222541, 2023. 2 [43] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199, 2024. 2, 3, 7 [44] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016. 2, 6 Figure 8. Qualitative comparisons with closed-source methods. Table 5. Evaluation results on RefEdit benchmark for both Easy and Hard categories. The best value is bolded and the second-best value is underlined. Structure Background preservation CLIP similarity Model Distance PSNR LPIPS MSE SSIM Whole Edited InstructPix2Pix MagicBrush HIVE InstructDiffusion HQ-Edit OmniEdit* InstructDiffusion-HA CosXLEdit FLUX-Omni-Edit UltraEdit RefEdit RefEdit-SD InstructPix2Pix MagicBrush HIVE InstructDiffusion HQ-Edit OmniEdit* InstructDiffusion-HA CosXLEdit FLUX-OmniEdit UltraEdit RefEdit RefEdit-SD3 E H 0.0305 0.0207 0.0287 0.0400 0.1130 0.0190 0.0252 0.0137 0.0400 0.0120 0.0199 0.0239 0.0435 0.0274 0.0367 0.0400 0.1502 0.0248 0.0226 0.0267 0.0500 0.0144 0.0206 0.0259 21.40 24.39 22.23 22.76 12.03 24.80 24.95 26.60 20.48 26.23 24.81 26. 18.87 20.56 20.01 18.96 10.96 20.80 21.12 21.61 16.97 23.64 21.56 22.15 0.1190 0.0701 0.1169 0.0900 0.3418 0.0645 0.0598 0.0695 0.1300 0.0740 0.0599 0.0572 0.1664 0.1074 0.1601 0.1300 0.4127 0.1005 0.0886 0.1237 0.2100 0.1006 0.0868 0.0911 0.0129 0.0076 0.0104 0.0200 0.0696 0.0070 0.0068 0.0062 0.0200 0.0042 0.0064 0.0069 0.0231 0.0151 0.0173 0.0300 0.0883 0.0140 0.0128 0.0240 0.0300 0.0067 0.0131 0.0152 0.7577 0.8065 0.7485 0.7800 0.4913 0.8116 0.8143 0.8962 0.7800 0.8358 0.8145 0. 0.6775 0.7337 0.6781 0.7000 0.3789 0.7413 0.7495 0.8241 0.6700 0.7743 0.7531 0.8460 24.41 25.39 23.75 24.34 20.48 25.15 24.73 25.21 21.44 25.29 25.48 25.79 25.60 26.59 24.88 25.62 20.88 26.54 26.36 26.65 21.02 27.03 26.74 26.46 20.90 20.94 20.68 20.39 18.33 20.92 20.85 20.79 17.5 20.96 21.07 20.84 19.97 20.21 20.03 19.36 17.8 20.18 19.60 19.91 16.05 19.82 20.30 19.66 Figure 9. Additional training samples. Figure 10. Qualitative results on image editing. The top 4 samples are from the Easy category and the bottom 3 samples are from the Hard category. As illustrated, our method attains the SOTA performance on comparison of all the methods. Figure 11. SC score prompt masked version. Figure 12. PQ score version."
        }
    ],
    "affiliations": [
        "Arizona State University"
    ]
}