{
    "paper_title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs",
    "authors": [
        "Yangning Li",
        "Weizhi Zhang",
        "Yuyao Yang",
        "Wei-Chieh Huang",
        "Yaozu Wu",
        "Junyu Luo",
        "Yuanchen Bei",
        "Henry Peng Zou",
        "Xiao Luo",
        "Yusheng Zhao",
        "Chunkit Chan",
        "Yankai Chen",
        "Zhongfen Deng",
        "Yinghui Li",
        "Hai-Tao Zheng",
        "Dongyuan Li",
        "Renhe Jiang",
        "Ming Zhang",
        "Yangqiu Song",
        "Philip S. Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 2 7 7 4 9 0 . 7 0 5 2 : r Towards Agentic RAG with Deep Reasoning: Survey of RAG-Reasoning Systems in LLMs Yangning Li1*, Weizhi Zhang2*, Yuyao Yang2, Wei-Chieh Huang2, Yaozu Wu3 Junyu Luo4, Yuanchen Bei5, Henry Peng Zou2, Xiao Luo6, Yusheng Zhao4 Chunkit Chan7, Yankai Chen2, Zhongfen Deng2, Yinghui Li1, Hai-Tao Zheng1, Dongyuan Li3, Renhe Jiang3, Ming Zhang4, Yangqiu Song7, Philip S. Yu1 1Tsinghua University 2University of Illinois Chicago 3The University of Tokyo 4Peking University 5University of Illinois Urbana-Champaign 6University of California, Los Angeles 7HKUST ynli23@mails.tsinghua.edu.cn, wzhan42@uic.edu"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multistep inference; conversely, purely reasoningoriented approaches often hallucinate or misground facts. This survey synthesizes both strands under unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (ReasoningEnhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/ DavidZWZ/Awesome-RAG-Reasoning."
        },
        {
            "title": "Introduction",
            "content": "The remarkable progress in Large Language Models (LLMs) has transformed wide array of fields, showcasing unprecedented capabilities across diverse tasks (Zhao et al., 2023). Despite these advancements, the effectiveness of LLMs remains hindered by two fundamental limitations: knowledge hallucinations, due to the static and parametric manner of their knowledge storage (Huang et al., 2025b); and struggles with complex reasoning, especially when tackling real-world problems (Chang et al., 2024). These limitations have driven the * Equal Contribution. 1 development of two major directions: RetrievalAugmented Generation (RAG) (Fan et al., 2024a), which provides LLMs with external knowledge; and various methods aimed at enhancing their inherent reasoning abilities (Chen et al., 2025c). The two limitations are inherently intertwined: missing knowledge can impede reasoning, and flawed reasoning hinders knowledge utilization (Tonmoy et al., 2024). Naturally, researchers have increasingly explored combining retrieval with reasoning, though early work followed two separate, one-way enhancements. The first, Reasoningenhanced RAG (Gao et al., 2023b) (Reasoning RAG), leverages reasoning to improve specific stages of the RAG pipeline. The second path, RAGenhanced Reasoning (Fan et al., 2024a) (RAG Reasoning), supplies external factual grounding or contextual cues to bolster LLM reasoning. While beneficial, the above methods remain bound to static Retrieval-Then-Reasoning (RTR) framework, offering only localized improvements to individual components. Several inherent limitations persist: (1) Retrieval Adequacy and Accuracy cannot be guaranteed; Pre-retrieved knowledge may fail to align with the actual knowledge needs that emerge during reasoning, especially in complex tasks (Zheng et al., 2025; Li et al., 2025d). (2) Reasoning Depth remains constrained. When retrieved knowledge contains errors or conflicts, it can adversely interfere with the models inherent reasoning capabilities (Li et al., 2025b; Chen et al., 2025a). (3) System Adaptability proves insufficient. The RTR framework lacks mechanisms for iterative feedback or dynamic retrieval during reasoning. This rigidity limits its effectiveness in scenarios that require adaptive reasoning, such as open-domain QA or scientific discovery (Xiong et al., 2025; Alzubi et al., 2025). As shown in Figure 1, these shortcomings have catalyzed paradigm shift toward Synergized ReFigure 1: Overview of the RAG-Reasoning System. The Reasoning-Enhanced RAG methods and RAG-Enhanced Reasoning methods represent one-way enhancements. In contrast, the Synergized RAG-Reasoning System performs reasoning and retrieval iteratively, enabling mutual enhancements. trieval and Reasoning within LLMs (RAG Reasoning). These methods support dynamic, iterative interplay where reasoning actively guides retrieval, and newly retrieved knowledge, in turn, continuously refines the reasoning process. This trend is further exemplified by recent Deep Research products from OpenAI1, Gemini2, Perplexity3, and others, which emphasize tightly coupled retrieval and reasoning (Zhang et al., 2025f). These systems employ agentic capabilities to orchestrate multi-step web search and leverage reasoning to comprehensively interpret retrieved content, solving problems demanding in-depth investigation. This survey charts the shift from isolated enhancements to cutting-edge synergized frameworks where retrieval and reasoning are deeply interwoven and co-evolve. While surveys on RAG (Fan et al., 2024a; Gao et al., 2023b) and LLM Reasoning (Chen et al., 2025c; Li et al., 2025e) exist, dedicated synthesis focusing on their integration remains lacking. Our goal is to provide comprehensive overview of how the symbiosis between retrieval and reasoning is advancing LLM capabilities, with particular emphasis on the move towards synergized RAG and Reasoning framework. The survey is structured as follows: Section 2 introduces the background; Section 3 and 4 review two one-way enhancements, respectively. Section 5 1https://openai.com/index/ introducing-deep-research/ 2https://gemini.google/overview/ deep-research/ unifies both lines into synergized RAGReasoning frameworks. Section 6 lists benchmarks, and Section 7 outlines open challenges."
        },
        {
            "title": "2 Background and Preliminary",
            "content": "RAG mitigates knowledge cut-off of LLMs through three sequential stages: (i) Retrieval, fetching taskrelevant content from external knowledge stores; (ii) Integration, deduplicating, resolving conflicts, and re-ranking the retrieved content; and (iii) Generation, reasoning over the curated context to produce the final answer. Concurrently, Chain-ofThought technique has significantly enhanced the reasoning capabilities of modern LLMs by encouraging them to think step by step before answering. The synergy between the structured RAG pipeline and these multi-step reasoning capacities grounds the emerging RAG-Reasoning paradigm explored in this survey."
        },
        {
            "title": "3 Reasoning-Enhanced RAG",
            "content": "Traditional RAG methods first retrieve relevant documents, then concatenate the retrieved knowledge with the original query to generate the final answer. These methods often fail to capture the deeper context or intricate relationships necessary for complex reasoning tasks. By integrating reasoning capabilities across Retrieval, Integration, and Generation stages of the RAG pipeline, the system can identify and fetch the most relevant information, reducing hallucinations and improving response accuracy.4 3https://www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research 4If reasoning only serves to better leverage fixed retrieved knowledge in unidirectional manner, it is considered within 2 Reasoning-Aware Query Reformulation (3.1.1) e.g. Collab-RAG (Xu et al., 2025b), DynQR (Anonymous, 2025), DeepRetrieval (Jiang et al., 2025) Retrieval Optimization (3.1) Retrieval Strategy and Planning (3.1.2) e.g. PAR-RAG (Zhang et al., 2025d), LPKG (Wang et al., 2024b), FIND (Jia et al., 2025) Retrieval Model Enhancement (3.1.3) e.g. GNN-RAG (Mavromatis and Karypis, 2024), RuleRAG (Chen et al., 2024c), Reasoning-enhanced RAG (3) Integration Enhancement (3.2) n e a f v A RAG-enhanced Reasoning (4) Synergized RAGReasoning (5) Generation Enhancement (3.3) External Knowledge Retrieval (4.1) In-Context Retrieval (4.2) Reasoning Workflow (5.1) Agent Orchestration (5.2) Relevance Assessment & Filtering (3.2.1) e.g. SEER (Zhao et al., 2024c), M-RAG-R (Yoran et al., 2024) Information Synthesis & Fusion (3.2.2) e.g. BeamAggR (Chu et al., 2024), DualRAG (Cheng et al., 2025) , CRP-RAG (Xu et al., 2024) Context-Aware Generation (3.3.1) e.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b) Grounded Generation Control (3.3.2) e.g. RARR (Gao et al., 2023a), TRACE (Fang et al., 2024), AlignRAG (Wei et al., 2025b) Knowledge Base (4.1.1) e.g. Premise-Retrieval (Tao et al., 2025), ReaRAG (Lee et al., 2025), CBR-RAG (Wiratunga et al., 2024) Web Retrieval (4.1.2) e.g. ALR2 (Li et al., 2024d) , RARE (Tran et al., 2024), Open-RAG (Islam et al., 2024) Tool Using (4.1.3) e.g. TATU (Li et al., 2024g), TRICE(Qiao et al., 2024), Re-Invoke (Chen et al., 2024a) Prior Experience (4.2.1) e.g. RAP (Kagaya et al., 2024), JARVIS-1 (Wang et al., 2024f), EM-LLM (Fountas et al., 2024) Example or Training Data (4.2.2) e.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023) Chain-based (5.1.1) e.g. IRCoT (Trivedi et al., 2023), Rat (Wang et al., 2024g), CoV-RAG (He et al., 2024a), RAFT (Zhang et al., 2024a) Tree-based (5.1.2) Graph-based (5.1.3) Signle-Agent ( 5.2.1) Multi-Agent ( 5.2.2) ToT e.g. RATT (Zhang et al., 2025a), Tree of Clarifications (Kim et al., 2023), GROVE (Wen et al., 2023), MCTS e.g. AirRAG (Feng et al., 2025), MCTS-RAG (Hu et al., 2025), SeRTS (Hu et al., 2024) Walk-on-Graph: e.g. QA-GNN (Yasunaga et al., 2021), LightRAG (Guo et al., 2024), StructRAG (Li et al., 2024h) Think-on-Graph: e.g. ToG (Sun et al., 2024b), ToG-2.0 (Ma et al., 2024a), Graph-CoT (Jin et al., 2024), Prompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023), INTERS (Zhu et al., 2024); RL: e.g. Search-R1 (Jin et al., 2025) R1-Searcher (Song et al., 2025) Decentralized: e.g. M-RAG (Wang et al., 2024e), MDocAgent (Han et al., 2025), Agentic reasoning (Wu et al., 2025c) Centralized: e.g. HM-RAG (Liu et al., 2025), SurgRAW (Low et al., 2025), Chain of Agents (Zhang et al., 2024c) Figure 2: Taxonomy of Recent Advances in RAG-Reasoning System."
        },
        {
            "title": "3.1 Retrieval Optimization",
            "content": "Retrieval optimization leverages reasoning to improve result relevance and quality. Existing methods are broadly categorized (1) Reasoning-Aware Query Reformulation, (2) Retrieval Strategy and Planning, and (3) Retrieval Model Enhancement."
        },
        {
            "title": "3.1.2 Retrieval Strategy and Planning\nThis section covers global retrieval guidance. Ad-\nvance planning uses a reasoning model to gener-\nate a complete retrieval blueprint prior to execu-\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\nfor multi-step planning, mitigating local optima.\nLPKG (Wang et al., 2024b) fine-tunes LLMs on\nknowledge graphs to encode relational structure. In\ncontrast, adaptive retrieval decision methods make\na one-step prediction on whether and how to re-\ntrieve. FIND (Jia et al., 2025) and adaptive RAG",
            "content": "3.3. In contrast, if reasoning dynamically triggers new retrieval, it is discussed in 5. (Jeong et al., 2024) use classifiers to assess query complexity and select retrieval strategies, reducing unnecessary calls. Marina et al. (2025) further adds features like entity popularity and question type."
        },
        {
            "title": "3.1.3 Retrieval Model Enhancement",
            "content": "A line of work enhances retrievers with reasoning via two strategies. The first one leverages structured knowledge: GNN-RAG (Mavromatis and Karypis, 2024) encodes knowledge graphs with GNNs for implicit multi-hop reasoning, while RuleRAG (Chen et al., 2024c) appends symbolic rules to guide retrieval toward logical consistency. Another strategy integrates explicit reasoning: Ji et al. (2024) combines CoT with the query to improve intermediate knowledge recall in multi-hop QA. 3."
        },
        {
            "title": "Integration Enhancement",
            "content": "Integration enhancement uses reasoning to assess relevance and merge heterogeneous evidence, preventing irrelevant content from disrupting generation. Methods fall into two categories: (1) relevance assessment and (2) information synthesis."
        },
        {
            "title": "3.2.1 Relevance Assessment & Filtering",
            "content": "These methods assess the relevance of each retrieved fragment to the user query through deeper reasoning. SEER (Zhao et al., 2024c) employs assessor experts to select faithful, helpful, and concise evidence while discarding irrelevant content. Yoran et al. (2024) improves robustness by filtering non-entailing passages using an NLI model, then 3 fine-tuning the LLM on mixed relevant/irrelevant contexts to help it ignore residual noise."
        },
        {
            "title": "3.3 Generation Enhancement",
            "content": "Even with retrieved context, traditional RAG may still generate unfaithful content without reasoning. Reasoning during generation addresses this issue through two main approaches: (1) context-aware synthesis and (2) grounded generation control."
        },
        {
            "title": "3.3.2 Grounded Generation Control\nGrounded generation control introduces verifica-\ntion mechanisms to ensure outputs remain an-\nchored to retrieved evidence through reasoning.\nFact verification methods use reasoning to assess\nfactual consistency between generated content and\nretrieved evidence, e.g., Self-RAG (Asai et al.,\n2023) introduces reflection markers during decod-\ning to trigger critical review and correction. Cita-\ntion generation links generated content to source\nmaterials to enhance traceability and credibility, as",
            "content": "in RARR (Gao et al., 2023a), which inserts citations while preserving stylistic coherence. Faithful reasoning ensures that each reasoning step adheres to retrieved evidence without introducing unverified content. TRACE (Fang et al., 2024) builds knowledge graphs to form coherent evidence chains, while AlignRAG (Wei et al., 2025b) applies criticism alignment to refine reasoning paths."
        },
        {
            "title": "4 RAG-Enhanced Reasoning",
            "content": "in-context Integrating external knowledge or knowledge during reasoning can help LLMs reduce hallucinations and bridge logical gaps. External retrieval leverages structured sources like databases or web content, providing factual grounding, like IAG (Zhang et al., 2023). In-context retrieval utilizes internal contexts like prior interactions or training examples, enhancing contextual coherence, like RA-DT (Schmied et al., 2024). Both strategies collectively improve factual accuracy, interpretability, and logical consistency of reasoning processes."
        },
        {
            "title": "4.1 External Knowledge Retrieval",
            "content": "External knowledge retrieval incorporates web content, database information, or external tools into reasoning, effectively filling knowledge gaps. Targeted retrieval improves factual accuracy, enabling language models to reliably address complex queries by grounding reasoning steps in verified external evidence."
        },
        {
            "title": "4.1.1 Knowledge Base",
            "content": "Knowledge base (KB) typically stores arithmetic, commonsense, or logical knowledge in databases, books, or documents, with retrieval approaches varying by task. For question answering (QA) reasoning, AlignRAG (Wei et al., 2025b), MultiHopRAG (Tang and Yang, 2024), and CRP-RAG (Xu et al., 2025a) retrieve interconnected factual entries from general KBs to enhance sequential reasoning. In specialized reasoning tasks, mathematical approaches like Premise-Retrieval (Tao et al., 2025) and ReaRAG (Lee et al., 2025) utilize formal lemmas from theorem libraries for structured deduction; legal approaches like CASEGPT (Yang, 2024) and CBR-RAG (Wiratunga et al., 2024) extract judicial precedents for analogical reasoning. For code generation tasks, CodeRAG (Li et al., 2025a) and Koziolek et al. (2024) access code snippets from repositories, ensuring syntactic correctness."
        },
        {
            "title": "4.1.2 Web Retrieval",
            "content": "Web retrieval accesses dynamic online content like web pages, news or social media. Specifically, in fact-checking tasks, approaches such as VeraCT Scan (Niu et al., 2024), Ragar (Khaliq et al., 2024), PACAR (Zhao et al., 2024b), and STEEL (Li et al., 2024b) verify claims step-by-step using evidence from news or social media, enhancing logical reasoning. Meanwhile, QA-based reasoning like RARE (Tran et al., 2024), RAG-Star (Jiang et al., 2024), MindSearch (Chen et al., 2024b), and OPEN-RAG (Islam et al., 2024) iteratively refine reasoning with broad web content, aligning with current trends in agentic search, which involve synthesizing complex online materials to enhance context-aware and robust reasoning. Conversely, in specialized areas like medical reasoning, approaches such as FRVA (Fan et al., 2024b) and ALR2 (Li et al., 2024d), retrieve literature for accurate diagnostics."
        },
        {
            "title": "4.1.3 Tool Using",
            "content": "Tool-using approaches leverage external resources like calculators, libraries, or APIs to enhance reasoning interactively. In QA-based reasoning, ReInvoke (Chen et al., 2024a), AVATAR (Wu et al., 2024), ToolkenGPT (Hao et al., 2023), and ToolLLM (Qin et al., 2023) invoke calculators or APIs (e.g., Yahoo Finance, Wikidata), improving numerical accuracy and factual precision. Within the context of scientific modeling, SCIAGENT (Ma et al., 2024b) and TRICE (Qiao et al., 2024) integrate symbolic computation tools (e.g., WolframAlpha), strengthening computational robustness. Similarly, in mathematical computation, llm-tool-use (Luo et al., 2025b) autonomously employs calculators for accurate numerical reasoning. Distinctively in code generation tasks, RAR (Dutta et al., 2024) retrieves code documentation via OSCAT libraries, ensuring syntactic accuracy and executable logic. 4.2 In-context Retrieval In-context retrieval leverages models internal experiences or retrieved examples from demonstrations and training data to guide reasoning. This retrieval provides relevant exemplars, guiding models to emulate reasoning patterns and enhancing accuracy and logical coherence in novel questions."
        },
        {
            "title": "4.2.1 Prior Experience",
            "content": "Prior experience refers to past interactions or successful strategies stored in models internal memIn tasks inory, with retrieval varying by task. volving planning and decision-making tasks such as robot path finding, RAHL (Sun et al., 2024a) and RA-DT (Schmied et al., 2024) leverage past decisions and reinforcement signals for sequential reasoning. For interactive reasoning tasks, JARVIS1 (Wang et al., 2024f), RAP (Kagaya et al., 2024), and EM-LLM (Fountas et al., 2024) dynamically recall multimodal interactions and conversational histories, facilitating adaptive reasoning for personalized interactions. In the domain for logical reasoning, CoPS (Yang et al., 2024a) retrieves structured prior cases like medical and legal cases for robust logical reasoning in medical and legal scenarios."
        },
        {
            "title": "5 Synergized RAG-Reasoning",
            "content": "Many real-world problems, such as open-domain question answering (Yang et al., 2015; Chen and Yih, 2020) and scientific discovery (Lu et al., 2024; Wang et al., 2023; Baek et al., 2024; Schmidgall et al., 2025), require an iterative approach where new evidence continuously informs better reasoning and vice versa. single retrieval step may not provide sufficient information, and single round of reasoning may overlook key insights (Trivedi et al., 2023). By tightly integrating retrieval and reasoning in multi-step, interactive manner, these systems can progressively refine both the search relevance of retrieved information and the reasoningbased understanding of the original query. We focus on two complementary perspectives within existing approaches: reasoning workflows, which emphasize structured, often pre-defined inference formats for multi-step reasoning; and agent orches5 tration, which focus on how agents interact with environment and coordinate with each others. et al., 2025) integrates adaptive MCTS retrieval to refine evidence and reduce hallucinations."
        },
        {
            "title": "5.1.3 Graph-based",
            "content": "Broadly, the reasoning workflows can be categorized as chain-based, tree-based, or graph-based, reflecting an evolution from linear reasoning chains to branching and expressive reasoning structures."
        },
        {
            "title": "5.1.1 Chain-based",
            "content": "Chain-of-Thought (CoT) (Wei et al., 2022) structures the reasoning process as linear sequence of intermediate steps. However, relying solely on the parametric knowledge of LLMs can lead to error propagation. To solve this, IRCoT (Trivedi et al., 2023) and Rat (Wang et al., 2024g) interleave retrieval operations between reasoning steps. Several recent methods further improve the robustness and rigor of this chain-based paradigm via verification and filtering. CoV-RAG (He et al., 2024a) introduces chain-of-verification that checks and corrects each reasoning step against retrieved references. To combat noisy or irrelevant context, approaches like RAFT (Zhang et al., 2024a) fine-tune LLMs to ignore distractor documents, while Chainof-Note (Yu et al., 2024) prompts the model to take sequential reading notes on retrieved documents to filter out unhelpful information."
        },
        {
            "title": "5.1.2 Tree-based",
            "content": "Tree-based reasoning methods typically adopt either Tree-of-Thought (ToT) (Yao et al., 2023a) or Monte Carlo Tree Search (MCTS) (Browne et al., 2012) approaches. ToT extends the CoT to explicitly construct deterministic reasoning tree and branch multiple logical pathways. Examples include RATT (Zhang et al., 2025a), which construct retrieval-augmented thought trees to simultaneously evaluate multiple reasoning trajectories. Such ToT principles avoid LLM being trapped by an early mistaken assumption and have been applied to address ambiguous questions (Kim et al., 2023), to cover different diagnostic possibilities (Yang and Huang, 2025), and to create complex stories (Wen et al., 2023). Conversely, MCTSbased approaches like AirRAG (Feng et al., 2025), MCTS-RAG (Hu et al., 2025), and SeRTS (Hu et al., 2024) employ probabilistic tree search, dynamically prioritizing exploration based on heuristic probabilities. To ensure retrieval and reasoning quality, AirRAG (Feng et al., 2025) incorporates self-consistency checks, and MCTS-RAG (Hu Walk-on-Graph methods mainly rely on graph learning techniques for the retrieval and reasoning. For example, PullNet (Sun et al., 2019), QA-GNN (Yasunaga et al., 2021), and GreaseLM (Zhang et al., 2022b) directly integrate graph neural networks (GNNs) to iteratively aggregate information from neighbor nodes, excelling at modeling the intricate relationships inherent in graph-structured data. Methods such as SR (Zhang et al., 2022a), LightRAG (Guo et al., 2024), and StructRAG (Li et al., 2024h) employ lightweight graph techniques such as vector indexing and PageRank to efficiently retrieve and reason in multihop context, providing the LLM with high-quality, structured content tailored for the queries. In contrast, Think-on-Graph methods integrate graph structures directly into the LLM reasoning loop, enabling dynamic and iterative retrieval and reasoning processes guided by the LLMs themselves. In the Think-on-Graph (ToG) framework (Sun et al., 2024b; Ma et al., 2024a), the LLM uses the KG as reasoning playground: at each step, it decides which connected entity or relation to explore next, gradually building path that leads to the answer. While Graph-CoT (Jin et al., 2024) introduces three-stage iterative loop (reasoning, graph interaction, and execution), KGP (Wang et al., 2024d) prioritize first constructing document-level KG, both enabling LLM-driven graph traversal agent to navigate passages in each step with globally coherent context. GraphReader (Li et al., 2024f) further refines this paradigm by coupling LLM reasoning with explicit subgraph retrieval and evidence anchoring at each step"
        },
        {
            "title": "5.2 Agent Orchestration",
            "content": "According to agent architectures (Luo et al., 2025a), we organize existing work into single-agent and multi-agent. Particularly, we have attached recent advances in agentic deep research and implementations in Appendix B."
        },
        {
            "title": "5.2.1 Single-Agent",
            "content": "Single agentic system interweaves knowledge retrieval (search) into an LLMs reasoning loop, enabling dynamic information lookup at each step of problem solving and incentivizing it to actively seek out relevant evidence when needed. 6 The ReAct (Yao et al., 2023b) paradigm and its derivatives (Li et al., 2025b; Alzubi et al., 2025) have pioneered this prompting strategy by guiding LLMs to explicitly alternate between reasoning steps and external tool interactions, such as database searches. Different from ReAct that separates reasoning and action, with explicit commands like search triggering external retrieval, methods such as Self-Ask (Press et al., 2023) and IRCoT (Trivedi et al., 2023) prompt the model to recursively formulate and answer sub-questions, enabling interleaved retrieval within the Chain-ofThought (step-by-step retrieval and reasoning). Involving self-reflection strategies, DeepRAG (Guan et al., 2025) and Self-RAG (Asai et al., 2024) empower LLMs to introspectively assess their knowledge limitations and retrieve only when necessary. Rather than relying solely on prompting or static retrievers, Toolformer (Schick et al., 2023) and INTERS (Zhu et al., 2024) represent complementary approach via supervised fine-tuning (SFT) LLMs on instruction-based or synthetic datasets that interleave search and reasoning. Synthetic data generation (Schick et al., 2023; Mao et al., 2024; Zhang et al., 2024a) aims to create large-scale, diverse, and task-specific datasets for search without the need for extensive human annotation. In contrast, instruction-based data reformulation (Zhu et al., 2024; Wang et al., 2024a; Lin et al., 2023; Nguyen et al., 2024) repurposes existing datasets into instructional formats to fine-tune models for improved generalization and alignment with humanlike reasoning. INTERS (Zhu et al., 2024) exemplifies this approach by introducing SFT dataset encompassing 20 tasks, derived from 43 distinct datasets with manually written templates. Reinforcement learning (RL)-incentivized approaches provides mechanism to optimize answer quality via reward signals on incentivizing agents behaviors what to search, how to integrate retrieved evidence, and when to stop, aiming at complex knowledge-intensive tasks (or deep research questions). Notable efforts like WebGPT (Nakano et al., 2021) and RAG-RL (Huang et al., 2025a) focus on improving reasoning fidelity by rewarding outputs based on factual correctness or human preference. More recent contributions operate directly in dynamic environments (e.g., live web search, local search tools), training agents to explore, reflect, and self-correct in noisy real-world conditions. For example, Search-R1 (Jin et al., 2025) learns to generate <search> token during reasoning and concurrently R1-Searcher (Song et al., 2025) builds on RL-driven search demonstrating strong generalization across domains. Deep-Researche (Zheng et al., 2025) make step further by introducing the first end-to-end RL-trained research agent that interacts with the open web. These settings showcase emergent capabilities, like decomposition, iterative verification, and retrieval planning, that supervised methods often hard to instill. Moreover, ReSearch (Chen et al., 2025b) and ReARTeR (Sun et al., 2025c) tackle deeper challenge: not just producing correct answers, but aligning reasoning steps with both factuality and interpretability."
        },
        {
            "title": "5.2.2 Multi-Agent",
            "content": "The exploration of multi-agent collaboration within RAG and reasoning has led to diverse orchestrations: centralized architectures (harness collective intelligence from workers-manager paradigm) and decentralized architectures (leverage complementary capabilities from role-specialized agents). Decentralized architectures deploy multiple agents to collaboratively perform retrieval, reasoning, and knowledge integration, aiming to broaden coverage of relevant information and fully exploit the heterogeneous strengths of specialized agents. Wang et al. (2024e) and Salve et al. (2024) introduce multi-agent systems where each agent retrieves from partitioned database or specific data source (relational databases, NoSQL document stores, etc.). Beyond retrieval, Collab-RAG (Xu et al., 2025b) and RAG-KG-IL (Yu and McQuade, 2025) integrate different model capacities and assign them different roles in reasoning and knowledge integration. This philosophy extends to multimodal settings as in MDocAgent (Han et al., 2025), which employs team of text and image agents to process and reason the document-based QA. general formulation is seen in Agentic reasoning (Wu et al., 2025c), which unites tool-using agents for search, computation, and structured reasoning, orchestrated to solve complex analytical tasks. Centralized architectures structure agents in hierarchical centralized patterns, supporting efficient task decomposition and progressive refinement. HM-RAG (Liu et al., 2025) and SurgRAW (Low et al., 2025) both employ decomposer-retrieverdecider architectures, where different agent roles isolate subproblems such as multimodal processing or surgical decision-making. Wu et al. (2025a) and Iannelli et al. (2024) emphasize dynamic routing and system reconfiguration, respectivelyenabling 7 intelligent agent selection based on task relevance or resource constraints. Chain of Agents (Zhang et al., 2024c) and the cooperative multi-agent control framework for on-ramp merging (Zhang et al., 2025c) illustrate hierarchical agent designs where layered processing enables long-context summarization or policy refinement. Collectively, these works demonstrate how centralized control and hierarchical pipelining foster efficiency and adaptability in multi-agent RAG-reasoning systems."
        },
        {
            "title": "6 Benchmarks and Datasets",
            "content": "Benchmarks and datasets for simultaneously evaluating knowledge (RAG) and reasoning capability cover wide range of complexities, from basic fact retrieval to intricate multi-step reasoning in general or specific domains. We categorize notable benchmarks in several tasks and list them in Table 1 and highlight their details and properties. These representative tasks include Web browsing, such as BrowseComp (Wei et al., 2025a), singlehop QA, such as TriviaQA (Joshi et al., 2017), multi-hop QA, such as HotpotQA (Yang et al., 2018), multiple-choice QA, such as MMLU-Pro (Wang et al., 2025b), mathematics, such as MATH (Hendrycks et al., 2021), and code-centric evaluations from LiveCodeBench (Jain et al., 2024). More tasks can refer to Appendix and Table 2."
        },
        {
            "title": "7 Future Work",
            "content": "Future research directions for Synergized RAGReasoning systems center around enhancing both reasoning and retrieval capabilities to meet realworld demands for accuracy, efficiency, trust, and user alignment. We outline several key challenges and opportunities below. Reasoning Efficiency. Despite their advantages in complex reasoning, Synergized RAG-Reasoning systems can suffer significant latency due to iterative retrieval and multi-step reasoning loops (Sui et al., 2025). For instance, executing single deep research query can take over 10 minutes in practical settings. This issue is especially pronounced in chain-based workflows discussed in Section 5. Future research should explore reasoning efficiency through latent reasoning approaches and strategic control over reasoning depth via thought distillation and length-penalty (Xia et al., 2025a; Zhang et al., 2025b). Beyond reasoning itself, emerging directions in models compression like quantization, pruning, and knowledge distillation is worth to explore for efficient small RAG-reasoning systems. Retrieval Efficiency. On the retrieval side, efficiency demands budget-aware query planning and memory-aware mechanisms that cache prior evidence or belief states to reduce redundant access (Zhao et al., 2024a). Additionally, adaptive retrieval control, learning when and how much to retrieve based on uncertainty signals can reduce wasteful operations. These technical paths push the system beyond static RAG, toward dynamic self-regulation of efficient retreival behaviors under real-world constraints. Human-Agent Collaboration. Many applications of RAG-Reasoning, such as literature reviews or interactive programming, are inherently personalized and cannot assume users know precisely what to ask or how to process retrieved results (Sun et al., 2025b). Corresponding to Section 5.2, humans can act as advanced agents, providing nuanced feedback to steer reasoning processes. Future systems should develop methods for modeling user intent under uncertainty (Zhang et al., 2025e; Yang et al., 2025), building interactive interfaces for iterative clarification, and designing agents that adapt reasoning strategies based on user expertise and preferences (Zhang et al., 2025g). This human-in-the-loop approach (Zou et al., 2025) is essential for creating robust and user-aligned RAGReasoning systems in open-ended domains. Agentic Structures and Capabilities. key feature of Synergized RAG-Reasoning is its agentic architecture, where the system autonomously decides the roles of different agents and which tools or retrieval strategies to invoke during inference stages (Luo et al., 2025a; Bei et al., 2025). To fully exploit this potential, future research should focus on developing agent frameworks capable of dynamic tool selection, retrieval planning, and adaptive orchestration across reasoning workflows. Such capabilities enable flexible, context-aware problem solving and are critical for handling diverse, complex tasks (Schneider, 2025). Multimodal Retrieval. As also shown in our benchmark analysis, most existing Synergized RAG-Reasoning systems remain confined to textonly tasks. However, real-world applications increasingly require the ability to retrieve and integrate multimodal content (Liang et al., 2024). 8 Task Dataset Domain Knowledge Source Knowledge Type Reasoning Size Input Output Web Browsing BrowseComp (Wei et al., 2025a) GAIA (Mialon et al., 2023) General General Human, Internet Internet, TooL Commonsense, Logical Commonsense, Logical Deductive Deductive 1,266 Question/Text Natural Language Natural Language Question/Text, Image/File/Code WebWalkerQA (Wu et al., 2025b) General Human, LLM Commonsense, Logical Deductive 680 Question/Text Natural Language Single-hop QA TriviaQA (Joshi et al., 2017) NQ (Kwiatkowski et al., 2019) General General Internet Internet Commonsense, Logical Commonsense, Logical Deductive Deductive 650,000+ 307,373 Question/Text Natural Language Question/Text Natural Language Multi-hop QA 2WikiMultiHopQA (Ho et al., 2020) General HotpotQA (Yang et al., 2018) General MuSiQue (Trivedi et al., 2022) General Internet Internet Previous Resource, Internet Commonsense, Logical Commonsense Commonsense, Logical Deductive Deductive Deductive 192,606 113,000 25,000 Question/Text Natural Language Question/Text Natural Language Question/Text Natural Language Multi-choice QA QuALITY (Pang et al., 2022) Narrative Books Commonsense, Logical MMLU-Pro (Wang et al., 2025b) Science Previous Resource, Internet Arithmetic, Commonsense, Logical Deductive, Abductive Deductive, Inductive 6,737 12,032 Question/Text, Options Question/Text, Options Options Natural Langue, Number, Options Math MATH (Hendrycks et al., 2021) AQuA (Ling et al., 2017) Math Math Exam Arithmetic, Logic Deductive 12,500 Exam, Internet, Previous Resource Arithmetic, Logic Deductive 100,000 Question/Text, Figure, Equation Question/Text, Options, Equation Natural Langue, Number Natural Langue, Options Code Refactoring Oracle (Tsantalis et al., 2020) LiveCodeBench (Jain et al., 2024) Software Internet, Human Contest Internet Logical Logical Deductive 7,226 Code, Instruction Code Deductive, Abductive 500+ Question/Text, Code, Instruction Code, Test Output Table 1: Overview of representative knowledge and reasoning intensive benchmarks by task category. Future research should move beyond the traditional vision-text paradigm to achieve genuine multimodality. This advancement necessitates strengthening foundational abilities of MLLMs, including grounding and cross-modal reasoning (Liang et al., 2024). Additionally, enhancing the agentic capabilities of these models through hybrid-modal chainof-thought reasoning is crucial, enabling interaction with the real world via multimodal search tools (Wang et al., 2025a). Concurrently, developing unified multimodal retrievers that can jointly embed images, tables, text, and heterogeneous documents is essential. Retrieval Trustworthiness. Synergized RAGReasoning systems remain vulnerable to adversarial attacks through poisoned or misleading external knowledge sources. Ensuring the trustworthiness of retrieved content is therefore crucial for maintaining fully reliable downstream reasoning (Huang et al., 2024). Techniques like watermarking and digital fingerprinting have been employed to enhance system traceability. However, theres pressing need to develop more dynamic and adaptive methods that can keep pace with the evolving landscape of LLMs, emerging attack techniques, and shifting model contexts (Liu et al., 2024). Existing studies have also individually explored uncertainty quantification and robust generation to bolster system reliability (Shorinwa et al., 2025). Future research should aim to integrate these approaches, as their combination can mutually reinforce system robustness and trustworthiness. Moreover, future efforts should also focus on extending current benchmarks to encompass multi-dimensional trust metrics beyond mere accuracy."
        },
        {
            "title": "8 Conclusion",
            "content": "This survey charts the rapid convergence of retrieval and reasoning in LLMs. We reviewed three evolutionary stages: (1) Reasoning-Enhanced RAG, which uses multi-step reasoning to refine each stage of RAG; (2) RAG-Enhanced Reasoning, which leverages retrieved knowledge to bridge factual gaps during long CoT; and (3) Synergized RAG-Reasoning systems, where singleor multiagents iteratively refine both search and reasoning, exemplified by recent Deep Research platforms. Collectively, these lines demonstrate that tight retrievalreasoning coupling improves factual grounding, logical coherence, and adaptability beyond one-way enhancement. Looking forward, we identify research avenues toward synergized RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and humancentric."
        },
        {
            "title": "Limitations",
            "content": "While this survey synthesizes over 200 research papers across RAG and reasoning with large language models, its scope favors breadth over depth. In striving to provide unified and comprehensive taxonomy, we may not delve deeply into the technical nuances or implementation details of individual methods-especially within specialized subfields of either RAG (e.g., sparse vs. dense retrieval, memory-augmented retrievers) or reasoning (e.g., formal logic solvers, symbolic methods, or long-context reasoning). Moreover, our cate9 gorization framework (reasoning-enhanced RAG, RAG-enhanced reasoning, and synergized RAG and reasoning) abstracts across diverse methodologies. While this facilitates high-level understanding of design patterns, it may obscure the finergrained trade-offs, assumptions, and limitations unique to each class of approach."
        },
        {
            "title": "References",
            "content": "Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2022. Topiocqa: Open-domain conversational question answering with topic switching. Transactions of the Association for Computational Linguistics, 10:468483. Firoj Alam, Ferda Ofli, and Muhammad Imran. 2018. Crisismmd: Multimodal twitter datasets from natural disasters. In Proceedings of the international AAAI conference on web and social media, volume 12. Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. 2025. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201. Anonymous. 2025. DynQR: Dynamic uncertaintyguided query rewriting for effective retrievalaugmented generation. In Submitted to ACL Rolling Review - December 2024. Under review. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Selfreflective retrieval augmented generation. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738. Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. 2025. Graphs meet ai agents: Taxonomy, progress, and future opportunities. arXiv preprint arXiv:2506.18019. Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. 2012. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145. Danqi Chen and Wen-tau Yih. 2020. Open-domain question answering. In Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts, pages 3437. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. 2025a. Research: Learning to reason with search for arXiv preprint llms via reinforcement learning. arXiv:2503.19470. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, Jeff Pan, et al. 2025b. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025c. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567. Yanfei Chen, Jinsung Yoon, Devendra Sachan, Qingze Wang, Vincent Cohen-Addad, Mohammadhossein Bateni, Chen-Yu Lee, and Tomas Pfister. 2024a. Reinvoke: Tool invocation rewriting for zero-shot tool retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 47054726. Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. 2024b. Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183. Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Xuhui Jiang, and Jian Guo. 2024c. Rulerag: Rule-guided retrieval-augmented generation with language models for question answering. arXiv preprint arXiv:2410.22353. Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. Uprise: Universal prompt retrieval for improving zero-shot evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1231812337. Rong Cheng, Jinyi Liu, Yan Zheng, Fei Ni, Jiazhen Du, Hangyu Mao, Fuzheng Zhang, Bo Wang, and Jianye Hao. 2025. Dualrag: dual-process approach to integrate reasoning and retrieval for multi-hop question answering. arXiv preprint arXiv:2504.18243. Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, 10 and Bing Qin. 2024. Beamaggr: Beam aggregation reasoning over multi-source knowledge for multihop question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1229 1248. Debrup Das, Debopriyo Banerjee, Somak Aditya, and Ashish Kulkarni. 2024. Mathsensei: toolaugmented large language model for mathematical reasoning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 942966. Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, ZhongZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, et al. 2024. Longdocurl: comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating. arXiv preprint arXiv:2412.18424. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2024. Chain-of-verification reduces hallucination in large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 35633578. Avik Dutta, Mukul Singh, Gust Verbruggen, Sumit Gulwani, and Vu Le. 2024. Rar: Retrieval-augmented retrieval for code generation in low resource languages. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2150621515. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024a. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6491 6501. Yue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan, and Jiye Liang. 2024b. Frva: Fact-retrieval and verification augmented entailment tree generation for explainable question answering. In Findings of the Association for Computational Linguistics ACL 2024, pages 91119128. Jinyuan Fang, Zaiqiao Meng, and Craig Macdonald. 2024. Trace the evidence: Constructing knowledgegrounded reasoning chains for retrieval-augmented generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 8472 8494. Weizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang, Bo Bai, Lei Deng, and Wei Han. 2024. Retrieval meets reasoning: Dynamic in-context editarXiv preprint ing for long-text understanding. arXiv:2406.12331. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. 2025. Airrag: Activating intrinsic reasoning for retrieval augmented genarXiv preprint eration via tree-based search. arXiv:2501.10053. James Ferguson, Matt Gardner, Hannaneh Hajishirzi, Tushar Khot, and Pradeep Dasigi. 2020. Iirc: dataset of incomplete information reading comprehension questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 11371147. Zafeirios Fountas, Martin Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. 2024. Humanlike episodic memory for infinite context llms. arXiv preprint arXiv:2407.09450. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023a. RARR: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1647716508, Toronto, Canada. Association for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023b. Retrievalaugmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2:1. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346 361. Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025. Deeprag: Thinking to retrieval step by step for large language models. arXiv preprint arXiv:2502.01142. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag: Simple and fast arXiv preprint retrieval-augmented generation. arXiv:2410.05779. Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: multi-modal multi-agent framework for document understanding. arXiv preprint arXiv:2503.13964. Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. In Advances in Neural Information Processing Systems, volume 36, pages 4587045894. Bolei He, Nuo Chen, Xinran He, Lingyong Yan, Zhenkai Wei, Jinchang Luo, and Zhen-Hua Ling. 2024a. Retrieving, rethinking and revising: The chain-of-verification can improve retrieval augmented generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1037110393. Yulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda: benchmark suite for retrieval augmented generation in real-world document analysis. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Pan. 2024b. Mintqa: multi-hop question answering benchmark for evaluating llms on new and tail knowledge. arXiv preprint arXiv:2412.17032. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024c. G-retriever: Retrievalaugmented generation for textual graph understanding and question answering. Advances in Neural Information Processing Systems, 37:132876132907. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609 6625. Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, and Irwin King. 2024. Serts: Self-rewarding tree search for biomedical retrieval-augmented generation. arXiv preprint arXiv:2406.11258. Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. 2025. Mcts-rag: Enhancing retrieval-augmented generation with monte carlo tree search. arXiv preprint arXiv:2503.20757. Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia Hockenmaier, and Tong Zhang. 2025a. Rag-rl: Advancing retrieval-augmented generation arXiv preprint via rl and curriculum learning. arXiv:2503.12759. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025b. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, et al. 2024. survey of safety and trustworthiness of large language models through the lens of verification and validation. Artificial Intelligence Review, 57(7):175. Michael Iannelli, Sneha Kuchipudi, and Vera Dvorak. 2024. Sla management in reconfigurable multi-agent rag: systems approach to question answering. arXiv preprint arXiv:2412.06832. Shayekh Islam, Md Asib Rahman, KSM Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan Parvez. 2024. Open-rag: Enhanced retrieval augmented reasoning with open-source large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 14231 14244. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 70297043. Yixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie Zhong, Xu Jia, and Min Zhang. 2024. Retrieval and reasoning on kgs: Integrate knowledge graphs into large language models for complex question answerIn Findings of the Association for Computaing. tional Linguistics: EMNLP 2024, pages 75987610. Mingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang. 2025. Find: Fine-grained information density guided adaptive retrieval-augmented generation for disease diagnosis. arXiv preprint arXiv:2502.14614. Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2024. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. arXiv preprint arXiv:2412.12881. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223. Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, et al. 2024. Graph chainof-thought: Augmenting large language models by reasoning on graphs. In Findings of the Association for Computational Linguistics ACL 2024, pages 163 184. 12 Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. generating chain-of-thoughts in query expansion. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1351413523. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611. Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi Li. 2025. Rearag: Knowledge-guided reasoning enhances factuality of large reasoning models with iterative retrieval augmented generation. arXiv preprint arXiv:2503.21729. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. 2024. Rap: Retrieval-augmented planning with contextual memarXiv preprint ory for multimodal llm agents. arXiv:2402.03610. Mohammed Khaliq, Paul Chang, Mingyang Ma, Bernhard Pflugfelder, and Filip Miletic. 2024. Ragar, your falsehood radar: Rag-augmented reasoning for political fact-checking using multimodal large language models. In Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER), pages 280296. Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. 2023. Tree of clarifications: Answering ambiguous questions with retrievalaugmented large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9961009. Neema Kotonya and Francesca Toni. 2020. Explainable automated fact-checking for public health claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 77407754. Heiko Koziolek, Sten Grüner, Rhaban Hark, Virendra Ashiwal, Sofia Linsbauer, and Nafise Eskandani. 2024. Llm-based and retrieval-augmented control In Proceedings of the 1st Intercode generation. national Workshop on Large Language Models for Code, pages 2229. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2024. Fact, and reason: unified evaluation of fetch, arXiv preprint retrieval-augmented generation. arXiv:2409.12941. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Sung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho Kang, and Seung-Hoon Na. 2024. Radcot: Retrievalaugmented distillation to specialization models for Dawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy DuongTran, Ying Ding, et al. 2024a. Dalk: Dynamic coaugmentation of llms and kg to answer alzheimers disease questions with scientific literature. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 21872205. Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, and Hao Liao. 2024b. Re-search for the truth: Multi-round retrievalaugmented large language models are strong fake news detectors. arXiv preprint arXiv:2403.09747. Guozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji, Ziyu Shang, Jiajun Liu, and Zijie Xu. 2024c. Recall, retrieve and reason: towards better in-context relation extraction. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 63686376. Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang, Vijay Viswanathan, Patrick Lewis, Taro Watanabe, and Yixuan Su. 2024d. Alr2: retrieve-thenreason framework for long-context question answering. arXiv preprint arXiv:2410.03227. Jia Li, Xianjie Shi, Kechi Zhang, Lei Li, Ge Li, Zhengwei Tao, Fang Liu, Chongyang Tao, and Zhi Jin. 2025a. Coderag: Supportive code retrieval on bigraph for real-world code generation. arXiv preprint arXiv:2504.10046. Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, and Michael Bendersky. 2024e. Can query expansion improve generalization of strong cross-encoder rankers? In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 23212326. Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, et al. 2024f. Graphreader: Building graph-based agent to enhance long-context abilities of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1275812786. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025b. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025c. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Fei Huang, Jingren Zhou, and Philip S. Yu. 2025d. Benchmarking multimodal retrieval augmented generation with dynamic VQA dataset and self-adaptive planning agent. In The Thirteenth International Conference on Learning Representations. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957. Zhi Li, Yicheng Li, Hequan Ye, and Yin Zhang. 2024g. Towards autonomous tool utilization in language models: unified, efficient and scalable framework. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1642216432. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025e. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, and Yongbin Li. 2024h. Structrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information structurization. arXiv preprint arXiv:2410.08815. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. 2024. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146. Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, and Philip Yu. 2024. survey of text watermarking in the era of large language models. ACM Computing Surveys, 57(2):136. Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, and Jun Ma. 2025. Hm-rag: Hierarchical multi-agent multimodal retrieval augmented generation. arXiv preprint arXiv:2504.12330. Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos Mazomenos, and Yueming Jin. 2025. Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence. arXiv preprint arXiv:2503.10265. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. 2025a. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460. Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Zhao. 2023. Dr. icl: Demonstration-retrieved in-context learning. arXiv preprint arXiv:2305.14128. Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile van Krieken, Pietro Lesci, and Pasquale Minervini. 2025b. Self-training large language models for tool-use without demonstrations. arXiv preprint arXiv:2502.05867. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2024a. Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided arXiv preprint retrieval augmented generation. arXiv:2407.10805. Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, and Aixin Sun. 2024b. Sciagent: Toolaugmented language models for scientific reasoning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1570115736. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2025. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:9596396010. Kelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo, Chenlong Deng, and Zhicheng Dou. 2024. Ragstudio: Towards in-domain adaptation of retrieval 14 In augmented generation through self-alignment. Findings of the Association for Computational Linguistics: EMNLP 2024, pages 725735. Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, and Viktor Moskvoretskii. 2025. Llm-independent adaptive rag: Let the question speak for itself. arXiv preprint arXiv:2505.04253. Costas Mavromatis and George Karypis. 2024. Gnnrag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332. Shashi Narayan, Shay Cohen, and Mirella Lapata. 2018. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 17971807. Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty. 2024. Sfr-rag: Towards contextually faithful llms. arXiv preprint arXiv:2409.09916. Cheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Juntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu, Shizhe Diao, and Tong Zhang. 2024. Veract scan: Retrieval-augmented fake news detection with justifiable reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 266277. Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: dataset for commonsense reasoning over entity knowledge. OpenReview. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, et al. 2022. Quality: Question answering with long In Proceedings of the 2022 Coninput texts, yes! ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 53365358. 2025. Humanitys last exam. arXiv:2501.14249. arXiv preprint Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. 2024. Making language models better tool learners with execution feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3550 3568. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Leonardo Ranaldi, Marco Valentino, and Andrè Freitas. 2024. Eliciting critical reasoning in retrievalaugmented language models via contrastive explanations. arXiv preprint arXiv:2410.22874. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Aniruddha Salve, Saba Attar, Mahesh Deshmukh, Sayali Shivpuje, and Arnab Mitra Utsab. 2024. collaborative multi-agent approach to retrieval-augmented arXiv preprint generation across diverse data. arXiv:2412.05838. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227. Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, and Sepp Hochreiter. 2024. Retrieval-augmented decision transformer: External memory for in-context rl. arXiv preprint arXiv:2410.07071. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Johannes Schneider. 2025. Generative to agentic ai: Survey, conceptualization, and challenges. arXiv preprint arXiv:2504.18875. 15 Eva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent: large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 22042213. Qiang Sun, Tingting Bi, Sirui Li, Eun-Jung Holden, Paul Duuring, Kai Niu, and Wei Liu. 2025b. Symbioticrag: Enhancing document intelligence through human-llm symbiotic collaboration. arXiv preprint arXiv:2505.02418. Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Ren, and Anirudha Majumdar. 2025. survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions. ACM Computing Surveys. Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and Han Li. 2025c. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding. arXiv preprint arXiv:2501.07861. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Chuanneng Sun, Songjun Huang, and Dario Pompili. 2024a. Retrieval-augmented hierarchical in-context reinforcement learning and hindsight modular reflections for task planning with llms. arXiv preprint arXiv:2408.06520. Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 23802390. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan Zhang. 2025a. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, HeungYeung Shum, and Jian Guo. 2024b. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. In The Twelfth International Conference on Learning Representations. Alon Talmor and Jonathan Berant. 2018. The web as knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641651. Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. arXiv preprint arXiv:2401.15391. Yicheng Tao, Haotian Liu, Shanwen Wang, and Hongteng Xu. 2025. Assisting mathematical formalization with learning-based premise retriever. arXiv preprint arXiv:2501.13959."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Thorne, Christos and Arpit Mittal. 2018. Christodoulopoulos, Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313. Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. 2024. Rare: Retrievalaugmented reasoning enhancement for large language models. arXiv preprint arXiv:2412.02830. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037. Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. IEEE Transactions 2020. Refactoringminer 2.0. on Software Engineering, 48(3):930950. Boxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. 2024a. Instructretro: Instruction tuning post retrievalaugmented pretraining. In International Conference on Machine Learning, pages 5125551272. PMLR. 16 Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. 2023. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):4760. Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024g. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint arXiv:2403.05313. Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, et al. 2024b. Learning to plan for retrieval-augmented large language models from knowledge graphs. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 78137835. Song Wang, Zihan Chen, Chengshuai Shi, Cong Shen, and Jundong Li. 2024c. Mixture of demonstrations for in-context learning. Advances in Neural Information Processing Systems, 37:8809188116. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025a. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. 2025a. Multimodal chain-of-thought reasonarXiv preprint ing: comprehensive survey. arXiv:2503.12605. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Yu Wang, Nedim Lipka, Ryan Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024d. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1920619214. Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi Sun. 2025b. Alignrag: An adaptable framework for resolving misalignments in retrieval-aware reasoning of rag. arXiv preprint arXiv:2504.14858. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2025b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:95266 95290. Yujing Wang, Hainan Zhang, Liang Pang, Binghui Guo, Hongwei Zheng, and Zhiming Zheng. 2025c. Maferw: Query rewriting with multi-aspect feedbacks for retrieval-augmented large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2543425442. Zheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024e. M-rag: Reinforcing large language model performance through retrieval-augmented genIn Proceedings eration with multiple partitions. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19661978. Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linpeng Tang, Wentao Zhang, et al. 2025d. Rare: arXiv Retrieval-augmented reasoning modeling. preprint arXiv:2503.23513. Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2024f. Jarvis-1: Open-world multi-task agents with memoryIEEE augmented multimodal language models. Transactions on Pattern Analysis and Machine Intelligence. Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, and Dongsheng Li. 2023. Grove: retrieval-augmented complex story generation framework with forest of evidence. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 39803998. Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu NkisiOrji, Ruvan Weerasinghe, Anne Liret, and Bruno Fleisch. 2024. Cbr-rag: case-based reasoning for retrieval augmented generation in llms for legal question answering. In International Conference on CaseBased Reasoning, pages 445460. Springer. Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, and Jing Gao. 2025a. Talk to right specialists: Routing and planning in multi-agent system for question answering. arXiv preprint arXiv:2501.07813. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. 2025b. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572. Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644. Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis, Karthik Subbian, Jure Leskovec, and James Zou. 2024. Avatar: Optimizing llm agents for tool usage via contrastive reasoning. Advances in Neural Information Processing Systems, 37:2598126010. 17 Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025a. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, and Haifeng Huang. 2025b. Improving retrieval augmented language model with self-reasoning. In Proceedings of the AAAI conference on artificial intelligence, volume 39, pages 2553425542. Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. 2025. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957. Kehan Xu, Kun Zhang, Jingyuan Li, Wei Huang, and Yuanzhuo Wang. 2024. Crp-rag: retrievalaugmented generation framework for supporting complex logical reasoning and knowledge planning. Electronics, 14(1):47. Kehan Xu, Kun Zhang, Jingyuan Li, Wei Huang, and Yuanzhuo Wang. 2025a. Crp-rag: retrievalaugmented generation framework for supporting complex logical reasoning and knowledge planning. Electronics (2079-9292), 14(1). Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce Ho, Haoyu Wang, and Carl Yang. 2025b. Collab-rag: Boosting retrieval-augmented generation for complex question answering via white-box and black-box llm collaboration. arXiv preprint arXiv:2504.04915. Chen Yang, Chenyang Zhao, Quanquan Gu, and Dongruo Zhou. 2024a. Cops: Empowering llm agents with provable cross-task experience sharing. arXiv preprint arXiv:2410.16670. Rui Yang. 2024. Casegpt: case reasoning framework based on language models and retrieval-augmented generation. arXiv preprint arXiv:2407.07913. Wooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei Han, Yu Wang, Junhyun Lee, and Philip Yu. 2025. Cold-start recommendation with knowledge-guided arXiv preprint retrieval-augmented generation. arXiv:2505.20773. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran Jiang, Ziyu Jiang, et al. 2024b. Crag-comprehensive rag benchmark. Advances in Neural Information Processing Systems, 37:1047010490. Yahe Yang and Chengyue Huang. 2025. Tree-based rag-agent recommendation system: case study in medical test data. arXiv preprint arXiv:2501.02727. Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: challenge dataset for open-domain quesIn Proceedings of the 2015 contion answering. ference on empirical methods in natural language processing, pages 20132018. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. In North American Chapter of the Association for Computational Linguistics (NAACL). Jaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim, and Seung-won Hwang. 2025. Perc: Plan-as-query example retrieval for underrepresented code generation. In Proceedings of the 31st International Conference on Computational Linguistics, pages 7982 7997. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2024. Making retrieval-augmented language models robust to irrelevant context. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Hong Qing Yu and Frank McQuade. 2025. Rag-kg-il: multi-agent hybrid framework for reducing hallucinations and enhancing llm reasoning through rag and incremental knowledge graph learning integration. arXiv preprint arXiv:2503.13514. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin Cao, Kaixin Ma, Jian Li, Hongwei Wang, and Dong Yu. 2024. Chain-of-note: Enhancing robustness in retrieval-augmented language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1467214685. Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. 2022a. Subgraph retrieval enhanced model for multi-hop knowledge base question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5773 5784. 18 Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, and Kunpeng Liu. 2025a. Ratt: thought structure for coherent and correct llm reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2673326741. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025b. Lightthinker: ThinkarXiv preprint ing step-by-step compression. arXiv:2502.15589. Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, and Tianyu Shi. 2025c. cascading cooperative multi-agent framework for onramp merging control integrating large language models. arXiv preprint arXiv:2503.08199. Ningning Zhang, Chi Zhang, Zhizhong Tan, Xingxing Yang, Weiping Deng, and Wenyong Wang. 2025d. Credible plan-driven rag method for multi-hop question answering. arXiv preprint arXiv:2504.16787. Tianjun Zhang, Shishir Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph Gonzalez. 2024a. Raft: Adapting language model to domain specific rag. In First Conference on Language Modeling. Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, et al. 2025e. Cold-start recommendation towards the era of large language models (llms): comprehensive survey and roadmap. arXiv preprint arXiv:2501.01945. Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al. 2025f. From web search towards agentic deep research: Incentivizing search with reasoning agents. arXiv preprint arXiv:2506.18959. Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, et al. 2025g. Personaagent: When large language model agents meet personalization at test time. arXiv preprint arXiv:2506.06254. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher Manning, and Jure Leskovec. 2022b. Greaselm: Graph reasoning enhanced language models. In International Conference on Learning Representations. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. 2024b. bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1526215277. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. 2024c. Chain of agents: Large language models collaborating on long-context tasks. Advances in Neural Information Processing Systems, 37:132208132237. Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao Cao. 2023. Iag: Induction-augmented generation framework for answering reasoning questions. arXiv preprint arXiv:2311.18397. Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna Qiu, and Lili Qiu. 2024a. Retrieval augmented generation (rag) and beyond: comprehensive survey on how to make your llms use external data more wisely. arXiv preprint arXiv:2409.14924. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Xiaoyan Zhao, Lingzhi Wang, Zhanghao Wang, Hong Cheng, Rui Zhang, and Kam-Fai Wong. 2024b. Pacar: Automated fact-checking with planning and customized action reasoning using large language In Proceedings of the 2024 Joint Inmodels. ternational Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1256412573. Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024c. Seer: Self-aligned evidence extraction for retrievalaugmented generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 30273041. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2021. Minif2f: cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Jiawei Zhou and Lei Chen. 2025. Openrag: Optimizing rag end-to-end via in-context retrieval learning. arXiv preprint arXiv:2503.08398. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. 2025a. Browsecomp-zh: Benchmarking web browsing abilarXiv ity of large language models in chinese. preprint arXiv:2504.19314. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. 2025b. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478. Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng Dou. 2024. Inters: Unlocking the power of large language models in search with instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27822809. Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, et al. 2025. survey on large language model based human-agent systems. arXiv preprint arXiv:2505.00753."
        },
        {
            "title": "A Full Benchmark",
            "content": "Section 6 introduces representative benchmarks for different RAG-reasoning tasks. This appendix complements that discussion with comprehensive list of benchmarks organized by task and domain. Table 2 details each benchmarks attributes, including the publication venue, code repository, task category, domain, primary knowledge sources, knowledge type, and reasoning capabilities. By consolidating these attributes into single table, we facilitate the selection and comparison of benchmarks, enabling researchers to identify the most suitable datasets for future studies on RAG-enhanced reasoning. Our benchmark compilation is primarily derived from the methods surveyed in Sections 3 to 5 of this paper, with particular focus on synergized approaches discussed in Section 5. We deliberately targeted benchmarks that require both external knowledge retrieval and internal deep reasoning, as this dual requirement reflects real-world scenarios where models must not only access relevant information but also integrate and reason over it effectively. For example, in the QA domain, we include datasets that necessitate synthesizing evidence across multiple documents to answer questions that cannot be resolved through single-sentence retrieval. HotpotQA (Yang et al., 2018) exemplifies this challenge, requiring reasoning across different Wikipedia articles. In coding tasks, benchmarks such as LiveCodeBench (Jain et al., 2024) and Refactoring Oracle (Tsantalis et al., 2020) extend beyond pure algorithmic problemsolving by demanding retrieval of external code snippets and documentation. Similarly, in mathematics, benchmarks like MATH (Hendrycks et al., 2021) and AQUA-RAT (Das et al., 2024) assess not only computational proficiency but also the retrieval of relevant theorems and formulas, testing the models ability to integrate external mathematical knowledge with internal reasoning processes. In addition to established benchmarks, we have incorporated newer and more challenging datasets that better mirror real-world applications. These datasets often demand extensive retrieval processes combined with expert-level or domain-specific reasoning, as seen in Humanitys Last Exam (HLE) (Phan et al., 2025) and web search evaluation tasks like BrowseComp (Wei et al., 2025a). Overall, our collection encompasses 46 benchmarks covering 13 distinct tasks across 12 domains, each explicitly annotated with features such as knowledge source, knowledge type, and reasoning capacity. This breadth ensures coverage of diverse domains and task types, forming solid foundation for evaluating the interplay between retrieval and reasoning in RAG systems. Within this benchmark set, single-hop QA datasets like TriviaQA (Joshi et al., 2017) focus on precise retrieval and fact recall, requiring models to locate and synthesize single piece of evidence. In contrast, multi-hop QA benchmarks such as HotpotQA (Yang et al., 2018) and MuSiQue (Trivedi et al., 2022) challenge models to chain information from multiple documents and employ deductive reasoning to bridge disparate facts into coherent answers. Structured knowledge benchmarks, such as GraphQA (He et al., 2024c), require reasoning over relational graph representations, integrating nodes and edges to resolve complex queries beyond plain text retrieval. Complementing these open-ended tasks, multiple-choice evaluations like MMLU-Pro (Wang et al., 2025b) test domainspecific knowledge in areas such as science, history, or law, assessing the models ability to perform various reasoning styles, including inductive and abductive inference. Multimodal QA benchmarks, like WebShop (Yao et al., 2022), test models capacity to align textual and visual information to determine the correct answer. Long-form QA datasets such as BENCH (Zhang et al., 2024b) evaluate models ability to maintain logical consistency and perform inductive reasoning over lengthy contexts. Collectively, these benchmarks establish comprehensive evaluation chain for systematically assessing RAG-reasoning capabilities. Beyond text-based QA, RAG-augmented benchmarks span diverse tasks involving long-form interactive reasoning, and domaingeneration, specific challenges in mathematics and programming. Mathematics benchmarks such as MATH (Hendrycks et al., 2021) draw from competition-level problems to assess arithmetic and symbolic reasoning. Summarization tasks like XSum (Narayan et al., 2018) evaluate models ability to condense entire news articles into concise summaries while preserving factual Fact-checking benchmarks, such correctness. as FEVER (Thorne et al., 2018), test the capacity for evidence retrieval and claim verification. Code-focused evaluations, including LiveCodeBench (Jain et al., 2024), examine deductive and abductive reasoning in the context of algoDataset Code Venue Resource Task Domain Knowledge Source Knowledge Type Reasoning Capability Size Input Output LiveCodeBench (Jain et al., 2024) Refactoring Oracle (Tsantalis et al., 2020) ColBench (Zhou et al., 2025b) Arxiv IEEE22 Arxiv"
        },
        {
            "title": "Logical",
            "content": "Deductive, Abductive 1,055 Question/Text, Code,"
        },
        {
            "title": "Software",
            "content": "Internet, Human"
        },
        {
            "title": "Deductive",
            "content": "7,226 Instruction Code, Instruction Code Instance, Test Output Code Instance"
        },
        {
            "title": "Software",
            "content": "LLM, Human"
        },
        {
            "title": "Logical",
            "content": "Abductive, Inductive 10,000+ Question/Text, Links/Sources, Code"
        },
        {
            "title": "Code Instance",
            "content": "Math MATH (Hendrycks et al., 2021) MiniF2F (Zheng et al., 2021) AQuA (Ling et al., 2017) NeurIPS"
        },
        {
            "title": "Link",
            "content": "ICLR22 Arxiv"
        },
        {
            "title": "Link",
            "content": "Domain-specific QA Domain-specific QA Domain-specific QA"
        },
        {
            "title": "Math",
            "content": "Exam/Competition Logical, Arithmetic"
        },
        {
            "title": "Deductive",
            "content": "12,500 Exam/Competition, Books Previous Source, Exam/Competition, Internet Logical, Arithmetic"
        },
        {
            "title": "Deductive",
            "content": "488 Arithmetic, Logical"
        },
        {
            "title": "Deductive",
            "content": "100,000 Question/Text, Equations Question/Text, Equations Question/Text, Options, Equations Number, Natural Language Number, Natural Language Natural Language, Options/Labels Fact Checking CRAG (Yang et al., 2024b) CREAK (Onoe et al., 2021) Fever (Thorne et al., 2018) PubHealth (Kotonya and Toni, 2020) Graph QA GraphQA (He et al., 2024c) GRBENCH (Jin et al., 2024) Long-form QA BENCH (Zhang et al., 2024b) Multimodal QA CrisisMMD (Alam et al., 2018) ALFWORLD (Shridhar et al.) MMLongBench-DOC (Ma et al., 2025) LongDocURL (Deng et al., 2024) SCIENCEQA (Lu et al., 2022) WebShop (Yao et al., 2022) SurgCoTBench (Low et al., 2025) NeurIPS24 Link Fact Checking General NeurIPS21 Link Fact Checking General ACL18 Link Fact Checking General EMNLP20 Link Fact Checking Health Internet Human Internet Internet Commonsense Deductive, Abductive 4,409 Question/Text Natural Language Commonsense Logical Deductive, Abductive, Analogical Deductive, Abductive 185,445 13,000 Question/Text Commonsense, Logical Abductive, Deductive 11, Question/Text, Links/Sources Question/Text Options/Labels, Natural Language Natural Language, Options/Labels Natural Language, Options NeurIPS24 Link Graph QA General Previous Source ACL24 Link Graph QA General LLM, Human Commonsense, Multimodal Logical Deductive, Abductive 107,503 Question/Text Natural Language Deductive, Inductive 1,740 Question/Text Natural Language Arxiv24 Link Long-form QA General Internet, Human Multimodal, Logical Inductive, Abductive 3,946 Question/Text, Code, Equations Natural Language, Number, Code Instance Arxiv18 Link Multimodal QA Crisis Response Media, Internet ICLR21 Link Multimodal QA Game Previous Source NeurIPS24 Link Multimodal QA Narrative Arxiv24 Link Multimodal QA Narrative Commonsense, Multimodal Multimodal Abductive 16,097 Deductive, Abductive 3,827 Multimodal Deductive, Abductive 1,082 Multimodal Deductive, Abductive 2,325 Multimodal Deductive Logical, Multimodal Deductive 29,590 21,000 Previous Source, Internet Internet, Previous Source, LLM Internet, Paper/Report Human UDA (Hui et al., 2024) NIPS24 Link Multimodal QA Narrative NeurIPS Link Multimodal QA Science NeurIPS22 Link Multimodal QA E-commerce Internet Multimodal Inductive, Abductive 12,087 Arxiv"
        },
        {
            "title": "Human",
            "content": "Multimodal, Logical Abductive, Deductive 14,176 Question/Text, Figure/Image Question/Text, Figure/Image Figure/Image, Question/Text, Documents Figure/Image, Question/Text, Documents Documents, Question/Text Question/Text, Options, Figure/Image Instruction, Question/Text Question/Text, Figure/Image, Options Options, Natural Language Natural Language Natural Language, Number Natural Language, Number Natural Language, Number Options, Natural Language, Number Natural Language, Image/Figure Options, Natural Language, Number Table 2: Full representative knowledge and reasoning intensive benchmarks across diverse task categories (Part 1). 22 Dataset Venue Resource Task Domain Knowledge Source Knowledge Type Reasoning Capability Size Input Output Multi-choice QA Bamboogle (Press et al., 2023) BIG-Bench (Srivastava et al., 2022) ADQA (Li et al., 2024a) QuALITY (Pang et al., 2022) MMLU-Pro (Wang et al., 2025b) Multi-hop QA EMNLP Link Multi-choice QA"
        },
        {
            "title": "General",
            "content": "Arxiv22 Link Multi-choice QA"
        },
        {
            "title": "Logical",
            "content": "Deductive, Abductive Commonsense, Logical Deductive, Abductive, Inductive, Analogical 125 204 EMNLP Link Multi-choice QA"
        },
        {
            "title": "Previous Source",
            "content": "NAACL22 Link Multi-choice QA"
        },
        {
            "title": "Books",
            "content": "NeurIPS24 Link Multi-choice QA"
        },
        {
            "title": "Science",
            "content": "Previous Source, Internet Commonsense, Logical Commonsense, Logical Arithmetic, Commonsense, Logical Deductive, Abductive 446 Deductive, Abductive 6, Deductive, Inductive 12,032 Question/Text"
        },
        {
            "title": "Natural Language",
            "content": "Question/Text, Options Question/Text, Options Question/Text, Options Question/Text, Options Natural Language, Number, Options/Labels Options"
        },
        {
            "title": "Options",
            "content": "Natural Language, Number, Options Arxiv24 FRAMES (Krishna et al., 2024) HotpotQA (Yang et al., 2018) GPQA (Rein et al., 2024) HLE (Phan et al., 2025) Arxiv25 Arxiv24 EMNLP"
        },
        {
            "title": "Link",
            "content": "Multi-hop QA"
        },
        {
            "title": "Link",
            "content": "Multi-hop QA"
        },
        {
            "title": "Link",
            "content": "Multi-hop QA"
        },
        {
            "title": "Link",
            "content": "Multi-hop QA"
        },
        {
            "title": "Human",
            "content": "Internet Internet Internet Previous Source, Internet Internet Commonsense, Logical, Arithmetic Commonsense"
        },
        {
            "title": "Deductive",
            "content": "824 Question/Text"
        },
        {
            "title": "Deductive",
            "content": "113,000 Question/Text"
        },
        {
            "title": "Logical",
            "content": "Deductive, Abductive 448 Logical, Arithmetic, Multimodal Deductive, Abductive 2,500 Commonsense Deductive 34,689 Question/Text, Options Question/Text, Options, Figure/Image Question/Text Natural Language, Number, Options Natural Language, Number, Options Natural Language Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Deductive 13,000+ Deductive 10,479 Question/Text, Links/Sources Question/Text Number, Natural Language Natural Language Deductive 25,000 Question/Text Natural Language Deductive 54,494 Question/Text Natural Language Deductive 192,606 Question/Text Natural Language Deductive 2,780 Question/Text Natural Language NAACL Link Multi-hop QA General EMNLP20 Link Multi-hop QA General Arxiv24 Link Multi-hop QA General ACL Link Multi-hop QA General TACL22 Link Multi-hop QA General COLING20 Link Multi-hop QA General Internet TACL21 Link Multi-step QA General Internet Arxiv Link Single-hop QA General LLM, Human Commonsense Deductive 4,326 Question/Text Natural Language ACL17 Link Single-hop QA General ACL19 Link Single-hop QA General Internet Internet EMNLP18 Link ACL19 Link Text Summarization Text Summarization Narrative Internet, Media Patent Internet Arxiv25 Link Web Browsing General Human, Internet Arxiv25 Link Web Browsing General Human, Internet ICLR23 Link Web Browsing General Internet, TooL Arxiv25 Link Web Browsing General Human, LLM Arxiv17 Link Dialog General Internet Commonsense, Logical Commonsense, Logical Logical, Commonsense Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Commonsense, Logical Deductive 650,000+ Question/Text Natural Language Deductive 307,373 Question/Text Natural Language Abductive 226,711 Question/Text Natural Language Abductive 1.3 Question/Text Natural Language Deductive 1,266 Question/Text Natural Language Deductive Deductive Deductive 289 680 Question/Text Natural Language Question/Text, Image/File/Code Question/Text Natural Language Natural Language 13,118 Question/Text Natural Language CWQ (Talmor and Berant, 2018) IIRC (Ferguson et al., 2020) MINTQA (He et al., 2024b) MuSiQue (Trivedi et al., 2022) TopiOCQA (Adlakha et al., 2022) 2WikiMultiHopQA (Ho et al., 2020) Multi-step QA StrategyQA (Geva et al., 2021) Single-hop QA SimpleQA (Wei et al., 2024) TriviaQA (Joshi et al., 2017) NQ (Kwiatkowski et al., 2019) Text Summarization XSum (Narayan et al., 2018) BIGPATENT (Sharma et al., 2019) Web Browsing BrowseComp (Wei et al., 2025a) BrowseComp-ZH (Zhou et al., 2025a) GAIA (Mialon et al., 2023) WebWalkerQA (Wu et al., 2025b) Dialog DailyDialog (Li et al., 2017) Table 3: Full epresentative knowledge and reasoning intensive benchmarks across diverse task categories (Part 2, continued)."
        },
        {
            "title": "Benchmark",
            "content": "TriviaQA, NQ HotpotQA, 2WikiMultiHopQA, MuSiQue, HLE MMLU-Pro, QUALITY"
        },
        {
            "title": "General",
            "content": "Scale & Noise: Retrieval from massive, noisy corpora. Ambiguity: Handling real-world queries that are often underspecified or ambiguous. Multi-document / High-dependency Synthesis: Requires finding and connecting evidence scattered across multiple Wikipedia articles. Multi-hop Deduction: Explicitly designed to test the ability to link two or more discrete facts into coherent reasoning path. Science, Narrative Expert-level Retrieval: Requires accessing deep specialized knowledge from academic or densely written narrative sources. Complex & Long-form Reasoning: MMLU-Pro demands expert-level problem-solving over rote memorization. QUALITY uniquely requires comprehension of very long texts (often >5,000 tokens). MATH, AQUA-RAT"
        },
        {
            "title": "Math",
            "content": "Formal Knowledge Retrieval: Locating precise mathematical theorems, lemmas, or formulas in formal corpora. Symbolic & Deductive Reasoning: Involves performing precise, multi-step logical and algebraic operations where each step must be correct. AQUA-RAT is unique in providing natural language rationales, thus testing the models ability to explain its formal reasoning."
        },
        {
            "title": "Code",
            "content": "BrowseComp, WebWalkerQA General (Web) Structural & Modal Heterogeneity: Must retrieve from diverse, heterogeneous sources such as code repositories, documentation, and community forums like Stack Overflow. Tool Use & Self-correction Reasoning: Requires applying retrieved code snippets/APIs, executing code, and reasoning based on test outputs to debug and iteratively improve solutions. Dynamism, Interactivity, and Long-tail Retrieval: Tests agentic planning and tool use in live, unstructured web environments. BrowseComp requires creative, persistent navigation to locate hard-to-find, intertwined information, while WebWalkerQA focuses on systematic traversal of websites subpages. Agentic & Strategic Reasoning: Requires planning and executing multi-step strategies (e.g., searching, clicking, extracting) in dynamic and unpredictable contexts to achieve defined goal. Table 4: The primary retrieval and reasoning challenges for different RAG-Reasoning benchmarks. rithmic problem-solving. Web-based tasks, exemplified by BrowseComp (Wei et al., 2025a), emulate real-world search behavior, requiring iterative query formulation and navigation across multiple webpages. In addition to cataloging datasets, Table 4 provides synthesized overview of the primary retrieval and reasoning challenges associated with each benchmark discussed in this survey. This comparative analysis reveals critical gaps in current benchmark coverage that future research must address. From domain perspective, most benchmarks still focus on limited set of general or academic scenarios, with few tackling real-world, realistic industrial or vertical-domain tasks where retrieval sources might be personalized, proprietary or highly specialized. Regarding retrieval capabilities, existing benchmarks rarely test systems ability to handle heterogeneous or multimodal content, nor do they systematically evaluate robustness against noisy, evolving, or conflicting information within unified framework for trustworthiness. In terms of reasoning capabilities, current benchmarks primarily assess deductive reasoning, leaving underexplored more complex forms such as deep causal reasoning, counterfactual thinking, decision-oriented reasoning, or analogical reasoning in specialized domains. Moreover, there is lack of standardized benchmarks and metrics for evaluating the entire reasoning-retrieval trajectory, including the efficiency of retrieval steps, the quality of intermediate queries, and the logical consistency of multi-step reasoning chains."
        },
        {
            "title": "B Deep Research Implementations",
            "content": "In this section, we extend the discussion of the agentic paradigm introduced in Section 5.2, in which RAG systems adopt the role of active researchers who plan multistep queries, interleave retrieval with reasoning, and coordinate specialized tools or agents. These characteristics collectively define what we refer to as deep research, representing the ability of system to autonomously break down complex questions, iteratively gather diverse evidence, and synthesize information through multiple reasoning steps. This paradigm seeks to enhance autonomy, reduce hallucinations, and improve factual accuracy in open-domain tasks. Such deep research systems can be realized through either single-agent or multi-agent architectures. Single-agent systems rely on single model to manage the entire process of question decomposition, retrieval, and synthesis, offering simplicity and shared context but facing limitations in handling highly specialized or multi-modal tasks. In contrast, multi-agent systems distribute these responsibilities among specialized agents, enabling modularity and potentially greater robustness. However, this collaborative design introduces 24 Name Base Model OptimizationReward Retriever Agent Architecture Train Data Evaluation Data Link N/A"
        },
        {
            "title": "Prompting",
            "content": "N/A"
        },
        {
            "title": "Prompting",
            "content": "N/A"
        },
        {
            "title": "Prompting",
            "content": "N/A Web Search, Local Retrieval Web Search"
        },
        {
            "title": "Hierarchical",
            "content": "N/A N/A N/A"
        },
        {
            "title": "GPQA",
            "content": "N/A N/A GRPO, PPO"
        },
        {
            "title": "Exact\nMatch",
            "content": "GRPO, PPO, Reinforce"
        },
        {
            "title": "Single",
            "content": "NQ, HotpotQA"
        },
        {
            "title": "Single",
            "content": "NQ, HotpotQA NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, MuSiQue, Bamboogle NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, MuSiQue, Bamboogle Agentic Reasoning (Wu et al., 2025c) gpt-researcher deep-searcher Search-R1 (Jin et al., 2025) ZeroSearch (Sun et al., 2025a) Webthinker (Li et al., 2025c) Deepseek, , Claude, Gemini, Qwen Qwen2.5-7B-Instruct, Qwen2.5-7B-Base, Qwen-2.5-3B-Instruct, Qwen-2.5-3B-Base Qwen2.5-3B-Base, Qwen2.5-7B-Base, Qwen2.5-7B-Instruct, Qwen2.5-3B-Instruct, LLaMA3.2-3B-Instruct, LLaMA3.2-3B-Base GPT-o1, GPT-o3, Deepseek-R1, QwQ-32B, Qwen2.5-32B-Instruct"
        },
        {
            "title": "Single",
            "content": "GPQA, GAIA, WebWalkerQA, Humanitys Last Exam"
        },
        {
            "title": "Link",
            "content": "SuperGPQA, WebWalkerQA, OpenThoughts, NaturalReasoning, NuminaMath N/A N/A N/A N/A N/A N/A N/A N/A nanoDeepResearch OpenAI series, Claude"
        },
        {
            "title": "Prompting",
            "content": "N/A"
        },
        {
            "title": "Centralized",
            "content": "DeerFlow Qwen,"
        },
        {
            "title": "Prompting",
            "content": "N/A"
        },
        {
            "title": "Decentralized",
            "content": "deep-research open-deep-research OpenAI series, Deepseek, Deepseek,"
        },
        {
            "title": "Prompting\nPrompting",
            "content": "N/A N/A"
        },
        {
            "title": "Decentralized",
            "content": "DeepResearcher (Zheng et al., 2025) R1-Searcher (Song et al., 2025) ReSearch (Chen et al., 2025a) Search-o1 (Li et al., 2025b) Claude, Gemini Qwen2.5-7B-Instruct Qwen2.5-7B-Base, Llama3.1-8B-Instruct Qwen2.5-7B-Instruct, Qwen2.5-32B-Instruct QwQ-32B-Preview GRPO, Reinforce++, SFT GRPO Prompting Retrieval, Format Web Search, Local Retrieval Format, Answer N/A"
        },
        {
            "title": "Web Search",
            "content": "Web Search r1-reasoning-rag Deepseek Prompting N/A Open Deep Search (Alzubi et al., 2025) node-DeepResearch deep-research Llama3.1-70B, Deepseek-R1 Gemini, Gemini, OpenAI series, Deepseek, Claude, Grok Prompting N/A Prompting Prompt N/A N/A Local Retrieval, Web Search Web Search Web Search Local Retrieval, Web Search NQ, TQ, HotpotQA, 2WikiMultiHopQA HotpotQA, 2WikiMultiHopQA MuSiQue, Bamboogle, PopQA, NQ, TQ, HotpotQA, 2WikiMultiHopQA HotpotQA, 2WikiMultiHopQA, MuSiQue, Bamboogle"
        },
        {
            "title": "MuSiQue",
            "content": "N/A N/A N/A N/A N/A HotpotQA, 2WikiMultiHopQA, MuSiQue, Bamboogle GPQA, MATH500, AMC2023, AIME2024, LiveCodeBench, Natural Questions, TriviaQA, HotpotQA, 2Wiki, MuSiQue, Bamboogle N/A SimpleQA, FRAME N/A N/A"
        },
        {
            "title": "Single",
            "content": "Single Single Single Single Single"
        },
        {
            "title": "Link",
            "content": "Link Link Link Link Link Table 5: Overview of deep research implementations. able application scenarios. Reasoning workflows vary from linear chain-based approaches, which are efficient but vulnerable to error propagation, to more complex tree-based and graph-based methods that offer higher recall and transparency at the cost of increased computational overhead. Similarly, agent orchestration strategies range from singleagent setups to multi-agent systems that distribute specialized roles among agents, enhancing robustness and scalability. However, these advanced designs often introduce additional communication overhead and complexity in conflict resolution. This comparison illustrates the trade-offs inherent in choosing particular workflows or orchestration architectures and underscores the need for adaptive systems that can dynamically balance efficiency, accuracy, and resource constraints in real-world applications. additional complexity in coordination and communication, as well as higher computational costs. Alongside these developments in agent orchestration, the nature of retrievers used in deep research has also evolved significantly. Early RAG systems relied on sparse keyword-based retrieval, later surpassed by dense retrievers employing biencoder architectures for semantic matching. More recent deep research systems increasingly integrate web search-based retrievers, allowing real-time access to open-domain information. Some retrievers have also been transformed into LLM-callable tools for flexible invocation. This evolution of retrievers has played crucial role in enabling the sophisticated information-gathering processes required for deep research."
        },
        {
            "title": "C Comparison of Reasoning Workflows\nand Agent Orchestration Strategies",
            "content": "Table 6 summarizes the diverse reasoning workflows and agent orchestration strategies employed in Synergized RAG-Reasoning systems, highlighting their respective strengths, limitations, and suit25 Single-hop or short multi-hop QA where each intermediate fact is easy to access. Ambiguous or multiple plausible paths tasks (e.g., HotpotQA, legal reasoning) where missing one clue kills accuracy. Deep-search problems under tight API-call or token budgets (e.g., biomedical QA)."
        },
        {
            "title": "Category",
            "content": "Sub-category"
        },
        {
            "title": "Reasoning\nWorkflow",
            "content": "Chain-based Tree-based (ToT) One retrieval per reasoning step; low latency and token cost. Easy to cache and monitor. High recall: explores multiple branches in parallel, hedges against early errors. Transparent what-if traces. An early wrong sub-query propagates; context grows fast on long chains. Quadratic cost; tree branches require many retrieval calls. Tree-based (MCTS) Budget-aware exploration: focuses calls on promising branches; graceful anytime stopping. Graph-based (Walk-on-Graph) Efficient in explicit KG/document graphs; short reasoning paths on KGs. Graph-based (Think-on-Graph) Adaptive and verifiable; LLM updates live evidence graph, allowing node-level citation checks and high factual accuracy. Tuning-heavy and may converge to suboptimal subtree. Requires high-quality KGs; fails if graphs lack explicit edges; less flexible for open-web contexts. Enterprise or domain-specific QA where curated KG exists (e.g., product catalogs). Higher latency; many micro-tool calls; search space can explode without pruning. Open-domain deep research or fact-dense synthesis tasks (e.g., BrowseComp, systematic reviews)."
        },
        {
            "title": "Agent\nOrchestration",
            "content": "Single-agent (Prompt-only) Simple implementation via ReAct loop; low resource overhead. Constrained by prompt engineering and system design flexibility. Prototyping demos and small-scale applications where simplicity outweighs performance. Single-agent (SFT) Clear, well-defined RAG and reasoning patterns; higher precision than prompt-only approaches. Requires large synthetic data; may overfit tool schemas, reducing out-of-domain generalization. Production chatbots with stable APIs and predictable query formats (e.g., internal customer support). Single-agent (RL) Adaptive RAG and reasoning yields high recall and accuracy; learns when to retrieve and reason. Challenging to define suitable reward signals; computationally expensive to train. Open-domain research or long-form QA where call costs are high and optimal stop conditions matter. Multi-agent (Decentralized) High recall via parallel domain experts; robustness to noisy or diverse corpora. High communication and consensus overhead; conflicting answers require resolution. Large-scale evidence aggregation across heterogeneous sources (e.g., meta-analysis, news tracking). Multi-agent (Centralized/Hierarchical) Budget-efficient: manager avoids duplicate searches and ensures clear provenance chain. Scales horizontally without exponential cost growth. Manager prompts or policies can become single-point bottleneck, limiting performance. Complex tasks requiring coordinated subtasks under strict API-call budgets. Table 6: Comparison of reasoning workflows and agent orchestration in Synergized RAG-Reasoning systems."
        }
    ],
    "affiliations": [
        "HKUST",
        "Peking University",
        "The University of Tokyo",
        "Tsinghua University",
        "University of California, Los Angeles",
        "University of Illinois Chicago",
        "University of Illinois Urbana-Champaign"
    ]
}