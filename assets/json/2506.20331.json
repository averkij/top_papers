{
    "paper_title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content",
    "authors": [
        "Rian Touchent",
        "Nathan Godey",
        "Eric de la Clergerie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. Biomed-Enriched: Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content 5 2 0 2 5 2 ] . [ 1 1 3 3 0 2 . 6 0 5 2 : r Rian Touchent, Nathan Godey & Eric de la Clergerie Sorbonne Universit√© INRIA Paris 48 rue Barrault, Paris 75013, France {firstname.lastname}@inria.fr almanach/Biomed-Enriched"
        },
        {
            "title": "Abstract",
            "content": "We introduce Biomed-Enriched, biomedical text dataset constructed from PubMed via two-stage annotation process. In the first stage, large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful paragraph is for college-level learning. These annotations are then used to fine-tune small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by 5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by 1%. Combinations of these techniques led to faster convergence, reaching same performance with third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across wide range of general tasks, from question answering to code generation. However, their performance often lags in specialized domains such as biomedical and clinical medicine, which demand domain expertise and precise terminology. This performance gap can be explained by the composition of standard pre-training corpora, which predominantly consist of web-scraped content from CommonCrawl. While diverse, these datasets lack sufficient 1 Preprint. Under review. representation of specialized knowledge required for complex biomedical reasoning. Although pre-training datasets are often supplemented with high-quality domain-specific corpora like PubMed, these curated additions represent only small fraction compared to the vast amount of general web text (Li et al., 2024). Available clinical text is particularly scarce in public datasets, with hospital records and clinical notes largely inaccessible due to strict privacy regulations. The situation is further complicated for non-English biomedical content, with resources like PMC containing over 98% English articles. central challenge in developing effective domain-specialized LLMs is therefore identifying strategies for curating, filtering, and upsampling domain-relevant documents to enhance performance on specialized tasks without compromising general capabilities."
        },
        {
            "title": "2 Related work",
            "content": "To address the domain gap issue, researchers have employed continual pre-training on large biomedical corpora such as PubMed and particularly its PMC Open Access Subset to enhance domain knowledge in LLMs. BioMistral (Labrak et al., 2024), for instance, underwent continual pre-training on 3 billion tokens from the PMC Open Access Subset, while Meditron (Chen et al., 2023) fine-tuned Llama-2 on 46 billion tokens comprising PubMed abstracts, full papers, general domain replay dataset, and clinical guidelines. Similarly, PMCLlama (Wu et al., 2024) processed 75 billion tokens from PMC Open Access and medical textbooks, achieving significant improvements on biomedical benchmarks. However, this process is compute-intensive for moderate increase in performance. Meditron-70B (Chen et al., 2023) required 128 A100 GPUs for 332 hours to achieve an average accuracy improvement of 1.8 percentage points on biomedical benchmarks, while BioMistral-7B (Labrak et al., 2024) used 32 A100 GPUs for 20 hours, resulting in 0.9-point performance decrease initially, but 2.9-point improvement when using ensembling with different merging techniques with the original model. PMC Open Access contains significant diversity and heterogeneity. Researchers typically employ filtering and upsampling strategies to better control training data composition. For instance, BioMistral (Labrak et al., 2024) noted that 98.75% of PMC Open Access articles are in English, leading them to upsample non-English articles. Meditron (Chen et al., 2023) focused on high-quality, clinically relevant research by scoring articles (01) using MeSH tags, publication type, journal reputation, recency, and citation count. They filtered out low-scoring content while increasing the representation of higher-scoring articles. These methods, however, operate at the article level, overlooking valuable information in predominantly English articles that contain sections in other languages, or in articles that might be deemed low-quality overall but contain specific high-value passages. More sophisticated filtering approaches have emerged to enhance pre-training data quality. While basic heuristic filtering using rules and perplexity scores from small language models trained on wikipedia showed improvements in language modeling, LLM-based semantic quality filtering has proven substantially more effective. FineWeb-Edu (Penedo et al., 2024) demonstrated the efficacy of model-based filtering by using Llama-3-70B-Instruct to annotate 500K documents from the FineWeb corpus based on educational value on scale of 1 to 5. They then trained smaller BERT-like model on these annotations and applied it to the entire FineWeb corpus, filtering out samples with scores below 3. Despite removing 92% of the initial dataset, this refined subset outperformed both the complete FineWeb corpus and other open web datasets on knowledge-intensive benchmarks like MMLU (Hendrycks et al., 2021), ARC (Clark et al., 2018), and OpenBookQA (Mihaylov et al., 2018). WebOrganizer (Wettig et al., 2025) takes complementary approach by organizing web content into structured taxonomies based on both topic and format. Rather than focusing solely on quality metrics, it unpacks monolithic web corpora into well-defined categories by distilling annotations from large language models into efficient classifiers. This systematic organization enables more refined data mixing strategies that improve model performance on downstream tasks. Importantly, their work demonstrates that domain-based organization provides valuable complementary benefits to quality-based filtering methods, as the two approaches can be combined to further enhance performance. Preprint. Under review. In our work, we develop more refined approach to biomedical dataset curation through Biomed-Enriched, which applies LLM-driven annotation at the paragraph level rather than the document level. Building on techniques from FineWeb-Edu (Penedo et al., 2024) and WebOrganizer (Wettig et al., 2025), we focus on biomedical content from PMC Open Access, creating rich metadata about paragraph type, domain, educational quality, and language. This fine-grained approach enables more meaningful filtering and upsampling strategies that capture valuable information overlooked by article-level methods, including highquality passages within lower-quality articles and non-English segments in predominantly English publications. We use two-stage annotation process based on smaller follow-up model to efficiently process the entire corpus. The detailed annotation allows us to extract valuable subsets particularly content related to clinical cases typically restricted due to privacy concerns. Our experiments show that this targeted data curation substantially improves efficiency in biomedical pre-training, resulting in faster convergence and targeted enhanced performance on domain-specific tasks. Our contributions can be summarized in the following points: We introduce Biomed-Enriched, biomedical dataset created through two-stage annotation process that enables fine-grained extraction of high-value content subsets from PubMed. We provide large-scale collection of openly available clinical cases (2M paragraphs, including 450K high-quality ones), addressing critical gap in accessible clinical text resources typically restricted by privacy regulations. We demonstrate targeted performance improvements through domain-specific upsampling, with clinical content upsampling yielding 5% gains on MMLU ProfMed and educational quality filtering of paragraphs improving medical QA tasks by 1%. We establish that strategically combining quality filtering and domain upsampling significantly improves data efficiency and targeted model performance, achieving equivalent scores using only one-third of the training tokens compared to standard approaches."
        },
        {
            "title": "3 Method",
            "content": "We present Biomed-Enriched, biomedical text dataset for enhanced biomedical training constructed through paragraph-level annotation and filtering. Our approach addresses the limitations of existing article-level filtering strategies by enabling more granular selection of high-value content. This is particularly relevant for clinical text which is traditionally difficult to access due to privacy constraints. 3.1 Data Collection and Preprocessing We extracted text from the PubMed Central (PMC) Open Access Subset (pmc), containing approximately 4.5 million full-text scientific articles. This corpus, while valuable, presents challenges including heterogeneous quality, predominance of English content ( 98%), uneven representation of clinical cases, and variable educational value. Using custom pipeline, we processed the raw XML files to extract article content, segment articles into 133M individual paragraphs, filter out non-textual elements, and retain only paragraphs containing minimum of 64 tokens. 3.2 Two-Stage Annotation Framework Our annotation process employed two-stage approach. First, we used Llama-3.1-70BInstruct (Touvron et al., 2024) to annotate diverse subset of 400,000 paragraphs from the PMC Open Access corpus across multiple dimensions: type classification, domain categorization, educational quality, and language identification. The annotation prompt instructed the model to assess each paragraph independently with reasoning for each rating (the complete prompt is provided in Appendix A.1). Preprint. Under review. Category Description Document Type: Clinical Case Detailed report of symptoms, diagnosis, treatment, and follow-up of Study Review Other Domain: Clinical Biomedical Other individual patients Research paragraph with methods, results, and discussion of experiments or observations Summary or synthesis of current knowledge on specific topic Content not fitting above categories (editorials, commentaries, policy paragraphs) Content relating to patient care, clinical trials, case reports, or practice guidelines Scientific aspects of medicine and biology Content mentioning biomedical topics but focusing on administrative, policy, or general communications Educational Quality: Score 1 Score Score 3 Score 4 Score 5 Basic information relevant to biomedical topics, may contain irrelevant content Addresses biomedical education elements but with limitations in coherence or depth Appropriate for college-level curricula, introduces key concepts with reasonable coherence Highly relevant educational content with clear writing style, minimal irrelevant information Outstanding educational value, detailed reasoning with profound insights for college-level learning Table 1: Classification used in our annotation framework. Document types categorize the structure and purpose of the content, domains identify the subject area focus, and educational quality scores assess pedagogical value for college-level biomedical learning on scale from 1 (minimal value) to 5 (exceptional value). To scale annotation to the full corpus, we distilled the LLM annotations into smaller XLM-RoBERTa-base model (Conneau et al., 2020) trained to jointly predict all annotation dimensions using standard multi-task learning framework. The distilled model achieved strong performance with 0.805 F1 score for domain classification, 0.854 F1 score for document type classification, and 0.245 MSE on educational quality score prediction, enabling efficient annotation of the entire corpus. 3.3 Dataset Construction and Filtering Using the distilled model, we annotated the full corpus and constructed several dataset variants through strategic filtering and upsampling: BE-Base: The complete unmodified PMC Open Access Subset serving as baseline. BE-Educational: Preserves all articles but removes paragraphs with educational quality scores below 3. BE-Clinical: Replicates articles with predominantly clinical domain content 10 in the training mix. BE-ClinicalCase: Replicates articles containing at least one clinical case paragraph 10 to increase exposure to clinical narratives. BE-Prefix: Prefixes each paragraph with its predicted annotations to allow modeling of metadata-content relationships. 4 Preprint. Under review. Figure 1: Distribution of educational quality scores by document type and domain. Reviews and studies show the highest proportion of high scores, while clinical texts display more variance. Figure 2: Performance comparison across dataset variants showing training progression. BE-All achieves target performance with approximately one-third of the training tokens required by BE-Base. BE-French: Upsamples articles containing French text 10 to address language imbalance. BE-All: Combines quality filtering (score 3), upsampling of clinical content, French text, and clinical cases, plus metadata prefixing. For all variants, we preserved the original article structure. To maintain the contextual relationships between paragraphs within scientific articles, we employed an 8K context window during pre-training. This approach ensures that models can process complete scientific articles, allowing them to capture dependencies where information presented in earlier paragraphs is essential for properly understanding later content."
        },
        {
            "title": "4 Data Analysis",
            "content": "Score distribution. The majority of PubMed paragraphs are rated with an educational score of 4, with mean of 3.48 and median of 4.00. This indicates strong skew toward moderately high-quality educational content, which supports the feasibility of filtering based on score. Score by document type. Reviews and studies are the richest sources of educational content: 86.9% of review paragraphs and 78.7% of study paragraphs score 4. Clinical cases, while less dominant, still show significant share of high scores (57.0% rated 4). 5 Preprint. Under review. Parameter Peak learning rate Minimal LR LR Decay Batch size Weight decay Context length Hardware Training time (hours / GPU-hours) Value 6.15e-5 6.15e-6 Linear 1024 0.1 8,192 tokens 128 MI250X GPUs 68 / 8700 Table 2: Hyperparameters used for continual pre-training Score by domain. Paragraphs tagged as biomedical are more likely to have high educational value (75.3% score 4), while clinical texts show more variance (44.0% score 4). In contrast, paragraphs labeled \"other\" rarely reach high scores (only 2.1% rated 4), validating their exclusion in BE-Educational and BE-All. Implications for filtering. These distributions justify the score threshold (score 3) used in BE-Educational and BE-All, enabling targeted retention of higher-quality educational content while discarding noisy or low-value segments. The correlation between domain or type and educational score also explains why combining filters (as in BE-All) leads to consistent gains across tasks. 4.1 Continual Pre-training Figure 3: Performance on FrenchMedMCQA showing BE-French outperforming other variants, demonstrating effective languagespecific improvement. Continual pre-training served as method to evaluate the relevance and utility of our annotations. Our evaluation focuses on isolating the effects of data curation rather than pursuing state-of-the-art scores on benchmarks. more powerful foundation model would likely yield higher absolute scores but would obscure the precise impact of our dataset. We selected OLMo27B-stage1 (OLMo et al., 2025) as our foundation model for continual pre-training, strategically choosing this intermediate checkpoint to better isolate the impact of our data curation techniques. While stage 1 has already developed strong language modeling capabilities, it precedes the knowledge-intensive tuning of stage 2, providing an ideal balance of baseline capabilities without the risk of catastrophic forgetting of instruction-following abilities during domain adaptation. Notably, the data mix used in phase 1 includes DCLM (Li et al., 2024), which is dataset obtained by filtering web-data using classifier trained on instruct-data. Hence, OLMo2-7B already has relatively strong question-answering capabilities after stage 1. Each Biomed-Enriched variant was trained for exactly 33.6 billion tokens using identical hyperparameters shown in Table 2. We follow the annealing strategy of OLMo2 (OLMo et al., 2025) used in the mid-training phase. By maintaining strict parameter parity across experiments, we created controlled environment focused solely on measuring the effectiveness of different data curation strategies. 6 Preprint. Under review. Medical QA MMLU Medical MedQA MedMCQA PubMedQA Anat Clin Bio Med Gen Prof Avg Llama-3-8B Meditron-70B OLMo2-7B-stage1 BE-Base BE-Clinical BE-ClinicalCase BE-Prefix BE-Educational BE-All 59.70 57.10 45.33 44.85 41.95 42.11 45.72 45.64 47.21 SOTA models (for reference) 57.47 46.80 74.80 76.60 68.89 53.30 74.72 66.70 78.47 76.30 61.85 63. 83.00 69.00 70.22 71.60 69.90 64.49 Benchmark Results by Dataset Variant 41.14 41.91 39.35 39.52 41.76 43.08 42. 75.60 76.40 76.60 76.60 77.80 77.00 76.60 54.81 57.04 53.33 57.04 57.04 57.04 60.00 63.40 64.15 63.40 64.91 64.53 65.28 65. 69.44 70.83 65.28 66.67 68.75 68.06 68.06 53.18 59.54 58.38 59.54 57.23 56.65 58.96 69.00 69.00 66.00 69.00 66.00 71.00 69. 59.93 59.93 63.97 62.87 61.76 58.82 61.40 59.09 60.41 58.70 59.81 60.07 60.29 61.08 Note: MMLU abbreviations: Anat=Anatomy, Clin=Clinical Knowledge, Bio=College Biology, Med=College Medicine, Gen=Medical Genetics, Prof=Professional Medicine. Table 3: Comprehensive performance results across medical benchmarks for different dataset enrichment strategies. 4.2 Evaluation Framework We evaluated how our annotation-guided corpus refinement affects model performance during continual pre-training. We measured performance at regular intervals throughout training on several biomedical benchmarks to understand how effectively models acquire domain knowledge from differently curated datasets. Our evaluation consists of zero-shot testing on MMLU medical subcategories, MedQA, MedMCQA, and PubMedQA. For the French adaptation assessment, we used 5-shot evaluation on FrenchMedMCQA (Labrak et al., 2022). We compared our models against three baselines: OLMo2-7B-stage1, Llama-3.1-8B, and Meditron-70B. Beyond final performance, we specifically analyzed the token-efficiency of each approach . This metric helps identify which annotation dimensions and filtering strategies provide the most valuable training signal for biomedical knowledge acquisition, offering practical insights for efficient domain adaptation."
        },
        {
            "title": "5 Results",
            "content": "Overall performance. BE-All achieved the highest average performance across benchmarks at 61.08%, surpassing BE-Base (60.41%) by small but consistent margin (+0.67 pts, Table 3). Its strongest improvements appeared in MedQA (47.21%), MMLU Anatomy (60.00%), and Clinical Knowledge (65.66%), suggesting the effectiveness of combining multiple targeted enrichment strategies. Clinical enrichment. BE-Clinical significantly boosted performance on MMLU Professional Medicine benchmark (63.97%, +4.04 pts vs. BE-Base, Figure 2). This improvement was stable from early training, highlighting how clinical narratives enhance the models clinical reasoning abilities efficiently. Educational filtering. BE-Educational consistently improved performance on medical question-answering tasks, notably Medical Genetics (71.00%, +2 pts), MedMCQA (43.08%, +1.17 pts), and PubMedQA (77.00%, +0.6 pts). These tasks likely benefit from the knowledge present in educationally high-quality paragraphs (Figure 2). Metadata prefixing. BE-Prefix specifically improved performance on PubMedQA (77.80%, +1.4 pts vs. BE-Base). Providing explicit paragraph-level metadata helped primarily with structured document comprehension, but it had limited benefits for other tasks. 7 Preprint. Under review. General biomedical knowledge trade-off. BE-Base performed better on College Biology (70.83%) than enriched variants. Building biology variant (BE-Bio) could be an interesting future direction, as the current dataset does not specifically target this domain. Non-English enrichment. BE-French showed clear improvements in French medical QA (FrenchMedMCQA), achieving 40.5% accuracy, significantly surpassing BE-Base and the OLMo2-7B-stage1 baseline (38.32%, Figure 1). These results illustrate effective adaptation to non-English contexts using only upsampling of annotated paragraphs which could be applied to other languages. Data efficiency and training stability. As shown in Figure 2, BE-All reached robust benchmark performance using roughly one-third of the tokens required by BE-Base. Individual enrichments (Educational, Clinical) also displayed early and stable improvements, underscoring potential reductions in training time and computational cost."
        },
        {
            "title": "6 Discussion",
            "content": "Our findings demonstrate that strategic data enrichment substantially improves training efficiency and targeted model effectiveness in biomedical pretraining. Benefits of paragraph-level annotation. Paragraph-level annotation allows us to identify specific types of content, such as clinical case descriptions, that may only appear in isolated sections of scientific articles. This makes it possible to collect and focus on content that is otherwise difficult to target at scale, like the 2 million clinical case paragraphs we were able to extract through this approach. Task-specific enrichment strategies. Our results highlight clear task-specific benefits: educational filtering supports knowledge-intensive QA, clinical upsampling improves clinical reasoning, and metadata enrichment enhances structured comprehension. This demonstrates the potential of targeted pretraining strategies, adaptable to other domains or annotations, to better serve specific use cases, beyond aiming for general-purpose models trained with massive compute. Non-English generalization. The successful adaptation via targeted French upsampling highlights our approachs flexibility and effectiveness for non-English scenarios without additional model complexity or extensive computational overhead, which can be applied to other languages. Combined vs. targeted strategies. Combining multiple enrichments (BE-All) generally produced the highest scores, but individual enrichments provide critical insights into their respective contributions and trade-offs. This emphasizes the value of selecting enrichments strategically based on specific tasks rather than relying exclusively on combined approaches. Limitations and trade-offs. Our study has some limitations worth noting. Our experiments used relatively small language models (7B parameters), and larger models might produce different or clearer patterns. Specialized enrichment strategies may slightly reduce broader biomedical knowledge, indicating the need to balance targeted knowledge enhancement against overall knowledge retention. Future studies should explicitly address these comparisons and explore optimal blends of detailed and general-domain knowledge enrichment to further improve biomedical NLP models."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced Biomed-Enriched, dataset created by annotating the PMC Open Access corpus at the paragraph level. This design enabled the discovery of valuable content, such as clinical case narratives and high-quality educational passages, that is often difficult to 8 Preprint. Under review. isolate through broader filtering methods. Our continual pretraining experiments show improved stability and data efficiency, reaching comparable performance using only third of the training tokens. While the combined strategy (BE-All) performs best overall, each targeted enrichment shows distinct strengths, demonstrating how aligning data selection with task requirements can yield meaningful improvements. This highlights the potential of fine-grained, annotationdriven curation to support more focused and efficient pretraining. Rather than aiming for generic models trained on as much data as possible, our findings support shift toward modular, adaptable strategies. The annotation pipeline used here can be extended to new domains, tasks, or languagesoffering flexible foundation for building specialized models that meet the evolving needs of the biomedical NLP community."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was granted access to the HPC resources of IDRIS under the allocation 2024AD011014393R1 made by GENCI."
        },
        {
            "title": "Ethics Statement",
            "content": "This work focuses on the creation and use of biomedical datasets derived from publicly available scientific literature. All data used in this study come from the PubMed Central Open Access Subset, which is explicitly licensed for text and data mining. No private or patient-identifiable clinical data were used or accessed. References PMC open access subset. https://www.ncbi.nlm.nih.gov/pmc/tools/textmining/. Accessed 2025-03-29. Yanis Labrak, Adrien Bazoge, Richard Dufour, B√©atrice Daille, Pierre-Antoine Gourraud, Emmanuel Morin, and Micka√´l Rouvier. FrenchMedMCQA: French MultipleChoice Question Answering Dataset for Medical domain. In Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI), pages 4146, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. https://aclanthology.org/2022.louhi-1.5/. doi: 10.18653/v1/2022.louhi-1.5. Zeming Chen, Alejandro Hern√°ndez-Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K√∂pf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440 8451, 2020. URL https://aclanthology.org/2020.acl-main.747/. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 9 Preprint. Under review. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. BioMistral: collection of open-source pretrained large language models for medical domains. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 58485864, Bangkok, Thailand, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.348. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. URL https://arxiv.org/abs/2406.11794. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 23812391, Brussels, Belgium, 2018. Association for Computational Linguistics. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. Guilherme Penedo, Hynek Kydl√≠Àácek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the web: Constructing domains enhances pre-training data curation. arXiv preprint arXiv:2502.10341, 2025. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. PMCLLaMA: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843, 2024. doi: 10.1093/jamia/ ocae045."
        },
        {
            "title": "A Appendix",
            "content": "A.1 LLM Annotation Prompt Below is the prompt used to instruct Llama-3.1-70B-Instruct for the first stage of our annotation process: 1 Below is an extract from scientific article . Evaluate whether the extract has high educational value and could be useful in an educational setting for teaching at the college level in biomedical sciences using the additive 5point scoring system described below . 2 3 Points are accumulated based on the satisfaction of each criterion : 4 - Add 1 point if the extract provides some basic information relevant to biomedical topics , even if it includes some irrelevant or non - academic content like advertisements and promotional material . Preprint. Under review. 5 - Add another point if the extract addresses certain elements pertinent to biomedical education but does not align closely with academic standards . It might mix educational content with non - educational material , offering superficial overview of potentially useful topics , or presenting information in disorganized manner and incoherent writing style . 6 - Award third point if the extract is appropriate for educational use and introduces key concepts relevant to college - level biomedical curricula . It is coherent though it may not be comprehensive or could include some extraneous information . It may resemble an introductory section of textbook or basic tutorial that is suitable for learning but has notable limitations like treating concepts that are too complex for introductory students . 7 - Grant fourth point if the extract is highly relevant and beneficial for educational purposes at the college level , exhibiting clear and consistent writing style . It could be similar to chapter from textbook or tutorial , offering substantial educational content , including exercises and solutions , with minimal irrelevant information , and the concepts are appropriate for college students . The content is coherent , focused , and valuable for structured learning . 8 - Bestow fifth point if the extract is outstanding in its educational value , perfectly suited for teaching at the college level in biomedical sciences . It follows detailed reasoning , the writing style is easy to follow and offers profound and thorough insights into the subject matter , devoid of any non - educational or overly complex content . 9 10 Based on these factors , give score from 1 to 5. 11 12 Also , classify the relevant domain as either \" biomedical \", \" clinical \", or \" other \" following the guidelines provided below : 13 - Clinical : Extract appears to be written in clinical context by healthcare professional . It should contain information directly related to patient care , such as details from clinical trials , case reports , or clinical guidelines . 14 - Biomedical : Extract contains substantive information on biomedical sciences . It could be from research paper or textbook , focusing on the scientific aspects of medicine and biology . 15 - Other : Extract mentions biomedical or clinical topics but doesn 't provide substantive content in these areas . This category includes : 16 17 18 1. Administrative or funding information about biomedical research 2. General news or public communications about medical topics 3. Policy discussions related to healthcare 4. Any content that talks about biomedical or clinical subjects without providing actual scientific or medical information 20 21 Additionally , identify the type of document , this category includes : 22 1. Clinical case : detailed report of the symptoms , signs , diagnosis , treatment , follow -up , etc . of an individual patient . 23 2. Study : Research - based document that includes methods , results , and discussions about experiments or observations , often involving multiple subjects or data points . 24 3. Review : document that summarizes or evaluates the current state of knowledge on specific topic . 25 4. Other : Any other type of document not fitting the above categories . 26 27 After examining the extract : 28 - Briefly justify your quality classification , up to 100 words on one line using the format : \" Explanation : < justification >\" 29 - Conclude with the quality classification using the format : \" Educational score : < classification >\" 30 - Conclude with the domain classification using the format : \" Domain : < classification >\" 11 Preprint. Under review. 31 - Conclude with the document type classification using the format : \" Document type : < classification >\""
        }
    ],
    "affiliations": [
        "Sorbonne Universit√© INRIA Paris"
    ]
}