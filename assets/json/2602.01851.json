{
    "paper_title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
    "authors": [
        "Huanyu Zhang",
        "Xuehai Bai",
        "Chengzu Li",
        "Chen Liang",
        "Haochen Tian",
        "Haodong Li",
        "Ruichuan An",
        "Yifan Zhang",
        "Anna Korhonen",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research."
        },
        {
            "title": "Start",
            "content": "How Well Do Models Follow Visual Instructions? VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Huanyu Zhang * 1 2 Xuehai Bai * 3 Chengzu Li * 4 Chen Liang 2 Haochen Tian 1 2 Haodong Li 5 Ruichuan An 6 Yifan Zhang 1 2 Anna Korhonen 4 Zhang Zhang 1 2 Liang Wang 1 2 Tieniu Tan 7 https://vibe-benchmark.github.io/ 6 2 0 2 2 ] . [ 1 1 5 8 1 0 . 2 0 6 2 : r Abstract Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose robust LMM-asa-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform opensource models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research. 1. Introduction Recent advances in generative models have demonstrated impressive capabilities in image editing (Google, 2025b; Seedream et al., 2025). However, most existing systems *Equal contribution 1School of Artificial Intelligence, University of Chinese Academy of Science 2Institute of Automation, Chinese Academy of Sciences 3Hangzhou Dianzi University 4Language Technology Lab, University of Cambridge 5South China University of Technology 6Peking University 7Nanjing University. Correspondence to: Huanyu Zhang <huanyu.zhang@cripac.ia.ac.cn>. Preprint. February 3, 2026. 1 Figure 1. Motivation and scope of the VIBE benchmark. Traditional image editing is largely text-guided, where conveying spatial intent relies on verbose descriptions and incurs high cognitive load. In contrast, visual instructions enable precise and explicit grounding, providing more human-aligned interaction paradigm. VIBE is designed to fill the evaluation gap by systematically benchmarking this visual intruction-guided multi-modal image editing. remain predominantly text-guided (Huang et al., 2025; Liu et al., 2025), paradigm that imposes significant doublesided cognitive burden. For the user, conveying precise spatial or structural intent through text alone is often cumbersome task, necessitating verbose and pedantic descriptions. Meanwhile, these dense textual instructions are equally demanding for the models to understand without ambiguity and reconstruct into accurate spatial intent (Li et al., 2024). In contrast, human communication is inherently multimodal: users naturally combine language with visual cues, such as sketches, arrows, or region annotations, to disambiguate intent and achieve precise control. These visual instructions offer more natural and efficient interaction paradigm, enabling explicitly grounded editing that aligns with how humans intuitively reason about visual content (Buxton, 2010; Tversky, 2013). While state-of-the-art models like Nano Banana Pro are beginning to follow these intuitive cues (see Figure 1), existing benchmarks (Zhang et al., 2023b; Zhao et al., 2025b; Wu et al., 2025d) are still limited to text-only guidance, failing to capture the efficiency and clarity of multimodal interaction. VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing, which is designed to systematically evaluate image editing guided by visual instructions. VIBE formalizes visual instructions as spatiallyanchored cues, such as sketches or manipulative vectors, that provide the geometric grounding necessary to resolve the ambiguities inherent in text-only instructions. To evaluate these capabilities, we structure VIBE along three-level interaction hierarchy that represents progression in communicative and reasoning complexity: (1) Deictic grounding to identify and isolate sptaial lcoations; (2) Morphological manipulation to specify shape, pose and geometric properties; (3) Causal reasoning to predict the logical visual consequence of specified action through manipulative vectors. Across this hierarchy, VIBE comprises 10 functionally diverse subtasks, capturing wide range of behaviors from simple attribute swapping to complex structural synthesis. In total, VIBE contains 1,034 samples, all of which are manually annotated or carefully verified by human. For evaluation, we develop specialized evaluation metrics tailored to the objectives of each task, and adopt an LMM-asa-judge framework to assess instruction-following behavior. To verify the reliability and effectiveness of the proposed evaluation framework, we further perform extensive experiments, observing high correlation between LMM-based judgment and human expert assessment. Through extensive benchmarking across 10 open-source and 7 proprietary models, our experiments yield three key findings. First, frontier proprietary models demonstrate emerging visual instruction-following capabilities, indicating reliable spatial grounding under explicit visual instructions. Second, substantial performance gaps persist between proprietary and open-source models across all levels, with proprietary systems consistently achieving higher scores. Third, across all proprietary models, performance degrades from the Deictic level to the Causal level. Even the strongest models achieve average scores below 50% on the Causal level, indicating that complex causal reasoning remains significant challenge. This consistent performance degradation further validates the hierarchical design of VIBE. In summary, our main contributions are as follows: We introduce VIBE, the first benchmark for systematically evaluating visual instruction-guided image editing, establishing comprehensive framework for assessing multimodal instruction-following. We formulate cognitively motivated hierarchy spanning deictic grounding, morphological manipulation, and causal reasoning, and design task-specific evaluation metrics supported by validated LMM-as-a-judge framework for scalable and reliable assessment. We conduct comprehensive evaluation of 17 models, revealing clear capability gaps and offering new insights into the strengths, limitations, and future chalFigure 2. Composition of VIBE. VIBE comprises 1,034 samples across 10 tasks, organized into three-level hierarchy that reflects increasing interaction and reasoning complexity, from deictic grounding and morphological manipulation to causal reasoning. lenges of visual instruction-guided image editing. 2. VIBE To bridge the gap between linguistic instructions and precise image manipulation, we introduce the VIBE benchmark. In this section, we first formalize the concept of visual instructions. Then, we present the hierarchical task suites within VIBE, ranging from deictic grounding to causal reasoning tasks involving physical interactions. Finally, we detail our data construction pipeline and task-specific metrics developed to quantify the precision of multi-modal instruction-following. 2.1. Visual Instruction Formulation In human communication, visual cues are often used to disambiguate intent and anchor meaning in space (Herring, 2015). Motivated by this, we formalize visual instructions as spatially explicit signals that provide direct grounding constraints for image editing. Unlike textual instructions, which require the model to translate abstract linguistic symbols into spatial coordinates, visual instruction acts as direct geometric interface, bridging the gap between highlevel intent and pixel-level execution. Given source image Iin, textual instruction , and visual instruction , provided either as separate image or as overlaid annotations, model ϕ is required to generate an edited output Iout as: Iout = ϕ(Iin, T, ). (1) 2 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 3. Overview of VIBE. VIBE organizes visual instruction-guided image editing into three-level interaction hierarchy with increasing task complexity. The Deictic Level treats visual instructions as selectors that specify localized regions or objects for basic spatial operations. The Morphological Level interprets visual instructions as blueprints that define abstract structural constraints. The Causal Level views visual instructions as catalysts that encode underlying physical or logical dynamics. 2.2. Benchmark Construction Based on how models interpret and execute visual instructions, we organize all tasks in VIBE into three-level interaction hierarchy. As demonstrated in Figure 3, each level reflects an increasing degree of abstraction and reasoning complexity, ranging from spatial grounding to structural realization and causal simulation. Level 1: Deictic Level: At this level, the instruction acts as selector. Markers like bounding boxes or arrows serve as digital deictic cues, indicating the spatial location of an intended edit. This level primarily evaluates models spatial grounding and basic visual awareness. As illustrated in Figure 2, there are four fundamental editing tasks including Addition, Removal, Replacement, and Translation. Specifically, Addition (AD) requires the model to introduce new visual content confined to the designated region, while preserving all pre-existing elements elsewhere. Removal (RM) instructs the model to eliminate the target object or region and plausibly reconstruct the underlying background without leaving residual artifacts. Replacement (RP) involves substituting the content within the specified region with different object, while maintaining the original spatial extent and placement. Translation (TR) focuses on repositioning selected object according to spatial cues, without modifying its visual appearance, structure, or identity. All four tasks share the same set of source images to ensure fair comparison across different editing operations. Besides, to ensure broad coverage of real-world and creative scenarios, we include images spanning diverse visual styles. Specifically, each task contains 100 samples, with balanced distribution of visual styles, including 34 real-world images, 33 animated images, and 33 sketch-based illustrations. And all visual instructions and annotations for the Deictic Level are manually created, guaranteeing precise spatial grounding and unambiguous task intent. Level 2: Morphological Level: At the Morphological Level, visual instructions function as blueprints. Sparse representations, including skeletons or schematic sketches, specify the structural constraints of the target transformation. The model is required to map these abstract forms into coherent, style-consistent geometries and appearances. As shown in Figure 2, we instantiate this level with three tasks: Pose Control, Reorientation, and Draft Instantiation. Pose Control (PC) requires the model to transform character in the input image to exactly match the pose provided by reference image, while preserving the characters identity, 3 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing appearance, and visual style. Reorientation (RO) focuses on aligning the orientation of an object with given reference viewing frustum or directional cue. Draft Instantiation (DI) challenges the model to convert sketch-based annotations overlaid on the input image into fully realized output, while maintaining consistency with the original scene in both content and style. For the Morphological Level, each task is also constructed using 100 distinct samples. In particular, for Draft Instantiation, the visual instructions consist of hand-drawn sketches directly overlaid on the input images, ensuring faithful representation of abstract structural intent. For Pose Control, each sample is constructed from manually curated pair of images, consisting of an input image and reference image that specifies the target pose. For Reorientation, the reference viewing frustums or directional cues are individually annotated by human annotators. These annotations precisely define the intended object orientation and ensure unambiguous structural constraints for evaluation. Level 3: Causal Level: The Causal Level represents the highest level of interaction in our hierarchy. At this level, visual instructions operate as catalysts. Visual cues such as force vectors or motion arrows do not depict the final outcome but instead encode the underlying causal dynamics to be applied. This requires models to possess an internal world model capable of predicting the logical outcome of physical events. As illustrated in Figure 2, this level contains three challenging tasks: Light Control, Flow Simulation, and Billiards. Light Control (LC) requires the model to modify the direction of illumination according to annotated arrows, updating shading, shadows, and highlights consistently with the new lighting direction. Flow Simulation (FS) instructs the model to simulate wind flow based on directional cues. Objects in the scene should respond plausibly to the implied airflow, exhibiting appropriate deformation or motion. Billiards (BI) challenges the model to predict the trajectory of ball under an applied force indicated by arrows. The task requires reasoning about interactions with the environment, including collisions and subsequent rebounds. In the Causal Level, both Flow Simulation and Light Control comprise 100 samples each, where all causal cues and annotations are manually specified to ensure physical plausibility and consistency with common-sense dynamics. The Billiards task is constructed using hybrid approach. We first synthesize 200 candidate cases by scripts, covering scenarios with increasing difficulty ranging from two to seven collisions. These candidates are then manually inspected, resulting in final set of 134 high-quality samples. To maintain high annotation quality, all samples undergo multiple rounds of manual verification. More details about data collection and annotation are provided in Appendix B. 2.3. Evaluation Pipeline Evaluating visual instruction-guided image editing remains challenging due to the open-ended nature of visual outputs and the absence of unique ground truth. Recent studies (Zhao et al., 2025a; OpenAI, 2025; Google, 2025c) have shown that large multimodal models (LMMs) exhibit strong visual reasoning and alignment capabilities, making them suitable as automated evaluators. Following this line of work, we adopt an LMM-as-a-Judge evaluation paradigm. Specifically, we employ frontier LMM (GPT5.11) as the evaluator. For each sample, the evaluator is provided with the input image, the textual instruction, the corresponding visual instruction, and the generated output. The evaluator is required to determine whether the generated result correctly fulfills the specified instruction. Evaluation criteria are designed in task-specific manner to reflect the distinct requirements of different tasks. For tasks in the Deictic Level, the evaluation is conducted along three complementary criteria that jointly capture instruction compliance, locality preservation, and visual quality: Instruction Adherence (IA) measures whether the model faithfully given the specified visual instruction. It is defined as the conjunction of three binary criteria: (i) visual instruction localization correctness, which verifies whether the model correctly identifies the target region specified by the visual cues.; (ii) visual operator type compliance, which checks whether the applied editing operation matches the specified visual operator; and (iii) textual action semantic compliance, which evaluates consistency with the textual instruction. IA is computed as the average of the three scores. Contextual Preservation (CP) evaluates whether nontarget regions remain intact after editing. It is assigned binary score based on whether unintended modifications to background elements, surrounding objects, or the global scene structure are presented, or not. Visual Coherence (VC) measures the perceptual integrity of the edited result. It is defined through three binary sub-criteria: (i) style consistency, assessing whether the edited image conforms to the artistic style of the source image; (ii) visual seamlessness, assessing whether the edited region integrates smoothly with its surroundings; and (iii) artifact-free generation, assessing the absence of visual artifacts such as blurring, distortion, or unnatural seams. VC is computed as the average of the three scores. Instruction Adherence serves as prerequisite for meaning1We use the 2025-11-13 version of GPT5.1 via the Azure API. VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 1. Experimental results on VIBE. We report task-wise and overall performance across the Deictic, Morphological, and Causal levels. The best and second-best results are highlighted in red and blue, respectively. Model Multi-Img Nano Banana Pro Nano Banana GPT-image-1 Seedream 4.5 Seedream 4.0 Wan 2.6 Wan 2.5 FLUX2-dev Qwen-Image-Edit-2509 Qwen-Image-Edit Edit-R1-Qwen-Image-Edit BAGEL-think BAGEL Step1X-Edit-v1p2 OmniGen2 UniWorld-V1 OmniGen AD 82.17 81.34 55.61 81.24 74.02 66.01 73.59 64.57 55.28 44.20 56.77 40.44 33.87 33.92 26.29 15.18 2.63 Deictic Level RP 88.26 79.05 62.63 81.82 79.35 68.15 76.99 54.40 30.13 30.48 29.47 35.38 29.26 28.17 20.84 22.03 5.26 TR 74.80 46.53 47.00 48.93 33.29 40.95 36.80 5.58 14.48 11.11 11.33 14.46 14.05 13.52 4.51 3.59 1.23 RM 94.07 93.50 69.00 95.82 93.04 92.90 96.90 8.00 14.38 24.88 4.86 14.59 11.33 12.59 26.20 14.52 7.48 Morphological Level Avg 84.83 75.11 58.56 76.95 69.93 67.00 71.07 33.14 28.57 27.67 25.61 26.22 18.21 22.05 19.46 13.83 4. PC 72.33 67.71 64.39 66.79 58.27 59.66 55.76 28.76 15.14 - 16.23 8.50 7.61 - 11.40 11.95 5.79 RO 36.04 33.45 11.09 20.11 30.37 34.89 25.77 22.77 17.67 21.33 20.27 23.60 28.05 25.17 17.44 16.57 13.88 DI 88.02 85.60 77.32 82.33 72.09 80.23 78.78 60.68 21.38 54.65 15.42 50.34 48.23 71.48 30.51 34.43 3.93 Avg 65.46 62.25 50.93 56.41 53.58 58.26 53.44 37.40 18.06 - 17.31 27.48 27.96 - 19.78 20.98 7.87 LC 60.34 34.75 25.18 50.50 47.00 44.46 33.33 33.74 28.00 22.00 25.67 21.17 21.57 25.00 17.33 15.50 2.33 Causal Level FS 59.25 52.64 39.48 45.55 43.59 50.79 51.98 30.81 44.40 32.38 40.22 28.04 33.63 29.67 17.61 9.28 2. BI 15.92 1.87 4.73 2.99 4.11 9.08 7.84 2.36 2.36 3.11 2.24 5.22 5.97 0.37 2.74 0.00 0.00 Avg 45.17 29.75 23.13 33.01 31.57 34.78 31.05 22.30 24.92 19.16 22.71 18.14 20.39 18.35 12.56 8.26 1.44 Overall 65.15 55.70 44.21 55.46 51.69 53.35 51.85 30.95 23.85 23.42 21.87 23.95 22.19 20.20 17.27 14.36 4.49 ful visual evaluation. If IA equals zero, VC is automatically set to zero. Similar to VIEScore (Ku et al., 2024), the final score for each sample is computed as the geometric mean of the three criteria: evaluator in our evaluation framework. To reduce stochastic variance and improve score stability, each sample is evaluated three times independently, and the final score is reported as the average over the three runs. Score = (IA CP VC) 1 3 . (2) 3.1. Main Results on VIBE This formulation ensures that high scores are assigned only when correct instruction execution, strict context preservation, and coherent visual synthesis are simultaneously achieved. To improve scoring reliability and reduce evaluator ambiguity, we design the majority of sub-metrics as binary decisions, encouraging the evaluator to focus on clear-cut judgments. Evaluation metrics for other tasks are designed following the similar principles as those of the Deictic Level, and detailed definitions as well as the exact evaluation prompts are provided in the Appendix D. 3. Experiments To ensure comprehensive evaluation, we benchmarked total of 17 models, categorized into proprietary and opensource systems. The proprietary category includes leading commercial models: Nano Banana (Google, 2025a) and its Pro variant (Google, 2025b), GPT-image-1 (Hurst et al., 2024), the Seedream series (4.0 and 4.5) (Seedream et al., 2025), and the Wan series (2.5 and 2.6) (Wan, 2025). Besides, the open-source models comprise FLUX2-dev (Black Forest Labs, 2025), Qwen-Image-Edit-2509, Qwen-ImageEdit (Wu et al., 2025a), Edit-R1-Qwen-Image-Edit-2509 (Li et al., 2025c), Bagel (Deng et al., 2025), Step1X-Edit-v1p2 (think + reflection version) (Liu et al., 2025), UniWorldV1 (Lin et al., 2025), and the OmniGen series (OmniGen (Xiao et al., 2025) and OmniGen2 (Wu et al., 2025b)). As discussed in Section 2.3, we employ GPT-5.1 as the We report the performance score on 100-point scale in Table 1, covering task-wise results, level-wise averages, and overall performance across all evaluated models. These results provide comprehensive assessment of model capabilities under progressively more demanding visual instruction settings. The results reveal several consistent performance trends across interaction levels and model categories. Proprietary models exhibit early-stage visual instructionfollowing capabilities. Specifically, nearly all proprietary models achieve scores above 60 on Addition, Removal, and Replacement tasks in the Deictic Level, suggesting reliable performance on explicit, region-based visual instructions. These models also perform well on more fine-grained visual instruction tasks, such as Pose Control and Draft Instantiation, which require structured manipulation guided by reference images or sketches. In contrast, performance is comparatively moderate on tasks involving more abstract directional or referential visual instructions, including Translation and Reorientation. These tasks demand reasoning over spatial relations and orientation changes, which are less directly grounded by localized visual cues. Together, these results suggest that while proprietary models have begun to acquire foundational visual instruction-following capabilities, challenges remain in handling abstract, direction-based instructions that require higher-level spatial reasoning. Substantial performance gaps persist between proprietary and open-source models across all levels. As shown VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 4. Performance across image styles on the Deictic Level. Left: Average Deictic Level scores across real-world, animation, and sketch images for four proprietary models. Right: Metric-level heatmaps for Seedream 4.5 and GPT-Image-1, illustrating style-dependent variations in Instruction Adherence, Contextual Preservation, and Visual Coherence. lect four proprietary models from different organizations, including Nano Banana Pro, Seedream 4.5, Wan 2.5 and GPT-Image-1, and report their average Deictic Level scores across styles in the left chart of Figure 4. We find that models exhibit distinct and model-specific preferences across visual styles on Deictic Level tasks. As shown in Figure 4, Nano Banana Pro and Wan 2.5 display relatively balanced performance across all three styles. In contrast, Seedream 4.5 shows pronounced performance degradation on sketch-style images, whereas GPT-Image1 shows the opposite tendency, achieving its strongest results on real-world images. Such differences are likely attributable to variations in training data composition and representation biases across models. To further investigate the source of the observed style preferences, we conduct more fine-grained analysis at the metric level. Specifically, we visualize the performance of Seedream 4.5 and GPT-Image-1 on individual evaluation metrics using the heatmap shown on the right side of Figure 4. For Seedream 4.5, the degradation on sketch-style images can be primarily attributed to substantial drop in Instruction Adherence and Visual Coherence, which are markedly lower than those on real-world and animation images. Qualitative examples in Figure 9 further illustrate this behavior, where it often fails to generate objects that are stylistically consistent within the sketch domain. In contrast, GPT-Image-1 shows clear preference for real-world images, where it outperforms animation and sketch inputs across all evaluation metrics. As result, its overall scores are highest on real-world inputs, as shown in Figure 4. In addition, we analyze style-wise performance on the Draft Instantiation task, as illustrated in Figure 5. Unlike Deictic Level tasks, this task exhibits clearer style preference, where most models achieve notably higher performance on animation-style images. This trend suggests that animated images, which often feature cleaner contours and more explicit structural cues, may better align with sketch-based visual instructions. Complete quantitative results and representative examples are provided in the Appendix C.2. Figure 5. Style-wise performance on Draft Instantiation. in Table 1, proprietary systems achieve substantially higher overall scores than open-source models. This performance gap reflects the advantages of proprietary models in terms of model scale and training data diversity, which contribute to more accurate interpretation and execution of visual instructions across all levels. At the same time, these results highlight clear opportunity for future open-source research to narrow this gap by improving instruction understanding, multimodal alignment, and higher-level reasoning capabilities. Performance on proprietary models degrades from the Deictic Level to the Causal Level. When results are aggregated at different task levels, proprietary models exhibit clear performance degradation from the Deictic Level to the Morphological Level, and further to the Causal Level. The observed degradation reflects the increasing interaction complexity, where higher levels require not only localized editing but also structural abstraction and causal reasoning. These results therefore validate the hierarchical design of VIBE, demonstrating systematic performance decline as models are challenged with progressively more demanding forms of visual instruction. Detailed error analyses across tasks and models are provided in Appendix C. 4. Discussion and Analysis 4.1. Style-wise Performance Analysis As described in Section 2.2, the four Deictic Level tasks share common set of 100 source images evenly distributed across real-world, animation, and sketch styles. We se6 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 6. Qualitative examples of multi-task visual instruction following. Examples illustrating model behavior under composed visual instructions. In the first two rows, models correctly execute individual instructions in isolation but fail when the same instructions are combined. The third row shows case where model succeeds with two instructions but fails when an additional instruction is introduced. Table 2. Single-task vs. Multi-task visual instruction following. Quantitative results of five proprietary models evaluated on singletask, double-task, and triple-task compositions of Deictic Level. Single Task Double Tasks Triple Tasks Nano Banana Pro Nano Banana GPT-image-1 Seedream 4.5 Seedream 4.0 84.83 75. 58.56 76.95 69.93 80.22 65.66 47. 60.16 62.62 75.48 66.77 42.85 61. 49.15 4.2. Multi-task Visual Instruction Following Motivated by the observation that proprietary models exhibit early-stage visual instruction-following capabilities on individual Deictic Level tasks, we further explore their performance under more demanding multi-task settings. Specifically, we examine whether models can follow multiple visual instructions within single query by constructing multi-task evaluations based on Deictic Level tasks. We form 6 two-task combinations by pairing all four tasks, and 4 three-task combinations by further composing them. To ensure comparability with the single-task setting, images are curated with balanced visual styles. For each task combination, we select five images per style (real-world, animation, and sketch), yielding 15 images per combination. Visual instructions are manually verified to ensure that different tasks do not overlap spatially or semantically. In total, this process yields 90 double-task cases and 60 triple-task cases. We evaluate five proprietary models, as reported in Table 2. Across all models, performance exhibits clear drop when Figure 7. Pearson correlation between human expert scores and LMM-based evaluation scores for Nano Banana Pro and GPTImage-1, demonstrating strong alignment between human judgments and the LMM-as-a-Judge evaluator. moving from single-task to multi-task settings. Figure 6 provides qualitative examples illustrating this behavior, where models successfully execute individual visual instructions in isolation but fail to correctly compose multiple instructions within single query. This trend indicates that the simultaneous execution of multiple visual instructions is more demanding for current models than single-instruction settings, even when individual instructions are handled correctly in isolation. The observed performance degradation highlights gap between single-instruction competence and compositional instruction understanding, indicating an important direction for future model development. 7 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 8. Two representative cases illustrating how textual and visual instructions interact. The first case shows that visual instructions can resolve target ambiguity that detailed text alone fails to address. The second case demonstrates that complex semantic constraints require the joint use of detailed textual and visual instructions. 4.3. Validity of LMM-as-a-Judge To validate the reliability of using LMM as evaluators, we analyze the correlation between LMM-based assessments and human expert judgments. Specifically, we randomly sample results from two representative models, Nano Banana Pro and GPT-Image-1. For each model, we select 10 edited samples from each of the 10 tasks, resulting in 100 evaluated samples per model. Four human experts independently assess all selected samples using the same evaluation criteria and metrics as the LMM evaluator. The annotation interface used for human evaluation is illustrated in Figure 13. Human scores are obtained by averaging the scores across the four experts. We then compute the Pearson correlation coefficient between the human-annotated scores and the scores produced by the LMM evaluator. As shown in Figure 7, our LMM-based evaluations exhibit strong correlation with human judgments. The overall Pearson correlation coefficient across all samples reaches = 0.9602. When analyzed by model, the correlation remains consistently high, with = 0.9673 for Nano Banana Pro and = 0.9531 for GPT-Image-1. These results indicate high level of agreement between the LMM evaluator and human experts across different tasks and models. Together, this analysis demonstrates that the proposed LMMas-a-Judge framework provides reliable and human-aligned evaluations across diverse tasks and models. 4.4. Synergy Between Textual and Visual Instructions We further conduct qualitative analysis to examine how textual and visual instructions interact in guiding image editing. Figure 8 presents two representative cases from 8 state-of-the-art models that illustrate the complementary and synergistic roles of the two instruction modalities. In the first case, we observe that providing highly detailed textual instruction alone does not guarantee correct editing behavior. Despite explicitly specifying the editing target in text, the model fails to localize the intended region. In contrast, when simple visual instruction is used to directly indicate the target object, much shorter and less detailed textual prompt suffices to produce the correct result. This example suggests that visual instructions provide strong grounding signal for target localization, effectively reducing ambiguity that is difficult to resolve through language alone. The second case reveals different failure mode. Here, neither detailed textual instruction alone nor combination of brief textual instruction with visual cue leads to correct outcome. Only when detailed textual instruction is paired with an explicit visual instruction does the model succeed. This indicates that visual instructions may benefit from complementary textual specification when complex semantic constraints need to be expressed. Together, these examples highlight that textual and visual instructions play distinct yet complementary roles in image editing. Visual instructions enable precise spatial grounding, while textual instructions are essential for conveying semantic intent and complementary information. Additional qualitative cases with visually embedded instructions are provided in Appendix C.4. VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing 5. Conclusion In this paper, we introduced VIBE, benchmark designed to systematically evaluate visual instruction-following capabilities in image editing. By organizing tasks into threelevel hierarchy consisting of the Deictic, Morphological, and Causal Level, we provide structured framework for assessing increasingly complex forms of visuallinguistic interaction. Through extensive experiments on wide range of state-of-the-art proprietary and open-source models, we show that current systems demonstrate early-stage competence on explicit, localized visual instructions, while performance degrades as interaction complexity increases. Our analysis further reveals pronounced differences across models, visual styles, and task compositions, highlighting challenges in reasoning, compositional instruction execution, and multi-task coordination. We hope that VIBE will serve as useful testbed for future research, encouraging the development of models with improved visual instructionfollowing and coordination with textual guidance."
        },
        {
            "title": "References",
            "content": "Black Forest Labs. intelligence. https://bfl.ai/blog/flux-2, December 2025. Flux.2: Frontier visual Buxton, B. Sketching user experiences: getting the design right and the right design. Morgan kaufmann, 2010. Chen, Z., Bai, X., Shi, Y., Fu, C., Zhang, H., Wang, H., Sun, X., Zhang, Z., Wang, L., Zhang, Y., et al. Opengpt4o-image: comprehensive dataset for advanced image generation and editing. arXiv preprint arXiv:2509.24900, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Fang, R., Duan, C., Wang, K., Huang, L., Li, H., Yan, S., Tian, H., Zeng, X., Zhao, R., Dai, J., et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Feng, K., Gong, K., Li, B., Guo, Z., Wang, Y., Peng, T., Wu, J., Zhang, X., Wang, B., and Yue, X. Video-r1: Reinforcing video reasoning in MLLMs. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=a2JTVVvcEl. Google. age, https://developers.googleblog.com/ Introducing state gemini of-the-art 2.5 image flash our immodel. introducing-gemini-2-5-flash-image/, August 2025a. Google. Introducing nano banana pro. https://blog. google/technology/ai/nano-banana-pro/, November 2025b. Google. new era of intelligence with gemini 3. https: //blog.google/products-and-platforms/ products/gemini/gemini-3/, November 2025c. Gu, S., Bao, J., Yang, H., Chen, D., Wen, F., and Yuan, L. Mask-guided portrait editing with conditional gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 34363445, 2019. Herring, S. C. New frontiers in interactive multimodal communication. In The Routledge handbook of language and digital communication, pp. 398402. Routledge, 2015. Huang, Y., Huang, J., Liu, Y., Yan, M., Lv, J., Liu, J., Xiong, W., Zhang, H., Cao, L., and Chen, S. Diffusion modelbased image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jiang, S., Zhang, Y., Zhou, C., Jin, Y., Feng, Y., Wu, J., and Liu, Z. Joint visual and text prompting for improved object-centric perception with multimodal large language models. arXiv preprint arXiv:2404.04514, 2024. Ku, M., Jiang, D., Wei, C., Yue, X., and Chen, W. Viescore: Towards explainable metrics for conditional image synIn Proceedings of the 62nd Annual thesis evaluation. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1226812290, 2024. Li, C., Zhang, C., Zhou, H., Collier, N., Korhonen, A., and Vulic, I. TopViewRS: Vision-language models as top-view spatial reasoners. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 17861807, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 106. URL https://aclanthology.org/2024. emnlp-main.106/. Li, C., Wu, W., Zhang, H., Li, Q., Gao, Z., Xia, Y., Hernandez-Orallo, J., Vulic, I., and Wei, F. 11plusbench: Demystifying multimodal llm spatial reasonarXiv preprint ing with cognitive-inspired analysis. arXiv:2508.20068, 2025a. 9 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulic, I., and Wei, F. Imagine while reasoning in space: Multimodal visualization-of-thought. In The Forty-Second International Conference on Machine Learning, 2025b. Li, C., Wang, Z., Li, J., Xu, Y., Zhou, H., Zhang, H., An, R., Jiang, D., An, Z., Vulic, I., Belongie, S., and Korhonen, A. Thinking in frames: How visual context and test-time scaling empower video reasoning, 2026. URL https: //arxiv.org/abs/2601.21037. Li, Z., Liu, Z., Zhang, Q., Lin, B., Wu, F., Yuan, S., Yan, Z., Ye, Y., Yu, W., Niu, Y., et al. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025c. Lin, B., Li, Z., Cheng, X., Niu, Y., Ye, Y., He, X., Yuan, S., Yu, W., Wang, S., Ge, Y., et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Wu, J., Zhang, Z., Xia, Y., Li, X., Xia, Z., Chang, A., Yu, T., Kim, S., Rossi, R. A., Zhang, R., et al. Visual prompting in multimodal large language models: survey. arXiv preprint arXiv:2409.15310, 2024a. Wu, J., Guan, J., Feng, K., Liu, Q., Wu, S., Wang, L., Wu, W., and Tan, T. Reinforcing spatial reasoning in vision-language models with interwoven thinkIn The Thirty-ninth Annual ing and visual drawing. Conference on Neural Information Processing Systems, 2025c. URL https://openreview.net/forum? id=yyWeSAsOhs. Wu, Y., Wang, Y., Tang, S., Wu, W., He, T., Ouyang, W., Torr, P., and Wu, J. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. In European Conference on Computer Vision, pp. 164182. Springer, 2024b. Liu, S., Han, Y., Xing, P., Yin, F., Wang, R., Cheng, W., Liao, J., Wang, Y., Fu, H., Han, C., et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Wu, Y., Li, Z., Hu, X., Ye, X., Zeng, X., Yu, G., Zhu, W., Schiele, B., Yang, M.-H., and Yang, X. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025d. OpenAI. Gpt5 is here. gpt-5/, August 2025. https://openai.com/ Seedream, T., Chen, Y., Gao, Y., Gong, L., Guo, M., Guo, Q., Guo, Z., Hou, X., Huang, W., Huang, Y., et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., and Taigman, Y. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88718879, 2024. Shuai, X., Ding, H., Ma, X., Tu, R., Jiang, Y.-G., and Tao, D. survey of multimodal-guided image editing with text-to-image diffusion models. arXiv preprint arXiv:2406.14555, 2024. Tversky, B. Visualizing thought. In Handbook of human centric visualization, pp. 340. Springer, 2013. Wan. Wan image edit. https://wan.video/, November 2025. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., et al. Omnigen2: Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Li, C., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Ye, Y., He, X., Li, Z., Lin, B., Yuan, S., Yan, Z., Hou, B., and Yuan, L. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Zhang, A., Fei, H., Yao, Y., Ji, W., Li, L., Liu, Z., and Chua, T.-S. Vpgtrans: Transfer visual prompt generator across llms. Advances in Neural Information Processing Systems, 36:2029920319, 2023a. Zhang, H., Li, C., Wu, W., Mao, S., Zhang, Y., Tian, H., Vulic, I., Zhang, Z., Wang, L., Tan, T., et al. Scaling and beyond: Advancing spatial reasoning in mllms requires new recipes. arXiv preprint arXiv:2504.15037, 2025a. Zhang, H., Wu, W., Li, C., Shang, N., Xia, Y., Huang, Y., Zhang, Y., Dong, L., Zhang, Z., Wang, L., et al. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms. arXiv preprint arXiv:2510.24514, 2025b. Zhang, K., Mo, L., Chen, W., Sun, H., and Su, Y. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023b. 10 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Zhang, Y., Zhang, H., Tian, H., Fu, C., Zhang, S., Wu, J., Li, F., Wang, K., Wen, Q., Zhang, Z., et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? In The Thirteenth International Conference on Learning Representations, 2025c. Zhang, Y.-F., Yu, T., Tian, H., Fu, C., Li, P., Zeng, J., Xie, W., Shi, Y., Zhang, H., Wu, J., et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025d. Zhang, Y.-F., Yu, T., Tian, H., Fu, C., Li, P., Zeng, J., Xie, W., Shi, Y., Zhang, H., Wu, J., et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025e. Zhao, X., Zhang, P., Tang, K., Zhu, X., Li, H., Chai, W., Zhang, Z., Xia, R., Zhai, G., Yan, J., et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025a. Zhao, X., Zhang, P., Tang, K., Zhu, X., Li, H., Chai, W., Zhang, Z., Xia, R., Zhai, G., Yan, J., et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025b. Zhou, E., Qin, Y., Yin, Z., Huang, Y., Zhang, R., Sheng, L., Qiao, Y., and Shao, J. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024. 11 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing A. Related Work Image Editing with Generative Models. Building on deep generative models, the image editing literature has explored how to modify existing images in controllable and semantic manner. Early approaches leveraged conditional GANs for localized editing, such as mask-guided portrait manipulation, demonstrating how learned generative priors can support targeted changes (Gu et al., 2019). Recent methods increasingly utilize diffusion-based frameworks for editing tasks, leveraging the iterative denoising process to incorporate multimodal guidance (e.g., text or exemplar images) and achieve high-quality edits while preserving source content (Huang et al., 2025). While traditional generator architectures are optimized for individual tasks (e.g., text-to-image or specific editing objectives), there has been growing interest in unified models that integrate multiple generative and editing capabilities within single framework (Google, 2025b; Wan, 2025; Seedream et al., 2025). These unified approaches represent an emerging direction toward general-purpose visual generative and transformative models that seamlessly integrate content creation and editing. Image Editing Benchmarks. Most existing image editing benchmarks rely exclusively on textual prompts as editing instructions and do not explicitly model visual prompts (Huang et al., 2025; Shuai et al., 2024). Consequently, they are limited in evaluating advanced models ability to understand and follow visual prompts for image editing. Early benchmarks focus on small set of primitive editing operations and limited task diversity. Representative examples include MagicBrush (Zhang et al., 2023b) and EMU-Edit (Sheynin et al., 2024), which primarily assess models performance on basic add, remove, and replace operations, and thus provide only coarse evaluation of editing capabilities. More recent benchmarks expand the scope of editing tasks and improve evaluation protocols. ImgEdit-Bench (Ye et al., 2025) and GEdit-Bench (Liu et al., 2025) extend the range of editing types and adopt VLM-as-a-judge paradigm to better capture semantic correctness. Beyond task diversity, several benchmarks are designed to assess reasoning grounded in world knowledge. RISEBench (Zhao et al., 2025b) evaluates temporal, spatial, and causal editing capabilities, while KRISBench (Wu et al., 2025d) introduces knowledge-based taxonomy that covers conceptual, factual, and procedural editing types. Multimodal Interaction. Multimodal interaction lies at the core of contemporary research on models that jointly reason about language and vision. With the emergence of large multimodal models (MLLMs) (OpenAI, 2025; Google, 2025c; Zhang et al., 2025e), interaction paradigms have expanded toward richer and more integrated reasoning over text and images (Zhang et al., 2025c; Feng et al., 2025; Zhang et al., 2025a;d). In particular, recent frameworks such as Thinking with Images propose that models should not only see images as static inputs, but incorporate visual information as intermediate steps in their reasoning process (Zhang et al., 2025b; Li et al., 2025b; Wu et al., 2025c; Zhou et al., 2024; Li et al., 2026). This perspective has been articulated as new paradigm where models leverage visual representations dynamically within multi-step reasoning processes to better handle complex tasks that cannot be solved through text alone. Complementary to this, recent works in multimodal reasoning and generation explore how multimodal chain-of-thought (CoT) and reasoning planning can be explicitly structured to improve performance. For example, GoT (Generation Chain-of-Thought) (Fang et al., 2025) introduces visual reasoning pipeline where Multimodal LLM generates structured intermediate reasoning steps before synthesis or editing, enabling fine-grained semantic and spatial control. While recent advances in generative and editing models have made significant progress in responding to textual instructions, existing work predominantly focuses on optimizing text-driven generation and image editing (Wu et al., 2025a; Seedream et al., 2025; Chen et al., 2025). This gap is particularly pronounced in spatially grounded editing scenarios, where explicit visual cues such as bounding boxes, arrows, or sketches carry essential information that cannot be fully captured by text alone (Wu et al., 2024a; Jiang et al., 2024; Zhang et al., 2023a; Wu et al., 2024b; Li et al., 2025a). To address this gap, we propose VIBE, the first benchmark specifically designed to systematically evaluate visual instruction-guided image editing. B. Data Collection and Annotation B.1. Deictic Level The four Deictic Level tasks (Addition, Removal, Replacement, and Translation) share the same set of 100 source images. All source images are collected from publicly available online resources and subsequently curated through manual filtering to ensure visual clarity and suitability for localized editing. The final dataset consists of 34 real-world images, 33 animationstyle images, and 33 sketch-style images, providing balanced coverage of diverse visual styles. All visual instructions in the Deictic Level are annotated manually. To minimize potential bias introduced by color variation and to ensure consistency across tasks, all annotations are rendered exclusively in red. Different annotation primitives are 12 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing used depending on the task type. For the Addition task, annotators draw bounding box at the target location where new visual content should be introduced. For the Removal and Replacement tasks, annotators draw bounding box tightly enclosing the object or region to be removed or replaced. For the Translation task, annotators first draw bounding box around the target object, and then add an arrow indicating the desired destination to which the object should be relocated. In all cases, annotations are designed to be spatially explicit and unambiguous, providing clear grounding cues for the intended edit. B.2. Morphological Level Pose Control. For the Pose Control task, we construct source-reference image pairs to explicitly evaluate pose transfer while preserving character identity. We first collect 27 images of human subjects from publicly available online sources, ensuring that each image contains single, clearly visible person with minimal occlusion. In addition, we collect 26 distinct pose reference images in the form of schematic, stick-figurelike pose diagrams, each depicting unique body configuration. Source images and pose references are then manually paired to form 100 sourcereference image pairs. This manual pairing process ensures sufficient pose diversity while avoiding trivial or ambiguous pose correspondences. Reorientation. For the Reorientation task, we collect images of objects with clearly defined facing directions. These images include, but are not limited to, vehicles, chairs, cameras, humans, and shoes, where orientation can be unambiguously inferred from semantic cues. For each image, we manually annotate viewing frustum indicating the target orientation. The annotated frustums are carefully designed to be visually clear and semantically meaningful, specifying yaw, pitch, and roll directions where applicable. This process results in 100 distinct reorientation cases. Draft Instantiation. For the Draft Instantiation task, we collect 100 source images spanning diverse visual styles, including 40 real-world images, 34 animation-style images, and 26 sketch-style images. For each image, annotators manually draw draft-level visual instructions directly on top of the source image to indicate the desired structural modifications. These drafts are intentionally sparse and schematic, serving as abstract blueprints rather than detailed renderings. All draft annotations are created manually to ensure faithful representation of the intended structural guidance and consistency across styles. B.3. Causal Level Light Control. For the Light Control task, we collect images from publicly available online sources that exhibit single, clearly identifiable light source and well-defined subject whose appearance reflects the lighting direction. Selected scenes are required to contain visible shading, shadows, or highlights that can meaningfully convey illumination changes. For each image, annotators manually draw an arrow to indicate the target lighting direction, representing the propagation direction from the light source toward the subject. The annotated lighting directions are deliberately chosen to differ from the original scene lighting while remaining physically plausible and visually interpretable. This process results in 100 annotated samples for the Light Control task. Flow Simulation. For the Flow Simulation task, we collect images containing objects that are sensitive to wind effects, such as human hair, hanging clothes, dandelions, and burning candles. These objects provide clear visual cues for inferring airflow direction through deformation or motion. Similar to Light Control, all visual instructions are manually annotated by drawing arrows that indicate the target wind direction. Annotations are designed to be distinct from the original scene conditions and to induce meaningful, causally consistent visual changes. In total, 100 annotated samples are constructed for the Flow Simulation task. Billiards. For the Billiards task, we generate data through procedural simulation pipeline. Each scene consists of fixed rectangular environment containing one white cue ball and five gray balls with distinct numeric labels. directional force is applied to the white ball, which is visualized by an arrow in the input image. The white ball subsequently undergoes multiple reflections against the scene boundaries before colliding with one of the gray balls. We control the number of boundary reflections to range from two to seven in order to vary the difficulty of the task. The complete trajectory of the white ball is rendered as green dashed line, and the final collision target is indicated by red bounding box around the impacted gray ball, forming the corresponding label image. All generated samples are manually inspected to ensure clarity of the trajectory and collision outcome. Cases with excessive overlap or visually ambiguous trajectories are filtered out. After this quality control process, the final Billiards dataset contains 134 distinct cases. 13 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 3. Experimental results on VIBE. We report task-wise and overall performance as mean standard deviation over 3 independent runs across the Deictic, Morphological, and Causal levels. Model Deictic Level Nano Banana Pro Nano Banana GPT-image-1 Seedream 4.5 Seedream 4.0 Wan2.6 Wan2.5 FLUX2-dev Qwen-Image-Edit-2509 Qwen-Image-Edit Edit-R1-Qwen-Image-Edit-2509 BAGEL-think BAGEL Step1X-Edit-v1p2 OmniGen2 UniWorld-V1 OmniGen Causal Level FS 59.253.62 52.640.87 39.486.07 45.551.07 43.593.18 50.790.84 51.980.37 30.812.54 44.401.83 32.381.41 40.221.59 28.041.57 33.632.40 29.672.07 17.615.07 9.282.68 2.002.00 Table 4. Complete quantitative results of style-wise performance on Draft Instantiation across proprietary models Morphological Level RO 36.042.58 33.451.43 11.092.51 20.115.62 30.374.47 34.894.54 25.771.55 22.771.15 17.674.29 21.332.17 20.273.99 23.63.85 28.055.06 25.170.84 17.441.69 16.576.38 13.882.15 RP 88.261.97 79.051.43 62.632.82 81.823.36 79.351.82 68.150.83 76.991.60 54.41.71 30.130.92 30.482.33 29.472.11 35.382.39 29.261.92 28.171.29 20.840.58 22.031.92 5.260.16 TR 74.802.13 46.531.93 47.004.62 48.931.18 33.294.28 40.952.69 36.801.41 5.581.09 14.480.57 11.112.41 11.333.69 14.462.61 14.051.71 13.522.94 4.510.98 3.590.92 1.230.07 RM 94.070.73 93.500.71 69.001.00 95.820.75 93.040.96 92.900.85 96.901.16 8.001.73 14.383.05 24.885.24 4.861.79 14.590.74 11.331.73 12.592.18 26.202.44 14.522.68 7.481. DI 88.022.08 85.600.75 77.322.36 82.332.17 72.092.11 80.232.63 78.780.48 60.681.89 21.381.77 54.650.96 15.420.79 50.340.50 48.231.67 71.480.07 30.510.36 34.433.00 3.930.41 PC 72.330.32 67.711.03 64.392.74 66.792.38 58.272.27 59.662.46 55.763.44 28.763.20 15.141.21 - 16.230.79 8.502.62 7.611.32 - 11.401.11 11.950.90 5.791.47 LC 60.344.78 34.754.96 25.186.71 50.502.60 47.003.46 44.463.45 33.334.63 33.743.65 28.001.32 22.001.80 25.670.76 21.172.36 21.571.41 25.002.78 17.332.02 15.52.29 2.330.76 AD 82.170.66 81.340.91 55.612.11 81.240.50 74.021.39 66.013.10 73.592.47 64.571.18 55.282.40 44.202.69 56.774.87 40.442.13 33.872.75 33.920.35 26.291.07 15.180.95 2.630.47 BI 15.920.78 1.870.99 4.730.43 2.990.00 4.110.65 9.081.20 7.840.37 2.360.43 2.360.78 3.110.57 2.240.37 5.220.37 5.970.65 0.370.00 2.740.77 0.000.00 0.000.00 Nano Banana Pro Nano Banana GPT-imageAnimation Sketch Real-world 94.61 90.75 80.34 90.06 81.08 84.81 79.89 71.02 79.40 Seedream 4.5 93.11 81.02 73.59 C. Additional Experiments and Analysis C.1. Full Experimental Results Seedream 4.0 Wan2.6 Wan2.5 84.28 74.61 76.55 90.13 65.99 59.81 87.82 75.45 76.89 The quantitative comparison results on the VIBE are summarized in Table 3. To ensure fair and robust evaluation, we report the results as the Mean Standard Deviation (Mean SD) over 3 independent runs. C.2. Style-wise Performance Analysis We report the complete style-wise performance results of all evaluated models on the Draft Instantiation task in Tables 4 and 5. We further provide qualitative analysis of style-dependent behavior on the Draft Instantiation task, with particular focus on Seedream 4.5. As shown in Figure 9, Seedream 4.5 exhibits noticeable difficulty in preserving the original sketch style after applying the draft-based visual instructions. Specifically, the edited regions often deviate from the input sketch domain, producing outputs with inconsistent rendering styles or mixed visual characteristics. C.3. Error Analysis We conduct an error analysis to better understand the failure modes exhibited by current models across different interaction levels and tasks. Representative examples are shown in Figures 10 and 11. Figure 9. Examples of editing results from Seedream 4.5 across different image styles 14 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 10. Qualitative incorrect examples on the Deictic and Morphological Level 15 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 5. Complete quantitative results of style-wise performance on Draft Instantiation across open-source models FLUX2-dev Qwen-Image-Edit-2509 Qwen-Image-Edit Edit-R1-Qwen-Image-Edit-2509 BAGEL-think BAGEL Step1X-Edit-v1p2 OmniGen2 UniWorld-V1 OmniGen Animation Sketch Real-world 73.90 57.93 51.24 18. 10.46 30.52 65.52 45.38 51.43 8. 9.39 25.38 52.92 55.32 44.91 54. 48.65 42.82 78.44 66.70 68.67 42. 5.85 36.36 30.54 25.39 44.93 0. 2.67 8.09 Deictic Level. For the Addition task, common failure mode is inaccurate spatial localization. As illustrated in Figure 10, models sometimes place the added object partially or entirely outside the annotated bounding box, resulting in incorrect In the Removal task, models may remove unintended content beyond the specified region, occasionally placement. deleting objects that are not marked for editing. Another frequent issue in both Removal and Replacement tasks is stylistic inconsistency. In particular, for Replacement, the newly generated content may not match the visual style of the original image, leading to perceptually incoherent results, as shown in Figure 10. Morphological Level. For Pose Control, models may fail to preserve character identity, generating outputs in which the edited subject is no longer consistent with the original character. In the Reorientation task, models sometimes struggle to align the object orientation with the annotated viewing frustum, producing results that only partially reflect the intended yaw, pitch, or roll. For Draft Instantiation, failures often arise when the generated output does not faithfully realize the entity or structure specified by the draft-based visual instruction, resulting in incomplete or incorrect instantiations. Causal Level. In the Light Control task, models may modify the lighting conditions of the scene but fail to align the illumination direction with the annotated arrow, leading to directionally inconsistent shading or shadows. Similar issues are observed in Flow Simulation, where wind effects are present but do not follow the specified direction, as illustrated in Figure 11. For the Billiards task, models may predict incorrect motion trajectories, including wrong reflection sequences or target collisions. In some cases, additional errors occur when the background or static elements of the scene are unintentionally altered, violating contextual preservation. Overall, these error cases highlight persistent challenges in precise spatial grounding, stylistic consistency, and causal reasoning under visual instruction guidance. C.4. Qualitative Case Study with Visually Embedded Instructions We further present qualitative case studies to examine whether models can follow instructions that are visually embedded within the input image. In these experiments, the textual input is fixed to minimal prompt, Edit this image following the instructions annotated on this picture. All task specifications, including both textual descriptions and symbolic cues, are embedded directly into the image as visual annotations. As shown in Figure 12, Nano Banana Pro demonstrates strong capability in interpreting and executing visually embedded instructions. The model successfully handles both single-task and multi-task scenarios, including addition, removal, and replacement operations, as well as tasks involving causal reasoning such as light direction control. These results indicate that the model is able to parse instruction content directly from the image, associate it with the corresponding operations, and apply the intended edits without relying on detailed textual prompts. Notably, even in cases that require causal reasoning, such as modifying illumination according to an annotated light direction, the model produces results consistent with the embedded instructions. This qualitative evidence suggests that frontier models can, to some extent, treat visually embedded instructions as primary guidance signals rather than auxiliary references. Such behavior further motivates the need for systematic benchmarks like VIBE to characterize visual instruction-following capabilities beyond conventional text-centric prompting paradigms. D. Evaluation D.1. Evaluation Metrics As described in Section 2.3, we design task-specific evaluation metrics tailored to the characteristics of each task. Metrics for Addition, Removal, Replacement, and Translation are detailed in the main paper. In this appendix, we provide the definitions of evaluation metrics for the remaining tasks. 16 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Pose Control: Evaluation for the Pose Control task focuses on whether the target pose is correctly realized while preserving character integrity and contextual consistency. We define four complementary criteria. Pose Consistency (PC) evaluates whether the target pose specified by the reference image is present in the generated output. This metric assesses pose correspondence regardless of whether the pose is realized through physically valid or anatomically correct human body. The human body is decomposed into four coarse parts: left arm, right arm, left leg, and right leg. Each part is evaluated independently as binary match against the reference pose. The final Pose Consistency score is computed as the average over the four parts, resulting in discrete values in {0, 0.25, 0.5, 0.75, 1}. Body Instance Integrity (BII) explicitly evaluates whether the target pose is realized by single coherent human body. It penalizes degenerate cases such as fragmented limbs, duplicated body parts, or pose realization through multiple inconsistent instances. BII is assigned binary score. Character Identity Consistency (CIC) measures whether the generated character remains identifiable as the same character as in the input image. This criterion evaluates the preservation of identity-related visual attributes and is scored binarily. Contextual Preservation (CP) evaluates whether visual content outside the characters pose remains unchanged. It penalizes unintended modifications to background elements or surrounding objects and is assigned binary score. The final score for the Pose Control task is computed as: Score = (PC BII + CIC + CP 3 1 2 . ) (3) Reorientation: The evaluation of the Reorientation task focuses on whether the target object is correctly aligned to the specified orientation while preserving object identity and visual integrity. We define three complementary metrics. Orientation Alignment (OA) evaluates whether the final orientation of the target object matches the target orientation specified by the reference indicator. The target orientation is defined along three independent axes: yaw, pitch, and roll. For each axis, binary score is assigned based on whether the final object orientation is aligned with the target orientation along that axis. Axis-wise alignment is judged solely based on the final result, regardless of whether modification was required. The Orientation Alignment score is computed as the average of the three axis-wise scores, yielding discrete values in {0, 1 3 , 2 3 , 1}. Identity Consistency (IC) evaluates whether the edited object in the generated image remains the same semantic entity as in the input image. This metric ignores changes directly induced by reorientation, such as pose, facing direction, or perspective. It penalizes object replacement, removal, duplication, or severe structural corruption. IC is assigned binary score. Visual Integrity (VI) evaluates whether the reorientation process introduces severe visual artifacts or layout corruption. This includes prominent visual pollution, large rendering artifacts, or structural image breakdown that significantly degrades readability. VI is assigned binary score. The final score for the Reorientation task is computed as the geometric mean of the three criteria: Score = (OA IC + VI 2 1 2 . ) (4) Draft Instantiation: The evaluation protocol for the Draft Instantiation task follows exactly the same metric design as that used for tasks in the Deictic Level. Specifically, performance is assessed using Instruction Adherence (IA), Contextual Preservation (CP), and Visual Coherence (VC), which is detailed in Section 2.3. Light Control: The Light Control task evaluates whether the generated image reflects the target lighting direction specified by an arrow while preserving non-lighting scene content. We define two metrics: Lighting Direction Consistency (LDC) and Contextual Preservation (CP). 17 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Lighting Direction Consistency (LDC) measures whether the dominant illumination direction on the subject in the generated image matches the target direction indicated by the arrow in the input image. LDC consists of two sub-metrics. Direction Matching Consistency (DMC) compares the arrow direction against the dominant lighting direction inferred from highlights and shadows on the subject (rather than from visible light sources). DMC is scored on three-level scale: 1.0 if the directions are nearly identical, 0.5 if the lighting is modified toward the target direction but exhibits noticeable angular deviation (within 90), and 0.0 if the lighting direction is largely different, unchanged, or ambiguous. Physical Lighting Consistency (PLC) evaluates whether the observed shading, shadowing, and highlight distribution are physically consistent with the target direction. PLC is only evaluated when DMC = 1.0; otherwise, PLC is set to 0 by default. PLC is scored binarily: 1 if the illumination pattern is physically consistent with the target direction, and 0 otherwise. We compute LDC as the average of its two sub-metrics. Contextual Preservation (CP) evaluates whether the generated image preserves all non-target content while applying the intended lighting-direction edit. CP is scored binarily and penalizes any unrelated content modifications, including object addition/removal, geometry or layout changes, and semantic alterations unrelated to lighting. CP is set to 1 only if all observable differences between input and output are attributable to lighting-related effects; otherwise, CP is set to 0. The final score for the Light Control task is computed as the geometric mean of the two metrics: Score = (LDC CP) 1 2 . (5) Flow Simulation: The Flow Simulation task evaluates whether the generated image correctly reflects the target wind direction while preserving the identity and placement of wind-affected subjects. We define two complementary metrics: Wind Direction Consistency (WDC) measures whether the dominant wind flow in the generated image aligns with the target direction specified by the arrow in the input image. Wind direction is inferred exclusively from visible, directionally consistent responses of wind-sensitive elements, such as hair, clothing, vegetation, smoke, particles, or flame shape. Abstract airflow cues without corresponding effects on scene elements are not considered valid evidence of wind. WDC is scored on three-level scale: 1.0 if at least one wind-sensitive element responds clearly and its motion closely matches the target direction; 0.5 if wind effects are present and generally follow the target direction with noticeable angular deviation (within 30); and 0.0 otherwise, including cases with no wind response, ambiguous motion, or inconsistent direction. Contextual Preservation (CP) evaluates whether wind-affected subjects preserve their semantic identity and overall placement after editing, while allowing changes directly attributable to wind. CP consists of two sub-metrics. WindIdentity Preservation (WIP) checks whether all wind-affected subjects remain the same semantic entities as in the input image, ignoring deformation or scattering caused by wind. Wind-Pose/Placement Preservation (WPP) evaluates whether the subjects global position and pose remain consistent after excluding wind-induced effects such as hair fluttering, cloth bending, or particle dispersal. WPP is evaluated only if WIP = 1; otherwise, it is set to 0 by default. Both sub-metrics are scored binarily, and CP is computed as their average. The final score for the Flow Simulation task is computed as the geometric mean of WDC and CP: Score = (WDC CP) 1 2 . (6) Billiards: The Billiards task evaluates models ability to reason about multi-step physical interactions, including ball motion, wall collisions, and target prediction. Given golden-label image depicting the correct initial state, future trajectory, and final target, the generated output is evaluated using three independent metrics. Path Correctness (PC) evaluates whether the predicted trajectory follows the same causal structure as the golden label. Rather than enforcing exact geometric overlap, this metric focuses on the topology and direction of motion. Specifically, it checks whether the trajectory proceeds in the same initial direction and whether it interacts with the same table cushions in the same order. Trajectories exhibiting incorrect wall collisions, reversed directions, or hallucinated loops are penalized. PC is assigned binary score. 18 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Collision Correctness (CC) evaluates whether the final target ball is correctly identified. This metric checks whether the predicted target region corresponds to the same ball number as specified in the golden label, regardless of minor deviations in the predicted path. CC is assigned binary score and focuses exclusively on target identity rather than trajectory quality. Contextual Preservation (CP) verifies whether the static environment remains unchanged. This metric checks whether all billiard balls are present, whether ball numbers are preserved, whether the spatial layout of non-moving balls is consistent with the golden label, and whether the directional arrow on the cue ball is correctly retained. Minor differences in lighting or rendering are ignored. CP is assigned binary score and is set to 0 if any ball is missing, added, renumbered, or significantly displaced, or if the directional arrow is incorrect. The final score for the Billiards task is computed as follows: Score = ( PC + CC 2 CP) 1 (7) D.2. Evaluation Prompt We provide the evaluation prompts of each metric in Table 6 29. 19 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 6. Evaluation Prompt for Instruction Adherence Instruction Adherence You are given THREE images and ONE text prompt. The first image: - This is the original image. The second image: - Visual instructions are drawn on this image based on the original image by the user. The third image: - This is the image generated by model after editing the second image. TEXT PROMPT: - {prompt} Your task is to evaluate whether the Output Image (The third image) correctly follows the instruction. The instruction consists of BOTH: (1) the visual instruction drawn on the second image, and (2) the textual description in the Text Prompt. You must independently evaluate the following THREE metrics and assign binary score (1 or 0) to each: 1. Visual Instruction Localization Correctness Did the main edit occur on the object or region explicitly indicated by the visual instruction on the Input Image (The second image)? 2. Visual Operator Type Compliance Was the type of edit consistent with the operation implied by the visual instruction? 3. Textual Action Semantic Compliance Did the model execute the core action specified in the Text Prompt? Scoring rules: - Score 1 if the requirement is clearly satisfied. - Score 0 if the requirement is not satisfied or is ambiguous. - If unsure, assign 0. - Partial compliance must be scored as 0. You may reason freely to reach your decision. Then, for EACH metric, provide: - score: an integer value of 0 or 1. - reason: ONE short factual sentence describing an observable outcome. This summary must strictly follow the output format specified below: { Visual Instruction Localization Correctness: { reason: brief factual summary, score: 1/0 }, Visual Operator Type Compliance: { reason: brief factual summary, score: 1/0 }, Textual Action Semantic Compliance: { reason: brief factual summary, score: 1/0 } } 20 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 7. Evaluation Prompt for Contextual Preservation 1/2 Contextual Preservation 1/ You are given TWO images and ONE text prompt. IMAGE 1 (Input Image): - Original image with user-drawn visual instructions. IMAGE 2 (Output Image): - Image generated by the model after editing IMAGE 1. - The output may be cropped or reframed. TEXT PROMPT: - {prompt} Your task is to evaluate Contextual Preservation. Definition: Contextual Preservation checks whether the model changed anything it was NOT supposed to change. It does NOT judge whether the edit was correct, precise, or well aligned. Errors in edit location, extent, or alignment belong to Instruction Adherence, not Contextual Preservation. Evaluation rules (follow strictly): 1) Cropping rule - If the output is cropped, only compare the overlapping visible region. - Ignore content missing only due to cropping. 2) Difference listing (what counts as difference) - List ONLY meaningful differences at the level of objects or semantic entities. - Do NOT list differences caused by: minor blur or softness, small texture or color shifts, pixel-level noise, slight position or alignment offsets. - difference should be listed ONLY if it: adds or removes complete object, changes the identity of an object, or damages the structural integrity of non-target object. 3) Target rule - Identify the intended edit target based ONLY on: (a) the visual instruction marks, and (b) the text prompt. 4) Classification rule - IN TARGET: - IN TARGET: any change within the intended target, OR any imperfect attempt to edit the target (including misplacement, offset, scale error, or incomplete coverage). VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 8. Evaluation Prompt for Contextual Preservation 2/2 Contextual Preservation 2/2 - OUT OF TARGET: any change to unrelated objects or regions, any addition or removal of unrelated semantic entities, any structural damage to non-target objects. 5) Scoring - Score = 1 if NO OUT OF TARGET differences exist. - Score = 0 if ANY OUT OF TARG Output format: First provide brief analysis with these sections: - ## Differences - ## Target - ## Classification - ## Decision Then output the final JSON as the last part of your response: { Contextual Preservation: { reason: string, score: 0 } } 5) Scoring - Score = 1 if NO OUT OF TARGET differences exist. - Score = 0 if ANY OUT OF TARG 22 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 9. Evaluation Prompt for Visual Coherence 1/2 Visual Coherence 1/ You are given THREE images and ONE text prompt. The first image: - This is the original image. The second image: - Visual instructions are drawn on this image based on the original image by the user. The third image: - This is the image generated by model after editing the second image. TEXT PROMPT: - {prompt} Your task is to evaluate the Visual Coherence of the Output Image. Visual Coherence evaluates whether the edited result is visually unified and generatively sound. This metric is STYLE-AGNOSTIC: you must NOT judge realism, beauty, or artistic preference. Instead, you must assess whether the edited image remains consistent with the source image and whether the output avoids clear generative artifacts. You must independently evaluate the following THREE metrics and assign binary score (1 or 0) to each: 1. Style Consistency Did the edited region adopt the same artistic or rendering domain as the Input Image (e.g., line-art, watercolor, oil painting, 3D render, photographic style, pixel art, animation)? - Score 1 if the edited region clearly belongs to the same visual domain as the source image. - Score 0 if the edited region introduces different artistic or rendering domain. - Do NOT judge whether the style looks good or realisticonly whether it matches the source domain. 2. Visual Seamlessness Is the edited region visually continuous with its surrounding area, without obvious signs of compositing? Focus on whether there are clear visual discontinuities such as: - unnatural seams or hard boundaries, - abrupt changes in texture, color, or resolution, - visible cut-and-paste artifacts. - Score 1 if the edited region integrates seamlessly with its surroundings. - Score 0 if there are clear and noticeable discontinuities. - Do NOT penalize stylistic roughness if it is consistent with the source image. 3. Artifact-Free Generation Does the Output Image avoid obvious, domain-independent generative artifacts? 23 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 10. Evaluation Prompt for Visual Coherence 2/2 Visual Coherence 2/ Consider artifacts such as: - unintended blurring or pixelation, - geometric distortion or deformation, - broken structures, duplicated patterns, or rendering collapse, - residual visual instruction marks such as arrows, boxes, strokes, or masks that should not appear in the final image. - Score 1 if no clear generative artifacts are present. - Score 0 if obvious, non-stylistic artifacts are visible. - Style-specific noise or abstraction is allowed if consistent with the source domain. General rules: - Evaluate ONLY visual coherence, not instruction correctness or contextual preservation. - Minor pixel-level differences are acceptable. - If you are unsure about case, assign score 0. - Partial compliance must be scored as 0. You may reason freely to reach your decision. Then, for EACH metric, you provide summary: - score: an integer value of 0 or 1. - reason: ONE short factual sentence describing an observable outcome. This summary must strictly follow the output format specified below: { Style Consistency: { reason: brief factual summary, score: 1/0 }, Visual Seamlessness: { reason: brief factual summary, score: 1/0 }, Artifact-Free Generation: { reason: brief factual summary, score: 1/0 } } 24 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 11. Evaluation Prompt for Billiards 1/2 Billiards 1/2 You are an expert computer vision evaluator for physics simulation models. You are provided with two images: 1. IMAGE 1 (Golden Label / Ground Truth): Depicts the correct initial state, the correct future trajectory (green dashed line), and the correct final target (red bounding box). 2. IMAGE 2 (Model Generation): Depicts predicted scene, trajectory, and target. YOUR TASK: Evaluate the Generated Image against the Golden Label based on three independent metrics. You must analyze the images step-by-step and output final JSON score. 1. Context Preservation (CP) Goal: Verify that the static environment remains unchanged. Check: - Are all billiard balls (balls with numbers and the white ball) present? - Are the ball numbers consistent? - Is the spatial layout (positions of non-moving balls) consistent with the ground truth? - Is the black arrow on the white ball preserved? Ignore: Slight changes in lighting, shading, or minor pixel-level rendering differences. Scoring: - 0: If any ball is missing, added, re-numbered, or significantly displaced; or if the arrow is missing or incorrect. - 1: If the static scene identity and layout are preserved. 2. Path Correctness (PC) Goal: Verify the topology and direction of the green dashed trajectory. Check: - Does the predicted path move in the same cardinal direction? - Does the trajectory bounce off the same specific walls or cushions in the same order? (e.g., if Truth hits Top-Wall then Left-Wall, Prediction must do the same). - Is the path free of hallucinations (e.g., random loops or squiggly lines)? Scoring: - 0: If the trajectory hits different walls, moves in different initial direction, or has completely different shape. - 1: If the trajectory follows the same sequence of wall impacts and general geometry. 3. Collision Correctness (CC) Goal: Verify the final target identity. Check: - Does the red bounding box surround the same specific ball number as in the Golden Label? Note: This metric is strictly about the identity of the target, regardless of whether the path (PC) looks perfect. Scoring: - 0: If the red box highlights different ball or an empty space. - 1: If the red box highlights the correct ball number. Output Instructions Step 1: Analysis & Decisions Provide text analysis under the headers ## Analysis and ## Decisions. For each metric, explicitly state the observable differences or similarities. Example for Path: The Golden path bounces off the top cushion. The Generated path bounces off the bottom cushion. These are different. Step 2: Final JSON Output the final score in the following JSON format. This must be the last part of your response. 25 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 12. Evaluation Prompt for Billiards 2/2 Billiards 2/2 Context Preservation: { { reason: One sentence summary focusing on ball count, numbers, and layout., score: 0 or 1 }, Path Correctness: { reason: One sentence summary focusing on trajectory direction and wall bounce sequence., score: 0 or 1 }, Collision Correctness: { reason: One sentence summary confirming if the correct ball number was targeted., score: 0 or 1 } } 26 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 11. Qualitative incorrect examples on the Causal Level 27 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 12. Qualitative examples with visually embedded instructions. All examples use the same minimal textual prompt, Edit this image following the instructions annotated on this picture. Task specifications are conveyed through text and symbols embedded directly in the input image. Nano Banana Pro correctly executes single-task, multi-task, and causal editing operations based on these visually embedded instructions. 28 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Figure 13. Screenshot of the developed data annotation system used in section 4.3. 29 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 13. Evaluation Prompt for Wind Contextual Preservation 1/ Wind Contextual Preservation 1/2 You are given TWO images. IMAGE 1 (Input Image): - The original image before wind editing. - This image may contain visual instruction markings (e.g., arrows) indicating wind direction. IMAGE 2 (Generated Image): - The model-generated image after wind editing. Your task is to evaluate Wind Contextual Preservation (W-CP), which consists of TWO sub-metrics: 1) Wind-Identity Preservation (W-IP) 2) Wind-Pose/Placement Preservation (W-PP) IMPORTANT GLOBAL RULES: - Visual instruction markings (e.g., arrows) are NOT scene content. Differences related to arrows MUST be ignored, unless they severely obstruct the subject or degrade visual quality. - Wind effects are assumed to be intentional and allowed. - Do NOT penalize changes directly caused by wind. SUB-METRIC 1: Wind-Identity Preservation (W-IP) W-IP checks whether the wind-affected subject(s) remain the SAME semantic entity after editing. Focus on identity ONLY: - Person identity (same person, same character) - Object identity (same candle, same dandelion, same clothing item) Explicitly IGNORE: - Deformation, bending, or scattering caused by wind (e.g., flying hair, dispersed seeds, flickering flame). Scoring: - Score = 1 If the identity of all wind-affected subjects is preserved. - Score = 0 If any subject is removed, replaced, duplicated, or transformed into different semantic entity. SUB-METRIC 2: Wind-Pose / Placement Preservation (W-PP) IMPORTANT: - Evaluate W-PP ONLY IF W-IP = 1. - If W-IP = 0, set W-PP = 0 by default. W-PP checks whether the overall position and pose of the subject(s) remain consistent with IMAGE 1, AFTER ignoring wind-induced effects. 30 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 14. Evaluation Prompt for Wind Contextual Preservation 2/ Wind Contextual Preservation 2/2 Ignore the following wind-induced changes: - Hair blowing direction or shape. - Cloth fluttering or bending. - Dispersal or motion of lightweight elements (e.g., dandelion seeds, smoke, flame shape). Check ONLY: - Whether the subjects main body has shifted position. - Whether the subjects global orientation or stance has changed. Scoring: - Score = 1 If the subjects overall position and pose are preserved. - Score = 0 If the subject has been significantly moved, rotated, or reposed beyond what wind could plausibly cause. Output Requirements: 1) First provide your reasoning under: - ## Identity Preservation Analysis - ## Pose / Placement Analysis 2) Then output the final JSON under: - ## JSON 3) The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): Wind-Identity Preservation: { { reason: string, score: 0 }, Wind-Other Preservation: { reason: string, score: 0 } } 31 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 15. Evaluation Prompt for Wind Direction Consistency 1/2 Wind Direction Consistency 1/2 You are given TWO images. IMAGE 1 (Input Image): - The original image. - visual arrow is drawn on this image indicating the desired wind direction. - The arrow defines the TARGET wind flow direction. IMAGE 2 (Generated Image): - The model-generated image after wind editing. Your task is to evaluate Wind Direction Consistency (WDC). IMPORTANT SCOPE: - Evaluate ONLY the wind direction. - Do NOT evaluate physical realism, wind strength, or scene preservation. - Ignore all non-wind-related changes. CRITICAL CAUSAL RULE (MUST FOLLOW): Wind direction is considered valid ONLY IF it produces visible, directionally consistent effects on at least one wind-sensitive element in the scene. Wind-sensitive elements include (but are not limited to): - Hair - Clothing or fabric - Vegetation (e.g., trees, grass, dandelions) - Smoke, mist, particles - Flame shape (e.g., candles) If wind is shown only as abstract airflow (e.g., lines or particles) WITHOUT affecting any wind-sensitive element, the wind direction MUST be scored as 0. PART 1: Direction description (no comparison) (1) Arrow direction in IMAGE 1: Describe the arrow direction as continuous spatial direction. Do NOT reduce it to simple categories (left/right/up/down). Use precise descriptions (e.g., from upper-left toward lower-right). (2) Wind direction in IMAGE 2: Infer the dominant wind direction based ONLY on visible wind effects, such as hair, clothing, vegetation, smoke, or particles. Describe the wind direction using the same level of precision as above. 32 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 16. Evaluation Prompt for Wind Direction Consistency 2/2 Wind Direction Consistency 2/2 PART 2: Direction comparison and scoring Compare the arrow direction and the inferred wind direction. Scoring: - Score = 1.0 ONLY if: (a) At least one wind-sensitive element clearly responds to wind, AND (b) The response direction closely matches the arrow direction. - Score = 0.5 ONLY if: (a) Wind-sensitive elements respond to wind, AND (b) The response direction generally follows the arrow, but with noticeable angular deviation (within 30 degrees). - Score = 0.0 In ALL other cases, including: - No wind-sensitive elements respond to wind. - Wind is shown only via abstract airflow cues. - Wind direction is unchanged, opposite, or ambiguous. - Wind-sensitive elements exist but do not move consistently with the arrow direction. Output Requirements: 1) First provide your reasoning under: - ## Causal Wind Evidence - ## Direction Description - ## Direction Comparison 2) Then output the final JSON under: - ## JSON 3) The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): { Wind Direction Consistency: { reason: string (one-sentence summary), score: 0.0 } } 33 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 17. Evaluation Prompt for Contextual Preservation in Light Control 1/2 Contextual Preservation in Light Control 1/2 You are given TWO images. IMAGE 1 (Input Image): - The original image before editing. IMAGE 2 (Generated Image): - The model-generated image after lighting editing. Your task is to evaluate Contextual Preservation (CP). CP checks whether IMAGE 2 preserves all non-target content from IMAGE 1 while applying the intended lighting-direction edit. IMPORTANT SCOPE: - This evaluation is NOT about lighting direction correctness. - Assume lighting direction editing is the intended and allowed operation. - CP ONLY checks whether unrelated scene content has been modified. Visual Instruction Rule: - Visual instruction markings in IMAGE 1 (e.g., arrows or guides) are NOT part of the scene content. - Differences related to the presence, absence, or appearance of these instruction markings in IMAGE 2 MUST be IGNORED, UNLESS they severely obstruct the subject or significantly degrade the visual quality of the image. PART 1: Allowed vs non-allowed changes Allowed changes (do NOT penalize): - Changes in illumination intensity, shading, or highlights. - Changes in shadow shape or placement caused by lighting. - Minor color shifts attributable to lighting. - Appearance of visible light sources or light-related effects introduced solely to express lighting, such as: bright light spots, visible lamps, sun glare, lens flare, light beams or glow effects, as long as they do not alter scene geometry or object structure. Non-allowed changes (must be penalized): - Addition or removal of non-lighting objects. - Changes in object geometry or structure. - Changes in scene layout or spatial arrangement. - Replacement of materials, textures, or clothing. - Introduction of new semantic scene elements unrelated to lighting (e.g., new furniture, new buildings). - Differences involving visual instruction markings (e.g., arrows) that do not affect scene content. PART 2: Difference-based comparison Compare IMAGE 1 and IMAGE 2 carefully. 34 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 18. Evaluation Prompt for Contextual Preservation in Light Control 2/2 Contextual Preservation in Light Control 2/2 Procedure: 1) List all observable differences between IMAGE 1 and IMAGE 2. 2) For each difference, determine whether it is: - lighting-related manifestation (allowed), OR - an unrelated content modification (not allowed). PART 3: Contextual Preservation decision Scoring (binary): - Score = 1 ONLY if ALL observed differences fall under allowed lighting-related changes. - Score = 0 If ANY difference corresponds to an unrelated content modification. If unsure whether difference is lighting-related or not, treat it as NOT preserved (score 0). Output Requirements: 1) First provide your reasoning under: - ## Difference Analysis - ## CP Decision 2) Then output the final JSON under: - ## JSON 3) The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): Contextual Preservation: { { reason: string (one-sentence summary), score: 0 } } 35 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 19. Evaluation Prompt for Lighting Direction Consistency 1/2 Lighting Direction Consistency 1/ You are given TWO images. IMAGE 1 (Input Image): - The original image. - visual arrow indicates the desired lighting direction. - The arrow defines the TARGET light propagation direction (from the light source toward the subject). IMAGE 2 (Generated Image): - The model-generated image after lighting editing. Your task is to evaluate Lighting Direction Consistency (LDC), which consists of TWO sub-metrics: 1) Direction Matching Consistency (DMC) 2) Physical Lighting Consistency (PLC) PART 1: Direction description (no comparison) (1) Arrow direction in IMAGE 1: Describe the arrow direction as continuous spatial direction. Use precise language (e.g., from upper-right toward lower-left, slightly downward from right to left). Do NOT reduce the direction to simple categories like left/right/up/down. (2) Lighting direction on the subject in IMAGE 2: Describe the dominant lighting direction ON THE SUBJECT, inferred ONLY from surface illumination (highlights and shadows), not from light source position or visible beams. Use the same level of directional precision as above. PART 2: Direction Matching Consistency (DMC) Compare the two described directions. Scoring: - Score = 1.0 If the directions are nearly identical with no clear angular deviation. - Score = 0.5 If the lighting direction is clearly modified to follow the arrow, but exhibits noticeable angular deviation (within 90 degrees). - Score = 0.0 If the lighting direction is largely different, unchanged, or ambiguous. PART 3: Physical Lighting Consistency (PLC) IMPORTANT: - Only evaluate PLC IF DMC = 1.0. - If DMC 1.0, set PLC = 0 by default. 36 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 20. Evaluation Prompt for Lighting Direction Consistency 2/2 Lighting Direction Consistency 2/ Assume the arrow direction is the TRUE light source direction. Check whether the lighting effects on the subject in IMAGE 2 are physically consistent with that direction. Consider: - Which side of the subject should be illuminated. - Which side should be in shadow. - Whether highlights and shading agree with the arrow direction. Scoring: - Score = 1 If the observed illumination pattern on the subject is physically consistent with the arrow direction. - Score = 0 If the illumination pattern contradicts the arrow direction. Output Requirements: 1) First provide your reasoning under: - ## Direction Description - ## Direction Comparison - ## Physical Consistency 2) Then output the final JSON under: - ## JSON 3) The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): Direction Matching Consistency: { { reason: string, score: 0.0 }, Physical Lighting Consistency: { reason: string, score: 0 } } VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 21. Evaluation Prompt for BII, CIC, and Context Preservation 1/2 BII, CIC, and Context Preservation 1/2 You are given TWO images. IMAGE 1 (Original Image): - The original character image. IMAGE 2 (Generated Image): - The model-generated image. Your task is to evaluate THREE independent metrics: 1) Body Instance Integrity (BII) 2) Character Identity Consistency (CIC) 3) Context Preservation (CP) IMPORTANT: - Do NOT evaluate pose correctness. - Do NOT compare limb positions or body posture. - Each metric must be judged independently. Metric definitions: (1) Body Instance Integrity (BII) Checks whether IMAGE 2 depicts exactly ONE coherent human body instance. Score 0 if ANY of the following occur: - Extra or duplicated limbs (e.g., more than two arms or legs). - Multiple bodies, torsos, or heads. - Visible skeletons, stick figures, or pose-reference structures overlaid on the body. - Limbs are completely missing or not visible at all (unless they are plausibly occluded by clothing such as long skirts). Otherwise, score 1. (2) Character Identity Consistency (CIC) Checks whether the character in IMAGE 2 is the SAME character as in IMAGE 1. Focus on: - Face and facial features (if visible). - Hair style and hair color. - Distinctive identity cues (e.g., glasses, accessories). Ignore: - Pose changes. - Background changes. - Minor rendering differences. Score 1 only if identity clearly matches. If identity is different or cannot be confidently verified, score 0. 38 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 22. Evaluation Prompt for BII, CIC, and Context Preservation 2/2 BII, CIC, and Context Preservation 2/2 (3) Context Preservation (CP) Checks whether elements OTHER THAN the characters pose are preserved. Focus on: - Background scene. - Clothing (style and type). - Overall visual domain (e.g., realistic, anime). Ignore: - Natural cloth deformation caused by pose change. - Minor occlusions or shading changes. Score 0 if background, clothing, or scene elements are added, removed, or replaced. Otherwise, score 1. Output requirements: 1) First provide your reasoning under these headers: - ## Analysis - ## Decisions 2) Then output the final JSON under: - ## JSON 3) Each metric must include: - reason: one short factual sentence - score: 0 or 1 4) The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): { Body Instance Integrity: { reason: string (one-sentence summary), score: 0/1 }, Character Identity Consistency: { reason: string (one-sentence summary), score: 0/1 }, Context Preservation: { reason: string (one-sentence summary), score: 0/1 } } 39 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 23. Evaluation Prompt for Pose Consistency 1/2 Pose Consistency 1/2 You are given TWO images. IMAGE 1 (Pose Reference): - pose schematic image (e.g., stick figure, skeleton, or line drawing). - This image defines the TARGET POSE only. IMAGE 2 (Generated Image): - model-generated image containing character. Your task is to evaluate POSE CONSISTENCY by comparing ONLY the limb poses. IMPORTANT SCOPE: - Ignore character identity, appearance, clothing, realism, or visual style. - The character in IMAGE 2 may be full human, skeleton, or line drawing. - ONLY compare limb pose. - Do NOT evaluate head or torso pose. OUTPUT STRUCTURE (must follow exactly): PART 1: Limb pose description (no comparison) Describe the limb poses in IMAGE 1 (pose reference): - Left Arm: - Right Arm: - Left Leg: - Right Leg: Describe the limb poses in IMAGE 2 (generated image): - Left Arm: - Right Arm: - Left Leg: - Right Leg: PART 2: Limb-by-limb comparison For each limb, compare IMAGE 2 against IMAGE 1 and assign ONE label: - MATCH: Limb pose matches the reference in orientation, bending, and functional role. - MISMATCH: Limb is visible but clearly differs from the reference (wrong bending, wrong pose, or incorrect side). - N/A: Limb cannot be reliably evaluated because it is fully occluded, cropped out, or visually indistinguishable. 40 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 24. Evaluation Prompt for Pose Consistency 2/2 Pose Consistency 2/2 Rules: - Minor spatial offsets and scale differences are allowed. - If unsure, choose MISMATCH. - Do NOT infer pose for limbs marked as N/A. PART 3: Final JSON output Output the comparison results as JSON. The JSON must be the LAST part of your response. FINAL JSON FORMAT (EXACT): { Pose Consistency: { Left Arm: MATCH MISMATCH N/A, Right Arm: MATCH MISMATCH N/A, Left Leg: MATCH MISMATCH N/A, Right Leg: MATCH MISMATCH N/A } } 41 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 25. Evaluation Prompt for Orientation Alignment 1/3 Orientation Alignment 1/3 You are given TWO images. ## IMAGE 1 (Input Image) - The original image. - FOUR-SIDED PYRAMID orientation indicator is shown at the BOTTOM-LEFT corner. - The pyramid defines the TARGET orientation: - The pyramids tip indicates the FORWARD direction. - The pyramid implicitly defines Yaw, Pitch, and Roll. ## IMAGE 2 (Generated Image) - The model-generated image after orientation editing. Your task is to evaluate whether the FINAL orientation in IMAGE 2 matches the TARGET orientation defined by the pyramid in IMAGE 1. ## STEP 1: Identify target object and orientation anchor Identify the main target object whose orientation is edited. Then determine its Orientation Anchor the semantic part that defines the objects forward-facing direction. Use these rules: - Shoes toe - Chairs backrest - Cars front / headlights - Cameras lens - Humans face - Animals head - Tools / devices functional front end State explicitly: - Target object - Orientation anchor ## STEP 2: Describe TARGET orientation (IMAGE 1 ONLY) Look ONLY at IMAGE 1. Describe the pyramids orientation using three independent axes: - Yaw: horizontal facing direction of the pyramids tip - Pitch: upward or downward tilt of the pyramids tip - Roll: leftward or rightward tilt of the pyramid body Rules: - Do NOT reference the object. - Do NOT compare with IMAGE 2. 42 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 26. Evaluation Prompt for Orientation Alignment 2/3 Orientation Alignment 2/3 - Do NOT use vague terms. - Use explicit spatial descriptions. ## STEP 3: Describe FINAL object orientation (IMAGE 2 ONLY) Look ONLY at IMAGE 2. Using the orientation anchor, describe the FINAL orientation of the object in IMAGE 2: - Yaw - Pitch - Roll Rules: - Describe the actual observed orientation, not intent. - Do NOT reference IMAGE 1 or the pyramid. - Do NOT infer past states. - Do NOT merge axes. - If an axis cannot be determined, mark it as N/A. ## STEP 4: Axis-wise alignment (comparison) Now compare the descriptions from STEP 2 and STEP 3. For EACH axis: - Score = 1 if the FINAL object orientation matches the TARGET pyramid orientation. - Score = 0 otherwise. IMPORTANT: - Scoring is based ONLY on final alignment. - Do NOT use any information about whether modification was needed. ## STEP 5: Needs Modification analysis (IMAGE 1 ONLY report only) Look ONLY at IMAGE 1 again. For each axis, decide whether the objects ORIGINAL orientation matched the pyramids TARGET orientation: - Needs Modification = YES - Needs Modification = NO Rules: - This step is for reporting only. - It MUST NOT affect the score. 43 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 27. Evaluation Prompt for Orientation Alignment 2/3 Orientation Alignment 2/3 - Do NOT reference IMAGE 2. - Do NOT infer from the final result. ## OUTPUT FORMAT First provide reasoning under the following sections: - ## Object & Anchor - ## Target Orientation (IMAGE 1) - ## Final Orientation (IMAGE 2) - ## Axis-wise Alignment - ## Needs Modification (Report Only) Then output the final result as JSON (LAST PART ONLY): { Yaw: { needs modification: true, reason: string, score: 0 }, Pitch: { needs modification: false, reason: string, score: 0 }, Roll: { needs modification: true, reason: string, score: 0 } } VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 28. Evaluation Prompt for Reorientation Contextual Preservation 1/2 Reorientation Contextual Preservation 1/2 You are given TWO images. ## IMAGE 1 (Input Image) - The original image before editing. ## IMAGE 2 (Generated Image) - The model-generated image after orientation editing. Your task is to evaluate TWO independent metrics: 1. Identity Consistency (IC) 2. Visual Integrity (VI) These two metrics MUST be evaluated separately and independently. ## PART 1: Identity Consistency (IC) Identity Consistency evaluates whether the edited subject in IMAGE 2 remains the same semantic object/entity as in IMAGE 1, ignoring changes that are directly caused by orientation editing. ### STEP 1: Identify the edited subject - Look at IMAGE 1 and identify the main subject intended to be edited. - Look at IMAGE 2 and identify the corresponding subject. ### STEP 2: Identity consistency check Decide whether the subject in IMAGE 2 is the same identity as in IMAGE 1. You MUST ignore: - Orientation or facing-direction changes. - Pose changes caused by reorientation. - Perspective changes due to rotation. - Lighting, shading, or color changes. - Minor texture or detail variations. You MUST penalize: - Object replacement (e.g., shoe different shoe type or different object). - Object removal or duplication. - Structural corruption (missing parts, extra parts, broken anatomy). - Conversion into fundamentally different semantic entity. ### IC Scoring (binary): - Score = 1 if the subject identity and overall structure are clearly preserved. - Score = 0 if the subject is replaced, missing, duplicated, or structurally corrupted. 45 VIBE: Systematic Benchmark for Visual Instruction-Driven Image Editing Table 29. Evaluation Prompt for Reorientation Contextual Preservation 1/2 Reorientation Contextual Preservation 1/2 If unsure, score 0. ## PART 2: Visual Integrity (VI) Visual Integrity evaluates whether the editing process introduced severe visual artifacts or layout corruption that significantly degrade image quality. ### STEP 3: Visual corruption inspection Compare IMAGE 2 against IMAGE 1 and check for severe issues such as: - Prominent UI or indicator pollution (e.g., orientation pyramid or markers appearing in the image center). - Large unnatural visual artifacts or rendering glitches. - Structural image collapse (e.g., extreme distortion, tearing, duplicated geometry). - Occlusions or obstructions that severely harm image readability. You MUST ignore: - Orientation changes of the edited subject. - Normal perspective changes caused by rotation. - Lighting or color changes. - Minor artifacts that do not significantly affect readability. ### VI Scoring (binary): - Score = 1 if IMAGE 2 remains visually coherent and readable, with no severe artifacts or layout corruption. - Score = 0 if IMAGE 2 exhibits clear visual breakdown or UI pollution. If unsure, score 0. ## OUTPUT FORMAT First provide your reasoning under: - ## Identity Consistency Analysis - ## Visual Integrity Analysis Then output the final result as JSON (LAST PART ONLY): { Identity Consistency: { reason: string (one-sentence summary), score: 0 }, Visual Integrity: { reason: string (one-sentence summary), score: 0 } }"
        }
    ],
    "affiliations": [
        "Hangzhou Dianzi University",
        "Institute of Automation, Chinese Academy of Sciences",
        "Language Technology Lab, University of Cambridge",
        "School of Artificial Intelligence, University of Chinese Academy of Science"
    ]
}