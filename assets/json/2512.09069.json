{
    "paper_title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "authors": [
        "Erfan Nourbakhsh",
        "Nasrin Sanjari",
        "Ali Nourbakhsh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT."
        },
        {
            "title": "Start",
            "content": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification Erfan Nourbakhsh Artificial Intelligence Department University of Isfahan Isfahan, Iran erfannourbakhsh2001@gmail.com Nasrin Sanjari* Labbafinejad Hospital Shahid Beheshti University of Medical Science Tehran, Iran nasrinsanjari@mail.mui.ac.ir Ali Nourbakhsh Department of Mechanical Engineering Isfahan University of Technology Isfahan, Iran nourbakhsh.a@me.iut.ac.ir Abstract Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, novel knowledge distillation framework, termed is proposed to compress high-performance KD-OCT, ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiencyaccuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KDOCT. Keywords Knowledge Distillation, Retinal OCT, AMD Classification, ConvNeXt, Healthcare AI, Model Compression I. INTRODUCTION Age-related macular degeneration (AMD) is leading cause of irreversible vision loss globally, representing about 8.7% of worldwide blindness and mainly impacting those over 60 [1, 2]. Designated priority eye disease by the World Health Organization, its prevalence is expected to surge with aging populations, potentially affecting 288 million people by 2040 [3]. As chronic disorder, AMD strains healthcare systems and reduces quality of life by causing gradual central vision loss. AMD manifests in two primary forms: dry and wet. Dry AMD, comprising 80-90% of cases, is characterized by the accumulation of drusen, extracellular deposits between the retinal pigment epithelium (RPE) and Bruch's membrane, leading to RPE atrophy and photoreceptor loss [4, 5]. In 1020% of instances, dry AMD progresses to wet AMD, involving choroidal neovascularization (CNV), fluid leakage, and rapid retinal damage [6]. Early detection is critical, as treatments like anti-vascular endothelial growth factor (antiVEGF) injections can mitigate wet AMD progression, though they are costly, require repeated administration, and carry risks of recurrence [7]. Optical coherence tomography has revolutionized AMD diagnosis as non-invasive, high-resolution imaging modality that provides cross-sectional views of retinal structures, enabling precise identification of drusen, CNV, and other pathologies [8, 9]. However, manual OCT interpretation is labor-intensive, especially given the volume of scans and the chronic monitoring required for AMD patients. This underscores the need for automated computer-aided diagnosis (CAD) systems to alleviate clinical workloads and improve screening efficiency. Recent advancements in deep learning have yielded promising OCT classification models, often incorporating multi-scale feature fusion or convolutional neural networks (CNNs) to handle varying lesion sizes [10, 11]. However, state-of-the-art models like ConvNeXtV2-Large [39], despite high accuracy, remain computationally demanding (197M parameters), restricting deployment in resource-limited clinical environments [12]. Knowledge distillation (KD) resolves this by transferring knowledge from large teacher models to compact student models [15, 16]. In KD, the student learns from both hard ground-truth labels and the teacher's softened probability distributions (soft labels), which encode nuanced inter-class relationships and boost generalization. This typically uses combined loss function balancing crossentropy on true labels with Kullback-Leibler [32] divergence on teacher-student outputs, enabling efficient compression without significant accuracy loss [13-17]. In this study, we introduce KD-OCT, new knowledge distillation framework that compresses high-performance ConvNeXtV2-Large teacher modelaugmented with advanced techniques, stochastic weight averaging, and focal lossinto compact EfficientNet-B2 student for classifying normal, drusen, and CNV in retinal OCT images. KD-OCT uses real-time distillation via temperature-scaled combined loss and is assessed on the Noor Eye Hospital (NEH) dataset with patient-level 5-fold cross-validation. Results show KDOCT attains near-teacher accuracy with 25.5 fewer parameters, surpassing similar multi-scale or feature-fusion OCT classifiers in efficiency-accuracy trade-off, enabling edge deployment for AMD screening. II. RELATED WORKS The automated classification of retinal pathologies from OCT images has evolved significantly, driven by the need for efficient screening of AMD and related conditions such as drusen and CNV. Early studies relied on traditional machine learning approaches, which typically involved multi-stage pipelines including preprocessing (e.g., denoising and retinal flattening), manual feature extraction using descriptors like histogram of oriented gradients (HOG), local binary patterns (LBP), or scale-invariant feature transform (SIFT), and classification via algorithms such as support vector machines (SVM) or random forests [27-29]. These methods achieved reasonable results but were limited by the time-consuming nature of feature engineering, expert dependency, and poor generalization across datasets due in interpretations. to variations With the rise of deep learning (DL), convolutional neural networks (CNNs) have emerged as the foundation for end-toend OCT classification, automatically extracting hierarchical features without manual input [30, 31]. Classic models like VGG [18], Inception [19], and ResNet [20] have been adapted for retinal disease detection, achieving high accuracy in AMD stage identification [18, 19, 20]. To tackle varying lesion sizes in OCT images (e.g., small drusen vs. extensive CNV), multiscale methods have become key. For example, multi-scale deep feature fusion (MDFF) merges features across scales to capture inter-scale differences and boost discriminative ability. Feature pyramid networks (FPN) integrate top-down propagation with lateral connections to retain fine details alongside high-level context, lowering model complexity. Spatial attention mechanisms in multi-scale setups, often with depthwise separable convolutions, highlight pathological areas while managing parameter expansion. Recently, Transformer-based models have been investigated for their global receptive fields, differing from CNNs' local emphasis [21, 22]. Vision Transformers (ViT) have been tailored for retinal OCT classification [22], including variants like structure-oriented Transformers that integrate clinical priors (e.g., structure-guided modules) for disease grading [23]. Hybrid CNN-Transformer models, featuring parallel branches for local and global feature extraction with adaptive fusion, have excelled in multi-class retinal disease tasks [24, 25]. ConvNeXt, Transformerinspired pure CNN architecture, exhibits robust feature learning on limited data, rendering it ideal as backbone for OCT analysis [26, 12]. in medical State-of-the-art models image analysis, especially for OCT classification, display notable differences in architecture, efficiency, and applicability to AMD detection tasks, as shown in Figure 1. ResNet [38] introduced residual learning via skip connections to train very deep CNNs, alleviating vanishing gradients and enabling robust feature extraction in medical imaging, although it depends on local receptive fields and may falter with global dependencies in intricate retinal structures. Conversely, Swin Transformer [37] features hierarchical Vision Transformer with shifted windows for efficient self-attention, grasping multi-scale contextual details and long-range interactions, which excels in tasks dense prediction like OCT segmentation and classification by managing varying lesion scales more adeptly than conventional CNNs. ConvNeXt [26] updates CNNs by adding Transformer-inspired components (e.g., larger kernels, GELU activations) to rival hierarchical Transformers, providing mix of computational efficiency and performance in resource-limited medical environments. Its successor, scalability using masked ConvNeXtV2 autoencoders for self-supervised pre-training, enhancing representation learning on scarce labeled data common in clinical OCT datasets and delivering superior generalization in multi-class retinal disease tasks over prior versions. [39], boosts Fig. 1. Comparison of block architectures in SOTA models for medical image analysis: (a) Swin Transformer Block [37], featuring shifted window-based multi-head self-attention for efficient hierarchical processing; (b) ResNet Block [38], utilizing residual connections with batch normalization and ReLU activations for deep network training; (c) ConvNeXt Block [26], incorporating depthwise convolutions, layer normalization, and GELU for Transformer-inspired CNN efficiency; (d) ConvNeXtV2 Block [39], enhancing the prior with global response normalization (GRN) for improved scaling and self-supervised learning. learning, class balancing, While these advancements have boosted accuracy, the computational demands of large models like ConvNeXtV2Large (197M parameters) limit clinical deployment [12]. Knowledge distillation (KD) serves as vital compression method, transferring knowledge from complex \"teacher\" to lightweight \"student\" through soft labels and intermediate representations [15]. In medical imaging, KD extends to semisupervised and privacy preservation, as noted in recent surveys [16, 32]. In retinal imaging, multi-task KD enables eye disease prediction from fundus images, with teacher ensembles distilling knowledge across coarse/fine-grained classification and textual diagnosis generation, yielding high performance on limited labeled data [33]. For anomaly detection in retinal fundus images, crossarchitecture KD compresses Vision Transformers (ViT) to CNNs for edge deployment on devices like NVIDIA Jetson Nano, maintaining 93% of teacher accuracy with 97.4% fewer parameters [17]. In OCT-specific applications, fundusenhanced disease-aware KD transfers unpaired fundus knowledge to OCT models via class prototype matching and similarity alignment, enhancing multi-label retinal disease classification without paired datasets [34]. Unsupervised anomaly detection in OCT employs Teacher-Student KD, training only on normal scans to detect pathologies (e.g., AMD, DME) and produce anomaly scores and maps for screening [35]. Equity-enhanced KD has been used for glaucoma progression prediction from OCT, ensuring demographic fairness [36]. Despite these advances, gaps remain in applying KD to clinical-grade AMD classification from OCT, particularly in cross-architecture distillation for efficiency, real-time teacher inference to avoid pre-computing labels, and integration with domain-specific enhancements like patient-disjoint validation for robust generalization on imbalanced datasets. Our KDOCT framework addresses these by compressing an enhanced ConvNeXtV2-Large teacher to an EfficientNet-B2 student, leveraging real-time distillation and tailored augmentations for scalable AMD screening. III. DATASET The proposed KD-OCT method was evaluated on two publicly available databases to assess its performance in classifying normal, drusen, and CNV cases from retinal OCT images. The primary dataset is the Noor Eye Hospital (NEH) dataset, consisting of anonymized OCT images acquired using the Heidelberg Spectralis SD-OCT imaging system at Noor Eye Hospital, Tehran, Iran [40]. The images contain no marks, features, or patient identifiers to ensure privacy, and all Bscans were labeled by retinal specialist. Inclusion criteria included individuals over 50 years of age, absence of any other retinal pathologies, and good image quality (signal strength 20). To simulate challenging conditions, only the worstcase B-scans per volume were retained (e.g., for CNV patients, scans prominently displaying CNV), resulting in 12,649 B-scans from an original total of 16,822 across 441 patients and 554 eyes. The class distribution includes 5,667 normal scans from 120 patients, 3,742 drusen scans from 160 patients, and 3,240 CNV scans from 161 patients. The secondary dataset is the University of California San Diego (UCSD) dataset [41], which includes four categories: CNV, diabetic macular edema (DME), drusen, and normal. The training set comprises 108,312 retinal OCT images from 4,686 patients, with 37,206 CNV, 11,349 DME, 8,617 drusen, and 51,140 normal images. The test set consists of 1,000 images from 633 patients, evenly distributed with 250 from each category. IV. PROPOSED APPROACH A. Data Preparation To ensure robust evaluation and fair comparison, the datasets were divided into training, validation, and test sets, as shown in Figure 2. For the Noor Eye Hospital (NEH) dataset, 20% of the total data was assigned to the test set for independent benchmarking, with the remaining 80% split into training and validation. From this 80%, 20% was set aside for validation to track performance and avoid overfitting, leaving the rest for training. This stratified split occurred at the patient level to prevent data leakage, ensuring no patient scan overlap across sets and enhancing generalization in clinical settings. For the UCSD dataset, the predefined test set of 1,000 images was kept unchanged, while the 108,312-image training set was subdivided with 20% for validation and the remainder for training. This setup aligns with baseline methods, like the Multi-Scale Convolutional Neural Network [10], which used comparable validation ratios to tune hyperparameters and assess performance on imbalanced retinal OCT classes. Fig. 2. Overview of data preparation B. Data Augmentation Data augmentation is vital in the KD-OCT framework, artificially enlarging the training dataset, boosting model robustness, and reducing overfitting, especially in knowledge distillation, where the student gains from varied inputs to replicate the teacher's generalizations on imbalanced retinal OCT data. As shown in Figure 3, the augmentation approach is customized for training, validation, and inference phases to balance complexity and efficiency while maintaining clinical relevance, including managing variations in scan orientation, lighting, and artifacts typical in OCT imaging. include rotations For the training pipeline, comprehensive sequence of transformations is applied to introduce variability and simulate real-world imperfections in retinal scans. The process begins with resizing the image to larger square dimension, followed by random crop to target square size, which normalizes dimensions while introducing spatial diversity to focus on varying retinal regions. We then apply fixed number of random operations from set including brightness, contrast, saturation, sharpness, rotation, and translation adjustments, automating policy selection to improve generalization without manual tuning. Subsequent steps to simulate probe orientation differences, affine transformations with shear and scale parameters for geometric distortions like misalignments due to patient movement, and color adjustments with brightness, contrast, saturation, and hue factors to account for intensity variations across devices. Horizontal and vertical flips, each with specified probability, add symmetry invariance, mimicking left/right or top/bottom scan flips. Blurring with kernel size and probability emulates blurry or noisy acquisitions, while bit reduction with probability handles quantization effects from compression. The image is then converted to normalized tensor range, followed by erasing with probability and scale range to simulate occlusions like blood vessels or artifacts, and finally normalized using ImageNet-derived mean and standard deviation statistics for consistency with pretrained models. The output is normalized tensor in channel-height-width format, promoting resilience to clinical variabilities in OCT scans. The validation pipeline is kept minimal to evaluate the model on near-original data, consisting of resizing to target square dimension, conversion to normalized tensor range, and normalization with the same statistics as training. This ensures an unbiased assessment without introducing traininglike variability. For inference, Test-Time Augmentation (TTA), technique that applies data augmentations during inference to generate multiple input variants and ensembles their predictions for is improved reliability and reduced uncertainty [43], employed to boost prediction reliability by generating multiple augmented versions of each input and averaging their outputs. The five augmentations include: (1) the original resized and normalized image; (2) horizontal flip after resize and normalize; (3) vertical flip after resize and normalize; (4) resize to larger dimension followed by center crop to the target size and normalize; and (5) resize with small rotation and normalize. This produces list of five normalized tensors, whose averaged logits enhance accuracy and robustness, particularly for subtle AMD features in OCT, by reducing sensitivity to minor input perturbations without additional training overhead. intermediate expansion, captures intricate retinal patterns like drusen deposits or CNV membranes, producing the stage output. The final downsampling to Stage 4 yields even higher channels at smaller resolution, processed by blocks with large expansion, outputting the final backbone features. The classification head applies global average pooling to reduce spatial dimensions, followed by dropout for regularization, and fully connected layer to generate raw logits for multiclass prediction without additional activation. Fig. 3. Overview of the data augmentation pipelines in KD-OCT, including the training sequence with RandAugment and geometric/color transforms, minimal validation steps, and Test-Time Augmentation (TTA) variants for inference. C. Teacher Model Architecture [39], backbone The core of the KD-OCT teacher model uses the ConvNeXtV2-Large cutting-edge convolutional neural network that integrates Transformerinspired efficiencies while preserving CNN strengths in inductive biases and computational scalability. Pretrained on ImageNet-22K and fine-tuned on ImageNet-1K via Fully Convolutional Masked AutoEncoder (FCMAE) [39] for selfsupervised learning, it features large parameter count and handles input images in batch-channel-height-width format. drop path rate provides stochastic depth regularization to boost generalization during training. As shown in Figure 4, the architecture includes stem layer, four hierarchical stages with downsampling transitions, and classification head, supporting progressive feature extraction from low-level details to high-level semantics for classifying retinal OCT scans as normal, drusen, or CNV. The stem layer initializes feature processing with convolutional kernel and stride, expanding input channels, followed by LayerNorm, resulting in an output with increased channels and reduced spatial dimensions. Stage 1 focuses on early feature extraction with several ConvNeXtV2 blocks at initial channels and resolution (with progressive drop path), each comprising DepthWise Conv, LayerNorm, Linear expansion, GELU activation, Global Response Normalization (GRN), and Linear reduction back to base channels, yielding the same dimension. Downsampling to Stage 2 uses LayerNorm and convolutional stride, doubling channels and halving resolution. Stage 2 employs blocks with similar components but expanded intermediate channels, outputting the updated dimension."
        },
        {
            "title": "Further  downsampling",
            "content": "to Stage 3 (LayerNorm + convolutional stride) increases channels while reducing resolution. This primary feature extraction stage, the deepest with numerous blocks (progressive drop path) and substantial Fig. 4. Overview of the teacher model training D. Knowledge Distillation to Integrating the preceding components, data preparation, augmentation, and teacher model architecture, the KD-OCT framework employs knowledge distillation transfer expertise from the high-capacity ConvNeXtV2-Large teacher to the lightweight EfficientNet-B2 student [44], enabling efficient deployment while preserving clinical-grade performance in retinal OCT classification, as illustrated in Figure 5. This cross-architecture distillation process [17] first involves training the teacher model end-to-end on the prepared and augmented data using focal loss [44] to handle class imbalance, stochastic weight averaging (SWA) for smoother convergence, and advanced like differential learning rates (head: 1e-4, backbone: 2e-5) with AdamW optimization [46], weight decay to prevent overfitting by regularizing model weights, 10-epoch warmup, and cosine annealing scheduler [45] over up to 150 epochs. The teacher's heavy augmentation pipeline ensures robust feature learning, capturing nuanced retinal pathologies like subtle drusen or CNV irregularities. techniques The focal loss is defined as: ùêπùêø = ùõº! . (1 ùúå!)\" . log(ùúå!) (1) where ùõº! is the class weighting factor, ùúå! is the predicted probability for the true class, and ùõæ is the focusing parameter (typically set to 2.0 in our experiments) that down-weights easy examples to emphasize hard ones."
        },
        {
            "title": "The  cosine  annealing  scheduler  adjusts  the  learning  rate",
            "content": "as: ùëôùëü = ùëöùëñùëõ_ùëôùëü + (ùëèùëéùë†ùëí_ùëôùëü ùëöùëñùëõ_ùëôùëü) 0.5 (1 + cos(ùúã . ùëùùëüùëúùëîùëüùëíùë†ùë†) ) (2) where ùëèùëéùë†ùëí_ùëôùëü is the initial learning rate, ùëöùëñùëõ_ùëôùëü is the minimum learning rate, and ùëùùëüùëúùëîùëüùëíùë†ùë† is the fractional progress through the annealing cycle. Fig. 5. Overview of the KD-OCT framework, showing knowledge transfer from the ConvNeXtV2-Large teacher to the EfficientNet-B2 student via realtime distillation Once trained, real-time KD is performed where the frozen teacher generates soft labels on-the-fly during student training, avoiding offline logit pre-computation and allowing dynamic knowledge transfer adapted to the student's progress [16]. The student, based on EfficientNet-B2, uses lighter augmentation strategy (e.g., reduced number of random operations to limit intensity, milder rotations to simulate subtle variations without excessive distortion, no blur/posterize) and unified learning rate with AdamW (weight decay to prevent overfitting by regularizing model weights, warmup period to stabilize initial training, cosine annealing scheduler to gradually reduce the learning rate for better convergence over multiple epochs, early stopping patience to halt training when validation performance plateaus). The combined loss balances low-weighted crossentropy for hard ground-truth labels with high-weighted, temperature-scaled Kullback-Leibler divergence for soft teacher knowledge, helping the student learn inter-class similarities and generalize on imbalanced datasets without focal loss or SWA. Batch configurations maintain an effective size (teacher: smaller batch size with higher accumulation steps; student: larger batch size with fewer accumulation steps) using FP16 mixed precision for efficiency. This approach compresses the model for edge deployment and outperforms baselines in efficiency-accuracy trade-offs for AMD screening. the finely The KD-OCT learning rates for V. HYPER-PARAMETERS tuned uses framework hyperparameters to optimize performance and enable efficient knowledge transfer from the ConvNeXtV2-Large teacher to the EfficientNet-B2 student. Key configurations include teacher (1e-4 for differential classification head, 2e-5 for backbone) with 0.05 weight decay, 10-epoch linear warmup, and cosine annealing scheduler decaying to 1e-7 over up to 150 epochs (early stopping patience 25), while the student employs unified 1e3 learning rate, 0.01 weight decay, 5-epoch warmup, and cosine annealing to 1e-6 over maximum of 100 epochs (patience 20). Both leverage AdamW optimization and FP16 mixed precision training with an effective batch size of 16 via gradient accumulation (teacher: batch size 4, accumulation 4; student: batch size 8, accumulation 2). Distillation applies 4.0 temperature for soft labels, with loss weights balancing hard supervision (Œ≤=0.3, cross-entropy) and soft transfer (Œ±=0.7, Kullback-Leibler divergence). Augmentations feature RandAugment (N=2, M=9 for teacher; M=7 for student), rotations (20 teacher; 15 student), and TTA using 5 variants to boost robustness. Training occurred on an NVIDIA H200 GPU, utilizing its high memory bandwidth to manage large models and batches effectively. VI. RESULTS The experimental results demonstrate the KD-OCT framework's superior efficacy in retinal OCT classification, balancing high accuracy with computational efficiency for clinical deployment. On the NEH dataset, evaluated via fivefold patient-level cross-validation for three-class classification (normal, drusen, CNV; Table I), the ConvNeXtV2-Large teacher achieved 92.6% accuracy, outperforming baselines such as FPN-VGG16 (92.0%) [10] and MedSigLIP (84.5%) [47]. This highlights the teacher's advanced architecture and robust training, including focal loss and heavy augmentations, for handling class imbalances and subtle pathologies like early drusen or CNV. Even more compelling is the performance of the distilled EfficientNet-B2 student model on the same NEH dataset, attaining 92.46% accuracy, nearly matching the teacher, while drastically reducing model size from 196.4 million to just 7.7 million parameters, 25.5 compression. This not only surpasses multi-scale competitors like FPN-DenseNet121 (90.9% accuracy) [10] and SF Net (82.6% accuracy) [52] but also underscores KD-OCT's strength in knowledge transfer, where the student inherits the teacher's nuanced understanding without the computational overhead, making it ideal for resource-limited clinical settings like portable OCT devices. TABLE I. THE RESULTS OF THREE-CLASS CLASSIFICATION TASK ON THE NEH DATASET, EVALUATED USING FIVE-FOLD CROSS-VALIDATION. (*THE RESULTS OF THIS MODEL ARE DIRECTLY REPORTED FROM [10].) Models HOG + SVM* VGG16* [18] ResNet50* [38] DenseNet121* [48] EfficientNetB0* [49] Kermany et al.* [41] Kaymak et al.* [50] Thomas et al.* [51] FPN-VGG16* [10] FPN-ResNet50* [10] FPNDenseNet121* [10] FPNEfficientNetB0* [10] SF net [52] Pram(mil) - 28.3 23.6 Accuracy 67.2 3.7 91.6 2.2 86.8 2.0 Sensitivity 66.99 3.1 91.4 2.0 86.4 1.6 Specificity 74.3 2.5 95.6 1.1 93.0 0.9 7.0 4. 90.0 1.4 89.7 1.7 94.7 0.8 85.4 2.6 84.5 2.2 92.1 1. 0.02 83.9 1.7 82.9 2.3 91.4 1.0 58.3 80.2 4. 80.0 4.4 89.4 2.5 2.5 68.5 4.9 69.1 4.3 83.8 2. 21.6 92.0 1.6 91.8 1.7 95.8 0.9 31.1 90.1 2. 89.8 2.8 94.8 1.4 14.3 90.9 1.4 90.5 1.9 95.2 0. 12.7 87.8 1.3 86.6 1.8 93.3 0.8 29.2 82.6 2. MedSigLIP [47] 430.4 84.5 3.2 80.4 2.8 81.81 4.64 96.2 0.6 94.42 1.09 KD-OCT(Ours) ConvNeXtV2Large KD-OCT(Ours) EfficientNet-B 196.4 92.6 2.3 92.9 2.1 98.1 0.8 7.7 92.46 1. 92.15 1.29 96.04 0.78 To validate generalizability, KD-OCT was tested on the UCSD dataset for four-class classification (normal, drusen, CNV, DME) using the predefined test set (Table II). Without fine-tuning or domain adaptation, both teacher and student models achieved 98.4% accuracy, outperforming baselines like Hassan et al. (98.6%, but requiring preprocessing) [53] and FPN-VGG16 (98.4%) [10]. This seamless transfer across datasets, despite imaging system differences and an added DME class, illustrates the framework's robustness, as distilled knowledge enables high-fidelity predictions on unseen data from diverse clinical environments. TABLE II. RESULTS OF FOUR-CLASS CLASSIFICATION TASK ON THE UCSD DATASET. (* THE RESULTS OF THIS MODEL ARE DIRECTLY REPORTED FROM [10].) Models VGG16* [18] ResNet50* [38] EfficientNetB0 * [49] Kermany et al.* [41] Kaymak et al.* [50] Hassan et al.* [53] FPN-VGG16* [10] KD-OCT(Ours) ConvNeXtV2Large KD-OCT(Ours) EfficientNet-B Preprocess Accuracy 93.9 96.7 Sensitivity 100 99.6 Specificity 90.8 94.8 95.0 96. 97.1 98.6 98.4 99.8 97.8 98. 98.27 100 91.4 97.4 99.6 99. 97.4 98.4 98.45 99.47 98.4 98. 99.47 In more stringent five-fold cross-validation on the UCSD training set (Table III), the teacher and student models further excelled with accuracies of 97.72% and 97.74%, respectively, eclipsing multi-scale approaches like Fang et al. (TMI) (90.1% accuracy) [42] and FPN-VGG16 (93.9% accuracy) [10]. These consistent gains highlight KD-OCT's generalization advantage, with cross-architecture distillation preserving diagnostic precision while reducing inference time, enabling scalable real-time AMD screening globally. TABLE III. RESULTS OF FOUR-CLASS CLASSIFICATION TASK ON THE UCSD DATASET, EVALUATED USING FIVE-FOLD CROSS-VALIDATION (* THE RESULTS OF THIS MODEL ARE DIRECTLY REPORTED FROM [10].) Models Fang et al. (JVCIR)* [54] Fang et al. (TMI)* [42] FPN-VGG16* [10] KD-OCT(Ours) ConvNeXtV2Large KD-OCT(Ours) EfficientNet-B2 Preprocess Accuracy Sensitivity Specificity 87.3 90.1 93.9 84. 86.6 93.4 95.8 96.6 98.0 97. 97.72 99.26 97.74 97.74 99.21 To further elucidate the contributions of the key enhancements in the teacher model, an ablation study was conducted on the NEH dataset using five-fold crossvalidation. Removing advanced augmentations (replacing with basic resizing and normalization) reduced the teacher's accuracy, sensitivity, and specificity, emphasizing their role in enhancing robustness to clinical variabilities like scan orientation and artifacts. Excluding stochastic weight averaging caused moderate performance decline, as it supports smoother optimization and better generalization on imbalanced classes. Omitting focal loss (reverting to standard cross-entropy) led to the largest drop, highlighting its value in tackling class imbalance by focusing on hard examples such as subtle CNV cases. Collectively, these enhancements improved the student's distilled performance over baseline, preserving near-teacher quality for efficient deployment. VII. CONCLUSION AND FUTURE WORKS In this study, we introduced KD-OCT, novel knowledge distillation framework that compresses high-performance ConvNeXtV2-Large teacher modelenhanced with advanced augmentations, focal loss, and stochastic weight averaginginto lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV in retinal OCT images. Using real-time distillation with temperature-scaled combined loss (balancing soft teacher knowledge and hard ground-truth supervision), KD-OCT attains near-teacher accuracy (92-98%) with 25.5 parameter reduction and faster inference, surpassing multi-scale and feature-fusion baselines in efficiency-accuracy trade-off on the NEH and UCSD datasets. This cross-architecture method, with patientdisjoint augmentations, and overcomes computational barriers in clinics, promoting robust generalization on imbalanced classes and edge deployment for scalable AMD screening. Future work will explore semisupervised KD to reduce labeled data reliance, multi-modal distillation with fundus images for improved accuracy, and extension to other retinal pathologies like diabetic macular edema, while optimizing for real-time integration in portable devices. cross-validation tailored"
        },
        {
            "title": "REFERENCES",
            "content": "[1] W. L. Wong et al., Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: systematic review and meta-analysis, The Lancet Global Health, vol. 2, no. 2, pp. e106e116, 2014. [2] D. J. Taylor, A. E. Hobby, A. M. Binns, and D. P. Crabb, How does age-related macular degeneration affect real-world visual ability and quality of life? systematic review, BMJ open, vol. 6, no. 12, p. e011504, 2016. [3] R. Rasti, H. Rabbani, A. Mehridehnavi, and F. Hajizadeh, Macular OCT classification using multi-scale convolutional neural network ensemble, IEEE transactions on medical imaging, vol. 37, no. 4, pp. 10241034, 2017. [4] A. Abdelsalam, L. Del Priore, and M. A. Zarbin, Drusen in age-related macular degeneration: pathogenesis, natural course, and laser photocoagulation--induced regression, Survey of ophthalmology, vol. 44, no. 1, pp. 129, 1999. [5] V. Das, S. Dandapat, and P. K. Bora, Multi-scale deep feature fusion for automated classification of macular pathologies from OCT images, Biomedical signal processing and Control, vol. 54, p. 101605, 2019. [6] K. B. Freund, L. A. Yannuzzi, and J. A. Sorenson, Age-related macular degeneration and choroidal neovascularization, American journal of ophthalmology, vol. 115, no. 6, pp. 786791, 1993. [7] D.-K. Hwang et al., Artificial intelligence-based decision-making for age-related macular degeneration, Theranostics, vol. 9, no. 1, p. 232, 2019. [8] M. E. Brezinski and J. G. Fujimoto, Optical coherence tomography: high-resolution imaging in nontransparent tissue, IEEE Journal of selected topics in quantum electronics, vol. 5, no. 4, pp. 11851192, 2002. [9] C. A. Puliafito et al., Imaging of macular diseases with optical coherence tomography, Ophthalmology, vol. 102, no. 2, pp. 217 229, 1995. [10] S. Sotoudeh-Paima, A. Jodeiri, F. Hajizadeh, and H. Soltanian-Zadeh, Multi-scale convolutional neural network for automated AMD classification using retinal OCT images, Computers in biology and medicine, vol. 144, p. 105368, 2022. [11] S. Pang et al., novel approach for automatic classification of macular degeneration OCT images, Scientific reports, vol. 14, no. 1, p. 19285, 2024. [12] J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 71327141. [13] A. Sevinc, M. Ucan, and B. Kaya, Distillation Approach to Transformer-Based Medical Image Classification with Limited Data, Diagnostics, vol. 15, no. 7, p. 929, 2025. OCT images, in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2023, pp. 639648. [35] G. Aresta, T. Ara√∫jo, U. Schmidt-Erfurth, and H. Bogunoviƒá, Anomaly Detection in Retinal OCT Images With Deep LearningBased Knowledge Distillation, Translational Vision Science & Technology, vol. 14, no. 3, pp. 2626, 2025. [14] W. Xu and Y. Wan, ELA: Efficient local attention for deep convolutional neural networks, arXiv preprint arXiv:2403. 01123, 2024. [36] S. O. Afolabi, L. Gheisi, J. Shan, L. Q. Shen, M. Wang, and M. Shi, Equity-Enhanced Glaucoma Progression Prediction from OCT with Knowledge Distillation, medRxiv, 2025. [15] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in [37] Z. Liu et al., Swin Transformer: Hierarchical Vision Transformer neural network, arXiv preprint arXiv:1503. 02531, 2015. [16] X. Li et al., Knowledge distillation and teacher-student learning in medical imaging: Comprehensive overview, pivotal role, and future directions, Medical Image Analysis, p. 103819, 2025. [17] B. Yilmaz and A. Aiyengar, Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano, arXiv preprint arXiv:2506. 18220, 2025. [18] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409. 1556, 2014. [19] Z. Li, K. Cheng, P. Qin, Y. Dong, C. Yang, and X. Jiang, Retinal oct image classification based on domain adaptation convolutional neural networks, in 2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2021, pp. 15. [20] A. Kumar, L. Nelson, and S. Gomathi, Automated diagnosis of retinal diseases from oct images using resnet-18, in 2024 Asia Pacific Conference on Innovation in Technology (APCIT), 2024, pp. 16. [21] A. Vaswani et al., Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [22] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010. 11929, 2020. [23] J. Shen, Y. Hu, X. Zhang, Y. Gong, R. Kawasaki, and J. Liu, Structure-Oriented Transformer for retinal diseases grading from OCT images, Computers in Biology and Medicine, vol. 152, p. 106445, 2023. [24] H. Yang, L. Chen, J. Cao, and J. Wang, Hrs-net: hybrid multiscale network model based on convolution and transformers for multiclass retinal disease classification, IEEE Access, 2024. [25] Z. Ma, Q. Xie, P. Xie, F. Fan, X. Gao, and J. Zhu, HCTNet: hybrid ConvNet-transformer network for retinal optical coherence tomography image classification, Biosensors, vol. 12, no. 7, p. 542, 2022. [26] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 1197611986. [27] P. P. Srinivasan et al., Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images, Biomedical optics express, vol. 5, no. 10, pp. 35683577, 2014. [28] A. Albarrak, F. Coenen, Y. Zheng, and Others, Age-related macular degeneration identification in volumetric optical coherence tomography using decomposition and local feature extraction, in Proceedings of 2013 international conference on medical image, understanding and analysis, 2013, pp. 5964. [29] G. Lema√Ætre, Classification of SD-OCT Volumes Using Local Binary Patterns: Experimental Validation for DME Detection, Journal of ophthalmology, vol. 2016, no. 1, 2016. [30] D. S. W. Ting et al., Artificial intelligence and deep learning in ophthalmology, British Journal of Ophthalmology, vol. 103, no. 2, pp. 167175, 2019. [31] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, nature, vol. 521, no. 7553, pp. 436444, 2015. [32] S. Kullback and R. A. Leibler, On information and sufficiency, The annals of mathematical statistics, vol. 22, no. 1, pp. 7986, 1951. [33] S. Chelaramani, M. Gupta, V. Agarwal, P. Gupta, and R. Habash, Multi-task knowledge distillation for eye disease prediction, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021, pp. 39833993. [34] L. Wang, W. Dai, M. Jin, C. Ou, and X. Li, Fundus-enhanced disease-aware distillation model for retinal disease classification from using Shifted Windows, in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 999210002. [38] K. He, X. Zhang, S. Ren, and J. Sun, Deep Residual Learning for Image Recognition, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770778. [39] S. Woo et al., ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders, in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 16133 16142. [40] S. Sotoudeh-Paima, Labeled retinal optical coherence tomography dataset for classification of normal, drusen, and CNV cases. Mendeley, 2021. [41] D. S. Kermany et al., Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning, Cell, vol. 172, no. 5, pp. 1122-1131.e9, 2018. [42] L. Fang, C. Wang, S. Li, H. Rabbani, X. Chen, and Z. Liu, Attention to lesion: Lesion-aware convolutional neural network for retinal optical coherence tomography image classification, IEEE transactions on medical imaging, vol. 38, no. 8, pp. 19591970, 2019. [43] G. Wang, W. Li, M. Aertsen, J. Deprest, S. Ourselin, and T. Vercauteren, Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks, Neurocomputing, vol. 338, pp. 3445, 2019. [44] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll√°r, Focal Loss for Dense Object Detection, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 318327, 2020. [45] I. Loshchilov and F. Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts, arXiv [cs.LG]. 2017. [46] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711. 05101, 2017. [47] A. Sellergren et al., Medgemma technical report, arXiv preprint arXiv:2507. 05201, 2025. [48] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, Densely connected convolutional networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 47004708. [49] M. Tan and Q. Le, Efficientnet: Rethinking model scaling for convolutional neural networks, in International conference on machine learning, 2019, pp. 61056114. [50] S. Kaymak and A. Serener, Automated age-related macular degeneration and diabetic macular edema detection on oct images using deep learning, in 2018 IEEE 14th international conference on intelligent computer communication and processing (ICCP), 2018, pp. 265269. [51] A. Thomas, P. M. Harikrishnan, A. K. Krishna, P. Palanisamy, and V. P. Gopi, novel multiscale convolutional neural network based agerelated macular degeneration detection using OCT images, Biomedical Signal Processing and Control, vol. 67, p. 102538, 2021. [52] S. Zheng and Y. Wang, SF Net: Pyramid-Based Feature Fusion Convolutional Neural Network With Embedded Squeeze-andExcitation Mechanism for Retinal OCT Image Classification, International Journal of Imaging Systems and Technology, vol. 35, no. 5, p. e70197, 2025. [53] T. Hassan, M. U. Akram, N. Werghi, and M. N. Nazir, RAG-FW: hybrid convolutional framework for the automated extraction of retinal lesions and lesion-influenced grading of human retinal pathology, IEEE journal of biomedical and health informatics, vol. 25, no. 1, pp. 108120, 2020. [54] L. Fang, Y. Jin, L. Huang, S. Guo, G. Zhao, and X. Chen, Iterative fusion convolutional neural networks for classification of optical coherence tomography images, Journal of Visual Communication and Image Representation, vol. 59, pp. 327333, 2019."
        }
    ],
    "affiliations": [
        "Artificial Intelligence Department, University of Isfahan",
        "Department of Mechanical Engineering, Isfahan University of Technology",
        "Labbafinejad Hospital, Shahid Beheshti University of Medical Science"
    ]
}