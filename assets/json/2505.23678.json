{
    "paper_title": "Grounded Reinforcement Learning for Visual Reasoning",
    "authors": [
        "Gabriel Sarch",
        "Snigdha Saha",
        "Naitik Khandelwal",
        "Ayush Jain",
        "Michael J. Tarr",
        "Aviral Kumar",
        "Katerina Fragkiadaki"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK for spatial reasoning, V*bench for visual search, and ScreenSpot and VisualWebArena for web-based grounding--ViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRL's performance on localizing small GUI elements and visual search, achieving 86.4% on V*Bench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the model's visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is a strong paradigm for imbuing models with general-purpose visual reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 8 7 6 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Grounded Reinforcement Learning\nfor Visual Reasoning",
            "content": "Gabriel Sarch Snigdha Saha Naitik Khandelwal Ayush Jain Michael J. Tarr Aviral Kumar Katerina Fragkiadaki"
        },
        {
            "title": "Carnegie Mellon University",
            "content": "visually-grounded-rl.github.io Figure 1: Grounded visual reasoning enables interpretable and accurate answers. ViGoRL decomposes the task into sequence of natural language thoughts anchored in image regions. In contrast, Vanilla GRPO and SFT baselines produce ungrounded and incorrect responses."
        },
        {
            "title": "Abstract",
            "content": "While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across diverse set of visual reasoning benchmarksincluding SAT-2 and BLINK for spatial reasoning, Vbench for visual search, and ScreenSpot and VisualWebArena for web-based groundingViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRLs performance on localizing small GUI elements and visual search, achieving 86.4% on VBench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the models visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is strong paradigm for imbuing models with general-purpose visual reasoning."
        },
        {
            "title": "Introduction",
            "content": "Visual reasoning tasks vary widely in structure and often demand different solution strategies depending on the problem at hand. Some tasks are dominated by salient visual cues, such as recognizing refrigerator centered in kitchen scene, while others, like locating pair of scissors in cluttered environment, require sequential visual search and selective attention. Despite this diversity, most state-of-the-art vision-language models (VLMs) operate in an end-to-end fashion, predicting answers directly in single forward pass. These models often lack the ability to adapt their computational strategies to different tasks or to expose intermediate reasoning beyond visual attention maps. Promptbased models, such as ViperGPT [54], VisualProg [19], and V* [67], explicitly decompose visual tasks into sequences of subgoals or intermediate steps to improve interpretability and performance without additional training. However, such models typically generate fixed reasoning chains that do not adapt to the structure of the input scene. Recent advances in reinforcement learning (RL) over reasoning chains have significantly enhanced the capabilities of LLMs in text-based domains [17, 58, 26], enabling them to learn diverse reasoning strategies tailored to the context. However, RL can only build upon skills or compose reasoning behaviors that are already latent in the base models sampling distribution [13, 82]. For e.g., Gandhi et al. [13] has identified key cognitive behaviors in text-based domains, such as setting sub-goals, backtracking, verification, that support self-improvement under RL. Models lacking these behaviors often do not benefit from RL and must be bootstrapped via supervised fine-tuning (SFT) on curated reasoning traces before RL is run [13, 82]. However, it remains unclear whether the cognitive behaviors identified in text-based domains similarly support generalization in visual reasoning tasks. Several recent works have attempted RL fine-tuning directly on base vision-language models (VLMs) [38, 37, 87, 55, 57, 84, 51, 41, 32], implicitly assuming RL alone can induce useful cognitive behaviors. However, our analysis reveals that such naïve applications of RL typically yield abstract, ungrounded reasoning rather than richer, visually grounded cognitive behaviors (see Section 3.1, 5.3). These findings align with prior research showing that explicitly prompting VLMs to reference spatial object locations improves performance and interpretability [67, 73, 14], suggesting that grounding thought in spatial regions may serve as key cognitive behavior for effective visual reasoning. Thus, critical open question arises: How can we embed useful cognitive behaviors in VLMs before applying RL to achieve robust visual reasoning? We hypothesize that models both see better and think better when their textual reasoning steps are explicitly grounded in specific image regions, promoting more targeted and systematic crossreferencing between textual and visual information during reasoning. This hypothesis is inspired by the fact that humans systematically shift their restricted gaze to selectively gather and integrate task-relevant information when reasoning about the world [75, 22, 74]. Grounding may serve similar role in models, functioning as spatial attention mechanism that enables accurate feature binding [61] and supports deictic reference [4] to simplify multi-step reasoning through localized perceptual anchoring. We move beyond prompt-based reasoning, proposing that learning to compose reasoning steps explicitly anchored in image coordinates induces structured region-level behaviors that support improved generalization in visual tasks. Our Approach. We introduce multi-turn RL framework for training VLMs to reason in grounded, visually-aware manner. This stands in contrast to LLM reasoning in math or code, where grounding in external input is not strictly required. Within each reasoning step, the model produces natural language thought along with corresponding spatial grounding (i.e., an (x, y) location in the image). This enables it to progressively refine its attention and gather task-relevant visual information as reasoning unfolds. By incorporating multi-turn interaction into the RL processwhere each turn consists of one or more reasoning steps followed by query to visual feedback toolthe model learns to iteratively request zoomed-in views of selected regions when fine-grained visual information is required. Critically, no external supervision or explicit human-provided grounding cues are used to supervise the spatial grounding of the thought; instead, the model autonomously learns to propose and utilize spatial grounding as an internal cognitive tool. Current methods for training VLMs to directly produce textual answers from visual inputs inherently bias them toward abstract, ungrounded reasoning, making it fundamentally difficult for RL methods alone to spontaneously discover systematic visual strategies at the region-level. To explicitly inject grounded reasoning behaviors before RL training, we employ Monte Carlo Tree Search (MCTS) to systematically stitch together independently sampled reasoning steps, generating diverse, visually2 grounded reasoning trajectories. We bootstrap the model via supervised fine-tuning (SFT) on these MCTS-constructed paths, thus embedding rich region-level reasoning strategies into the model. We then apply RL, through Group Relative Policy Optimization (GRPO) [50], to further reinforce grounded sequences that lead to correct answers. Finally, we introduce novel multi-turn RL formulation with visual feedback loops, allowing the model to dynamically zoom into image regions via tool calling for more detailed visual inspection when needed. This multi-turn variant of our method improves the models capacity to localize and reason about fine-grained visual elements. Empirical Results. We evaluate ViGoRL across suite of visual reasoning benchmarks, including SAT-2 [48], BLINK [11], RoboSpatial [53], ScreenSpot [6, 31], VisualWebArena [29], and VBench [67]. Our approach consistently outperforms existing methods on all tasks. Specifically, ViGoRL achieves substantial improvements over vanilla GRPO, with accuracy gains of 12.9 points on SAT-2 and 2.0 points on BLINK. In fine-grained web grounding scenarios, our method surpasses both vanilla GRPO and large-scale web-finetuned models on ScreenSpot-Pro. By leveraging multi-turn RL for dynamic, zoomed-in visual feedback, ViGoRL further improves performance on ScreenSpotPro, effectively localizing small elements within high-resolution images. Moreover, multi-turn RL significantly enhances visual search capabilities, allowing ViGoRL to outperform both VLM tool-use pipelines and proprietary VLMs on VBench, achieving an accuracy of 86.4%. On VisualWebArena, benchmark requiring live web interaction from image inputs alone, without access to HTML, ViGoRL outperforms both direct SFT and vanilla GRPO, and surpasses the previous state-of-the-art for this model size, ICAL [49], despite using only visual input. Ablation studies confirm the importance of grounding: models trained without spatial anchoring perform significantly worse. Further, we find that grounding amplifies other visual cognitive behaviors such as region exploration, goal setting, and visual verification. Human evaluations show that our models reasoning is both spatially accurate and helpful to understanding the models reasoning steps. Our results point to visually grounded RL as strong paradigm for general-purpose visual reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Programmatic Reasoning in VLMs. Vision-Language Models (VLMs) [46, 27, 2, 5, 63, 56] excel on multimodal tasks through large-scale pretraining, but struggle with complex reasoning such as counting [47], spatial reasoning [48], and compositional understanding [60]. Prompting strategies like chain-of-thought (CoT) [65, 30], mm-CoT [86], IoT prompting [91], and Minds Eye [69] guide models to generate explicit reasoning steps grounded in images. Methods such as V* [67], Sketchpad [24], VisProg [19], REFOCUS [12], and ViperGPT [54] use language models to produce executable visual plans but rely on frozen backbones and hand-crafted prompts. Distillation and Supervised Fine-Tuning. To robustly instill reasoning skills, supervised fine-tuning (SFT) methods train on curated reasoning trajectories. For text tasks, STaR [83] generates reasoning via few-shot prompting and selects based on correctness, while S1 [40] distills reasoning chains into smaller models. Similar approaches in VLMs include LLAVA-CoT [72], which distills CoT reasoning from GPT-4o [1], and ICAL [49], VPD [25], and Mulberry [77], which employ LLMs or MCTS to generate reasoning data. Methods like VOCOT [33] improve grounding of entities via SFT. However, distillation methods rely solely on positive examples, neglecting failed paths. RL addresses this by learning from both successes and failures, outperforming SFT alone in our experiments. Reinforcement Learning on Chains of Thought. Applying RL to chains of thought has improved reasoning in verifiable domains like math and coding [26, 17, 58]. Early methods [85, 43, 10, 68, 16] iteratively refine reasoning using preference-based approaches, while recent efforts like DeepSeekR1 [17] and Kimi-1.5 [58] leverage online RL with outcome-based rewards. While initially believed to induce novel cognitive behaviors, analyses [13, 82, 36, 79] suggest RL primarily amplifies existing capabilities already found in the base model. Most prior work focuses on text-only domains, leaving visual reasoning behaviors largely unexplored. Visual-RFT [37] applies RL to textual reasoning in VLMs without incentivizing new visual behaviors. In contrast, we explicitly ground reasoning steps visually, amplifying exploration, verification, and backtracking via learned visual interactions."
        },
        {
            "title": "3 Preliminaries",
            "content": "Reasoning in Vision-Language Models. The overarching objective of our work is to improve the reasoning capabilities of vision-language models (VLMs). We consider reasoning tasks defined by dataset of problem instances (I, q, a), where is visual input (e.g., an image), is natural language query about the image, and is the correct, verifiable answer (e.g., class label, bounding box, or discrete actions such as click [element]). The goal is to train vision-language policy πθ parameterized by θ that outputs reasoning trace τ consisting of sequential textual reasoning steps s1, s2, . . . , sT culminating in an answer a. This policy factorizes autoregressively: (cid:0)τ I, q(cid:1) = πθ (cid:16) (cid:89) t=1 (cid:0)st I, q, s<t πθ (cid:1)(cid:17) πθ (cid:0)a I, q, sT (cid:1). While supervised fine-tuning can teach models to mimic reasoning chains provided in training data, RL offers the potential to directly reinforce reasoning behavior sampled from the base model based on correctness or other reward signals [42, 50, 13]. Specifically, RL allows us to optimize policies over reasoning traces τ that maximize expected returns based on task performance and adherence to desired format structure. Formally, the RL objective can be expressed as: (cid:2)Eτ πθ s.t. τ satisfies format constraints, E(I,q,a)D (cid:2) R(τ )(cid:3)(cid:3) , max θ where the reward R(τ ) typically includes correctness of the final answer, and proper adherence to structured reasoning formats. 3.1 Do Current RL Recipes Amplify VLM Behaviors That Support Visual Performance? It has been shown that RL on chains of thought alone cannot necessarily induce new behaviors from scratch; it can only amplify or chain reasoning primitives that are already present in the base models sampling distribution [13, 82]. Do current base VLMs exhibit desirable visual reasoning behaviors and can RL amplify these behaviors to improve performance? Following previous work from LLMs [13], we categorize the behaviors of base VLMs when tasked with the Spatial Aptitude Test [48] spatial reasoning benchmark, which requires the synthesis of evidence in multiple regions of one or multiple images to answer. Our complete analysis and experimental protocol is detailed in Section 5.3 and Appendix A6. Our analysis reveals two key insights: Takeaway 1: Current VLMs often fail to reference fine-grained image inputs; their reasoning is largely ungrounded. Without explicit grounding, models treat vision as static context rather than actively referenced input, spending tokens on abstract thought instead of analyzing visual regions. Qwen2.5-VL-3B examines only 1.44 regions per task with minimal visual verification (0.14) and no backtracking. We provide an examples of such an output in Figure 2. Takeaway 2: Standard RL optimization exacerbates ungrounded reasoning. RL-tuning with tasklevel rewards slightly increases region exploration (1.8) but eliminates visual subgoal setting (0.00) and still shows no backtracking (0.00). Figure 2 and Appendix A9.1 illustrates typical outputs under this setup. This demonstrates that optimizing for correctness without encouraging grounded reasoning fails to instill visual reasoning skills. Furthermore, prompt engineering for broader visual behaviors is ignored during RL training (Section 5.2, Table 4). Figure 2: Without actively reinforcing visually grounded behaviors, RL collapses onto shortcuts that maximize immediate rewards at the expense of richer visual reasoning. Standard CoT and Vanilla GRPO (left and center) exhibit visually ungrounded reasoning, relying on vague references to scene elements (shown in yellow), which often results in incorrect answers (marked in red). In contrast, Visually Grounded RL (right) explicitly references object positions, demonstrating precise spatial grounding (shown in blue) and more often producing correct reasoning outcomes (marked in green). See Section 5.3 for further analysis. 4 Why does standard RL fail here? We hypothesize that the failure mode of RL comes down two interrelated issues: (1) the initial sampling distribution of pretrained VLMs heavily biases the model toward abstract, language-based strategies rather than region-level analysis (as shown in Takeaway 1 above), (2) since standard RL provides rewards based solely on final correctness and general formatting, it amplifies behaviors that attain high rewards irrespective of how they actually maximize reward. Without actively biasing the initial policy and reinforcing visually grounded behaviors, RL naturally collapses onto shortcuts that maximize immediate rewards at the expense of richer visual reasoning. Thus, we hypothesize that encouraging explicit grounding biases the models reasoning distributions toward exploring relevant visual regions, setting meaningful visual subgoals, verifying visual hypotheses, and effectively backtracking."
        },
        {
            "title": "4 Visually Grounded Reinforcement Learning (ViGoRL)",
            "content": "The analysis in Section 3.1 shows that naïve RL on small VLMs may degrade into ungrounded reasoning strategies, and that incentivizing grounding may improve visual reasoning behaviors that improve generalization. To incorporate explicit visual grounding into the reasoning process, we redefine each reasoning step as tuple: nt = st, (xt, yt), where st is textual thought and (xt, yt) anchors it to specific image location. The full trajectory becomes τ = [n1, . . . , nT , a]. This modifies the original factorization from Section 3: πθ(τ I, q) = πθ(nt I, q, n<t) πθ(a I, q, nT ), (cid:32) (cid:89) (cid:33) t=1 where each nt now includes both the reasoning step and its visual grounding. By introducing this grounding constraint, we explicitly guide the model to systematically reference specific image locations as evidence for its reasoning. This grounding incentivizes the model to iteratively explore and verify distinct visual regions, formulate and ground intermediate subgoals visually, and revisit prior regions when uncertainty or errors arise. 4.1 Our Approach for Grounded Reinforcement Learning Building on our findings in Section 3.1, we introduce comprehensive approach that directly addresses the ungrounded reasoning patterns in VLMs. We propose two-stage pipeline to incorporate grounded reasoning as detailed in the previous section: (1) warm-start supervised finetuning that biases the model toward generating structured reasoning chains with explicit spatial grounding, followed by (2) reinforcement learning that systematically refines these grounded behaviors. Finally, we extend our approach to multi-turn RL (Section 4.3.1), enabling fine-grained visual feedback at each reasoning step. This pipeline yields models that invest test-time compute into examining diverse image regionsprecisely the behaviors our analysis showed were absent in standard VLM reasoning. Figure 3: Overview of the ViGoRL approach. (Left) We use MCTS with teacher model to generate reasoning chains grounded in specific image regions. (Middle) These reasoning trees are linearized and used for supervised fine-tuning (SFT) to train base model. (Right) We apply GRPO with an outcome-based reward to further refine the grounded reasoning. 4.2 Warm-Start Data Generation via MCTS MCTS with Visual Grounding. We employ MCTS to generate grounded reasoning traces, where each node is reasoning step nt = st, (xt, yt), anchoring thought st to image coordinates (xt, yt) (Figure 3, left panel). At each iteration: (1) Selection: nodes are traversed using UCB, prioritizing high-value, under-explored paths; (2) Expansion: the VLM samples new grounded steps st, (xt, yt) for unexplored nodes, each referencing distinct image region; (3) Simulation: rollouts are performed by recursively generating visually grounded steps until terminal answer is produced; (4) Backpropagation: judge scores terminal nodes, with rewards propagated up the path. This process ensures efficient exploration of promising image regions and reasoning steps. We provide additional details on our MCTS procedure in Appendix A5.5. Why MCTS? Even large models (Qwen2.5-VL-72B) deployed with standard prompting explore only 23 regions, set few subgoals, and never backtrack  (Table 5)  . Purely human-curated traces are costly to collect at scale, while linear rollouts cannot enforce the iterative exploration and corrective loops we desire. Empirically, distillation from such linear rollouts without MCTS leads to degraded generalization, performing worse on out-of-distribution spatial tasks after GRPO training  (Table 4)  . In contrast, MCTS lets us systematically search the space of grounded reasoning steps, trading off exploration breadth and depth, and cheaply generate thousands of richly annotated paths that exhibit behaviors like wide exploration, through early branching to cover diverse image regions, and backtracking, by abandoning failing branches and revisiting alternatives. Moreover, since our reasoning steps are already defined as (st, (xt, yt)) tuples, they map naturally onto MCTS nodes and transitions, making it both conceptually clean and computationally efficient. Teacher-guided Search. We employ frozen, high-capacity teacher (Qwen2.5-VL-72B) to expand each node in the MCTS tree. At node n, we prompt the teacher to either (a) generate new grounded reasoning step sn with coordinate, or (b) emit candidate answer. We score leaf answers by correctness and backpropagate to guide tree expansion. From 1,500 prompts we derive 30k high-quality reasoning tracesa dataset orders of magnitude smaller than typical SFT corpora, but densely packed with exploration, verification, and backtracking behaviors. Linearization for SFT. We then linearize selected root-to-leaf paths into two types of training examples: 1) Direct chains: successful trajectories leading to the correct answer with no detours, and 2) Corrected chains: trajectories where an initial rollout fails, triggers wait, that seems off backtrack, and proceeds to the correct solution (Figure 3, left panel). We denote the VLM finetuned on these MCTS chains as πθ0 (Figure 3, middle). After fine-tuning, we commonly observe reasoning chains with dense visual subgoal setting, visual verification, broad region exploration, and visual backtracking. representative trace can be seen in Appendix A9.2. 4.3 Reinforcement Learning with Spatially-Grounded Reasoning Steps Although πθ0 imitates high-quality traces, it does not reason optimally for new queries. We therefore apply Group Relative Policy Optimization (GRPO; 50) to directly maximize task reward while preserving fluency and grounding (Figure 3, right). More details can be found in Appendix A5.1. Reward Design. We wish to incentivize reasoning traces that include explicit grounding coordinates, and reasoning traces that lead to the final answer. Our total reward is weighted sum: R(T ) = λfmtrfmt + λtaskrtask, where rfmt encourages valid and interpretable output format, and rtask captures task-specific correctness. Importantly, along with checking outputs are formatted with correct <think> and <answer> tags, we award +1 rfmt only if all coordinate references are valid. We provide more details on task reward in Appendix A5.2. 4.3.1 Multi-Turn Reinforcement Learning for Visual Feedback Grounded reasoning chains generated by our MCTS procedure already push the model to look at many regions, but the visual encoder still processes the same globally resized image at every step. Fine-grained cues (small text, icons, object boundaries) are therefore blurred away, potentially limiting the benefit of additional reasoning if image region details cannot be percieved by the model. Inspired by how humans zoom in after selecting candidate region, we let the model request higher-resolution crop, ot, after predicting coordinate. This interactive microscope supplies fresh evidence at detail level impossible to encode in the initial global view. Multi-turn warm start: from single-turn chains to dialogs. To prepare the model for multi-turn rollouts, we convert single-step MCTS-derived reasoning traces from Section 4.2 into dialog-style traces. Given linearized MCTS trace τ = [(s1, p1), . . . , (sT , pT ), a], we convert it into dialog: 1. At turn t, the model generates textual thought st+1 (tagged <think> . . . </think>). 2. The model then emits <tool_call> {name: crop, arguments: {coordinate: pt}} </tool_call>, or the answer (tagged <answer> . . . </answer>), then the round terminates. 6 3. If function call, the environment responds with ot, with tag <observation> containing w crop centered at pt, resized to pixels, followed by </observation> and the loop repeats. We fine-tune the base model on these multi-turn traces to initialize it for multi-step GRPO with visual feedback. We then apply GRPO, allowing the model to roll out full multi-turn dialogs, with crop feedback upon tool call outputs. Reward Design for Multi-Turn RL. Multi-turn settings introduce new failure modes: repeating coordinates, skipping tool use, or violating dialog structure. To mitigate these, we define composite format reward: (1) Grammar reward rgrammar: 1 if the dialog obeys strict tag automaton: <think> </think> <tool_call> </tool_call> <observation> </observation> repeat or <answer> </answer>. The dialog must end with complete </answer> and contain no malformed or out-of-order tags. (2) Diversity bonus rdiv: +0.2 for each sufficiently distinct coordinate in tool calls ( 10px from all previous), up to 4 times. Finally, this leads to the overall format reward: rfmt = rgrammar + rdiv. Additional multi-turn RL details can be found in Appendix A5.3."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate our model, ViGoRL, on spatial reasoning and web grounding tasks, comparing against baseline and ablation variants to understand the contributions of grounding, MCTS-based warm-start supervision, and GRPO. Our results demonstrate significant gains in visual reasoning and web-based tasks when models are trained with our grounded reasoning training recipe. We investigate the following research questions: 1 RQ1: How much does grounded reasoning help when evaluated on visual reasoning tasks? 2 RQ2: How important is each component of ViGoRL? 3 RQ3: What visual reasoning behaviors are amplified by grounded reasoning? 4 RQ4: Is the grounded reasoning accurate and interpretable? Training Datasets. For spatial reasoning, we use SAT-2 [48], sampling 32k training and 1k validation examples. The model is tasked with selecting the correct textual option, with randomized answer order to reduce position bias. For web grounding, we draw 12k screenshot, referring expression, box examples from OS-ATLAS [70] (4k each from mobile, web, and desktop), plus 1.5k warm start and 1.5k validation samples evenly split by domain. For web action prediction, we use ICAL [49], dataset of 92 web navigation trajectories. We remove chain-of-thought and textual set-of-marks annotations to focus on visual grounding, training on image, instruction, action history to predict the correct next action at each step. For visual search, we curate 11k question-answer pairs over small objects from Segment Anything [28], using GPT-4o given an object mask and scripted filtering to generate fine-grained image, question, choices, answer tuples. Each question targets uniquely identifiable small object (<0.1% of image) and tests visual discrimination (e.g., color, material) and relation questions between object pairs. 5.1 RQ1: How much does grounded reasoning help on visual reasoning tasks? Baselines. We compare against the following baselines: 1. Method Comparison: We compare ViGoRL to baselines utilizing the same training data and base model (Qwen-2.5-VL) as used in our method, but differing in their training recipe: 1) SFT-direct: Supervised fine-tuning on our trajectory dataset using final answers only. 2) Vanilla GRPO: GRPO applied to the base model with standard rewards for <think> and <answer> formatting, and answer correctness, similar to previous work [37, 38]. 2. General Proprietary and Open-Source Models: General-purpose vision-language models with closed-weights accessible through APIs [1] and open-weight models [35, 7, 8, 63, 3, 59]. 3. VLM Tool-Using Pipelines: We compare against prompt-based models that explicitly decompose visual tasks into sequences of subgoals or intermediate steps [67, 19, 23, 88, 76, 66]. 4. Web-Grounding Models: Models trained to specialize in web grounding tasks, through large-scale supervised finetuning and instruction tuning on curated web data [6, 21, 70, 34, 15, 45], reinforcement learning of chain of thought with outcome reward [39], or human-in-the-loop action and chain of thought annotations collected from live webpages [49]. 7 5.1.1 Visual Reasoning Evaluations. Spatial Reasoning. We evaluate on SAT-2 [48] validation (4,000 questions across 5 categories), BLINK [11] (depth ordering, multi-view reasoning, spatial relationships), and ROBOSPATIALconfiguration and compatibility split (228 questions on real-world RGBD scenes) [53]. These benchmarks test generalization to novel environments, objects, and language configurations. Accuracy is measured via multiple-choice answer matching. GUI Understanding. For grounding evaluation, we use ScreenSpot v2 [70, 6] (single-step localization), and ScreenSpot Pro [31] (high-resolution professional environments). To test small element grounding in low resolution, we additionally evaluate on ScreenSpot Pro with images downsampled to lower resolution of 1920x1920, which we call ScreenSpot-Pro-LR (results in Appendix Table A6). Performance is measured by checking if predicted coordinates fall within target element bounding boxes. We also test on VisualWebArena [29] live web evaluation, using the visual-only configuration where models receive set-of-marks annotated webpage screenshots without HTML or text inputs. Task success is determined automatically by checking for specific criteria in the agent trajectory (see Koh et al. [29] for details). Visual Search. The visual search benchmark VBench tests fine-grained visual understanding using 191 high-resolution images from the SA-1B dataset (avg. 22461582). It includes 115 attribute recognition samples (e.g., color, material) and 76 spatial relationship samples, evaluating models ability to analyze detailed object properties and relative positions in complex scenes. 5.1.2 Results Table 1: Performance (mean 95% CI) on ScreenSpot and VisualWebArena. Multi-t = multiturn RL with visual feedback. Model ScreenSpot -V2 ScreenSpot -Pro VWA (Vision Only) Proprietary Models GPT-4o Claude Comp. Use Open-source Models Qwen2-VL-7B Kimi-VL-16B-MoE Web Grounding Models SeeClick CogAgent-18B OS-Atlas-4B OS-Atlas-7B ShowUI-2B UGround-7B UGround-V1-7B UI-TARS-2B UI-R1-3B ICAL-7B 18.1 - - 92.8 55.1 - 71.9 84.1 - - 84.7 0.8 17. 1.6 34.5 1.1 7.7 3.7 18.9 7.7 16.5 31.1 27.7 17.8 - Method Comparison 9.3% (1.4) 68.4% (2.6) Qwen2.5-VL-3B 25.0% (2.1) 80.6% (2.2) + SFT direct 84.4% (2.0) 29.0% (2.2) + Vanilla GRPO ViGoRL-3b (Ours) 86.5% (1.9) 31.1% (2.3) ViGoRL-3b (Multi-t) 86.1% (1.9) 32.3% (2.4) 19.8 (w/ text) 2.9 (w/ text) 8.2 (w/ text) 4.2% (1.3) 4.5% (1.4) 4.8% (1.4) 6.4% (1.5) - Qwen2.5-VL-7B ViGoRL-7b (Ours) 73.6% (2.4) 14.8% (1.8) 91.0% (1.6) 33.1% (2.3) 5.5% (1.5) 11.2% (2.1) Table 2: Accuracy (mean 95% CI) for spatial reasoning. Model SAT-2 Val BLINK RoboSpatial Proprietary Models GPT-4o GPT-4 Turbo Claude 3 Opus Gemini Pro 1.0 General Open-source Models LLaVA v1.6 34B instructBLIP 13B Molmo 60.0% 54.6% 44.1% 45.2% 46.8% 42.2% - 76.2% 67.1% Method Comparison Qwen2.5VL-3B + SFT direct + Vanilla GRPO 54.4% (6.5) 62.3% (6.3) 69.7% (6.0) ViGoRL-3b (Ours) 62.9% (1.5) 48.5% (2.3) 67.1% (6.1) 46.1% (1.5) 58.3% (1.5) 50.0% (1.6) 44.4% (2.3) 46.4% (2.3) 46.5% (2.3) Qwen2.5-VL-7B 46.7% (9.5) ViGoRL-7b (Ours) 67.5% (1.5) 54.1% (2.3) 76.4% (7.5) 52.9% (2.3) 52.6% (1.6) Grounded reasoning improves spatial accuracy. As shown in Table 2, on variety of spatial understanding benchmarks, ViGoRL significantly outperforms baselines without grounded reasoning. On SAT-2 test set, ViGoRL-3B achieves 62.9% accuracyan improvement of +16.8 points over the base model and +12.9 points over Vanilla GRPO. Similar trends hold on the out of distribution benchmarks of RoboSpatial (67.1%) and BLINK (48.5%), with ViGoRL-3B outperforming SFT Direct by 2.3% and 4.8% on BLINK and RoboSpatial, respectively, and vanilla GRPO by 2.0% on BLINK. At the 7B scale, ViGoRL-7B further improves SAT-2 accuracy to 67.5%, demonstrating the methods scalability and effectiveness across model sizes. We observe the same trend in BLINK and RoboSpatial, with improved accuracies of 54.1% and 76.4%, respectively. Importantly, these gains 8 come without sacrificing general visual-language capabilities, as our method maintains performance on standard out-of-distribution VLM benchmarks (Table A7). Grounded reasoning helps localize complex web elements. We evaluate performance on visiononly web interfaces  (Table 1)  , where models must resolve ambiguous UI instructions via grounded image understanding. ViGoRL-3B outperforms both the base model, direct answer SFT, and vanilla GRPO across all tasks. For instance, on ScreenSpot-Pro, requiring grounding of small elements in high resolution webpage images, accuracy improves 21.8 points over the base model, 6.1 points over SFT direct, and 2.1 points over vanilla GRPO. ViGoRL-7B achieves 91.0% on ScreenSpot-V2 and 33.1% on ScreenSpot-Pro, outperforming open-source VLMs of comparable size, including OS-Atlas and UGround variants, which finetune on order of magnitudes more GUI grounding examples (up to 13M training samples). ViGoRL improves accuracy on live visual web evaluation. On VisualWebArena, which requires live interaction with web pages using only set-of-marks image inputs (no access to HTML or underlying text), ViGoRL outperforms direct SFT and vanilla GRPO. Despite relying solely on images, ViGoRL surpasses the previous state-of-the-art for the same model size, ICAL [49], by 3.0%even though ICAL has access to textual set-of-marks inputs derived from HTML. Multi-turn RL with visual feedback improves visual search and small element detection. On VBench, ViGoRL-7B significantly outperforms both proprietary models and open-source tool-using pipelines. As shown in Table 3, our method reaches 86.4%, surpassing proprietary VLMs like GPT-4o (66.0%) and Gemini-Pro (48.2%), and even advanced tool-using pipelines like VisProg (41.4%) Sketchpad-GPT-4o (80.3%). Table 3: Accuracy (mean 95% CI) for visual search on VBench. Model Name Proprietary Models Gemini-Pro GPT-4V GPT-4o VLM Tool-Using Pipelines VisProg VisualChatGPT MM-React Sketchpad-GPT-4o IVM-Enhanced GPT-4V VBench 48.2 55.0 66.0 41.4 37.6 41.4 80.3 81. Open-source Models LLaVA-1.5-7B LLaVA-1.6-13B SEAL 48.7 61.8 74.8 Our model with multi-turn RL that incorporates zoomed-in visual feedback additionally shows significant improvements in small element grounding tasks. It achieves 32.3% accuracy on ScreenSpotPro, 1.2 percentage point improvement over our best non-visual feedback model variant (31.1%)  (Table 1)  . In low-resolution environments, the zooming capability delivers even more substantial relative gains, outperforming our best non-visual feedback method by 2.4% (Table A6). These gains illustrate that our approach enables more robust and precise visual reasoning through structured visual grounding and dynamic visual feedback interactions. Qwen2.5-3B-VL 74.2 (6.2) Multi-t ViGoRL-3B (Ours) 81.2 (5.6) Qwen2.5-7B-VL 78.0 (5.9) Multi-t ViGoRL-7B (Ours) 86.4 (5.0) 5.2 RQ2: Ablation Studies Table 4: Ablation results on SAT-2 Val and BLINK. Top-1 accuracy (95% CI) with relative change. Model never produced correct formatting. Gnd = Explicit grounding in the reasoning steps. SFT = SFT Direct pretraining. Distill = warm-start teacher distillation. SATGRPO MCTS Gnd Distill Variant BLINK Full (ours) GRPO MCTS Grounded Distill w/o MCTS + SFT direct pre-train 62.93 (1.50) 58.83 (1.53) (4.10%) N/A 58.69 (1.53) (4.28%) 63.28 (1.49) (+0.35%) 63.26 (1.49) (+0.33%) 48.50 (2.33) 44.97 (2.32) (3.53%) N/A 45.44 (2.32) (3.06%) 46.18 (2.32) (2.32%) 48.56 (2.33) (+0.06%) To understand which components drive performance, we conduct targeted ablations  (Table 4)  : 1. GRPO remains important. Removing GRPO reduces performance by 4.1 points on SAT-2 and 3.5 points on BLINK, confirming the importance of RL refinement. 2. MCTS-generated warm-start is essential. Without structured traces from MCTS, the model fails to emit valid outputs, underscoring the need for scaffolded learning signals. 3. Explicit grounding helps. Running our same method without introducing explicit grounding in the warm start data resulted in 3.4 point drop on BLINK, even with the same MCTS warm start and GRPO recipe. 9 4. Teacher distillation without MCTS preserves in-distribution performance but degrades outof-distribution generalization. When using warm-start data from successful teacher linear rollouts (which contain grounded reasoning steps but no search), SAT-2 validation accuracy remains nearly unchanged (63.26% vs. 62.93%). However, performance on the more challenging out-of-distribution BLINK benchmark drops significantly by 2.32%. 5. SFT direct pretraining adds little. Adding an additional SFT stage to directly predict the answer before warm start training and GRPO yields marginal gains (<1 point), reinforcing the idea that grounded thought processes drive improvement. Model Table 5: Average visual behaviors per example. Regions Grounded Visually Subgoals Explored 5.3 RQ3: What visual reasoning behaviors are amplified by grounded reasoning? Following previous work on behavioral coding in language models [13], we code VLM behavior when tasked with the SAT-2 spatial reasoning benchmark, which requires synthesizing evidence across multiple image regions, in two scenarios: (1) Zero-shot prompting with \"think stepby-step\" instructions without explicit grounding, and (2) RL-tuned models including vanillo GRPO, ViGoRL, and an ablated version without explicit grounding incentives. We quantify reasoning behaviors on 300 representative SAT-2 samples using GPT-4o evaluation, measuring visual regions explored, subgoal setting, verification, and backtracking. Additional study details can be found in Appendix A6. Qwen2.5-VL-72B (Zero-Shot) Standard CoT Qwen2.5-VL-3B (Zero-Shot) Standard CoT Qwen2.5-VL-3B (RL-tuned) Vanilla GRPO Ours w/o grounding Verify Backtrack Acc. 0.00 1.1 0.02 0.48 0.64 0. 0.00 0.47 0.02 0.17 0.39 0.27 1.8 3.5 1.7 0.27 0.07 0. 0.27 0.00 0.65 0.38 0.00 1. 2.3 Findings. As shown in Table 5, explicit grounding substantially amplifies visual reasoning behaviors. ViGoRL explores more visual regions (3.5 vs. 1.44/1.8 in zero-shot/vanilla GRPO) and demonstrates dramatic increases in grounded subgoals (1.1 vs. 0.07, 15 higher) and visual verification (0.39 vs. 0.14, 3 higher). It uniquely develops visual backtracking behavior (0.47) absent in all baselines. These enhanced behaviors enable our 3B parameter model to achieve accuracy (0.64) comparable to the 72B model (0.65). The ablation study confirms these benefits stem specifically from explicit grounding incentives, as removing them causes substantial regression in all measured behaviors, particularly regions explored (1.7 vs. 3.5) and grounded subgoals (0.02 vs. 1.1). 5.4 RQ4: Human Evaluation Shows Accuracy and Helpfulness of Grounded Reasoning To assess our models grounded reasoning traces, we conducted human study evaluating whether predicted coordinates (1) correctly referred to the intended image region, and (2) helped participants understand the associated reasoning step (details are shown in Appendix A7). Findings. As shown in Figure 4, 72.8% of predictions were judged as accurately referring to the described region (95% CI: [66.8, 78.7], N=20). On 5-point Likert scale, participants rated the helpfulness of the highlighted region at 3.35 on average (95% CI: [3.03, 3.68], N=10). Helpfulness increased substantially when the prediction was correct (3.81; 95% CI: [3.51, 4.10]) and dropped when incorrect (2.26; 95% CI: [1.57, 2.94]). Figure 4: Human evaluation of grounded reasoning. Participants judged the grounded predictions as both accurate and helpful when correct. These findings indicate that accurate spatial grounding meaningfully improves human interpretability and usefulness of the models reasoning process, indicating that improvements in accuracy of reasoning step grounding can also improve human interpretability."
        },
        {
            "title": "6 Conclusion and Discussion\nWhy does visual grounding help? Our findings suggest that spatially anchoring each reasoning step\nforces the model to engage in a more structured, human-aligned form of cognition. ViGoRL learns\nto iteratively reference, inspect, and verify content in specific visual regions – amplifying cognitive\nbehaviors such as subgoal formulation, visual verification, and backtracking.",
            "content": "10 This model architecture mirrors insights from cognitive science: humans rely on spatial attention and visual routines to decompose complex problems into manageable, perceptually grounded steps [62, 4]. Grounding serves not merely to reduce computational load (as human spatial attention is often characterized), but to scaffold reasoning with external visual structure effectively using the content of the world as part of the thinking process [78, 20]. We observe similar benefits in models: spatial grounding enables better generalization, especially in out-of-distribution settings, and improves interpretability by making intermediate steps physically referable. Rather than treating grounding as visualization tool or auxiliary supervision signal, our results argue for it as central architectural and algorithmic principle. By training models to reason with deictic reference pointing, zooming, verifying future systems may better reflect the iterative, grounded strategies that underlie human problem-solving. This opens up promising directions for building agents that not only reason effectively, but in ways that are queryable, adaptable, and aligned with perceptual experience. Acknowledgments. This material is based upon work supported by National Science Foundation grants GRF DGE1745016 & DGE2140739 (GS), ONR award N00014-23-1-2415, AFOSR Grant FA9550-23-1-0257, and DARPA No. HR00112490375 from the U.S. DARPA Friction for Accountability in Conversational Transactions (FACT) program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Army, the National Science Foundation, or the United States Air Force. This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research."
        },
        {
            "title": "References",
            "content": "[1] Openai. gpt-4 technical report. arXiv preprint arxiv:2303.08774, 2023. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 2371623736, 2022. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Dana Ballard, Mary Hayhoe, Polly Pook, and Rajesh PN Rao. Deictic codes for the embodiment of cognition. Behavioral and Brain Sciences, 20(4):723742, 1997. [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: jointly-scaled multilingual language-image model, 2023. URL https://arxiv.org/abs/2209.06794. [6] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. URL https://arxiv.org/abs/2305. 06500. [8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and 11 pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [9] Zhengyuan Ding, Yuwei Chen, Yichong Xu, Zhe Wang, Xintao Han, Dong Yu, and Zhou Yu. Attention over learned object embeddings enables complex visual reasoning. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [10] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models, 2025. URL https://arxiv.org/abs/2411.14432. [11] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. [12] Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding, 2025. URL https://arxiv.org/abs/2501.05452. [13] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [14] Sarthak Ghosh, Ben Lee, Jean-Baptiste Alayrac, Xuhong Zhai, Christoph Feichtenhofer, Joao Carreira, and Ishan Misra. Grounded decoding with visual descriptions reduces hallucination in large vision-language models. In International Conference on Learning Representations (ICLR), 2024. [15] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=kxnoqaisCT. [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [19] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training, 2022. URL https://arxiv.org/abs/2211.11559. [20] Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3): 335346, 1990. [21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [22] David Hoppe and Constantin Rothkopf. Multi-step planning of eye movements in visual search. Scientific reports, 9(1):144, 2019. [23] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [24] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models, 2024. URL https://arxiv.org/abs/2406.09403. [25] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models, 2024. URL https://arxiv.org/abs/ 2312.03052. [26] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [27] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 49044916. PMLR, 2021. [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. [29] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [30] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205. 11916. [31] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use, 2025. URL https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf. Preprint. [32] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning, 2025. URL https://arxiv.org/abs/2504.06958. [33] Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Xuanjing Huang, and Zhongyu Wei. Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models, 2025. URL https://arxiv.org/abs/2405.16919. [34] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. [36] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https: //arxiv.org/abs/2503.20783. [37] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning, 2025. URL https://arxiv.org/ abs/2503.01785. [38] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning, 2025. URL https://arxiv.org/abs/2503.21620. 13 [39] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [40] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [41] NVIDIA, :, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, and Zhe Zhang. Cosmos-reason1: From physical common sense to embodied reasoning, 2025. URL https://arxiv.org/abs/2503.15558. [42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [43] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. URL https://arxiv.org/abs/2408.07199. [44] Jinyi Qi, Tao Zhang, Rui Chen, Xiaoxue Li, Yizhou Zhang, and Kai-Wei Chang. Cogcom: Compositional visual reasoning with chain-of-manipulations. In International Conference on Learning Representations (ICLR), 2025. [45] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [47] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind: Failing to translate detailed visual features into words, 2025. URL https://arxiv.org/abs/2407.06581. [48] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. Sat: Dynamic spatial aptitude training for multimodal language models, 2025. URL https://arxiv.org/abs/2412.07755. [49] Gabriel Herbert Sarch, Lawrence Jang, Michael Tarr, William Cohen, Kenneth Marino, and Katerina Fragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs of thought. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300. [51] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. URL https://arxiv.org/ abs/2504.07615. 14 [52] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [53] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. RoboSpatial: Teaching spatial understanding to 2D and 3D vision-language models for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. To appear. [54] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. [55] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning, 2025. URL https://arxiv.org/abs/2503.20752. [56] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, and et. Al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. [57] Kimi Team, Du, Gao, Xing, Jiang, Chen, Li, Xiao, Du, Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms, 2025. URL https://arxiv. org/abs/2501.12599. [58] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. [59] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, and et. Al. Kimi-VL technical report, 2025. URL https://arxiv.org/abs/2504.07491. [60] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality, 2022. URL https://arxiv.org/abs/2204.03162. [61] Anne M. Treisman and Garry Gelade. feature-integration theory of attention. Cognitive Psychology, 12(1):97136, 1980. [62] Shimon Ullman. Visual routines. Cognition, 18(1-3):97159, 1984. [63] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [64] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025. URL https://arxiv. org/abs/2504.20073. [65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. [66] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models, 2023. URL https://arxiv.org/abs/2303.04671. [67] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. [68] Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Thinking llms: General instruction following with thought generation, 2024. URL https: //arxiv.org/abs/2410.10630. [69] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models, 2024. URL https://arxiv.org/abs/2404.03622. [70] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [71] xAI. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v, 2024. Accessed: 2025-0521. [72] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. URL https://arxiv.org/abs/2411.10440. [73] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. [74] Scott Cheng-Hsin Yang, Mate Lengyel, and Daniel Wolpert. Active sensing in the categorization of visual patterns. Elife, 5:e12215, 2016. [75] Scott Cheng-Hsin Yang, Daniel Wolpert, and Máté Lengyel. Theoretical perspectives on active sensing. Current opinion in behavioral sciences, 11:100108, 2016. [76] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023. [77] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. URL https://arxiv.org/abs/2412.18319. [78] Alfred L. Yarbus. Eye Movements and Vision. Springer, 1967. [79] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [80] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv. org/abs/2503.14476. [81] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502. 16 [82] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [83] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465. [84] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning, 2024. URL https://arxiv.org/abs/ 2405.10292. [85] Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. Advances in Neural Information Processing Systems, 37:333356, 2024. [86] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024. URL https://arxiv.org/ abs/2302.00923. [87] Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, and Wenwu Zhu. Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning, 2025. URL https://arxiv.org/abs/2504.12680. [88] Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, and Xianyuan Zhan. Instruction-guided visual masking. arXiv preprint arXiv:2405.19783, 2024. [89] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. [90] Yaowei Zheng, Shenzhi Wang Junting Lu, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/ hiyouga/EasyR1, 2025. [91] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-ofthought prompting for visual reasoning refinement in multimodal large language models, 2024. URL https://arxiv.org/abs/2405.13872. [92] Richard Zhuang*, Trung Vu*, Alex Dimakis, and Maheswaran Sathiamoorthy. Improving multi-turn tool use with reinforcement learning. https://www.bespokelabs.ai/blog/improvingmulti-turn-tool-use-with-reinforcement-learning, 2025. Accessed: 2025-04-17. 17 A1 Appendix Overview The structure of this Appendix is as follows: Section A2 contains limitations and future work. Section A3 contains broader impacts. Section A4 contains additional discussion and connection to cognitive science. Section A5 contains additional methods details. Section A6 contains additional details on the model behavioral analysis. Section A7 contains additional details on the human evaluation. Section A8 contains additional experimental results. Section A9 contains example model outputs. A2 Limitations and Future Work While our approach demonstrates strong improvements in visual reasoning performance, several limitations remain and open avenues for future research: Intermediate Reward. Our current reinforcement learning setup provides rewards only at the final answer level. In spatial reasoning tasks, this sometimes results in reward hacking, where the model receives positive reward despite generating partially incorrect or underspecified reasoning. Although our human evaluation confirms that the majority of grounding predictions are accurate and helpful, there remains measurable room for improvement. We hypothesize that introducing dense intermediate rewardsfor both correct reasoning steps and accurate groundingcould mitigate these issues and improve alignment between grounding, reasoning, and reward. We leave this exploration to future work. Expanded Tool Use and Adaptive Control. While our model can point, crop, and zoom into visual regions, other forms of tool-augmented perception (e.g., highlighting, region comparison, or 3D navigation) may further support compositional reasoning. Future work could explore mechanisms that learn to balance visual actions dynamically. Learning When and How Much to Reason. We observe that the model often generates long reasoning chains, even for relatively simple questions. This inefficiency echoes known challenges in language-only chain-of-thought models [65, 30], where reasoning is applied uniformly regardless of task complexity. promising direction for future work is to develop methods that help models learn when to reason and how much reasoning is necessary, adapting the depth of their chain-of-thought to the demands of the task. Interpreting Attention Patterns. Although our behavioral and human studies demonstrate that explicit grounding amplifies structured visual behaviors and improves interpretability, the mechanisms through which grounding improves reasoning remain underexplored. Future work should examine how explicit grounding influences models internal attention dynamics and whether these patterns resemble human gaze behaviors. Further comparisons to human cognitive strategiessuch as visual routines [62], task-directed attention [78], and deictic planning [4]could yield deeper insights into both model behavior and human cognition. A3 Broader Impacts Positive Societal Impacts. Grounded reasoning improves model transparency by producing interpretable chains of thought that reference specific image regions. This interpretability is especially valuable in high-stakes domains such as medical imaging, accessibility tools, assistive robotics, and scientific image analysis, where understanding the basis for model decisions is crucial. Additionally, our approach may enhance human-AI collaboration by making the models internal decision process legible and actionable for human users. Because the model reasons over localized visual subproblems, it can better align with human intuition, which may foster trust and oversight. 18 Potential Negative Impacts. Despite its benefits, the ability to produce detailed visual reasoning traces could be misused for surveillance or automated profiling. grounded model that can isolate visual elements, track reasoning over time, and justify actions may inadvertently enable more fine-grained tracking of individuals or objects in sensitive contexts. Moreover, models that appear interpretable may be perceived as more reliable than they truly are; if the reasoning trace is superficially plausible but incorrect, users may over-trust the models output. This risk is particularly relevant in domains where visual ambiguity or annotation bias affects ground truth. Our approach also assumes the availability of visual data and may propagate or amplify dataset biases, especially in domains where certain object types, environments, or affordances are overrepresented. Spatial grounding mechanisms may lock the models attention onto biased or stereotyped regions of the image, reinforcing existing disparities unless care is taken in dataset construction and evaluation. Mitigation Strategies. To mitigate these risks, future work could incorporate uncertainty estimation or confidence calibration into the reasoning trace, explicitly marking when the model is uncertain about grounding decision. It is also important to develop tools that allow users to audit the reasoning trace and provide corrections. In addition, efforts to diversify training data and evaluate model behavior across demographic and situational axes are essential for fair deployment. Finally, deployment in sensitive settings should involve human-in-the-loop oversight, particularly when grounded outputs inform consequential decisions. A4 Additional Discussion Our results show that explicitly grounding reasoning steps in spatial coordinates substantially improves performance across visual reasoning benchmarks. To better understand why, we draw from both the recent machine learning literature and long-standing cognitive science theories that point to grounding as core mechanism in human and artificial reasoning. Grounding as Cognitive Scaffold. Classic studies in visual cognition suggest that humans reason about complex scenes by sequentially directing attention to spatially localized regions in service of goal-driven subproblems. Yarbus [78] demonstrated that eye movement patterns depend heavily on task demands, suggesting deep link between overt visual attention and internal reasoning. Ullman [62] introduced the concept of visual routines, showing that reasoning over scene involves decomposable, spatially localized operations. Ballard et al. [4] formalized this via deictic codes, where visual fixations and gestures act as pointers binding abstract variables to perceptual content. These mechanisms reduce cognitive load (as opposed to raw computational load), support compositionality, and facilitate subgoal execution by grounding symbolic reasoning in visual input. Grounding Reduces Hallucination and Enhances Generalization. In machine learning, recent studies demonstrate that grounding in visual regions curbs hallucination and promotes generalization. Ghosh et al. [14] show that large VLMs hallucinate less and reason more accurately when first prompted to generate grounded visual descriptions. Qi et al. [44] propose chain-of-manipulations method where VLMs perform spatially localized actions (e.g., zoom, crop, verify), achieving stronger generalization and interpretability. These findings echo Treismans seminal Feature Integration Theory [61], which argues that spatial attention is essential for integrating visual features, and Harnads symbol grounding problem [20], which proposes that abstract reasoning is only meaningful when connected to perceptual representations. Object-centric and Compositional Inductive Biases. Explicit grounding may also encourage models to adopt object-centric and modular representations. Ding et al. [9] show that attention over learned object embeddings improves structured visual reasoning. Gupta et al. [18] demonstrate that language-guided neural modules that condition on spatial cues improve the compositional generalization of the model. These results align with the cognitive perspective that humans naturally build scene representations around individual entities and their relations [4]. Overall, the convergence of cognitive science and ML literature suggests that explicit grounding does not merely reduce the search space, but acts as structural inductive bias that enhances compositionality, verification, and goal-directed reasoning, core ingredients for generalization in both human and machine learners. 19 A5 Additional Methodological Details A5.1 Group Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO) [50] stabilizes policy learning from long-form trajectories by computing group-normalized advantages and applying clipped token-level PPO-style updates. Specifically, for group of trajectories = {τ (i)}G i=1 conditioned on input x, each trajectory τ (i) has scalar reward r(i) = R(τ (i)). GRPO computes the centered advantage ˆA(i) = r(i) R, where = 1 Let τ (i) be the t-th token of trajectory τ (i). GRPO minimizes the following clipped surrogate loss: r(i) is the group mean. (cid:80) LGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 τ (i) (cid:88) min (cid:104) ρ(i) ˆA(i), clip(ρ(i) , 1ε, 1+ε) ˆA(i)(cid:105) + β KL[πθπref], (1) = πθ(τ (i) πold(τ (i) where ρ(i) the KL penalty coefficient. This approach stabilizes learning in long-horizon, multimodal reasoning settings. is the importance weight, ε = 0.2 is the clipping parameter, and β is <t ,x) <t ,x) τ (i) τ (i) A5.2 RL Reward Functions Task Rewards. We define reward functions specific to each benchmark: Spatial Reasoning (SAT-2). binary reward: rtask = 1 if the predicted answer matches the ground truth, and 0 otherwise. Web Grounding (OS-Atlas). binary reward: rtask = 1 if the predicted coordinate lies inside the annotated bounding box. Web Action Prediction (ICAL). decomposed reward: rtask = rtype + rarg, where rtype = 0.5 if the predicted action type matches, and rarg = 0.5 if the predicted argument (e.g., DOM ID, string) matches. A5.3 Multi-turn Reinforcement Learning Details We first apply supervised fine-tuning (SFT) on multi-turn traces. These are derived from the same MCTS chains used to train the single-turn model, but reformatted into multi-turn training data by: (1) For each node, formulating the text in think block followed by taking the coordinate and formulating it into tool call. If it is terminal node, think block is added with any remaining text followed by the predicted answer in answer tags. (2) Cropping around the coordinate to obtain the feedback image, and appending this image to the training data as user turn in the sample to be used for SFT. (3) Continuing (1) and (2) until terminal. We additionally include backtracking as described in Section 4.2. For multi-turn scenarios, we apply GRPO over full dialog trajectories. Observation tokens are masked from the loss, ensuring gradients flow solely through the language model while retaining visual input to the encoder. We additionally mask samples not ending in an EOS token [80]. Termination Enforcement. During RL rollouts, if dialog reaches Tmax = 5 turns without emitting an <answer> block, we append soft prompt to the final assistant message: <think> Please provide your response now </think> This maintains structural fidelity and bounds rollout length. 20 KL collapse. While concurrent work observed occasional KL collapse in multi-turn RL when applied to base model with vision inputs [64, 92], we found that our initial warm start enabled stable training and allowed us to maintain moderate KL coefficient (0.01). Diversity bonus. We introduced the concept of diversity turn bonus into our reward formulation in Section 4.3 of the main paper. In Figure A1, we show the response length over RL training with and without the diversity bonus. Without the diversity bonus, the model quickly converges to single turn outputs (green line), whereas the model avoids this and outputs >1 turn on average with the bonus reward (blue line). Figure A1: Response length with and without turn bonus. Without the bonus, the model converges to always taking single turn (as also verified by examining model outputs), whereas the bonus enables the model to stabilize multi-turn. A5.4 Training Implementation Details General Setup. Training is conducted on 8 A100 GPUs with Qwen2.5-VL models (3B and 7B). Supervised fine-tuning uses 3 epochs, while GRPO is applied for 500 rollout-update iterations. Evaluation uses decoding temperature of 0.5. We build on Llama-Factory [89] for SFT, and EasyR1 [90, 52] for GRPO. Training Hyperparameters Overview We provide all training hyperparameters used in SFT, and singleand multi-turn RL in Tables A1-A3. Table A1: Tuning (SFT). Supervised FineTable A2: GRPO Training. Table A3: Multi-turn GRPO. Hyperparameter Value Epochs Learning rate Weight decay Warmup ratio Batch size Gradient accumulation Effective batch size Scheduler Precision Flash attention Freeze vision tower Max sequence length Deepspeed config 3 1e-6 0.01 0.03 8 4 32 Cosine bf16 fa2 True 8192 ZeRO Stage Hyperparameter Value Hyperparameter Training steps Learning rate Weight decay Warmup ratio Optimizer Group size KL coefficient Clip ratio Gradient clipping Rollout batch size Gradient step batch size Rollout engine Max prompt length Max response length Top-p Temperature Filter overlong Freeze vision tower Mixed precision 500 1e-6 0.01 0.05 AdamW (bf16) 5 0.01 0.28 Max norm 1.0 128 32 vLLM 4096 2048 0.99 1.0 True True bf16 Max prompt length Max response length (includes observations) Max generation per turn Max turns Crop resize Crop size Learning rate KL coefficient Weight decay Warmup ratio Clip ratio Gradient clipping Rollout batch size Gradient step batch size Training steps Group size Rollout engine Temperature Top-p Mixed precision Freeze vision tower Value 4096 4096 1024 5 384x384 100x100 1e-6 0.01 0.01 0.05 0.28 Max norm 0.2 128 64 500 8 vLLM 1.0 0.99 bf16 True 21 A5.5 MCTS Implementation Details To generate high-quality reasoning traces for fine-tuning, we use Monte Carlo Tree Search (MCTS) to explore possible sequences of reasoning steps over an image and question. Our MCTS procedure extends standard tree search with VLM as the generative model for reasoning steps and judge to evaluate final answers. We use the same reward as described in Section A5.2. Search Structure. Each node in the search tree corresponds to reasoning step (a candidate <think> or <answer> block). The root node contains the input question. Nodes maintain the text of the current step, accumulated visited coordinates, running estimate of expected reward, and visit count. Algorithm Phases. MCTS operates via the standard four-phase loop: Selection: Starting at the root, the search follows path through children using the UCB policy: + c(cid:112)log N/n, where is the average reward, is the parents visit count, the childs, and = 2.0 is the exploration constant. The search terminates when node has unvisited children or reaches terminal state. Expansion: At each expandable node, we generate up to three children using the VLM, each corresponding to new thought or final answer. If generated step includes terminal <answer> block, it is marked as terminal. An example prompt for this step can be found in Listing A1. Rollout: For each new child, we simulate reasoning steps using the VLM until final answer is reached or rollout depth limit is exceeded. These simulated trajectories are not added to the tree. Each rollout receives scalar reward from the judge model comparing the predicted and true answers. Backpropagation: Final 0/1 rewards are backpropagated up the tree along the visited path, updating the value estimate of each node using an incremental mean and incrementing visit counts. We use the same task reward as described in Section A5.2. We include our MCTS hyperparameters in Table A4. A5.5.1 Linearization of Search Trees into Reasoning Chains To convert MCTS-generated search trees into training data for supervised fine-tuning, we developed structured linearization procedure that extracts diverse, grounded reasoning trajectories from the tree. Tree Traversal and Chain Extraction. Each MCTS trace is stored as tree, with nodes representing reasoning steps (<think>) or final answers (<answer>). From each trace, we recursively enumerate all root-to-leaf paths that include at least one terminal node. For each terminal node, we extract: Correct rollouts: Paths ending in high-reward ( 1.0) final answer. Incorrect rollouts: Lower-reward paths used to generate synthetic backtracking examples. If both correct and incorrect rollouts exist at node, we concatenate the incorrect trace with fixed backtracking phrase (e.g., Wait, this seems off. Lets try something else.) followed by the corrected reasoning path, forming complete trace with embedded revision behavior. All steps are wrapped in single <think> block, and the final answer in separate <answer> tag. Token Cleanup and Deduplication. Each reasoning trace is cleaned to remove residual XML markers (<think>, <answer>) before joining, then wrapped again after concatenation. Chains are deduplicated across samples to prevent redundancy in training. MCTS accuracy. As shown in Table A5, we observe improved top-1 accuracy using our MCTS procedure. 22 Table A4: MCTS hyperparameters for Web Grounding. Hyperparameter Value Model Simulations per input Max tree depth Rollouts per node Children per expansion cpuct Sampling temperature Top-p Max tokens per node Estimated time per sample Parallel processes Qwen2.5-VL-72B-Instruct 20 10 2 3 2.0 1.0 1.0 512 21 minutes 10 A6 Behavioral Analysis Protocol To systematically evaluate reasoning behaviors in VLMs, we implement behavioral annotation pipeline that categorizes model-generated chain-of-thought (CoT) traces using GPT-4o as an evaluator. This procedure is based on Gandhi et al. [13] and enables fine-grained analysis of emergent reasoning strategies across different model variants and training regimes. Sample Selection and Preprocessing. We run the same 300 reasoning traces (obtained by randomly selecting examples from the SAT-2 validation set) per model condition where the models final answer was verified correct (judge_score = 1). From each trace, we isolate the contents of the <think> block. Behavioral Categories. We define four behavioral dimensions of interest: Visual Verification: Instances where the model confirms or checks property of the visual scene (e.g., Looking at the image, can confirm. . . ). Backtracking: Self-correction or re-interpretation of previously described visual element. Subgoal Setting: Decomposition of the visual reasoning process into smaller steps across regions (e.g., First will check X, then Y. . . ). Visual Regions Explored: Count of distinct visual regions explicitly referenced in the reasoning trace. Annotation via GPT-4o. For each trace, we construct four behavior-specific prompts and submit them to GPT-4o with temperature 0.0 and max token limit of 256. Each prompt asks the model to identify and count instances of specific behavior, outputting numeric value between custom <count> tags. These counts are extracted and recorded as the behavioral profile for the trace. Our full prompts are displayed in Listing A2. Comparison Across Conditions. We apply the above process across multiple model variants (e.g., Our full model, Naive GRPO, Ablated grounded reasoning) and aggregate the behavior counts to compute the average number of reasoning behaviors per trace. A7 Human Evaluation Setup To assess the interpretability and spatial accuracy of the models grounded visual reasoning outputs, we conducted structured human evaluation study using Prolific. The study was designed to evaluate whether the (x,y) coordinate output by the model accurately corresponded to the referenced region in the reasoning step, and whether this visual cue helped participants interpret the reasoning step. We obtained 80 samples randomly from the robospatial evaluation, as this evaluation contained real images without visual marks on the image. Data curation. We take 80 samples from our model run on Robospatial, and extract each sentence from the samples. To simplify the study for the participants and target step-level analysis, we filter out sentences that require context from the entire reasoning trace using GPT4o. For each reasoning 23 step and coordinate, we draw 100x100 pixel blue circle on the image centered at the coordinate location, and display the reasoning step text. Study Design. Each trial presented participants with single image containing blue circle annotation, along with natural language sentence that included the phrase [shown with blue circle] (which replaced the (x,y) coordinate) to denote the region being referenced. Participants answered two questions per image: 1. Accuracy (binary + unsure): Participants were asked whether the blue circle overlapped with the region described by the sentence. Options were: Yes if any part of the described region was inside the blue circle. No if the region was entirely outside the circle. Unsure if the sentence was ambiguous or the region could not be clearly judged. Responses marked as Unsure were excluded from accuracy and clarity score calculations to avoid inflating or deflating agreement metrics with ambiguous judgments. 2. Interpretability (Likert): Participants rated how much the blue circle helped them understand the sentence reference on 5-point Likert scale: 1 Strongly disagree 2 Disagree 3 Neutral 4 Agree 5 Strongly agree Interface and Instructions. Participants began the study by entering participant ID. detailed instruction panel introduced the task goals, decision criteria, and included two illustrative examples with images. These examples demonstrated both Yes and No cases for spatial accuracy, and clarified how to interpret the clarity rating. Participant Recruitment and Demographics. We recruited participants via the Prolific platform. total of 20 participants completed the study, with all submissions manually reviewed and approved. The participant pool was demographically diverse: ages ranged from 21 to 71 (mean = 39.4, SD = 13.6), with balanced gender representation (11 female, 9 male). Participants were based in the United States or the United Kingdom, and all reported fluency in English. Participants completed the task in an average of 22 minutes, and were compensated at rate of $12/hr. A8 Additional Experimental Results MCTS accuracy on OS-Atlas and SAT-2 We evaluate the accuracy of our MCTS procedure using Qwen2.5-VL-72B by measuring how often it reaches the correct answer, as determined by an oracle verifier. On 123 held-out samples from OS-Atlas (Table A5), MCTS achieves an accuracy of 82.1%, indicating strong search effectiveness when guided by reliable verifier. On the SAT-2 benchmark, MCTS reaches 96% accuracy. ScreenSpot-Pro Low Resolution Results. We provide results on ScreenSpot-Pro-LR in Table A6. This variant of ScreenSpot-Pro has downsampled images to max resolution of 1920x1920. On ScreenSpot-Pro-LR, the base Qwen2.5-VL-3B model achieves only 1.96% accuracy, while SFT improves performance to 16.89%. Incorporating GRPO further increases accuracy to 20.30%, and our full ViGoRL achieves 21.32%. Notably, our multi-turn ViGoRL reaches 23.72%, demonstrating that iterative visual feedback is especially beneficial in challenging perceptual settings. Accuracy on Out-of-Distribution Benchmarks. To ensure that our model does not lose foundational knowledge after our pipeline, we evaluate on some popular VQA benchmarks: MMMU [81], RealWorldQA [71], and V* [67]. On MMMU and RealWorldQA, we evaluate our models performance when prompted without thinking, thus performing an apples-to-apples comparison to the base model. On V*, we prompt our model to think to compare to the V* visual search paradigm. On all datasets, we find that ViGoRL either matches or exceeds the performance of the base model. 24 Table A5: Held-out performance of Qwen2.5-VL-72B on 123 samples in OS-Atlas validation and 958 samples in SAT-2 validation. * indicates MCTS Accuracy of the hold-out MCTS set used in the training pipeline. Model Top-1 Accuracy Top-3 Accuracy MCTS Accuracy OS-Atlas SAT-2 42.3% 61.48% 54.5% 76.30% 82.1% 96.0%* Table A6: Accuracy (mean with 95% confidence intervals) on ScreenSpot-Pro-LR benchmark. Model ScreenSpot-Pro-LR Qwen2.5-VL-3B Base + SFT direct + Vanilla GRPO ViGoRL-3b (Ours) ViGoRL-3b (Multi-turn) 1.96% (0.68) 16.89% (1.85) 20.30% (2.24) 21.32% (2.28) 23.72% (2.39) Table A7: Accuracy (mean with 95% confidence intervals) on Out-of-Distribution benchmarks. * indicates that the model was prompted to think. Model MMMU RealWorldQA V* Bench Qwen2.5-VL-3B Base ViGoRL-3b (Ours trained on SAT-2) ViGoRL-3b (Ours trained on UGround) 47.44% (3.26) 46.44% (3.26) 47.56% (3.26) 55.65% (3.52) 60.65% (3.46) 57.52% (3.50) 74.21% (6.22) 74.87% (6.15)* 75.13% (6.16)* A9 Example Model Outputs A9.1 Failure Cases in Current VLMs Despite high image-text alignment, models like Qwen2.5-VL-7B fail often produce abstract, shallow reasoning without examining many different image regions or describing content in visual depth, which often leads to errors. representative trace is below: <think> From the image, the twotiered metal cart is positioned closer to the modern TV stand in blue. The cart is nearer to the TV stand, which is evident from its proximity and the angle at which it is seen. Therefore, the twotiered metal cart is closer to the modern TV stand in blue. </think> <answer>twotiered metal cart</answer> A9.2 Warm Start Reasoning Traces Fine-tuning with MCTS-generated warm traces leads to structured and grounded reasoning. Example trace: 25 <think> need to locate Candice Georgiadis in the list. see the name Candice Georgiadis at (330, 440). Now will check for the Follow button next to it. Next to Candice Georgiadiss name at (330, 440), see Follow button at (440, 440). will verify if there are any other mentions of this name. should check another area to ensure theres no other Candice Georgiadis. Theres another sibling list and see the name Candice Georgiadis at (660, 770) but no Follow button is associated with this name. Having verified the name Candice Georgiadis and the Follow button, and ensuring there was no other mention of the name with follow button, am confident the desired location is at the first identification at (440, 440). Wait, this seems off. Lets try something else. need to locate Candice Georgiadis in the list. see the name Candice Georgiadis at (330, 440). Now will check for the Follow button next to it. Next to the name Candice Georgiadis at (330, 440), see button at (365, 450) that looks like it says Follow. should check another name on the page for similar formatting to confirm this is the correct Follow button. To ensure that this is indeed the correct Follow button, will check the name John Doe and its corresponding button at (230, 210). This button has the same format as the one observed for Candice Georgiadis at (365, 450). It reinforces that the button next to Candice Georgiadis is the one need. Lastly, to eliminate all doubt, will examine different region of the image where the Follow button for Fred Smith at (500, 600) is located. It aligns with the same design as the others, confirming the earlier observations. Therefore, can now finalize the answer for Candice Georgiadiss Follow button. </think> A9.3 Additional Model Output Examples We provide example outputs on RoboSpatial questions for two models: the vanilla GRPO model trained on SAT-2 (Figure A2), and our proposed ViGoRL trained on SAT-2 (Figure A3). 26 Listing A1: MCTS prompt for web grounding. You are helpful assistant tasked with grounding an element on web page . You should systematically reason through the problem step by step by checking and verifying relevant webpage regions , while grounding reasoning steps to specific (x , ) points in the image : nEach reasoning step must be enclosed within < think > tags and reference exactly one specific coordinate (x , ) : < think > { Single reasoning step with grounded point } (x , ) . </ think > nWhen ready to provide the final answer , enclose it within < answer > tags : < answer > ( xf , yf ) </ answer > nYour task is to help the user identify precise (x , ) coordinates of described area / element / object based on description . - Generate ONLY ONE reasoning step OR the final answer per response . - Regions are distinct , non - overlapping areas ( . . , quadrants like top - left , elements like tree / button , zones like background / foreground ) . - Each step should describe the region then evaluate it for its relevance to the task and to previous steps . - Never repeat coordinates from previous steps . - Begin by exploring diverse regions , even if they seem less likely , to ensure comprehensive coverage before narrowing down . - Prioritize broad coverage of diverse candidates before deciding . - Aim for accurate , representative points in the described area / element / object . - If unclear , infer based on likely context or purpose . - Verify each step by examining multiple possible solutions before selecting final coordinate . - Format points as (x , ) Listing A2: Prompts used to evaluate visual reasoning behaviors in chain-of-thought outputs. Here is chain - of - reasoning that Language Model generated while analyzing an image . The chain - of - reasoning output from the model is : { reasoning } Evaluate whether the chain - of - reasoning contains any visual verification steps . visual verification step is when the model confirms or checks something it sees in the image . Examples include : \" can see that the object is not cat , but dog \" , \" The text confirms this visual aspect is correct \" , \" can verify this is indeed red \" , or \" Looking at the image , can confirm ...\". Count both explicit mentions of image regions and implicit verifications . Count all instances where the model verifies information from the image and provide the count between the tags < count > </ count >. If the chain - of - reasoning does not contain any visual verification steps , please provide count of 0 as < count >0 </ count >. --- Here is chain - of - reasoning that Language Model generated while analyzing an image . The chain - of - reasoning output from the model is : { reasoning } Evaluate whether the chain - of - reasoning contains any backtracking behavior , where the model changes its interpretation or corrects itself . Examples include : \" At first thought , but looking more carefully see it actually \" , \" initially interpreted this as circle , but it actually an oval \" , \" On second thought ...\" , \" Actually , notice that ...\" , or \" Let me correct my earlier observation ...\". Count all instances where the model revises its understanding and provide the count between the tags < count > </ count >. If the chain - of - reasoning does not contain any backtracking behavior , please provide count of 0 as < count >0 </ count >. --- Here is chain - of - reasoning that Language Model generated while analyzing an image . The chain - of - reasoning output from the model is : { reasoning } Evaluate whether the chain - of - reasoning contains any visual subgoal setting , where the model breaks down the image analysis into smaller steps or focuses on different parts of the image in sequence . Examples include : \" First , ll examine this part , then ll look at that object \" , \" Let me check each element one by one \" , \" need to identify what in this area \" , or any structured approach to analyzing different parts of the image . Count all instances where the model sets up plan or approach for analyzing the image and provide the count between the tags < count > </ count >. If the chain - of - reasoning does not contain any visual subgoal setting , please provide count of 0 as < count >0 </ count >. --- Here is chain - of - reasoning that Language Model generated while analyzing an image . The chain - of - reasoning output from the model is : { reasoning } Count how many distinct visual regions or elements the model explicitly mentions examining in the image . Examples include : \" can see dog in the corner \" , \" There text at the top of the image \" , \" The object in the center appears to be ...\" , \" Looking at the left side ...\" , or any reference to specific part or element of the image that the model is analyzing . Count all distinct visual regions or elements mentioned and provide the count between the tags < count > </ count >. If the chain - of - reasoning does not mention any specific visual regions , please provide count of 0 as < count >0 </ count >. 28 Figure A2: Example reasoning traces from vanilla GRPO on the RoboSpatial dataset, which does GRPO directly on the base model without warm start. Figure A3: Example reasoning traces from ViGoRL on the RoboSpatial dataset."
        }
    ],
    "affiliations": []
}