{
    "paper_title": "LLM-guided Hierarchical Retrieval",
    "authors": [
        "Nilesh Gupta",
        "Wei-Cheng Chang",
        "Ngot Bui",
        "Cho-Jui Hsieh",
        "Inderjit S. Dhillon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 1 2 3 1 . 0 1 5 2 : r LLM-GUIDED HIERARCHICAL RETRIEVAL Nilesh Gupta Wei-Cheng Chang Ngot Bui Cho-Jui Hsieh UT Austin (cid:135) https://github.com/nilesh2797/lattice UCLA Google Inderjit S. Dhillon"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM based IR has shown great promise, the current retrieve-then-rerank paradigm inherits the limits of embedding-based retrieval, parametric generative approaches are difficult to adapt to new information, and long-in-context approaches that put the entire corpus in context are computationally infeasible for large document corpora. To this end, we introduce hierarchical retrieval framework LATTICE that enables an LLM to reason and navigate large corpus with search complexity that is logarithmic in the number of documents, achieved by imposing semantic tree structure on the corpus. Our approach comprises two stages: (1) an offline process where we organize the document collection into semantic hierarchy we explore two LLM-driven strategies for this, bottom-up agglomerative approach and top-down divisive approach using multi-level summaries; and (2) an online traversal stage where \"search LLM\" navigates this tree. central challenge in using LLMs for this search is that the LLMs relevance judgments are noisy, context-dependent, and unaware of the underlying hierarchy, making it difficult to compare nodes across different branches and levels of the tree. To solve this, our traversal algorithm estimates calibrated latent relevance scores from the LLMs local outputs, which are combined into path relevance metric to guide the search globally across the tree. Our training-free framework achieves state-ofthe-art zero-shot performance on the reasoning-intensive BRIGHT (Su et al., 2024) benchmark (with up to 420K corpus size), demonstrating improvements of up to 9% in Recall@100 and 5% in nDCG@10 over the next zero-shot baseline. Moreover, compared to the highly specialized and fine-tuned SOTA method DIVER-v2 (Long et al., 2025), it achieves comparable results on BRIGHT subsets that use static corpus for evaluation. Figure 1: (Left) Recall@100 on BRIGHTs StackExchange subsets. Comparison between zero-shot LATTICE (Gemini-2.5-flash) against BM25 retriever (with GPT-4 query expansion) and fine-tuned dual-encoder ReasonIR-8B (Shao et al., 2025) (with GPT-4 query expansion). LATTICE yields the highest average recall (74.8%) and substantially outperforms BM25 across all subsets (avg +9.5 pp) and ReasonIR-8B on average (+4.0 pp), with particularly large gains on some datasets like Economics and Robotics. (Right) LLM cost (measured in avg. number of input tokens given to LLM) vs. ranking quality (nDCG@10) on the Robotics subset. Reranking baselines (BM25+rerank, ReasonIR-8B+rerank) with varying top-k shortlist use same Gemini-2.5-flash as reranker exhibit early gains but quickly plateau. LATTICE starts with shallow flat region (cost of traversing tree levels) but then scales more effectivelysurpassing the baselines and continuing to improve to higher final nDCGdemonstrating that guided hierarchical traversal using LLM can be more compute efficient. 1 Figure 2: high-level overview of our proposed framework, LATTICE. The process consists of two stages. (Left) In the offline stage, we organize an unstructured document corpus into semantic tree. (Right) In the online stage, search LLM performs best-first traversal over calibrated path relevance scores to find documents relevant to user query. The path relevance score is defined as the exponentially moving average of calibrated scores of nodes on the path. Score calibration is achieved by comparing nodes against high-relevance candidates from sibling branches and previously seen leaves, ensuring globally coherent search."
        },
        {
            "title": "INTRODUCTION",
            "content": "The proliferation of Large Language Models (LLMs) has catalyzed paradigm shift in Information Retrieval (IR), moving beyond simple fact-finding towards complex problem solving that demands nuanced understanding and reasoning. Modern user queries often require not just keyword or semantic matching, but deeper level of inference, categorized as reasoning-based retrieval (Su et al., 2024). For instance, user might seek solution to coding bug by describing its behavior, or ask for math problems that require applying specific theorem. Answering such queries effectively requires retrieval of documents that help reason through the problem, task for which traditional IR systems are poorly equipped. Current LLM-based IR systems primarily fall into three paradigms, each with inherent drawbacks. The first, Retrieve-then-Rerank, employs computationally cheap retriever (e.g., BM25 or dense retrieval) to fetch broad set of candidate documents, which are then re-ordered by more powerful but expensive LLM. Although scalable, this approach is constrained with the limits of the initial retrieval stage (Weller et al., 2025); if crucial document is not captured in the initial candidate set, even perfect reranker cannot recover it. Furthermore, the initial retrieval often relies on shallow semantic similarity, failing to perform the multi-step reasoning needed to identify relevant documents for complex queries. The second paradigm, Generative Retrieval (GenIR), uses the LLM itself to synthesize an answer. This can be parametric (Tay et al., 2022), where the corpus is stored implicitly in the model weights, making the system prone to hallucinations and difficult to update with new information. Alternatively, long-context GenIR (Lee et al., 2024a) places the entire corpus (or large subset) explicitly into the LLMs context. While this allows the LLM to reason over the full text, it is computationally infeasible for typical retrieval corpora, as the self-attention mechanisms quadratic/super-linear complexity leads to prohibitive costs and latency. To this end, we propose an LLM-guided hierarchical retrieval framework LATTICE, framework that combines the logarithmic search efficiency of hierarchical structures with the sophisticated reasoning capabilities of modern LLMs. Our method first organizes document corpus into semantic tree offline, with internal nodes represented by rich, LLM-generated textual summaries. Then, at query time, search LLM navigates this semantic hierarchy using greedy, best-first traversal, processing beam of top candidates at each step. To ensure the search remains globally coherent, the traversal algorithm computes path relevance score for each node by aggregating calibrated local scores from the LLM along the path from the root, allowing our method to robustly compare nodes across different branches and levels and efficiently reach the most relevant documents. Our main contributions are: We introduce novel retrieval framework where an LLM directly performs the traversal of semantic hierarchy, using its reasoning capabilities to guide the search path at each step, achieving state-of-the-art zero-shot results on the reasoning-intensive BRIGHT benchmark with improvements of up to 9% in Recall@100 and 5% in nDCG@10. 2 We propose robust LLM-guided search algorithm that reliably performs greedy search on semantic tree using noisy LLM judgments. We design and compare two distinct LLM-driven strategies for corpus organization: bottom-up agglomerative clustering method and top-down divisive summarization approach. As LLMs increasingly become the fundamental unit of intelligent systems, the main goal of this paper is to show promise for an LLM-native retrieval system where LLMs are more deeply integrated in the search process than current IR systems."
        },
        {
            "title": "2.1 LLMS FOR INFORMATION RETRIEVAL",
            "content": "Retrieve-then-Rerank Paradigm. The dominant paradigm in modern IR is two-stage retrievethen-rerank pipeline (Zhu et al., 2023). LLMs have excelled as powerful rerankers in this framework, applied in either pointwise (score each document independently) or listwise fashion (rank list of documents) (Reddy et al., 2024; Sun et al., 2024). However, the overall performance is irreversibly bottlenecked by the quality of the initial retrieval stage (Rathee et al., 2025). In the retrieval stage, LLMs are increasingly used as backbones for dense embedding models (Luo et al., 2024; Lee et al., 2025), though this often involves adapting their autoregressive architecture for representation learning which is not directly aligned with their pre-training task. Generative Paradigms. To overcome the limitations of the cascading pipeline, alternative paradigms have emerged. Generative Retrieval, such as the Differentiable Search Index (DSI) (Tay et al., 2022; Li et al., 2024), reframes IR as sequence-to-sequence task, mapping query directly to document identifier. While conceptually elegant, these methods face challenges in scaling and updating the index (Pradeep et al., 2023). Long-Context Retrieval proposes placing the entire corpus into the LLMs context window (Lee et al., 2024a), but this remains computationally infeasible for even moderate-scale applications. Our work offers middle ground by using semantic hierarchy to structure the corpus, thus enabling an LLM to navigate it efficiently without the scalability / updatability issues of generative retrieval or the computational cost of long-context models. 2.2 HIERARCHICAL RETRIEVAL Vector Hierarchies. Hierarchical structures have been long used to improve computational efficiency in tasks with large output spaces, such as hierarchical softmax for language modeling (Morin & Bengio, 2005) and tree-based methods for extreme multi-label classification (Prabhu & Varma, 2014; Yu et al., 2022; Gupta et al., 2022). In vector search, algorithms such as Hierarchical Navigable Small World (HNSW) (Malkov & Yashunin, 2018) use multilevel graph for an efficient approximate nearest-neighbor search, although this hierarchy is geometric rather than semantic. Textual Hierarchies. More recently, models like RAPTOR (Sarthi et al., 2024) construct semantic hierarchy by recursively clustering and summarizing text chunks from the bottom up. This creates tree with nodes representing different levels of abstraction. However, RAPTOR relies on conventional embedding-based similarity search to traverse this tree. Our work differs fundamentally by employing an LLM as an active traversal agent during the online retrieval phase. Instead of static vector comparison, our model uses in-context reasoning at each node to decide the optimal path, transforming the retrieval into an intelligent navigation process. 2.3 AGENTIC AND REASONING-BASED IR Reasoning as Pre-processing Step. common approach to incorporate reasoning in IR is through query expansion (QE) (Wang et al., 2023; Gao et al., 2023). In this setup, an LLM enriches the query with generated text or chain-of-thought analysis before it is passed to standard retrieval system. While effective, this treats reasoning as discrete pre-retrieval step, leaving the core search 3 Figure 3: An illustration of the search process of LATTICE for real query from the BRIGHT benchmark. The color of each node corresponds to its computed path relevance; the highlighted yellow path shows the path to ground-truth documents. The search LLM makes step-by-step decision at each internal node to determine which branch to explore next. The expanded callout provides \"glass box\" view into one such decision, detailing the LLMs explicit reasoning process as it scores the child nodes. mechanism unchanged, often resulting in complex multi-component pipelines (Long et al., 2025; Shao et al., 2025) and ill-suited for lengthy and complex queries. Agentic Frameworks. The emerging field of Agentic IR (Jin et al., 2025; Zhang et al., 2024) conceptualizes retrieval as multi-step, goal-oriented process. However, current implementations typically involve an LLM agent calling an external, black-box search tool, making its success contingent on the tools effectiveness. Similarly, Graph-RAG (Edge et al., 2024; Zhang et al., 2025) leverages LLMs to reason over pre-structured knowledge graphs, but the role of LLMs to retrieve information from these graphs are limited. Our work integrates the reasoning agent more deeply into the retrieval process itself. The LLM is not just pre-processor or tool-caller but the core search mechanism, more specifically, is an agent whose environment is the corpus semantic tree. The tree provides essential scaffolding, constraining the agents action space to make the search tractable, while the agents reasoning enables intelligent traversal decisions, offering more fundamental fusion of reasoning and retrieval."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We begin by formalizing the task setup and notations in Section 3.1, followed by detailed description of the search procedure in Section 3.2, and ending with the tree construction procedures in Section 3.3. 3.1 SETUP The fundamental task is retrieval: given large corpus of documents, = {d1, d2, . . . , dD}, and complex natural language query q, the objective is to retrieve ranked list of documents Drel D. We define the core components and notations of our framework as follows: Semantic Tree: The corpus is organized into tree = (V, E), with single root node, vroot. Nodes (v ): The set of nodes is partitioned into leaf nodes VL (corresponding to documents) and internal nodes VI (representing document clusters). Edges (E): The set of directed edges consists of ordered pairs (u, v), where = parent(v). The set of immediate children of node is denoted as C(u). Node Representation (ϕ(v)): Every node has textual representation ϕ(v). For vl VL, ϕ(vl) is its documents content. For vi VI , ϕ(vi) is an LLM-generated summary of its children. 4 Search LLM (L): For the purpose of this paper we assume that the search LLM can be abstracted out as listwise scoring function. Given query and list of candidate nodes [v1, . . . , vk], it returns list of real-valued scores (along with reasoning trace): L(q, [ϕ(v1), . . . , ϕ(vk)]) = [s1, . . . , sk] where si [0, 1], = 1, . . . , k. higher score implies higher preference. The prompt used in our experiments to prompt an LLM as is detailed in Figure 7."
        },
        {
            "title": "3.2 ONLINE LLM-GUIDED HIERARCHICAL SEARCH",
            "content": "The core challenge in using an LLM for hierarchical search is that its relevance judgments are inherently noisy, context-dependent and unaware of the underlying hierarchy. The score assigned to node depends on the query as well as on the other nodes present in the list of options provided to the LLM. On top of this, these scores are inherently noisy due to the non-deterministic reasoning chain / inference of LLMs. This makes it difficult to compare the promise of node in one branch against node in completely different branch or at different level of the tree. Given search query, the goal of our traversal algorithm is to prioritize the exploration of relevant nodes in the tree by predicting path relevance score, ˆprel(v), which converts these noisy, local signals into globally coherent signal. The algorithm, depicted in Figure 2 and formalized in Algorithm 1, proceeds in following steps. 1. Initialization. The search begins with max-priority queue, the frontier (F ), which is initialized with the root node vroot. Its score is set to ˆprel(vroot) 1.0. We also initialize an empty prediction set (Pred) to store candidate leaf nodes and history of all observed scores, ScoreHistory . 2. Beam Expansion. The search runs for iterations, where in each iteration we expand (i.e. evaluate the children nodes of the chosen node) beam of the top most promising nodes from the frontier . These nodes are selected based on their current path relevance scores ˆprel. 3. Slate Construction with Calibration. For each node in the beam, we construct slate for the search LLM to evaluate. This slate consists of the children of the current node C(v), augmented with set Aug(v). The composition of Aug(v) depends on the type of nodes being evaluated: If C(v) are internal nodes, Aug(v) consists of the highest scoring sibling of to provide cross-reference between different branches. If C(v) are leaf nodes, Aug(v) consists of ℓ (a hyperparameter) leaf nodes sampled from Pred according to probability distribution proportional to ˆprel(u), anchoring the evaluation against the best candidates found so far and giving chance for best scoring candidates to be evaluated again in different context. In Figure 4, we show that this is essential for the final ranking. 4. Latent Score Estimation and Path Relevance Update. After the search LLM evaluates the slate and produces local scores, we perform global calibration step before updating path relevance. We model the observed score si for node in given slate as linear transformation of an underlying, slate-independent latent relevance score ˆsv: ˆsv + bi si where is single global scale parameter and bi is per-slate bias parameter. After each new slate is evaluated, we update our estimates for all latent scores {ˆsv}, a, and biases {bi} by treating this as Maximum Likelihood Estimation (MLE) problem. We find the parameters that minimize the Mean Squared Error (MSE) across all scores observed thus far: min a,{ˆsv},{bi} (cid:88) (cid:88) vslatei (si (a ˆsv + bi))2. Note that without the a, bi parameters ˆsv reduces to the mean of all the scores seen so far for node in ScoreHistory, we notice improved performance with this formulation as it can account for noise in scoring. While other objectives such as margin-based losses or probabilistic models like 5 Algorithm 1 LLM-guided Hierarchical Search end for LatentScores SolveMLE(ScoreHistory) {Minimize MSE to find all ˆsv} for all in Beam that were just expanded do Slate C(v) + Aug(v) LocalScores [sv]vSlate L(q, [ϕ(v)]vSlate) Add {(slate_idi, v, sv) Slate} to ScoreHistory Beam Extract top nodes from for all in Beam do 1: Parameters: q, T, L, B, N, K, α 2: Initialize: 3: Frontier new MaxPriorityQueue(), Pred 4: ScoreHistory , LatentScores 5: ˆprel(vroot) 1.0, F.push(vroot, ˆprel(vroot)) 6: for = 1 to do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end for 28: return Top-K nodes from Pred sorted by ˆprel end for for all in C(v) do if is leaf node then F.push(v, ˆprel(v)) for all in Slate do end if end for Add to Pred end for else ˆsv LatentScores[v] ˆprel(v) α ˆprel(parent(v)) + (1 α) ˆsv Plackett-Luce could be applied, we found the simple modified MSE optimization to be the most consistent. The resulting latent score ˆsv is used to define the path relevance: ˆprel(v) = α ˆprel(parent(v)) + (1 α) ˆsv Here α is hyperparameter in [0, 1]. After scoring, the newly evaluated internal nodes are added to the frontier , and leaf nodes are added to the prediction set Pred. 5. Termination. The algorithm terminates after iterations. The final output is the set of top-K documents from Pred, ranked by their final path relevance scores. 3.3 OFFLINE TREE CONSTRUCTION The objective is to create tree = (V, E) where every leaf node VL is connected to the root node vroot via single path and each node is annotated with textual representation ϕ(v). The maximum branching factor of any node is constrained by hyperparameter , i.e., C(v) . While our traversal algorithm can be adapted for more general Directed Acyclic Graph (DAG) structures, we focus on tree for simplicity. We now describe our bottom-up construction approach, which is conceptually similar to recursive clustering and summarization methods like RAPTOR (Sarthi et al., 2024). 3.3.1 APPROACH 1: BOTTOM-UP CLUSTERING AND SUMMARIZATION This approach constructs the tree layer by layer, starting from the leaf nodes and iteratively clustering and summarizing them until single root node is formed. To do this, we require two main components: An embedding function, : text Rd, which maps textual representation ϕ(v) to d-dimensional vector. We use Gecko embeddings (Lee et al., 2024b) in our experiments. 6 clustering function, C. Given set of vectors = {x1, . . . , xn}, the function produces partition {K1, . . . , Km} of X, such that for all {1, . . . , m}, Kj and Ki Kj = for = j. This can be implemented via iterative application of standard clustering algorithms like spectral clustering. The construction process, formalized in Algorithm 2, proceeds as follows: 1. Initial Layer Formation. The process begins with the set of leaf nodes, VL. We form an initial set of parent nodes, Vcurrent, one level above the leaves. This can be done in two ways: From Scratch: Apply the embedding and clustering functions to all documents to form the initial parent nodes. Using Metadata: For datasets where documents are passages from smaller set of source articles (stackexchange sub-datasets in BRIGHT), we leverage this inherent structure. We form initial clusters by grouping all passages belonging to the same source document. If any of the resulting cluster contains more than passages, we further group nodes in the cluster based on location proximity in the source document until all sub-clusters satisfy the branching factor constraint. This metadata-driven approach often yields more semantically coherent initial groupings. Further implementation details are provided in Appendix B.3. 2. Iterative Clustering and Summarization. Starting with the initial set of parent nodes, Vcurrent, we iteratively repeat summarize-embed-cluster cycle. In each iteration, we first generate textual summary ϕ(v) for each node in Vcurrent, embed these new summaries, and cluster them to form the next, higher level of the tree. 3. Termination. We repeat this process until the number of nodes at the current level is less than or equal to . These final nodes are assigned as the children of the root node, vroot, completing the tree. 3.3.2 APPROACH 2: TOP-DOWN DIVISIVE CLUSTERING As an alternative to the agglomerative bottom-up method, we also explore top-down divisive approach. Conceptually, this method is similar to hierarchical k-means, where we begin with single cluster containing the entire document corpus and recursively partition it. The standard implementation would use an embedding and clustering function at each step. However, we observed that this can produce noisy, suboptimal clusters at the higher levels of the tree where partitions should be based on broad conceptual similarities rather than keyword overlap. To address this, we employ an LLM as more powerful clustering function. Since providing the entire corpus to an LLM is infeasible due to context limits, we introduce prerequisite step: hierarchical summarization. For each leaf node vl, we prompt an LLM to generate five summaries in increasing order of complexity (we quantify the complexity of summary by its length, for e.g. first level of summary is 1-2 word, next is 3-4 words, and so on, more details in Section B.3.2), yielding set of multi-level representations {ϕ(vl)i}5 i=1. The top-down construction, detailed in Algorithm 4, proceeds as recursive partitioning process: 1. Initialization. The process begins with work queue containing the root node vroot, whose children are initially all leaf nodes VL. 2. Recursive Partitioning. We iteratively process nodes from the queue. For each node to be partitioned, we first select an appropriate summary level for its leaf descendants (details in the Section B.3.2). We then provide the set of unique summaries at that level to an LLM, prompting it to group them into conceptual topics. 3. Node Creation and Re-assignment. The LLM returns description for each of the topics and mapping from the unique input summaries to these topics. We create new internal nodes, assign them the topic descriptions, and partition the leaf descendants of among these new nodes according to the LLMs mapping. These new nodes become the children of v. Any new node that still contains more than leaves is added to the queue for further partitioning. 7 4. Termination. The process terminates when the queue is empty, meaning all internal nodes in the tree satisfy the maximum branching factor constraint."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Benchmark. All experiments are conducted on the BRIGHT benchmark (Su et al., 2024), collection of 12 reasoning-intensive retrieval tasks. The benchmark is specifically designed to evaluate deep reasoning and is composed of complex questions from diverse sources, including StackExchange, Leetcode, and TheoremQA, spanning topics from biology and economics to programming and mathematics. Evaluation Metrics. We use two standard IR metrics to measure performance: nDCG@10 (Normalized Discounted Cumulative Gain at 10) to evaluate the ranking quality of the top 10 results, and Recall@100 to measure the comprehensiveness of the retrieval within the top 100 results. Baselines. We compare LATTICE against several strong baselines. SOTA Systems: We compare against state-of-the-art systems like DIVER-v1/v2 (Long et al., 2025), RaDeR (Das et al., 2025), ReasonRank (Liu et al., 2025) and ReasonIR (Shao et al., 2025), which are trained and highly specialized for the BRIGHT benchmark. Controlled Reranking Baseline: To ensure fair, apples-to-apples comparison, we include strong retrieve-then-rerank baseline XRR21(BM25 + Rerank) that uses the same base LLM (Gemini-2.5-flash) as our method. XRR2 first retrieves 100 candidates using BM25 with GPT-4 expanded query and then reranks them using Gemini-2.5-flash model for total 5 iterations. This allows us to isolate the performance gains attributable to directly using an LLM to search the space versus just reranking small retrieved corpus. Implementation Details. For all LLM-driven components of our method (tree construction, summarization, and online search), we use Gemini-2.5-flash (Comanici et al., 2025). For the online traversal, we set the path relevance momentum to α = 0.5, the number of iterations to = 20, ℓ = 10 and the beam size to = 2. This configuration results in approximately 250 documents being evaluated by the LLM per query. For tree construction, the maximum branching factor was set to 10 20. For datasets derived from StackExchange, we employed the bottom-up clustering method; for all others, we used the top-down divisive approach. Our method, LATTICE, is evaluated in strictly zero-shot setting, without any fine-tuning or ensembling with any other method for the BRIGHT benchmark tasks. Further details are provided in Appendix B. 4.2 PERFORMANCE ON THE BRIGHT BENCHMARK Ranking Performance (nDCG@10) We present main ranking results on the BRIGHT benchmark in Table 1. On the seven StackExchange datasets, which use standard static corpus, LATTICE achieves an average nDCG@10 of 51.6, significantly outperforming the controlled reranking baselines score of 47.4. Furthermore, our zero-shot performance is highly competitive with the fine-tuned SOTA, Diver-v2 (52.2), and even achieves the best results in several sub-domains like Economics and Robotics. On the 3/5 Coding and Theorem-based tasks (LeetCode, AoPS & TheoremQ), our methods performance is noticably lower than the baselines. This is attributable to specific benchmark artifact: the use of query-dependent dynamic corpus, where unique large list (can be > 10K) of documents (which are potential positives) is excluded from the search space. While we prune the excluded leaf nodes at query time, the pre-computed summaries (ϕ(v)) of their parent nodes do not update dynamically. Consequently, these summaries often misguide the traversal (please see Figure 6, Section C.2). In contrast, retrieve-then-rerank pipelines can simply filter excluded documents from their candidate list post-retrieval without penalty. We would like to note that most real-world IR systems operate on query-independent corpus. 1https://github.com/jataware/XRR2/tree/main 8 Method FineStackExchange tuned Avg. Bio. Earth. Econ. Psy. Rob. Stack. Coding Sus. Avg. Leet. Theorem-based Avg. Pony Avg. AoPS ThQ. ThT. BM25 SBERT gte-Qwen1.5-7B OpenAI Google ReasonIR-8B RaDeR-7B DIVER ReasonIR DIVER v1 ReasonRank XRR2 DIVER v2 LATTICE 34.8 18.2 28.4 27.7 30.2 33.1 30.1 35.8 41.7 46.1 46.8 47.4 52.2 53.6 18.5 35.5 35.2 36.4 43.6 36.1 51.9 59.8 62.2 62.7 63.1 68. 54.1 26.3 43.1 40.1 45.6 42.9 42.9 53.5 53.2 58.7 55.5 58.2 62.5 24.3 17.5 24.3 25.1 25.6 32.7 25.2 29.5 32.0 34.4 36.7 38.5 42.0 Retriever with GPT-4 REASON-query 38.7 27.2 34.3 38.0 38.2 38.8 37.9 41. 43.6 52.9 54.6 52.9 58.2 18.9 8.8 15.4 13.6 18.7 20.9 16.6 21.4 27.7 11.8 22.9 18.2 29.5 25.8 27.4 27.5 26.3 17.5 23.9 24.2 17.9 27.5 25.0 26.1 Retrieve-then-rerank 28.8 35.6 35.7 37.1 41. 38.7 36.5 38.0 37.6 44.3 36.0 42.9 44.8 44.6 49.2 18.4 17.3 15.3 15.5 17.4 25.5 23.3 22.6 34.0 32.1 27.5 28.4 33.8 LLM-guided Hierarchical Retrieval 19.3 24.3 25.4 24.5 31.1 31.5 34.8 33. 33.2 38.9 29.5 21.9 34.8 17.6 10.3 5.2 6.5 3.7 19.6 11.9 11.7 34.8 25.4 25.6 35.0 32.9 14.6 16.9 22.6 18.1 22.7 25.4 31.0 29.5 29.4 37.1 35.5 31.8 38.6 3.9 5.0 4.6 7.7 10.0 7.4 12.0 9. 7.9 18.3 14.4 15.7 19.1 19.2 22.3 28.7 22.9 27.8 33.1 37.7 39.3 32.6 40.0 42.0 34.4 44.3 20.8 23.5 34.6 23.8 30.4 35.7 43.4 39.7 47.7 53.1 50.1 45.5 52.6 27.0 17.7 24.8 23.3 26.2 29.9 29.2 32. 37.3 41.5 40.8 40.3 45.7 51.6 64.4 62.4 45.4 57. 47.6 37.6 46.4 26.9 19.9 34. 30.0 12.0 30.1 47.8 42.1 Table 1: nDCG@10 performance of various retrievers and rankers on the BRIGHT benchmark. Bold represents overall best numbers, underline represents best numbers among zero-shot methods, denotes subsets with dynamic corpus. Retrieval Performance (Recall@100) As illustrated in Figure 1, our method demonstrates superior overall retrieval comprehensiveness. On average, LATTICE achieves Recall@100 of 74.8, outperforming both the BM25 baseline (65.3) and the specialized ReasonIR-8B model (70.8). This strong performance is consistent across the majority of subsets, with our method achieving the highest recall in four of the seven domains, including Economics and Psychology. Cost-Performance Analysis. To analyze the computational cost of our method, we compare the trade-off between performance (nDCG@10) and cost (measure in number of tokens processed by the LLM) against two retreive-then-rerank baselines using the gemini-2.5-flash as the ranking LLM and varying top-k predictions from the retriever. Figure 1 plots this relationship for the Robotics subset. While the reranking baselines exhibit diminishing returns, LATTICEs performance scales far more effectively on this subset. The performance initially remains flat as the model needs to take atleast tree height number of slate comparisons to reach leaf node. This shows promise that our guided hierarchical search can be more efficient use of the LLMs computational budget than reranking long, flat list of documents, where many of the tokens are spent on irrelevant candidates."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Effect of # Cross-Branch Calibration (ℓ). Figure 4 shows the impact of including ℓ top-scoring nodes from sibling branches in the leaf slates on bio subset. The results demonstrate that this calibration is critical for effective search. The baseline with no calibration (ℓ = 0) performs significantly worse and fails to improve with more search iterations. Performance consistently increases with ℓ, with substantial gains from ℓ = 1 to ℓ = 5. The gains diminish after ℓ = 5. Figure 4: nDCG@10 vs. ℓ. Impact of Method Components To quantify the contribution of each component of LATTICE, we conduct detailed ablation study with results presented in Table 2. We compare our full method against several variants: version without score calibration (always taking the latest score given Configuration Avg. Bio. Earth. Econ. Psy. Rob. Stack. Sus. LATTICE (Full Method) No Score Calibration (ˆsv = last si v) No Path Relevance (α = 0) No Reasoning (thinking_budget= 0) 51.57 64.38 62.36 45.37 57.35 47. 37.58 46.35 49.36 48.62 49.33 64.45 63.62 63.69 58.98 55.89 57.32 44.27 41.90 43. 54.41 52.99 57.33 46.70 42.14 45.73 32.93 40.68 33.16 43.80 43.09 43.95 Table 2: Ablation study on the core components of our traversal algorithm, evaluated across all StackExchange subsets of the BRIGHT benchmark. All values are nDCG@10. by the search LLM to node), one without path relevance (disabling path smoothing with α = 0), and one with zero reasoning budget to the LLM (passing thinking_budget= 0 in search LLM calls and strictly constraining it to output only the scores field in its output json). Disabling path relevance smoothing causes the largest degradation, followed by removing either the LLMs reasoning or score calibration mechanism reducing the average score by over 2.2 nDCG points. Beam Size vs. Search Iterations. Figure 5 presents budgetmatched analysis of beam size (B) versus search iterations (N ), where the total number of node expansions (B ) is kept roughly constant. The results indicate that for fixed computational budget, prioritizing search depth (more iterations) over breadth (a larger beam) is better. The configurations with smaller beams, = 1 and = 2, achieve the highest final nDCG@10 scores but are more sequential. This validates our choice of using small beam size (B = 2) with moderate number of iterations. Figure 5: nDCG@10 vs. beam-size. TheoT. Biology nDCG@10 R@ Impact of Tree Construction Strategy We investigate the impact of the tree construction strategy on two representative datasets in Table 3. The results show that aligning the tree construction method with the corpuss underlying structure is critical for zero-shot performance. For the Biology dataset, which is composed of passages from larger source documents, the bottom-up approach is superior, improving nDCG@10 by over 9 points. We hypothesize that this is because it leverages the inherent part-whole relationships in the data. Conversely, for the TheoT. dataset, which is collection of distinct documents under high-level topic, the top-down approach excels, improving nDCG@10 by nearly 12 points. We hypothesize that this method is better suited to discovering the latent conceptual clusters among independent documents. Table 3: Tree construction comparison. nDCG@10 R@100 Bottom-Up Tree Top-Down Tree 35. 61.82 55.22 67.31 87.53 64.38 47. 73."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This research was supported in part by NSF award #2439754. We would also like to express our gratitude to Divy Thakkar for generously helping with additional gemini-api credits, which were essential for conducting our experiments."
        },
        {
            "title": "REFERENCES",
            "content": "Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Debrup Das, Sam Nuallain, and Razieh Rahimi. Rader: Reasoning-aware dense retrieval models. arXiv preprint arXiv:2505.18405, 2025. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17621777, 2023. Nilesh Gupta, Patrick Chen, Hsiang-Fu Yu, Cho-Jui Hsieh, and Inderjit Dhillon. Elias: End-to-end learning to index and search in large output spaces. Advances in Neural Information Processing Systems, 35:1979819809, 2022. 10 Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121, 2024a. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024b. Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025. Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. Learning to rank in generative retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 87168723, 2024. Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, and Zhicheng Dou. Reasonrank: Empowering passage ranking with strong reasoning ability. arXiv preprint arXiv:2508.07050, 2025. Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, and Jiahai Wang. Diver: multi-stage approach for reasoning-intensive information retrieval. arXiv preprint arXiv:2508.07995, 2025. Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, and Kang Liu. Large language models as foundations for next-gen dense retrieval: comprehensive empirical assessment. arXiv preprint arXiv:2408.12194, 2024. Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs, 2018. URL https://arxiv.org/abs/1603. 09320. Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In International workshop on artificial intelligence and statistics, pp. 246252. PMLR, 2005. Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 14, 2001. Yashoteja Prabhu and Manik Varma. Fastxml: fast, accurate and stable tree-classifier for extreme In Proceedings of the 20th ACM SIGKDD international conference on multi-label learning. Knowledge discovery and data mining, pp. 263272, 2014. Ronak Pradeep, Kai Hui, Jai Gupta, Adam Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Tran. How does generative retrieval scale to millions of passages? arXiv preprint arXiv:2305.11841, 2023. Mandeep Rathee, Sean MacAvaney, and Avishek Anand. Guiding retrieval using llm-based listwise rankers. In European Conference on Information Retrieval, pp. 230246. Springer, 2025. Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. First: Faster improved listwise reranking with single token decoding. arXiv preprint arXiv:2406.15657, 2024. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-Tau Yih, Pang Wei Koh, and Luke Zettlemoyer. ReasonIR: Training retrievers for reasoning tasks. arXiv [cs.AI], April 2025. 11 Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-Yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv [cs.CL], July 2024. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agents, 2024. URL https://arxiv.org/abs/2304.09542. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as differentiable search index. Advances in Neural Information Processing Systems, 35:2183121843, 2022. Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678, 2023. Orion Weller, Michael Boratko, Iftekhar Naim, and Jinhyuk Lee. On the theoretical limitations of embedding-based retrieval. arXiv preprint arXiv:2508.21038, 2025. Hsiang-Fu Yu, Kai Zhong, Jiong Zhang, Wei-Cheng Chang, and Inderjit Dhillon. Pecos: Prediction for enormous and correlated output spaces. Journal of Machine Learning Research, 23(98):132, 2022. Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. Agentic information retrieval. arXiv preprint arXiv:2410.09713, 2024. Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, and Yong Li. survey of large language model empowered agents for recommendation and search: Towards next-generation information retrieval. arXiv preprint arXiv:2503.05659, 2025. Yutao Zhu, Huaying Yuan, Zhengyi Liu, Chenxi Li, Ahmed Awadallah, Haolan Wang, and Ji-Rong Wen. Large language models for information retrieval: survey. arXiv preprint arXiv:2308.07107, 2023."
        },
        {
            "title": "A LIMITATIONS AND FUTURE WORK",
            "content": "Our work introduces novel framework for hierarchical retrieval, but it also presents several avenues for future research. One of the limitation of our current approach is the use of static semantic tree. As demonstrated in our experiments on dynamic corpora, the pre-computed summaries of internal nodes do not update when leaf nodes are filtered, which can occasionally misguide the search. Future work could explore methods for efficient, localized updates to the trees summaries, allowing the hierarchy to adapt to changing corpus without the need for full reconstruction. Second, the offline tree construction process, while one-time cost, can be computationally intensive for extremely large corpora due to the repeated use of LLMs for clustering and summarization. Research into more efficient construction methods, perhaps by combining traditional clustering for the lower levels with LLM-based summarization for only the top, most abstract layers, could further improve scalability. Finally, our traversal algorithm opens up new research directions. The score calibration method, while effective, uses simple linear model. More sophisticated probabilistic models, could be explored for even more robust latent score estimation. Furthermore, while our greedy, best-first traversal is effective in zero-shot setting, the entire process could be framed as reinforcement learning problem, where the search LLM is an agent trained to optimize policy for navigating the tree to maximize retrieval rewards. We believe that exploring these directions will further establish hierarchical, LLM-driven navigation as powerful new paradigm in information retrieval."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 HYPERPARAMETERS This section provides detailed list of all hyperparameters and implementation choices used in our experiments to ensure full reproducibility. B.1.1 OFFLINE TREE CONSTRUCTION Maximum Branching Factor (M ): We set the maximum number of children for any node to = 10 20. Embedding Model (E): We use gecko (Lee et al., 2024b) embeddings to generate vector representations for the clustering steps. Clustering Algorithm (C): Our implementation uses an iterative spectral clustering (Ng et al., 2001) algorithm to partition nodes into at most clusters at each level of the hierarchy. Summarization LLM: We use Gemini-2.5-flash for all summarization tasks (both for internal nodes in the bottom-up method and for the multi-level document summaries in the top-down method). The exact prompt template used is detailed in Appendix D. Top-Down Summary Levels: For the top-down method, we generate 5 levels of hierarchical summaries for each document. B.1.2 ONLINE TRAVERSAL Search LLM (L): We use Gemini-2.5-flash as the search agent that performs the listwise scoring. The prompt structure is provided in Appendix D. Number of Iterations (N ): We run the search for = 20 iterations for all main experiments. Beam Size (B): We use beam size of = 2 for parallel node expansion in each iteration. Path Relevance Momentum (α): The smoothing factor for the path relevance score is set to α = 0.5. Calibration Nodes (l): We augment each leaf slate with ℓ = 10 cross-branch leaf nodes for calibration, based on our ablation study. 13 Reasoning Budget: The default thinking budget for the LLMs reasoning step is set to -1, meaning the model gets to decide how long it wants to thin. MLE Solver: The latent scores are updated after each batch of slate evaluations. The MSE loss is minimized using the Adam optimizer with learning rate of 102 for 100 steps. Usage of LLMs During the preparation of this manuscript, LLM were used as collaborative writing assistant to aid with drafting, refining prose for clarity and conciseness, and structuring arguments; all core ideas, experiments, and analyses were conducted by the authors. B.2 DATASET DETAILS All experiments are conducted on the BRIGHT benchmark (Su et al., 2024), comprehensive collection of 12 datasets designed to evaluate reasoning-intensive retrieval. summary of the statistics for each subset is provided in Table 4. Dataset Subset # Queries Corpus Size (D) Avg. Doc Length StackExchange Biology Earth Science Economics Psychology Robotics Stack Overflow Sustainable Living Coding LeetCode Pony Math AoPS TheoremQA-Q TheoremQA-T 103 116 103 101 101 117 108 142 112 111 194 76 57,359 121,249 50,220 52,835 61,961 107,081 60,792 413,932 7,894 188,002 188,002 23, 83.6 132.6 120.2 118.2 121.0 704.7 107.9 482.6 98.3 250.5 250.5 354.8 Table 4: Statistics for the 12 subsets of the BRIGHT benchmark used in our experiments. The datasets exhibit two key characteristics relevant to our work. First, the StackExchange subsets are composed of passages derived from longer source documents. We leverage this structure for our metadata-based initial clustering in the bottom-up tree construction method. Second, the Coding and Theorem-based datasets (excluding Pony and TheoremQA Theorems) utilize query-dependent corpus, where unique list of documents (often >10k) must be excluded from the search space for each query. This feature, discussed in our main results analysis, poses unique challenge for static index structures like our semantic tree. B.3 TREE CONSTRUCTION B.3.1 BOTTOM-UP The Bottom-up tree constructions algorithms are defined in Alogirthm 2, 3. B.3.2 TOP-DOWN The Top-down tree constructions algorithm is defined in Algorithm 4, the two subroutines used are described below. The SelectSummaryLevel function implements heuristic to find the optimal summary granularity for given set of leaf nodes. It begins with the most abstract summary level (i = 1) and iteratively Algorithm 2 Bottom-Up Tree Construction Vcurrent CreateNodesFromClusters(VL, InitialClusters, V, E) {Summarize the current layer before clustering} for all in Vcurrent do Embeddings {E(ϕ(v)) : VL} Clusters C(Embeddings) Vcurrent CreateNodesFromClusters(VL, Clusters, V, E) 1: Parameters: Corpus D, E, C, Summarize LLM, , Optional InitialClusters 2: Initialize: VL {Node(d) D}, VL, 3: if InitialClusters is provided then 4: 5: else 6: 7: 8: 9: end if 10: while Vcurrent > do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end while 21: vroot NewInternalNode(), ϕ(vroot) 22: C(vroot) Vcurrent 23: {vroot}, {(vroot, c) C(vroot)} 24: return Tree = (V, E) end for Vnext_layer Embeddings {E(ϕ(v)) : Vcurrent} Clusters C(Embeddings) Vnext_layer CreateNodesFromClusters(Vcurrent, Clusters, V, E) Vcurrent Vnext_layer ϕ(v) Summarize({ϕ(c) C(v)}) Vsource: The set of nodes in the layer to be clustered. Clusters: The partition of Vsources embeddings from C. V, E: The global node and edge sets for the tree (passed by reference). Algorithm 3 CreateNodesFromClusters Subroutine 1: function CreateNodesFromClusters(Vsource, Clusters, , E) 2: Input: 3: 4: 5: 6: Initialize: Vnew_layer 7: for all cluster in Clusters do vnew NewInternalNode() 8: C(vnew) {v Vsource K} 9: {vnew} 10: {(vnew, c) C(vnew)} 11: Vnew_layer Vnew_layer {vnew} 12: 13: end for 14: return Vnew_layer checks the number of unique summaries, selecting the first level where the count of unique summaries is sufficient for meaningful clustering (e.g., greater than ) while remaining under maximum token limit for the LLM context. The ClusterLLM function is realized via structured prompt (see 9. The LLM is provided with the list of unique summaries and tasked with grouping them into coherent conceptual clusters. The prompt instructs the model to first generate short, descriptive title for each of the clusters, and then to output mapping from each input summary to one of these cluster titles. The final output is structured object containing the topic descriptions (which become the ϕ(v) for the new nodes) and the mapping. Algorithm 4 Top-Down Divisive Tree Construction i=1. PartitionQueue.enqueue(vroot) 1: Parameters: Corpus D, Summarize LLM, Cluster LLM, Max branching factor 2: Initialize: 3: For each document dl D, generate multi-level summaries {ϕ(vl)i}5 4: VL {Node(d) D}, VL 5: vroot NewInternalNode(), C(vroot) VL 6: {vroot}, {(vroot, c) VL} 7: PartitionQueue new Queue() 8: if VL > then 9: 10: end if 11: while PartitionQueue is not empty do PartitionQueue.dequeue() 12: LeafDescendants GetLeafDescendants(v, ) 13: SelectSummaryLevel(LeafDescendants) 14: UniqueSummaries unique({ϕ(c)i LeafDescendants}) 15: TopicDescs, Mapping ClusterLLM(UniqueSummaries, ) 16: NewChildren 17: for = 1 to do 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end if 29: end for 30: 31: end while 32: return Tree = (V, E) end for ReassignChildren(LeafDescendants, Mapping, NewChildren, T) {(v, c) C(v)} {Disconnect old children} C(v) NewChildren {(v, c) NewChildren} {Connect new children} for all if C(v NewInternalNode(), ϕ(v {v j}, NewChildren NewChildren {v j} in NewChildren do j) > then PartitionQueue.enqueue(v j) j) TopicDescs[j]"
        },
        {
            "title": "C SUBJECTIVE ANALYSIS",
            "content": "C.1 SAMPLE SCORING RESPONSE FROM LLM To provide more intuitive understanding of our method, Figure 3 presents qualitative case study of the search process for real query from the BRIGHT benchmark. The user query is code snippet asking about yaw rotation, complex 3D graphics problem. The figure visualizes the semantic tree and the traversal path taken by LATTICE (highlighted in yellow) to successfully locate relevant document deep within the hierarchy. The expanded callout provides \"glass box\" view into the search LLMs reasoning at critical decision point. The LLMs generated Reasoning explicitly connects the users query to the nodes topic, noting that the user is attempting to perform yaw rotation using quaternion_from_euler. It then performs detailed, comparative evaluation of the children nodes. It correctly identifies Candidate 1 as highly relevant because it discusses support for converting between different 3D rotation representations, including matrices, quaternions, and Euler angles, which directly addresses the users problem. This example demonstrates that our method does not rely on shallow semantic similarity; instead, the search is an active process guided by the LLMs deep, step-by-step reasoning about the query in the context of the corpus hierarchy. 16 Figure 6: Search failing due to dynamically excluded search corpus, red edges denote excluded leaf nodes, gold edges denote ground-truth path C.2 SEARCH FAILURE ON DYNAMIC CORPUS Figure 6 provides qualitative case study of search failure, visually demonstrating the primary challenge our method faces on datasets with dynamic corpus. The figure shows the search tree for random query from the AoPS dataset. Red edges indicate leaf nodes that were dynamically excluded for this specific query, while the yellow path highlights the ideal traversal route to the ground-truth document. As the figure shows, the search agent correctly follows the ground-truth path for the first two levels. However, it then reaches an internal node whose pre-computed summary is now misleading; the summary was generated based on all of its children, including the large number that have since been pruned from the search space (the red nodes). This inaccurate, stale summary causes the search LLM to make an incorrect judgment, deviating from the correct path and ultimately failing to retrieve the relevant document. This example visually confirms the specific failure mode of static hierarchical index when faced with dynamic corpus, reinforcing the quantitative analysis in our main results section."
        },
        {
            "title": "D PROMPTS",
            "content": "17 You are an intelligent search agent navigating hierarchical semantic tree of topics. Your mission is to predict the most promising candidates to find the answer to the users query using the relevance definition below. **Relevance Definition:** {relevance_defintion} --- ## USER QUERY {query} --- ## CANDIDATES Here are the candidates, each is identified by unique node_id provided at the very start in [] (e.g., [0]). {child_node_options} --- ## YOUR EVALUATION TASK 1. First, identify the essential problem in the query. 2. Think step by step to reason about why each candidate is relevant or irrelevant (based on the relevance definition). Provide this analysis in the reasoning field. 3. Rank these passages based on their relevance to the query. Provide your ranking in the ranking field. 4. Assign relevance score from 0 to 100 (based on the relevance definition and the ranking). Provide relevances in the relevance_scores field. --- ## OUTPUT FORMAT You must provide your response as single, clean JSON object. The JSON should have three keys: reasoning , ranking, and relevance_scores. * reasoning: This must be **string**. * ranking: This must be an **array of integers** representing the order of the candidates. * relevance_scores: This must be an **array of arrays** where each inner array contains [node_id, relevance_score]. For example: [[0, 85], [1, 92], [2, 73]]. --- ## YOUR RESPONSE Figure 7: Prompt template used in our experiments for scoring list of nodes for L. 18 You are an expert in information retrieval and keyword generation. Your task is to analyze provided list of informational passages and generate hierarchically sorted list of search keywords for each passage, strictly adhering to the 5-level rubric below. ## Keyword Generation Rules (5 Levels): Level 1: 1-2 Word, Core Subject / Domain (Broadest) Meaning: The absolute fundamental, overarching subject area or discipline. Characteristics: Only 1 to 2 word, very high-level (e.g., \"Technology\", \"Science\", \"History\") Level 2: 3-4 Word, General Topic / Sub-domain Meaning: Narrows Level 1; the specific major topic or branch within the broader field. Characteristics: Only 3 to 4 words, still general but more focused Level 3: 4-6 Word, Key Concepts / Main Themes Meaning: The central ideas, significant concepts, or primary themes directly discussed. Characteristics: Only 4 to 6 words, core messages, primary subjects, often main sections Level 4: 7-10 Word, Very Concise Passage Summary Meaning: very short, concise summary of what the entire passage is about. This should encapsulate the essential idea or purpose of the passage. Characteristics: Only 7 to 10 words Level 5: 11-20 Word, Concise Passage Summary (Most Specific) Meaning: concise summary but more descriptive than level 4 of what the entire passage is about. This should encapsulate the main idea or purpose of the passage. Characteristics: single sentence, 11 to 20 words. ### General Keyword Requirements: - All keywords must be actionable terms or phrases user would realistically search. - Ensure comprehensive coverage of the passages content across all 5 levels. ## Output Format Your output must be single JSON object. This object will contain top-level key: \"passages_keywords\". The value associated with this key must be JSON array. Each element in this array will be an object with two keys: \"passage_id\": An integer that exactly matches the \"id\" from the corresponding input passage. \"hierarchical_keywords\": JSON array of strings of length 5. Each string represents hierarchical level ( Level 1 at index 0, Level 2 at index 1, and so on). ## List of Input Passages: {desc_list} Figure 8: Prompt template used in our experiments for generating multi-level keywords to be used in top-down tree construction. 19 You are an expert data analyst and taxonomist. Your task is to analyze list of keywords and their associated counts which indicate how many that keyword appears in the corpus. ## Goal - Group the following keywords into **k** semantically coherent and **well-balanced** (i.e. each cluster should aim to contain similar weighted count) clusters, where is between [{min_k}, {max_k}]. The primary basis for grouping must be the **topic and meaning** of the keywords. - Use the provided count as measure of each keywords **importance or popularity**. This weight should help you decide which topics are most significant. - Try to always maximize the number of clusters but **without** sacrificing the quality of the clustering, **quality of clustering is paramount**. For every cluster, generate: * descriptive cluster_name. * An information-dense cluster_description summarizing the core themes. * list of all input keywords that constitute this cluster or apply to this cluster. ## Input Data Here is the list of keywords and their importance counts: {keywords_list_with_count} ## Desired Output Format Your final output must be single JSON object, with no other text or explanation. The JSON object must have key: \"clusters\". {{ \"clusters\": [ {{\"name\": \"Name of Cluster 1\", \"description\": \"A very information dense description of the cluster\", \" keywords\": [\"keyword 1\", \"keyword 2\", ...] }}, {{\"name\": \"Name of Cluster 2\", \"description\": \"A very information dense description of the cluster\", \" keywords\": [\"keyword 3\", \"keyword 4\", ...] }}, ... ], }} --- ## Your Response Figure 9: Prompt template used for ClusterLLM to be used in top-down tree construction i.e. clustering given set of keywords into [Mmin, Mmax] clusters. 20 You are an expert AI analyst and summarizer. Your mission is to create highly informative and \" discriminative signpost\" for navigating search agent. This signpost (a summary) must guide the agent to the correct cluster of nodes to answer users query. You will follow strict, step-by-step cognitive process. You must analyze the children nodes in target parent node (the \"Positive Set\"). Prompt ID: {prompt_id} (ignore, this is just for watermarking purposes). ## INPUTS ### POSITIVE SET: Information about the target parent node to be summarized {positive_set_descriptions} --- ## YOUR TASK & OUTPUT FORMAT Your entire output must be single, valid JSON object. Inside this JSON, you will follow the 3-step thinking process outlined below, populating each field as instructed. ### JSON Structure and Instructions: {{ \"detailed_fingerprints\": [ // For EACH children node in the POSITIVE SET (target parent node), extract structured object of its key, queryable facts. {{ \"one_line_summary\": \"...\", // write very information dense and very concise one-line summary for the information contained in this node \"key_entities\": [\"...\"], // List very few key entities which is central to this node \"genre_or_category\": [\"...\"], // List few key genre / categories this node can be classified into \"name\": \"...\", // Name the node }} ], \"common_theme\": \"...\", // Reason deeply what are the common themes between the nodes in the POSITIVE SET \"summary\": \"...\", // Based on step 1 and step 2, write very information dense description of the target node, **make sure to include all key entities**. }} --- ## Your Response Figure 10: Prompt template for generating bottom-up summaries of group of nodes."
        }
    ],
    "affiliations": [
        "Google",
        "UCLA",
        "UT Austin"
    ]
}