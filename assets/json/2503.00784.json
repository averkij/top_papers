{
    "paper_title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
    "authors": [
        "Kai Lv",
        "Honglin Guo",
        "Qipeng Guo",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding."
        },
        {
            "title": "Start",
            "content": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting Kai Lv1*, Honglin Guo1, Qipeng Guo2, Xipeng Qiu1 1Fudan University 2Shanghai AI Laboratory klv23@m.fudan.edu.cn 5 2 0 2 2 ] . [ 1 4 8 7 0 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) exhibit exceptional performance across wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https: //github.com/KaiLv69/DuoDecoding."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated impressive performance across wide range of domains and have been extensively deployed (OpenAI, 2023; Dubey et al., 2024; Yang et al., 2024a; DeepSeek-AI et al., 2024; Cai et al., 2024b). However, their massive parameter sizes and computational requirements pose significant deployment challenges. In particular, the autoregressive generation process (Vaswani et al., 2017) requires complete forward pass of the entire model for each *Work done during internship at Shanghai AI Laboratory. Figure 1: Wall time for draft model autoregressive generation and target model parallel verification of 8 tokens with varying input lengths. The draft phase has become comparable bottleneck to the verification phase, and executing the lightweight draft model on CPU does not compromise generation efficiency. new token sequentially, leading to considerable latency and limiting practical utility. Recent advances in speculative decoding (Chen et al., 2025; Sun et al., 2024; Li et al., 2024b,a; Du et al., 2024) have shown promise in reducing latency without compromising generation quality. Speculative decoding treats the original language model as the target model and employs smaller draft model to speculate the target models output. Each decoding iteration consists of two phases: (1) the draft phase, where the draft model autoregressively generates multiple candidate tokens, and (2) the verification phase, where the target model evaluates all candidate tokens in single forward pass and verifies them through speculative sampling according to the output distribution. This process allows the target model to generate multiple tokens in single forward pass while maintaining its original output distribution (Leviathan et al., 2023). However, the draft model introduces additional computational overhead and the draft phase has emerged as bottleneck comparable to the verification phase (Zafrir et al., 2024; Anonymous, 2024; Liu et al., 2024a). The draft model also introduces undesirable side effects, including increased GPU memory consumption and longer time to first token (TTFT). While several approaches have been proposed to reduce the draft model overhead, they generally rely on heuristics, failing to match the draft quality of draft language models (Saxena, 2023; Fu et al., 2024; He et al., 2024; Zhao et al., 2024). In contrast to previous approaches, we propose deploying the draft model on CPU, which shifts additional computational overhead to CPU without compromising draft quality. key assumption underlying this approach is that the draft model should maintain an acceptable generation speed on CPU; otherwise, the potential acceleration effect would be limited. We provide empirical validation in Figure 1, which demonstrates that the wall time required for auto-regressive generation of 8 tokens by the draft model (Llama-68m) on CPU matches that of the parallel verification of 8 tokens by the target model (Llama-2-7B) on GPU across various sequence lengths. In this paper, we introduce DuoDecoding, hardware-aware heterogeneous speculative decoding method with dynamic multi-sequence drafting. By deploying the draft model and target model on CPU and GPU respectively, we not only shift the overhead of draft model to CPU, but also enable concurrent execution of the draft and verification processes. During this parallel decoding process, we employ hardware-aware optimal draft budget to minimize idle time on either CPU or GPU. When this draft budget becomes high, tokens positioned later in the draft sequence exhibit diminished acceptance rates. To improve the acceptance rate of the draft, we introduce dynamic multi-sequence drafting based on the uncertainty of draft outputs. Additionally, we adapt the verification procedure for our novel speculative decoding process, ensuring that the output distribution of DuoDecoding is consistent with that of the target model. Experimental results across seven different tasks demonstrate that DuoDecoding can significantly reduce the generation latency of LLMs, achieving up to 2.61x speedup. Compared to conventional speculative decoding (Chen et al., 2023; Leviathan et al., 2023), DuoDecoding achieves 17% reduction in time to first token (TTFT). Comprehensive ablation studies demonstrate the contribution of each component in our proposed method. Through detailed analysis, we validate the effectiveness of our uncertainty-based dynamic multisequence drafting strategy."
        },
        {
            "title": "2 Related Work",
            "content": "Speculative Decoding Stern et al. (2018) introduces draft-then-verify generation framework to improve the generation speed of autoregressive models through increasing parallelism. The verification phase of speculative decoding (Chen et al., 2025; Sun et al., 2024) is centered around speculative sampling (Leviathan et al., 2023; Chen et al., 2023). By comparing vocabulary-level probabilities between the draft and target models, this technique achieves higher acceptance rates than conventional rejection sampling while maintaining consistency with the target models output distribution. BiLD (Kim et al., 2024), Medusa (Cai et al., 2024a), and Hydra (Ankner et al., 2024) propose different modified verification strategies, exploring the trade-off between generation speed and distribution fidelity. While our method also incorporates draft-verify mechanism, it innovates by executing draft and target models concurrently rather than sequentially. Draft Overhead Reduction The additional overhead introduced by the draft step has become one of the main bottlenecks in speculative decoding (Zafrir et al., 2024; Anonymous, 2024), leading to series of efforts aiming to address this challenge. Lookahead Decoding (Fu et al., 2024) caches n-grams generated during decoding and employs Jacobian decoding (Santilli et al., 2023) for parallel generation. Ouroboros (Zhao et al., 2024) constructs list of phrases to accelerate the draft models and lengthen the drafts. PLD (Saxena, 2023) retrieves n-grams from the prompt to serve as drafts. REST (He et al., 2024) extends this idea by establishing larger-scale retrieval corpus and using longest prefix matching from the datastore to generate drafts. However, these methods generally yield outputs with lower distribution alignment to the target model compared to dedicated draft models. Most similar to our work, Liu et al. (2024a) utilizes additional GPU resources to distribute the draft overhead. Our work differs by identifying and exploiting the potential of heterogeneous devices, while dynamically adapting the draft process based on available computational resources and draft outputs. Draft Performance Enhancement The draft outputs directly influence the acceptance rate. Typically, improving the alignment between the output distributions of the draft model and the tarAlgorithm 1: DuoDecoding Input: target model Mp, draft model Mq, prefix = [x1, x2, . . . , xn], max generation tokens L, hardware-aware optimal draft budget γ Initialize: empty unverified prefix init_process_group(world_size=2) while x.length < do Forward in parallel on heterogeneous devices Draft Process on CPU: qn, ˆq[s,s/γ], ˆx[s,s/γ] dynamic_drafting(xn, γ) Target Process on GPU: pn Mp(xn) Inter-process communication Synchronization probability via inter-process communication Verification (Details in Algorithm 2) n, x, DuoDecVerify(n, x, x, qn, ˆq[s,s/γ], ˆx[s,s/γ], pn) get model enhances draft performance. DistillSpec (Zhou et al., 2024) and Online Speculative Decoding (Liu et al., 2024b) employ knowledge distillation techniques to enhance distribution consistency. Glide (Du et al., 2024) improves performance by training draft models to reuse the target models KV cache. Eagle (Li et al., 2024b,a) introduces an additional layer in the target model to perform autoregressive generation at the feature level, thereby improving prediction accuracy. SpecInfer (Miao et al., 2023) combines multiple draft models, each fine-tuned collectively, to jointly predict the outputs of target model. These approaches are orthogonal to our method and could potentially be integrated with our framework for complementary benefits."
        },
        {
            "title": "3 Method",
            "content": "As shown in Algorithm 1, the overall process of DuoDecoding can be divided into three stages: parallel execution of the draft model and target model on heterogeneous devices, communication for synchronizing output probabilities, and verification."
        },
        {
            "title": "3.1 Heterogeneous Parallel Decoding",
            "content": "The distinct computational requirements of the target and draft models in speculative decoding naturally lend themselves to deployment across heterogeneous computing devices. For typical server setups, the target model can be placed on the GPU, while the computationally lighter draft model can run on the CPU. This heterogeneous deployment strategy not only alleviates the computational burden on the GPU but also enables concurrent execution of both models. Consequently, it eliminates the sequential dependencies inherent in traditional speculative decoding, enhancing parallelism and addressing the performance bottleneck caused by the draft models overhead. Specifically, in each iteration, both the draft and target models receive identical inputs and run simultaneously. The draft model, running on CPU, autoregressively generates multiple tokens to speculate the target models output. Concurrently, the target model, executing on GPU, validates all draft tokens from the previous iteration and predicts the next token, which serves to verify the draft results in the current iteration. Hardware-aware Drafting Budget In parallel decoding, optimal performance requires balancing the execution time between CPU and GPU operations. Inefficiencies arise when CPU processing either finishes prematurely, suggesting potential for increased drafting length, or extends too long, leading to idle time on the GPU. The optimal drafting budget varies across different hardware configurations due to varying relative speeds between GPU and CPU. We propose to measure the cost coefficient c, defined as the ratio of forward pass time between the target model on GPU and draft model on CPU. By setting the drafting budget γ equal to c, we achieve approximate temporal alignment between draft and target model executions, thereby maximizing hardware utilization efficiency."
        },
        {
            "title": "3.2 Dynamic Multi-Sequence Drafting",
            "content": "As the drafting budget increases, the acceptance rate for tokens in later positions of single drafted sequence tends to decline significantly. Given that each iteration validates at most the first token generated in the current draft, we propose multiAlgorithm 2: Verification Process Input: n, x, x, qn, ˆq[s,s/γ], ˆx[s,s/γ], pn from Algorithm 1 Function DuoDecVerify: Verify the unverified part in the prefix x.length r1, . . . , rl Uniform(0, 1) max({i0 < l, rni > pni qni if x.length == 0 or == 0 then Verify multi-sequence drafts 1 for 1 to do }) ri Uniform(0, 1) if ri < pn/ˆqi,0 then x [x1, x2, . . . , xn] + ˆxk,: + k/γ ˆxs,: return n, x, else norm(max(pn ˆqi,0, 0)) All sequences rejected if == 1 then x [x1, x2, . . . , xn, t] + 1 empty else norm(max(pnk qnk, 0)) [x1, x2, . . . , xnk, t] + 1 empty return n, x, x"
        },
        {
            "title": "3.3 Verification",
            "content": "We design our verification strategy based on speculative sampling (Leviathan et al., 2023; Chen et al., 2023). Since the draft and target models receive identical inputs in each generation iteration, some draft tokens may not be verified within the same iteration they are generated. Therefore, we first verify the draft tokens that were not verified in the previous iteration. This verification process follows the same procedure as speculative sampling. If all draft tokens from the previous iteration have been successfully verified, we proceed with the verification of the first token in the multisequence draft. Verified sequences are then appended to the prefix for the next iteration. If none Figure 2: Dynamic multi-sequence drafting. pi,j represents the probability of the j-th ranked token at the i-th position in the generated sequence. θ = p1,1 p2,1 serves as the threshold. Tokens with probabilities p1,k exceeding the threshold θ will continue to be predicted sequentially, forming independent draft sequence. sequence drafting to maximize the utilization of this validation information. Draft Uncertainty as Proxy Since we cannot know in advance whether draft token will be accepted, we use the predicted probabilities from the draft model as proxy for acceptance rates. Intuitively, higher draft probabilities indicate greater model confidence at that position, suggesting higher likelihood of correct prediction. Draft Sequences Construction The number of draft sequences is dynamically determined based on the draft uncertainty. Figure 2 illustrates the construction process of draft sequences. Let pi,j denote the probability of the j-th ranked token at position in the generation sequence. We use pi,j to approximate the acceptance probability of this token. The threshold θ = p1,1 p2,1 represents the probability of accepting the first two tokens in the sequence with the highest probabilities. To improve acceptance rates in subsequent drafting, we search for tokens at the first position of the generation sequence, whose acceptance rate exceeds the threshold θ to generate the next token. Specifically, tokens with p1,k whose probabilities exceed θ will continue to be predicted sequentially to form an independent sequence. Method c m Vanilla SpS PLD REST Lookahead DuoDec Vanilla SpS PLD REST Lookahead DuoDec MT-Bench Trans Sum QA Math RAG Code Avg. TPS φ TPS φ TPS φ TPS φ TPS φ TPS φ TPS φ TPS φ 44.78 1.00 63.65 1.42 62.66 1.40 58.83 1.31 62.18 1.39 74.73 1. 45.62 1.00 56.74 1.24 55.14 1.26 49.85 1.09 62.57 1.37 66.02 1.45 44.29 1.00 66.77 1.51 95.31 2.15 54.52 1.23 58.72 1.33 73.76 1.67 43.78 1.00 60.08 1.37 48.74 1.11 65.82 1.50 57.67 1.32 65.38 1.49 44.65 1.00 63.38 1.42 66.03 1.48 50.98 1.14 66.70 1.49 68.54 1.54 43.87 1.00 44.98 1.00 44.22 1.00 44.10 1.00 44.31 1.00 56.94 1.30 105.63 2.35 78.65 1.78 96.39 2.19 88.01 1.99 86.18 1.94 100.69 2.28 134.97 3.05 52.29 1.19 106.28 2.36 56.98 1.27 64.45 1.47 54.32 1.23 55.53 1.26 62.17 1.40 61.88 1.40 73.54 1.66 85.22 1.89 47.48 1.08 77.74 1.76 85.84 1.94 139.57 3.18 150.67 3.35 101.67 2.29 139.08 3.15 44.03 1.00 66.50 1.51 75.21 1.71 60.76 1.38 55.81 1.27 73.12 1. 39.99 1.00 49.73 1.24 79.84 2.00 53.31 1.33 45.49 1.14 92.58 2.32 45.10 1.00 67.57 1.50 55.81 1.24 69.93 1.55 59.34 1.32 72.00 1.60 44.61 1.00 63.53 1.42 65.56 1.47 58.67 1.32 60.43 1.35 70.51 1.58 43.75 1.00 44.76 1.00 78.85 1.80 76.57 1.71 91.89 2.10 83.00 1.85 60.02 1.37 73.41 1.64 68.33 1.53 65.67 1.50 89.52 2.00 114.13 2.61 Table 1: Performance comparison across different tasks and models. We report tokens per second (TPS) and speedup ratio (φ) relative to vanilla autoregressive generation for both Vicuna-7b-v1.5 and Llama2-7b models. Higher values indicate better performance. The best results are highlighted in bold. of the sequences are verified, we sample token from the normalized distribution and append it to the prefix for the next iteration. Decoding (PLD) (Saxena, 2023), Retrieval-based Speculative Decoding (REST) (He et al., 2024), and Lookahead Decoding (Fu et al., 2024)."
        },
        {
            "title": "4.1 Setup",
            "content": "Tasks To comprehensively evaluate our methods effectiveness across different scenarios, we conduct experiments on seven diverse task categories. We incorporate the widely-adopted SpecBench (Xia et al., 2024) and extend our evaluation to code generation. Specifically, we assess performance on multi-turn dialogue generation using MTbench (Zheng et al., 2023), machine translation using WMT14 DE-EN (Bojar et al., 2014), summarization using CNN/Daily Mail (Nallapati et al., 2016), question answering using Natural Questions (Kwiatkowski et al., 2019), mathematical reasoning using GSM8k (Cobbe et al., 2021), and retrieval-augmented generation using Natural Questions with concatenated 5 Wikipedia documents (Karpukhin et al., 2020). Additionally, we evaluate code generation capabilities using HumanEval (Chen et al., 2021). Models Following SpecBench (Xia et al., 2024), we employ Vicuna-7b-v1.5 (Chiang et al., 2023) as the target model and Vicuna-68m (Yang et al., 2024b) as the draft model. To assess acceleration performance on base models, we also evaluate our method on Llama2-7b (Touvron et al., 2023). Baselines In addition to vanilla autoregressive generation, we compare DuoDecoding against four representative methods: Speculative Decoding (SpS) (Leviathan et al., 2023), Prompt Lookup Hardware and Implementation Details All experiments are conducted on single A800 GPU and 16-core Intel Xeon CPU. Our implementation primarily builds on the transformers library (Wolf, 2019), with CPU inference implemented through the Python interface (Abetlen, 2023) of llama.cpp (Gerganov, 2023). Models on GPU run in FP16 precision, while models on CPU run in the GGUF format with Q5_K_M quantization (GGML-org)."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 presents the comprehensive evaluation results of DuoDecoding and baseline methods. We report tokens per second (TPS) and the speedup ratio (φ) relative to vanilla autoregreesive generation. DuoDecoding demonstrates consistent superior performance across all tasks and model architectures. For Vicuna-7b-v1.5, our method achieves an average speedup of 1.58, outperforming all baseline methods. The improvement is particularly notable in MT-bench and summarization tasks, where DuoDecoding reaches 1.67 speedup. While PLD shows strong performance in summarization (2.15), it exhibits significant performance variations across different tasks. Other baselines like SpS and Lookahead demonstrate moderate but stable improvements, with average speedups of 1.42 and 1.35 respectively. The advantages of DuoDecoding become even more pronounced when applied to Llama2-7b, achieving remarkable average speedup of 2.57. Figure 3: Comparison of Time to First Token (TTFT) across different tasks and models. The y-axis shows the relative TTFT normalized by vanilla autoregressive generation. Lower values indicate better latency performance. The method shows exceptional effectiveness in mathematical reasoning (3.31), question answering (3.18), and translation (3.00) tasks. This substantial improvement over Vicuna-7b-v1.5 suggests higher consistency between the output distributions of draft and target models. The enhanced performance also indicates that employing more capable draft model could potentially yield even higher speedup ratios. While baseline methods like PLD and SpS also show stronger performance on Llama2-7b (2.10 and 1.80 respectively), they still fall significantly short of DuoDecodings consistent high performance across all tasks. These results demonstrate that DuoDecoding not only provides superior acceleration but also maintains consistent performance across different tasks, addressing the stability limitations observed in existing methods."
        },
        {
            "title": "4.3 Time to First Token",
            "content": "While overall generation latency improvements are crucial, the latency of producing the first token (TTFT) is equally important for real-world applications, especially in interactive scenarios. We compares the relative TTFT of DuoDecoding against Speculative Decoding across different tasks in Figure 3. Both Vicuna-7b-v1.5 and Llama2-7b exhibit similar trends across different tasks, demonstrating the consistent behavior of these acceleration methods. DuoDecoding maintains lower TTFT overhead compared to Speculative Decoding across all tasks, with an average relative TTFT around 1.3-1.4. In contrast, Speculative Decoding shows higher latency overhead with relative TTFT ranging from 1.5 to 1.7. On average, DuoDecodings #Seq MT Trans Math Code a 1 2 3 Dynamic 98.71 102.30 101.22 101.67 131.15 118.48 113.42 139.08 149.68 136.00 124.76 150.67 89.06 84.15 80.54 89.52 Table 2: Impact of different sequence drafting strategies. TTFT is approximately 83% of Speculative Decodings TTFT, representing significant improvement in initial response time. The superior TTFT performance of DuoDecoding stems from its efficient utilization of heterogeneous computing resources, allowing parallel forward passes of both draft and target models. In contrast, Speculative Decoding requires sequential execution of these operations. However, its worth noting that DuoDecoding still exhibits higher TTFT compared to vanilla autoregressive generation, primarily due to the additional system overhead and verification operations required before generating the first token. Moreover, the draft model may take longer than the target model during parallel decoding."
        },
        {
            "title": "4.4.1 Ablation Study",
            "content": "conduct ablation using experiments We Llama-2-7b on multi-turn dialogue, translation, mathematical reasoning, and code generation tasks, and report the number of tokens generated per second (TPS). Dynamic Multi-Sequence Drafting Table 2 presents comparative analysis of different seBudget MT Trans Math Code γ 2 γ 1 γ γ + 1 γ + 2 97.44 97.77 98.71 98.08 98.38 128.65 130.75 131.15 130.90 130.37 145.32 147.90 149.68 147.12 148.60 87.47 89.00 89.06 88.00 87.11 Table 3: Impact of different drafting budgets. γ represents the optimal hardware-aware budget. quence drafting strategies. Our dynamic sequence drafting method consistently outperforms other approaches across all four tasks, highlighting its effectiveness. For static sequence numbers, no single configuration emerges as universally optimal across all tasks. The sequence number of 1 generally performs well, due to its ability to maximize draft length under the fixed computational budget. Increasing the sequence number will reduce the available draft length for each sequence. However, MTbench presents an exception where sequence numbers greater than 1 show better performance. This can be attributed to cases where the draft models initial predictions are incorrect. In such scenarios, multiple shorter sequences are more advantageous than single longer sequence, as they allow for more diverse speculation paths. These findings support the necessity of dynamically adjusting the sequence number. Different contexts and generation stages may benefit from varying sequence numbers, and our dynamic approach successfully adapts to these changing requirements, leading to the best performance across diverse tasks among these sequence drafting strategies. Hardware-aware Drafting Budget Table 3 investigates the effect of varying the drafting budget around our hardware-aware optimal value, γ. To isolate the impact of drafting budget adjustments and eliminate confounding factors, we fix the number of draft sequences to 1 across all experiments. The hardware-aware budget γ consistently delivers the best performance across all tasks. When the budget is lower than γ, the draft model finishes generation early and remains idle, missing the opportunity to generate more tokens. Conversely, when the budget exceeds γ, the target model experiences idle time while waiting for the draft model to complete its generation. In both Figure 4: Profiling of time and number of processed token in one generation iteration for different decoding strategies. cases, suboptimal resource utilization leads to decreased performance. While the performance differences between various budget settings are relatively slight, this is primarily due to our baseline γ being set to 24 tokens. Variations of 1 or 2 tokens represent small proportional changes relative to this substantial base value."
        },
        {
            "title": "4.4.2 Profiling",
            "content": "We provide detailed profiling of the time and number of processed tokens per iteration for different decoding strategies in Figure 4. Autoregressive generation shows the lowest iteration time but processes only one token per iteration. This represents the baseline approach with minimal overhead but high average latency. Speculative decoding demonstrates significantly higher iteration time, with substantial portion consumed by the draft model. While this increases the total processing time, it enables the handling of 5 tokens per iteration, significantly reducing the average latency. The verification and communication overhead are negligible. DuoDecoding improves efficiency by parallel execution of draft and target models. While maintaining similar iteration time to autoregressive generation, it dramatically increases the number of processed tokens to 24 per iteration. The draft model execution overlaps with the target models computation, effectively eliminating the additional time overhead seen in speculative decoding. As with speculative decoding, the verification and communication costs remain minimal. Pred Actual =1 >1 Total =1 >1 Total 0.348 0.230 0.578 0.087 0.335 0. 0.435 0.565 1.000 Table 4: Confusion matrix of acutal and predicted sequence number. 4.4.3 Sequence Number Prediction In Table 4, we analyze the prediction accuracy of our dynamic sequence drafting strategy. Specifically, \"Actual\" refers to whether the top-1 probability draft sequence in the decoding process is accepted, while \"Pred\" indicates whether the number of sequences used in DuoDecoding exceeds 1. Since we compare our dynamic multi-sequence drafting strategy against static single-sequence drafting (sequence number = 1), we specifically focus on cases where predicted and actual sequence numbers are equal to or greater than 1. The analysis shows that in 56.5% of cases where the sequence number is 1, the entire prediction sequence would be rejected. This indicates significant proportion of cases where speculative prediction requires drafting more diverse tokens starting from the first position to improve the acceptance rate. For predictions of sequence numbers greater than 1, we achieved good prediction accuracy: only 8.7% of cases were incorrectly predicted (i.e., should have been 1), while in 33.5% of cases, we accurately predicted the opportunity to use multiple sequences, allowing us to explore more tokens. This low false positive rate not only effectively reduces computational resource waste but also provides substantial opportunities for acceleration. Together with our previous experimental analysis, our dynamic multi-sequence drafting strategy effectively balances accuracy and efficiency, successfully improving overall performance."
        },
        {
            "title": "4.4.4 Sequence Number Distribution\nWe evaluated the distribution of the number of se-\nquences during actual execution on two tasks,Math\nand Translation, in Figure 5. In both tasks, the case\nwhere the sequence number is 1 occurs much more\nfrequently than other sequence numbers.",
            "content": "The distribution of sequence numbers differs significantly between the two tasks. In the Math task, the majority of sequences have sequence number of 1, while in the Translation task, this occurs Figure 5: Distribution of sequence numbers during generation process in DuoDecoding. much less frequently, with larger proportion of cases using higher sequence numbers. We attribute this difference to the generation process of the two tasks. In the Math task, many sequences involve the repetition of numbers and previously mentioned names or entities, allowing the draft model to predict most tokens with high probability. In contrast, the Translation task involves more distinct inputoutput pairs, with translations of the same sentence showing greater diversity. This increased variability leads to higher uncertainty, necessitating the use of more sequences."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose DuoDecoding, heterogeneous speculative decoding method designed to reduce the draft overhead inherent in conventional speculative decoding. We strategically deploy the draft and target models on CPU and GPU respectively, enabling parallel execution. We minimize the idle time caused by mutual waiting between the CPU and GPU with hardware-aware optimal drafting budget. Additionally, we propose to draft with dynamic multi-sequences to enhance the quality of the draft. Extensive experiments across multiple tasks demonstrate that DuoDecoding consistently achieves lower generation latency compared to baseline methods. Further ablation and analysis confirm the effectiveness of each component of our approach. We hope that our work will inspire further research on leveraging heterogeneous resources for language model inference, as the speculative decoding framework provides promising solution for collaborative inference."
        },
        {
            "title": "Limitations",
            "content": "Our work has several limitations. First, since speculative decoding primarily focuses on reducing generation latency, we did not explore the performance of different methods under large batch sizes. Second, although we tested the performance across both base and chat models, our experiments were limited to target models with 7B parameters, and the effectiveness of our approach on larger models remains unexplored. Finally, due to hardware constraints, our evaluations were conducted on single hardware configuration, and the performance characteristics on different computing platforms remain to be investigated."
        },
        {
            "title": "References",
            "content": "Abetlen. 2023. llama-cpp-python: Python bindings for llama.cpp. Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. 2024. Hydra: Sequentially-dependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109. Anonymous. 2024. Designing draft models for speculative decoding. In Submitted to ACL Rolling Review - April 2024. Under review. Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve SaintAmand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1258, Baltimore, Maryland, USA. Association for Computational Linguistics. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024a. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Xiaomeng Zhao, and et al. 2024b. Internlm2 technical report. CoRR, abs/2403.17297. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Zhuoming Chen, Avner May, Ruslan Svirschevski, YuHsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. 2025. Sequoia: Scalable and robust speculative decoding. Advances in Neural Information Processing Systems, 37:129531129563. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, Tao Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, and Xiaowen Sun. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434. Cunxiao Du, Jing Jiang, Yuanchen Xu, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, and Yang You. 2024. Glide with cape: low-hassle method to accelerate speculative decoding. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Georgi Gerganov. 2023. llama.cpp: Llm inference in c/c++. GGML-org. ggml: Tensor library for machine learning. Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. 2024. REST: retrieval-based speculative decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 1582 1595. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 67696781. Association for Computational Linguistics. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. 2024. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via specIn International Conference on ulative decoding. Machine Learning, pages 1927419286. PMLR. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024a. EAGLE-2: faster inference of language models with dynamic draft trees. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 74217432. Association for Computational Linguistics. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024b. EAGLE: speculative sampling reIn Fortyquires rethinking feature uncertainty. first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of LLM inference using lookahead decoding. In Forty-first International Conference on Machine Learning, ICML Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, and Winston Hu. 2024a. Parallel speculative decoding with adaptive draft length. arXiv preprint arXiv:2408.11850. Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024b. Online speculative decoding. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. 2023. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çaglar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280290. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. 2023. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1233612355. Association for Computational Linguistics. Apoorv Saxena. 2023. Prompt lookup decoding. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31. Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. 2024. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. In First Conference on Language Modeling. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Wolf. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 76557671. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024a. Qwen2.5 technical report. CoRR, abs/2412.15115. Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. 2024b. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706. Ofir Zafrir, Igor Margulis, Dorin Shteyman, and Guy Boudoukh. 2024. Fastdraft: How to train your draft. arXiv preprint arXiv:2411.11055. Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2024. Ouroboros: Speculative decoding with large model enhanced drafting. arXiv preprint arXiv:2402.13720. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. 2024. Distillspec: Improving speculative decoding via knowledge distillation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net."
        },
        {
            "title": "A Experimental Details",
            "content": "Template For Vicuna-7B-v1.5, we used the official template. For Llama-2-7B, the templates we used are as follows. MT-Bench chat between curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. USER: {{QUESTION}} ASSISTANT: Translation Translate German to English. German: {{QUESTION}} English: Summarization Summarize: {{QUESTION}} TL;DR: QA chat between curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. USER: {{QUESTION}} ASSISTANT:"
        },
        {
            "title": "Math",
            "content": "{{QUESTION}} Lets think step by step."
        },
        {
            "title": "RAG",
            "content": "A chat between curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. USER: {{QUESTION}} ASSISTANT:"
        },
        {
            "title": "Code",
            "content": "{{QUESTION}} included Datasets The in datasets SpecBench (Xia et al., 2024) are the same 80 samples as those used in SpecBench. For Humaneval (Chen et al., 2021), we use the full set of 164 samples."
        },
        {
            "title": "B License for Scientific Artifacts",
            "content": "In this research, we strictly adhere to the license terms of all utilized datasets and models. All resources are publicly available and our usage complies with their original intended purposes and license scopes."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory"
    ]
}