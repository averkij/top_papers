{
    "paper_title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?",
    "authors": [
        "Wenxuan Shen",
        "Mingjia Wang",
        "Yaochen Wang",
        "Dongping Chen",
        "Junjie Yang",
        "Yao Wan",
        "Weiwei Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 4 6 3 0 . 8 0 5 2 : r ARE WE ON THE RIGHT WAY FOR ASSESSING DOCUMENT RETRIEVAL-AUGMENTED GENERATION? Wenxuan Shen1, Mingjia Wang2, Yaochen Wang2, Dongping Chen3, Junjie Yang1, Yao Wan2, Weiwei Lin1 1South China University of Technology, 2Huazhong University of Science and Technology, 3University of Maryland https://double-bench.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce DOUBLE-BENCH: new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 singleand multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source DOUBLE-BENCH provide rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis."
        },
        {
            "title": "INTRODUCTION",
            "content": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has emerged as transformative technique in textual information retrieval, enhancing Large Language Models (LLMs) by retrieving the most relevant content from knowledge bases in response to queries. This approach has driven significant advances in context engineering, particularly for knowledge-intensive NLP tasks. Besides text-only scenarios, vision documentsincluding scanned files (Breci et al., 2024), charts (Masry et al., 2022), and slides (Tanaka et al., 2023)serve as rich information sources that have traditionally required substantial manual effort to examine. These documents are now being efficiently enhanced and handled through multimodal document RAG systems, enabling advanced document understanding (Faysse et al., 2024; Wang et al., 2025; Cho et al., 2024). Despite the growing importance of document RAG systems (Mortaheb et al., 2025a; Yu et al., 2024; Mortaheb et al., 2025b), effectively evaluating them in detail presents significant challenges. ExistEqual Contribution, Project Lead, Corresponding Authr 1 ing document RAG evaluation benchmarks (Friel et al., 2024) suffer from four critical shortcomings as we identified through pilot experiments shown in Figure 1: (1) Limited evaluation scope: Current benchmarks only focus on specific parts such as embedding model or VQA model, failing to reveal the bottlenecks of current RAG system in holistic and comprehensive way. (2) Unrealistic prior knowledge assumptions: Many VQA-style benchmarks (Li et al., 2024; Wu et al., 2025) presuppose that the target page or document is already known, making queries inappropriate for evaluating real-world global retrieval scenarios. (3) Ambiguous or non-unique evidence: Synthetic queries are often crafted from single page and assume one-to-one mapping between query and evidence, neglecting cases where multiple pages could be relevant. (4) Unlinked multi-hop compositions: Synthesized multi-hop queries are frequently constructed from loosely connected single-hops, failing to evaluate retrieval models ability on multi-hop reasoning across documents and modalities. To overcome these limitations, we introduce DOUBLE-BENCH (DOcument Unified Broadcoverage Logical hops Evaluation Benchmark), consisting of 5,168 human-validated singleand multi-hop queries and 3,276 documents in 6 languages and 4 types of document data. First, we assemble and preprocess large diverse document corpus spanning PDFs, scanned documents, slides and web pages by two-stage filtering and modality decomposition. Next, we synthesize and rigorously label both singleand multi-hop queries (Zhang et al., 2025b; Tang & Yang, 2024), using combination of iterative clarity-oriented refinement, knowledge-graph-guided composition and exhaustive evidence search. Finally, expert annotators review (Chiang et al., 2024; Chen et al., 2024a) and correct machine-assigned evidence to ensure high-precision ground truth for largescale realistic multimodal retrieval evaluation. As shown in Figure 2, DOUBLE-BENCH contains high quality queries with low ambiguity. To avoid potential data contamination issues, DOUBLEBENCH is also designed to support streamlined dynamic updates with minimal human intervention. Figure 1: Existing document RAG benchmarks suffer from ambiguous queries that are insufficiently specified to retrieve relevant results, failing to authentically evaluate current document retrieval models and systems. Based on DOUBLE-BENCH, we conduct extensive experiments across 9 state-of-the-art textual, visual and multimodal embedding models, 4 MLLMs and 4 advanced document RAG frameworks. Our findings demonstrate that text embedding models are narrowing the gap with visual embedding models, and Colqwen2.5-3b achieve strong performance with an 0.795 averaged hit@5 score. Regarding different languages, retrieval models perform generally better on high-resource language than low-resource like Arabic and French. Regarding different document types, clean and structured documents, such as PDFs and HTML pages, are generally easier for models to inspect. Moreover, MLLMs low accuracy across both singleand multi-hop queries demonstrates the inherent challenges in multimodal long document understanding. Multi-hop queries prove particularly challenging for current document RAG frameworks, achieving only 0.655 accuracy even when ground truth pages are directly provided. In summary, our contributions are three-fold: We diagnose several major limitations in existing document RAG evaluation, including incomplete scope, unrealistic prior knowledge assumptions, ambiguous or non-unique evidence, and non-grounded multi-hop query design. We introduce DOUBLE-BENCH, the first-of-its-kind live evaluation system for multilingual and multimodal document RAG system, featuring diverse document corpus, fine-grained page decomposition, and high-quality singleand multi-hop QA pairs with manually labeled evidence. Our experiments across several state-of-the-art embedding models, MLLMs, and document RAG system uncover critical limitations in current RAG frameworks, providing insights and findings for the research community. Figure 2: Our pilot study reveals critical limitations in existing document RAG system benchmarks, which make them unable to reliably assess current system in fine-grained and realistic manner."
        },
        {
            "title": "PILOT STUDY",
            "content": "2.1 TASK FORMULATION 1, pi 2, ..., pi Let be large corpora consisting of documents {d1, d2, ..., dn}. Each document di is stored by page images {pi m}. Given query Q, the objective is to retrieve top-k possible evidence pages Er from the entire corpora to formulate the answer A. For single-hop queries Qs, answer can be found if one or more evidence pages from the evidence set Eq is successfully retrieved. For multi-hop queries Qm, the requirement extends to having one or more evidence page for every evidence set Eq,j of each query hop j. These conditions are formated in Equation (1) and (2). (Er Eq = ) = Enable(AQs, Er) (Er Eq,j = ) = Enable(AQm, Er) (cid:94) j= (1) (2) 2.2 FOUR MAJOR OVERLOOKED ISSUES FOR DOCUMENT RAG EVALUATION Though practical document RAG scenarios typically involve queries that clearly state an informational need, the core utility of RAG is demonstrated when users do not possess specific prior knowledge about individual documents, such as their titles, filenames, or the precise location of some document component, even if users have general familiarity with the topics within the collection. Therefore, we start by investigating whether existing benchmarks are fully appropriate for evaluating real-world document RAG scenarios. By screening existing benchmarks with concrete rules, we illustrate these limitations in Figure 2. Current benchmarks fail to comprehensively evaluate document RAG systems with finegrained breakdown. As shown in Table 1, current document-related benchmarks usually focus on embedding-based retrieval models (Mace et al., 2025) or response models (Ma et al., 2024), which are only components within document RAG systems, failing to provide comprehensive 3 Table 1: Comparison between existing multimodal document related benchmarks and the proposed DOUBLE-BENCH, where each symbol represents: PDFs; Scanned documents; (cid:158) Slides; (cid:128) HTML pages. Half-tick denotes dependent on the specific benchmark component or insufficient evaluation. GT: Ground Truth evidence lables. M.H.: Multi-Hop. Lang.: Suppoorted language number. Dyna.: Support dynamic benchmark update when datapoint are contamintaed. Benchmarks DocVQA MMLongbench-Doc MMDocIR UDA-QA ViDoRe v1 ViDoRe v2 ViDoSeek REAL-MM-RAG DOUBLE-BENCH Size Queries Labels Evaluation Target Document Doc Avg. #Pages Query 6,071 135 6,818 2,965 5,000 65 1,142 163 3,276 1.0 47.5 65.1 46.3 1.0 48.6 18.4 49.1 22.3 50,000 1,082 73,843 29,590 500 913 1,142 4,553 5, Clarity i.i.d. M.H. GT M.H. Chain - - - - Embed Model MLLMs RAG System Lang. Dyna. Type 1 1 1 1 2 2 1 6 P (cid:158) (cid:158) (cid:158) (cid:158) (cid:128) (cid:158) (cid:128) assessment. This fragmented evaluation obscures critical interaction effects between retrieval and generation components that often determine real-world system performance. Benchmark design issues with prior knowledge assumptions. VQA benchmarks, such as MMLongbench-Doc (Ma et al., 2024), DocVQA (Mathew et al., 2021), and MMDocIR (Dong et al., 2025), are inherently designed with given page or document as prior knowledge, rendering their queries ambiguous for identifying the ground-truth page within global document corpus. Manually inspected benchmarks designated for RAG, such as ViDoSeek (Wang et al., 2025) and MRAMGBench (Yu et al., 2025), have significant gains in query information. However, such benchmarks tend to insert the exact name or page of the ground document, failing to align with intended applications where users do not have any specific prior knowledge over individual documents. Such queries create gaps between evaluation and real use. Queries with multiple interpretations and scattered evidence. Most benchmarks construct queries by selecting ground truth page beforehand, and assume the evidence used is unique (Chen et al., 2024d; Tang & Yang, 2024). This generally holds true when the corpus is small, such as individual benchmarks in ViDoRe (Faysse et al., 2024), but the problem becomes uncontrolled when the corpus scales up. Some queries may also have unexpected multiple interpretations given different content in the same document, which further undermines the unique assumption. The linearity in multi-hop query synthesis is overlooked. The inclusion of trivial multi-hop queries in evaluations may overstate the reasoning capabilities of RAG frameworks, inflating perceived performance without accurately assessing genuine multi-step reasoning. These queries are essentially simple linkings of independent parts, do not necessitate complex reasoning to deconstruct and can be processed in parallel (Hui et al., 2024)."
        },
        {
            "title": "3 DOUBLE-BENCH: THE BENCHMARK",
            "content": "To address existing limitations, we introduce DOUBLE-BENCH, benchmark with manually verified multi-modal, multi-lingual content and an automatic benchmark construction suite via three-stage pipeline shown in Figure 3. Detailed benchmark statistics can be found in Figure 4 and Appendix. We also provide extensive metadata, such as queried modality, language, evidence chains/lists and parsed page chunks to advance document RAG research community. 3.1 METADATA COLLECTION AND PREPROCESSING This section details the preprocessing steps applied to the raw document corpus: (1) Large Corpus Collection, (2) Two-stage Filtering, and (3) Modality Split. Metadata Collection. To ensure comprehensive evaluation, we collect diverse range of document types and languages. The initial database comprises four popular types of documents collected from various sources. As shown in Figure 4, we include high-quality PDF files, scanned documents, slides, and HTML pages to ensure the diversity coverage of our raw data: 4 Figure 3: Overview of the DOUBLE-BENCH construction pipeline. The preprocessing stage filters the collected corpora and splits the content by modality. To alleviate identifed problems explained in Figure 2, an iterative clarity-oriented refinement pipeline is introduced for single-hop query generation, while knowledge graphs are additionally constructed to assist multi-hop query generation. All document pages are thoroughly checked by annotators to produce list-of-evidence and set-ofevidence labels. PDFs: Includes high-quality PDFs from DocVQA (Mathew et al., 2021), MMLongBench-Doc (Ma et al., 2024), and Arxiv papers published by reputable organizations after 2023, all known for rich context and multimodal content. Scanned Documents: Printed or handwritten documents sampled from DocVQA (Mathew et al., 2021), MMLongBench-Doc (Ma et al., 2024), and the CommonCrawl corpus. Scanned content presents greater challenges for models due to defects introduced by the document creation and scanning process, e.g., variability in font styles, layout irregularities, background noise. Slides: subset from SlideVQA (Tanaka et al., 2023), augmented with multilingual slides from the Commoncrawl corpus. Collected slides are scanned with Qwen2.5-VL (Bai et al., 2025), and only those with at least 30% multimodal pages remain. HTML Pages: 600 Wikipedia entries, randomly crawled across various topics, with equal nonoverlapping samples per language. Filtering and Preprocessing. Our preprocessing pipeline begins with coarse-grained, rule-based filter (Chen et al., 2024b; Pu et al., 2025) to select documents that meet basic structural and language requirements. GPT-4o (OpenAI, 2024) reads the first three pages of every document to determine the primary language. Only documents with 10 to 50 pages and primary language listed in Figure 4 are retained. Following this initial selection, each document page undergoes modality split, where it is parsed and decomposed into its constituent text, table, and figure components using tools like Docling (Livathinos et al., 2025) and MinerU (Wang et al., 2024). Finally, we apply fine-grained content filter where parsed chunks are reviewed with their adjacent context to ensure semantic coherence, filtering out any content that is irrelevant or lacks meaningful information for query generation. 3.2 SINGLE-HOP QUERY SYNTHESIS Principles. Single-hop VQA queries often lack enough detail for precise document retrieval. We enhance them by adding supportive descriptions, making queries self-contained, focused on key 5 Table 2: Overview of our dataset statistics, grouped by document type. Document Type Doc Count Avg. Length 1-hop Qs 2-hop Qs 3-hop Qs Text Modality Table Modality Figure Modality PDFs Scanned Doc Slides HTML Pages Total 1435 386 683 772 3276 22.28 21.49 21.94 22. 22.25 1109 136 299 956 2500 188 49 86 256 579 900 229 261 2083 1453 271 371 1158 3253 915 236 222 428 1801 329 36 139 619 unimodal information, and diverse in type including factual and analytical. This produces robust queries for evaluating single-hop retrieval. Synthesis Process. The Single-hop query synthesis process goes through an iterative refinement process to ensure clarity. Provided parsed page component obtained by preprocessing, we leverage GPT-4o to formulate an initial query based on the following four principles: (1) Query self-containment; (2) Target on most significant unimodal content; (3) Prohibition of explicit source eeferencing; (4) Variety and naturality in queries. This initial query is then validated against the corpus. Two high-performance embedding models of different modalities, colqwen and qwen3-embedding, are used to retrieve the top-10 candidate pages each. Subsequently, Qwen2.5-VL-32B processes the merged candidates seperately to identify all ground truth pages containing direct answer. If more than five ground truth pages are found, the model is prompted to refine the query by incorporating distinguishing detail extracted from one of the identified ground truth pages. This validation and refinement loop continues until the query yields five or fewer ground truth pages, ensuring the overall difficulty. 3.3 MULTI-HOP QUERY SYNTHESIS Principles. While multi-hop queries benefit from information across hops, which mitigates the lack of information problem, their direct generation by LLMs is challenging, even with techniques like Chain-of-Thought or inference scaling. Problems in current synthetic multi-hop benchmarks, such as trivial connections and non-realistic intents, have undermined accurate evaluations of RAG frameworks. Our multi-hop query synthesis pipeline addresses this by using knowledge graphs and intent-driven walks. This approach simplifies sub-query linking by replacing key entities with new sub-queries, forming linearly combined queries. Synthesis Process. Our multi-hop query synthesis process uses knowledge graph-based approach to generate complex reasoning chains. We construct knowledge graphs for each document using LightRAG (Guo et al., 2024), ensuring extracted relationships uniquely identify target entities to prevent sub-query ambiguity. An LLM agent selects an initial node from high-degree entities, infers query intent based on the node and document summary, then performs guided graph walk by iteratively selecting neighbors that best align with the inferred intent. The final multi-hop question is built iteratively along this path: for each step, sub-query is generated and nested within the cumulative query by identifying the queried entity, replacing it with the sub-query, and rearranging for natural flow. This transforms simple entity-relation path into grammatically natural, complex question requiring sequential reasoning to answer. 3.4 POST-PROCESSING Figure 4: Statistics of the DOUBLEBENCH dataset. See Appendix for more details. Query Quality Inspection. The query drafting module strives to generate high-quality queries, but quality inspection is required to ensure all criteria are met. For single-hop queries, the checklist reviews all generation requirements. For multi-hop queries, we create separate checklist, assessing: (1) Final question quality (clarity, specificity, no explicit final answer); (2) Logical necessity and 6 Table 3: Retrieval accuracy of state-of-the-art text and multimodal embedding models across query types, showing performance degradation as reasoning complexity increases. Model Average Single Hop 2-Hop 3-Hop hit@1 hit@ hit@5 hit@1 hit@3 hit@5 hit@1 hit@ hit@5 hit@1 hit@3 hit@5 Qwen3-Embedding-4B NV-Embed-v2 gte-Qwen2-7B-instruct bge-m3 colqwen2.5-3b-multilingual vdr-2b-multi jina-embeddings-v4 gme-Qwen2-VL-7B-Instruct colpali-v1. 0.489 0.443 0.404 0.355 0.533 0.463 0.451 0.428 0.403 0.699 0.650 0.611 0.525 0.727 0.648 0.641 0.614 0.571 Text Embedding Models 0.726 0.626 0.585 0. 0.852 0.756 0.749 0.648 0.886 0.796 0.804 0.695 0.776 0.724 0.697 0.591 0.314 0.333 0.288 0.180 Visual & Multimodal Embedding Models 0.795 0.725 0.720 0.697 0. 0.778 0.688 0.671 0.638 0.584 0.865 0.813 0.804 0.775 0.679 0.895 0.847 0.844 0.822 0.717 0.326 0.283 0.264 0.249 0.230 0.598 0.604 0.503 0.366 0.622 0.491 0.468 0.472 0. 0.663 0.689 0.603 0.428 0.693 0.589 0.570 0.579 0.525 0.235 0.240 0.205 0.182 0.277 0.225 0.222 0.208 0.220 0.531 0.526 0.466 0.412 0.579 0.482 0.479 0.449 0. 0.668 0.641 0.588 0.502 0.696 0.606 0.603 0.570 0.588 correctness of intermediate reasoning steps; (3) Uniqueness of step answers and rigor of relations; and (4) Significance and relevance of the overall query. Queries failing any criteria are discarded. Evidence Labeling. For each query, we locate all ground truth by thoroughly searching each page within the document. Pages are marked as evidence only if they directly provide or lead to the answer. We provide set-of-evidence labels for single-hop queriesa set of all evidence pages. Chainof-evidence labels, which distinguishes the set-of-evidence labels for every hop, is provided for multi-hop queries. Human Refinement. To improve benchmark quality and ensure accuracy, we further conduct human refinement. Although automated evidence labeling is sufficiently accurate, human annotators reviewed and adjusted 8% of labels with discrepancies. With 92% agreement, this step ensures precise ground truth data, enhancing DOUBLE-BENCH reliability."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Evaluation Protocol. Following task formulation setting, we define the hit rate for retrieval accuracy evaluation of singleand multi-hop queries. The accuracy of the final answer is evaluated using LLM-as-a-judge (Zheng et al., 2023). GPT-4o rates the correctness the generated answer compared to the ground truth answer on scale of 0 to 10. Answers with score not lower than 7 count as correct, not higher than 3 count as incorrect, others count as partially correct. Evaluated Models and Frameworks. We evaluate 4 competitive text embedding models, namely bge-m3 (Chen et al., 2024c), gte-Qwen2 (Li et al., 2023), NV-Embed-v2 (Lee et al., 2024), Qwen3-Embedding (Zhang et al., 2025a), 5 competitive open-source document page embedding models, namely colpali (Faysse et al., 2024), colqwen (Faysse et al., 2024), gme (Zhang et al., 2024), vdr-2b (LlamaIndex, 2025), jina-embeddings-v4 (Gunther et al., 2025), and 3 advanced document RAG frameworks, namely M3DocRAG (Cho et al., 2024), MDocAgent (Han et al., 2025), VidoRAG (Wang et al., 2025). To understand how each part of RAG framework affects answer accuracy, we add reference framework, Colqwen-gen, by directly pairing the strongest embedding model Colqwen with GPT-4o. Baseline results are reported with GPT-4o directly answering the queries without RAG. The Oracle setting estimates the upper-bound performance of RAG frameworks. In this setting, we provide the parsed contents of all the ground truth pages to the MLLM, together with prompt that instructs the MLLM to first extract relevant information and think before providing the final answer. Experiment Setups. All experiments utilize 8A100 server. For embedding model and framework evaluation, all 2500 single-hop and 2668 multi-hop queries are assessed using the entire corpus for retrieval. Text-based embedding models use parsed document chunks, with graphs and figures converted to descriptive captions (generated by Qwen2.5-VL-32b-Instruct) before merging text. In RAG framework evaluation, the same page text is used by ViDoRAG and MDocAgent. ViDoRAG can invoke each component up to twice before final answer generation. All frameworks retrieve 5 most relevant pages to answer the query. 7 Table 4: Performance of RAG Frameworks. Colqwen-gen achieves comparable performance with MDocAgent, the best among evaluated frameworks. This observation highlights the need for more advanced retrieval stage frameworks. Framework Retrieval MDocAgent (Han et al., 2025) ViDoRAG (Wang et al., 2025) M3DOCRAG (Cho et al., 2024) Colqwen-gen (Faysse et al., 2024) hit@5 0.688 0.682 0.608 0.795 Average Single Hop Answer 0.645 0.126 0.229 0.536 0.138 0.326 0.451 0.121 0.428 0.604 0.135 0.261 Retrieval hit@ 0.830 0.822 0.709 0.895 Answer 0.757 0.132 0.111 0.623 0.144 0.233 0.538 0.138 0.324 0.676 0.160 0.164 Retrieval hit@5 0.572 0.539 0.490 0.693 2-Hop Answer 0.567 0.065 0.367 0.457 0.112 0.431 0.330 0.088 0.582 0.462 0.143 0.395 3-Hop Answer 0.532 0.135 0.332 0.447 0.137 0.416 0.382 0.110 0.508 0.554 0.100 0. Retrieval hit@5 0.549 0.544 0.519 0."
        },
        {
            "title": "4.1 MAIN RESULTS",
            "content": "Document-specified embedding model outperform general ones, and gap between text and image embedding models is narrowing. DOUBLE-BENCH provides clear divergence in the retrieval performance of various embedding models, as detailed in Table 3. The model rankings within DOUBLE-BENCH align well with popular text embedding leaderboards MTEB (Muennighoff et al., 2022) and document retrieval benchmark ViDoRe v2 (Mace et al., 2025), demonstrating the robustness of our benchmark. ColQwen2.5-3B significantly outperforms general multimodal embedding models like jina-embeddings-v4 and GME, achieving 9% higher average hit rate and demonstrating strong potential in document retrieval. Other multimodal embedding models show limited capability, even underperforming compared to the purely textual embedding model Qwen3-Embedding. We attribute this to recent advancements of text embedding communitys sophisticated training techniques, including complex multi-stage training, dedicated hard negative sampling, and large-scale high-quality data synthesis. These techniques are difficult to transfer to visual embedding models due to training costs, limited text-and-image data, and model structural constraints. Although visual embedding models have inherent advantages for visual content retrieval, the semantic complexity of document RAG tasks negates this advantage. The critical influence of both visual observation and textual understanding abilities incentive combined strategies such as interleaved embedding models and advanced multimodal understanding pipelines. DOUBLE-BENCH is high-quality and low contaminated that MLLMs still needs retrieval details to answer question correctly. As shown in Table 5, state-of-the-art MLLMs like GPT-4o, Gemini, and Qwen are able to make general responses without context, with 50% to 70% of responses being partially correct. Providing evidence pages to MLLMs substantially boosts accuracy, with 3x to 5x responses being completely correct compared to w.o. RAG setting. This indicates that our benchmark is well-suited for evaluating the retrieval and synthesis components of RAG systems, as it clearly distinguishes context-grounded reasoning from models inherent knowledge. Notably, the robust performance of Qwen2.5-VL observed in the upper bound setting, which closely mirrors our benchmark curation pipeline, further suggesting the robustness and effectiveness of our pipeline in identifying correct evidence pages of queries. Table 5: Evaluation of MLLMs long docOracle ument understanding capability. means directly providing evidnece page. The stark contrast between w.o. RAG and Oracle settings of all MLLMs reflect the high quality and low-contaminated of our DOUBLEBENCH. Model & Setting Single Hop Multi Hop Models w.o. RAG Qwen3-32B text-only 0.242 0.488 0.271 0.193 0.293 0.515 Qwen2.5-VL-7B w.o. RAG 0.053 0.557 0.390 0.127 0.168 0.705 0.109 0.748 0.144 0.197 0.332 0.472 GPT-4o w.o. RAG Qwen2.5-VL-32B w.o. RAG 0.200 0.621 0.179 0.159 0.319 0.521 Llama 4 Maverick w.o. RAG 0.245 0.480 0.275 0.215 0.193 0.592 Models Oracle Qwen2.5-VL-7B Oracle GPT-4o Oracle Llama 4 Maverick Oracle Qwen2.5-VL-32B Oracle 0.406 0.490 0.104 0.456 0.241 0.303 0.678 0.141 0.181 0.538 0.271 0.191 0.601 0.350 0.049 0.524 0.192 0.284 0.874 0.061 0.066 0.643 0.312 0.045 Document RAG frameworks bottleneck still lies on retrieval accuracy, where designing advanced strategies may help. Most frameworks strive to design complex information mining pipelines to extract maximum value from retrieved pages, yet tend to pay little attention to the retrieval stage itself. However, our experiments demonstrate strong correlations between retrieval accuracy and answer accuracy, as shown in Table 4. Equipped with single MLLM pass, Colqwen-gen even partially outperforms MDocAgent on multi-hop queries, despite the latter Average hit@5 performance across Textual embedding model Table 6: Qwen3-Embedding-4B even outperform multimodal models and frameworks in low-resource languages such as Arabic and Japanese, unveiling the challenge of current multimodal embedding models generalizability to low-resource domains. languages. Model & Framework Arabic Chinese English French Japanese Spanish Average Qwen3-Embedding-4B NV-Embed-v2 gte-Qwen2-7B-instruct bge-m colqwen2.5-3b-multilingual vdr-2b-multi jina-embeddings-v4 gme-Qwen2-VL-7B-Instruct colpali-v1.3 MDocAgent ViDoRAG M3DOCRAG Colqwen-gen Text Embedding Models 0.685 0.546 0.600 0.331 0.696 0.654 0.623 0.451 0.809 0.819 0.721 0. 0.646 0.660 0.625 0.453 Visual & Multimodal Embedding Models 0.427 0.364 0.369 0.352 0.271 0.457 0.455 0.377 0.432 0.694 0.680 0.587 0.631 0.300 0.860 0.782 0.792 0.750 0. 0.702 0.607 0.602 0.585 0.607 Document RAG System 0.679 0.676 0.579 0.821 0.785 0.778 0.704 0.793 0.658 0.654 0.558 0.829 0.801 0.662 0.722 0. 0.786 0.740 0.743 0.686 0.335 0.707 0.702 0.621 0.781 0.754 0.698 0.657 0.489 0.798 0.711 0.685 0.693 0.694 0.661 0.668 0.599 0.771 0.732 0.673 0.658 0. 0.711 0.647 0.630 0.616 0.498 0.658 0.655 0.573 0.738 seamlessly integrating multiple agents to provide final answers. This underscores the critical importance of optimizing the retrieval stage, potentially through finer-grained document preprocessing, exploiting the hierarchical and semantic structure of documents and developing more powerful or integrated embedding models. 4.2 IN-DEPTH ANALYSIS The overconfidence dilemma: trading trustworthiness for answers. To investigate the bottleneck in existing RAG frameworks, we breakdown each reponse of M3DocRAG and MDocAgent to analyze whether the error comes from retrieval or answering, and look into the trade-off between answering accuracy and the ability to identify insufficient information (also known as honesty (Gao et al., 2024)). Figure 5 reveals striking divergence in agent behavior. Simpler agents like M3DocRAG adopt cautious strategy, answering lower proportion of queries with successfully retrieved context but reliably identifying retrieval failures and refusing to respond. In contrast, more complex agents like MDocAgent and ViDoRAG exhibit significant overconfidence. While they achieve higher accuracy on retrieval hits, they indiscriminately attempt to answer nearly every query, regardless of whether sufficient information was retrieved. This frequently leads to speculative or entirely hallucinated content when evidence pages are missed. Figure 5: Breakdown of retrieval and response model performance of frameworks under single (s) and multi-hop (m) queries. Our analysis reveals that performance drops on multi-hop questions are mainly due to retrieval failures that cause models to abstain from answering. This observation indicates that recent document RAG development has over-emphasized maximizing answer generation at the expense of epistemic humility, i.e., the crucial skill of knowing what it doesnt know and admitting when an answer cannot be found. Consequently, we argue that future research should pursue more trustworthy RAG frameworks where identifying informational gaps is as valued as accuracy (Zhou et al., 2025; Huang et al., 2025). 9 Framework Figure Text Table MDocAgent ViDoRAG 0.692 0.106 0.202 0.665 0.114 0.221 0.547 0.148 0.305 0.609 0.122 0.269 0.557 0.138 0.305 0.489 0.151 0.360 M3DOCRAG 0.456 0.127 0.418 0.512 0.110 0.378 0.371 0.114 0.515 Colqwen-gen 0.657 0.186 0.157 0.619 0.114 0.268 0.545 0.104 0.351 Table 7: Performance of multimodal document RAG system across different modalities. Inference patterns of MLLMs as response model. We also observe different answering strategy in MLLMs in Table 4 and Appendix. When directly provided with multi-hop query, response model tend not to process them hop-by-hop. On the contrary, they first collect signature informationthe most distinguishing or identifiable piecesfrom the various hops. Following this, models tend to perform direct inclusion based elimination to arrive the final answer. This mechanism differentiates significantly from our expectation of how models might sequentially solve multi-hop queries. This provides compelling point of view: merely increasing the number of hops may not increase its difficulty. case study in Appendix reveals that MLLMs do not process multi-hop queries stepby-step as expected. Instead, they gather key signature information from each hop and use inclusionbased elimination to find the answer. This challenges the assumption that more hops always increase difficulty, suggesting further investigation is needed. Time efficiency of frameworks. Agent efficiency is as important metric as effectiveness. Since API completion times may vary across models, Figure 6 reports the normalized time efficiency of the evaluated frameworks. Both MDocAgent and ViDoRAG employ sequential agent coordination pattern, which significantly increases their inference time. Note that ViDoRAG dynamically controls the generation process, so we report the lower and upper bound theoretical time efficiency of ViDoRAG estimated by API call times."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Figure 6: Time efficiency of each document RAG system. Multimodal Document Retrieval. Different from traditional text retrieval (Zhao et al., 2024; Blandon et al., 2025), documents (Masry et al., 2022; Tanaka et al., 2023) often contain multimodal information, which may be time consuming and would cause information loss when directly parsed to text. Therefore, recent works have dedicated great effort to improve the accuracy and efficiency of document retrieval with MLLMs. One line of work adopts high quality synthetic data (Zhang et al., 2024; Chen et al., 2025), hardness aware training (Lan et al., 2025; Lee et al., 2024) and retrieval-optimized network architectures (Faysse et al., 2024) for more precise embedding models. Another line of work leverages LLM/MLLM agentic flows to process different modalities in parallel (Han et al., 2025; Ouyang et al., 2025) and perform iterative inference steps for more grounded and informative answers (Wang et al., 2025). Document RAG Benchmarks. The increasing attention on Document RAG (Ma et al., 2024) and VQA (Mathew et al., 2021) necessitates comprehensive multimodal retrieval benchmarks. Common practices often use VQA dataset queries (Friel et al., 2024; Faysse et al., 2024; Cho et al., 2024; Fu et al., 2025), but these are document-specific and lack information for global retrieval. Other benchmarks (Wang et al., 2025; Dong et al., 2025) craft informative queries from single pages, yet often only mark that single page as relevant, ignoring other potential matches and risking evaluation inaccuracies. Some recent benchmarks (Mace et al., 2025; Wasserman et al., 2025; Xu et al., 2025) have identified contextual gaps between artificial and realistic queries, and strive to provide evaluations that fully reflect real use scenarios."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce DOUBLE-BENCH, large-scale, multimodal, multilingual benchmark designed to reflect realistic retrieval-augmented generation scenarios, overcoming limitations of prior work with validated chain-of-evidence and comprehensive assessment. Evaluations of leading embedding models and RAG frameworks reveal several crucial bottlenecks. We hope our fully open-sourced code, framework and dataset establish strong foundation for document RAG system."
        },
        {
            "title": "LIMITATIONS",
            "content": "We leverage LLMs to revise our paper and serving as metrics in our evaluation. We include humanannotation in Appendix to validate the LLM-as-a-Judge process. Despite DOUBLE-BENCHs comprehensive design, several limitations constrain its scope and applicability. First, although our benchmark provides the broadest coverage across 6 languages and 4 document types compared to previous document RAG benchmarks, it potentially misses important linguistic communities and specialized domains such as legal or medical documents that present unique RAG challenges. Second, our query synthesis pipeline relies heavily on large language models (primarily GPT-4o and Qwen2.5-VL-32B-Instruct), thereby introducing systematic biases in question generation patterns that may not reflect the full diversity of human informationseeking behavior."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Marıa Andrea Cruz Blandon, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, and Marcello Federico. Memerag: multilingual end-to-end meta-evaluation benchmark for retrieval augmented generation. arXiv preprint arXiv:2502.17163, 2025. Eleonora Breci, Luca Guarnera, and Sebastiano Battiato. novel dataset for non-destructive inspection of handwritten documents. arXiv preprint arXiv:2401.04448, 2024. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024a. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. MLLM-as-a-judge: Assessing multimodal LLM-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=dbFEFHAD79. Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. arXiv preprint arXiv:2502.08468, 2025. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024c. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1775417762, 2024d. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. 11 Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. M3docrag: Multimodal retrieval is what you need for multi-page multi-document understanding. arXiv preprint arXiv:2411.04952, 2024. Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, and Yong Liu. Mmdocir: Benchmarking multi-modal retrieval for long documents. arXiv preprint arXiv:2501.08828, 2025. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations, 2024. Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrievalaugmented generation systems. arXiv preprint arXiv:2407.11005, 2024. Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, and Dongping Chen. LiveVQA: Assessing models with live visual knowledge. In Synthetic Data for Computer Vision Workshop @ CVPR 2025, 2025. URL https://openreview.net/forum?id=sLFrSp7xNs. Chujie Gao, Siyuan Wu, Yue Huang, Dongping Chen, Qihui Zhang, Zhengyan Fu, Yao Wan, Lichao Sun, and Xiangliang Zhang. Honestllm: Toward an honest and helpful large language model. arXiv preprint arXiv:2406.00380, 2024. Michael Gunther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, jina-embeddings-v4: Sedigheh Eslami, Scott Martens, Bo Wang, Nan Wang, and Han Xiao. Universal embeddings for multimodal multilingual retrieval. arXiv preprint arXiv:2506.18902, 2025. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrievalaugmented generation. arXiv preprint arXiv:2410.05779, 2024. Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. MdocaarXiv preprint gent: multi-modal multi-agent framework for document understanding. arXiv:2503.13964, 2025. Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, et al. On the trustworthiness of generative foundation models: Guideline, assessment, and perspective. arXiv preprint arXiv:2502.14296, 2025. Yulong Hui, Yao Lu, and Huanchen Zhang. Uda: benchmark suite for retrieval augmented generation in real-world document analysis. arXiv preprint arXiv:2406.15187, 2024. Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language arXiv preprint and vision embedding models with hardness-weighted contrastive learning. arXiv:2503.04812, 2025. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33: 94599474, 2020. Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Fei Huang, Jingren Zhou, et al. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937, 2024. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. 12 Nikos Livathinos, Christoph Auer, et al. Docling: An efficient open-source toolkit for AI-driven document conversion. arXiv preprint arXiv:2501.17887, 2025. LlamaIndex. vdr-2b-multi-v1. Hugging Face Model Card, 2025. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. Quentin Mace, Antonio Loison, and Manuel Faysse. Vidore benchmark v2: Raising the bar for visual retrieval. arXiv preprint arXiv:2505.17166, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Matin Mortaheb, Mohammad Amir Khojastepour, Srimat Chakradhar, and Sennur Ulukus. Rag-check: Evaluating multimodal retrieval augmented generation performance. arXiv preprint arXiv:2501.03995, 2025a. Matin Mortaheb, Mohammad Amir Khojastepour, Srimat Chakradhar, and Sennur Ulukus. arXiv preprint Re-ranking the context for multimodal retrieval augmented generation. arXiv:2501.04695, 2025b. Niklas Muennighoff, Nouamane Tazi, Loıc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. OpenAI. Gpt-4o, 2024. URL https://openai.com/gpt-4o. Accessed: 2025-05-01. Geliang Ouyang, Jingyao Chen, Zhihe Nie, Yi Gui, Yao Wan, Hongyu Zhang, and Dongping Chen. nvagent: Automated data visualization from natural language via collaborative agent workflow. arXiv preprint arXiv:2502.05036, 2025. Shu Pu, Yaochen Wang, Dongping Chen, Yuhang Chen, Guohao Wang, Qi Qin, Zhongyi Zhang, Zhiyuan Zhang, Zetong Zhou, Shuang Gong, et al. Judge anything: Mllm as judge across any modality. arXiv preprint arXiv:2503.17489, 2025. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1363613645, 2023. Yixuan Tang and Yi Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. arXiv preprint arXiv:2401.15391, 2024. Bin Wang, Chao Xu, and Xiaomeng Zhao. MinerU: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao. Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017, 2025. Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, and Leonid Karlinsky. Real-mm-rag: real-world multi-modal retrieval benchmark. arXiv preprint arXiv:2502.12342, 2025. Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, and Wenya Wang. Visual-rag: Benchmarking textto-image retrieval augmented generation for visual knowledge intensive queries. arXiv preprint arXiv:2502.16636, 2025. 13 Chuan Xu, Qiaosheng Chen, Yutong Feng, and Gong Cheng. mmrag: modular benchmark for retrieval-augmented generation over text, tables, and knowledge graphs. arXiv preprint arXiv:2505.11180, 2025. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. Evaluation of retrievalaugmented generation: survey. In CCF Conference on Big Data, pp. 102120. Springer, 2024. Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, and Wentao Zhang. Mramgbench: beyondtext benchmark for multimodal retrieval-augmented multimodal generation. arXiv e-prints, pp. arXiv2502, 2025. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025a. Zhuocheng Zhang, Yang Feng, and Min Zhang. Levelrag: Enhancing retrieval-augmented genarXiv preprint eration with multi-hop logic planning over rewriting augmented searchers. arXiv:2502.18139, 2025b. Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained language models: survey. ACM Transactions on Information Systems, 42(4):160, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, and Emine Yilmaz. Trustrag: Enhancing robustness and trustworthiness in retrievalaugmented generation. arXiv preprint arXiv:2501.00879, 2025."
        },
        {
            "title": "A DETAILED BENCHMARK CONSTRUCTION",
            "content": "Step 1: Data Collection and Preprocessing. We collected rich documents from various sources, including PDFs, scanned documents, slides and HTML pages in multiple languages. We then selected documents with length between 10-50 pages and eliminated irrelevant content. Using tools such as Docling and MinerU, we split each document into text, tables, and figures, ensuring that each modality was processed separately. We further used GPT-4o to filter out low-quality or irrelevant data chunks and retain only important content for query generation. Finally, the detailed data for our benchmark is shown in Table 8, Table 2 and Figure 7. Step 2: One-Hop Query Synthesis. To generate singlehop queries, we began by utilizing parsed document chunks. For textual content, we focused on identifying key entities, data, and concepts to craft self-contained questions. Meanwhile, for visual elements, we prioritized understanding critical components and patterns. Furthermore, to handle ambiguous queries that yielded more than five ground truth pages, we designated specific evidence page and prompted the model to analyze it in contrast with the other pages, thereby adding distinguishing detail to refine the query and ensure its specificity. Finally, each query was meticulously reviewed to ensure clarity, relevance, and direct answerability from the document. Step 3: Multi-Hop Query Synthesis. To address the challenge of multi-hop questions that require reasoning across multiple steps, we employed knowledge graph-based approach. Using LightRAG, we constructed knowledge graphs for each document, extracting entities and relations from both textual and visual content. These graphs leveraged the connectivity of nodes and edges to represent the relationships between different pieces of information. We selected query paths through these graphs, starting with high-dimensional entities and expanding paths based on content depth and connectivity. Subqueries were synthesized for each hop and iteratively combined into coherent multi-hop query. Each query was rigorously checked to ensure its logical coherence, unique answers, and overall relevance to the document. Figure 7: KDE distribution of document lengths by document type. Table 8: DOUBLE-BENCH statistics. Statistic Documents Languages Avg. pages per doc Avg. words per page Avg. tables per page Avg. figures per page Total Questions Single-hop questions Avg. evidence pages Multi-hop questions Avg. hops Number 3276 6 22.3 289.9 0.397 1.078 5168 2500 2.91 2668 2.78 Step 4: Post-Processing. After query synthesis, the generated queries underwent model filtering using advanced models to ensure they met quality standards, removing poorly structured or irrelevant queries. Evidence pages for both single-hop and multi-hop queries were annotated by MLLMs like Qwen2.5-32B-VL, ensuring that supporting evidence was accurately identified. Finally, human annotators reviewed the filtered queries and evidence pages for accuracy and consistency. Human refinement addressed potential discrepancies, thereby enhancing the overall reliability of the benchmark.. This comprehensive post-processing approach ensured the robustness of the benchmark and its suitability for evaluating multimodal document retrieval systems."
        },
        {
            "title": "B HUMAN ANNOTATION DETAILS",
            "content": "The annotation is conducted by 5 authors of this paper and 1 volunteers independently. As acknowledged, the diversity of annotators plays crucial role in reducing bias and enhancing the reliability of the benchmark. These annotators have knowledge in this domain, with different genders, ages, and educational backgrounds. To ensure the annotators can proficiently mark the data, we provide them with detailed tutorials, teaching them how to evaluate model responses more objectively. The annotation UI is shown in Figure 8. 15 To guarantee the quality and reliability of our benchmark, we implemented comprehensive human verification process for both the generated queries and the ground truth evidence labels. First, every query underwent rigorous human review after an initial automated screening. This step focused on ensuring the questions were high-quality, realistic, and compliant with our generation standards, culminating in 97% agreement rate between the models judgments and human annotators on 200-item subsample. In parallel, the evidence page labels were subjected to thorough human audit, which revealed an initial 92% agreement rate between the automated labels and human judgment. Our annotators then meticulously resolved the remaining discrepancies. This dual-layered human verification process ensures that both the queries and their ground truth labels are of high fidelity, establishing robust foundation for evaluation. Figure 8: Human Annotation UI."
        },
        {
            "title": "C DETAILED EXPERIMENT SETTINGS",
            "content": "C.1 EXPERIMENTAL METHODOLOGY C.1.1 EXPERIMENT 1: EVALUATION OF EMBEDDING MODELS To assess the recall capabilities of state-of-the-art embedding models, we conducted experiments on both multimodal and text embedding models. For multimodal embedding models, we directly embedded the visual and textual content of the documents. In contrast, for text embedding models, we first applied OCR to extract text from images and then embedded the extracted text. Additionally, for images and tables, we used Vision-Language Model (VLM) to generate descriptive captions, which were subsequently embedded. Finally, we also counted the hit@5 of these embed models in single-hop, 2-hop, and 3-hop queries in different languages, as shown in Table 9, Table 10 and Table 11.The experimental results highlight strong competition between two leading models: the multimodal colqwen2.5-3b-multilingual and the text-based Qwen3-Embedding-4B. Overall, colqwen2.5-3b performs slightly better than Qwen3-Embedding-4B. However, in certain languages, such as Arabic, the text-based Qwen3-Embedding-4B shows superior performance. C.1.2 EXPERIMENT 2: PERFORMANCE OF LLMS WITH AND WITHOUT GOLDEN PAGES We tested the performance of advanced Large Language Models (LLMs) and Vision-Language Models (VLMs), including Qwen3-32B, GPT-4o, Llama 4 Maverick, Qwen2.5-VL-7B and Qwen2.5-VL-32B, in two different contexts. First, we evaluated their performance when provided with the exact 5 golden pages as context, which allowed us to assess their ability to generate accurate answers with direct access to relevant information. Second, we tested their performance without the golden pages, relying solely on their inherent knowledge and reasoning capabilities. This dual evaluation provided insights into how these models perform in both ideal and more challenging conditions. Table 9: Single-Hop hit@5 performance of embedding model across languages."
        },
        {
            "title": "Arabic Chinese English French Japanese Spanish",
            "content": "Qwen3-Embedding-4B NV-Embed-v2 gte-Qwen2-7B-instruct bge-m"
        },
        {
            "title": "Text Embedding Models",
            "content": "0.668 0.495 0.639 0.361 0.791 0.636 0.667 0.535 0.963 0.933 0.881 0.919 0.883 0.820 0.808 0.614 Visual & Multimodal Embedding Models colqwen2.5-3b-multilingual vdr-2b-multi jina-embeddings-v4 gme-Qwen2-VL-7B-Instruct colpali-v1. 0.466 0.413 0.394 0.418 0.255 0.841 0.771 0.760 0.721 0.333 0.983 0.964 0.948 0.942 0.962 0.925 0.838 0.835 0.832 0.799 0.872 0.653 0.766 0.508 0.872 0.781 0.827 0.766 0. 0.881 0.814 0.814 0.631 0.927 0.895 0.895 0.836 0.825 Table 10: 2-Hop hit@5 performance of embedding model across languages."
        },
        {
            "title": "Arabic Chinese English French Japanese Spanish",
            "content": "Qwen3-Embedding-4B NV-Embed-v2 gte-Qwen2-7B-instruct bge-m"
        },
        {
            "title": "Text Embedding Models",
            "content": "0.730 0.635 0.587 0.222 0.731 0.827 0.673 0.462 0.737 0.796 0.664 0.620 0.435 0.571 0.506 0.325 Visual & Multimodal Embedding Models colqwen2.5-3b-multilingual vdr-2b-multi jina-embeddings-v4 gme-Qwen2-VL-7B-Instruct colpali-v1. 0.556 0.365 0.413 0.349 0.381 0.673 0.692 0.538 0.673 0.423 0.796 0.701 0.737 0.650 0.664 0.539 0.435 0.416 0.429 0.461 0.782 0.709 0.691 0.436 0.818 0.745 0.764 0.764 0. 0.754 0.678 0.593 0.432 0.797 0.661 0.585 0.686 0.669 Table 11: 3-Hop hit@5 performance embedding model across languages."
        },
        {
            "title": "Arabic Chinese English French Japanese Spanish",
            "content": "Qwen3-Embedding-4B NV-Embed-v2 gte-Qwen2-7B-instruct bge-m"
        },
        {
            "title": "Text Embedding Models",
            "content": "0.658 0.507 0.575 0.411 0.567 0.500 0.529 0.356 0.728 0.728 0.617 0.643 0.620 0.588 0.560 0.419 Visual & Multimodal Embedding Models colqwen2.5-3b-multilingual vdr-2b-multi jina-embeddings-v4 gme-Qwen2-VL-7B-Instruct colpali-v1. 0.260 0.315 0.301 0.288 0.178 0.567 0.577 0.462 0.500 0.144 0.802 0.681 0.691 0.657 0.717 0.641 0.549 0.556 0.495 0.562 0.750 0.625 0.708 0.514 0.667 0.694 0.639 0.528 0. 0.628 0.603 0.564 0.404 0.669 0.576 0.576 0.557 0.587 C.1.3 EXPERIMENT 3: RAG FRAMEWORKS EVALUATION To further evaluate the effectiveness of Retrieval-Augmented Generation (RAG) frameworks, we conducted experiments using MDocAgent, ViDoRAG, and M3DOCRAG. In addition to these systems, we established baseline named Colqwen-Gen, which pairs the retrieval results from Colqwen-3B with the generation capabilities of GPT-4o. We tested both the recall capabilities of 17 Table 12: Single-hop hit@5 performance of multimodal document RAG frameworks across languages. Framework Arabic Chinese English French Japanese Spanish 0.413 MDocAgent ViDoRAG 0.420 M3DOCRAG 0.475 Colqwen-gen 0.466 0.786 0.769 0.621 0.841 0.942 0.945 0.856 0.983 0.808 0.791 0.627 0.925 0.775 0.773 0.658 0. 0.841 0.847 0.690 0.927 Table 13: Multi-hop hit@5 performance of document RAG frameworks across languages. Framework Arabic Chinese English French Japanese Spanish 0.500 MDocAgent ViDoRAG 0.489 M3DOCRAG 0.279 0.397 Colqwen-gen 0.571 0.582 0.537 0.801 0.628 0.611 0.551 0. 0.507 0.517 0.489 0.732 0.638 0.631 0.583 0.690 0.480 0.488 0.508 0.614 Table 14: Single-hop hit@5 performance of document RAG framework across document types. Framework PDF Slides HTML Pages Scanned Doc 0.781 0.858 MDocAgent ViDoRAG 0.776 0.852 M3DOCRAG 0.778 0.592 0.905 0.873 Colqwen-gen 0.842 0.834 0.681 0.901 0.636 0.638 0.566 0.808 Table 15: Multi-hop hit@5 performance of document RAG frameworks across document types. Framework PDF Slides HTML Pages Scanned Doc MDocAgent 0.567 0.570 0.572 0.552 ViDoRAG M3DOCRAG 0.560 0.538 0.711 0.689 Colqwen-gen 0.538 0.532 0.505 0.669 0.536 0.540 0.321 0.726 Table 16: Single-hop hit@5 performance of document RAG system across modalities. Framework Figure Text Table 0.917 MDocAgent 0.920 ViDoRAG M3DOCRAG 0.629 0.877 Colqwen-gen 0.820 0.731 0.802 0.738 0.849 0.641 0.857 0.917 Table 17: Multi-hop hit@5 performance of document RAG frameworks across modalities. Framework Figure Text Table 0.615 MDocAgent ViDoRAG 0.606 M3DOCRAG 0.564 0.704 Colqwen-gen 0.557 0.559 0.561 0.552 0.522 0.509 0.706 0.688 these frameworks in retrieving relevant documents and their ability to generate accurate answers based on the retrieved information. In addition, we also analyzed the Time efficiency of Frameworks,as shown in Figure 6.We also counted and analyzed the hit@5 and answer performance of these frameworks in different languages, document types, and modalities, as shown in Table 12, Table 13, Table 14, Table 15, Table 16, Table 17 Table 7 and Table 18. 18 Table 18: Performance of multimodal document RAG system across different document types. Framework PDF Slides HTML Pages Scanned Doc 0.661 0.123 0.216 0.675 0.118 0.206 0.631 0.149 0.220 0.625 0.130 0.245 MDocAgent ViDoRAG 0.534 0.131 0.335 0.541 0.127 0.332 0.522 0.152 0.325 0.539 0.147 0.314 M3DOCRAG 0.488 0.121 0.391 0.472 0.107 0.421 0.454 0.084 0.462 0.354 0.132 0.514 0.575 0.156 0.269 0.560 0.180 0.260 Colqwen-gen 0.636 0.130 0.235 0.619 0. 0."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "Full prompt templates and examples are provided in Figure 10, Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, Figure 16, Figure 17, Figure 18, Figure 19, Figure 20, Figure 21, Figure 22, Figure 23, Figure 24, Figure 25 and Figure 26."
        },
        {
            "title": "E CASE STUDY",
            "content": "A detailed case study is shown in Figure 9 . Case Study: Multi hop Question: What disease does the entity that launched its first nationwide public service campaign in 1980 implement programs to prevent, which causes blindness in children due to vitamin deficiency? MLLM response: The disease that the entity likely implemented programs to prevent, which causes blindness in children due to vitamin deficiency, is Xerophthalmia. Note: This discussion is missing the step of identifying the first nationwide public service campaign in 1980 that implemented such prevention programs. While pinpointing single entity that launched its first nationwide public service campaign specifically in 1980 in the United States focused solely on preventing xerophthalmia through vitamin sufficiency proves challenging based on available search information, the 1980s were crucial period for understanding and combating vitamin deficiency and its impact on childhood blindness and mortality. Research in the 1980s, significantly contributed to by organizations like Helen Keller Intl in collaboration with researchers such as Dr. Alfred Sommer, highlighted the profound impact of vitamin supplementation in preventing not only blindness but also reducing childhood deaths from common illnesses. This groundbreaking evidence galvanized international health organizations and NGOs to expand their efforts in vitamin distribution and public health initiatives in areas where deficiency was prevalent. Therefore, while specific US-based first nationwide public service campaign in 1980 by named entity for this exact issue wasnt definitively identified, organizations like Helen Keller Intl were actively working on implementing programs to prevent xerophthalmia due to vitamin deficiency during this period, likely as part of broader public health or nutritional initiatives that may have included public awareness components. The impact of their work and the research from that era significantly contributed to the global understanding and efforts to combat this preventable cause of childhood blindness. Figure 9: Case study showing MLLMs shortcut reasoning behavior on multi-hop query. 19 Query Reviewer Prompt System Prompt: Task have some QA data here, and you can observe that the questions can be divided into two categories: The category #A: When you see this question alone without given document, you are sure to find unique document in corpus to provide unique answer. The question having some key words to help you locate the document from corpus. The category #B: When you see this question alone without given document, you will find hard to locate document to give deterministic answer for this question, because you will find multiple candidate documents in corpus, which may lead to different answers for this question. The question do not have any special key words to help you locate the document from corpus. Examples The number mentioned on the right of the leftside margin? #B What is the date mentioned in the second table? #B What is the full form of PUF? #A What is the number at the bottom of the page, in bold? #B Who presented the results on cabin air quality study in commercial aircraft? #A What is the name of the corporation? #B Which part of Virginia is this letter sent from? #B who were bothered by cigarette odors? #A which cigarette would be better if offered on thicker cigarette? #A Cigarettes will be produced and submitted to O/C Panel for what purpose? #A What is the heading of first table? #B What is RIP-6 value for KOOL KS? #A Which test is used to evaluate ART menthol levels that has been shipped? #A How much percent had not noticed any difference in the odor of VSSS? #A What is the cigarette code of RIP-6(W/O Filter) 21/4SE? #A What mm Marlboro Menthol were subjectively smoked by the Richmond Panel? #A What are the steps of Weft Preparation between Spinning bobbin and Weaving? #A What level comes between Middle Managers and Non-managerial Employees? #A What are the six parts of COLLABORATION MODEL of the organization where James has role of leading the UK digital strategy? #A Figure 10: Screening prompt for pilot study. 20 Visual Filter System Prompt System Prompt: Task: You are strict document image analyst for document-based RAG systems, specializing in evaluating whether images can support the generation of high-quality Q&A pairs. Carefully evaluate each images content following these guidelines. Core Objective: Analyze the provided image and its surrounding page text to determine whether the content of the image can effectively support generating factually grounded QA pairs for document-based RAG systems. Evaluation Criteria: 1. Essential Content Requirements: The image must exhibit semantic cohesion with its surrounding text content. Visual elements should convey self-contained informational completeness. Demonstrate capacity to generate verifiable factual statements. 2. Exclusion Imperatives: Automatically reject images displaying: Decorative non-functionality. Semantic ambiguity preventing definitive interpretation. Information density below operational thresholds. Contextual detachment from document flow. Appendices, reference lists, or other images lacking specific meaningful information. Response Requirements: Strictly respond with \"Yes\" or \"No\" only. \"Yes\" indicates the visual/textual content of the image can effectively support generating factually grounded QA pairs. \"No\" indicates the visual/textual content of the image cannot effectively support generating factually grounded QA pairs. Figure 11: System prompt for evaluating visual content in document-Based RAG. Singlehop QA Generation Prompt (Text) Generate one QA pair based on the following guideline: Question Requirements: Create self-contained questions requiring no contextual knowledge from other pages. Focus on explicitly mentioned key entities, concepts, or processes. Avoid page-specific references (e.g., in this section or as mentioned above). Include both factual questions (who/what/when) and explanatory questions (how/why). Answer Specifications: Answers may be moderately summarized but must strictly adhere to source content. Prohibit any speculation or supplementation beyond original text. Format Rules: Response must be in JSON format containing \"question\" and \"answer\" fields. Example Response: { \"question\": \"What are the clinical diagnostic criteria for Parkinsons disease?\", \"answer\": bradykinesia combined with either resting tremor or rigidity, often presenting with asymmetric onset.\" } \"Diagnosis requires Figure 12: Single-hop QA prompt template for generating self-contained question-answer pairs. 22 Singlehop QA Generation Prompt (Figure/Table) Generate QA pair based on the following guideline: Question Requirements: Formulate globally valid questions without image-dependent references. Expressions such as \"in the image/table\" or \"according to\" are prohibited. Focus on key visual elements: meaningful knowledge and insights rather than superficial visual elements. Prioritize questions that: Extract domain-specific knowledge. Identify key patterns and relationships. Explore underlying mechanisms or principles. Analyze trends and their implications. Examine cause-and-effect relationships. Avoid trivial questions about: Simple counting of visual elements. Basic visual descriptions. Surface-level observations. Generic image properties. Employ diverse question types and perspectives: What / When / How / Where / Which. Answer Specifications: Answers must strictly derive from image content but not captions/context. Prohibit extrapolation beyond visually verifiable information. Focus on providing substantive, knowledge-rich responses. Format Rules: Response must be in JSON format containing \"question\" and \"answer\" fields. Example Response: { \"question\": \"How does the introduction of carbon pricing mechanisms correlate with industrial emission reduction rates across different sectors in 2009?\", \"answer\": \"Manufacturing sectors showed 30% reduction in emissions after carbon pricing implementation, while energy sectors achieved 45% reduction, demonstrating stronger responsiveness to the policy.\" } Figure 13: Single-hop QA generation prompt for image-based question-answering. Singlehop Filter Prompt(Text) You are professional QA pair screening specialist with expertise in information quality assessment. Your responsibility is to filter the QA pair for retrieval systems. Carefully analyze the QA pair and read the relevant context, determine whether the question is worth keeping according to the following requirements. Filter Requirements: 1. Question must be clear, self-contained, and explicitly reference entities/concepts. important: Reject questions containing \"according to the \"in the provided \"in the given information\", Most text\", text\" or similar phrases. Reject vague or non-specific questions. Reject excessive inferences not mentioned in the original text. Reject queries based on appendices, reference lists, or images that do not contain specific meaningful information. Only retain questions about practical facts, data, processes, or concrete concepts. 2. Answer must be fully supported by the provided text. Reject answers not directly extractable from the text. Reject answers with factual errors or hallucinations. Output Format: Respond with JSON format containing two keys: \"reason\": Brief explanation for your decision (12 sentences). \"keep\": \"Yes\" (meets ALL criteria, worth keeping) or \"No\" (fails ANY criterion). Example: { \"reason\": \"Question is clear and answer is well-supported by the text\", \"keep\": \"Yes\" } Figure 14: Single-hop text filter prompt for QA pair screening. 24 Singlehop Filter Prompt(Figure/Table) You are professional QA pair screening specialist with expertise in information quality assessment. Your responsibility is to filter the QA pair for retrieval systems. Carefully analyze the QA pair, read the images and relevant context, determine whether the question is worth keeping according to the following requirements. Filter Requirements: 1. Question must be clear, self-contained, and explicitly reference entities/concepts. important: Reject questions containing \"according to the \"in the \"in the given data\", \"in the image\", Most table\", provided text\" or similar phrases. Reject vague or non-specific questions. Reject excessive inferences not mentioned in the original text. Reject queries based on appendices, reference lists, or images that do not contain specific meaningful information. Only retain questions about practical facts, data, processes, or concrete concepts. 2. Answer must be fully supported by the provided text. Reject answers not extractable from the image. Reject answers with factual errors or hallucinations. Output Format: Respond with JSON format containing two keys: \"reason\": Brief explanation for your decision (12 sentences). \"keep\": \"Yes\" (meets ALL criteria, worth keeping) or \"No\" (fails ANY criterion). Example: { \"reason\": \"Question references specific visual elements and answer is supported by the image\", \"keep\": \"Yes\" } Figure 15: Single-hop image filter prompt for QA pair screening. 25 Table-to-Text Caption Prompt Role: You are data analyst specializing in precise verbalization of structured data. Task: Convert tabular data from the documents contextual documents into natural language descriptions. Core Requirements: Use table entities/objects as direct grammatical subjects without mentioning the table structure. Do not begin the description with phrases such as \"the survey\", \"the table\", or \"the figure\". The description must be clear and understandable even when taken out of the original context, ensuring it can clearly refer to and express the intended meaning and be unambiguously answered. Exhaustively describe every data cell using original values and labels. Maintain absolute objectivity no analysis, interpretations, or subjective terms. Form cohesive paragraphs using transitional phrases (no bullet points/list formats). Embed specific context from source documents into descriptions. If the table contains footnotes or annotations, include their explanations. Figure 16: Table-to-text caption prompt for converting tabular data into descriptive text. Figure-to-Text Caption Prompt Role: You are visual analyst specialized in exhaustive objective description of visual content. Task: Generate comprehensive descriptions of images strictly based on their pictorial content and document context. Core Requirements: Use depicted entities/objects as grammatical subjects (prohibited: \"The bar chart shows...\"). Do not begin the description with \"the survey\", \"the table\", or \"the figure\". The description must be clear and understandable even when taken out of the original context, ensuring it can clearly refer to and express the intended meaning and be unambiguously answered. Describe all visual elements: For infographics: Every data point, axis labels, trend lines, flow directions, and legend entries. For objects/people: Physical attributes, spatial relationships, and observable actions. Maintain objectivity: No subjective terms. No analytical conclusions (e.g., \"This suggests...\"). No contextual assumptions beyond provided documentation. Preserve data and information integrity. Form cohesive paragraphs using transitional phrases (no bullet points/list/markdown formats). Embed specific context from source documents into descriptions. Figure 17: Figure-to-text caption prompt for objective image description. Relationship Evaluation and Selection Prompt You are an expert in knowledge graph reasoning. Your task is to evaluate relationship candidates and select the best one for constructing an unambiguous reasoning question. The ideal relationship should uniquely identify the target entity. When forming question like \"What entity [relation] with [current entity]?\", the answer should be specific enough that only one reasonable entity fits. It is strictly forbidden to select vague relationships such as \"is related to\". Figure 18: Prompt for evaluating and selecting knowledge graph relationships. 27 Relationship Selection Prompt Given the current entity {current node}, have the following candidate entities and their relations to the current entity: {candidates json} Please evaluate each relationship and select the one that would create the most unambiguous and specific reasoning question. The chosen relationship should make it possible to uniquely identify the target entity when given the current entity and the relationship. Return your response as JSON object with: 1. \"reasoning\": Brief explanation of why this relationship is the most specific/unique. 2. \"selected index\": The index (0-based) of the chosen candidate. Example response format: { \"reasoning\": \"This relationship is the inventor of creates the most unique connection...\", \"selected index\": 2 } Figure 19: Prompt for selecting the most unambiguous knowledge graph relationship. Step Question Generation Prompts General Prompt: Generate simple question (Q) and answer (A) pair about the relationship between two entities. Given an entity and relation, ask for the entity at the other end. The answer should be the specific entity name provided. Return the response as JSON object with keys \"question\" and \"answer\". Note: If the answer is in all uppercase letters, you must convert it to the appropriate case. User-Specific Prompt: Given Entity {current node id} and the relationship {relation text}, generate question (Q) that asks for the entity connected by this relationship. The answer (A) is {next node id}. Note: If the answer is in all uppercase letters, you must convert it to the appropriate case. Return the response as JSON object with keys \"question\" and \"answer\". Figure 20: Step question generation prompts for entity-Relationship QA pairs. Multi-hop Question Chaining Prompts General Prompt: Combine two questions to form natural-sounding multi-hop reasoning question. Guidelines: 1. Analyze both questions and identify exactly how the entity appears in Q2. 2. Consider different ways to phrase the combined question that sound natural. 3. Think about what phrasing would be most clear to human reader. 4. Reason about whether any ambiguity might be introduced by the combination. Goal: Seamlessly integrate the first question into the second question by replacing specific entity reference. The combined question should: Be grammatically correct and flow naturally. Avoid awkward phrases like \"the entity that...\" or \"the thing which...\". Maintain the original meaning and logical connection between questions. Sound like question human would ask, not an artificial construction. Accurately preserve the reasoning chain between the questions. After your analysis, provide the final combined question in JSON format. User Prompt: Combine the following questions: Question 1 (Q1): {previous cumulative q} Question 2 (Q2): {new step q} Entity to replace: {entity to replace} First, explain your reasoning: analyze how you will approach combining these questions naturally. Think about how to best replace {entity to replace} with Q1 in way that reads fluently. Then, \"chained question\". provide your final combined question as JSON object with key Examples: Example 1: Q1: \"What is the capital of France?\" Q2: \"What river flows through Paris?\" Combined: \"What river flows through the capital of France?\" Example 2: Q1: \"Who directed Pulp Fiction?\" Q2: \"What other movies did Quentin Tarantino make?\" Combined: Fiction make?\" \"What other movies did the director of Pulp JSON format example: { \"chained question\": \"Your combined question here\" } Figure 21: Multi-hop question chaining prompts for combining reasoning steps. Multi-hop QA Filter Prompt You are strict expert in knowledge graph reasoning and natural language question generation quality assessment. Your task is to rigorously evaluate provided multi-hop reasoning question and its underlying reasoning steps based on set of strict quality criteria. You must analyze the entire reasoning path, from the initial step to the final question and answer, to determine if the question is high-quality, unambiguous, logically sound, and meaningful. Evaluate the following multi-hop reasoning question and its construction process based on the criteria provided below. Question Data: Initial Question: {initial question} Initial Answer (Entity ID): {initial answer} Final Question: {final question} Final Answer (Entity ID): {final answer} Reasoning Steps (Logical Flow and Chaining): {steps description} Evaluation Criteria: 1. Final Question Clarity and Context Independence: The Final Question must be clearly, fluently, and naturally phrased and must be understandable and answerable. The Final Answer entity name must NOT be present within the Final Question text itself. The Final Question should be specific enough to uniquely identify the Final Answer given the preceding context. The Final Question and all step questions must avoid artificial phrasing like \"the entity that...\", \"the thing which...\", etc., and sound like natural human questions. 2. Necessity of Reasoning Steps: Every intermediate step described in the Reasoning Steps must be logically essential to derive the Final Answer. For each step question, the answer should correspond exactly to the question and not be random or irrelevant. Removing any intermediate step should break the logical chain required to answer the Final Question. 3. Rigor and Uniqueness of Steps: Each step (current node next node via relation text) must be specific enough to imply unique answer (next node) within the context of typical broad knowledge base (MOST IMPORTANT). The answer entity for any step must NOT appear directly within the Step Question. The cumulative questions should correctly integrate each step question. 4. Significance: The Final Question must address meaningful query about the entities and their relationships. It should not be trivial, overly generic, or based on obscure, impractical connections. Based on your evaluation, first provide brief textual explanation (reason) summarizing your evaluation and conclusion (why it passed or failed). Then, provide your final decision as JSON object with two keys: \"reason\": (string) Your explanation as described above. \"keep\": (string) Must be either \"yes\" or \"no\". Return ONLY the JSON object after your explanation. Figure 22: Multi-hop QA filter prompt for evaluating reasoning questions. 30 Page Ground Truth (GT) Prompt You are professional document analysis expert tasked with determining page relevance. Follow these guidelines precisely: Task Definition: Analyze the provided document page along with the question-answer pair to determine if the page contains relevant information. Decision Criteria: Respond with \"Yes\" ONLY if ALL of these conditions are met: The pages information is ESSENTIAL for understanding the query and answer. Respond with \"No\" if: The page contains no information related to the question. The information is only tangentially related and provides minimal value. The page contains partial information that requires significant inference or external knowledge to answer the question. Format Instructions: You must respond ONLY with \"Yes\" or \"No\" no explanations or additional text. Figure 23: Page ground truth prompt for document relevance assessment. Question Refinement Prompt You are an expert specializing in query analysis and refinement for multimodal RetrievalAugmented Generation (RAG) system. Your primary function is to disambiguate user questions by making them more specific. Task Definition: Your objective is to refine an ambiguous Original Question by leveraging designated Ground Truth Image and contrasting it with several irrelevant distractor images. Instructions: You will be given question, single Ground Truth Image (which will always be the first image provided), and one or more distractor images. Your task is to analyze the Original Question and compare the Ground Truth Image against the distractor images.By contrasting them, identify the key, distinguishing detail within the Ground Truth Image that makes it the unique and correct answer to the question. This detail should be absent or different in the distractor images. Finally, rewrite the Original Question to create new, more precise Refined Question that seamlessly incorporates this key detail, making the question unambiguous. Format Instructions: Your response MUST be valid JSON object with exactly two keys:: reason: string containing your reasoning process. question: Your refined question. Figure 24: Refinement Prompt for Ambiguous Question 31 Answer Generation Prompt(Oracle) You are professional document analyst Task Definition: Your primary task is to thoroughly analyze the user-provided image(s) to answer their question. Your answer must be supported by the visual evidence and details found within the image(s) Instruction: First, formulate brief reasoning process. This should explain how you derived your answer from the visual evidence and any contextual knowledge you applied. Second, provide concise and direct final answer in {language}. Format Instructions: Your entire response MUST be single, valid JSON object. This JSON object must contain exactly two keys: reason: string containing your English reasoning process. answer:A string containing your final answer in the requested language. Figure 25: Answer generation prompt for oracle experiments. 32 Answer Evaluation Prompt You are fair and objective grader. Your judgment should be based on balanced assessment. Task Definition: Your primary task is to evaluate the \"Generated Answer\" by comparing it against the \"Ground Truth Answer\", taking into account the original Question. Based on this evaluation, you will assign single integer score from 1 to 10. Scoring Rubric: You must adhere to the following 1-10 scale. Score 1-3 (Poor): The answer is largely incorrect, irrelevant, contains significant inaccuracies or hallucinations, or demonstrates fundamental misunderstanding of the question. Score 4-6 (Acceptable): The answer is partially correct but either misses important information, is somewhat vague, or contains minor inaccuracies. It shows some understanding but is not comprehensive. Score 7-8 (Good): The answer is correct and aligns well with the ground truth. It covers most key aspects but might lack few minor details, could be slightly less concise, or have some minor phrasing improvements. Score 9-10 (Excellent): The answer is fully correct, complete, and concise. accurately captures all essential aspects of the ground truth and is well-articulated. It Format Instructions: Your response MUST be valid JSON object with exactly two keys:: reason: reason: brief, one-sentence justification for your score . score: An integer from 1 to 10. Figure 26: Answer evaluation prompt for main experiments."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "South China University of Technology",
        "University of Maryland"
    ]
}