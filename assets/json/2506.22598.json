{
    "paper_title": "RExBench: Can coding agents autonomously implement AI research extensions?",
    "authors": [
        "Nicholas Edwards",
        "Yukyung Lee",
        "Yujun",
        "Mao",
        "Yulu Qin",
        "Sebastian Schuster",
        "Najoung Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 9 5 2 2 . 6 0 5 2 : r REXBENCH: Can coding agents autonomously implement AI research extensions? Nicholas Edwards1 Yukyung Lee2 Yujun (Audrey) Mao2 Yulu Qin2 Sebastian Schuster1,3 Najoung Kim2 1University College London 2Boston University 3University of Vienna nicholas.edwards@ucl.ac.uk sebastian.schuster@univie.ac.at {ylee5, amao, yuluqin, najoung}@bu.edu"
        },
        {
            "title": "Abstract",
            "content": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is critical capability for such systems, and introduce REXBENCH to support the evaluation of this capability. REXBENCH is benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. REXBENCH is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance."
        },
        {
            "title": "Introduction",
            "content": "Interesting research necessarily builds on other research. In this regard, extension of existing research is an important starting point to new investigations, potentially building up towards exciting novel discoveries. In light of recent growing interest in building LLM agents that can conduct scientific research in an autonomous manner, we propose REXBENCH, benchmark aiming to evaluate LLM agents ability to extend existing AI research, with an initial focus on Natural Language Processing (NLP) and Machine Learning (ML). More specifically, REXBENCH tests whether LLM agents can autonomously implement research extension experiments via code in hypothesis-guided manner [31], where the extension hypotheses are provided to the system as verbal instructions along with relevant background material including the research paper(s) and the corresponding codebase. Our benchmark consists of 12 realistic research extension tasks based on recently published research papers in the field, accompanied by domain expert-written extension instructions (See Appendix ,Equal contribution. The order of co-first authors was randomly determined. The benchmark and submission instructions can be found at https://rexbench.com/, and our agent implementations can be found at https://github.com/tinlaboratory/RexBench. Preprint. Under review. Figure 1: End-to-end workflow of REXBENCH: (1) An LLM agent receives inputs consisting of the research paper(s), the original codebase, and an extension instruction; (2) the system implements the extension and patch file is obtained; (3) the patch is applied to the original code and executed via our evaluation infrastructure; and (4) the results are evaluated using specified metrics. for sample task instruction). The extension tasks cover various aspects of implementation involving changes to the model, algorithm, data, and evaluation method. The main metric of success is numerical replication of the outcome of domain-expert implemented gold solutions for the extension task. We provide an automatic evaluation infrastructure to execute the LLM agent-implemented solutions and evaluate the outcomes. The executions of both the gold solutions and system solutions are conducted in virtual machines with exactly the same specifications to control for experimental variation. REXBENCH furthermore is robust to data contamination issues that affect the majority of existing benchmarks: the solutions and the success criteria for our extension tasks only exist in our held-out evaluation infrastructure and do not exist anywhere online. We tested nine agents based on an array of Large Language Model (LLM) backbones (Claude 3.7 Sonnet [2], o1 [17], o4-mini, DeepSeek R1 [14]), using three different agent frameworks (aider, Claude Code, OpenHands). Many agents struggled on our benchmark, achieving success rates close to zero for most tasks. Agents with Claude 3.7 Sonnet and o4-mini as backbone showed promise, often showing qualitative signs of success even when they did not arrive at correct final solutions. Nevertheless, even the best-performing agents succeeded only one fourth of the time on average (25% success rate for OpenHands + Claude 3.7 Sonnet and Claude Code), leaving much headroom for future progress. While the current REXBENCH tasks pose substantial challenges for the agents tested, most extensions do not require major rewriting of the codebase and are not extremely challenging in terms of complexity (at least to PhD-level domain expert). We thus consider the release of this specific set of tasks and the paper as contribution about the broader framework for evaluating research extensions (and the opportunities it may bring), which will motivate the development of more challenging extensions covering broader scientific domains, inviting contributions from the community."
        },
        {
            "title": "2 Related Work",
            "content": "Recent advancements in LLMs and agentic frameworks motivated discussions about their applicability to scientific research. This includes using LLMs and LLM-based agents for research automation [29, 38, 18, 51, 6, 4, 12, 24, 11] and benchmarking such systems in their ability to perform research in the domains of social sciences, statistics, and natural science [41, 38, 5, 13, 26]. For ML research in particular, current attempts span automation across all stages of the research process: from ideation [36] to experiment execution [3, 37, 39, 46, 34] to paper review and meta review [9]. There have also been early attempts to automate the full research pipeline [30, 25]. Another relevant line of effort is benchmarking the ability for coding and software engineering. Specific skills targeted include resolving GitHub issues (SWE-Bench, [19]), debugging LeetCode problems (DebugBench, [42]) and resolving configuration/dependency issues in research environment setups (SUPER, [3]). In similar vein, other benchmarks assess more comprehensive ML problemsolving and code implementation skills. MLE-bench [35] and DSBench [20] design machine learning and data science tasks akin to Kaggle-style competitions; MLAgentBench [16] gathers classical ML tasks such as regression and model training problems as well as Kaggle challenges; DataSciBench 2 Table 1: List of papers that form the bases for extensions in REXBENCH. Identifier CheckEval Extension Type Source paper Evaluation CheckEval: reliable LLM-as-a-Judge framework for evaluating text generation using checklists [27] COGS Model Entity Tracking Model COGS: Compositional Generalization Challenge Based on Semantic Interpretation [22] The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers [7] Code Pretraining Improves Entity Tracking Abilities of Language Models [23] Explain then Translate Algorithm Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations [40] Instruction Tuning Model Instruction Following without Instruction Tuning [15] Mission Impossible Data/Evaluation Mission: Impossible Language Models [21] Othello Data/Evaluation Reasoning or Reciting Model Re-reading Algorithm Tree of Thoughts Algorithm VariErr-NLI Model/Data WinoDict Data/Evaluation Emergent World Representations: Exploring Sequence Model Trained on Synthetic Task [28] Emergent Linear Representations in World Models of Self-Supervised Sequence Models [32] Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks [45] NAACL 2024 Re-Reading Improves Reasoning in Large Language Models [48] EMNLP 2024 Tree of Thoughts: Deliberate Problem Solving with Large Language Models [49] VARIERR NLI: Separating Annotation Error from Human Label Variation [44] WinoDict: Probing language models for in-context word acquisition [10] NeurIPS 2023 ACL 2024 EACL 2023 Venue HEAL CHI EMNLP 2020 EMNLP 2021 Preprint EMNLP Findings 2023 Preprint ACL ICLR 2023 BlackboxNLP 2023 evaluates data analysis and visualization skills with novel evaluation pipelines [50]; and ML-DevBench [33] focuses on the full ML development workflow. The most directly relevant efforts to ours are benchmarks that evaluate ML problem-solving and software engineering capabilities in research settings. Curie [25] aims to evaluate the ability to plan and execute experiments, Paper2Code [34] introduces multi-agent LLM framework to translate ML papers into codebases through stage-wise design, and PaperBench [39] evaluates such paper replication systems using compilation of replication tasks based on 20 ICML papers. REXBENCH has similar goal as PaperBench and, to some extent, Curie: the benchmarking of ML and AI research code generation. However, key distinction is that instead of evaluating replications (PaperBench) or very general questions that can often also be answered without running experiments (Curie), we focus on novel research extensions. Thus, REXBENCH is able to evaluate agent performance on previously unseen/unimplemented research hypotheses which greatly alleviates data contamination concerns."
        },
        {
            "title": "3 Benchmark Design",
            "content": "3.1 Research Extension Task Task We define our research extension task as code implementation problem, where the input consists of an existing research paper, an accompanying codebase, and an instruction that verbally describes an extension proposal and how this should be tested. An example of simple extension is: What would happen if the same experiment in paper used an open-source model like Llama 3 70B instead of GPT-4o? (see Appendix for an actual example). Given this input, system must produce as output edits to the input codebase that implements the extension proposal. Desiderata The core aim of our benchmark is to automatically assess how well an agent can autonomously implement realistic research extensions. These goals are to some extent in conflict with each other: Realistic research extensions tend to be quite open-ended, which makes automatic assessment challenging or impossible; limiting tasks to the availability of simple automatic measures, on the other hand, may constrain the task too much for it to be still realistic. We strike balance between these two goals by using automatic tests that allow the agent to tackle the task through any 3 means, as long as this leads to results comparable to the ones from our gold implementation. The task setting of requiring implementation on top of an existing codebase and evaluation through controlled execution environments (random seed, hardware, packages, etc.) serves to improve the reliability of the numeric output-based automatic evaluation. Nevertheless, each extension proposal included in the benchmark still cannot be too open-ended or exploratory, and therefore consist of specifically-scoped questions that can have well-defined numeric targets. To ensure that agents autonomously implement extensions, the granularity of our instructions are calibrated at level that still requires the model to thoroughly analyze the codebase and form its own plan for the extension. Furthermore, at no point of the evaluation do humans provide additional supervision. Finally, one of the biggest challenges with LLM evaluation is data contamination. If solutions to any of the tasks are openly available on the web, LLMs that serve as the backbone for the agents may have been trained on the solutions (also noted as possible issue in PaperBench [39]), rendering it impossible to establish whether success stems from memorization or autonomously solving the task. We circumvent this problem by including only novel research extensions, either in terms of the idea itself or implementation. To the best of our knowledge, none of our extensions exist on top of the existing codebases publicly; we store all the gold extensions in private Bitbucket repositories.2 Furthermore, our privately hosted evaluation infrastructure prevents models from gaining access to the evaluation scripts or reference solutions."
        },
        {
            "title": "3.2 Benchmark Composition",
            "content": "Our benchmark includes research extensions building upon 12 papers and codebases primarily in the NLP and broader AI domains, taking into consideration the availability of expertise within the team as well as the availability/replicability of the code released. The full list of papers is in Table 1.3 The specific extension proposals were selected to span various dimensions of change including changes to the model, dataset, algorithm, and evaluation. In addition to this consideration, we imposed the following constraints on the extension proposals for scientific rigor and feasibility of the experiments: (1) Important empirical trends from the original paper relevant to the extension proposal must replicate; (2) the gold implementation of the extension proposal must be replicable (e.g., if the gold implementation requires making calls to closed API-based model, this may not be replicable in the future due to model deprecation); and (3) the estimated runtime of each gold implementation should be shorter than 12 hours on single A100 GPU. The final dataset includes the extension instruction, target research papers in both .pdf and .md format (converted using PyMuPDF4LLM to accommodate agents that lack the ability to read .pdf files), and the original codebase. 3.3 Benchmark Construction Process For each extension proposal, domain expert (PhD student-level or above) first verified that the original codebase replicates the results of the associated paper on our virtual machines (details to follow). Then, they implemented the gold edits for the target extension and recorded the numerical outcomes, ensuring that the runtime does not exceed 12 hours. This implementation process and the outcomes were validated by at least one other author. Finally, the domain expert wrote the instruction that consists of brief description of the original paper, the extension proposal, and how this proposal should be tested. The description of the how was deliberately high-level to meet the desideratum of evaluating sufficiently autonomous capacity. Nevertheless, since the instructions should not be confusing or ambiguous, they were polished through multiple rounds of revisions by multiple authors to improve clarity. Importantly, if we foresaw degrees of implementation freedom that may introduce random variation, we controlled for this by specifying constraints (e.g., use an implementation of Pearson correlation function from the scipy package as opposed to implementing this from scratch). During this revision process we furthermore ensured that each extension was self-containedno part of the gold edits required information external to the set of inputs provided to the system. As part of the revisions for self-containment, we provided information such as specific model identifiers and explanations of necessary hyperparameters not in any README or the paper as part of the instruction, and added version information for all of the packages (via an environment.yml file in the repository). full instruction for one of our tasks (WinoDict) is shown in Appendix A. 2We use Bitbucket instead of GitHub since GitHub data has been used in the past to train LLMs and it is unclear whether this may also be true for some private repositories. 3Two of the tasks (COGS, Othello) involve implementing an extension proposal from another paper on top of the codebase of the original paper, where the implementation of the extension is either not publicly available or is not implemented as an edit of the original codebase. For these tasks, there are two relevant papers. 4 3.4 Evaluation Metrics Our main metric is final success rate, which measures whether the outcome of executing the modelimplemented code falls within the target range (more details below). We define two additional metrics for finer-grained analyses: execution success rate and file recall. We describe each metric below. Final Success Rate Final success rate evaluates whether the model correctly implements the specified research extension. This evaluation either checks whether the final results exactly match the results of the reference implementation (if the run is fully deterministic) or it checks whether the results fall within gold range that we obtained by running the gold implementation multiple times with different random seeds to account for output variability. In the latter case, model solution is considered successful if its final execution outcome falls within this bound. Execution Success Rate Execution success rate checks whether the generated code runs without errors in our evaluation environment. This metric evaluates the general well-formedness of the code and contextual understanding sufficient to avoid runtime errors. File Recall File recall quantifies whether files edited in the gold solution were also edited by the model: File Recall = Filesagent Filesgold / Filesgold. The limitation of this measure is the dependency on the gold solution. Technically, solution could achieve zero file recall with perfect final successe.g., if model solution was exactly equivalent to gold but created new files with identical content instead of editing, and changed references appropriately in the repository, this would be the case. Still, we take human expert edits to reflect reasonably efficient set of modifications. 3.5 Evaluation Infrastructure Submission format Our metrics defined above require execution of agent generated code. We propose to conduct this execution through virtual machine to control for hardware specification and packages installed. We will host this infrastructure using our own resources, and conduct evaluation asynchronously at regular interval to update the leaderboard with the submissions we receive, similarly to [19]. The submissions will be received in the form of git patch files (as opposed to the full repositories with agent implemented edits) to streamline the submission process. Additionally, we will request agent log files to verify that the task was completed autonomously by an agent. Infrastructure pipeline We host our evaluation infrastructure based on the OpenStack platform on an academic cloud computing service. For each patch file received, we launch an Ubuntu virtual machine instance with 20GB root disk, where we run task-specific Apptainer container [8] that has the original codebase and evaluation scripts pre-loaded and the environment set up. Each instance is also equipped with task-specific hardware: either single NVIDIA A100 40GB GPU, single K80 12GB GPU, or just CPU (see Appendix B, Table 2). To control for random variation of the execution outcomes to the best of our effort, we (1) fix all random seeds in the codebase wherever possible, and (2) run the evaluations with exactly the same hardware configuration as our gold runs. Inside the container, we apply the patch file and execute single bash script run_apptainer.sh that contains all necessary commands (this requirement is also provided in the task instructions). We limit the runtime to 12 hours, which is around twice the duration of the gold solution with the longest runtime among our extension tasks (see Table 2 for all estimated runtimes). Once task execution is complete or the attempt crashes, any result files and task execution logs are copied to an external storage volume. We then delete the virtual machine instance and evaluate the results. This setup ensures fully containerized and task-level parallelizable evaluation infrastructure."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Main Experiment We follow steps shown in Figure 1 and evaluate nine LLM agents, combining three agent frameworks with various LLM backbones (discussed below). We pass the full set of inputs for each task one by one to the agent to evaluate each task independently of each other. We run each task three times with the same agent model to account for agent random variation. 5 Figure 2: Agent performance on REXBENCH. The color coding indicates the agent framework and the axis indicates the the backbone LLM. Results include three runs per task to account for agent random variation. Error bars show standard error of the mean of all runs per model computed using the closed form formula (2σ, no normality assumption). 4.1.1 Baseline agent design We used three different agent frameworks: two open-source (aider [1] and OpenHands [43]) that we adapted for the task, as well as the fully proprietary Claude Code. aider and OpenHands both support range of backbone LLMs. We evaluated both with the OpenAI models o1 and o4-mini, the Anthropic Claude 3.7 Sonnet model, and the open-weight DeepSeek-R1 model. We discuss few design decisions shared between our agent models belownote that this does not imply that future submissions to our benchmark should be subject to the same design decisions. Shared design considerations For better runtime controllability, we disabled Python code execution for all agents to the best of our effort. Regarding the settings of the backbone LLMs, we set the temperature to 0.7 for Claude 3.7 Sonnet and DeepSeek-R1, and specified reasoning effort as medium for all OpenAI models. As discussed in Section 3.5, our evaluation infrastructure requires git patch files. We created the patch files using separate script after the agents had made changes to the codebase. We discuss individual implementation details for each agent in Appendix C. 4.2 Experiment with Additional Hints We conduct an additional set of experiments where we provide different levels of hints to the agents. This experiment serves two purposes: (1) as layer of sanity check that our tasks are possible to solve; (2) to gain more fine-grained understanding of where the difficulties lie, if the agents do find the tasks difficult without hints. We design two levels of hints, where the first level of hints mainly provides help with information localization, and the second level of hints provides step-by-step implementation guidance. Information localization hints, for instance, help find specific locations of edits by directly naming file to be edited (You would need to edit test_function() in src/testfile.py), help find necessary information (Look at the README to find the descriptions of the hyperparameters), or directly provide certain pieces of information that are part of the given input but nontrivial to find (Use ID #1014 for the special token). On the other hand, the second level of hints breaks down the gold solution into concrete implementation steps. Therefore, we expect the second level of hints to yield substantially higher success rates. In our experiments, hints are cumulative; when providing the second level of hints, the first level of hints is also provided. 4.3 Results Main experiment Our main results are shown in Figure 2. We see that most agents struggle with the task, with the best performing agents (OpenHands + Claude 3.7 Sonnet and Claude Code) achieving 25% average final success rates. Claude 3.7 Sonnet seemed to be the best backbone LLMwhen different LLMs were available it yielded the strongest performance, and Claude Code was tied as the best agent overall. o4-mini was the only other backbone LLM that yielded nonzero final success rate. All agents achieved nonzero (but generally low) execution success rate, demonstrating (perhaps Figure 3: Final success rates for each agent-LLM combination and hint level. Figure 4: Cost effectiveness and time efficiency of coding agents on REXBENCH. surprising) general failure of the agents at writing executable code. The agents overall achieved high file recall, showing that they were able to at least locate core edit targets based on the instructions. Additional hints Figure 3 (and Table 4 in Appendix E) show the results of additional experiments with two different hint levels. Generally, hints improve the final success rate, but tend to help less when the default success rate was zero, suggesting there is base level of competence required to make use of the hints provided. With the hints, we could substantially boost the performance of OpenHands + Claude 3.7 Sonnet, one of the best agents, up to 39% final success rate. Surprisingly, the second level of hints (that essentially takes care of high-level planning and edit localization) did not yield additional benefits on top of the first level of hints. 4.4 Resource Consumption Based on the final success rate, we plot the cost/time vs. performance tradeoff (Figure 4), showing that aider + o4-mini and OpenHands + Claude 3.7 Sonnet lie on the Pareto frontier for both cost and time, and additionally aider + Claude 3.7 Sonnet and Claude Code for time. We provide the full time and cost estimates for agent runs in Appendix E, Table 6. In terms of token usage statistics, aider consistently used 2 turns due to its non-iterative design. Claude Code used 2535 turns and OpenHands used 1758 turns, making use of active multi-turn structures. Due to its closed-source nature, we could not obtain token counts for Claude Code.4 OpenHands used the most tokens, especially with Claude 3.7 Sonnet and OpenAI o4-mini, reaching up to 542K prompt tokens (almost 180 times more than aider). As the hint levels increased, both turns and token usage in OpenHands tended to grow, while the turns in Claude Code decreased. See Table 5 in Appendix for token usage statistics by model and by hint levels."
        },
        {
            "title": "5 Analysis and Discussion",
            "content": "5.1 Patterns of Error We discuss notable error patterns, dividing them into explicit and implicit errors. We treat cases where the agent-generated code failed to execute as explicit errors, and cases where the execution succeeded but the experimental outcome did not match the numerical criteria as implicit. 4As of the papers release, token counts for Claude Code have become available, but this feature was not available when our experiments were conducted. 7 Explicit errors Explicit errors were automatically identifiable from execution logs. The most common source of error was empty diff file due to the failure of the agent to modify any code. The majority of these errors were from aider + {DeepSeek, o4-mini}. Often, the agents tried to solve the entire extension task in one shot rather than breaking it down, leading to incomplete or failed command executions during agent runs. Beyond this, most explicit errors were Python errors and they were mostly Python native errors rather than library-specific errors. Agents with Claude as backbone led to fewer SyntaxErrors (in particular, OpenHands + Claude had no SyntaxErrors), whereas o1 produced SyntaxErrors frequently. There were also several cases of execution timeout, which occurs when the experiment runtime exceeded the limit of 12 hours we set (no gold solution required more than 6 hours of execution time). This issue was predominantly encountered for the Instruction Tuning task, where most agent solutions were much more inefficient compared to the human-implemented solution. The full error distribution is shown in Figure 7 and Table 7 in Appendix E. Implicit errors Analysis of implicit errors (execution success but mismatch with gold outcome) involved greater manual effort because it required holistic review of agent edits. Therefore, we focused our analysis on the top 2 agents (OpenHands + Claude & Claude Code). Overall, the agents implicit errors were roughly split 3:2 between errors in implementation logic and errors in value (e.g., within-bounds index error, incorrect hyperparameter). We also estimated the debugging difficulty from the manually identified sources of error, using the scale of easy (requires small local fix), medium (requires logical but local revisions), and hard (requires holistic revisions). For both models, the majority of the errors were medium difficulty, with OpenHands + Claude having more implicit errors, especially ones falling into the easy category (5 easy, 9 medium, 1 hard) compared to Claude Code (1 easy, 9 medium, 2 hard), revealing qualitative difference in the agents solutions although the quantitative success rates were similar. 5.1.1 Qualitative Observations Errors without execution failure are difficult to analyze high-level observation is general pitfall associated with better performing models (for our task and coding tasks more generally)the cause of failure is difficult to identify. Better models more often produced code that successfully executes, in which case the reasons behind failure were not always easily traceable even for the human experts who implemented the solutions. This highlights the need for heavy sanity checks (perhaps supported by system design) if research agent were to be deployed in practice. Plausible-looking implementations that successfully execute can lead researchers to draw conclusions from faulty implementations, and over-reliance on coding agents may lead to proliferation of incorrect results. Overthinking is often an issue prominent issue with several backbone LLMs (Deepseek-R1, o1, o4-mini) was overthinking, where the thinking process was excessive both in terms of the number of output tokens and agent runtime, frequently leading to no actual output in terms of code generation. aider + DeepSeek-R1 was especially prone to this behavior, overthinking being one of the most prominent failure modes (close to one third of total failures). One possibility is that models reasoning behavior somehow clashes with the reasoning/thinking loop of the agent framework, although Claude 3.7 was an exception to this behavior despite being considered reasoning model. Agents vary in their ability to make use of hints As briefly noted in Section 4.3, providing additional guidance through hints did not necessarily improve the agents success rate, nor were the improvements always greater with more hints. We observed idiosyncratic task-level variation as well; for instance, for the Othello task, the 2 best performing agents (OpenHands + Claude 3.7 Sonnet, Claude Code) achieved 100% success rate with no hints and with the first level of hints, but 0% success rate when additionally given the second level of hints. Upon closer observation, these agents employed qualitatively different strategy with the second level of hints. It was not the case that this particular hint was misleading, since different agent (OpenHands + o4-mini) was able to use this hint to achieve 100% success rate on this task. This can be interpreted as models varying in their ability to implement different equally plausible solutions, and the step-by-step guideline in the second level of hints specifying different solution from the one that the model could implement easily. We noticed this pattern for two tasks (Othello and Tree-of-Thoughts), but not in general. Figure 5: Regression coefficients with 95% confidence intervals for predictors of final (Regression model: final_success line_change + file_count + success. repository_popularity + citation + (1 model)). (: < .05, : < .01) 5.2 What makes an extension difficult for agents? We hypothesize four sources of difficulty that could contribute to agent failure: (1) the amount of implementation required; (2) the size of the original codebase; (3) unfamiliarity with the codebase; and (4) unfamiliarity with the research topic. We operationalize them as: (1) line of code change in our gold solution; (2) file counts of the original codebase; (3) GitHub stars + forks (repository popularity); and (4) Google Scholar citations of the research paper(s), respectively. We use these as predictors of final success in mixed-effects linear regression model that includes the model identity as random effect. Figure 5 shows the regression coefficients with 95% CI. Line of gold changes has significant negative effect (β = 0.036, < 0.01) on final success, indicating that tasks requiring more changes to the codebase are more difficult. Repository popularity had significant effect but the effect size was negligible. Other factors (file and citation count) were not statistically significant."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented REXBENCH, benchmark evaluating the autonomous capacity of AI systems to implement hypothesis-driven research extensions in the domain of AI research. REXBENCH consists of realistic but well-scoped extension tasks motivated by existing research. To perform well on this benchmark, the system must be able to understand the expert-written extension instructions situated in specific research context, understand the structure and logic of the codebase that implements the original experiments, and autonomously plan and implement the requested extension. Our tasks are by design robust to data contamination due to the extensions requiring novel implementations whose solutions are not available publicly. Experiments testing an array of agent frameworks combined with competent backbone LLMs show that most systems struggle on our benchmark, with the best performing models (OpenHands + Claude Sonnet 3.7 and Claude Code) achieving 25% extension success rate. Notably, agents with o1 or DeepSeek R1 models as backbone showed (close to) zero success rate. Nevertheless, closer analysis of the best models revealed promise: they made substantially fewer syntax errors often leading to executable code, and often was on the right track for implementation, painting promising outlook. This observation, taken together with the large headroom, highlights the utility of REXBENCH for guiding future developments of research agents. The future of REXBENCH As discussed in the Introduction, we view the release of REXBENCH and this paper as motivating start to larger community-driven effort. While our tasks were primarily in the AI domain with focus on topics aligning with the expertise of our team, we believe the format of the extension task and evaluation framework illustrated in Figure 1 has much broader generalizability outside of our specific set of tasks. We hope the current set of tasks serves to draw interest of the community to research extension as an interesting problem for agents, and hope to collaborate with more researchers and/or solicit community contributions for more comprehensive coverage of task domains and implementation complexities. Limitations and broader impacts As discussed in Section 3.1, benchmark task being realistic inherently conflicts with the ease of automatic evaluation. In particular, task like research extension can be extremely open ended in reality, even when constrained with specific extension proposal and hypothesis. We opted for middle ground where we do not enforce strong limitations on how system 9 may implement the target extension and condition final success on alignment of numerical outcomes. This necessitated stronger control for sources of variation, which led us to write instructions as self-contained and unambiguous as possible. This setting is idealized in that they are much more informative and clearer than an actual task human researcher may face, even in scenarios where the extension idea is provided to them (e.g., an advisor suggesting to PhD student How about we try this time?), missing out on the real difficulties lying in the initial trial-and-error concretization step. Furthermore, while we provide three automatic metrics that measure different aspects of success, additional process-level metrics such as landmark evaluation [47, 3] would help alleviate the difficulty of post-hoc error analysis discussed in Section 5.1, especially for implicit errors, as well as reducing reward hacking or gamification of the benchmark. However, this requires substantial manual effort for creating and validating tailored automatic evaluation metrics at the individual task-level. Regarding broader societal impacts, the baseline agents we developed for this work did not reach the level of competence that we believe would translate into autonomous research extension capacities in the real world. Still, our benchmark may contribute to developing such systems in the future, which may have positive impacts such as contributing to better replicability and faster iterations of empirical hypothesis verification. On the other hand, given the difficulty of debugging errors, deployment of such systems without rigorous verification measures faces the danger of leading researchers to draw conclusions from faulty implementations and of the erosion of trust in published results."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by funding from Good Ventures Foundation and Open Philanthropy awarded to NK and SS, from Google awarded to NK, and from WWTF through the project Understanding Language in Context (WWTF Vienna Research Group VRG23-007) awarded to SS. We acknowledge that the computational work reported on in this paper was performed on the Shared Computing Cluster which is administered by Boston Universitys Research Computing Services and the shared computing cluster which is administered by New England Research Cloud (NERC). We additionally thank Augustine Abaris from BU SCC for technical advice, Max Nadeau and Ajeya Cotra from Open Philanthropy for initial project advice, and Zilu Tang for help with setting up the Explain then Translate task."
        },
        {
            "title": "References",
            "content": "[1] Aider AI. 2023. Aider: AI pair programming in your terminal. https://github.com/ Aider-AI/aider. Accessed: 2025-05-12. [2] Anthropic. 2024. Claude 3.7 Sonnet System Card. https://assets.anthropic.com/ m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf. Accessed: 2025-05-14. [3] Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1262212645, Miami, Florida, USA. Association for Computational Linguistics. [4] Daniil Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature, 624(7992):570578. [5] Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, et al. 2024. ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery, October 2024. arXiv:2410.05080. [6] Jonathan H. Choi. 2024. How to use large language models for empirical legal research. Journal of Institutional and Theoretical Economics (JITE), 180(2):214233. [7] Róbert Csordás, Kazuki Irie, and Juergen Schmidhuber. 2021. The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619634, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 10 [8] Singularity Developers. 2021. Singularity. [9] Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Cheng Jiayang, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. 2024. LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 50815099, Miami, Florida, USA. Association for Computational Linguistics. [10] Julian Martin Eisenschlos, Jeremy R. Cole, Fangyu Liu, and William W. Cohen. 2023. WinoDict: Probing language models for in-context word acquisition. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 94102, Dubrovnik, Croatia. Association for Computational Linguistics. [11] Kanishk Gandhi, Michael Li, Lyle Goodyear, Louise Li, Aditi Bhaskar, Mohammed Zaman, and Noah Goodman. 2025. BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery. arXiv:2501.01540. [12] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. 2025. Towards an AI co-scientist. arXiv:2502.18864. [13] Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike Merrill, Jeffrey Heer, and Tim Althoff. 2024. BLADE: Benchmarking Language Model Agents for Data-Driven Science. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1393613971, Miami, Florida, USA. Association for Computational Linguistics. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv:2501.12948. [15] John Hewitt, Nelson F. Liu, Percy Liang, and Christopher D. Manning. 2024. Instruction following without instruction tuning. arXiv:2409.14254. [16] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. MLAgentBench: evaluating language agents on machine learning experimentation. In Proceedings of the 41st International Conference on Machine Learning, pages 2027120309. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. OpenAI o1 system card. arXiv:2412.16720. [18] Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. 2025. CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation. arXiv:2503.22708. [19] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues? In The Twelfth International Conference on Learning Representations. [20] Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. 2024. DSBench: How Far Are Data Science Agents to Becoming Data Science Experts? arXiv:2409.07703. [21] Julie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. Mission: Impossible Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1469114714, Bangkok, Thailand. Association for Computational Linguistics. 11 [22] Najoung Kim and Tal Linzen. 2020. COGS: Compositional Generalization Challenge In Proceedings of the 2020 Conference on Empirical Based on Semantic Interpretation. Methods in Natural Language Processing (EMNLP), pages 90879105, Online. Association for Computational Linguistics. [23] Najoung Kim, Sebastian Schuster, and Shubham Toshniwal. 2024. Code pretraining improves entity tracking abilities of language models. arXiv:2405.21068. [24] Hiroaki Kitano. 2021. Nobel Turing Challenge: creating the engine for scientific discovery. NPJ systems biology and applications, 7(1):29. [25] Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, and Ang Chen. 2025. Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents. arXiv:2502.16069. [26] Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. 2024. Lab-bench: Measuring capabilities of language models for biology research. arXiv:2407.10362. [27] Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang, and Najoung Kim. 2025. Checkeval: reliable LLM-as-a-judge framework for evaluating text generation using checklists. arXiv:2403.18771. [28] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Emergent World Representations: Exploring Sequence Model Trained on Synthetic Task. In The Eleventh International Conference on Learning Representations. [29] Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, and Hengxing Cai. 2025. SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding. In The Thirteenth International Conference on Learning Representations. [30] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv:2408.06292. [31] Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. 2025. LLM4SR: Survey on Large Language Models for Scientific Research. arXiv:2501.04306. [32] Neel Nanda, Andrew Lee, and Martin Wattenberg. 2023. Emergent Linear Representations in World Models of Self-Supervised Sequence Models. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 1630, Singapore. Association for Computational Linguistics. [33] Harshith Padigela, Chintan Shah, and Dinkar Juyal. 2025. ML-Dev-Bench: Comparative Analysis of AI Agents on ML development workflows. arXiv:2502.00964. [34] Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. 2025. Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning. arXiv:2504.17192. [35] Chan Jun Shern, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. 2024. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. arXiv:2410.07095. [36] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can LLMs generate novel research ideas? large-scale human study with 100+ NLP researchers. arXiv:2409.04109. [37] Zachary Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, and Arvind Narayanan. 2024. CORE-Bench: Fostering the Credibility of Published Research Through Computational Reproducibility Agent Benchmark. Transactions on Machine Learning Research. [38] Michael D. Skarlinski, Sam Cox, Jon Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, and Andrew D. White. 2024. Language agents achieve superhuman synthesis of scientific knowledge. arXiv:2409.13740. 12 [39] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. 2025. PaperBench: Evaluating AIs Ability to Replicate AI Research. arXiv:2504.01848. [40] Zilu Tang, Mayank Agarwal, Alexander Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. 2023. Explain-then-translate: an analysis on improving program translation with self-generated explanations. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 17411788, Singapore. Association for Computational Linguistics. [41] Minyang Tian, Luyu Gao, Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, HAO TONG, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. 2024. SciCode: Research Coding Benchmark Curated by Scientists. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [42] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Hui Haotian, Liu Weichuan, Zhiyuan Liu, and Maosong Sun. 2024. DebugBench: Evaluating Debugging In Findings of the Association for Computational Capability of Large Language Models. Linguistics: ACL 2024, pages 41734198, Bangkok, Thailand. Association for Computational Linguistics. [43] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2025. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. In The Thirteenth International Conference on Learning Representations. [44] Leon Weber-Genzel, Siyao Peng, Marie-Catherine De Marneffe, and Barbara Plank. 2024. VariErr NLI: Separating Annotation Error from Human Label Variation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22562269, Bangkok, Thailand. Association for Computational Linguistics. [45] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2024. Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 18191862, Mexico City, Mexico. Association for Computational Linguistics. [46] Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. 2025. SciReplicateBench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers. arXiv:2504.00255. [47] Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig. 2024. TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks. arXiv:2412.14161. [48] Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou, and Shuai Ma. 2024. Re-Reading Improves Reasoning in Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1554915575, Miami, Florida, USA. Association for Computational Linguistics. [49] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc. 13 [50] Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, and Yisong Yue. 2025. Datascibench: An LLM agent benchmark for data science. arXiv:2502.13897. [51] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2024. Can Large Language Models Transform Computational Social Science? Computational Linguistics, 50(1):237291. An Example Task Instruction (Extension of WinoDict) WinoDict Task Instruction"
        },
        {
            "title": "Problem Description",
            "content": "Background The paper WinoDict: Probing language models for in-context word acquisition (Eisenschlos et al. 2023) attempts to measure LLMs ability to learn novel words during inference. They rewrite Winograd-style co-reference resolution problems by replacing the key concept word with synthetic but plausible English word and adding the definition of the new concept as suffix. Building on this work, we would like to further consider learning setting where the form of the learned words coincides with existing English words and explore how their existing meanings may interfere with the models word acquisition from the given definition. The hypothesis is that overriding existing words would be more difficult, and the frequency of the existing words may also modulate the effect. The paper will be available inside the provided repository in both PDF format as eisenschlos_et_al_2023.pdf and markdown format as eisenschlos_et_al_2023.md if you need to refer to it. Extension to be implemented Your task is to modify the codebase provided to generate new Winodict datasets by replacing the target word being learned with an existing English word. The new dataset should be stored under the directory ./data. Your replacement should consider the POS tags of the original word - they should be matched. We will only consider four POS categories for word replacement: nouns, verbs, adjectives, and adverbs. To test the possible effect of frequency, sample the candidates from different frequency groups: 1. Top Group: Verbs, Nouns, Adverbs: Select the top 20% most frequent words Adjectives: Select the top 35% most frequent adjectives (to match the sample set size) 2. Bottom Group: Verbs, Nouns, Adverbs: Select the bottom 20% least frequent words Adjectives: Select the bottom 35% least frequent adjectives. 3. All Group: Verbs, Nouns, Adjectives, Adverbs: Include all words, no frequency-based filtering the frequency information will be provided in form of each POS, four files correAssume that 2_all_rank_verb.txt, named 1_all_rank_noun.txt, sponding to 3_all_rank_adjective.txt, 4_all_rank_adverb.txt, under the directory ./words/. Each file lists words in descending order of frequency from the British National Corpus. To generate the dataset, you need to create word candidates based on the files and sample words from those candidates. From each group, sample words from the candidate lists to generate the new Winodict dataset. Ensure that the replacement word is inflected to match the morphological properties of the original word being replaced. For instance, if the original word is past tense verb, the selected replacement must also be in the past tense. Please use spaCy with the lemminflect module to inflect the selected words as necessary. Using the new dataset, you should run experiments on the Winodict-Winograd dataset under the 5-shot setting. Assume that the model can be found under /stage/hf_cache/gemma-2-9b, although this may not be visible in the provided repository right now. Furthermore, we will only consider the setting where definitions are appended as suffixes, which are represented as the last_def template in the codebase. Save your results as three different files under ./results/, corresponding to the three sampling groups defined above. They should be named res_top.json, res_bottom.json, and res_all.json. Please make the experiment runnable by implementing single script called run_final.sh in the root of the repository. This script should call all necessary commands with all parameters specified and should not have any command line arguments itself. It should handle both the dataset generation as well as the execution of the experiments on this new dataset. Try your best to keep everything else not specified above constant in the original repository. Also, the environment is already set up, so you do not need to install any dependencies or download any more datasets/models. Please refer to environment.yml in the repository for an overview of the installed libraries and their versions. Evaluation To evaluate the extension, we will execute the run_final.sh script you wrote. We will use the three .json files mentioned above that contain the final results to evaluate the experimental outcome."
        },
        {
            "title": "B Detailed Experimental Setup",
            "content": "Table 2: Resource requirements for each task. Task Instance Type Runtime Duration (Gold Solution) CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CPU K80 A100 CPU A100 A100 K80 A100 A100 A100 A100 1m 5h 2h <1m 5h 4h 1h 6h 30m 20m 10m 30m Table 2 shows the details about the execution environment for each task."
        },
        {
            "title": "C Detailed Agent Configurations",
            "content": "Table 3: Agent components Component aider Claude Code OpenHands Repo navigation Tool use Bash execution Python execution Table 3 provides an overview of what kind of abilities each agent has. aider aider is an open-source agent framework. We implemented our most basic agent based on aider, using the diff edit format where the LLM specifies file changes as search/replace blocks. We allowed up to 5 retries to handle API-side overload errors. Since aider lacks built-in file search capabilities, we added preliminary stage where the LLM is given the codebases directory tree along with the task instruction to identify files requiring modification. Unlike Claude Code and OpenHands, our aider implementation does not use bash execution or tools. Claude Code Claude Code is an interactive command-line tool that can be run from within repository. We used Claude 3.7 Sonnet as the backbone in our experiments. Claude Code can then navigate the repository and make edits to files or create new files. We manually evaluated Claude Code with the prompt Read the instructions in instructions.md and carry out the specified task. Furthermore, to avoid Python executions by the agent, we added the instruction Please do not execute any code, just read relevant files and make any necessary modifications. to the task-specific instructions. Since this tool does not allow for modification of system prompts or support any other customization, everything else about this agent was left as defined by its developers. We additionally applied minimal postprocessing to patches (only needed for the Re-reading task) containing an absolute filepaththe agent was evaluated locally and receives the absolute path to the codebase directory as input in an uncontrollable fashion, but patches are evaluated inside virtual machine with different filepath structure. OpenHands OpenHands is an open-source agent framework that uses an LLM to control range of pre-defined tools for understanding and modifying codebases. To make this agent more fairly 16 comparable to Claude Code, we modified the system prompt and the agent to disable execution of Python code. The agent was allowed to execute bash commands such as grep and cat, browse the web, load PDFs in browser (if compatible with the backbone LLM), and edit files. We prompted this agent with the same one-line instruction as for Claude Code. We evaluated this agent in headless mode in which the agent executes the task without any user input until the LLM signals task completion to the agent, or the agent detects loop or reaches maximum number of steps (250). As with Claude Code, we applied postprocessing to absolute filepaths to make them compatible with the virtual machine evaluation environment, since the OpenHands agent is run inside its own Docker container. Tool use/action distribution OpenHands agents interact with external tools during execution, and we analyze how their tool usage varies across different LLMs. Claude 3.7 Sonnet and OpenAI o4-mini showed the highest overall tool usage. File operations (str_replace_editor) and bash commands (execute_bash) were the most frequently used tools across all models (see Figure 6) but occasionally the agent did also perform web searches or use browser to render the paper PDF. Figure 6: Tool usage distribution across OpenHands agent implementations. Percentages indicate the frequency of each tool type, while the total usage count is shown in each column header."
        },
        {
            "title": "E Detailed Experimental Results",
            "content": "Table 4 shows the detailed results for all metrics and each agent-LLM combination across all three hint levels. Table 5 shows the number of turns as well as the number of input and output tokens, averaged across the three runs for each agent. Table 6 shows the costs and duration for running each agent on single task on average, as well as the total cost and total durations, based on the main experiment only (providing no hints). Including preliminary and failed runs not reported in the main paper, we estimate that the total compute required for the full project was approximately 45x the reported amount. Table 7 shows the detailed breakdown of errors for each agent and LLM combination. Tables 8 to 16 show the detailed breakdown of task specific performance for each agent and LLM combination."
        },
        {
            "title": "F License Information",
            "content": "The codebase portion of REXBENCH is constructed from public repositoriesdetails of the licenses for each task are provided in Table 17. When the codebase did not contain any license information, we reached out to the authors for more information and used their suggestion (one response still pending at the time of writing, but we make an educated guess that the repository will be associated with permissive license given that the paper was written by authors with primarily academic affiliations, 17 Table 4: Detailed performance on REXBENCH, evaluated across three hint levels. Results are averaged across three runs. Model Hints Level File Recall Execution Success Final Success Agent aider Claude 3.7 Sonnet OpenAI OpenAI o4-mini DeepSeek-R1 Claude Code Claude 3.7 Sonnet OpenHands Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek-R1 No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints 0.87 0.86 0.86 0.84 0.78 0.80 0.43 0.43 0.43 0.18 0.13 0.13 0.76 0.84 0.88 0.76 0.87 0.92 0.64 0.67 0.78 0.68 0.77 0.74 0.74 0.76 0.71 0.39 0.31 0.33 0.22 0.31 0.39 0.19 0.25 0.31 0.00 0.00 0.00 0.33 0.50 0.42 0.42 0.53 0.53 0.31 0.33 0.39 0.39 0.36 0.47 0.14 0.14 0. 0.14 0.17 0.08 0.00 0.00 0.03 0.03 0.19 0.14 0.00 0.00 0.00 0.25 0.28 0.25 0.25 0.39 0.36 0.00 0.08 0.03 0.08 0.19 0.14 0.00 0.00 0.08 Figure 7: Distribution of execution errors across Python, Bash, and timeout categories. Errors with fewer than 5 occurrences are grouped as Others. and from the fact that the public availability of their codebase is mentioned in the paper). We release our data and code under dual license (MIT and Apache 2.0), given the mixed license of the repositories included in the full benchmark suite. Table 5: Token usage statistics across agents and models. Model Hints Level Total Turns (Avg.) Prompt Tokens (Avg.) Output Tokens (Avg.) Agent aider Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek-R Claude Code Claude 3.7 Sonnet OpenHands Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek-R1 No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints No hints Hints Detailed Hints 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 34.64 29.61 25. 50.94 47.92 43.72 16.58 23.61 27.94 53.47 54.64 57.72 34.00 34.78 36.19 3,053.60 3,029.10 3,529.20 2,964.60 3,061.50 3,447.60 2,910.60 3,002.80 4,457.30 2,963.80 3,045.80 3,446.20 542,311.69 540,580.78 458,430.67 82,144.81 137,508.86 183,731.94 522,734.39 511,121.25 565,430.33 194,548.08 202,214.92 293,836.28 5,204.20 4,222.20 3,996.20 5,302.20 6,052.80 6,011.10 4,286.10 2,875.30 3,388.80 3,557.10 3,751.50 3,378.70 7,492.75 7,116.36 6,882.17 9,592.97 13,816.28 18,861.33 25,140.58 25,899.81 26,719.36 18,099.22 19,673.08 20,307.64 Table 6: Cost and duration statistics across agents and models (main experiment). Model Avg. Cost ($) Avg. Duration Total Cost ($) Total Duration Agent aider Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek-R1 Claude Code Claude 3.7 Sonnet OpenHands Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek-R1 0.41 0.62 0.03 0.04 0.60 0.40 1.30 0.61 0.04 1m 44s 3m 6s 41s 4m 38s 2m 44s 2m 43s 2m 25s 5m 2s 13m 57s 14.77 22.37 1.02 1.46 21.94 14.22 46.93 22.09 1.60 1h 2m 40s 1h 51m 58s 24m 56s 2h 42m 10s 1h 38m 45s 1h 38m 6s 1h 27m 4s 3h 0m 11s 7h 40m 38s 19 Error Type aider Claude Code OpenHands Table 7: Breakdown of error counts by agent and error type. Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek Claude 3.7 Sonnet Claude 3.7 Sonnet OpenAI o1 OpenAI o4-mini DeepSeek R1 Python Errors AssertionError AttributeError FileNotFoundError ImportError IndentationError IndexError IsADirectoryError KeyError ModuleNotFoundError NameError NotImplementedError OSError RuntimeError SyntaxError TypeError UnboundLocalError ValueError EOFError Python Library Errors DatasetNotFoundError NotFoundError OutOfMemory ArgumentError ScannerError Bash Errors cannot create directory empty patch empty or missing unable to write file Permission denied syntax error cannot access Execution Timeout 0 11 23 6 0 0 0 5 0 1 0 0 0 0 3 0 10 1 1 0 0 0 0 0 0 0 0 0 0 0 2 0 5 11 6 0 1 0 2 3 1 0 0 1 5 2 1 8 0 0 1 0 0 1 0 4 0 2 0 0 7 1 2 4 0 0 0 0 6 0 0 0 0 0 4 6 0 6 0 0 0 0 0 0 0 35 5 0 0 0 0 0 3 6 2 0 1 0 4 0 0 0 0 0 2 10 0 24 0 0 1 0 0 1 0 0 0 0 0 1 0 4 0 7 4 0 0 1 0 4 3 0 0 0 0 0 13 0 11 0 0 0 0 0 0 0 1 0 0 0 1 0 2 2 3 0 10 1 0 1 0 5 0 0 0 13 2 0 4 0 0 1 0 0 0 1 9 3 0 1 0 0 2 2 1 1 8 0 0 2 1 2 1 0 0 7 7 0 3 0 0 0 2 5 0 0 10 7 0 0 0 0 2 0 5 3 3 11 1 1 10 1 4 0 1 0 5 7 0 8 0 0 0 0 0 0 9 4 0 1 0 0 1 0 0 2 0 0 0 0 1 0 1 0 0 0 0 3 0 4 0 0 0 0 0 0 0 59 25 3 0 0 0 20 Table 8: Detailed performance on aider + Claude 3.7 Sonnet. Agent Model Hint Level Task File Recall Execution Success Final Success 1.00 0.50 1.00 1.00 0.50 1.00 1.00 0.60 1.00 1.00 1.00 0.75 1.00 0.50 1.00 1.00 0.50 1.00 1.00 0.40 1.00 1.00 1.00 0.75 1.00 0.50 1.00 1.00 0.50 1.00 1.00 0.40 1.00 1.00 1.00 0.75 0.00 0.00 0.33 1.00 0.67 1.00 1.00 0.00 0.33 0.00 0.00 0.33 0.00 0.00 0.67 1.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.67 0.00 0.00 1.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.33 0.00 0.00 0.33 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.67 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 aider Claude 3.7 Sonnet No Hints Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 21 Agent Model Hint Level Task File Recall Execution Success Final Success Table 9: Detailed performance on aider + o1. aider o1 0.83 1.00 1.00 1.00 0.00 1.00 1.00 0.33 1.00 1.00 1.00 0.75 0.67 1.00 1.00 1.00 0.00 1.00 1.00 0.60 1.00 1.00 1.00 0.67 0.67 1.00 1.00 1.00 0.00 1.00 1.00 0.47 1.00 0.83 1.00 0. 0.33 0.00 1.00 0.33 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.67 0.67 0.00 1.00 0.00 0.00 0.67 0.33 0.00 0.00 1.00 0.00 0.67 0.00 0.67 1.00 0.33 0.00 0.67 1.00 0.00 0.33 0.00 0.00 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Hints No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 22 Table 10: Detailed performance on aider + o4-mini. Agent Model Hint Level Task File Recall Execution Success Final Success 0.67 0.00 0.67 0.67 0.00 0.83 1.00 0.27 0.00 0.83 0.33 0.25 0.00 0.00 1.00 1.00 0.00 1.00 1.00 0.33 0.00 0.17 0.33 0.58 0.33 0.33 1.00 0.67 0.17 0.67 0.67 0.47 0.00 0.67 0.67 0.42 0.00 0.00 0.67 0.33 0.00 0.33 0.33 0.00 0.00 0.00 0.00 0.33 0.00 0.00 1.00 0.00 0.00 0.67 1.00 0.00 0.00 0.00 0.00 0.33 0.00 0.67 1.00 0.67 0.33 0.33 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.33 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.67 0.33 0.00 0.00 0.67 0.00 0.00 0.00 0.00 0.00 aider o4-mini No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval Hints COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 23 Table 11: Detailed performance on aider + Deepseek-R1. Agent Model Hint Level Task File Recall Execution Success Final Success 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.00 0.33 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.13 0.00 0.00 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 aider Deepseek-R1 No Hints Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 24 Table 12: Detailed performance on Claude Code + Claude 3.7 Sonnet. Agent Model Hint Level Task File Recall Execution Success Final Success 0.50 0.50 1.00 1.00 0.83 0.67 1.00 0.40 1.00 1.00 1.00 0.25 0.50 0.50 1.00 1.00 0.83 1.00 1.00 0.40 1.00 1.00 1.00 0.75 0.83 0.50 1.00 1.00 1.00 1.00 1.00 0.40 1.00 1.00 1.00 0.75 0.00 1.00 0.00 1.00 0.00 0.33 1.00 0.33 0.00 0.00 0.00 0.33 0.00 1.00 0.00 1.00 0.33 0.67 1.00 0.00 0.67 1.00 0.67 0.00 0.00 1.00 0.33 1.00 0.33 1.00 0.67 0.33 0.33 0.00 0.00 0.33 0.00 1.00 0.00 0.67 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.33 0.00 1.00 0.00 0.33 0.33 0.00 1.00 0.00 0.00 0.67 0.00 0.00 0.00 1.00 0.33 1.00 0.00 0.67 0.00 0.00 0.00 0.00 0.00 0.00 Claude-Code Claude 3.7 Sonnet No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval Hints COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 25 Table 13: Detailed performance on OpenHands + Claude 3.7 Sonnet. Agent Model Hint Level Task File Recall Execution Success Final Success 0.50 0.50 1.00 1.00 1.00 0.50 1.00 0.40 1.00 1.00 1.00 0.25 0.67 1.00 1.00 1.00 0.50 1.00 1.00 0.40 1.00 1.00 1.00 0.75 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.40 1.00 0.83 1.00 0. 0.00 1.00 0.00 1.00 0.67 0.00 1.00 0.00 0.33 0.33 0.33 0.33 0.67 1.00 1.00 1.00 0.00 0.67 1.00 0.00 0.00 0.67 0.00 0.00 0.33 1.00 1.00 1.00 0.67 1.00 1.00 0.00 0.00 0.33 0.00 0.00 0.00 0.67 0.00 1.00 0.33 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.33 1.00 1.00 0.67 0.33 0.00 1.00 0.00 0.00 0.33 0.00 0.00 0.00 1.00 1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.33 0.00 0.00 OpenHands Claude 3.7 Sonnet No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval Hints COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 26 Table 14: Detailed performance on OpenHands + o1. Agent Model Hint Level Task File Recall Execution Success Final Success OpenHands o1 0.50 0.33 1.00 1.00 0.17 0.50 1.00 0.20 1.00 0.83 0.67 0.25 0.50 0.50 1.00 1.00 0.00 0.50 1.00 0.20 1.00 0.83 0.83 0.17 0.83 0.83 0.67 1.00 1.00 0.67 1.00 0.40 1.00 0.83 0.83 0.25 1.00 0.00 0.33 1.00 0.00 0.00 0.00 0.00 0.00 0.67 0.00 0.33 1.00 0.00 1.00 0.33 0.00 0.67 0.00 0.00 0.00 0.67 0.00 0.33 0.00 1.00 0.67 0.67 0.33 0.00 0.33 0.00 0.00 1.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. Hints No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 27 Table 15: Detailed performance on OpenHands + o4-mini. Agent Model Hint Level Task File Recall Execution Success Final Success 0.50 0.50 1.00 1.00 0.67 0.17 0.83 0.40 1.00 1.00 0.67 0.25 0.33 0.50 1.00 1.00 0.67 0.67 1.00 0.27 1.00 1.00 1.00 0.67 0.50 0.50 0.67 1.00 0.67 1.00 1.00 0.40 1.00 0.67 0.50 0.67 0.33 0.33 1.00 0.67 0.00 0.00 0.33 0.33 0.67 0.67 0.00 0.33 0.00 0.67 1.00 0.33 0.00 0.67 0.67 0.00 1.00 0.00 0.00 0.00 0.33 0.67 0.67 0.67 0.00 0.67 1.00 0.67 1.00 0.00 0.00 0.00 0.00 0.33 0.33 0.00 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.33 0.00 0.67 0.67 0.00 0.00 0.00 0.67 0.00 0.33 0.00 0.00 0.00 0.00 0.67 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 OpenHands o4-mini No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval Hints COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 28 Table 16: Detailed performance on OpenHands + Deepseek-R1. Agent Model Hint Level Task File Recall Execution Success Final Success OpenHands Deepseek-R1 No Hints CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Detailed Hints CheckEval Hints COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict 0.50 0.50 0.67 1.00 0.33 0.33 1.00 0.27 0.67 1.00 0.83 0.25 0.83 0.50 0.67 1.00 0.67 0.67 1.00 0.40 1.00 0.83 1.00 0.17 1.00 0.50 0.67 1.00 0.67 1.00 1.00 0.27 1.00 0.67 1.00 0.25 0.00 0.33 0.00 0.33 0.33 0.00 0.33 0.00 0.00 0.33 0.00 0.00 0.33 0.00 0.00 0.67 0.00 0.00 0.33 0.00 0.33 0.00 0.33 0.00 0.00 0.33 0.67 1.00 0.00 0.00 0.33 0.00 0.33 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.67 0.00 0.00 0.33 0.00 0.00 0.00 0.00 0.00 Table 17: Licenses for each Github repository. Task CheckEval COGS Entity Tracking Explain then Translate Instruction Tuning Mission Impossible Othello Reasoning or Reciting Re-reading Tree of Thoughts VariErr-NLI WinoDict Repository yukyunglee/CheckEval najoungkim/COGS najoungkim/code-models-entity-tracking PootieT/explain-then-translate john-hewitt/implicit-ins jkallini/mission-impossible-language-models likenneth/othello_world ZhaofengWu/counterfactual-evaluation EleutherAI/lm-evaluation-harness princeton-nlp/tree-of-thought-llm mainlp/VariErr-NLI License MIT MIT Apache 2.0 MIT Apache 2.0 ??? MIT MIT MIT MIT MIT google-research/language/tree/master/language/wino_dict Apache 2."
        }
    ],
    "affiliations": [
        "Boston University",
        "University College London",
        "University of Vienna"
    ]
}