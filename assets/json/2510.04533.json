{
    "paper_title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling",
    "authors": [
        "Hyunmin Cho",
        "Donghoon Ahn",
        "Susung Hong",
        "Jee Eun Kim",
        "Seungryong Kim",
        "Kyong Hwan Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 3 3 5 4 0 . 0 1 5 2 : r TAG: TANGENTIAL AMPLIFYING GUIDANCE FOR HALLUCINATION-RESISTANT DIFFUSION SAMPLING Hyunmin Cho1, Donghoon Ahn2, Susung Hong3, Seungryong Kim4, Kyong Hwan Jin1 1Korea University, 2University of California, Berkeley, 3University of Washington, 4KAIST AI Jee Eun Kim1,"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is plug-and-play, architectureagnostic module that improves diffusion sampling fidelity with minimal computational addition, offering new perspective on diffusion guidance.1 (a) No Guidance (b) TAG Update Figure 1: Conceptual visualization of Tangential Amplifying Guidance (TAG) from modeinterpolation perspective (Aithal et al., 2024). Unlike (a) no guidance case, (b) TAG decomposes the base increment k+1 on the latent sphere into parallel Pk+1k+1 and orthogonal (i.e., tangential) k+1k+1 components (equation 6). By preserving the parallel component while adding scaled tangential component, TAG isolates the data-relevant part of the update (3) and can more effectively navigate the data manifold, leading to samples that contain more semantic structure. We make this precise by proving that amplifying the tangential has the effect of guiding the trajectories toward regions of higher model density while mitigating off-manifold drift (4, equation 15)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Hallucination in diffusion models refers to the phenomenon of generating samples that violate the data distribution or contradict conditioning, thus failing to provide meaningful outputs. For example, co-first authors {hyun_cho@korea.ac.kr, donghoon@berkeley.edu, susung@cs.washington.edu} correspondence to {seungryong.kim@kaist.ac.kr, kyong_jin@korea.ac.kr} 1Project page is available at: https://hyeon-cho.github.io/TAG/ 1 it often manifests as mixed-up objects (Okawa et al., 2023) or anatomically implausible structures (e.g., extra-fingers hands). Recent evidence suggests that the primary source of such errors lies in failure mode known as mode interpolation. During sampling, trajectories may traverse low-density valleys between distinct modes of the data distribution, causing attribute mismatches and structural inconsistencies (Aithal et al., 2024). widely adopted remedy involves inference-time guidance strategies, such as classifier-free guidance (CFG) (Ho & Salimans, 2021) and their variants (Hong et al., 2023; Ahn et al., 2024; Karras et al., 2024; Rajabi et al., 2025; Kwon et al., 2025; Sadat et al., 2025; Dinh et al., 2025; Hong, 2024). Under the assumption that deviating from low-probability regions enhances sample quality, most of these methods employ residual scaling, using the difference between the conditional and unconditional branches to guide the generation process away from the unconditional models outputs. While effective, these mechanisms are fundamentally indirect: instead of navigating along the intrinsic geometry of the data distribution, they proceed by repeatedly moving away from an unconditional estimate at each step of the process. In contrast, we propose more efficient direct solution grounded in Tweedies identity (Tweedie et al., 1984), which relates the score to the posterior mean under Gaussian corruption. This link motivates decomposition of the model update based on its intrinsic geometry: drift component that advances the radius along the prescribed noise schedule (i.e., noise level), and tangential component that moves along the data-manifold, approximately preserving the overall radius while refining the samples structure and semantics. We observe that the tangential component carries rich structural information (Figure 2), and amplifying it reduces out-of-distribution samples (Figure 3). Drawing upon the principle of amplifying the tangential component during inference, we derive Tangential Amplifying Guidance (TAG), plug-and-play method that emphasizes the tangential component of the score update. TAG steers the sampling trajectory to follow the underlying data manifold closely. TAG integrates seamlessly with standard diffusion backboneswhether conditioned or notwithout requiring additional denoising evaluations or retraining. We can summarize our contributions as follows: We establish concrete link between the scores intrinsic geometry and sample quality, proving that amplifying the tangential components of the scores steers sampling trajectories toward the in-distribution manifold. We introduce TAG, computationally efficient and architecture-agnostic algorithm that realizes this geometric principle in practice."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Score-based Diffusion Model. Score-based generative models learn time-indexed score function that approximates the gradient of the log-density of noise-perturbed data, sθ(x, tk) log p(x tk), tk {tK > > t0} denotes the k-th discretized timestep, to reverse gradual noising process for sample generation. This approach provides continuoustime framework that unifies earlier discrete-time Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) through the lens of stochastic differential equations (SDEs) (Song et al., 2020b). The core idea involves forward-time SDE that transforms complex data into simple prior distribution, given by dxk = (xk, tk)dtk + g(tk)dWtk. Generation is then performed by corresponding reverse-time SDE, which becomes tractable by substituting the unknown true score with the learned model sθ (Anderson, 1982). This score network, typically noise-conditional U-Net, is trained efficiently via denoising score matching across various noise levels (Vincent, 2011; Song & Ermon, 2019). For sampling, one can use numerical methods like predictor-corrector schemes to simulate the stochastic reverse SDE, or solve an associated deterministic ordinary differential equation (ODE) known as the probability-flow ODE. This continuous-time framework not only provides theoretical basis for widely used deterministic samplers like DDIM (Song et al., 2020a) but has also inspired modern refinements, such as the preconditioning and parameterization in EDM (Karras et al., 2022), which further enhance the trade-off between sample quality and efficiency. k A 281 481 681 781 881 581 = = 501 Gen. sample Figure 2: Amplifying the tangential component enhances semantic content by isolating it from noise. This figure illustrates the decomposition of the update step into normal and tangential components. Subtracting the unstructured, noisy normal component Pkk from the original update acts as denoising operation, revealing the tangential component k, which preserves the principal semantic structure. Images decoded from intermediate timesteps (t=981, 501) indicate that semantic information is most salient in the tangential component. Motivated by this observation, our method TAG amplifies this semantically rich component, yielding clearer and more coherent final sample (far right) than that obtained from the unmodified (Please zoom-in for details). = 981 181 Inference-Time Guidance. Numerous methods modify the update field during sampling to improve fidelity, typically without requiring retraining. Early approaches (Ho & Salimans, 2021) often rely on residual signals, which scale the update residual to better align samples with desired condition. However, as Dhariwal & Nichol (2021); Kynkäänniemi et al. (2024) show, naïve residual scaling (e.g., geometry-agnostic) can reduce sample diversity or disrupt the samplers behavior. These observations motivate geometry-aware view of guidanceasking not only how much to scale, but which directions to emphasize (Sadat et al., 2025). complementary line of work replaces external cues with model-internal signals (Hong et al., 2023; Ahn et al., 2024; Hong, 2024). The common aim of these strategies is to steer the inference-time update to suppress directions associated with off-manifold drift while preserving the learned prior. More recent formulations make this objective explicit by decomposing the update into components parallel to reference direction and orthogonal to it (Sadat et al., 2025; Kwon et al., 2025). Such geometry-aware perspectives offer principled basis for guidance design and integrate cleanly with modern solvers."
        },
        {
            "title": "3 MOTIVATION AND INTUITION",
            "content": "Under Gaussian corruption, Tweedies formula (Tweedie et al., 1984) links the posterior mean of the clean signal to the noisy observation via the score (i.e., the gradient of the log marginal density): E[x0xk] = (cid:16) xk (cid:124)(cid:123)(cid:122)(cid:125) := drift term + kx log p(x tk)(cid:12) σ2 (cid:124) (cid:123)(cid:122) (cid:12)x=xk (cid:125) := Tweedie increment Tw , (a.k.a. data term) (cid:17) / αk. (1) Geometrically, the score field log p(xtk)(cid:12) points in the direction of steepest increase of the marginal density. Tweedies formula therefore adjusts xk in this ascent direction, nudging the state toward higher-probability regions. Therefore, the aim of modeling is to bias this movement toward data-driven directions. (cid:12)x=xk However, naively guiding the states to chase higher-probability regions can disturb the schedulers prescribed radius/SNR trajectory and may degrade sample quality (Figure 4). Accordingly, to avoid altering the radial term, we isolate xk and reweight only the increment by decomposing it into normal and tangential parts with respect to (cid:98)xk := xk/xk2: Pk = (cid:98)xk (cid:98)x = Pk. Guided by this separation, we form the amplified state x+, where the normal component is fixed and only the tangential component is amplified, via and x+ = (cid:0)xk + PkTw (cid:1) + η (2) By doing so, we can preserve the radial first-order term (equation 17) while biasing the step toward higher-probability regions, log p(xtk)(cid:12) (Empirical evidence is provided in Figure 2 and 3). In the following section (4.1), we formalize this bias as constrained MLE update that allocates first-order gain to the tangential subspace. , with Tw (cid:12)x=xk η 1. 3 (d) TAG (Ours) (a) No Guidance (b) Naive Truncation (c) CFG Figure 3: Sampling on 2D branching distribution (Karras et al., 2024) under different guidance methods. (a) No guidance: probability mass drifts off the data manifold, yielding fragmented branches and OOD (Out of Distribution) points. (b) Naive truncation: suppresses some OOD but oversimplifies the geometry, dropping fine branches. (c) CFG: reduces boundary violations but also reduces diversity and can still leave OOD strays in our run. (d) TAG (Ours): trajectories are steered toward high-density regions along the branches, suppressing off-manifold outliers while retaining detail. (e) Ground truth. Overall, TAG achieves the highest similarity to the GT distribution without additional #NFEs, concentrating mass on the correct branches while substantially reducing residual OOD outliers. (e) Ground Truth"
        },
        {
            "title": "4 TAG: TANGENTIAL AMPLIFYING GUIDANCE",
            "content": "We introduce Tangential Amplifying Guidance (TAG), which reweights base increments along normal/tangential directions on the latent space. Definitions & Algorithm. We work per sample on RCHW = Rd with Euclidean inner product , and norm 2. Let {tk}0 k=K be descending timesteps with tK > > t0, and let ϵθ denote the denoiser. Given xk+1 at time tk+1, the denoiser predicts εk+1 = ϵθ(xk+1, tk+1). base solver (e.g., DDIM) then produces provisional state (Karras et al., 2022) xk = ak+1 xk+1 + bk+1 εk+1, where ak+1, bk+1 are base solver coefficients. (3) Corresponding base increment at xk+1 is defined as k+1 := xk xk+1. For any Rd, we define the unit vector and orthogonal projectors (cid:98)x = / x2, (x) = (cid:98)x(cid:98)x, (x) = (x). Given positive scales η 1, TAG reweights the base increment at xk+1: xk = xk+1 + Pk+1 k+1 + η k+1 k+1 where Pk+1 = (xk+1) and k+1 = (xk+1). (4) (5) (6) Algorithm 1 Tangential Amplifying Guidance (TAG) Require: Denoiser ϵθ(), timesteps {tk}0 1: Sample xK (0, I) 2: for = 1, . . . , 0 do 3: 4: 5: 6: 7: 8: εk+1 ϵθ(xk+1, tk+1) xk ak+1xk+1 + bk+1 εk+1 k+1 xk xk+1 (cid:98)xk+1 xk+1/xk+12 Pk+1 (cid:98)xk+1 (cid:98)x xk xk+1 + Pk+1k+1 + η (P k+1 Pk+1 k+1k+1) k+1, 4 k=K, base solver coefficients ak+1, bk+1, TAG scale η 1 noise prediction e.g., scheduler.step base increment projectors at xk+1 TAG amplification"
        },
        {
            "title": "4.1 WHY DOES TAG IMPROVE IMAGE QUALITY?",
            "content": "Log-likelihood maximization. foundational goal of training generative models is to maximize the log-likelihood of the data, as formalized by the Maximum Likelihood Estimation (MLE) principle: max θ (cid:88) log pθ(xi). (7) This principle suggests that high-quality samples should concentrate in regions of high probability. To connect this idea to an update rule, we relate likelihood increase to movement along the score via local linearization: log pθ(x) = log pθ(x0) + (x x0)x log pθ(x)(cid:12) (cid:12)x=x0 + O( 2). (8) (cid:12)x=xk Diffusion models (Song et al., 2020b; Ho et al., 2020) are designed to predict score function, log p(x tk)(cid:12) ϵθ(xk, tk)/σk, which operates on noisy versions of the data. Because diffusion models learn this score field, optimizing the global likelihood (equation 7) for sample x0 during inference is not directly tractable. Therefore, we propose to apply the spirit of MLE at each local step of the sampling trajectory. log p(xk tk+1) log p(xk+1 tk+1)+(xk xk+1)x log p(x tk+1)(cid:12) +O(2). (9) (cid:12)x=xk+1 The idea of enhancing pre-trained score function with inference-time guidance has proven effective. For instance, when the score function is well trained on given training sets and this leads to well-trained maximum log-likelihood, we observe that the pre-trained score function could be improved by CFG (Ho & Salimans, 2021) which linearly biases the score toward the conditional target. Inspired by this, our approach provides inference-time guidance on the score function by maximizing the following local log-likelihood term, thereby guiding the sampling trajectory towards high-likelihood regions of the data distribution and reducing off-manifold artifacts (hallucination): (xk xk+1)x log p(x tk+1)(cid:12) (10) (cid:12)x=xk+ max xk Single-step increment decomposition. For deterministic DDIM/ODE samplers, the single-step score state decomposition can be written as k+1 := xk xk+1 = αkϵθ(xk+1, tk+1) + βkxk+1, (11) with coefficients αk := σk αk αk+1 σk+1, βk := αk αk+1 1, with αk < 0, βk > 0, where α is the standard diffusion cumulative product term. Using the projection operators, which satisfy k+1xk+1 = 0 and Pk+1xk+1 = xk+1, yields the projection-wise identities k+1k+1 = αkP Pk+1k+1 = αkPk+1ϵθ(xk+1, tk+1) + βkxk+1. k+1ϵθ(xk+1, tk+1), (12) Substituting equation 12 into the equation 6 gives xTAG = xk+1 + αk (cid:2)Pk+1 + ηP k+1 (cid:3)ϵθ(xk+1, tk+1) + βkxk+1, with η 1. (13) Therefore, the TAG update TAG original update k+1: k+1 can be expressed in terms of the decomposed components of the (cid:1)k+1. k+1 = (cid:0)Pk+1 + η TAG (14) k+1 In this way, as visualized in Figure 2, semantic information can be isolated from the update vector k+1 via the tangential projection, thereby enabling semantics-aware amplification. To quantify its effect on the log-likelihood, assume the log-density is smooth (i.e., log p(tk+1) is 2 in k+1 Rd neighborhood of xk+1). The first-order Taylor expansion gain for small TAG update TAG is G(η) := (cid:0)TAG k+1 (cid:1) log p(x tk+1)(cid:12) (cid:12)x=xk+1 . (15) Next, we prove that increasing η provides monotonic increase in this first-order gain. Table 1: Quantitative results across previous guidance methods and +TAG sampling settings for unconditional generation. Evaluated on the ImageNet val with 30K samples. All images are sampled with Stable Diffusion (SD) v1.5 using the DDIM sampler. Methods DDIM (Song et al., 2020a) DDIM + TAG DDIM + TAG DDIM + TAG SAG (Hong et al., 2023) SAG + TAG PAG (Ahn et al., 2024) PAG + TAG SEG (Hong, 2024) SEG + TAG Guidance Scale TAG Amp. (η) 0.2 0.2 3 3 3 3 1.05 1.15 1.25 1.15 1.15 1.15 #NFEs 50 50 50 50 50 50 50 50 50 #Steps 50 50 50 50 25 25 25 25 25 25 FID 76.942 67.971 67.805 71.801 71.984 65.340 64.595 63.619 65.099 60.064 IS 14.792 16.620 16.487 15.815 15.803 17.014 19.30 19.90 17.266 18.606 Table 2: Quantitative results of TAG on various Stable Diffusion baselines. The table presents comparison for Stable Diffusion (SD) v2.1 and SDXL, evaluated on 10K ImageNet validation images using the DDIM sampler with 50 NFEs. Methods SD v2.1 (Rombach et al., 2022) SD v2.1 + TAG SDXL (Podell et al., 2024) SDXL + TAG TAG Amp. (η) 1.15 1. #NFEs 50 50 50 50 #Steps 50 50 50 50 FID 100.977 88.788 124.407 113.798 IS 11.553 13.311 9.034 9.716 Table 3: Quantitative results for unconditional image generation on the ImageNet dataset. We leverage Stable Diffusion (SD) v1.5. All metrics are calculated using 30K samples. We further demonstrate that strong performance is achievable even with fewer #NFEs. We measure the inference time using torch.cuda.Event and report the average over 100 consecutive runs on NVIDIA RTX 4090 GPUs. Methods DDIM (Song et al., 2020a) DDIM + TAG DDIM + TAG DPM++ (Lu et al., 2025) DPM++ + TAG TAG Amp. (η) 1.15 1.15 1.15 #NFEs 50 25 50 10 10 Inference Time (s) 1.9507 1.0191 1.9674 0.4433 0.4522 FID 76.942 72.535 67.805 85.983 74.238 IS 14.792 15.528 16.487 13.037 14.930 Theorem 4.1 (Monotonicity of the First-order Taylor Gain). Assume deterministic base step with k+1 = αkϵθ(xk+1, tk+1) + βkxk+1 and αk 0. Let Pk+1 0 and k+1 0 be the projectors defined above. For the TAG step TAG k+1k+1, the first-order Taylor gain log p(x tk+1)(cid:12) G(η) := (cid:0)TAG k+1 = Pk+1k+1 + η satisfies (cid:1) k+1 (cid:12)x=xk+1 G(η) η αk σk+1 (cid:13) (cid:13)P k+1ϵθ(xk+1, tk+1)(cid:13) 2 2 0, (cid:13) and, in particular, GTAG Gbase = σ k+1 (cid:0)αk(η 1)(cid:1) (cid:125) (cid:123)(cid:122) 0 as αk 0 (cid:124) (cid:13) (cid:13)P k+1ϵθ(xk+1, tk+1)(cid:13) 2 2 0, (cid:13) Equality holds iff η = 1. The proof is provided in Appendix A. Log-likelihood improvements via TAG. We cast inference-time guidance as maximizing loglikelihood gain (equation 10). TAG simply reweights the update step by amplifying the component that is orthogonal to the current state while leaving the parallel component unchanged. By Theorem 4.1, increasing the orthogonal weight monotonically raises the first-order Taylor gain, so TAG steers the sampler toward higher-density regions of the data manifold, improving image quality. 6 Avoidance of normal amplification. Amplifying the tangential component monotonically increases the first-order term of Taylor gain of log p( tk+1) (Theorem 4.1), which produces samples with less hallucination. However, amplifying the normal component increases radial contraction and leads to over-smoothing (Figure 4). This radial component of the single-step is aligned with the radial part of Tweedies correction, which links xk to the posterior mean E[x0xk] via the score function (Tweedie et al., 1984; Song et al., 2020b). Formally, rescaling the normal part by κ (> 1), the radial firstorder change is multiplied by κ: Figure 4: Effectiveness of TAG. At 50 NFEs, TAG surpasses the sample quality at 250 NFEs from baseline. In contrast, +Normal causes severe over-smoothing. + TAG + Normal #NFEs=50 Uncond. #NFEs=250 + TAG #NFEs= Uncond. #NFEs=50 (16) Therefore, value of κ (> 1) excessively strengthens this contraction under the VP/DDIM schedule, leading to over-smoothing. In contrast, tangential scaling preserves the radial firstorder term: k+1 = κ (cid:98)xk+1, k+1. (cid:98)xk+1, (κ) (17) To summarize, normal amplification breaks onestep calibration and induces over-smoothing, whereas tangential boosting improves alignment without disturbing the radial schedule. k+1 = (cid:98)xk+1, k+1. (cid:98)xk+1, TAG 4.2 TANGENTIAL AMPLIFYING GUIDANCE FOR CONDITIONAL GENERATION Cond. only Cond.+ TAG Our analysis (3, 4) shows that the tangential component encodes data-relevant directions and is radius-preserving to first-order; so amplifying it improves image quality by steering updates along data-aligned directions. In CFG (Ho & Salimans, 2021), the guided score combines conditional and unconditional branches (cid:101)εk = ϵθ(xk, c) + ω(ϵθ(xk, c) ϵθ(xk, )). (18) Because these two scores follow distinct trajectories, an incoherence between them can arise, and such an effect can degrade generation quality, an issue recently highlighted by Kwon et al. (2025). Motivated by this established score mismatch, and informed by our core intuition that the tangential field encodes data geometry (equation 1), we posit that this incoherence is fundamentally tangential in nature; that is, persistent mismatch exists primarily between the conditional and unconditional tangential components. prompt = ... man brushing ... Figure 5: Conditional generation without CFG. Adding TAG produces more faithful semantics for the prompt at matched NFEs. Conditionalunconditional tangent reconciliation. Let gk := ϵθ(xk, c) ϵθ(xk, ) denote the CFG guidance where ϵθ(, c), ϵθ(, ) denote the cond/unconditional predicted noise. We form conditional-relative tangent by removing the unconditional tangent from the conditional one, = (xk)(cid:0) ϵθ(xk, c) ϵθ(xk, )(cid:1) = (xk)gk, (19) and project the conditional score ϵθ(xk, c) onto this tangent subspace. We then amplify this condition relative tangent: )ϵθ(xk, c)(cid:1) , where ω is the usual CFG scale and η controls the extra tangential emphasis. εk = ϵθ(xk, c) + ωgk + η (cid:0)σ1 (g k=K, CFG scale ω, TAG scale η 0 (20) initialize from prior Algorithm 2 Conditional TAG (C-TAG) Require: Denoiser ϵθ(), timesteps {tk}0 1: Sample xK (0, I) 2: for = 1, . . . , 0 do 3: 4: 5: 6: 7: (εu, εc) ϵθ(xk+1, tk+1, ) gk εc εu (cid:98)xk+1 xk+1/xk+12 k+1 (cid:98)xk+1 (cid:98)x k+1 k+1 gk (cid:16) εc,g εk εu + ωgk + η 2 2 xk STEP( εk, tk+1, xk+1) 8: 9: k (cid:17) uncond / cond noise CFG direction in ε-space projector at xk+1 tangential component TAG-augmented CFG scheduler step 7 Table 4: Quantitative results across guidance-only (i.e. CFG, PAG, SEG) and guidance w/ TAG sampling settings. Evaluated on the MS-COCO 2014 val split with 10k random text prompts. cfg_scale=2.5, All images are sampled with Stable Diffusion v1.5 using the DDIM sampler. pag_scale=2.5 and seg_scale=2.5 are applied for each experiments. Methods Condition-Only Condition-Only + TAG CFG (Ho & Salimans, 2021) CFG + C-TAG PAG (Ahn et al., 2024) PAG + C-TAG SEG (Hong, 2024) SEG + C-TAG FID CLIPScore 19.77 3.43 85.145 21.88 2.99 58.438 22.60 3.28 26.266 22.82 3.21 23.414 22.72 3.25 24.280 22.07 3.49 22.109 18.17 3.55 29.215 16.94 3.96 23.446 TAG Amp. (η) 1.2 2.5 1.25 1.25 #NFEs 30 30 100 30 50 50 50 50 #Steps 30 30 50 15 25 25 25 25 l B + Unconditional Gen. with SD3 (Podell et al., 2024) Conditional Gen. with PAG Conditional Gen. with SEG Figure 6: Qualitative comparison of TAG across unconditional and conditional generation settings. The left four columns demonstrate that for unconditional generation, TAG enhances the detail and coherence of samples from the SD3 (Podell et al., 2024). The right four columns show that for conditional generation, TAG can be applied on top of existing guidance methods (e.g., PAG (Ahn et al., 2024), SEG (Hong, 2024)) to further improve their outputs."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Backbones and inference setup. We apply TAG at inference on pretrained backbones, using Stable Diffusion v1.5 (Rombach et al., 2022) for major experiments and Stable Diffusion 3 (Esser et al., 2024) for flow matching. Unconditional results are reported on ImageNet-1K val dataset (Deng et al., 2009). Text-conditional results use MS-COCO 2014 val dataset (Lin et al., 2015). The number of function evaluations (#NFEs) follows each table. TAG is inserted after every solver update with amplification η. Metrics include FID (Heusel et al., 2017), IS (Salimans et al., 2016), CLIPScore (Hessel et al., 2021), and NFEs. FID is computed with pytorch-fid (Seitzer, 2020), IS with InceptionV3 (Szegedy et al., 2016), and CLIPScore is computed with OpenAI CLIP ViT-L/14. All runs use fixed seeds and identical preprocessing to the corresponding baselines. Improvements on conditional generation. Table 4 presents quantitative results on the MS-COCO, demonstrating that augmenting existing guidance samplers with TAG consistently yields substantial improvements in sample fidelity while largely preserving text-image alignment. Notably, TAG enables 30 #NFEs sampling process to outperform the 100 #NFEs CFG baseline. Even in condition only setting, TAG dramatically reduces FID and increases CLIPScore, confirming its foundational benefits independent of guidance signal. Furthermore, this trend extends to other guidance techniques such as PAG and SEG, where TAG again reduces FID at the same computational cost. The qualitative improvements are visualized in Figure 7, which demonstrates TAGs ability to produce higher-fidelity images with fewer artifacts. ω = 2.5, η = 0.0 ω = 5.0, η = 0.0 ω = 2.5, η = 1.0 Figure 7: Qualitative Results with CFG. TAG produces higher-fidelity samples with fewer hallucinations, outperforming even baselines with higher CFG scale ω. 8 η = 1.0 η = 1. η = 1.2 η = 1.3 (a) Impact of TAG amplification η on FID () and IS () for unconditional SD v1.5 generation. Figure 8: Ablation on TAG amplification η. Figure 8a and Table 1 show gains at moderate η and degradation when amplification is excessive. Figure 8b confirms the same trend for Flow-matching, underscoring the need to select an appropriate η. (b) Qualitative comparison across amplification levels η for SD3 unconditional generation: moderate tangential amplification enhances detail and coherence, while excessive amplification degrades fidelity. Improvements on unconditional generation. For unconditional generation, TAG consistently improves sample quality across range of models and samplers. As shown in Table 1, it reduces FID and increases IS at matched NFEs. Notably, TAG acts as plug-and-play module for existing guidance methods (e.g., SAG, PAG, SEG), enhancing their performance without architectural changes or additional model evaluations. Moreover, TAG significantly pushes the computequality frontier by enabling both faster inference and higher quality. With samplers like DDIM and DPM++, TAG can achieve superior results with as few as half the NFEs  (Table 3)  . Concurrently, it substantially boosts performance on foundational models like SD v2.1 and SDXL at fixed computational cost  (Table 2)  . This dual benefit provides practical path to faster inference and extends to SOTA models like SD3  (Table 5)  , with qualitative improvements visualized in Figures 6 and 9. Improvements on Flow Matching. By consistently guiding the sampling trajectories toward regions of high probability, TAG serves as broadly applicable enhancement for generative ODE solvers, whether the underlying training scheme is score-based or flow-matching. Figure 6 and Table 5 indicate that TAG transfers to flow-matching backbones (Esser et al., 2024). Inserted as lightweight tangential reweighting after each solver step, without architectural changes or additional function evaluations. TAG yields modest but consistent FID improvement at matched compute and visibly reduces artifacts in unconditional samples. These results show TAGs potential to be model-agnostic across diverse architectures, including modern large-scale models. Table 5: Quantitative results for flow matchingbased generator. Evaluations are conducted on ImageNet val with 30K samples; all images are generated with 50 NFEs. Methods w/ SD3 Esser et al. (2024) + TAG TAG Amp. (η) 1.05 FID 96.383 91.706 IS 11.831 12."
        },
        {
            "title": "6 LIMITATION & FUTURE WORK",
            "content": "An ablation of η reveals that moderate tangential amplification improves quality, whereas performance degrades for larger values (Fig. 8a, Tab. 1; see also Fig. 8b for flow matching). Analytically, the post-step state norm under TAG satisfies k+1 2 xk+1 + TAG 2 = xk+1 + k+12 (21) Therefore, for η = 1 + δ with sufficiently small positive δ (0 < δ 1), the additive term is negligible, and the first-order radial term remains unchanged. As η grows, however, the additive term increasingly perturbs the schedulers radial calibration, which explains the observed degradation. promising direction is to model these higher-order effects and design adaptive gains ηk, potentially yielding hyperparameter-free variant. 2 + additive term (cid:123) (cid:125)(cid:124) (cid:122) k+1k+12 (η2 1) 2 ."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This paper introduces new perspective for addressing the problem of hallucinations in diffusion models, demonstrating that the tangential component of the sampling update encodes critical semantic structure. Based on this geometric insight, we propose Tangential Amplifying Guidance (TAG), 9 practical, architecture-agnostic method that amplifies the tangential component. By doing so, TAG effectively steers the sampling trajectory toward higher-density regions of the data manifold, generating samples with fewer hallucinations and improved fidelity. Our method achieved good samples without requiring retraining or incurring any additional heavy computational overhead, offering practical, plug-and-play solution for enhancing existing diffusion model backbones."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We use PyTorch (Paszke et al., 2019) and the HuggingFaces Diffusers library (von Platen et al., 2022) to implement our models and all the baselines."
        },
        {
            "title": "REFERENCES",
            "content": "Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pp. 117. Springer, 2024. Sumukh Aithal, Pratyush Maini, Zachary C. Lipton, and J. Zico Kolter. Understanding hallucinations in diffusion models through mode interpolation. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 134614134644. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2024/ 2024. file/f29369d192b13184b65c6d2515474d78-Paper-Conference.pdf. Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313326, 1982. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Anh-Dung Dinh, Daochang Liu, and Chang Xu. Representative guidance: Diffusion model sampling with coherence. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=gWgaypDBs8. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview. net/forum?id=qw8AKxfYbI. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 37:6674366772, 2024. 10 Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 74627471, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. Mingi Kwon, Jaeseok Jeong, Yi Ting Hsiao, Youngjung Uh, et al. Tcfg: Tangential damping classifier-free guidance. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 26202629, 2025. Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pp. 122, 2025. Maya Okawa, Ekdeep Lubana, Robert Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on synthetic task. Advances in Neural Information Processing Systems, 36:5017350195, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. URL https://arxiv.org/abs/1912.01703. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, 2024. URL synthesis. https://openreview.net/forum?id=di52zR8xgf. Javad Rajabi, Soroush Mehraban, Seyedmorteza Sadat, and Babak Taati. Token perturbation guidance for diffusion models. arXiv preprint arXiv:2506.10036, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Seyedmorteza Sadat, Otmar Hilliges, and Romann M. Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models, 2025. URL https://arxiv.org/abs/ 2410.02416. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/ pytorch-fid, August 2020. Version 0.3.0. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. RethinkIn Proceedings of the IEEE conference on ing the inception architecture for computer vision. computer vision and pattern recognition, pp. 28182826, 2016. Maurice CK Tweedie et al. An index which distinguishes between some important exponential families. In Statistics: Applications and new directions: Proc. Indian statistical institute golden Jubilee International conference, volume 579, pp. 579604, 1984. Pascal Vincent. connection between score matching and denoising autoencoders. Neural Computation, 23(7):16611674, 2011. doi: 10.1162/NECO_a_00142. Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and https://github.com/ Thomas Wolf. Diffusers: State-of-the-art diffusion models. huggingface/diffusers, 2022."
        },
        {
            "title": "APPENDIX",
            "content": "Symbol xk Rd (cid:98)x := x/x2 Pk := (cid:98)xk (cid:98)x := Pk u, v, u2 sθ(x, t) ϵθ(x, t) gk TAG η 1 NFEs Meaning latent at step (time tk). Unit vector in the direction of x. Projector onto span(xk) (normal to the iso-noise surface at x). Tangential projector orthogonal to xk. Euclidean inner product and norm. Model score. Model noise prediction; sθ(x, t) = ϵθ(x, t)/σt. Guidance residual direction. Base solver increment without TAG at step k. TAG-modified increment at step k. TAG tangential amplification factor (scales Number of function evaluations. ). PROOF & DERIVATION Proof for Theorem 4. Proof. Assume the deterministic base step k+1 = αk ϵθ(xk+1, tk+1) + βk xk+1, with αk 0, and let Pk+1, k+1 be the orthogonal projectors with Pk+1xk+1 = xk+1 and k+1xk+1 = 0. Applying the projectors to the base decomposition gives k+1k+1 = αk k+1ϵθ(xk+1, tk+1), Pk+1k+1 = αk Pk+1ϵθ(xk+1, tk+1) + βk xk+1. (22) (23) (24) (25) Therefore, the TAG update rule step is k+1 = (cid:0)Pk+1 + η TAG k+ (cid:1)k+1 = αk (cid:2)Pk+1 + ηP k+1 (cid:3)ϵθ(xk+1, tk+1) + βkxk+1. The first-order Taylor gain with respect to TAG update at tk+1 is defined as: G(η) := (cid:0)TAG k+1 (cid:1) log p(x tk+1)(cid:12) (cid:12)x=xk+1 (cid:17) (cid:16)(cid:0)Pk+1 + η = (cid:1)k+1 log p(x tk+1)(cid:12) (cid:12)x=xk+1 We analyze this gain by approximating the true score with the models score function sθ(xk+1, tk+1) = σ1 k+1ϵθ(xk+1, tk+1), k+1 thus: G(η) = (cid:16)(cid:0)Pk+1 + η (cid:17) k+1 (cid:1)k+1 (cid:16)(cid:0)Pk+1 + η log p(x tk+1)(cid:12) (cid:17) (cid:12)x=xk+1 sθ(xk+1, tk+1) k+1 (cid:1)k+1 (cid:16)(cid:0)Pk+1 + η k+ = σ1 k+1 (cid:17) (cid:1)k+1 (cid:0)xk+1, tk+1 (cid:1) ϵθ (26) Substitute equation 23 into equation 26, then: G(η) σ1 k+1 = σ1 k+1 (cid:16) αkPk+1ϵθ + βkxk+1 + η αkP (cid:16) αk(Pk+1ϵθ)ϵθ + βkx (cid:17) ϵθ k+1ϵθ k+1ϵθ + η αk(P k+1ϵθ)ϵθ Since and are symmetric and idempotent, thus vP = v2 2 13 (cid:17) . (27) (28) is established. Therefore, G(η) σ1 k+1 (cid:18) (cid:13) (cid:13)Pk+1ϵθ αk (cid:13) 2 2 + βkx (cid:13) k+1ϵθ + η αk (cid:13) (cid:13)P k+1ϵθ (cid:13) 2 (cid:13) 2 (cid:19) . Differentiating the gain G(η) in equation 29 with respect to η yields: G(η) η αk σk+1 (cid:12) (cid:12)P k+1ϵθ(xk+1, tk+1)(cid:12) 2 2 0. (cid:12) (29) (30) This derivative is guaranteed to be non-negative, since the DDIM sampler coefficient αk 0 by definition, while σk+1 and the squared L2-norm are strictly non-negative. This proves that the first-order gain G(η) is monotonically non-decreasing function of η. Consequently, amplifying the tangential component of the update step via TAG is guaranteed to improve the first-order loglikelihood gain compared to the base update step. Analysis on pure TAG gain. Subtracting each gain Gbase G(η = 1) and GTAG G(η > 1), (cid:122) (cid:16) TAG update gain, GTAG (cid:125)(cid:124) (cid:17) (cid:16) σ1 k+1 TAG k+1 (cid:0)xk+1, tk+1 ϵθ (cid:123) (cid:1)(cid:17) (cid:122) (cid:16) base update gain, Gbase (cid:125)(cid:124) (cid:17) (cid:16) (cid:123) (cid:1)(cid:17) ϵθ k+1 σ1 k+1 k+1 (cid:0)TAG k+1 (cid:0)(η 1) k+1 k+1 = σ1 = σ k+1k+1 (cid:0)xk+1, tk+1 (cid:1) (cid:1) (cid:0)xk+1, tk+1 ϵθ (cid:1) (cid:0)xk+1, tk+1 ϵθ Using k+1 = αk ϵθ(xk+1, tk+1) + βk xk+1, k+1 be: Thus, substitute equation 32 into equation 31 then: k+1k+1 = k+1 αk ϵθ(xk+1, tk+1). (cid:1). (31) (32) GTAG Gbase = σ1 k+1 (cid:0)αk(η 1)(cid:1) (cid:123)(cid:122) (cid:125) scalar (cid:124) This simplifies to the final quadratic form: (cid:0)P k+1 ϵθ(xk+1, tk+1)(cid:1) (cid:0)xk+1, tk+1 (cid:1). ϵθ (33) GTAG Gbase = σ1 k+1 (cid:0)αk(η 1)(cid:1) (cid:123)(cid:122) (cid:125) 0 as αk 0 (cid:124) (cid:13) (cid:13)P k+1ϵθ(xk+1, tk+1)(cid:13) 2 2, (cid:13) (34) This proves that the difference in gain is non-negative for any η 1. Therefore, the first-order log-likelihood gain of the TAG update is always greater than or equal to that of the base update, with equality holding if and only if η = 1 or the tangential component of the score is zero."
        },
        {
            "title": "B IMPLEMENTATION OF THE TANGENTIAL AMPLIFYING GUIDANCE",
            "content": "Algorithm 3 Code: Tangential Amplifying Guidance (TAG) output = scheduler.step(noise_pred, t, latents, return_dict=False) if apply_tag: post = latents eta_v, eta_n = t_guidance_scale, 1 v_t = post / (post.norm(p=2, dim=(1,2,3), keepdim=True) + 1e-8) latents = output delta = latents - post = (delta * v_t).sum(dim=(1,2,3), keepdim=True) u_n = * v_t u_t = delta - u_n latents = post + eta_v * u_t + eta_n * u_n else: latents = output Algorithm 4 Code: Conditional Tangential Amplifying Guidance (C-TAG) def proj_par(z, n): return (z * n).sum(dim=(1,2,3), keepdim=True) * def proj(z, v): = / (v.norm(p=2, dim=(1,2,3), keepdim=True) + 1e-8) return (z * v).sum(dim=(1,2,3), keepdim=True) * eps_u, eps_c = HeadToEps(noise_pred, latents, t, scheduler, do_cfg) s_u = -eps_u / (sigma + 1e-12) s_c = -eps_c / (sigma + 1e-12) = latents / (latents.norm(p=2, dim=(1,2,3), keepdim=True) + 1e-8) = s_c - s_u = s_c - proj_par(s_c, n) t_c t_u = s_u - proj_par(s_u, n) g_aligned = proj(s_c, t_c - t_u) = + t_guidance_scale * g_aligned s_star = s_u + guidance_scale * = -sigma * s_star eps model_out = EpsToHead(eps, latents, t, scheduler) latents = scheduler.step(model_out, t, latents, return_dict=False)"
        },
        {
            "title": "C ADDITIONAL QUALITATIVE RESULTS",
            "content": "Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Figure 9: Qualitative results for unconditional generation across backbones. For each model (SD1.5/2.1 (Rombach et al., 2022), SDXL (Podell et al., 2024), SD3 (Esser et al., 2024)), the top row shows baseline sampling and the bottom row shows +TAG at matched NFEs. TAG yields sharper, more coherent structure with fewer artifacts while preserving diversity."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "Korea University",
        "University of California, Berkeley",
        "University of Washington"
    ]
}