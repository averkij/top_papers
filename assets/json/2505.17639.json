{
    "paper_title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval",
    "authors": [
        "Zehua Pei",
        "Ying Zhang",
        "Hui-Ling Zhen",
        "Xianzhi Yu",
        "Wulong Liu",
        "Sinno Jialin Pan",
        "Mingxuan Yuan",
        "Bei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 3 6 7 1 . 5 0 5 2 : r PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval Zehua Pei1, Ying Zhang2, Hui-Ling Zhen2, Xianzhi Yu2, Wulong Liu2, Sinno Jialin Pan1, Mingxuan Yuan2, Bei Yu1 1The Chinese University of Hong Kong 2Noahs Ark Lab, Huawei"
        },
        {
            "title": "Abstract",
            "content": "Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. Experiments across multiple MoE architectures demonstrate PreMoes effectiveness: DeepSeek-R1 671B maintains 97.2% accuracy on MATH500 when pruned to 8/128 configuration (50% expert reduction), and still achieves 72.0% with aggressive 8/32 pruning (87.5% expert reduction). PanguUltra-MoE 718B achieves 97.15% on MATH500 and 81.3% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, from question answering to reasoning and creative content generation [1, 2]. The scaling law observed in these models suggests that increasing model size leads to improved performance [3, 4]. However, training and deploying increasingly larger dense models are constrained by computational resources and inference costs. The mixture-of-experts (MoE) architecture has emerged as promising approach to scale model capacity without proportional increase in computational requirements [5, 6, 7]. By activating only subset of parameters for each input token, MoE models can achieve performance comparable to much larger dense models while significantly reducing computational costs during training and inference. Preprint. Figure 1 Overview of our proposed PreMoe framework. Recent MoE-based LLMs, such as DeepSeek-R1 [8] with 671B parameters, Mixtral 87B [9], and Qwen-MoE [10, 11], have demonstrated state-of-the-art performance across various benchmarks. Despite these advantages, deploying large MoE models presents substantial challenges across all computational environments. primary hurdle is memory: although only fraction of experts are computationally active for any given token, the parameters of all unique experts, along with shared model components, must typically reside in memory. This results in significant memory footprint that scales with the total number of unique parameters across all experts and the shared backbone. For instance, large-scale MoE model system like DeepSeek-R1, cited as having 671B parameters, would theoretically require over 1.3TB of memory in 16-bit precision for its full deployment if all parameters were loaded simultaneously. Notably, the routed experts alone account for 654B of DeepSeek-R1s 671B parameters, constituting the overwhelming majority of its memory footprint and representing the critical bottleneck for deployment. Even with optimized implementations, the memory requirements remain prohibitively large for many computing environments, from cloud servers with resource constraints to edge devices and consumer hardware. This substantial memory requirement restricts its use to specialized high-end computational infrastructures and starkly contrasts with the sparse computational nature of MoE architectures, where efficiency gains are predicated on selective expert use, yet memory capacity remains dictated by the full ensemble of experts. Prior works have explored various techniques to reduce the memory footprint of large language models, including quantization [12, 13], pruning [14, 15, 16, 17], and knowledge distillation [18, 19]. For MoE models specifically, methods like FasterMoE [20] focus on efficient distributed inference, while [21] propose token-to-expert assignments for more balanced expert utilization. However, these approaches either yield limited memory reduction, degrade model performance, or fail to leverage the inherent task-specific patterns in MoE architectures. In this work, we make key observation: experts in MoE layers exhibit strong task-specific specialization patterns. Through extensive analysis of the DeepSeek-R1 model across diverse tasks, we find that the expert activation patterns are remarkably differentiated between tasks, where different sets of experts are consistently activated for different types of tasks. This task-specific specialization becomes more pronounced in models designed for complex reasoning tasks like DeepSeek-R1. This observation suggests that for any specific task, large portion of experts is rarely activated and could potentially be pruned without significant performance degradation. Building upon this insight, we introduce PreMoe (Pruning Retrieval MoE), novel framework designed to dramatically reduce the memory requirements of large MoE models while maintaining their performance across diverse deployment scenarios. PreMoe comprises two main components. First, Probabilistic Expert Pruning (PEP) formulates expert importance quantification as probabilistic process, introducing the Task-Conditioned Expected Selection Score (TCESS), metric that identifies the most critical experts for specific task by analyzing router logit patterns conditioned on local confidence scores. Second, Task-Adaptive Expert Retrieval (TAER) provides an efficient 2 mechanism that stores compact expert activation patterns for representative tasks and dynamically loads only task-relevant experts based on query similarity, enabling rapid adaptation with minimal computational overhead. The overview of PreMoe is illustrated in Fig. 1. PreMoe enables dramatic reductions in memory usage while maintaining competitive performance across various deployment environments. Our comprehensive evaluation demonstrates PreMoes effectiveness across multiple MoE architectures: DeepSeek-R1 671B maintains 97.2% accuracy on MATH500 with 50% expert reduction, Pangu-Ultra-MoE 718B achieves 97.15% on MATH500 with similar pruning. These results enable deployment on memory-constrained systems where full models would be prohibitively expensive. Our contributions include: comprehensive analysis demonstrating substantial task-specific specialization in MoE models, especially pronounced in reasoning-focused architectures like DeepSeek-R1. The introduction of Probabilistic Expert Pruning with principled metric (TCESS) to identify task-critical experts in MoE layers. Task-Adaptive Expert Retrieval mechanism that enables efficient expert selection and loading for inference. The PreMoe framework that combines these techniques, enabling deployment of large MoE models on memory-constrained systems while maintaining strong performance, with comprehensive evaluation across multiple MoE architectures including 671B DeepSeekR1, 718B Pangu-Ultra-MoE, and 72B Pangu-Pro-MoE, demonstrating significant memory reduction while preserving competitive performance. These contributions represent significant step toward democratizing access to state-of-the-art MoE models, making them accessible to researchers and practitioners across wide range of computational environments with limited resources."
        },
        {
            "title": "2 Background",
            "content": "In the algorithm, we mainly focus on the DeepSeek-MoE architecture [7, 22], where Ns shared experts and Nr routed experts, and router network are composed in MoE layer. However, our analysis and findings can be adapted to other MoE architectures, such as the Mixtral MoE [9] and Pangu-MoE [23]. Let Rd denote the MoE input for single token; the output of the MoE layer is obtained by: Ns Nr FM oE = + Es (x) + giEr (x), gi = i=1 (cid:88) TopK( si, si 0, otherwise, (cid:26) i=1 (cid:88) 1 si { gi = gi Nr j=1 gj , (1) , K), Nr} (cid:80) and = G(x) = Sigmoid(s) = Sigmoid(xWG), where gi is the score for the i-th expert, (pre-sigmoid); WG comprising highest scores among the affinity scores calculated for on all experts; Es denote the i-th shared expert and the i-th routed expert, respectively. RNr is the token-to-expert affinity, i.e. the output of Nr is the weight of the router network; and TopK( , K) denotes the set and Er Rd For expert pruning, typically we consider pruning the routed experts by determining selection index set Sr, which decides the indices of the routed experts to be kept. The expert pruning problem can be formulated as minimization problem: min 1,...,Nr { } Sr L(F Sr oE, FM oE), s.t. Sr M, (2) where Sr Ns i=1 Es oE is the MoE function with only the selected experts in Sr (i.e., Sr (x) + oE = + (x)), is loss function that measures the performance degradation due Sr giEr to pruning, and < Nr is the target number of routed experts to keep. (cid:80) (cid:80) 3 Figure 2 Heatmap of average activated router logits across BigBench tasks (rows) and experts (columns) in DeepSeek-R1. Red indicates high positive logits, blue negative ones. The sparse patterns of red highlights task-specific expert specialization."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Expert Specialization Analysis To understand expert operational dynamics, we analyzed router logit distributions in DeepSeek-R1s MoE layers. DeepSeek-R1 uses sigmoid-based routing: raw router logit sj(x) for expert (given token x) is passed through sigmoid function, sj(x) = Sigmoid(sj(x)). Experts are typically activated if sj(x) meets certain criteria (e.g., Top-K). Our analysis focuses on the raw logits sj(x) of experts that were indeed activated. For numerous BigBench benchmark tasks [24], we computed the average of these activated raw router logits for each expert. Specifically, for task, an experts average activated logit is the mean of its raw logits from instances where it was activated across all tokens; if never activated, this average is considered null or zero for visualization (see Figure 2 for visual representation via heatmap). This heatmap, where rows are tasks and columns are experts, shows red for high positive average activated logits and blue for negative. Our key finding is that for any given task, only small subset of experts are frequently activated with high positive average logits. Most experts are either rarely activated or do not consistently show high positive logits when activated. This pronounced task-specific specialization, evidenced by the sparse distribution of strong positive average activated logits per task, implies that many experts are infrequently activated or contribute minimally for specific task. They could thus be pruned or deprioritized for that task with minimal impact on efficacy. The identified heterogeneity in these average activated logit distributions presents both challenge (preserving performance across tasks with reduced model size) and an opportunity (dynamically loading only pertinent experts). This insight motivates our Probabilistic Expert Pruning (Section 3.2) and Task-Adaptive Expert Retrieval (Section 3.3) methodologies. 3.2 Probabilistic Expert Pruning Building upon our observations of task-specific expert specialization, we introduce principled framework for expert pruning. Inspired by CMoE [25], which leverages activation rates to identify and construct shared experts based on neuron-level activations, our approach focuses on identifying the most relevant experts for specific tasks by analyzing their router logits within probabilistic context. 4 We develop new criterion for expert pruning based on their task-specific router logits. Let represent specific task drawn from distribution of tasks T. Let sj(x) denote the raw router output logit for expert given an input token x. For given task with set of input tokens XT = , we process each token as follows to determine expert contributions. The selection of logits for calculating expert importance involves two hyperparameters: Ka and r. First, the parameter Ka identifies an initial pool of candidate experts (in the same layer). We select the set of Ka experts, denoted EKa (x), corresponding to the top Ka Nr j=1. Next, to obtain refined confidence scores for these Ka selected experts, raw logits from } EKa (x). This is achieved by we compute locally normalized probabilities, pi(x), for each expert applying softmax function only over the raw logits of these Ka chosen experts: x1, . . . , xq} { sj(x) { pi(x) = exp(si(x)) EKa (x) exp(sk(x)) for EKa (x). (3) EKa (x), they are not considered further in this step. To further ensure that only For experts / highly confident logits are collected, the hyperparameter [0, 1] serves as minimum confidence threshold, which is applied to these locally normalized probabilities pi(x). The task-conditioned activation marker aT (x) for each expert captures the original raw logit si(x) if the confidence conditions are met: (cid:80) aT (x) = si(x), if EKa (x) pi(x) r, (cid:26) 0, otherwise. In this formulation, the original raw logit si(x) contributes to the final score for expert only if that expert was among the top Ka selected based on raw logits, and its subsequent locally normalized probability pi(x) meets or exceeds the confidence threshold r. lower Ka value means selecting fewer initial candidates for local softmax normalization, while higher value imposes stricter confidence requirement on the resulting local probabilities. For instance, consider Ka = 2: If the raw logits for two experts, say s1(x) and s2(x), are selected as the top Ka, and their corresponding local probabilities are p1(x) = 0.9 and p2(x) = 0.1. If the confidence threshold is = 0.6, then for expert 1, p1(x) r, so the value s1(x) is collected. For expert 2, p2(x) < r, so zero is collected for this expert. (4) If, for another token, the top Ka raw logits led to local probabilities p1(x) = 0.55 and p2(x) = 0.45, and = 0.6, then neither p1(x) nor p2(x) meets the threshold, so zero is collected for both corresponding experts. Typically, relationship like 1/Ka can be informative. This hyperparameter Ka (for selecting the initial pool for local softmax) is distinct from the number of experts an MoE layer routes to during its standard forward pass (often denoted K). We then calculate what we term the Task-Conditioned Expected Selection Score (TCESS) for each expert with respect to task , denoted as TCESST : 1 (5) TCESST = aT (x). XT (cid:88) XT This score, TCESST , now represents the average raw logit contribution of expert for task , conditioned on it passing the confidence checks. Specifically, it reflects the average of si(x) for tokens where expert is raw logit was among the top Ka, and its locally normalized probability pi(x) exceeded the threshold r. Experts with higher TCESST values (i.e., those that frequently pass the confidence filter and have high raw logits when they do) are considered more critical for task . Given this formulation, we reframe the expert pruning problem (initially described in Section 2) as selecting the subset of experts ST that maximizes the cumulative TaskConditioned Expected Selection Score, serving as proxy for preserving task performance under pruning constraints. The objective is: 1, 2, . . . , Nr} { ST = argmax 1,...,Nr { } (cid:88)i TCESST s.t. M, (6) where is the target number of experts to keep. This optimization is solved by selecting the experts with the highest TCESST values: ST = : TCESST { TopK( TCESST 1 { , ) Nr} . } (7) 5 This approach allows us to identify and retain only the experts that demonstrate the highest expected confident selection strength for specific task, aligning with our observations of task-specific expert specialization. By loading only these selected experts, we aim to significantly reduce the memory footprint while robustly maintaining performance on the target task. This probabilistic expert pruning method presents several advantages. It directly leverages the routers existing mechanism for expert selection and the inherent information within router probabilities. Furthermore, the approach naturally adapts to task-specific expert utilization patterns by concentrating on task-conditioned logits. Finally, it establishes principled and interpretable framework that holds potential for extension to multi-task scenarios or dynamic expert allocation strategies. 3.3 Task-Adaptive Expert Retrieval While the probabilistic expert pruning methodology provides principled approach to identify task-critical experts, its direct application would require computing TCESS values for each new user querycomputationally inefficient for deployment scenarios. We address this challenge by introducing task-adaptive expert retrieval mechanism that enables efficient, on-demand loading of task-specific experts based on the characteristics of user queries. Our approach leverages the observation that expert activation patterns exhibit high task specificity, as discussed in Section 3.1. By pre-computing and storing expert importance patterns for various tasks, we can rapidly match incoming queries to their closest task pattern and load only the most relevant experts accordingly. Offline Pattern Storage. For given set of representative tasks , we compute and store their corresponding expert importance patterns. For computational and storage efficiency, we adopt two-part storage strategy: T1, T2, . . . , TD} { 1. For the first MoE layer, we d,2, . . . , TCESS1 [TCESS1 TCESS value for expert in the first MoE layer for task Td. d,1, TCESS1 d,Nr ] complete TCESS pattern τ 1 = is the for each task Td, where TCESS1 d,i store the 2. For subsequent layers ℓ > 1, rather than storing the complete TCESS patterns, we store permutation vectors that align their expert importance ordering with the first layer. For each layer ℓ and task Td, the permutation vector πℓ Let τ 1 denote the TCESS patterns for task Td in the first and ℓ-th MoE layers, respectively. Let rank(τ , i) represent the rank (position) of expert when experts are sorted by their TCESS values in descending order according to pattern τ . The permutation vector πℓ maps the importance ranking from the first layer to layer ℓ: πℓ ZNr is computed as follows: and τ ℓ rank(τ 1 , i) = rank(τ ℓ d[i] = j, , j). s.t. (8) [k] : rank(τ 1 τ 1 Equivalently, for each rank position 0, 1, . . . , Nr { ; = argmaxk{ = argmaxk{ This permutation vector enables us to align expert importance orderings across layers, ensuring that when we select the top-M experts from the first layer, we can correspondingly select the appropriate experts in subsequent layers. 1 } [k] : rank(τ ℓ τ ℓ ; πℓ } , k) = , k) = d[i] = j. (9) } : Query-Based Retrieval. When new query is received, we execute the following retrieval process: 1. Forward the query through the model until the first MoE layers router, computing the querys TCESS pattern τ 1 . 2. Identify the most similar stored task pattern by computing the L2 distance between τ 1 and each stored first-layer pattern τ 1 : = argmin 1,...,D { τ 1 τ 1 2. } 3. Retrieve the associated pattern τ 1 and permutation vectors number of MoE layers. 6 (10) πℓ L ℓ=2, where is the } { Model Reconstruction with Task-Specific Experts. key advantage of our approach is that the full model with all experts need not be loaded into memory initially. Instead, we maintain lightweight model skeleton without experts and dynamically load only the necessary experts after task identification: 1. For the first MoE layer, select the top-M experts according to the retrieved TCESS pattern τ 1 (set values in patterns are 0 to -100 before selecting to avoid never-collected-experts): (11) TopK(τ 1 E1 = : τ 1 { [i] , ) . } 2. For each subsequent layer ℓ > 1, apply the corresponding permutation vector to map the selected experts from the first layer: πℓ { ℓ=1 into the model, dramatically reducing memory requirements compared to loading all experts (also prune and permute the router weights). 3. Load only the selected experts [i] : Eℓ { Eℓ = (12) . } } This task-adaptive expert retrieval mechanism enables efficient deployment of large MoE models on resource-constrained devices by: (1) significantly reducing TCESS computation at inference time, as it is only required for the first layer of the user query rather than for all layers or for re-evaluating stored task patterns, (2) allowing for efficient retrieval of the closest task pattern from pre-computed database, and (3) loading only small subset of experts tailored to the specific task represented by the user query. Our empirical evaluation demonstrates that this approach maintains near-original performance while achieving significant memory efficiency and minimal retrieval overhead, making it particularly suitable for deployment in memory-constrained environments, from cloud servers with limited resources to edge devices and consumer hardware, where large-scale MoE models would otherwise be impractical or impossible to run."
        },
        {
            "title": "4 Experiments",
            "content": "PreMoe is implemented based on Pytorch [26]. We collect the expert patterns offline by running the full models on servers with 32 Ascend 910B2-64GB NPUs and 192-cores Kunpeng-920 CPUs. Then we online evaluate the performance of PreMoe on 32 or 16 Ascend 910B2-64GB NPUs. We set Ka = 2 and = 0.55 in default. For each task, we collect the expert patterns based on task-specific synthetic data [27, 28]. We generate these patterns with both the question and the corresponding reasoning output of the full models. 4.1 Reasoning Performance Benchmarks. We evaluate the models generation quality on these commonly used mathematics reasoning benchmarks: MATH500 [29], AIME24 [30], AIME25 [31]; These benchmarks assess the models ability to perform complex reasoning and retrieve domain-specific knowledge, showcasing the effectiveness of our task-adaptive expert pruning approach for preserving critical model capabilities. We use greedy and zero-shot generation in all the evaluations. We report Memory alongside performance metrics to demonstrate the efficiency and resource requirements of different models, highlighting the trade-off between computational cost and model capability. Table 1 details the performance of various MoE models under different pruning configurations facilitated by PreMoe. The full 671B parameter model (8/256), requiring 1.3TB of memory, achieves accuracies of 96.8% on MATH500, 76.67% on AIME24, and 63.33% on AIME25. Remarkably, with PreMoe reducing the active expert pool to 128 per layer (DeepSeek-R1 8/128; 344B parameters, 688GB memory), performance is not only maintained but slightly improves on MATH500 to 97.2%, and matches the full models accuracy on AIME24 (76.67%) and AIME25 (63.33%). Further demonstrating the frameworks capability for more aggressive pruning, configuration retaining only 32 experts per layer (8/32; 98B parameters, 196GB memory) still achieves strong 72.0% accuracy on MATH500 (though not detailed for all benchmarks in Table 1). The table also demonstrates PreMoes effectiveness on Pangu-MoE models [23]. Pangu-UltraMoE 718B shows excellent performance retention when pruned from 8/256 (1.36TB) to 8/128 Figure 3 Example of Reasoning generation of DeepSeek-R1 with different pruning ratios. (738GB), maintaining 97.15% accuracy on MATH500 and perfect preservation of 81.3% accuracy on AIME24. Even with more aggressive pruning to 4/64 (390GB), it achieves 96.95% on MATH500 and 79.7% on AIME24. The smaller Pangu-Pro-MoE 72B demonstrates similar resilience, with its 8/64 configuration (130GB) achieving an impressive 97.6% on MATH500 and 75.62% on AIME24, while the more memory-efficient 4/48 configuration (87GB) still maintains 95.1% accuracy on MATH500. These results collectively underscore PreMoes efficacy in identifying and retaining critical experts across different model architectures, enabling significant memory reduction while preserving performance on complex reasoning tasks. Figure 3 qualitatively illustrates the impact of pruning on reasoning. It presents DeepSeek-R1s step-by-step solution to math problem under different PreMoe pruning ratios (8/256, 8/128, and 8/32). The example shows that even with significantly fewer experts (e.g., 8/32), the model often preserves the correct reasoning trajectory, affirming PreMoes ability to maintain core reasoning functions despite aggressive pruning. You can find more examples in Appendix A. 8 Table 1 Performance comparison on Reasoning quality benchmarks. For MATH500, AIME24, and AIME25, we report accuracy (%). (M/X) denotes activating experts from pool of available experts after pruning. Model Memory MATH500 AIME24 AIME25 DeepSeek-R1 671B (8/256) DeepSeek-R1 671B (8/128) Pangu-Ultra-MoE 718B (8/256) Pangu-Ultra-MoE 718B (8/128) Pangu-Ultra-MoE 718B (4/64) Pangu-Pro-MoE 72B (8/64) Pangu-Pro-MoE 72B (4/48) 1.3TB 688GB 1.36TB 738GB 390GB 130G 87G 96.80 97.20 97.40 97.15 96.95 97.60 95.10 76.67 76.67 81.30 81.30 79.70 75.62 69. 63.33 63.33 70.00 70.00 66.70 70.00 66.70 Table 2 Predictive quality evaluation. We report log-likelihood accuracy (%). Models are evaluated with 4-bit quantization (W4). (M/X) denotes activating experts from pool of available experts after pruning. Model Method Memory GPQA Biology Chemistry Physics DeepSeek-R1 671B (8/256) Origin 1.3TB DeepSeek-R1-W4 (8/32) DeepSeek-R1-W4 (8/16) DeepSeek-R1-W4 (8/8) DeepSeek-R1-W4 (8/32) DeepSeek-R1-W4 (8/16) DeepSeek-R1-W4 (8/8) Random Random Random PreMoe PreMoe PreMoe 70GB 40GB 36GB 70GB 40GB 36GB 42.57 26.73 26.73 26.73 43.56 34.15 26.73 31. 28.57 25.94 25.94 32.24 26.94 26.73 30.41 24.40 27.00 25.60 41.00 32.80 26.20 4.2 Memory-Efficient Deployment Quantization. To further reduce memory requirements for deployment in resource-constrained environments, we adopt basic min-max asymmetric 4-bit per-group quantization with groupsize of 128 (denoted W4), which balances computational efficiency and model performance. This quantization scheme reduces memory requirements while maintaining acceptable inference quality. Quantization also has the advantage of reducing the workload difficulty during the loading of the routed experts. Predictive Quality Evaluation. To ensure our pruning strategy, combined with 4-bit quantization (W4), preserves fundamental language modeling capabilities across various deployment scenarios, we evaluated loglikelihood accuracy on GPQA sub-categories [32]. In this section, we only collect the patterns on the task-specific synthetic questions. Table 2 presents these results, comparing different configurations against the original DeepSeek-R1 (8/256) model. Remarkably, PreMoe, when applied to the quantized model, not only maintains strong predictive quality but can even surpass the full, unquantized model in certain configurations. For instance, DeepSeek-R1-W4 (8/32) with PreMoe achieves loglikelihood accuracies of 43.56% on GPQA Biology and 41.00% on GPQA Physics, outperforming the original full model which scores 42.57% and 30.41% on these respective sub-categories. On GPQA Chemistry, the PreMoe 8/32 configuration (32.24%) is also competitive with the full model (31.20%). Even the more aggressively pruned PreMoe 8/16 model shows strong results, such as 34.15% on Biology and 32.8% on Physics. These findings suggest that the combination of PreMoes intelligent expert pruning and quantization can sometimes lead to performance improvements over the original model, potentially by reducing noise or regularizing the model. This approach enables effective deployment in memory-constrained environments, including cloud servers with limited resources and even consumer PCs, where use case that would be impossible with the full model. 4.3 Throughput Performance Analysis Table 3 presents throughput improvements for Pangu-Ultra-MoE (718B) and Pangu-Pro-MoE (72B) across different configurations. For Pangu-Ultra-MoE, reducing experts from 8/256 to 8/128 (BF16) 9 yields 10% throughput increase, while the 4/64 W4 configuration delivers 35% improvement. Similarly, Pangu-Pro-MoE shows 7% gain with 8/32 BF16 and 26% with 4/48 W4 compared to its 8/64 BF16 baseline. These results demonstrate that PreMoes expert pruning, particularly when combined with quantization, provides substantial throughput benefits complementing the memory efficiency advantages, enhancing the frameworks utility for resource-constrained deployment scenarios. Table 3 Comparison on Throughputs per second for Pangu-MoE models (included the results after quantization)."
        },
        {
            "title": "Configuration",
            "content": "Pangu-Ultra-MoE 718B Configuration Pangu-Pro-MoE 72B 8/256 BF16 8/128 BF16 4/64 W4 1.00x 1.10x 1.35x 8/64 BF16 8/32 BF16 4/48 W4 1.00x 1.07x 1.26x 4.4 Ablation Studies Randomly Selected Experts. To validate PreMoes TCESS-based expert selection, we compared its performance on GPQA (DeepSeek-R1-W4) against baseline of randomly selecting an equivalent number of experts, as detailed in Table 2. PreMoe consistently and significantly outperforms random selection across all sub-categories and pruning ratios. For instance, in the 8/16 setting, PreMoe achieves 34.15% log-likelihood accuracy on GPQA Biology versus 26.73% for random selection, with similar advantages observed for Chemistry (26.94% vs. 25.94%) and Physics (32.8% vs. 27.00%). This underscores the efficacy of our intelligent expert selection in maintaining performance under aggressive pruning, far exceeding naive random selection. Output Patterns Collection. Our default strategy for TCESS pattern collection uses both the input query and the models reasoning output. An ablation where patterns were collected using only the input query demonstrated significant degradation in the pruned models generation quality. As illustrated in Figure A.1 (Appendix A), models pruned with query-only patterns may exhibit repetitive loops or incomplete solutions (e.g., the DeepSeek-R1 8/32 model). This highlights that capturing expert dynamics during the entire generation, including reasoning steps, is crucial for identifying truly representative and effective set of experts for task-specific pruning, ensuring selected experts are vital for both understanding the query and generating the complete reasoned answer."
        },
        {
            "title": "5 Conclusion",
            "content": "Deploying massive Mixture-of-Experts (MoE) models in memory-constrained environments presents significant challenge across diverse computational settings. This work demonstrates that pronounced task-specific expert specialization within MoE models enables substantial, performance-preserving compression across different architectures and scales. Our PreMoE framework capitalizes on this insight through two key components: Probabilistic Expert Pruning (PEP), with its novel TaskConditioned Expected Selection Score (TCESS), identifies task-critical experts based on router logits, while Task-Adaptive Expert Retrieval (TAER) efficiently matches user queries to pre-computed expert patterns, enabling dynamic loading of minimal, task-tailored expert subsets. Our comprehensive evaluation across multiple MoE architectures validates PreMoEs effectiveness: DeepSeek-R1 671B maintains 97.2% accuracy on MATH500 with 50% expert reduction and achieves 72.0% with 87.5% expert reduction; Pangu-Ultra-MoE 718B preserves 97.15% on MATH500 and 81.3% on AIME24 with significant memory savings. These results enable deployment across diverse scenarios with memory constraints, from cloud servers to edge devices. This work validates task-specific expert specialization as potent strategy for MoE model compression, broadening accessibility to state-ofthe-art AI across varying computational environments and paving the way for further research in dynamic, sparse model architectures."
        },
        {
            "title": "References",
            "content": "[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [2] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [5] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [6] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [7] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [10] Qwen Team. Qwen1. 5-moe: Matching 7b model performance with 1/3 activated parameters, february 2024. URL https://qwenlm. github. io/blog/qwen-moe. [11] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. [13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [14] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [15] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, 22(241):1124, 2021. [16] Zehua Pei, Hui-Ling Zhen, Xianzhi Yu, Sinno Jialin Pan, Mingxuan Yuan, and Bei Yu. Fusegpt: Learnable layers fusion of generative pre-trained transformers. arXiv preprint arXiv:2411.14507, 2024. [17] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems, 34:2809228103, 2021. [18] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. 11 [19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [20] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021. [21] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. [22] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [23] Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, et al. Pangu ultra moe: How to train your big moe on ascend npus. arXiv preprint arXiv:2505.04519, 2025. [24] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. [25] Zehua Pei, Lancheng Zou, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, and Bei Yu. Cmoe: Fast carving of mixture-of-experts for efficient llm inference. arXiv preprint arXiv:2502.04416, 2025. [26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [27] Zihong Chen, Wanli Jiang, Jingzhe Li, Zhonghang Yuan, Chenyang Wang, Huanjun Kong, and Nanqing Dong. GraphGen, April 2025. [28] Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, and Kai Chen. Condor: Enhance llm alignment with knowledge-driven data synthesis and refinement, 2025. [29] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [30] American invitational mathematics examination (aime), 2024. [31] American invitational mathematics examination (aime), 2025. [32] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024."
        },
        {
            "title": "A Examples of Generation",
            "content": "In this section, we provide additional qualitative examples of reasoning generation by DeepSeek-R1 under various pruning configurations using our PreMoe framework. These supplement the examples discussed in the main experimental section (Fig. A.1,Fig. A.2, Fig. A.3). Figure A.1 Comparison Reasoning generation of DeepSeek-R1 when collecting TCESS patterns with or without considering the models reasoning output. The top example uses our default collection strategy (query + reasoning output), while the bottom example derives patterns only from the input query, leading to repetitive output for DeepSeek-R1 (8/32)."
        },
        {
            "title": "B Discussions",
            "content": "Limitations. While PreMoe demonstrates significant promise in reducing the memory requirements of large MoE models, certain limitations warrant discussion. First, the effectiveness of the TaskAdaptive Expert Retrieval (TAER) component is dependent on the diversity and representativeness of the pre-computed task patterns. If novel user query significantly deviates from all tasks in the stored database, the retrieved expert set might be suboptimal, potentially impacting performance. Expanding the task database or developing more sophisticated online adaptation mechanisms could address this. Second, while the L2 distance for matching query TCESS patterns to stored patterns is computationally efficient, it might not always capture the full semantic similarity between tasks, especially for nuanced queries. Exploring more advanced semantic matching techniques could further enhance retrieval accuracy, though potentially at the cost of increased retrieval latency. Finally, although we demonstrate significant memory savings, the process of loading even subset of experts still incurs some I/O overhead. For applications requiring extremely low latency, this overhead, while 13 Figure A.2 Example 2 of Reasoning generation of DeepSeek-R1 with different pruning ratios (8/256, 8/128, 8/32) using PreMoe. minimized by our approach, remains factor to consider. These limitations offer avenues for future research and refinement of the PreMoe framework. Impact Statement. This paper aims to contribute to the advancement of the Machine Learning field. While our work may have various societal implications, we do not find it necessary to emphasize any particular consequences here. Figure A.3 Example 3 of Reasoning generation of DeepSeek-R1 with different pruning ratios (8/256, 8/128, 8/32) using PreMoe."
        }
    ],
    "affiliations": [
        "Noahs Ark Lab, Huawei",
        "The Chinese University of Hong Kong"
    ]
}