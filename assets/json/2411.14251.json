{
    "paper_title": "Natural Language Reinforcement Learning",
    "authors": [
        "Xidong Feng",
        "Ziyu Wan",
        "Haotian Fu",
        "Bo Liu",
        "Mengyue Yang",
        "Girish A. Koushik",
        "Zhiyuan Hu",
        "Ying Wen",
        "Jun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 1 5 2 4 1 . 1 1 4 2 : r Preprint. Work in progress. . NATURAL LANGUAGE REINFORCEMENT LEARNING Xidong Feng1 , Ziyu Wan2 , Haotian Fu3, Bo Liu4, Mengyue Yang5, Girish A. Koushik6, Zhiyuan Hu4, Ying Wen2, Jun Wang1 1University College London, 2Shanghai Jiao Tong University, 3Brown University, 4National University of Singapore, 5University of Bristol, 6University of Surrey Figure 1: Practical pipeline for implementing NLRL in the Tic-Tac-Toe game. LLMs can serve as the language policy ①, the language-based value function approximator ②, the language Monte Carlo or temporal-difference operator ③, and the language policy improvement operator ⑤. By distilling (④, ⑥) the improved evaluations from ② and the enhanced actions from ⑤, the NLRL agent can iteratively refine its language policy and evaluation capabilities."
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RLlike policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github. com/waterhorse1/Natural-language-RL. *Equal contribution, correspondence to fengxidongwh@gmail.com Core contributors. 1 Preprint. Work in progress."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) (Sutton & Barto, 2018) provides rigorous framework Markov Decision Process (MDP) for solving general decision-making problems. It transforms the policy learning problem into mathematical optimization task. While RL has achieved breakthroughs across various domains, several challenges remain. For example, traditional RL algorithms generally lack task-specific prior knowledge, requiring extensive sampling to approximate environment dynamics. RL policy also lacks interpretability. Even in superhuman-performing models like AlphaZero (Silver et al., 2017), strategic reasoning remains elusive, even to professional players. RL training is also unstable (Zheng et al., 2023; Andrychowicz et al., 2020) due to its reliance on scalar rewards as the sole supervision signal. This one-dimensional feedback is particularly limiting in real-world scenarios where richer, multi-modal signals are naturally available, such as textual feedback (Bai et al., 2022; Madaan et al., 2024), visual demonstrations (Bousmalis et al., 2023; Xu et al., 2024), or other sensory inputs (Qiao et al., 2024). To tackle these challenges, we seek new RL paradigm shift, inspired by language-centric decisionmaking. Unlike traditional RL, which relies heavily on formalized mathematical modeling, humans can leverage natural language to interpret tasks, devise strategies, and communicate their reasoning. This language-driven approach enables rapid generalization using text-based prior knowledge, enhances interpretability through explicit reasoning, and provides access to rich, informative signals from linguistic data. Thus, natural language represents largely untapped resource for improving the efficiency, stability, and interoperability of RL systems. The recent success of language-based transformers (Vaswani, 2017) further opens new avenues for integrating language into the RL framework. Large language models (LLMs) have demonstrated their unprecedented proficiency in generating, understanding, and processing complex language-based information. Building upon language-centric decision-making and advancement of LLMs, we introduce Natural Language Reinforcement Learning (NLRL), novel RL paradigm that combines RLs mathematical rigor with the representational richness of natural language. In NLRL, core RL componentssuch as task objectives, policies, value functions, and the Bellman equationare reinterpreted as language-based constructs. The medium of natural language largely facilitates the integration of prior knowledge stored in LLMs, and effectively translates decision-making processes into form that is both intuitive and interpretable. NLRL also provides systematic solution for leveraging rich textual feedback in sequential decision-making tasks, enabling stable training. Building on this new paradigm, we can efficiently implement RL algorithms in language representation space, resulting in various LLM applications. Leveraging the unsupervised nature of RL, all training and improvements can be achieved through environment interaction without the need for labeled data from humans or advanced models. In particular, Sec 4.2 illustrates how NLRL algorithms can enhance an LLM agents critique and planning abilities using pure prompting. Sec 4.3 details the training of natural language value function to serve as an evaluator or validator, providing reliable assessments for any given state. comprehensive NLRL training pipeline, covering iterative training of both language policies and critics, is outlined in Sec 4.4. Similar to the traditional actor-critic framework (Barto et al., 1983; Sutton et al., 1999), this pipeline enables the LLM to learn purely from textual environmental feedback, generating language-based Chain-of-Thought (Wei et al., 2022b) enhanced policies and critiques. We empirically validate these use cases in environments such as the Maze game, Breakthrough board game, and Tic-Tac-Toe, demonstrating the effectiveness and superiority of the NLRL framework."
        },
        {
            "title": "2 PRELIMINARY OF REINFORCEMENT LEARNING",
            "content": "Reinforcement Learning models the decision-making problem as Markov Decision Process (MDP), defined by the state space S, action space A, probabilistic transition function : [0, 1], discount factor γ [0, 1) and reward function : [Rmax, Rmax]. The goal of RL aims to learn policy π : [0, 1], which measures the action as probability given the state s: π(as) = Pr (At = St = s). In decisionthe optimal policy tends to maximize the expected discounted cumulative remaking tasks, t=0 γtr (st, at) (cid:3). The state-action and state value functions ward: π(as) = arg maxπ Eπ are two key concepts that evaluate states or state-action pairs by measuring the cumulative re- (cid:2) (cid:80) 2 Preprint. Work in progress. ward starting from them: Qπ(st, at) = E(s,a)t+1:Pπ at,(s,a)t+1:Pπ icy π and dynamic transition . i=t γitr (si, ai) st (cid:2)(cid:80) (cid:3), Vπ(st) = (cid:3), where Pπ is the trajectory distribution given the poli=t γitr (si, ai) st, at (cid:2)(cid:80) Given the definition of Vπ(st), the relationship between temporally adjacent states value (e.g., (st) and (st+1)) can be derived as the Bellman expectation equation (Bellman et al., 1965). Here is an example of one-step Bellman expectation equation: Vπ(st) = Eatπθ (cid:104) r(st, at) + γE (cid:105) st+1p(st+1st,at)[Vπ(st+1)] , st (1) similar equation can also be derived for Qπ(s, a). Given these basic RL definitions and equations, many RL algorithms fall in the scope of generalized policy iteration (GPI). GPI is an iterative process and each iteration consists of two procedures: policy evaluation and policy improvement. Policy Evaluation. The target of the policy evaluation process is to estimate state value function Vπ(s) or state-action value function Qπ(s, a) for given policy π. For simplicity, we only utilize Vπ(s) in the following illustration. Two common value function estimation methods are the Monte Carlo (MC) estimate and the Temporal-Difference (TD) estimate (Sutton, 1988). Starting from the definition of Vπ(st), the Monte Carlo (MC) estimate uses sampling over complete trajectories to )(cid:3) where we average the cucalculate an unbiased estimate: Vπ(st) 1 n=1 mulative rewards over multiple full paths starting from st. The Temporal Difference (TD) estimate, on the other hand, builds on the temporal relationship between states without requiring complete trajectories. It estimates Vπ(st) using the immediate reward and the estimated value of the next t+1)(cid:3). This approach effectively uses bootstrap by state: Vπ(st) 1 depending on the estimated value of the next state to approximate Vπ(st). i=t γitr(sn ) + γVπ(sn (cid:2)r(st, an , an (cid:2)(cid:80) (cid:80)K (cid:80)K n=1 Policy Improvement. The policy improvement process aims to update and improve policy according to the result from the policy evaluation process. Specifically, it replaces the old policy πold with the new one πnew to increase the expected return: Vπnew (s0) Vπold (s0). In the environment with small, discrete action spaces, such improvements can be achieved by greedily choosing the action that maximizes Qπold (s, a) at each state: πnew( s) = arg max π(s)P(A) Eaπ [Qπold(s, a)] , S, Another improvement method involves applying policy gradient ascent (Sutton et al., 1999). parameterizes the policy πθ with θ and we can derive the analytical policy gradient: (cid:21) (cid:20) θVπθ (s0)θ=θold = E(s,a)Pπθold θ log πθ(as)Qπθold (cid:12) (cid:12) (s, a) (cid:12)θ=θold , (2) It (3) Where s0 denotes the initial state. By choosing small step-size α > 0 to conduct gradient ascent: θnew = θ + αθVπθ (s0)θ=θold , we can guarantee the policy improvement: Vπnew (s0) Vπold (s0)."
        },
        {
            "title": "3 NATURAL LANGUAGE REINFORCEMENT LEARNING",
            "content": "In contrast to the precise statistical models used in traditional RL, NLRL frames all elementsincluding task objectives, value evaluations, and strategic policieswithin the form of natural language. This section aims to navigate decision-making tasks using natural language, aligning it with traditional RL concepts, definitions, and equations. Due to the inherent ambiguity of natural language, the equations presented here are not strictly derived from mathematical definitions. Instead, they are analogical and based on empirical insights into original RL concepts. We leave rigorous theoretical definition and analysis for future work."
        },
        {
            "title": "3.1 ANALOGIES",
            "content": "We start with analogies in traditional RL to model NLRL. We provide Fig. 2 to illustrate most concepts discussed in this section. 3 Preprint. Work in progress. Text-based MDP: To conduct RL in natural language space, we convert traditional MDP to the text-based one, which leverages text descriptions to represent MDPs basic concepts, including state s, action a, and environment feedback (state transitions and reward r). Language Task instruction: For decisionmaking tasks, NLRL defines natural language task instruction TL, like reaching the goal or opening the door. Then, we denote metric by that measures the completeness of the task instruction given the trajectory description DL(τπ), where DL is language descriptor that can transform the trajectory distribution τπ into its corresponding language description DL(τπ). The objective of NLRL is reformulated as Figure 2: RL and NLRL framework max π (DL(τπ), TL) (4) That is, NLRL is trying to optimize the policy so that the language description of the trajectory distribution τπ can show high completeness of the task instruction. Language Policy: Instead of directly modeling action probability, NLRL determines the action with chain-of-thought process (Wei et al., 2022b), including strategic thoughts, logical reasoning, and planning. Thus, we represent the policy on language as πL(a, cs) = πL(cs)πL(ac, s), which will first generate such thought process πL(cs), then output the final action probability π(ac, s). Language Value Function: Similar to the definition of and in traditional RL, NLRL leverages language value function, relying on natural language evaluation to assess the policy effectiveness. The language state value function π are defined as: (5) π and language state-action value function QL π (st, at) = (cid:0)(s, a)t+1: Pπ st, at, TL QL π (st) = (cid:0)at, (s, a)t+1: Pπ st, TL (cid:1) Given the current state st or state-action (st, at), QL π leverage language descriptions instead of scalar value to demonstrate the effectiveness of policy for achieving the task objective TL. The language value functions are intuitively rich in the information of values and enhance interpretability rather than the traditional scalar-based value. It can represent the evaluation results from different perspectives, consisting of the underlying logic/thoughts, prediction/analysis of future outcomes, comparison among different actions, etc. (cid:1) , π and Language Bellman Equation: In the traditional Bellman equation (Equ. 1), the state evaluation value Vπ(st), can be decomposed into two parts: : (1) the intermediate transition, which include immediate at, reward rt, and next state st+1. (2) the next state evaluation Vπ(st+1). Based on such decomposition intuition, we argue that the language value function π should analogically satisfy the language Bellman equation Equ.6. π (st) = Gat,st+1Pπ 1 (cid:16) G2 (cid:0)d (cid:0)at, (st, at) , st+1), π (st+1 (cid:1)(cid:1) (cid:17) , st S, (6) where (at, (st, at) , st+1)) depicts the language description of intermediate transition, while G1 and G2 serves as two information aggregation functions. By drawing an analogy to Equ. 1, G2 mimics the add the summation operation + in the original Bellman equation, aggregating information from intermediate transitions description and future evaluation π (st+1). Meanwhile, G1 serves the role of the expectation operator E, aggregating information accross different (at, st+1) pairs by sampling from the transition distribution Pπ."
        },
        {
            "title": "3.2 LANGUAGE GENERALIZED POLICY ITERATION",
            "content": "In this part, we introduce how language GPI is conducted. Similar to traditional GPI, language GPI also consists of two procedureslanguage policy evaluation and language policy improvement."
        },
        {
            "title": "3.2.1 LANGUAGE POLICY EVALUATION",
            "content": "Language policy evaluation aims to estimate language value function present how two classical estimations: MC and TD estimate work in language policy evaluation. π for each state. We π and QL 4 Preprint. Work in progress. Language Monte-Carlo Estimate. Starting from the state st, MC estimate is conducted over text rollouts (i.e. full trajectories {at, (s, a)t+1:}) given the policy π. Since we cannot take the average operation in language space, we instead leverage language aggregator G1 to aggregate information over finite trajectories, approximating the expected evaluation: π (st) G1 (cid:16)(cid:8)an , (s, a)n t+1: (cid:9)K n= (cid:17) , (7) Language Temporal-Difference Estimate. Language TD estimate mainly relies on the one-step language Bellman equation illustrated in Equ. 6. Similar to the language MC estimate, we aggregate onestep samples to approximate the expected evaluation: π (st) G1 (cid:16)(cid:8)G (cid:0)d(st, an , r(st, an ), sn t+1), π (sn t+1)(cid:1)(cid:9)K n=1 (cid:17) , st S, (8) where d, G1 and G2 has the same meaning for that in Equ.6. Check Appendix for more about how language MC/TD corresponds to the traditional MC/TD. Language MC estimate is free from estimation bias * as it directly utilizes samples from complete trajectories. However, the MC method is prone to high variance considering the significant number of variations (Large K) in the long-term future steps. Such variability poses challenge for the language aggregator G1 in Equ. 7 to effectively extract crucial information from diverse trajectories. On the contrary, while the inaccuracy of the next state evaluation π (st+1) can bring estimation bias to TD estimate, they effectively reduce variance by discarding future variations. G1 and G2 are only required to conduct simple one-step information aggregation with limited variations."
        },
        {
            "title": "3.2.2 LANGUAGE POLICY IMPROVEMENT",
            "content": "Similar to traditional policy improvement, the motivation of language policy improvement also aims to select actions that maximize the task completeness function : πnew( s) = arg max π(s)P(A) (QL πold(s, a), TL), (9) The task completeness is difficult to quantify for general language-based tasks, as it largely relies on human textual prior knowledge. Given this complexity, instead of mathematically optimizing , we also refer to the chain-of-thought processNLRL leverages language analysis process that can generate the thought process to guide policy optimization and action selection. πnew( s), = I(QL πold(s, a), TL), π( s) P(A), S. (10) Language policy improvement conducts strategic analysis to determine the most promising action for task completion as the new policy πnew( s). Ideally, this analysis is mainly based on the correlation judgment between the language evaluation QL πold(s, a) and task objective TL."
        },
        {
            "title": "4 PRACTICAL NLRL IMPLEMENTATION WITH LLMS",
            "content": "Section 3 outlines the core philosophy of NLRL, which involves translating key concepts from RL into their human natural language counterparts. To practically implement these key concepts, model capable of understanding, processing, and generating language is essential. Large language models (LLMs), trained on vast corpora of human language and knowledge, emerge as natural choice to emulate human behaviour and implement language-based RL components. In the following sections, we demonstrate how LLMs can function in various roles within the NLRL framework. *We use quotes for bias and variance to indicate that we draw on their conceptual essence, not their strict statistical definitions, to clarify concepts in NLRL. 5 Preprint. Work in progress."
        },
        {
            "title": "4.1 COMPONENTS AND PROCEDURES",
            "content": "① LLMs as language policy (πL). Many works adopted LLMs as the decision-making agent (Wang et al., 2023a; Feng et al., 2023a; Christianos et al., 2023; Yao et al., 2022) with Chain-of-thought process (Wei et al., 2022b). By setting proper instructions, LLMs can leverage natural language to describe their underlying thought for determining the action, akin to human strategic thinking. ② LLMs as language value function approximator (QL, L). The original idea of value function approximation (Sutton et al., 1999) is to use parameterized one-dimensional output function to replace the big value table and serve as value function approximator. Such design largely helps RL to handle high-dimensional and large-scale decision-making problems. Similarly, in NLRL, we can naturally leverage (multi-modal) LLMs, to evaluate the state or state-action pair (s, a), and serve as the language value function approximator for QL, L. This exactly corresponds to what (multimodal) LLMs are capable of they are designed to take in the features from the task state, such as low-dimension statistics, text, or images, and output the corresponding language understanding. By further prompting or fine-tuning over evaluation dataset, LLMs can generate language assessment. ③ LLMs as language Monte-Carlo and TD operator (G1, G2). As mentioned in Sec 3.2.1, we can estimate the language value function by language MC or TD estimate. The key issue is implementing language aggregator G1, G2. To achieve this, we can leverage the inherent strengths of LLMs in aggregating and synthesizing diverse linguistic inputs. Specifically, LLMs can be powerful information summarizers (Zhang et al., 2023), extractors (Xu et al., 2023), and aggregators to help us fuse intermediate transitions and future language evaluations. LLMs can serve as G1, G2 by prompting them to summarize and aggregate multiple rollout trajectories (language MC), or multiple few-step transitions and future states evaluation (language TD). ④ Distilling language value estimation into language value function approximator. ③ provides an unsupervised and scalable way to generate language evaluation data through environment interaction, which can be leveraged to train our language value function approximator in ②. This corresponds to traditional critic training for value-based RL (e.g, DQN (Mnih et al., 2015)) or actorcritic algorithms (e.g, PPO (Schulman et al., 2017)), but happens in natural language representation space. The only difference is the training objective the scalar-based value function regresses the value mean with L2 mean squared error while the language value function tries to imitate the language aggregation result with supervised-finetunings cross-entropy loss. ⑤ LLMs as policy improvement operator (I). With the chain-of-thought process and prior knowledge about the world, LLMs are better to determine the most promising action πnew( s) by taking language analysis over the correlation of language evaluation QL πold(s, a) and task objective TL. The underlying idea also aligns with some recent works (Kwon et al., 2023a; Rocamonde et al., 2023) that leverage LLMs or Vision-language models as the rewardthey can accurately model the correlation. Specifically, for given state s, we prompt the LLM with several action candidates and their corresponding language evaluations QL, by which we can obtain the improved action with chain-of-thought process analyzing different actions evaluation results. ⑥ Language policy training by distilling improved policy. Similar to ④, we can train our language policy LLM in ① by supervised-finetuning over the improved chain-of-thought based language policy data from ⑤. This corresponds to the traditional policy training in policy-based RL (e.g, REINFORCE (Williams, 1992)) or actor-critic algorithms (e.g, PPO (Schulman et al., 2017)). However, NLRL relies on the more stable supervised loss instead of the policy gradient loss. We also provide example visualizations in Fig 1 to illustrate these components and procedures. Building upon these components and procedures, we can create various NLRL applications. Here we illustrate three use cases, though many more possibilities exist beyond these examples."
        },
        {
            "title": "4.2 LANGUAGE GPI BOOSTS LLM’S CRITIC AND POLICY BY PROMPTING (①, ②, ③, ⑤)",
            "content": "Our first case utilizes language GPI to enhance LLMs critic and policy solely through prompting, which can be particularly beneficial for improving proprietary models such as GPT-4 (OpenAI, 2023) or Gemini (Team et al., 2023). Specifically, we first combine ①, ②, ③ to build language policy evaluation pipeline. Take language TD shown in Equ. 8 as an example. We prompt LLMs to π (st+1) (②) (2) serve as the TD operator G1, G2 (③). By (1) evaluate the subsequent states value performing one-step look-ahead with at π (①) and leveraging G1, G2 to aggregate information 6 Preprint. Work in progress. from intermediate transition alongside the subsequent evaluation π (st+1), language TD can generate new and improved language evaluation, π (st)new. Building upon this new evaluation, LLM-based policy improvement operator (⑤) converts the evaluation into better policy πnew. We refer the reader for Algorithm 1 for detailed procedures."
        },
        {
            "title": "4.3 TRAINING NATURAL LANGUAGE VALUE FUNCTION FOR A GIVEN POLICY (②, ③, ④)",
            "content": "Our second case aims to train an LLM critic capable of evaluating any given state with natural language explanations, similar to chess annotator who provides insightful commentary on boards, moves, and strategies. For example, we can build an iterative language TD pipeline by combining ②, ③, and ④. First, we leverage tunable LLM and prompt it to become language value function approximator ②. Combined with the look-ahead transitions (at, st+1) Pπ by taking rollouts with policy π, and the subsequent state evaluation π (st+1) generated by A, we prompt LLM for the language TD estimate (③), similar to Sec 4.2. The model is further finetuned by such language TD estimates (④) and will be plugged back for π (st+1)new in new iteration. Iteratively, we can obtain the final converged natural language value function. Check Algorithm 2 for more details."
        },
        {
            "title": "4.4 NATURAL LANGUAGE ACTOR-CRITIC LEARNING (①, ②, ③, ④, ⑤, ⑥)",
            "content": "Last, we combine all these procedures and build the full natural language actor-critic pipeline. Similar to traditional actor-critic (Barto et al., 1983; Sutton et al., 1999), our natural language actor-critic framework simultaneously learns both language policy and language critic through unsupervised environment interactions. For each iteration, we first use language policy ① to take rollouts in the environment. With new trajectories, we update our language value model ② by language MC or TD ③ and train it with supervised-finetuning loss (④). For language policy improvement, we query our updated language value to evaluate action candidates for states extracted from the rollout trajectories. Further LLM-based improvement operator ⑤ brings us stronger policy, which will be used to train our language policy (⑥). Check Algorithm 3 for more details."
        },
        {
            "title": "5.1 LANGUAGE GPI BY PROMPTING (SEC 4.2)",
            "content": "Our first experiments explore Sec 4.2 and leverage language GPI to improve LLM capability with pure prompting. Specifically, we choose the maze games of LMRL (Abdulhai et al., 2023), aiming to validate that Language TD Estimate and Language Policy Improvement can benefit the evaluation and further improve policy. We use the original settings of LMRL Gym Maze, where the agent is required to navigate to goal position in Double or Medium maze. We consider the fully-observable setting, where the agents observation (described by text) includes the agents current position in the maze, the agents action history, the walls position around the agent (if any), and the goal position. The action space is discrete, including moving up / down / right / left. We evaluate the performance on 30 different initial positions, each with 3 random seeds. Refer to Appendix B.1.2 for more experimental details and result visualizations. Figure 3: The double maze and the medium maze. At the beginning of each test, the agent randomly respawns in white space. For language TD estimate in Equ. 8, we prompt gpt-4o-mini-2024-07-18 as the language aggregator G1, language state-action value aggregator G2, and language state value function π respectively. Specifically, given an environment state, for each candidate action, we use fully random policy to rollout steps into the future and use the language state value function π to evaluate the look-ahead state. For each state we repeat this process for times and use G2 to aggregate into state-action value estimation. For language policy improvement in Equ. 10, the same GPI-4o-mini is leveraged as the improvement operator I. We compare Language GPI with few different baselines, including prompt-based language policy πL (①), as well as prompt-based language value function QL(s, a) + language policy improvement I. As shown in Table 1, Language TD produces better performance than the prompt-based language 7 Preprint. Work in progress. Table 1: Language GPI results with ablations on look-ahead steps and variations number K. Double Maze Medium Maze Avg Reward Language policy πL(s) (①) 27.05 5.27 Language value function QL(s, a) + improvement (②,⑤) 33.57 14.41 Language GPI (1 variation, 3 look ahead steps) (②, ③, ⑤) 20.85 7.59 Language GPI (4 variations, 1 look ahead steps) (②, ③, ⑤) 12.65 4.72 Language GPI (4 variations, 3 look ahead steps) (②, ③, ⑤) 15.09 4.44 Language GPI (6 variations, 3 look ahead steps) (②, ③, ⑤) 15.44 4.97 Language GPI (8 variations, 3 look ahead steps) (②, ③, ⑤) 11.19 2.86 12.23 4.49 27.29 4.43 18.33 6.11 17.85 3.68 17.48 4.53 12.74 4.47 12.15 2.96 value in both mazes, and more variations & look ahead steps can have even better results. Language policy improvement can only benefit stably for language-TD-enhanced language value (② and ③)."
        },
        {
            "title": "5.2 TRAINING NATURAL LANGUAGE VALUE FUNCTION WITH LANGUAGE TD (SEC 4.3)",
            "content": "Our second experiment is an implementation of Sec 4.3. Specifically, we aim to train language value function with language TD in the 5x5 breakthrough board game (Fig 4). Breakthrough is turnbased board game for two players, white and black. Each player has few lines of pawns. Black moves first and in each turn, the player chooses one piece to move one square forward (straight or diagonal) onto empty squares or capture diagonally. The goal is to break through the opponents line and reach their home row. As mentioned in Sec 4.3, this value function can serve as reliable board evaluator/annotator. However, evaluating board states in this game is challenging for LLMs. First, the 5x5 breakthrough variant is niche, leading to sparse pre-training data on board evaluations. Most LLMs have minimal knowledge of it. Second, despite the small board size, its state space complexity can reach 108 (Saffidine et al., 2012), making natural language assessments significantly more difficult. Refer to Appendix B.2 for more experimental details and result visualizations. Figure 4: 5x5 Breakthrough."
        },
        {
            "title": "5.2.1 ENVIRONMENT, DATASET, MODEL, AND EVALUATION",
            "content": "Text-Breakthrough. As mentioned in the text-based MDP, we textualize the game, including board representation, pieces, moves, positions, and information about capturing or termination. π (s). Our first task is to determine the π in Policy π in π (s) measures the value for specific policy, according to Equ 5. Given that our ultimate goal is to train reliable board annotator, it makes sense to select strong or near-optimal policyanalogous to the way we place greater trust in grandmasters annotations when evaluating position. In our experiment, our π is the Monte-Carlo Tree-Search (MCTS) algorithm (Kocsis & Szepesvari, 2006) from OpenSpiel (Lanctot et al., 2019), with high number of simulations and rollouts to ensure an extensive search. π (s) since Building State Dataset (s). Since π (s) is over state s, our second step is to build state dataset Dsthe distribution (s) that our π works on. To ensure that π can assess positions across different levels of gameplay, we build mixed state dataset. Specifically, we maintain pool of MCTs policies, where each policy variant is characterized by distinct number of simulations and rollouts. The mixed state dataset is then created by pairwise combining policies from this pool, running rollouts for each policy pair, and merging the resulting data from all combinations. Then we can easily split Ds to build training state set Dtrain Building TD training dataset. The TD training dataset is built by conducting look-ahead expansion with rollout policy π, from state training set Dtrain . For each state st, we conduct few times l-step look-ahead rollout and deduplicate to distinct variations: {(st+i, at+i, rt+i, st+i+1)l1 Models. We use two LLMs for the full language-TD pipelines. For the language TD operator (G1, G2), we prompt the large LLaMA-3.1-70B-Instruct (Dubey et al., 2024) model since it preserves stronger aggregation and reasoning ability. Because of the limitation of computing resources, the language value function π (s) is trained from the small LLaMA-3.1-8B-Instruct model. and test state set Dtest i=0}K. . 8 Preprint. Work in progress. Figure 5: Breakthrough experiment results. (a) Performance comparison with baselines. (b,d) Ablation study over look-ahead step number and variation number. (c) Results for state scaling law. Evaluation. The final step is to develop an automatic metric for evaluating the language value function/board evaluation, which is challenging even for advanced LLMs like GPT-4o, given their limited domain knowledge. Thus, we adopt coarser metricjudging which side has the advantageas our evaluation criterion. Specifically, for any given state, we estimate win rates using Monte Carlo simulations by running rollouts to game completion and computing the outcomes. The side with the higher win rate serves as the ground-truth label. To reduce ambiguity, we only include states where one sides win rate surpasses predefined threshold in the evaluation set."
        },
        {
            "title": "5.2.2 EXPERIMENTAL RESULTS",
            "content": "Performance vs Baselines. Our fig 5.a shows the evaluation accuracy comparison on Dtest , between our best language value functions training curve and prompting-based LLMs (LLaMA-3.170B, LLaMA-3.1-8B and GPT-4o-08-06), which clearly demonstrates the necessity of language TD training. Due to the lack of domain knowledge, all prompting-based LLMs perform poorly. The best accuracy (0.61) is only slightly higher than the random guess (0.5), while our trained language value function dominates the task and the accuracy reaches more than 0.85 on the test set Dtest . Look-ahead Ablation. Fig 5.b and Fig 5 shows further ablation study over variation number (m) and look-ahead steps k. The test set performance is shown in Fig 5.b.2 and Fig 5.d.2, while Fig 5.b.1 and Fig 5.d.1 present subset of in-training state evaluations. The ablation study indicates that determining the appropriate and is crucial for optimal performance. Notably, in Fig 5.d.1 and Fig 5.d.2, using 8 steps resulted in significantly higher in-training accuracy but slightly lower test set accuracy compared to 4 steps. This suggests that longer look-ahead steps may lead to overfitting, potentially hindering generalization to novel states. State Scaling Law. We also examine how scaling the training state size affects performance in Fig 5.c, with datasets size 0.4k, 2k, 10k, and 45k. The results demonstrate robust scalability of our algorithm, with consistent improvements in test set performance as the training set size increases."
        },
        {
            "title": "5.3 NATURAL LANGUAGE ACTOR-CRITIC LEARNING (SEC 4.4)",
            "content": "For validating Sec 4.4, we implement the natural language actor-critic pipeline in model-free setting (Sutton & Barto, 2018) in the Tic-Tac-Toe game (Ju, 2024), where the system learns purely from sampled trajectories without access to environment dynamics. As supplement to language TD in Sec. 5.1 and 5.2, we utilize the language MC (Equ. 7) for the language policy evaluation."
        },
        {
            "title": "5.3.1 STABLIZING LANGUAGE VALUE FUNCTION",
            "content": "Despite the soundness of Sec 4.4, we identify training instability resulting from the language value function, mainly from two perspectives. First, we observed that the language value function π can 9 Preprint. Work in progress. Figure 6: Natural Language Actor Critic Pipeline training results against different opponents. (a) Training results against the First-Move Opponent. (b) Training results against the Random-Move Opponent. These results demonstrate that our proposed Natural Language Actor Critic pipeline can stably improve under deterministic and stochastic dynamics. easily hallucinate when evaluating unseen state-action pairs, leading to incorrect language policy improvement. To address this, we add an action selection mask to restrict language policy improvement only to moves that are likely to be chosen by the language policy πL during rollout. Instead of prompting LLM to select from all available moves, we sample the language policy Nsample times and build an action candidate list. The language policy improvement is conducted from the lists top-m moves. Such selection mask can help constrain policy updates within limited range, akin to how KL divergence is used in RLHF objective function (Ouyang et al., 2022). Second, we encountered the challenge of catastrophic forgetting for value training. Specifically, LLM can rapidly overfit to the state-action pairs encountered in the current iteration while forgetting past experiences, e.g. state-action pairs encountered only few iterations ago. To address this, we enlarge the language value function training set by merging the last Kbuf er value function buffers, which help stabilize the training of the language value function by preserving past knowledge."
        },
        {
            "title": "5.3.2 EXPERIMENT SETUP AND METHODS",
            "content": "Text-TicTacToe. Similar to text-breakthrough, we textualize all information in the Tictactoe game. Model Architecture. Our implementation uses three language models: one LLaMA-3.1-70BInstruct and two LLaMA-3.1-8B-Instruct models. The 70B model implements the language aggregator (G1) and policy improvement operator - notably, we restrict this model to use only its general language processing and information aggregation capabilities, rather than any game-specific knowledge it may have learned during pre-training. This restriction is enforced through carefully designed prompts (see Appendix C.3 for details). The two 8B models implement our trainable components: one serves as the language policy πL, generating actions through chain-of-thought reasoning, while the other implements the language value function QL π , evaluating state-action pairs. Dataset Construction. For each iteration, we collect 512 complete game trajectories with πL. For Monte Carlo value estimation, we collect KM = 5 complete trajectories for each state-action pair. For policy improvement, πL proposes Nsample = 10 candidate actions per state and we select the top-m (m = 10) actions from the candidate action list. Evaluation Protocol. To thoroughly evaluate our approach, we test against two different types of opponents. The first is deterministic opponent that always takes the first available action, while the second is stochastic opponent that plays randomly, presenting more challenging case for policy evaluation. We measure performance through metrics including win/loss/tie rates. Baselines. We compare our algorithm with prompting-based method using GPT-4o-08-06, LLaMA3.1-8B/70B-Instruct with step-by-step reasoning. We also include traditional RL baseline by finetuning LLaMA-3.1-8B-Instruct with standard PPO (Schulman et al., 2017)."
        },
        {
            "title": "5.3.3 EXPERIMENTAL RESULTS",
            "content": "Performance against baselines with different opponents The learning curves in Fig. 6 demonstrate our systems superiority against all other baselines in different settings. Our algorithm out10 Preprint. Work in progress. Figure 7: Ablation studies on number of training epochs, Monte Carlo sample size KM C, experience buffer size Kbuf er, and number of rollout trajectories. performs all other baselines, including the PPO baseline, which helps verify the effectiveness and efficiency brought by NLRL. The improvement is substantial, especially in the random-move opponent setting (Fig. 6(b)), demonstrating NLRLs robustness despite the stochastic dynamics. Fig. 6 also presents the necessity of our proposed action selection mask in Sec 5.3.1. Ablation Studies We conduct comprehensive ablation studies under reduced computational budget of 128 trajectories per iteration to investigate key hyperparameters. Our analysis of training epochs reveals that increasing from 1 to 3 epochs significantly improves stability and adaptation, as shown in Fig. 7(a). Second, our investigation of Monte Carlo sampling with KM values of 1, 2, and 5 shows in Fig. 7(b) that increased sampling leads to more stable training, where KM = 5 achieves the best performance. Third, we examine the effect of experience buffer size Kbuf er on learning in Fig. 7(c), demonstrating that when Kbuf er = 3 helps maintain consistent performance by preserving past experiences. Finally, Fig. 7(d) shows how the number of rollout trajectories affects training, with 512 trajectories per iteration yielding the most robust learning curve. Additional ablation studies on action selection parameter top-m can be found in Appendix B.3.5, where we demonstrate that larger values enhance exploration while maintaining stable learning. Refer to Appendix B.3 for more experimental details and result visualizations."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Language Model Based Autonomous Agent. Inspired by the strong emergent capabilities of LLMs in complex reasoning and planning scenarios (Brown et al., 2020; Wei et al., 2022a; Anil et al., 2023; OpenAI, 2023; Feng et al., 2023b). The field of language model-based autonomous agents (Feng et al., 2023a; Christianos et al., 2023; Zhang et al., 2024) has witnessed growing trend of leveraging LLMs for high-level planning purposes. For text agents, ReAct(Yao et al., 2022) leverages chain-of-thought planning about the intermediate goals and task-specific generation via few-shot prompting. Reflexion (Shinn et al., 2023) is built upon ReAct (Yao et al., 2022) with selfreflection, named verbal reinforcement learning, to generate tips given online feedback. Such tips strengthen agents capability through in-context learning. (Zhong et al., 2024) proposes to distill such reflection ability from the large model to train small policy feedback model that can guide the policy. Their underlying ideas share similarities with NLRLs language Monte-Carlo estimate, while NLRL formulates this process more formally and incorporates it into trainingwe extract core information/concepts by sampling multiple trajectories and leverage them to train our natural language value function. Another line of work like Zhang et al. (2023); Xu et al. (2023) leverages large language models to summarize and extract information to help fuse intermediate changes. They are related to the aggregation idea of language value function training. Interpretable Reinforcement Learning. One of the major purposes of interpretable RL is to automatically seek explanations from non-AI experts. For instance, methods of concept-based explanations for sequential decision-making tasks. Ji et al. (2023) provide concept-based explanation for 3D action recognition CovNets by clustering learned human interpretable features. Sreedharan et al. (2020) formulates concept-based explanations upon state preconditions and action costs, representing any factual statement user associates with given state. Similarly, Hayes & Shah (2017) uses logical formulas to summarize policies. Additionally, Das et al. (2023) trains joint embedding model for state-action pairs and concept-based explanations. 11 Preprint. Work in progress. Learning from Language Feedback. Our work is also related to research on learning from language feedback. While Cheng et al. (2023) focuses on benchmarking algorithms, we aim to propose new algorithmic framework. Studies such as (Yang et al., 2023; Yuksekgonul et al., 2024; Cheng et al., 2024) introduce an LLM-based optimization paradigm that leverages natural language to represent optimization operators like gradients and backpropagation, achieving end-to-end generative optimization. NLRL, on the other hand, represents parallel approach, offering generative framework specifically designed for RL optimization problems. LLMs as evaluation function. Our language value function aligns with recent efforts in NLP that leverage LLMs as generative evaluators or verifiers, as seen in (Wang et al., 2023b; Li et al., 2023; Jiang et al., 2023; Gao et al., 2024; Zhang et al., 2024; Mahan et al., 2024), or that utilize LLMs selfevaluation capabilities to enhance task-solving performance (Putta et al., 2024; Yao et al., 2023; Hao et al., 2023). These approaches often rely on the LLMs self-assessment (which can be unreliable), or distill on stable supervision signals like human annotations or guidance from stronger LLMs. In contrast, NLRLs training of language value function offers novel unsupervised approach: we can train generative evaluator solely by aggregating language feedback from the environment, without distilling any strong modelexperimental results in Sec. 5.2 show that our language value functions performance is much better than any LLMs we utilize during the training. Successor Features. Successor features (Dayan, 1993; Barreto et al., 2017) aim to learn highdimensional value function representation, where each component captures the expected future occurrence of all state representations under fixed policy. As shown in (Barreto et al., 2017), successor features help decouple the environment dynamics from the rewards, facilitating transfer learning in RL. The language value function in NLRL is conceptually similar to successor features since it also represents state values in high-dimensional space and satisfies the language Bellman equation."
        },
        {
            "title": "7 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "content": "In this work, we propose new paradigm NLRL, which reformulates the RL process with natural language representation. We believe NLRL opens new door for understanding and implementing RL algorithms in natural language space, improving agents effectiveness, efficiency and interpretability. NLRL is also compatible with the current language model and LLM-based agent. It serves potential way for generating high-quality language synthetic data for policy and critic, which could be crucial in training more advanced language models and agents. Limitations. Currently, NLRL still faces several limitations. First, our experiments demonstrate its effectiveness primarily in environments with discrete action spaces and low-dimensional (textualisable) states. Its performance in environments involving continuous action spaces and highdimensional states (e.g., low-level robotic control tasks) remains largely unexplored. Second, our experiments are primarily conducted in small-scale settings due to the substantial computational cost associated with invoking large language models. The time efficiency of the current NLRL approach is significantly lower than that of traditional RL methods using smaller networks. Future work. Given NLRLs potential, there are several promising directions for future research. First, theoretical foundation for the entire framework in the language space is needed to move beyond analogical reasoning towards more robust, principled basis. This would help formalise the framework and enhance its generalisability. Another important direction is exploring how NLRL can be integrated more organically with existing research on self-evaluation, self-improvement, LLM agent framework, and LLMs planning. Such integration could unlock new capabilities and efficiency improvements. Additionally, in terms of application domains, there is significant potential to extend NLRL beyond the current setupswe are actively exploring extending NLRL to general domains beyond games, such as reasoning tasks (Cobbe et al., 2021) and code (Chen et al., 2021)."
        },
        {
            "title": "8 ACKNOWLEDGEMENT",
            "content": "We thank Ziyan Wang and Yali Du for their efforts in the early-stage research of this paper. 12 Preprint. Work in progress."
        },
        {
            "title": "REFERENCES",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What arXiv preprint matters in on-policy reinforcement learning? arXiv:2006.05990, 2020. large-scale empirical study. Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, Hado van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. Andrew Barto, Richard Sutton, and Charles Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834846, 1983. Richard Bellman, Robert Kalaba, et al. Dynamic programming and modern control theory, volume 81. Citeseer, 1965. Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Manon Devin, Alex Lee, Maria Bauza Villalonga, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: self-improving generalist agent for robotic manipulation. Transactions on Machine Learning Research, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language feedback. arXiv preprint arXiv:2312.06853, 2023. Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms. arXiv preprint arXiv:2406.16218, 2024. Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. Pangu-agent: fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Devleena Das, Sonia Chernova, and Been Kim. State2explanation: Concept-based explanations to benefit agent learning and user understanding. arXiv preprint arXiv:2309.12482, 2023. Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural computation, 5(4):613624, 1993. 13 Preprint. Work in progress. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. Chessgpt: Bridging policy learning and language modeling. arXiv preprint arXiv:2306.09200, 2023a. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. AlphazeroarXiv preprint like tree-search can guide large language model decoding and training. arXiv:2309.17179, 2023b. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. Llm-based nlg evaluation: Current status and challenges. arXiv preprint arXiv:2402.01383, 2024. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Bradley Hayes and Julie Shah. Improving robot controller transparency through autonomous policy explanation. In Proceedings of the 2017 ACM/IEEE international conference on humanrobot interaction, pp. 303312, 2017. Ying Ji, Yu Wang, and Jien Kato. Spatial-temporal concept based explanation of 3d convnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1544415453, 2023. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. TigerarXiv preprint score: Towards building explainable metric for all text generation tasks. arXiv:2310.00752, 2023. Kim Jeong Ju. gym-tictactoe: tic tac toe environment for openai gym, 2024. URL https: //github.com/haje01/gym-tictactoe. GitHub repository. Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282293. Springer, 2006. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023a. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023b. Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, Janos Kramar, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. OpenSpiel: framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019. URL http://arxiv.org/abs/1908.09453. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. 14 Preprint. Work in progress. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch via selfplanning. arXiv preprint arXiv:2401.05268, 2024. Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. VisionarXiv preprint language models are zero-shot reward models for reinforcement learning. arXiv:2310.12921, 2023. Abdallah Saffidine, Nicolas Jouandeau, and Tristan Cazenave. Solving breakthrough with race patterns and job-level proof number search. In Advances in Computer Games: 13th International Conference, ACG 2011, Tilburg, The Netherlands, November 20-22, 2011, Revised Selected Papers 13, pp. 196207. Springer, 2012. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, and Subbarao Kambhampati. Bridging the gap: Providing post-hoc symbolic explanations for sequential decision-making problems with inscrutable representations. arXiv preprint arXiv:2002.01080, 2020. Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:944, 1988. Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023b. 15 Preprint. Work in progress. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022b. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. Large language models for generative information extraction: survey. arXiv preprint arXiv:2312.17617, 2023. Jie Xu, Hanbo Zhang, Xinghang Li, Huaping Liu, Xuguang Lan, and Tao Kong. Sinvig: selfevolving interactive visual agent for human-robot interaction. arXiv preprint arXiv:2402.11792, 2024. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh arXiv preprint Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv:2408.15240, 2024. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848, 2023. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. Victor Zhong, Dipendra Misra, Xingdi Yuan, and Marc-Alexandre Cˆote. Policy improvement using language feedback models. arXiv preprint arXiv:2402.07876, 2024. Preprint. Work in progress."
        },
        {
            "title": "A Algorithms",
            "content": "A.1 Language GPI by prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Train natural language language value function with language TD . . . . . . . . . A.3 Natural language actor critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Experimental details",
            "content": "B.1 Maze Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.1 Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.2 Language GPI Visualization . . . . . . . . . . . . . . . . . . . . . . . . . B.1.3 The effect of Language TD on the Value function . . . . . . . . . . . . . . B.2 Breakthrough Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Language TD Visualization . . . . . . . . . . . . . . . . . . . . . . . . . B.2.3 Comparison of language value function . . . . . . . . . . . . . . . . . . . B.3 TicTacToe Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.1 Training Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.2 Algorithm Implementation Details . . . . . . . . . . . . . . . . . . . . . . B.3.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.4 Baseline Implementation Details . . . . . . . . . . . . . . . . . . . . . . . B.3.5 Additional Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Prompts",
            "content": "C.1 Maze experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Breakthrough Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 TicTacToe Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Analogy",
            "content": "D.1 Language MC . D.2 Language TD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 18 20 20 20 20 23 24 25 29 30 31 32 42 42 43 43 46 49 49 50 17 Preprint. Work in progress."
        },
        {
            "title": "A ALGORITHMS",
            "content": "In this section, we provide the full pseudo-code for three cases mentioned in Sec. 4.2, 4.3, and 4.4. A.1 LANGUAGE GPI BY PROMPTING Algorithm 1 Language GPI by Prompting Require: Initial language policy π, language value function π , LLM implementing operators G1, G2, policy improvement operator I, number of lookahead steps , number of variations // Language TD estimation for = 1 to do 1: for each state st do 2: 3: 4: 5: Sample action a(i) Simulate steps ahead to get trajectory τ (i) Prompt LLM to evaluate Generate intermediate description d(i) = d(a(i) π (s(i) Compute value estimate (i) = G2(d(i), π (s(i) t+N ) , s(i) t+1) , r(i) t+N )) 6: 7: π (st) = G1({V (i)}K i=1) 8: 9: 10: 11: 12: 13: end for end for Aggregate value estimates // Policy improvement Use π (st) and prompt LLM as operator to select the optimal action at (①) (②) (③) (③) (⑤) A.2 TRAIN NATURAL LANGUAGE LANGUAGE VALUE FUNCTION WITH LANGUAGE TD Algorithm 2 Train natural language language value function with language TD Require: Pre-defined policy π, LLM-based language value model π , language descriptor d, language aggregator G1, G2, state buffer Ds, number of iterations , number of trajectories , number of MC trajectories 1: // Collect few-step rollout data using fixed policy for TD data buffer 2: Initialize TD data buffer {} 3: for state st in state buffer Ds do 4: Starting from st, run policy π for few l-step look-ahead rollouts and collect distinct variations: Vst = {(st+i, at+i, rt+i, st+i+1)l1 i=0}K Vst 5: 6: end for 7: // Language TD learning 8: for iteration = 1 to do // Language TD estimate ②, ③ 9: 10: DV {} {Value training data} 11: 12: for each look-ahead data (st, Vst) in do 13: 14: 15: 16: 17: 18: end for end for // Update value function ④ Train For each variation n, generate the final state evaluation describe intermediate transitions dn = d(st, an , rn (cid:17) π (st)new G1 DV DV (cid:16)(cid:8)G2 π (st)new t+1)(cid:1)(cid:9)K (cid:0)dn, π (sn n=1 π (sn t+1, ..., sn t+l) with t+l1, an , sn {l-step, k-variation Language TD} π , and utilize to t+l1) t+l1, rn π using language TD estimate dataset DV with supervised loss 18 Preprint. Work in progress. A.3 NATURAL LANGUAGE ACTOR CRITIC Algorithm 3 Natural Language Actor-Critic Learning Require: Initial language policy model πL, language value model evaluation G1/G1,G2, improvement operator π , large language model for j=0} if using MC estimate then else if using TD estimate then for each state-action pair (s, a) in τ do k=1 ) {Language MC estimate} end for // Language value estimation ②, ③ Sample KM trajectories starting from (s, a): {τk}KM k=1 G1({τk}KM // Collect trajectories using language policy ① τ {} {Initialize trajectory buffer} for = 1 to do Run policy πL to collect trajectory: τi {(sj, aj, rj, sj+1)Hi τ τ {τi} Require: Number of iterations , number of trajectories , number of MC trajectories 1: Initialize replay buffer history {} 2: for iteration = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: DV {} {Value training data} 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: Dπ {} {Policy training data} for each state in τ do 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: end for Sample actions {ai}Nsample Select top-m actions Ak based on sampling frequency Query value estimates: QL {V πnew, I(QL, TL) {Language improvement operator} Dπ Dπ {(s, πnew, c)} end for // Update value function ④ {DV } {Add to buffer history} Train // Language policy improvement ⑤ Sample KT transitions starting from (s, a): {(sk, ak, rk, G1({G2(d(sk, ak, rk, end for // Update policy ⑥ Train πL on Dπ with supervised loss π on merged data from last Kbuf er buffers in with supervised loss end if DV DV {(s, a, v)} k=1 ) {Language TD estimate} π (s, a)a Ak} from πL(s) k))}KT k)}KT k=1 k), π (s i= 19 Preprint. Work in progress."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 MAZE EXPERIMENT B.1.1 EXPERIMENT DETAILS Our Maze experiment is conducted using LMRL Gym (Abdulhai et al., 2023). We use the original codebase for the environment. We consider the fully-observable setting, where the agents observation (described by text) includes the agents current position in the maze, the agents action history, the walls position around the agent (if any), and the goal position. The action space is discrete, including moving up / down / right / left. We evaluate the performance on 30 different initial positions, each with 3 random seeds. We prompt gpt-4o-mini-2024-07-18 (temperature = 0.7) as the language aggregator G1, language state-action value aggregator G2, and language state value function π respectively. B.1.2 LANGUAGE GPI VISUALIZATION We present the language GPIs prompt and response visualization for language state value function π and language state-action value aggregator G2 in the LMRL Maze experiment."
        },
        {
            "title": "Maze prompts",
            "content": "Role SYSTEM USER (V π ) You only respond in Content You are an expert maze solver. json. You are playing simple maze game. An agent is moving in the maze and the objective is to reach the goal in as few steps as possible. The possible actions are \"move up\", \"move down\", \"move left\", \"move right\". You are highly skilled evaluator in this game and is expected to function similar to state value function in reinforcement learning. At each step will provide you with the move history of the agent (from old to new), including where the goal is, your current position, the walls that surround the agent. Your output evaluation should be json array that includes the following concepts: - \"thoughts\": Lets think step by step. detailed thought process and evaluation of the agents position and the distance towards the goal. - \"final evaluation\": agents current position is compared to all the possible positions in the maze, in terms of reaching the goal. Lets start new game. Now, please give your evaluation of the current state given the move history of the agent: The goal is at position 8, 6. Your current position is at position 5, 7. you. move up The goal is at position 8, 6. Your current position is at position 5, 7. you. move down The goal is at position 8, 6. Your current position is at position 5, 7. you. move right There are walls above you, below There are walls above you, below There are walls above you, below Concisely judge how good the Generate your 20 Preprint. Work in progress. ASSISTANT There are walls above you, below [\"The goal is located at (8, 6) and the The goal is at position 8, 6. Your current position is at position 5, 8. you. move right The goal is at position 8, 6. Your current position is at position 5, 9. There are walls to your right, below you. thoughts: agents current position is (5, 9).\", To evaluate the distance to the goal, will calculate the Manhattan distance, which is the sum of the absolute differences of the coordinates: 6., \"The agents current position (5, 9) has walls directly below (6, 9) and to the right (5, 10). This limits movement options.\", The possible actions for the agent are to move left to (5, 8) or move up to (4, 9). walls., Moving left to (5, 8) would bring the agent back to previous position, while moving up to (4, 9) opens potential new pathways., Overall, the agent is relatively far from the goal, with limited movement options due to surrounding walls.], final evaluation: 9) is not ideal as it is 6 steps away from the goal and has limited movement options. Moving up to (4, 9) is the best immediate option to explore new paths.\" Moving down or right is not possible due to \"The agents current position (5, 8 - 5 + 6 - 9 = 3 + 3 = 21 Preprint. Work in progress. Role SYSTEM USER (G2) ASSISTANT Your You only respond in Lets think step by step. Now Concisely judge how good the Your output evaluation should be json Content You are an expert maze solver. json. You are playing simple maze game. An agent is moving in the maze and the objective is to reach the goal in as few steps as possible. The possible actions are \"move up\", \"move down\", \"move left\", \"move right\". You are highly skilled evaluator in this game, particularly adept at making accurate assessments through look-ahead of the current maze position after taking the given action. At each step will provide you with the move history of the agent (from old to new), including where the goal is, your current position, the walls that surround the agent, the action that the agent is going to take, *along with several key variations of trajectory pieces after taking this action (and the corresponding natural language evaluations of the trajectory pieces)*. task is to understand these look-ahead information and summarize, derive non-trivial analysis and understanding the *the agents position after taking the action*. array with the following *two* concepts: Summarize the \"thoughts\": look-ahead information of the variations after taking action move down. \"final evaluation\": chosen action is, in terms of reaching the goal. Now, please give your evaluation given action move down, the *current environment state*: The goal is at position 8, 6. at position 5, 7. and the look-ahead information of different variations after taking action move down: AGGREGATION OF VALUE ESTIMATES FOR ALL VARIATIONS thoughts: is three rows down and one column to the left of the current position (5, 7).\", \"The agent attempts to move up from (5, 7) but cannot because there is wall above.\", \"Each variation shows the agent getting stuck in loop of ineffective moves after trying to move up, consistently returning to (5, 7) or moving to (5, 9), which is still not closer to the goal.\", \"The walls surrounding the agent restrict movement, making it impossible to progress toward the goal efficiently.\", \"In all variations, the agent finds itself either returning to previous positions or moving to areas blocked by walls.\", \"The agents progression towards the goal becomes increasingly limited as it cannot move up, down, or directly toward the goal from its current position.\"], final evaluation: \"goal position\": \"possible moves\": moves\": chosen action move upıs ineffective as it does not change the agents position, and the surrounding walls create dead end, making it impossible to reach the goal efficiently.\"}\" Your current position is There are walls above you, below you [\"move down\", \"move left\"], \"blocked \"The [\"move up\", \"move right\"], \"evaluation\": [\"The goal is located at (8, 6), which \"(8, 6)\", \"manhattan distance\": \"{ \"current position\": \"(5, 7)\", 6, 22 Preprint. Work in progress. B.1.3 THE EFFECT OF LANGUAGE TD ON THE VALUE FUNCTION Below we show comparison of the Language state-action values with/without Language TD. Comparing the two Language State-action Value, we can see that by analyzing various potential outcomes, Language TD identifies both immediate and future benefits or drawbacks of actions, which is reflected in its more detailed thoughts and final evaluation. The inclusion of varied trajectory information allows Language TD to provide more balanced and informed assessment, mitigating over-reliance on immediate next-step evaluations. Language State-action Value (Language TD) For action \"move down\", {\"thoughts\": [\"The agents current position is (5, 7) with the goal at (8, 6).\", \"After attempting to move down, the agent remains at (5, 7) due to walls above and below.\", \"In Variation 1, the agent eventually moves to (6, 6), which is one step away from the goal at (8, 6).\", \"Variation 2 shows that moving down leads to blocked paths, necessitating lateral movement, but ultimately keeps the agent away from the goal.\", \"Variation 3 allows the agent to successfully reach (7, 6), positioning it just one step away from the goal.\", \"Variation 4 also confirms that moving down to (6, 6) is effective, placing the agent two steps away from the goal.\", \"Overall, the evaluations indicate that while the agent faces wall constraints, moving down leads to favorable outcomes that progress toward the goal.\"], \"final_evaluation\": {\"current_position\": \"(5, 7)\", \"distance_to_goal\": 3, \"proposed_action\": \"move down\", \"evaluation\": \"Although the move down is initially blocked, subsequent evaluations indicate that this action can lead to optimal positions in the following moves. The agent can reach the goal within two more steps if it navigates correctly.\"}} Language State-action Value (without Language TD) For action \"move down\", { \"thoughts\": [ \"The agents current position is (5, 7) and the goal is at (8, 6).\", \"The possible action is to move down to (6, 7).\", \"There are walls above (4, 7) and below (6, 7), which means the agent can only move down if it is not blocked.\", \"Moving down to (6, 7) is valid since there is no wall at that position.\", \"After moving down, the new position will be (6, 7).\", \"The distance to the goal can be calculated as follows: the horizontal distance is 8 - 6 = 2 and the vertical distance is 6 - 7 = 1.\", \"The total distance to the goal after moving down would be 2 + 1 = 3.\" ], \"final_evaluation\": { \"current_position\": \"(6, 7)\", \"distance_to_goal\": 3, \"evaluation\": \"The agents position after moving down is better than the previous one, as it moves closer to the goal. There are no walls blocking further movement towards the goal from this position.\" } } 23 Preprint. Work in progress. B.2 BREAKTHROUGH EXPERIMENT B.2.1 EXPERIMENT DETAILS Our breakthrough experiment is conducted with Openspiel (Lanctot et al., 2019) 5x5 breakthrough game and MCTS policy. We slightly modified Openspiels codebase so the 5x5 breakthrough game supports two lines of pieces for each side. For the policy π, as mentioned in the paper, we choose an MCTS policy with uctc = 1, 1000 simulations for search, and 100 rollouts for the random rollout evaluator. For the state dataset, we define the MCTS policies using grid of configurations based on the number of simulations and rollouts. Specifically, we use the following values for these parameters: Simulation numbers: 2, 10, 100, 1000 Rollout numbers: 1, 10, 100, 1000 This results in 4 4 = 16 unique MCTS policies, each characterized by specific combination of simulation and rollout numbers. To generate the mixed state dataset, we pair every possible combination of these 16 policies (including self-pairing), leading to 16 16 = 256 policy pairs. For each pair of policies, we perform the same number of rollouts where both policies interact within the environment. The states encountered during these rollouts are recorded, ensuring that the dataset captures diverse distribution of positions. By merging the data from all policy pairs, we create the final mixed state dataset Ds, which is designed to represent states arising from wide range of gameplay strategies and skill levels. We use vLLM (Kwon et al., 2023b) for any LLM inference used in the experiment, including language value function inference and aggregation inference. We sample 3000 states from the state dataset for evaluation to serve as the Ds test. When computing prediction accuracy, we only count state evaluation which can pass our rule-based parser to extract the advantageous side judgment. Here we present the hyperparameters used in our experiment:"
        },
        {
            "title": "Value",
            "content": "192 4 4 True Table 4: Rollout Parameters"
        },
        {
            "title": "Temperature\nTop K\nTop P\nMax Tokens",
            "content": "1.0 50 0.95 512 Table 5: LLM sampling parameters for prompting."
        },
        {
            "title": "Training",
            "content": "Preprint. Work in progress."
        },
        {
            "title": "Value",
            "content": "1024 0.03 2e-5 Constant bfloat16 4 8 2 4 FSDP Table 6: Data Collection Parameters Evaluation. We use the same LLM sampling parameters as the prompting process. B.2.2 LANGUAGE TD VISUALIZATION Here we present an example of how language TD works for query and response. Language State-action Value (Before) Role SYSTEM Content Here is the rule for the Breakthrough board game: The game is played on an 5x5 board for 2 players (white and black), with each player starting with 10 pawns. white pawns are on the first two rows and black pawns are on the last two rows. Black moves first. In each turn, players can move one of their pieces one square forward, diagonally forward if the target square is empty. opponents piece if that square is one step diagonally forward. The game ends when one player successfully break through the opponent lines -- either move piece to the opposite last row of the board or captures all of the opponents pieces. Or it can capture an For board representation, we use for black pieces, for white pieces, and . for empty squares. (1-5) and (a-e) are used to show the rows and columns index respectively. You are highly skilled evaluator in this game, particularly adept at making accurate assessments through look-ahead analysis of the current board position. will provide you with current board representation, *along with several key variations starting from this position (and their corresponding natural language evaluations of the subsequent positions)*. Your task is to aggregate and compare these look-ahead information, to summarize, derive non-trivial analysis about the *current board*. Your output should include the following concepts: Any immediate threats, 1. potential tactics, or vulnerabilities in the position. *Tactical Considerations*: 25 Preprint. Work in progress. Consideration of pawn *Suggested Moves*: *Positional Evaluation*: 2. structure, piece activity, control of key squares, and game safety. 3. moves for the side to move, along with brief rationale for comparing different moves. 4. determine if white or black takes advantage. Use <white> or <black> to represent the winning side. Based on all previous rationale, One or two strong candidate *Advantage*: USER Your response should be informative and concise. \"*The board you need to evaluate:* 5bb.b. 4b..b. 3..bw. 2w.w.. 1wwwww abcde It is Whites turn. White pieces are at: Black pieces are at: d3, a2, c2, a1, b1, c1, d1, e1. a5, b5, d5, a4, d4, c3. Here are the look-ahead variations from the current board position: *Key Variations and Subsequent Evaluation:*: *Variation 1:* Description of variations move sequence: The action sequence is: d3e4,d5e4*,a2b3,a4b3*. Move 1:White moves piece from d3 (Column d, Row 3) to e4 (Column e, Row 4). Move 2:Black moves piece from d5 (Column d, Row 5) to e4 (Column e, Row 4), capturing White piece. Move 3:White moves piece from a2 (Column a, Row 2) to b3 (Column b, Row 3). Move 4:Black moves piece from a4 (Column a, Row 4) to b3 (Column b, Row 3), capturing White piece. Subsequent position evaluation: The subsequent board is: 5bb... 4...bb 3.bb.. 2..w.. 1wwwww abcde It is Whites turn. White pieces are at: Black pieces are at: c2, a1, b1, c1, d1, e1. a5, b5, d4, e4, b3, c3. The evaluation of this subsequent board is: **Current Board Analysis:** **Tactical Considerations:** 26 Preprint. Work in progress. Whites most significant tactical consideration is the potential to capture Blacks pieces on d4 and e4, which would gain significant advantage. Whites pieces are well-positioned to launch decisive attack. Blacks pieces on a5 and b5 are somewhat isolated and vulnerable to potential attacks. **Positional Evaluation:** The current position slightly favors White due to their piece activity, control of key squares, and potential to create strong pawn center. Whites pawns on a1 and d1 provide potential support for the central pawns. Blacks pawns are somewhat isolated, but they still have chance to reorganize. **Suggested Moves:** Based on the look-ahead analysis, two strong candidate moves for White are: **e1-d2**: This move captures Blacks potential 1. piece on d2 (in case Black plays d4-d2) and creates strong initiative. 2. **c2-b3**: prepares to break through to the opposite side, gaining strong initiative. This move attacks Blacks position and **Advantage:** <white> Overall, White has slight advantage in this position, with multiple ways to break through Blacks lines and gain significant advantage. However, Blacks position is not yet critical, and they can still reorganize their pieces to defend. Whites advantage is mainly due to their piece activity, control of key squares, and potential to create strong pawn center. *Variation 2:* Description of variations move sequence: The action sequence is: c1b2,c3d2,e1d2*,d4e3. Move 1:White moves piece from c1 (Column c, Row 1) to b2 (Column b, Row 2). Move 2:Black moves piece from c3 (Column c, Row 3) to d2 (Column d, Row 2). Move 3:White moves piece from e1 (Column e, Row 1) to d2 (Column d, Row 2), capturing Black piece. Move 4:Black moves piece from d4 (Column d, Row 4) to e3 (Column e, Row 3). Subsequent position evaluation: The subsequent board is: 5bb.b. 4b.... 3...wb 2wwww. 1ww.w. abcde It is Whites turn. White pieces are at: Black pieces are at: d3, a2, b2, c2, d2, a1, b1, d1. a5, b5, d5, a4, e3. 27 Preprint. Work in progress. The evaluation of this subsequent board is: **Tactical Considerations:** The current position presents strong opportunity for White to create breakthrough on the file, as demonstrated by the two variations. structure is somewhat disjointed, making it difficult to respond to Whites advances on the and files. Blacks pawn **Positional Evaluation:** Whites pawn structure is more compact and cohesive, controlling key central squares. The pawns on and files provide additional support for the potential breakthrough on and e. and d5 are somewhat isolated, making it difficult to coordinate defense. Blacks pieces on a5 **Suggested Moves:** Considering the look-ahead variations, the strongest candidate move for White is **d3d4**. pressure on Blacks position and sets up the potential breakthrough on the and files. for this move is that it creates an immediate threat and forces Black to respond, potentially weakening their position. This move puts The rationale Another possible move could be **d2d3**, but it is less aggressive and does not put as much pressure on Black. **Advantage:** Based on the analysis, the advantage is **<white>**. The two variations demonstrate clear path to victory for White, and Blacks position appears to be under significant pressure. Please provide your analysis and understanding of the current board position based on the provided look-ahead information. Your response should be informative and concise. \"**Current Board Analysis:** ASSISTANT **Tactical Considerations:** The primary tactical consideration for White is to utilize the pawns on the file to create breakthrough, potentially capturing Blacks pieces on d4 and e4. Additionally, White should focus on controlling the key squares and putting pressure on Blacks somewhat isolated pieces on a5 and b5. needs to reorganize their pawns and pieces to defend against Whites central pawn advance. Black **Positional Evaluation:** The position slightly favors White due to their more compact and cohesive pawn structure, piece activity, and control of key central squares. Whites pawns on the and files provide additional support for the potential breakthrough on the and files. it challenging to coordinate defense. Blacks pawns are somewhat isolated, making **Suggested Moves:** Preprint. Work in progress. Based on the look-ahead analysis, two strong candidate moves for White are: **d3d4**: This move puts pressure on Blacks 1. position, sets up potential breakthrough on the and files, and forces Black to respond. 2. piece on d2 and creates strong initiative. This move captures Blacks potential **e1-d2**: **Advantage:** <white> Overall, White has slight advantage in this position, with multiple ways to break through Blacks lines and gain significant advantage. The two variations demonstrate clear path to victory for White, and Blacks position appears to be under significant pressure. piece activity, control of key squares, and potential to create strong pawn center. Whites advantage is mainly due to their The provided look-ahead information suggests that White should prioritize controlling the central squares, putting pressure on Blacks position, and utilizing the pawns on the file to create breakthrough.\" B.2.3 COMPARISON OF LANGUAGE VALUE FUNCTION Here we present another visualization comparison between the initial LLaMA-3.1-8B-Instruct Model and our trained model. The evaluation is conducted on the board 5..b.b 4.bb.. 3.w... 2.w... 1ww..w abcde, where white takes large advantage in this place. By our language TD training, the language value function can accurately evaluate this board and generate insightful annotation, largely decreasing the hallucination. Language State Value (Before) Board: 5..b.b 4.bb.. 3.w... 2.w... 1ww..w abcde, white turn Based on the current board position, heres the evaluation for Whites turn: *Tactical Considerations:* - Black has an opportunity to attack Whites pawn at e1 with pawn from e5 or c5, which might put White in difficult position. - Whites pawn at b3 and b2 is target for Blacks pawns at b4 and c4. - White needs to focus on safely advancing their pawns and creating barrier for Blacks pieces. *Positional Evaluation:* - White has relatively weak pawn structure, with some pawns on the first row. - The center is somewhat controlled by White, but Black has solid presence on the fifth row. - Black has strategic advantage in pawn structure, but White has the opportunity to influence the game with proper planning. *Suggested Moves:* 1. Move the pawn from b2 to b3. This move has the goal of preparing to defend and possibly create barrier against Blacks pieces. Preprint. Work in progress. 2. Move the pawn from a1 to a2. This move aims to create more space and put pressure on Blacks pieces, which will make it harder for them to maneuver. *Advantage:* Based on the current board position, <black> is favored. Language State Value (After training) Board: 5..b.b 4.bb.. 3.w... 2.w... 1ww..w abcde, white turn **Analysis and Evaluation of the Current Board Position** **Tactical Considerations:** The key variation shows an immediate threat of White breaking through Blacks lines. Whites piece on b3 is strategically positioned to capture Blacks piece on c4, which creates an opportunity for decisive push. **Positional Evaluation:** White has significant advantage in terms of piece activity and control of key squares. With pieces on the third and second rows, White is poised to push forward and break through Blacks lines. Blacks pieces are concentrated on the fifth and fourth rows, which limits their mobility and ability to respond to Whites attack. **Suggested Moves:** Two strong candidate moves for White are: 1. b3-c4*: This move captures Blacks piece and creates weakness in Blacks position, allowing White to potentially break through. 2. b2-c3: This move develops another White piece and prepares to support the potential breakthrough on the c-file. However, given the look-ahead variations, the most promising move is b3-c4*, as it leads to terminal state where White wins. **Advantage:** Based on the analysis, White has decisive advantage. The ability to capture Blacks piece on c4 and break through Blacks lines gives White strong initiative and clear path to victory. Therefore, <white> is the winning side. B.3 TICTACTOE EXPERIMENT Our experiments are conducted on the Tic-tac-toe environment (Ju, 2024), standard twoplayer game environment that provides clear testbed for evaluating sequential decisionmaking. The implementation uses several key components: https://github.com/haje01/ gym-tictactoe for the Tic-tac-toe environment, FSDP (Fully Sharded Data Parallel) for distributed model training, vLLM (Kwon et al., 2023b) for efficient language model inference, and custom parallel framework for trajectory collection. The environment is wrapped with parallel execution layer to enable efficient batch processing of multiple games simultaneously. For policy evaluation, we employ Monte Carlo sampling to estimate state-action values, requiring complete game trajectories until terminal states. Policy improvement utilizes structured sampling approach with temperature-controlled exploration and frequency-based action selection. To manage computational resources efficiently, we employ gradient checkpointing and use the FSDP strategy across our GPU cluster. This configuration allows us to train larger language models while maintaining reasonable memory requirements and training speed. 30 Preprint. Work in progress. The parallel data collection framework is designed to maximize GPU utilization during training. It maintains queue of active environments and processes state-action transitions in batches, significantly reducing the overall training time compared to sequential execution. B.3.1 TRAINING INFRASTRUCTURE"
        },
        {
            "title": "Data Collection Pipeline",
            "content": "For each training iteration, we collect data using the following configuration:"
        },
        {
            "title": "Parameter",
            "content": "Parallel Environments Trajectories per Iteration Monte Carlo Samples (KM C) Policy Samples per State (Nsample) Top-k Actions"
        },
        {
            "title": "Value",
            "content": "64 512 5 10 10 Table 8: Data Collection Parameters"
        },
        {
            "title": "Model Architecture and Training",
            "content": "Both policy and value networks are trained from LLaMA-3.1-8B-Instruct using the following parameters:"
        },
        {
            "title": "Learning Rate\nLearning Rate Schedule\nTraining Epochs per Iteration\nFSDP Configuration\nGradient Checkpointing\nBatch Size\nMax Sequence Length\nTraining Hardware",
            "content": "1e-5 Constant 2 Full Sharding Enabled 8 1024 4 H100 GPUs Table 9: Model Training Hyperparameters"
        },
        {
            "title": "Value Function Buffer Management",
            "content": "To prevent catastrophic forgetting, we maintain experience buffers with the following configuration:"
        },
        {
            "title": "Value",
            "content": "Buffer History Length (Kbuf er) Merging Strategy Buffer Content 3 iterations Equal sampling State-action pairs with MC estimates Table 10: Buffer Management Configuration B.3.2 ALGORITHM IMPLEMENTATION DETAILS"
        },
        {
            "title": "Policy Evaluation Details",
            "content": "For Monte Carlo evaluation, we use the following configuration: 31 Preprint. Work in progress."
        },
        {
            "title": "Value",
            "content": "MC Trajectories per State-Action Trajectory Completion Value Aggregation Sampling Temperature Action Space Size"
        },
        {
            "title": "5\nFull game\nAverage over returns\n0.7\n9 positions (0-8)",
            "content": "Table 11: Policy Evaluation Configuration B.3.3 VISUALIZATION Below we show comparison of the Language state-action values before and after Language MC estimation. The evaluation is conducted on the following board: 3 4 7 9 Through analyzing multiple complete game trajectories, Language MC helps the value function develop comprehensive understanding of long-term consequences of actions, rather than just immediate rewards. This is particularly evident in the enhanced evaluation quality after MC estimation, where the model provides more strategic insights and nuanced assessment of game positions. Language State-action Value (Before) Role SYSTEM The You are learning Your answer consists of two elements: Lets think step by step. Generate your Content You are player of the game of Tic Tac Toe. The game goal is to get multiple of ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. The board consists of \"O\", \"X\" and grid numbers. grid number indicates an empty grid. how to evaluate (board, action) pair in the Tic Tac Toe game by playing the game given the (board, action) pair and reflecting on the playing history. The playing history depicts game in progress with final result. - \"thought\": detailed evaluation over the (board, action) pair by merely reflecting the playing history after this pair from different perspectives. You should only rely on the playing history as context and shouldnt evaluate the game with your own judgment. Your evaluation should contain the following elements: probability, Threat, and Potential strategies. - \"final evaluation\": judge which side takes advantage. 1 means you think takes advantage, -1 means you think takes advantage, and 0 means you think the game will be draw. You should output your answer in the JSON format. EXAMPLE: user: Board: --------- 4 6 --------- The (board, action) to evaluate is Os turn: After all of your thoughts, Win 32 Preprint. Work in progress. wins. The Os move is 4. wins by occupying the \"It appears that the initial board 7 8 9 Action: Below is the rollout sequence after this (board, action): After Os move 4, the board position is: --------- 6 --------- 7 8 9 After Xs move 7, the board position is: --------- 6 --------- 8 9 The game is over. positions 3, 5, 7. assistant: { \"thought\": { \"Reflection\": position --------- 4 6 --------- 7 8 9 and action 4 were not favorable for O, as was able to block on Os moves and ultimately win the game.\", \"Win probability\": \"The win probability for is large, while the win probability for is low.\", \"Threat\": win by playing corner 7. and create potential winning opportunity.\", \"Potential strategies\": \"Potential strategies for include playing the corner 7 to block as opposite corner and win by occupying 1, 4, 7. occupied 3, 5, 7 to win the game. has already occupied 3, 5, and there is 1 step to complete the plan.\" }, \"final evaluation\": } The board to evaluate is Os turn: Board: 3 --------- 4 6 --------- 7 9 Action: Below is the rollout sequence 1 after this (board, action): After taking action 3, the board position is: --------- 4 6 --------- 7 9. After taking action 4, the board position is: --------- \"X has played center 5 and corner 3. can was able to occupy 1, 4, 7 The Os move is 3. could have -0.9 33 USER Preprint. Work in progress. ASSISTANT wins. wins by occupying the \"The initial board position 6 --------- 7 9. After taking action 7, the board position is: --------- 6 --------- 9. The game is over. positions [3, 5, 7]. Now generate your evaluation for the (board, action) pair. { \"thought\": { \"Reflection\": 3 --------- 4 6 --------- 7 9 and action 3 were favorable for O, as was able to occupy strategic position and ultimately win the game.\", \"Win probability\": while the win probability for is low.\", \"Threat\": can win by playing corner 7. 5s opposite corner 9 but failed to block Os winning line.\", \"Potential strategies\": \"Potential strategies for include playing the corner 7 to win by occupying 3, 5, 7. could have occupied 4, 6, 9 to block Os winning line but failed to do so.\" }, \"final evaluation\": } \"O has played center 5 and corner 3. has played center 0. \"The win probability for is high, Language State-action Value (After) Role SYSTEM Content You are player of the game of Tic Tac Toe. The game goal is to get multiple of ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. The board consists of \"O\", \"X\" and grid numbers. grid number indicates an empty grid. how to evaluate (board, action) pair in the Tic Tac Toe game by playing the game given the (board, action) pair and reflecting on the playing history. The playing history depicts game in progress with final result. - \"thought\": detailed evaluation over the (board, action) pair by merely reflecting the playing history after this pair from different perspectives. You should only rely on the playing history as context and dont evaluate the game with your own judgment. contain the following elements: Win probability, Threat, and Potential strategies. Your answer consists of two elements: lets think step by step. Generate your Your evaluation should You are learning The 34 Preprint. Work in progress. wins. The Os move is 4. After all of your thoughts, The (board, action) to evaluate is Os turn: - \"final evaluation\": judge which side takes advantage. 1 means you think takes advantage, -1 means you think takes advantage, and 0 means you think the game will be draw. You should output your answer in the JSON format. EXAMPLE: user: Board: --------- 4 6 --------- 7 8 9 Action: Below is the rollout sequence after this (board, action): After Os move 4, the board position is: --------- 6 --------- 7 8 9 After Xs move 7, the board position is: --------- 6 --------- 8 9 The game is over. positions 3, 5, 7. assistant: { \"thought\": { \"Reflection\": position --------- 4 6 --------- 7 8 9 and action 4 were not favorable for O, as was able to block on Os moves and ultimately win the game.\", \"Win probability\": \"The win probability for is large, while the win probability for is low.\", \"Threat\": win by playing corner 7. and create potential winning opportunity.\", \"Potential strategies\": \"Potential strategies for include playing the corner 7 to block as opposite corner and win by occupying 1, 4, 7. occupied 3, 5, 7 to win the game. has already occupied 3, 5, and there is 1 step to complete the plan.\" }, \"final evaluation\": } The board to evaluate is Os turn: Board: 3 --------- 4 6 --------- 7 9 \"X has played center 5 and corner 3. can was able to occupy 1, 4, 7 \"It appears that the initial board wins by occupying the could have -0.9 35 USER Preprint. Work in progress. Action: The Os move is 9. Below is the rollout sequence 1 after this (board, action): After taking action 9, the board position is: 3 --------- 4 6 --------- 7 O. The game is over. positions [1, 5, 9]. wins by occupying the wins. Below is the rollout sequence 2 after this (board, action): After taking action 9, the board position is: 3 --------- 4 6 --------- 7 O. The game is over. positions [1, 5, 9]. wins by occupying the wins. Below is the rollout sequence 3 after this (board, action): After taking action 9, the board position is: 3 --------- 4 6 --------- 7 O. The game is over. positions [1, 5, 9]. wins by occupying the wins. Below is the rollout sequence 4 after this (board, action): After taking action 9, the board position is: 3 --------- 4 6 --------- 7 O. The game is over. positions [1, 5, 9]. wins by occupying the wins. Below is the rollout sequence 5 after this (board, action): After taking action 9, the board position is: 3 --------- 4 6 --------- 7 O. The game is over. positions [1, 5, 9]. wins by occupying the wins. ASSISTANT Now generate your evaluation for the (board, action) pair. { \"thought\": { \"Reflection\": position \"It appears that the initial board 36 Preprint. Work in progress. \"The win probability for is 3 --------- 4 6 --------- 7 9 and action 9 were highly favorable for O, as was able to win the game immediately after taking action 9 in all rollout sequences.\", \"Win probability\": extremely high, while the win probability for is extremely low.\", \"Threat\": the rollout sequences, as was able to win the game immediately after taking action 9.\", \"Potential strategies\": \"The potential strategy for was to occupy the corner 9 to win the game by occupying the positions [1, 5, 9]. in all rollout sequences.\" }, \"final evaluation\": } \"There were no threats from in any of This strategy was successful"
        },
        {
            "title": "Policy Improvement Details",
            "content": "Below we show comparison of the Language Policy Improvement procedure at the beginning of training and at the end. The evaluation is conducted on the following board: 3 4 6 7 9 The evolution in policy quality demonstrates how our framework effectively leverages value function assessments to enhance decision-making. The improved policy shows more sophisticated strategic reasoning, considering not just immediate moves but also their implications for future game states. This transformation is achieved while maintaining our restriction against using the models pretrained game knowledge, instead relying purely on learned evaluation capabilities. Language Policy Improvement (Beginning) Role SYSTEM The goal is to get multiple of Content You are playing the game tic-tac-toe on 3*3 board. Tic Tac Toe is two-player game played on grid. Players take turns marking space with their respective symbol. ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. The board consists of \"O\", \"X\" and grid number. The grid number indicates an empty grid. Your task is to determine the best move for the next player based on the given board position and the next player. The evaluations of (board, action) pairs after possible moves are given. DO NOT judge the board based on your knowledge, only use the evaluations to determine the best move. The evaluation for the next board is in the format of JSON, consisting of two elements: - \"thought\": Evaluation of the board and action pair. 37 Preprint. Work in progress. USER Judge which side takes advantage. \"The initial board position \"The win probability for is high, \"O has played corner 1 and center 5. - \"final evaluation\": 1 means takes advantage, -1 means takes advantage, and 0 means the game will be draw. Here is the board position and the next player is O: Board: 3 --------- 4 6 --------- 7 9. The possible moves are [3, 7, 4, 9]. The following are the boards after each possible move: ### Evaluation for taking action 3: { \"thought\": { \"Reflection\": 3 --------- 4 6 --------- 7 9 and action 3 were favorable for O, as was able to win the game in all rollout sequences.\", \"Win probability\": while the win probability for is low.\", \"Threat\": can win by playing corner 3, 4, 7 or 9. has played corner 2 and 8 but failed to block Os winning lines.\", \"Potential strategies\": \"Potential strategies for include playing the corner 3 to create potential winning opportunity. or 1, 5, 9 or 3, 5, 7 to win the game. blocked Os winning lines by playing 4 or 7 but failed to do so.\" }, \"final evaluation\": } ### Evaluation for taking action 7: { \"thought\": { \"Reflection\": 3 --------- 4 6 --------- 7 9 and action 7 were favorable for O, as was able to win the game in all rollout sequences.\", \"Win probability\": while the win probability for is low.\", \"Threat\": win by playing corner 7 and then 9, or 4 and then 7. has played corner 3 and 8 but failed to block Os winning lines.\", \"Potential strategies\": \"Potential strategies for include playing the corner 7 to create potential winning opportunity and then playing 9 or 4 to win the game. winning lines but failed to do so.\" }, \"final evaluation\": } ### Evaluation for taking action 4: \"O has played corner 1 and center 5. can could have occupied 3, 6, 9 to block Os \"The win probability for is high, could have occupied 1, 4, 7 \"The initial board position could have 0.9 0. 38 Preprint. Work in progress. 0.9 \"The initial board position \"It appears that the initial board \"The win probability for is high, \"O has played center 5 and corner 1. can { \"thought\": { \"Reflection\": 3 --------- 4 6 --------- 7 9 and action 4 were favorable for O, as was able to win the game in all rollout sequences.\", \"Win probability\": while the win probability for is low.\", \"Threat\": win by playing corner 7 or 9. has played center 6 and corner 3 but failed to block Os winning lines.\", \"Potential strategies\": \"Potential strategies for include playing the corner 7 or 9 to win the game. could have occupied 3, 6, 9 to block Os winning lines but failed to do so.\" }, \"final evaluation\": } ### Evaluation for taking action 9: { \"thought\": { \"Reflection\": position 3 --------- 4 6 --------- 7 9 and action 9 were favorable for O, as was able to win the game in both rollout sequences.\", \"Win probability\": while the win probability for is low.\", \"Threat\": can win by playing corner 9. and center 6 is empty, but was not able to block Os moves and ultimately lost the game.\", \"Potential strategies\": \"Potential strategies for include playing the corner 9 to create potential winning opportunity and then playing 3 or 7 to win the game. winning opportunity, but was not able to do so.\" }, \"final evaluation\": } Now, please give your evaluation and the best move for based on the given board position: 3 --------- 4 6 --------- 7 9. You should output your answer in the JSON format. answer consists of two elements: - \"thought\": Lets think step by step. your detailed reflection by analyzing the next board positions and their evaluations. could have occupied 3, 6, 7 to block Os \"O has played center 5 and corner 1. \"The win probability for is high, has played corner 3 Generate Your 0.9 39 Preprint. Work in progress. ASSISTANT The The best move for the next player. \"Analyzing the given board position - \"best move\": move should be in the format of number from 1 to 9, indicating the position on the board. And the move should be in the available positions [3, 7, 4, 9]. Dont output extra information except for the JSON format. { \"thought\": 3 --------- 4 6 --------- 7 9 and the evaluations of the possible moves, we can see that all four moves (3, 7, 4, 9) have the same final evaluation of 0.9, indicating that has high chance of winning with any of these moves. we should consider the potential strategies and threats for each move. Move 3 and 7 allow to create potential winning opportunity by playing the corner, while move 4 allows to play the center and create potential winning opportunity by playing the corner 7 or 9. Move 9 allows to play the corner and create potential winning opportunity by playing 3 or 7. Considering the potential strategies, all four moves seem to have similar advantages. to have slight advantage as it allows to create potential winning opportunity and then play 9 to win the game, while also putting pressure on to block the winning line. Therefore, the best move for is 7.\", \"best move\": } However, move 7 seems However, 7 Language Policy Improvement (End) Role SYSTEM USER The goal is to get multiple of Content You are playing the game tic-tac-toe on 3*3 board. Tic Tac Toe is two-player game played on grid. Players take turns marking space with their respective symbol. ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. The board consists of \"O\", \"X\" and grid numbers. grid number indicates an empty grid. Your task is to determine the best move for the next player based on the given board position and the next player. The evaluations of (board, action) pairs after possible moves are given. DO NOT judge the board based on your knowledge, only use the evaluations to determine the best move. The evaluation for the next board is in the format of JSON, consisting of two elements: - \"thought\": - \"final evaluation\": 1 means takes advantage, -1 means takes advantage, and 0 means the game will be draw. Here is the board position and the next player is O: Board: Evaluation of the board and action pair. Judge which side takes advantage. The 40 Preprint. Work in progress. This strategy was successful \"The win probability for is \"It appears that the initial board \"There were no threats from in any of 3 --------- 4 6 --------- 7 9. The possible moves are [9]. The following are the boards after each possible move: ### Evaluation for taking action 9: { \"thought\": { \"Reflection\": position 3 --------- 4 6 --------- 7 9 and action 9 were highly favorable for O, as was able to win the game immediately after taking action 9 in all rollout sequences.\", \"Win probability\": extremely high, while the win probability for is extremely low.\", \"Threat\": the rollout sequences, as was able to win the game immediately after taking action 9.\", \"Potential strategies\": \"The potential strategy for was to occupy the corner 9 to win the game by occupying the positions [1, 5, 9]. in all rollout sequences.\" }, \"final evaluation\": } Now, please give your evaluation and the best move for based on the given board position: 3 --------- 4 6 --------- 7 9. You should output your answer in the JSON format. answer consists of two elements: - \"thought\": Lets think step by step. your detailed reflection by analyzing the next board positions and their evaluations. - \"best move\": move should be in the format of number from 1 to 9, indicating the position on the board. And the move should be in the available positions [9]. Dont output extra information except for the JSON format. { \"thought\": position after taking action 9, it appears that this move is highly favorable for O. The win probability for is extremely high, while the win probability for is extremely low. There were no threats from in any of the rollout sequences, as was able to win the game immediately after taking action 9. The potential strategy for was to occupy the corner 9 to win the game by occupying the positions [1, 5, 9]. This strategy was successful in all rollout sequences. Therefore, taking action 9 is the best move for O.\", \"Based on the evaluation of the board The best move for the next player. Generate Your The 1 ASSISTANT Preprint. Work in progress. \"best move\": } 9 B.3.4 BASELINE IMPLEMENTATION DETAILS For our PPO baseline, we implement custom PPO agent adapted for language models. The agent consists of two key components: an actor network based on LLaMA-3.1-8B-Instruct for action selection, and critic network for value estimation. Architecture Details The actor network uses LLaMA-3.1-8B-Instructs architecture with added special tokens for moves 1-9. The critic network consists of two-layer MLP (Multi-Layer Perceptron) that takes the last hidden state from the actor as input: Input dimension: LLaMA-3.1-8B hidden size Hidden layer: ReLU activation Output layer: Single value with Tanh activation Training Configuration The PPO training uses the following hyperparameters:"
        },
        {
            "title": "Parameter",
            "content": "Learning Rate (Actor) Learning Rate (Critic) PPO Epochs Batch Size Gradient Accumulation Steps Value Loss Coefficient Entropy Coefficient PPO Clip Range Discount Factor (γ) GAE Parameter (λ)"
        },
        {
            "title": "Value",
            "content": "1e-5 1e-5 1 16 2 0.5 0.01 0.2 0.99 0.95 Table 16: PPO Training Hyperparameters Implementation Details For each training iteration: We collect 512 complete game trajectories using 8 parallel environments Actions are selected using temperature-controlled sampling (T = 0.7) over the logits The policy proposes Nsample = 1 candidate actions per state and selects top-k (k = 1) based on sampling frequency We use 1 A100 GPU for training Gradient checkpointing is enabled to optimize memory usage Both networks are trained using AdamW optimizer with weight decay of 0.01. We maintain experience buffers with history length of 1 iterations for on-policy optimization. B.3.5 ADDITIONAL ABLATION STUDIES Action Selection Parameter Study To complement the main ablation studies presented in Section 5.3.3, we conduct additional experiments on the action selection parameter top-m. Our results demonstrate that increasing from 2 to 10 enhances exploration capabilities while maintaining training stability. Specifically, larger value (10) show more stable training compared to smaller values (2-5). 42 Preprint. Work in progress. Figure 8: Ablation studies on action selection parameter top-m."
        },
        {
            "title": "C PROMPTS",
            "content": "Here we provide all the prompt templates in three experiments. C.1 MAZE EXPERIMENT Language Value/Evaluation Prompt EVAL_USER_PROMPT_S_V = f\"\"\" You are playing simple maze game. An agent is moving in the maze and the objective is to reach the goal in as few steps as possible. The possible actions are \"move upn\", \"move downn\", \"move leftn\", \"move rightn\". You are highly skilled evaluator in this game and is expected to function similar to state value function in reinforcement learning. At each step will provide you with the move history of the agent (from old to new), including where the goal is, your current position, the walls that surround the agent. Your output evaluation should be json array that includes the following concepts: - \"thoughts\": Lets think step by step. Generate your detailed thought process and evaluation of the agents position and the distance towards the goal. - \"final_evaluation\": Concisely judge how good the agents current position is compared to all the possible positions in the maze, in terms of reaching the goal. Lets start new game. Now, please give your evaluation of the current state given the move history of the agent: {{game_content}} \"\"\" 43 Preprint. Work in progress."
        },
        {
            "title": "Language TD Prompt",
            "content": "EVAL_USER_PROMPT_S_TD_G2_new = f\"\"\" You are playing simple maze game. An agent is moving in the maze and the objective is to reach the goal in as few steps as possible. The possible actions are \"move upn\", \"move downn\", \"move leftn\", \"move rightn\". You are highly skilled evaluator in this game, particularly adept at making accurate assessments through look-ahead of the current maze position after taking the given action. At each step will provide you with the move history of the agent (from old to new), including where the goal is, your current position, the walls that surround the agent, the action that the agent is going to take, *along with several key variations of trajectory pieces after taking this action (and the corresponding natural language evaluations of the trajectory pieces)*. Your task is to understand these look-ahead information and summarize, derive non-trivial analysis and understanding the *the agents position after taking the action*. Your output evaluation should be json array with the following *two* concepts: - \"thoughts\": Lets think step by step. Summarize the look-ahead information of the variations after taking action {{chosen_action}}. - \"final_evaluation\": Now Concisely judge how good the chosen action is, in terms of reaching the goal. Now, please give your evaluation given action {{chosen_action}}, the *current environment state*: {{game_content}} and the look-ahead information of different variations after taking action {{chosen_action}}: \"\"\""
        },
        {
            "title": "Policy Improvement Prompt",
            "content": "POLICY_IMPROVEMENT_PROMPT_TD = f\"\"\" You are playing simple maze game. An agent is moving in the maze and the objective is to reach the goal in as few steps as possible. Your task is to determine the best action for the next time step given the current state (the move history of the agent (from old to new), including where the goal is, your current position, the walls that surround the agent). Your possible actions are \"move upn\", \"move downn\", \"move leftn\", \"move rightn\". The evaluations of the agent after possible actions are given. Each of them consists of two elements: - \"thoughts\": Summarization of the look-ahead information of the variations after taking the chosen action. - \"final_evaluation\": Judge how good the chosen action is, in terms of reaching the goal. DO NOT judge the action based on your exterior knowledge, only use the given evaluations to determine the best move. Here are the evaluations of each possible action: 44 Preprint. Work in progress. For action \"move up\", {{evaluations_up}} , For action \"move down\", {{evaluations_down}} , For action \"move left\", {{evaluations_left}} , For action \"move right\", {{evaluations_right}} Return the best action (choose only one from the possible actions) given the evaluations in json array with key \"action\". \"\"\" C.2 BREAKTHROUGH EXPERIMENT Language Value/Evaluation Prompt GAME_RULE_PROMPT = \"\"\"Here is the rule for the Breakthrough board game: The game is played on an 5x5 board for 2 players (white and black), with each player starting with 10 pawns. white pawns are on the first two rows and black pawns are on the last two rows. Black moves first. In each turn, players can move one of their pieces one square forward, diagonally forward if the target square is empty. Or it can capture an opponents piece if that square is one step diagonally forward. The game ends when one player successfully break through the opponent lines -- either move piece to the opposite last row of the board or captures all of the opponents pieces. For board representation, we use for black pieces, for white pieces, and . for empty squares. (1-5) and (a-e) are used to show the rows and columns index respectively.\"\"\" EVAL_SYSTEM_PROMPT = f\"\"\"{GAME_RULE_PROMPT} You are highly skilled evaluator in this game. will provide you with specific board information representing the current board. Your output should include the following concepts: 1. *Tactical Considerations*: Any immediate threats, potential tactics, or vulnerabilities in the position. 2. *Positional Evaluation*: Consideration of pawn structure, piece activity, control of key squares, and game safety. 3. *Suggested Moves*: One or two strong candidate moves for the side to move, along with brief rationale for comparing different moves. 4. *Advantage*: Based on all previous rationale, determine if white or black takes advantage. Use <white> or <black> to represent the winning side. Your response should be informative and concise.\"\"\" EVAL_USER_PROMPT = \"\"\"*The board you need to evaluate:* {board}\"\"\""
        },
        {
            "title": "Language TD Prompt",
            "content": "TD_SYSTEM_PROMPT = f\"\"\"{GAME_RULE_PROMPT} You are highly skilled evaluator in this game, particularly adept at making accurate assessments through look-ahead analysis of the current board position. will provide you with current board representation, *along with several key variations starting from this position (and their 45 Preprint. Work in progress. corresponding natural language evaluations of the subsequent positions)*. Your task is to aggregate and compare these look-ahead information, to summarize, derive non-trivial analysis about the *current board*. Your output should include the following concepts: 1. *Tactical Considerations*: Any immediate threats, potential tactics, or vulnerabilities in the position. 2. *Positional Evaluation*: Consideration of pawn structure, piece activity, control of key squares, and game safety. 3. *Suggested Moves*: One or two strong candidate moves for the side to move, along with brief rationale for comparing different moves. 4. *Advantage*: Based on all previous rationale, determine if white or black takes advantage. Use <white> or <black> to represent the winning side. Your response should be informative and concise.\"\"\" TD_USER_PROMPT = \"\"\"*The board you need to evaluate:* {board} Here are the look-ahead variations from the current board position: *Key Variations and Subsequent Evaluation:*: {variations} Please provide your analysis and understanding of the current board position based on the provided look-ahead information. Your response should be informative and concise.\"\"\" VARIATION_PROMPT = \"\"\"*Variation {i}:* Description of variations move sequence: {move_desc} Subsequent position evaluation: {subsequent_eval}\"\"\" SUBSEQUENT_PROMPT = \"\"\"The subsequent board is: {sub_board} The evaluation of this subsequent board is: {sub_eval}\"\"\" C.3 TICTACTOE EXPERIMENT"
        },
        {
            "title": "Policy Inference Prompt",
            "content": "POLICY_INFERENCE_SYSTEM_PROMPT = \"\"\" You are an expert agent at playing the game tic-tac-toe on 3*3 board. Tic Tac Toe is two-player game played on grid. Players take turns marking space with their respective symbol. The goal is to get multiple of ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. Your task is to choose the best move given board position. The board consists of \"O\", \"X\" and grid number. The grid number indicates empty grid. You should output your answer in the json format. Your answer consists of two elements: 46 Preprint. Work in progress. - \"thought\": lets think step by step. Generate your detailed thought process and plan for the next move. - \"best_move\": the best move for the next player. The move should be in the format of number from 1 to 9, indicating the position on the board. And the move should be in the available positions.\"\"\" POLICY_INFERENCE_EXAMPLE_USER_PROMPT = \"\"\" Here is the board position and the next player is O: --------- 4 6 --------- 7 8 9 The available move positions are 4, 6, 7, 8, 9. \"\"\" POLICY_INFERENCE_EXAMPLE_ASSISTANT_PROMPT = \"\"\" {\"thought\": \"It appears that the initial board position --------- 4 6 --------- 7 8 9 was favorable for X, as has occupied the positions 3, 5 and can win by occupying the position 7. has occupied the positions 1, 2. Therefore, the best move for is to occupy the position 7 to block and create potential winning opportunity by occupying the positions 1, 4, 7.\", \"best_move\": 7} \"\"\" POLICY_INFERENCE_USER_PROMPT = \"\"\" Here is the board position and the next player is {next_player}: {state}. The available move positions are {available_positions}. \"\"\""
        },
        {
            "title": "Policy Improvement Prompt",
            "content": "POLICY_IMPROVEMENT_SYSTEM_PROMPT = \"\"\" You are playing the game tic-tac-toe on 3*3 board. Tic Tac Toe is two-player game played on grid. Players take turns marking space with their respective symbol. The goal is to get multiple of ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. The board consists of \"O\", \"X\" and grid number. The grid number indicates empty grid. Your task is to determine the best move for the next player based on the given board position and the next player. The evaluations of (board, action) pairs after possible moves are given. DO NOT judge the board based on your knowledge, only use the evaluations to determine the best move. The evaluation for the next board is in the format of json format, consisting of two elements: - \"thought\": evaluation of the the board and action pair. - \"final_evaluation\": Judge which side takes advantage. 1 means takes advantage, -1 means takes advantage, and 0 means the game will be draw. \"\"\" POLICY_IMPROVEMENT_USER_PROMPT = \"\"\" Here is the board position and the next player is {next_player}: {state}. The possible moves are {available_positions}. 47 Preprint. Work in progress. The following are the boards after each possible move: {next_states} Now, please give your evaluation and the best move for {next_player} based on the given board position {state}. You should output your answer in the json format. Your answer consists of two elements: - \"thought\": lets think step by step. Generate your detailed reflection by analyzing the next board positions and their evaluations. - \"best_move\": the best move for the next player. The move should be in the format of number from 1 to 9, indicating the position on the board. And the move should be in the available positions {available_positions}. Dont output extra information except for the json format. \"\"\""
        },
        {
            "title": "Policy Evaluation Prompt",
            "content": "POLICY_EVALUATION_SYSTEM_PROMPT = \"\"\"You are player of the game of Tic Tac Toe. nThe game goal is to get multiple of ones own symbols in row, either horizontally, vertically, or diagonally, before the opponent does. If all nine squares are filled and no player has three in row, the game is draw. nThe board consists of \"O\", \"X\" and grid number. The grid number indicates empty grid. nYou are learning how to evaluate (board, action) pair in the tic tac toe by playing the game given the (board, action) pair and reflect the playing history. nThe playing history depicts game in progress with final result. Your answer consists of two elements: - \"thought\": lets think step by step. Generate your detailed evaluation over the (board, action) pair by merely reflecting the playing history after this pair from different perspectives. You should only rely on the playing history as context and dont evaluate game with your own judgement. Your evaluation should contain the following elements: Win probability, Threat, Potential strategies. - \"final_evaluation\": After all of your thoughts, judge which side takes advantage. 1 means you think takes advantage, -1 means you think takes advantage, and 0 means you think the game will be draw. You should output your answer in the json format.\"\"\" POLICY_EVALUATION_EXAMPLE_USER_PROMPT = \"\"\"The (board, action) to evaluate is Os turn: Board: --------- 4 6 --------- 7 8 9 Action: The Os move is 4. Below is the rollout sequence after this (board, action): After Os move 4, the board position is: --------- 6 --------- 7 8 9 After Xs move 7, the board position is: --------- 6 --------- 48 Preprint. Work in progress. 8 9 The game is over. wins. wins by occupying the positions 3, 5, 7. \"\"\" POLICY_EVALUATION_EXAMPLE_ASSISTANT_PROMPT = \"\"\" {\"thought\": {\"Reflection\": \"It appears that the initial board position --------- 4 6 --------- 7 8 9 and Os move 4 were not favorable for O, as was able to block on Os move at 7 and ultimately win the game.\", \"Win probability\": \"The win probability for is large, while the win probability for is low.\", \"Threat\": \"X has played at 5 and 3. can win by move 7. can occupy 1, 4, 7, and create potential winning opportunity. occupies 5, which is key position to win the game.\", \"Potential strategies\": \"Potential strategies for include playing at 7 to block and create potential win by occupying 1, 4, 7. could have occupied 3, 5, 7 to win the game. has already occupied 3, 5, and needs only 1 move to complete the win.\"} \"final_evaluation\": -0.8} \"\"\" POLICY_EVALUATION_USER_PROMPT = \"\"\"The board to evaluate is {player}s turn: Board: {board} Action: The {player}s move is {action}. Below is the rollout sequence 1 after this (board, action): After {player} taking action {action}, the board position is: {rollout_board_1} The game is over. {winner_1} wins. {winner_1} wins by occupying the positions {positions_1}. ... Below is the rollout sequence after this (board, action): After {player} taking action {action}, the board position is: {rollout_board_N} The game is over. {winner_N} wins. {winner_N} wins by occupying the positions {positions_N}. Now generate your evaluation for the (board, action) pair.\"\"\""
        },
        {
            "title": "D ANALOGY",
            "content": "D.1 LANGUAGE MC As mentioned in the main paper and figure, language MC is an analogy of traditional MC. Specifically, the mean and sum operation in traditional MC estimate can be replaced by the new lew language aggregator G1. can correspond to the intermediate transition for action, reward, and next state. The discount factor γ can also have an interesting analogy in G1 if G1 acts as lossy information compressor. Specifically, just as the discount factor reduces the weight of future rewards, G1 can perform lossy compression of future information, selectively retaining only the recent and most relevant aspects. 49 Preprint. Work in progress. D.2 LANGUAGE TD Most Analogies between language TD and traditional TD share similarities with that in the language MC setting. G1 is language aggregator over multiple lookahead variations, corresponding to the trajectory mean operation, while G2 combines immediate change/intermediate transition and future value evaluation, akin to the sum operation."
        }
    ],
    "affiliations": [
        "Brown University",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "University College London",
        "University of Bristol",
        "University of Surrey"
    ]
}