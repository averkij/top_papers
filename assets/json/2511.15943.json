{
    "paper_title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
    "authors": [
        "Zihan Li",
        "Yiqing Wang",
        "Sina Farsiu",
        "Paul Kinahan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \\href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 3 4 9 5 1 . 1 1 5 2 : r BOOSTING MEDICAL VISUAL UNDERSTANDING FROM MULTI-GRANULAR LANGUAGE LEARNING Zihan Li University of Washington Seattle, WA 98195, USA {zhanli}@uw.edu Yiqing Wang Duke University Durham, NC 27708, USA {yq.wang}@duke.edu Sina Farsiu Duke University Durham, NC 27708, USA {sina.farsiu}@duke.edu Paul Kinahan University of Washington Seattle, WA 98195, USA {kinahan}@uw.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive LanguageImage Pretraining (CLIP) has played pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with pointwise constraints to enhance alignment. MGLL employs smooth KullbackLeibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the large-scale image-text pretraining has significantly improved the performance of downstream computer vision tasks. Among these approaches, Contrastive Language-Image Pretraining (CLIP) Radford et al. (2021) has gained widespread popularity for its ability to learn aligned visual and textual representations from paired data. CLIP has been extensively utilized in multimodal learning, ensuring that representations from different modalities remain semantically consistent. Consequently, CLIP has been employed for pretraining vision foundation models and fine-tuning on various downstream tasks, such as classification, image segmentation, and object detection. Despite the success of CLIP and related pretraining methods in aligning images with textual categories, simple image-text pair matching remains inadequate in medical domains such as imaging, biosignal analysis, and genomics. single medical image or signal often maps to multiple target categories, requiring both multi-label and multi-granularity alignment. As shown in Fig. 1, retinal fundus image may present both Diabetic Macular Edema and Diabetic Retinopathy, along with finer-grained labels like Severe Diabetic Macular Edema and Moderate Non-Proliferative Diabetic Retinopathy. This calls for alignment across multiple semantic levels. Existing multi-label contrastive methods Wang et al. (2022b); Saporta et al. (2024); Naeem et al. (2024) focus on instancelabel correlations but struggle with cross-granular semantics and generalization. Compared to natural images, medical images encode more complex, hierarchical informationspanning diagnoses, Equal contribution. Co-corresponding author. 1 Figure 1: The illustrative comparison of input and outcome between CLIP and MGLL. structures, lesions, and texturesyet suffer from data scarcity due to privacy and annotation costs, further compounding the challenge. In this study, we aim to address the challenges of multi-label alignment and cross-granularity alignment through generalizable image-text contrastive learning framework simultaneously. Here, we define label as high-level disease category that an image belongs to, whereas granularity represents different levels or aspects of medical annotations, such as diagnostic attributes or clinical explanations. Unlike previous image-text contrastive pretraining approaches, which rely on singlegranular, single-label supervision, we construct multi-granular, multi-label datasets by collecting rich textual descriptions associated with the labels. Furthermore, we extend the original CLIP image and text loss Radford et al. (2021) to incorporate soft-label supervision and introduce point-wise constraints to enhance multi-label alignment. At the same time, we define contrastive learning objectives for each granularity level and employ smooth KullbackLeibler (KL) divergence to achieve cross-granularity alignment. By jointly optimizing these learning objectives, our proposed MGLL (Multi-Granular Language Learning) effectively aligns image-text pairs across both multiple labels and multiple granularities. Notably, our method does not introduce any granularity-sensitive encoders, ensuring no additional computational cost. This allows MGLL to function as plugand-play module that can be integrated into any vision foundation model or large vision-language model Touvron et al. (2023). We hope our method and experiments can provide new insights into medical vision-language pretraining and facilitate more effective visual representation learning. Our contributions are as follows: We propose MGLL, novel contrastive learning framework using multi-granular language that enables simultaneous multi-label and cross-granularity alignment. We provide set of architecture-agnostic, multi-label, multi-granularity learning objectives that can be seamlessly integrated into vision-language models and foundation models to enhance medical visual understanding. We design structured multi-granular, multi-label system and construct large-scale multigranular retinal and X-ray image-text datasets. Extensive experiments on over ten downstream datasets demonstrate that MGLL consistently outperforms other state-of-the-art (SOTA) methods, exhibiting superior generalization ability."
        },
        {
            "title": "2 RELATED WORK\n2.1\nLarge-scale image-text pretraining underpins modern multimodal learning. Contrastive methods\nlike CLIP Radford et al. (2021) align visual and textual features via paired data. Variants such as",
            "content": "IMAGE-TEXT CONTRASTIVE LEARNING 2 Figure 2: The overview of MGLL (Multi-Granular Language Learning) pretraining pipeline. SILC Naeem et al. (2024) use local-to-global pairwise learning, Symile Saporta et al. (2024) models higher-order multimodal relations, and Long-CLIP Zhang et al. (2024) handles extended text via stretched embeddings. MedCLIP Wang et al. (2022b) addresses false negatives in medical data by semantic matching. Recent foundation models Silva-Rodriguez et al. (2025); Du et al. (2024); Li et al. (2025); Zhang et al. (2023) tailor contrastive learning to specific domains. Still, standard frameworks often underperform in medical settings due to data scarcity and complex semantics. 2.2 MULTI-LABEL LEARNING While conventional deep networks perform well in single-label classification, real-world objects often carry multiple labels across entities, actions, and attributes Zhang et al. (2022); Khattak et al. (2024); Dai et al. (2024). Early work by Wang et al. Wang et al. (2016) learned joint imagelabel embeddings, and later introduced recurrent attention module for interpretability Wang et al. (2017b). SupCon Khosla et al. (2020) extended contrastive learning to supervised settings using label structures, while Zhang et al. Zhang et al. (2022) proposed hierarchy-preserving loss. Yet, these methods remain limited in vision-language integration, with fixed label spaces constraining semantic flexibility. 2.3 MULTI-GRANULARITY LEARNING Visual and textual data convey semantics across multiple granularities. Recent work explores this via diverse frameworks Zhao et al. (2024); Li et al. (2024a); Liu et al. (2024a). Wang et al. Wang et al. (2022a) use bidirectional cross-attention for fine-grained alignment, Zhao et al. Zhao et al. (2024) propose multi-granularity vision flow, and Xiong et al. Xiong et al. (2022) align inter-/intra-modal features with decision fusion. Du et al. Du et al. (2022) capture cross-modal multi-granular semantics for retrieval. Most of the these approaches rely on fixed training pipelines that limit their ability to incorporate heterogeneous or hierarchical annotations. Our MGLL provides more flexible and generalizable framework for learning both multi-label and cross-granularity visuallanguage representations. MGLL can effectively utilize different types of granularity information across diverse datasets without requiring specific annotation formats or model architectures. MGLL also displays robust performance under complex scenarios such as mixed granularity and noised annotations."
        },
        {
            "title": "3 MULTI-GRANULAR LANGUAGE LEARNING",
            "content": "3.1 OVERVIEW To achieve multi-label and cross-granularity matching between images and text, we propose MGLL, multi-granularity language-based contrastive learning framework. As shown in Fig. 2, our framework consists of an image encoder and text encoder, where we use Vision Transformer Dosovitskiy et al. (2020) and BERT Devlin et al. (2019) as the default choices. First, we collect rich textual descriptions for both fundus and X-ray images, constructing two multi-granularity datasets: MGLL-Fundus and MGLL-Xray. Next, we leverage the encoded image representations and multi3 granularity text representations for multi-label contrastive learning, employing smoothed KL divergence to align cross-granularity representations. We then describe how to transform multigranularity text into hierarchical representations and detail the MGLL objective. Furthermore, we provide both empirical and theoretical analyses, demonstrating that MGLL captures richer imagetext correlations than CLIP without additional parameters. This enables the learning of more discriminative visual features, improving downstream vision tasks. Finally, we introduce the construction of large-scale multi-granular datasets: MGLL-Fundus and MGLL-Xray."
        },
        {
            "title": "3.2 THE MGLL OBJECTIVES\nMost contrastive learning methods rely on the traditional CLIP loss, but our primary goal is to\nachieve simultaneous multi-label and cross-granularity alignment between image-text pairs. To this\nend, we improve the standard CLIP loss by introducing the soft CLIP loss, the point-wise loss, and\nthe smooth KL (Kullback–Leibler) divergence loss in our proposed multi-granularity language\nlearning objective. The soft CLIP loss LsCLIP enhances the visual encoder by enabling better align-\nment with multi-label features. The point-wise loss optimizes the alignment of visual features with\nspecific text features at a given granularity, further improving multi-label alignment. The smooth\nKL divergence loss helps different granularity features converge toward a unified feature space, fa-\ncilitating cross-granularity alignment of visual representations. To quantify the similarity between\nimage and text features, we adopt a soft alignment strategy, allowing an image Vi to align not only\nwith a single label Ti but also with multiple related labels Tik, k ∈ {1, 2, . . . , Mi} as Eqs. (1)\nand (2), where N is the total number of images, Mi is the number of text labels associated with\nthe i-th image, Vi and Tik represent the encoded features of the i-th image and its corresponding\nk-th text label, respectively. sim(Vi, Tik) is the similarity function measuring their alignment. The\ntemperature parameter τ controls the sharpness of the probability distribution, while the weight fac-\ntor wik determines the contribution of the k-th text label to the alignment of the i-th image. The\ntext-to-image loss lki can be obtained simply by swapping the roles of the image and text terms of\nthe image-to-text loss lik in Eq. (2).",
            "content": "(1) (2) (3) lik = wik log (cid:80)N LsCLIP = exp(sim(Vi, Tik)/τ ) (cid:80)Mn n=1 m=1 exp(sim(Vi, Tnm)/τ ) Mi(cid:88) 1 2 (cid:80)N i=1 Mi (lik + lki) (cid:88) i= k=1 Each image feature Vi is treated as multiple pairs (Vi, Ti1), (Vi, Ti2), ..., (Vi, TiMi), and wik is the probability of selecting Tik as valid label for Vi. wik is derived from the co-occurrence matrix normalization as Eq. (3). Instead of forcing the model to align strictly with one label like CLIP, MGLL allows multi-label optimization and prevents the model from being biased toward single label. wik = coocurrence(Vi, Tik) coocurrence(Vi, Tik) (cid:80) To further optimize the alignment between visual and textual features, we employ binary cross entropy as point-wise loss LP to refine multi-label alignment as Eq. (4), where ij = σ(xij), xij = sim(Vi, Tj) represents the similarity logits between the encoded image feature Vi and text feature Tj before applying activation. The binary label yij {0, 1} indicates whether the image-text pair is valid match. σ(x) is the Sigmoid activation function, defined as σ(x) = 1 1+ex , which normalizes the logits into probability range. Tj denotes the annotation corresponding to single label at specific granularity level. denotes the total number of annotations, and represents the total number of images. These annotations are consistent with those defined in Eq. (1). Since the pointwise loss does not explicitly model the relationships among annotations, we omit the label subscripts of and Tj for simplicity. By explicitly supervising individual image-text pairs, this loss enhances fine-grained multi-label alignment and improves the discriminability of visual representations. LP = (cid:88) (cid:88) i=1 j= yij log ij + (1 yij) log(1 ij) (4) To achieve cross-granularity alignment, we employ the smooth KullbackLeibler (KL) divergence loss LsKL, formulated as follows. Given similarity logits between the encoded image feature and 4 the text feature {Pi}m i=1, we define the mean distribution as the average of all predicted distributions: (cid:80)m = 1 i=1 Pi. Then, we compute the KL divergence between each predicted distribution and the mean distribution as Eqs. (5) and (6), where (j) represents the predicted probability of the i-th model for category j. This loss encourages consistency across different granularity levels by aligning their predicted distributions toward the mean distribution , which achieves cross-granularity alignment. (j) log (cid:88) (j) (j) DKL(PiM ) = LsKL = (cid:88) i=1 DKL(PiM ) (5) (6) The final loss with weight factors is as Eq. (7), where α1 is 0.5, α2 is 1, and α3 is 1 by experimental setting. LMGLL = α1LsCLIP + α2Lp + α3LsKL (7) 3.3 EMPIRICAL AND THEORETICAL ANALYSIS OF MGLL 3.3.1 EMPIRICAL ANALYSIS CLIP aligns each image with single text label, limiting its effectiveness in multi-label scenarios. MGLL addresses this by using soft CLIP loss and point-wise loss to align visual features with multiple correlated text labels on shared manifold. For multi-granularity alignment, MGLL encodes each granularity in separate spaces and aligns image features accordingly. smooth KL divergence loss further promotes consistency by aligning features across granularities with their mean distribution, preventing overfitting to any single level. This enables MGLL to distinguish both coarse and fine-grained categories (e.g., Glaucoma vs. Diabetic Macular Edema, and Severe vs. Moderate Diabetic Macular Edema), where CLIP typically fails. 3.3.2 THEORETICAL ANALYSIS We provide theoretical comparison between MGLL and CLIP. CLIP maximizes similarity between image and corresponding text features while minimizing contrastive loss for mismatched pairs, as defined in Eq. (8), where Vi and Ti are image and text features, sim(I, ) = IT IT is cosine similarity, and τ is temperature parameter. LCLIP = 1 (cid:88) i=1 log exp(sim(Vi, Ti)/τ ) j=1 exp(sim(Vi, Tj)/τ ) (cid:80)N (8) However, CLIP only aligns an image Vi with single text label Ti, limiting its effectiveness in multi-label settings. It also projects text features of different granularities into the same space, which is suboptimal when finer semantic distinctions are needed. MGLL overcomes these issues by introducing Soft CLIP Loss, Point-wise Loss, and Smooth KL Divergence Loss to support multilabel and cross-granularity alignment in appropriate feature subspaces. (1) Soft CLIP Loss: MGLL allows an image feature Vi to align with multiple text features {Ti1, Ti2, ..., TiMi}. At optimality, this leads to the condition in Eq. (9), implying Eq. (10), where Vi converges to the weighted center of its associated text features. This contrasts with CLIP, which aligns each image to single text feature, highlighting MGLLs advantage in multi-label learning. Mi(cid:88) k=1 wikVisim(Vi, Tik) = 0 Mi(cid:88) k= wik Tik Tik = Vi Vi (9) (10) (2) Point-wise Loss: To enhance image-text alignment, we introduce point-wise binary crossentropy loss Lp with its gradient shown in Eq. (11). If yij = 1, the objective is to maximize σ(xij), strengthening similarity between Vi and Tj. If yij = 0, it minimizes σ(xij), suppressing 5 similarity with irrelevant text. This encourages alignment with all valid labels while filtering out noise, improving over CLIPs single-label constraint. Lp xij = σ(xij) yij (11) (3) Smooth KL Divergence Loss: To enforce cross-granularity consistency, we introduce Smooth KL Divergence Loss. The mean distribution is defined as = 1 i=1 are predicted distributions. By the non-negativity of KL divergence, Eq. (12) achieves equality only when Pi = . Minimizing LKL thus enforces P1 = P2 = = Pm = , encouraging consistent representations across granularities and improving alignment in feature space. i=1 Pi where {Pi}m (cid:80)m DKL(PiM ) 0, (12) While CLIP optimizes image-text alignment, it overlooks feature variability across granularities and lacks consistency in visual alignment. By aligning each image feature Vi with single text feature Ti, it risks biased representations. In contrast, MGLL drives text features of different granularities toward shared mean distribution , promoting common semantic grounding and aligning visual features with all granularity levels, not just one. 3.4 LARGE-SCALE MULTI-GRANULAR DATASETS 3.4.1 MGLL-FUNDUS DATASET In this study, we construct large-scale multi-granularity fundus image-text dataset, MGLL-Fundus, consisting of 246,389 pairs of fundus images and corresponding multi-granularity textual descriptions. The image data in MGLL-Fundus originates from 49 public datasets, covering more than 50 disease categories (details are provided in the supplementary material). The multi-granularity textual descriptions mainly include two levels of granularity: disease category and clinical explanation. The disease-level granularity comprises normal/abnormal labels along with specific disease categories. The clinical explanation granularity provides detailed textual descriptions derived from label explanations in datasets and EyeWiki EyeWiki (2024). As shown in Fig. 2, the diseaselevel description is Abnormal, Age-related Macular Degeneration, while its corresponding clinical explanation is Changes in the retinal pigment epithelium. By incorporating multi-granularity textual descriptions, we establish hierarchical labeling system for fundus images, including normal/abnormal classification, disease categorization, and detailed clinical descriptions, which enable cross-granularity image-text alignment and enhance performance across different granularity levels. Our multi-granularity approach can also be adopted to other modalities. 3.4.2 MGLL-XRAY DATASET In radiology research, the heterogeneity of study descriptions in DICOM (Digital Imaging and Communications in Medicine) headers complicates patient cohort selection, especially with manual methods. MIDRC (Medical Imaging and Data Resource Center) MIDRC (2024) highlight this challenge, where over 138,000 studies are categorized into only 97 unique descriptions, while the rest are described by 1,300 different descriptions. Therefore, we need LOINC (Logical Observation Identifiers Names and Codes), which provides standardized coding system to enhance data sharing and analysis. To facilitate data coordination with, we collect 190,882 X-ray images from the MIDRC repository MIDRC (2024). We convert the images from DICOM to PNG format while extracting key metadata. The extracted multi-granularity textual information includes modality, study description, and series description. Modality includes CR (Computed Radiography), which has lower resolution and signal-to-noise ratio (SNR), and DX (Digital Radiography), which uses flatpanel detectors for higher-quality imaging. Study Description provides an exam-level overview, such as Chest X-ray, while Series Description details specific imaging sequences like PA View (posteroanterior) or Lateral View. These multi-granularity textual features serve as the textual component of MGLL-Xray dataset. 4 EXPERIMENTS 4.1 SETUP We construct large-scale multi-granularity fundus image-text dataset for pre-training, with further details provided in the supplementary material. To evaluate our models performance, we conduct experiments on eleven downstream datasets: FIVES Jin et al. (2022), IDRiD Porwal et al. (2018), OIA-DDR Li et al. (2019b), ADAM Fang et al. (2022), PALM Fang et al. (2024), REFUGE Orlando et al. (2020), RIM-ONE Batista et al. (2020), RFMiD Pachade et al. (2021), MIDRC-XR MIDRC 6 Figure 3: The quantitative comparison (AUC) between baseline methods and proposed MGLL on nine fundus downstream datasets. (2024), MIDRC-XR-Portable MIDRC (2024), ChestX-ray14 Wang et al. (2017a) under both linear probing and full fine-tuning settings. In our quantitative evaluation, we employ AUC (Area Under the receiver operating characteristic Curve), mAP (mean Average Precision), and ACC (Accuracy) as assessment metrics. As for the multi-label setting, we report the category-wise average accuracy as ACC. We adopt ViT-L/14 Dosovitskiy et al. (2020) as the image encoder and BiomedicalBERT Alsentzer et al. (2019) as the text encoder by default. All experiments were conducted under identical settings, with baselines pre-trained on our self-constructed multi-granularity datasets to ensure fair comparison. We strictly followed the official data splits of all downstream datasets. During pretraining, we only used the training sets for model training and the validation sets for pretraining evaluation, while the test sets were never accessed. 4.2 COMPARISON WITH STATE-OF-THE-ART METHODS 4.2.1 EVALUATION ON RETINAL FUNDUS DATASETS Utilizing the multi-granularity image-text fundus dataset we constructed, we pretrain our model within the Multi-Granular Language Learning (MGLL) framework to enhance its capability in feature representation for retinal fundus images. We conduct comprehensive experiments to compare the performance of MGLL against several state-of-the-art (SOTA) baseline methods across nine downstream datasets, covering wide range of retinal diseases. The AUC results for both linear probing and full fine-tuning are presented in Fig. 3, while more detailed results can be found in the supplementary material (Tables 15 to 23). MGLL consistently achieves significant performance improvements across all nine datasets, with particularly strong gains in the linear probing setting. Notably, on the multi-label dataset RFMiD Pachade et al. (2021), MGLL outperforms other methods by at least 16.6% in linear probing and 6.7% in full fine-tuning, demonstrating its superior capability in handling imbalanced data distributions. Fig. 4 visualizes class activation maps (CAMs) from CLIP and MGLL on two cases with different retinal diseases. It is evident that CLIP fails to extract meaningful features, instead assigning nearly uniform attention weights across the entire fundus image. In contrast, MGLL effectively localizes key regions of interest (ROIs) for different diseases. Specifically, MGLL accurately highlights hard exudates for chorioretinitis and the retinal pigment epithelium for age-related macular degeneration. These quantitative and qualitative evaluations collectively indicate that MGLL possesses extraordinary capability in effective feature extraction and performance enhancement across diverse retinal diseases. 4.2.2 EVALUATION ON X-RAY DATASETS We pretrain MGLL on the MGLL-Xray dataset and conduct experiments on MGLL and other SOTA baseline methods (Radford et al. (2021); Tiu et al. (2022); Zhang et al. (2023); Zhou et al. (2023a); Dai et al. (2024); Khattak et al. (2024); Lai et al. (2024); Xie et al. (2025)) on the MIDRC-XR In the linear probe setting, and MIDRC-XR-Portable datasets, which are shown in the Table 1. MGLL achieves significant advancements over the second-best method (UniChest Dai et al. (2024) on MIDRC-XR and UniMed-CLIP Khattak et al. (2024) on MIDRC-XR-Portable), with improvements of 2.23% and 3.81% in AUC, respectively, indicating superior representation learning capabilities. The performance gap becomes even more significant in the fully fine-tuned setting. To 7 Method MIDRC-XR CLIP (ICML-21) Table 1: The performance evaluation on MIDRC-XR, MIDRC-XR-Portable, and ChestX-ray14. Bold indicates best performance and underline shows second-best. MIDRC-XR-Portable Linear Probe (%) Fully Fine-tune (%) Linear Probe (%) Fully Fine-tune (%) Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP AUC ACC mAP AUC ACC mAP AUC ACC mAP AUC ACC mAP 54.72 51.18 16.62 88.52 80.83 62.04 71.43 78.22 22.31 91.83 90.08 83.94 69.75 78.09 18.33 82.05 87.58 31.79 CheXzero (Nat. BME-22) 51.31 43.85 12.71 80.11 73.26 55.46 72.84 80.13 23.56 92.47 92.42 85.23 68.72 76.98 15.97 81.81 87.39 31.52 53.44 47.13 14.86 85.74 78.39 60.12 73.53 80.71 23.88 93.41 92.98 85.96 73.72 78.95 21.87 83.80 89.13 34.01 56.23 53.61 17.73 90.67 83.95 64.76 79.38 86.05 27.72 96.52 95.07 86.95 74.63 79.87 23.23 84.28 89.57 35.62 59.02 54.78 19.32 92.51 86.32 66.93 78.49 85.28 27.37 95.44 94.32 86.38 76.15 81.72 25.52 85.84 89.99 37.97 UniMed-CLIP (arXiv-24) 57.33 54.07 18.06 94.15 87.47 68.49 80.05 86.63 28.16 94.31 93.55 86.19 75.54 81.21 24.96 82.59 88.36 32.19 57.92 54.43 18.79 93.48 86.94 67.62 75.24 82.67 25.65 92.94 92.66 85.67 77.32 83.94 26.88 82.95 88.65 32.86 58.31 54.59 19.03 93.29 86.71 67.44 80.31 86.77 28.27 96.93 95.74 87.42 76.62 82.35 25.98 85.10 89.73 37.02 61.25 56.57 21.19 99.08 90.06 73.33 83.86 89.06 30.62 99.75 98.80 89.87 82.94 90.41 28.53 87.37 92.71 39.17 CARZero (CVPR-24) FG-CLIP (ICML-25) MGLL KAD (Nat. Com-23) MRM (ICLR-23) UniChest (TMI-24) ChestX-ray14 Figure 4: The Class Activation Maps of different diseases from CLIP and MGLL. demonstrate its generalization capability, we conduct additional experiments using multi-granular labels constructed from the MIMIC-CXR dataset Johnson et al. (2019), evaluating performance on the ChestX-ray14 benchmark Wang et al. (2017a). The results reveal its exceptional transferability, with substantial performance advantages across all other baseline methods. In the linear probe setting, MGLL achieves 82.94% AUC, 90.41% accuracy, and 28.53% mAP, surpassing the second-best method (CARZero Lai et al. (2024)) by 5.62%, 6.47%, and 1.65% respectively. The improvement highlights its superior representation learning capacity, suggesting that its multi-granular approach captures more generalizable features that transfer effectively across datasets. These consistent and substantial improvements also demonstrate that MGLL enables more robust feature extraction. 4.3 PERFORMANCE WITH MGLL IN MLLMS To evaluate MGLLs impact as specialized vision encoder within multimodal large language models (MLLMs) for ophthalmological diagnostics, we design multiple-choice benchmark involving 2,233 clinical cases over ten ophthalmological conditions, where each fundus image prompted models to select the correct diagnosis from four options (one correct, three random alternatives). We replace the standard vision encoders in seven advanced MLLMs with our pretrained MGLL: InstructBLIP Dai et al. (2023), Mini-Gemini Li et al. (2024b), Qwen-VL Bai et al. (2023), InternVL Chen et al. (2024a), LLaVA Liu et al. (2024a), LLaVA-Med Li et al. (2024a), Med-Flamingo Moor et al. (2023), and Janus-Pro Chen et al. (2025). All MLLMs were fine-tuned on the target dataset to ensure fair comparison. Results demonstrate consistent and substantial improvements across all tested MLLMs, with average accuracy gains ranging from 4.6% (InternVL) to 34.1% (LLaVA-Med) as shown in Table 2. Notably, medically-specialized models exhibited the most dramatic enhancements, with Med-Flamingo and LLaVA-Med showing 31.7% and 34.1% increases respectively. This dramatic improvement can be attributed to the alignment between MGLLs ophthalmologyspecific visual feature extraction capabilities and the medical reasoning frameworks already embedded within these models. Even the high-performing general-purpose MLLMs like LLaVA (72.73% to 79.98%) also achieve significant gains with MGLL. The improvements across challenging conditions like Tessellation and Retinitis underscore MGLLs capacity to extract clinically relevant visual features from fundus images, highlighting its robust adaptability across models. 8 Table 2: Comparison of multiple-choice accuracy with MGLL in multimodal large language models on selected ten representative diseases. Method InstructBLIP Dai et al. (2023) + MGLL Mini-Gemini Li et al. (2024b) + MGLL Qwen-VL Bai et al. (2023) + MGLL InternVL Chen et al. (2024a) + MGLL LLaVA Liu et al. (2024a) + MGLL LLaVA-Med Li et al. (2024a) + MGLL DR 38.71% 16.13% Tessellation 44.25% 11.11% 63.79% 41.67% Glaucoma Media Haze Myopia Retinitis DME AMD Cataract CSR 80.17% 80.00% 0.00% 76.51% 59.30% 83.63% 85.00% 28.57% 82.55% 65.43% 45.16% 52.65% 44.44% 74.14% 58.33% 61.99% (14.7% ) 76.61% 85.00% 14.29% 79.87% 67.90% 82.46% 85.00% 42.86% 84.56% 72.22% 58.06% 64.16% 55.56% 65.52% 41.67% 65.21% (10.4% ) 81.87% 75.00% 28.57% 80.54% 78.40% 85.96% 80.00% 42.86% 89.93% 87.04% 70.97% 80.97% 33.33% 89.66% 41.67% 70.24% (9.5% ) 81.29% 85.00% 71.43% 94.63% 89.51% 86.55% 90.00% 71.43% 96.64% 90.74% 67.74% 91.15% 55.56% 94.83% 75.00% 81.96% (4.6% ) 83.04% 90.00% 42.86% 87.25% 91.36% 84.80% 90.00% 57.14% 93.96% 91.98% 61.29% 90.71% 66.67% 96.55% 66.67% 79.98% (7.3% ) 16.37% 15.00% 42.86% 26.85% 25.31% 58.48% 65.00% 57.14% 77.18% 59.26% 51.61% 57.08% 44.44% 55.17% 58.33% 58.37% (34.1% ) 23.89% 33.33% 16.67% 16.67% 88.50% 44.44% 93.10% 58.33% 76.55% 22.22% 84.48% 25.00% 88.05% 44.44% 87.93% 66.67% 58.41% 33.33% 60.34% 33.33% Average 47.29% 54.84% 60.75% 77.35% 72.73% 25.81% 64.52% 48.39% 54.78% 24.28% Med-Flamingo Moor et al. (2023) 25.73% 30.00% 57.14% 36.91% 24.07% 22.58% 18.58% 22.22% 24.14% 8.33% 26.97% + MGLL Janus-Pro Chen et al. (2025) + MGLL 69.01% 75.00% 71.43% 80.54% 61.11% 54.84% 45.58% 44.44% 51.72% 33.33% 58.70% (31.7% ) 88.30% 75.00% 42.86% 93.29% 90.74% 90.64% 85.00% 71.43% 96.64% 95.06% 67.74% 90.27% 55.56% 70.69% 75.00% 79.80% (10.88% ) 87.17% 33.33% 62.07% 58.33% 58.06% 68.92%"
        },
        {
            "title": "4.4 ABLATION STUDIES\n4.4.1 ABLATION STUDY ON MGLL OBJECTIVES\nWe conduct an ablation study to analyze the effectiveness of each objective in MGLL on the RFMiD\ndataset, as shown in Table 3. The standard CLIP model performs the worst, highlighting its limita-\ntions in medical image understanding. Incorporating the point-wise Loss LP significantly improves\nperformance, demonstrating its ability to enhance feature extraction. The soft CLIP loss LsCLIP also\nimproves over CLIP, which enables soft alignment with multiple labels. Combining both losses\n(LsCLIP + LP) further boosts performance, indicating their complementary effects. Finally, adding\nthe soft-KL loss LsKL leads to the best performance, demonstrating its role in refining feature consis-\ntency across different learning objectives. These results validate the effectiveness of each objective.",
            "content": "Table 3: Ablations of different MGLL objectives on RFMiD. Table 4: Ablations of granularity count on MIDRC-XR-Portable. Method Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 44.66 92.53 7.28 65.10 92.86 17.31 70.34 92.69 23.83 88.25 94.31 56.46 67.86 92.63 20.52 85.13 93.58 50.67 75.73 92.77 30.16 90.31 94.87 62.27 LsCLIP + LP + LsKL 79.62 92.84 34.08 92.83 95.48 64.99 CLIP LP LsCLIP LsCLIP + LP Method Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP CLIP 71.43 78.22 22.31 91.83 90.08 83.94 MGLL1 80.54 86.97 28.32 95.96 94.66 86.54 MGLL2 82.92 88.35 29.43 97.26 96.84 87.68 MGLL3 83.86 89.06 30.62 99.75 98.80 89.87 CLIP Method Table 5: Ablations of image encoder on RFMiD. Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 44.66 92.53 7.28 65.10 92.86 17.31 ConvNext-Base 74.45 92.73 27.94 88.55 94.57 57.62 ConvNext-Large 78.34 92.80 32.03 91.29 94.95 62.93 75.53 92.76 30.11 89.46 94.83 61.84 79.62 92.84 34.08 92.83 95.48 64.99 79.18 92.81 33.42 92.07 95.29 63.85 ViT-B/16 ViT-L/14 ViT-H/14 Method Table 6: Ablations of text encoder on RFMiD. Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 44.66 92.53 7.28 65.10 92.86 17.31 CLIPtext 68.93 92.66 22.14 88.76 94.58 58.14 BERT 79.62 92.84 34.08 92.83 95.48 64.99 LLaMA 74.89 92.75 29.38 90.97 95.04 62.86 CLIP 4.4.2 ABLATION STUDY ON GRANULARITY COUNT The ablation study on granularity count demonstrates the significant impact of multi-granular language supervision on model performance. As shown in Table 4, incrementally increasing the number of granularity levels consistently improves performance across all evaluation metrics. MGLL3, which utilizes three distinct granularity levels (modality, study description, and series description), achieves superior results compared to both MGLL1 (all textual information combined into single granularity) and MGLL2 (two granularity levels). Specifically, under linear probe evaluation, MGLL3 outperforms the baseline CLIP by substantial margins (+12.43% AUC, +10.84% ACC, +8.31% mAP) and shows marked improvement over MGLL1 (+3.32% AUC, +2.09% ACC, +2.30% mAP). The similar trend persists in fully fine-tuned scenarios. These results confirm that preserving the hierarchical structure of medical imaging information enables more comprehensive visionlanguage alignment than flattened representations, validating the core idea of the Multi-Granular Language Learning framework."
        },
        {
            "title": "4.4.3 SELECTION OF IMAGE ENCODER AND TEXT ENCODER\nThe ablation study on image encoders reveals significant performance variations across different\narchitectural choices when evaluated on the RFMiD dataset as shown in Table 5. Vision Trans-\nformer (ViT) Dosovitskiy et al. (2020) generally outperforms CNN counterparts (ConvNeXt Liu\net al. (2022b)), with ViT-L/14 achieving optimal results across all metrics. Interestingly, the larger\nViT-H/14 model shows slightly diminished performance compared to ViT-L/14, suggesting a poten-\ntial overfitting scenario or diminishing returns with increased model complexity in this domain.",
            "content": "The comparative analysis of text encoders demonstrates that the choice of language model significantly impacts the models ability to align textual and visual representations. BERT emerges as the optimal text encoder, achieving the highest performance across all evaluation metrics as shown in Table 6. The standard CLIP text encoder (denoted as CLIPtext) shows the limited performance among the tested alternatives, though it still substantially improves upon the baseline CLIP model. These findings suggest that the bidirectional attention mechanisms are suitable for the structured, hierarchical medical terminology utilized in MGLL."
        },
        {
            "title": "4.4.4 ABLATION STUDY ON IMAGE QUALITY AND TEXT QUALITY\nImage resolution is a critical factor in the performance of MGLL as evidenced by the ablation exper-\niments on the MIDRC-XR-Portable dataset. The performance exhibits a clear monotonic relation-\nship with image resolution, with Standard-Resolution (512×512) significantly outperforming both\nLow-Resolution (128×128) and Ultra Low-Resolution (64×64) configurations across all evaluation\nmetrics as shown in Table 7. These findings underscore the importance of preserving fine-grained\nvisual details in medical imaging applications, as higher resolution allows the model to capture sub-\ntle radiological features. However, MGLL substantially outperforms the baseline CLIP even at Ultra\nLow-Resolution, suggesting that MGLL provides robust improvements regardless of image quality.",
            "content": "The integrity of textual information significantly impacts model performance, as demonstrated through controlled degradation experiments on the MIDRC-XR-Portable dataset. The analysis contrasts standard textual descriptions against two degraded conditions: Error (20% partial errors in modality, study, or series descriptions) and Missing (20% partial omissions in these same fields). Standard textual descriptions yield superior performance across all metrics as show in Table 8. The Missing condition demonstrates intermediate performance, while the Error condition shows more performance degradation, suggesting that incorrect information is more detrimental than incomplete information. Nevertheless, both degraded conditions still significantly outperform the baseline CLIP model, indicating the robustness of MGLL to textual noise. These findings have important practical implications for clinical deployment scenarios, where reporting systems may contain documentation gaps or transcription errors, and suggest that MGLL maintains considerable diagnostic utility even under suboptimal documentation conditions. Table 7: Ablations of image quality on MIDRCXR-Portable. Table 8: Ablations of text quality on MIDRCXR-Portable. CLIP Method Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 71.43 78.22 22.31 91.83 90.08 83.94 Ultra Low-Res 78.82 85.68 27.49 94.48 93.69 86.22 80.66 87.02 28.53 98.18 97.65 88.46 Standard-Res 83.86 89.06 30.62 99.75 98.80 89. Low-Res Method Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 71.43 78.22 22.31 91.83 90.08 83.94 80.02 86.48 28.01 97.71 97.22 87.96 Missing 81.14 87.25 28.86 98.62 97.95 88.74 Standard 83.86 89.06 30.62 99.75 98.80 89.87 CLIP Error"
        },
        {
            "title": "5 CONCLUSION",
            "content": "This study introduces Multi-Granular Language Learning (MGLL), novel contrastive learning framework that addresses limitations in existing vision-language pretraining methods. MGLL utilizes textual information on different granular levels while employing soft-label supervision with point-wise constraints to enhance representation quality, which advances multi-label and crossgranularity alignment capabilities simultaneously. The implementation of smooth KullbackLeibler divergence also ensures cross-granularity consistency. Our evaluations across multiple downstream datasets demonstrate that MGLL consistently outperforms state-of-the-art methods in downstream tasks, particularly in domains requiring multi-label understanding at various granular levels. The results validate its ability to capture complex semantics in visual data, establishing MGLL as an advancement in developing future vision-language models. 10 Ethics statement This research uses exclusively publicly available medical imaging datasets and does not require Institutional Review Board (IRB) approval. All datasets utilized in this study, including the 49 public fundus imaging datasets comprising MGLL-Fundus, the MIDRC repository for X-ray images, MIMIC-CXR, and other evaluation datasets, have been previously released for research purposes with appropriate ethical clearances and patient consent procedures handled by the original data providers. All patient identifiers have been removed from the datasets prior to our access, ensuring full de-identification in compliance with HIPAA and other relevant privacy regulations. We have strictly adhered to the usage terms and conditions specified by each dataset provider and have not attempted to re-identify any individuals. Our multi-granularity learning approach is designed to improve automated medical image analysis, particularly for diabetic retinopathy, glaucoma, age-related macular degeneration, and chest X-ray interpretation. The MIDRC dataset applications focus on enhancing radiological assessment capabilities, which could potentially assist healthcare providers in resource-limited settings and improve diagnostic consistency. However, we emphasize that our models are intended as diagnostic support tools and should not replace clinical judgment. We commit to responsible AI development by ensuring transparent reporting of model limitations, encouraging rigorous clinical validation before any real-world deployment, and advocating for appropriate human oversight in all clinical applications. We do not claim our models are ready for direct clinical use without further validation and regulatory approval. Reproducibility statement We have made extensive efforts to ensure the reproducibility of our work. The complete implementation, including the original source code and README file with detailed instructions, is provided in the supplementary materials. Training configurations and hyperparameters are fully documented in both the source code and Section 4.1. Step-by-step mathematical derivations of the proposed methodology are presented in Section 3.3 and Section A. All datasets employed in this study are publicly available, with references and preprocessing procedures described in Section 3.4 and Section B. The experimental setup is described in Section 4.1 and more details are described in Section C. To ensure fair comparison and reproducibility, all experiments used identical settings with baseline models pre-trained on our custom multi-granularity datasets."
        },
        {
            "title": "REFERENCES",
            "content": "cataract dataset. https://www.kaggle.com/datasets/jr2ngb/ cataractdataset/, 2020. Accessed: 2025-02-20. Vietai advance course - retinal disease detection, 2020. https://www.kaggle.com/ competitions/vietai-advance-retinal-disease-detection-2020/data [Accessed: (May. 6, 2024)]. Augemnted ocular diseases, 2021. https://www.kaggle.com/datasets/ nurmukhammed7/augemnted-ocular-diseases [Accessed: 2025-02-20]. Derbi hackathon retinal fundus image dataset, 2022. https://www.kaggle.com/ datasets/nikkich9/derbi-hackathon-retinal-fundus-image-dataset [Accessed: 2025-02-20]. Michael Abr`amoff, James Folk, Dennis Han, et al. Automated analysis of retinal images for detection of referable diabetic retinopathy. JAMA ophthalmology, 131(3):351357, 2013. Kedir Adal, Peter van Etten, Jose Martinez, Lucas van Vliet, and Koenraad Vermeer. Accuracy assessment of intra-and intervisit fundus image registration for diabetic retinopathy screening. Investigative ophthalmology & visual science, 56(3):18051812, 2015. Syed Samiul Alam, Samiul Based Shuvo, Shams Nafisa Ali, Fardeen Ahmed, Arbil Chakma, and Yeong Min Jang. Benchmarking deep learning frameworks for automated diagnosis of ocular IEEE Access, toxoplasmosis: comprehensive approach to classification and segmentation. 2024. Emily Alsentzer, John Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, arXiv preprint Publicly available clinical bert embeddings. and Matthew McDermott. arXiv:1904.03323, 2019. 11 Jinze Bai et al. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Muhammad Naseer Bajwa, Gur Amrit Pal Singh, Wolfgang Neumeier, Muhammad Imran Malik, Andreas Dengel, and Sheraz Ahmed. G1020: benchmark retinal fundus image dataset for computer-aided glaucoma detection. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 17. IEEE, 2020. Francisco Jose Fumero Batista, Tinguaro Diaz-Aleman, Jose Sigut, Silvia Alayon, Rafael Arnay, and Denisse Angel-Pereira. Rim-one dl: unified retinal image database for assessing glaucoma using deep learning. Image Analysis and Stereology, 39(3):161167, 2020. Veronica Elisa Castillo Benıtez, Ingrid Castro Matto, Julio Cesar Mello Roman, et al. Dataset from fundus images for the study of diabetic retinopathy. Data in brief, 36:107068, 2021. Grace Maria Binu. Retinal occlusion dataset, 2023. https://www.kaggle.com/datasets/ gracemariabinu/retinal-occlusion-dataset [Accessed: 2025-02-20]. Attila Budai, Rudiger Bock, Andreas Maier, Joachim Hornegger, and Georg Michelson. Robust International journal of biomedical imaging, 2013(1): vessel segmentation in fundus images. 154860, 2013. Ling-Ping Cen, Jie Ji, Jian-Wei Lin, et al. Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks. Nature communications, 12(1):4828, 2021. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Zhe Chen et al. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024a. Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et al. Chexagent: Towards foundation model for chest x-ray interpretation. arXiv preprint arXiv:2401.12208, 2024b. Tianjie Dai, Ruipeng Zhang, Feng Hong, Jiangchao Yao, Ya Zhang, and Yanfeng Wang. Unichest: Conquer-and-divide pre-training for multi-source chest x-ray classification. IEEE Transactions on Medical Imaging, 2024. Wenliang Dai et al. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. Parisa Karimi Darabi. Diagnosis of diabetic retinopathy, 2024. https://www.kaggle.com/ datasets/pkdarabi/diagnosis-of-diabetic-retinopathy [Accessed: 202502-20]. Coen De Vente et al. Airogs: Artificial intelligence for robust glaucoma screening challenge. IEEE transactions on medical imaging, 2023. Etienne Decenciere, Guy Cazuguel, Xiwei Zhang, et al. Teleophta: Machine learning and image processing methods for teleophthalmology. Irbm, 34(2):196203, 2013. Aysen Degerli, Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Muhammad EH Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, and Moncef Gabbouj. Covid-19 infection map generation and detection from chest x-ray images. Health information science and systems, 9(1): 15, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. 12 Andres Diaz-Pinto, Sandra Morales, Valery Naranjo, et al. Cnns for automatic glaucoma assessment using fundus images: an extensive validation. Biomedical engineering online, 18:119, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020. Jiawei Du, Jia Guo, Weihang Zhang, et al. Ret-clip: retinal image foundation model pre-trained with clinical diagnostic reports. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 709719. Springer, 2024. Yunhao Du, Binyu Zhang, Xiangning Ruan, Fei Su, Zhicheng Zhao, and Hong Chen. Omg: Observe multiple granularities for natural language-based vehicle retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 31243133, 2022. Emma Dugas, Jared, Jorge, and Will Cukierski. Diabetic retinopathy detection. https:// kaggle.com/competitions/diabetic-retinopathy-detection, 2015. Kaggle. Team EyeWiki. The eye encyclopedia, 2024. https://eyewiki.org [Accessed: 2025-02-20]. Huihui Fang, Fei Li, Huazhu Fu, et al. Adam challenge: Detecting age-related macular degeneration from fundus images. IEEE transactions on medical imaging, 41(10):28282847, 2022. Huihui Fang, Fei Li, Junde Wu, et al. Open fundus photograph dataset with pathologic myopia recognition and anatomical structure annotation. Scientific Data, 11(1):99, 2024. Taimur Hassan, Hina Raja, Bilal Hassan, Muhammad Usman Akram, Jorge Dias, and Naoufel Werghi. composite retinal fundus and oct dataset to grade macular and glaucomatous disorders. In 2022 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2), pp. 16. IEEE, 2022. Qiao Hu, Michael Abr`amoff, and Mona Garvin. Automated separation of binary overlapping trees in low-contrast color retinal images. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2013, pp. 436443. Springer, 2013. Shih-Cheng Huang, Liyue Shen, Matthew Lungren, and Serena Yeung. Gloria: multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 39423951, 2021. Mir Tanvir Islam, Shafin Mashfu, Abrar Faisal, Sadman Chowdhury Siam, Intisar Tahmid Naheen, and Riasat Khan. Deep learning-based glaucoma detection with cropped optic cup and disc and blood vessel segmentation. Ieee Access, 10:28282841, 2021. Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: fundus image dataset for artificial intelligence based vessel segmentation. Scientific data, 9(1):475, 2022. Alistair EW Johnson, Tom Pollard, Seth Berkowitz, et al. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. Karthik, Maggie, and Sohier Dane. Aptos 2019 blindness detection. https://kaggle.com/ competitions/aptos2019-blindness-detection, 2019. Kaggle. Tomi Kauppi, Valentina Kalesnykiene, Joni-Kristian Kamarainen, et al. The diaretdb1 diabetic retinopathy database and evaluation protocol. In BMVC, volume 1, pp. 10. Citeseer, 2007. Muhammad Uzair Khattak, Shahina Kunhimon, Muzammal Naseer, Salman Khan, and Fahad Shahbaz Khan. Unimed-clip: Towards unified image-text pretraining paradigm for diverse medical imaging modalities. arXiv preprint arXiv:2412.10372, 2024. Prannay Khosla, Piotr Teterwak, Chen Wang, et al. Supervised contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. Ungsoo Kim. Machine learn for glaucoma, 2018a. URL https://doi.org/10.7910/DVN/ 1YRRAC. 13 Ungsoo Kim. Machine learning for pseudopapilledema, 2018b. https://osf.io/2w5ce/ [Accessed: 2025-02-20]. Oleksandr Kovalyk, Juan Morales-Sanchez, Rafael Verdu-Monedero, et al. Papila: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment. Scientific Data, 9(1):291, 2022. JR Harish Kumar, Chandra Sekhar Seelamantula, JH Gagan, et al. Chaks.u: glaucoma specific fundus image database. Scientific data, 10(1):70, 2023. Haoran Lai, Qingsong Yao, Zihang Jiang, et al. Carzero: Cross-attention alignment for radiology zero-shot classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1113711146, 2024. Larxel. Ocular disease recognition, 2021. https://www.kaggle.com/datasets/ andrewmvd/ocular-disease-recognition-odir5k [Accessed: 2025-02-20]. Chunyuan Li et al. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024a. Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang, and Hanruo Liu. Attention based glaucoma detection: large-scale database and cnn model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1057110580, 2019a. Tao Li, Yingqi Gao, Kai Wang, Song Guo, Hanruo Liu, and Hong Kang. Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening. Information Sciences, 501:511522, 2019b. Yanwei Li et al. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024b. Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul Kinahan, and Yu Qiao. Visionunite: vision-language foundation model for ophthalmology enhanced with clinical knowledge. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Li Lin, Meng Li, Yijin Huang, Pujin Cheng, Honghui Xia, Kai Wang, Jin Yuan, and Xiaoying Tang. The sustech-sysu dataset for automated exudate detection and diabetic retinopathy grading. Scientific Data, 7(1):409, 2020. Haotian Liu et al. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Ruhan Liu, Xiangning Wang, Qiang Wu, et al. Deepdrid: Diabetic retinopathygrading and image quality estimation challenge. Patterns, 3(6), 2022a. Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, and Yansong Tang. Universal segmentation at arbitrary granularity with language instruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 34593469, 2024b. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. DongAo Ma, Jiaxuan Pang, Michael Gotway, and Jianming Liang. fully open ai foundation model applied to chest radiography. Nature, pp. 111, 2025. Team MIDRC. The medical imaging and data resource center, 2024. https://www.midrc.org [Accessed: 2025-02-20]. Michael Moor et al. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pp. 353367. PMLR, 2023. 14 Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, et al. Silc: Improving vision language pretraining with self-distillation. In European Conference on Computer Vision, pp. 3855. Springer, 2024. Luis Filipe Nakayama, Mariana Goncalves, Zago Ribeiro, et al. brazilian multilabel ophthalmological dataset (brset). PhysioNet https://doi. org/10, 13026:2, 2023. Arijit Nandi. Glaucoma-classification-ml-dl, 2022. URL https://github.com/ officialarijit/Glaucoma-classification-ML-DL. Accessed: 2025-02-20. Meindert Niemeijer, Xiayu Xu, Alina Dumitrescu, et al. Automated measurement of the arteriolarto-venular width ratio in digital color fundus photographs. IEEE Transactions on medical imaging, 30(11):19411950, 2011. Jose Ignacio Orlando et al. Refuge challenge: unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. Medical image analysis, 59:101570, 2020. Samiksha Pachade, Prasanna Porwal, Dhanshree Thulkar, et al. Retinal fundus multi-disease image dataset (rfmid): dataset for multi-disease detection research. Data, 6(2):14, 2021. Sachin Panchal, Ankita Naik, Manesh Kokare, et al. Retinal fundus multi-disease image dataset (rfmid) 2.0: dataset of frequently and rarely identified diseases. Data, 8(2):29, 2023. Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, and Johan Verjans. Decomposing disease descriptions for enhanced pathology detection: multi-aspect vision-language pre-training framework. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1149211501, 2024. Ramon Pires, Herbert Jelinek, Jacques Wainer, Eduardo Valle, and Anderson Rocha. Advancing bag-of-visual-words representations for lesion classification in retinal images. PloS one, 9(6): e96814, 2014. Prasanna Porwal, Samiksha Pachade, Ravi Kamble, et al. Indian diabetic retinopathy image dataset (idrid): database for diabetic retinopathy screening research. Data, 3(3):25, 2018. Jianing Qiu, Jian Wu, Hao Wei, et al. Development and validation of multimodal multitask vision foundation model for generalist ophthalmic artificial intelligence. NEJM AI, 1(12):AIoa2300221, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Manuel Alejandro Rodrıguez, Hasan AlMarzouqi, and Panos Liatsis. Multi-label retinal disease IEEE Journal of Biomedical and Health Informatics, 27(6): classification using transformers. 27392750, 2022. Adriel Saporta, Aahlad Puli, Mark Goldstein, and Rajesh Ranganath. Contrasting with symile: arXiv preprint Simple model-agnostic representation learning for unlimited modalities. arXiv:2411.01053, 2024. Dandan Shan, Zihan Li, Yunxiang Li, Qingde Li, Jie Tian, and Qingqi Hong. Stpnet: Scale-aware text prompt network for medical image segmentation. IEEE Transactions on Image Processing, 2025. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Julio Silva-Rodriguez, Hadi Chakor, Riadh Kobbi, et al. foundation language-image model of the retina (flair): Encoding expert knowledge in text supervision. Medical Image Analysis, 99: 103357, 2025. 15 Hidenori Takahashi, Hironobu Tampo, Yusuke Arai, et al. Applying artificial intelligence to disease staging: Deep learning for improved staging of diabetic retinopathy. PloS one, 12(6):e0179790, 2017. Ekin Tiu, Ellie Talius, Pujan Patel, Curtis Langlotz, Andrew Ng, and Pranav Rajpurkar. Expertlevel detection of pathologies from unannotated chest x-ray images via self-supervised learning. Nature biomedical engineering, 6(12):13991406, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu. Multi-granularity cross-modal alignment for generalized medical visual representation learning. Advances in Neural Information Processing Systems, 35:3353633549, 2022a. Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, and Wei Xu. Cnn-rnn: unified framework for multi-label image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 22852294, 2016. Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Scientific reports, 10(1): 19549, 2020. Meng Wang, Tian Lin, Aidi Lin, Kai Yu, Yuanyuan Peng, Lianyu Wang, Cheng Chen, Ke Zou, Huiyu Liang, Man Chen, et al. Enhancing diagnostic accuracy in rare and common fundus diseases with knowledge-rich vision-language model. Nature Communications, 16(1):5528, 2025. Xiaosong Wang, Yifan Peng, Le Lu, et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 20972106, 2017a. Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang Lin. Multi-label image recognition by recurrently discovering attentional regions. In Proceedings of the IEEE international conference on computer vision, pp. 464472, 2017b. Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2022, pp. 3876, 2022b. Qijie Wei, Xirong Li, Hao Wang, Dayong Ding, Weihong Yu, and Youxin Chen. Laser scar detection in fundus images using convolutional neural networks. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part IV 14, pp. 191206. Springer, 2019. Junde Wu, Huihui Fang, Fei Li, et al. Gamma challenge: glaucoma grading from multi-modality images. Medical Image Analysis, 90:102938, 2023. Ruiqi Wu, Chenran Zhang, Jianle Zhang, et al. Mm-retinal: Knowledge-enhanced foundational In International Conference on Medical Image pretraining with fundus image-text expertise. Computing and Computer-Assisted Intervention, pp. 722732. Springer, 2024. Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei arXiv preprint Leng, and Yuhui Yin. Fg-clip: Fine-grained visual and textual alignment. arXiv:2505.05071, 2025. Peixi Xiong, Yilin Shen, and Hongxia Jin. Mga-vqa: multi-granularity alignment for visual question answering. arXiv preprint arXiv:2201.10656, 2022. Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023. 16 Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: In Proceedings of the Language-aware vision transformer for referring image segmentation. IEEE/CVF conference on computer vision and pattern recognition, pp. 1815518165, 2022. Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledgeguided context optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 67576767, June 2023. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pp. 310325. Springer, 2024. Jiong Zhang, Behdad Dashtbozorg, Erik Bekkers, Josien PW Pluim, Remco Duits, and Bart ter Haar Romeny. Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores. IEEE transactions on medical imaging, 35(12):26312644, 2016. Shu Zhang, Ran Xu, Caiming Xiong, and Chetan Ramaiah. Use all the labels: hierarchical multilabel contrastive learning framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1666016669, 2022. Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Weidi Xie, and Yanfeng Wang. Knowledge-enhanced visual-language pre-training on chest radiology images. Nature Communications, 14(1):4542, 2023. Zhuo Zhang, Feng Shou Yin, Jiang Liu, et al. Origa-light: An online retinal fundus image database for glaucoma analysis and research. In 2010 Annual international conference of the IEEE engineering in medicine and biology, pp. 30653068. IEEE, 2010. Xiangyu Zhao, Xiangtai Li, Haodong Duan, et al. Mg-llava: Towards multi-granularity visual instruction tuning. arXiv preprint arXiv:2406.17770, 2024. Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu. Advancing radiograph representation learning with masked record modeling. In ICLR, 2023a. Hong-Yu Zhou, Julian Nicolas Acosta, Subathra Adithan, Suvrankar Datta, Eric Topol, and Pranav Rajpurkar. Medversa: generalist foundation model for medical image interpretation. arXiv preprint arXiv:2405.07988, 2024. Yukun Zhou, Mark A. Chia, Siegfried K. Wagner, et al. foundation model for generalizable disease detection from retinal images. Nature, 622:156 163, 2023b. URL https://api. semanticscholar.org/CorpusID:264168236."
        },
        {
            "title": "Table of Contents",
            "content": "A Detailed Theoretical Analysis of MGLL A.1 Soft CLIP Loss . A.2 Point-wise Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Smooth KL Divergence Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dataset Details B.1 Pretrain Datasets . . . B.2 Downstream Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Setup Details C.1 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Detailed Experimental Results D.1 Detailed Results on Retinal Fundus Datasets . . . . . . . . . . . . . . . . . . . . . D.2 Zero-Shot Comparisons Across Medical and Natural Domains . . . . . . . . . . . D.3 Performance Evaluation on Region Segmentation . . . . . . . . . . . . . . . . . . D.4 More Detailed Results of MGLL in MLLMs . . . . . . . . . . . . . . . . . . . . . D.5 More CAMs on Retinal Fundus Datasets . . . . . . . . . . . . . . . . . . . . . . . D.6 Ablation Study on Weight Factors of Loss . . . . . . . . . . . . . . . . . . . . . . D.7 Ablation studies on the temperature coefficient (τ ) . . . . . . . . . . . . . . . . . . D.8 Performance on datasets with artificially introduced noise . . . . . . . . . . . . . . D.9 Performance on datasets with mixing granularity levels . . . . . . . . . . . . . . . Discussion and Future Work 19 19 20 22 24 26 28 28 28 29 32 32 33 37 38 38 38"
        },
        {
            "title": "A DETAILED THEORETICAL ANALYSIS OF MGLL",
            "content": "A.1 SOFT CLIP LOSS Formulation of the Soft CLIP Loss In contrast to standard CLIP, which forces an image representation Vi to align with single text label Ti. MGLL uses the Soft CLIP Loss to allow an image to align with multiple text features {Ti1, Ti2, . . . , TiMi}. In doing so, the loss is designed so that the optimal image feature becomes the weighted center of its associated text features. This mitigates bias toward any single label and better captures the multi-label nature of the data. The loss for an imagelabel pair is defined as the following formula: lik = wik log esim(Vi,Tik)/τ (cid:80)Mn m=1 esim(Vi,Tnm)/τ (cid:80)N n=1 (13) where wik is the weight assigned to the k-th text label for image i, derived via the following formula: wik = cooccurrence(Vi, Tik) cooccurrence(Vi, Tik) (cid:80) (14) where sim(Vi, Tik) measures the similarity between image and text features (typically cosine similarity). τ is the temperature parameter controlling the sharpness of the resulting probability distribution. The overall loss is given by LsCLIP = 1 2 (cid:80)N i=1 Mi (cid:88) Mi(cid:88) (cid:16) (cid:17) lik + lki i=1 k=1 (15) Derivation of the Optimality Condition (a) The Goal of the Optimization For the purposes of our analysis, we focus on how the loss aligns Vi with its multiple text features Tik. At an optimum, the gradient of the loss with respect to the image feature Vi must vanish. That is, we require ViLsCLIP = 0 . (16) Focusing on the part of the loss involving the alignment between Vi and its labels, we can write simplified optimality condition (ignoring symmetric contributions from text-to-image terms): Mi(cid:88) k= wik Visim(Vi, Tik) = 0 . (b) Computing the Gradient Assume for simplicity that both image and text features are normalized to unit norm: Under this assumption, the cosine similarity reduces to dot product: Vi = 1 and Tik = 1 . The derivative with respect to Vi is then straightforward: sim(Vi, Tik) = Vi Tik . Thus, the optimality condition becomes Vi(Vi Tik) = Tik . (17) (18) (19) (20) Mi(cid:88) k= wik Tik = λ Vi , (21) where λ is scalar (a Lagrange multiplier that arises from the normalization constraint on Vi). Because Vi is unit norm, this equation can be re-arranged to yield Vi = (cid:80)Mi k=1 wik Tik k=1 wik Tik (cid:80)Mi (cid:13) (cid:13) (cid:13) . (cid:13) (cid:13) (cid:13) (22) This result shows that at the optimal solution, the visual feature Vi is aligned with the weighted average (or the center) of its associated text features. (c) Rewriting in Normalized Form More generally, even when the features are not explicitly normalized in the network, one can express the optimality condition in terms of normalized vectors: Mi(cid:88) k=1 wik Tik Tik = Vi Vi . (23) This is equivalent to the expression provided earlier, emphasizing that Vi converges to the weighted centroid of the normalized text features. Interpretation and Discussion Unlike CLIP, which uses one-to-one imagetext matchingthe Soft CLIP Loss allows each image to be simultaneously aligned with multiple text descriptions. The weighting wik (derived from the co-occurrence matrix) ensures that each text label contributes to the final image representation in proportion to its relevance. The optimality condition guarantees that the image representation Vi is not overly biased by any single text feature but is instead the center of all its semantic descriptors. This is crucial in multi-label settings where an image may contain several objects or concepts. In standard CLIP, the loss encourages Vi to align closely with single text label Ti. Here, the Soft CLIP Losss optimality condition shows that the ideal Vi is instead weighted aggregate of multiple text labels, overcoming the limitation of forcing one-toone alignment in multi-label context. The detailed derivation shows that under the Soft CLIP Loss, the following gradient condition Mi(cid:88) k=1 wik Visim(Vi, Tik) = 0 (24) leads directly to the interpretation that the optimal image feature Vi is the normalized weighted sum of its associated text features. This theoretical result underpins MGLLs capability to perform multilabel alignment, ensuring that image representations capture the combined semantic information provided by multiple labels. A.2 POINT-WISE LOSS Formulation of the Point-wise Loss The point-wise loss LP is designed to refine the alignment between visual and textual features on per-pair basis. Unlike global or batch-level losses, this loss explicitly supervises each imagetext pair, ensuring that every valid pair (where yij = 1) is pulled closer together in the feature space while non-matching pairs (where yij = 0) are pushed apart. This detailed supervision is achieved via binary cross-entropy formulation applied to the similarity logits between an image feature Vi and text feature Tj. The point-wise loss is defined as LP = (cid:88) (cid:88) i=1 j= yij log ij + (1 yij) log(1 ij) 1 (25) where xij = sim(Vi, Tj) are the similarity logits (before activation) between the i-th image and the 1+exij is the probability computed by the Sigmoid activation. yij j-th text. ij = σ(xij) = 1 {0, 1} is the binary label indicating whether the image-text pair is valid match. This formulation is typical for binary classification tasks and allows the network to learn to distinguish relevant from irrelevant imagetext pairs. Derivation of the Gradient To understand how the loss drives the optimization, we derive the gradient with respect to the logits xij. (a) Loss for Single Pair Consider the binary cross-entropy loss for single imagetext pair (i, j): ℓij = [yij log σ(xij) + (1 yij) log(1 σ(xij))] (26) (b) Derivative with Respect to xij We can let pij = σ(xij). Using the chain rule, the derivative of ℓij with respect to xij is: ℓij xij = ℓij pij dpij dxij Step 1. Compute ℓij pij . We can differentiate ℓij with respect to pij as follows: (cid:18) yij pij 1 yij 1 pij ℓij pij = (cid:19) (27) (28) Step 2. Compute dpij dxij . Since pij = σ(xij) and the derivative of the sigmoid function is as follows: dσ(xij) dxij = σ(xij)(1 σ(xij)) = pij(1 pij) So we have dpij dxij = pij(1 pij) Step 3. Combine the Derivatives. We can multiply the two derivatives gives as follow: (cid:18) yij pij 1 yij 1 pij pij(1 pij) ℓij xij = (cid:19) Expanding and simplifying, we have: ℓij xij = (yij(1 pij) (1 yij)pij) Distributing the negative sign results in: ℓij xij = pij yij Thus, for each imagetext pair, the gradient is: ℓij xij = σ(xij) yij (29) (30) (31) (32) (33) (34) Since the overall point-wise loss LP is the average over all pairs, the gradient with respect to each xij remains the same: LP xij = σ(xij) yij (35) Interpretation and Discussion This gradient expression, σ(xij) yij, provides clear insights into the optimization dynamics: 21 For Positive Pair (yij = 1): The gradient becomes σ(xij) 1. If the predicted probability σ(xij) is less than 1, the gradient is negative, prompting an increase in xij (and hence σ(xij)). This drives the features Vi and Tj closer together. For Negative Pair (yij = 0): The gradient simplifies to σ(xij). If σ(xij) is positive (which it always is, since σ(x) (0, 1)), the gradient is positive, pushing xij downward. This reduces the similarity, ensuring that irrelevant imagetext pairs are further separated. Thus, the gradient directs the model to increase similarity for valid pairs (driving σ(xij) towards 1). And it also decreases similarity for invalid pairs (driving σ(xij) towards 0). In contrast to the standard CLIP framework, which optimizes global alignment between an image and single text description, the point-wise loss enables the model to adjust each imagetext pair individually, leading to more discriminative and robust feature space. Every potential imagetext pair is evaluated, allowing the model to learn subtle distinctions. By penalizing both false positives and false negatives at the individual pair level, the loss helps to create more separable and robust embedding space. The derivation of its gradient, LP = σ(xij) yij, clearly shows that the optimization encourages xij high similarity for valid pairs and low similarity for invalid pairs. Point-wise Loss not only enhances the discriminability of the visual representations but also supports the multi-label learning methods. A.3 SMOOTH KL DIVERGENCE LOSS Overview and Motivation The Smooth KL Divergence Loss is introduced to enforce consistency across predictions obtained at different granularities. In scenarios where multiple predicted logits {zi}m i=1 are available (each corresponding to different granularity or viewpoint), we wish to align their probability distributions so that they all agree on the prediction. This is achieved by first converting the logits into probability distributions via the Softmax function: and then encouraging each Pi to be close to the average (mean) distribution: Pi = Softmax(zi) The overall loss is given by = 1 (cid:88) i=1 Pi LsKL = (cid:88) i=1 DKL(PiM ) where for each the KL divergence is defined as DKL(PiM ) = (j) log (cid:88) (j) (j) Properties of KL Divergence There are two key properties of the KL divergence: 1. Non-negativity (Gibbs Inequality): for any two probability distributions and Q. 2. Zero if and Only if Equality: DKL(P Q) 0 DKL(P Q) = 0 if and only if = (36) (37) (38) (39) (40) (41) These properties imply that the divergence is minimized (and equals zero) when the two distributions are identical. Detailed Derivation and Proof (a) Expressing the Loss 22 Given the predicted distributions {Pi}m i=1 and the mean distribution , the loss is (cid:88) LsKL = (cid:88) (j) log i= (j) (j) (cid:124) (cid:123)(cid:122) DKL(PiM ) (cid:125) Because each DKL(PiM ) 0, it follows that LsKL 0 (b) Conditions for Zero Loss The loss for single term, DKL(PiM ), equals zero if and only if (j) = (j) for every category Since this must hold for every i, we have: P1 = P2 = = Pm = This is the necessary and sufficient condition for minimizing the loss: LsKL = 0 P1 = P2 = = Pm (42) (43) (44) (45) (46) Thus, by minimizing LsKL, the model is encouraged to produce consistent predictions across different granularities. So the mean distribution is defined as = 1 m (cid:88) i=1 Pi If all Pi are equal, then it is trivial to see that = Pi (47) (48) Therefore, the minimization process pushes each individual Pi toward the common distribution , ensuring consistency across different predictions. (c) Gradient Considerations While the explicit gradient derivation is more involved due to the dependency of on every Pi, we can outline the intuition of individual KL divergence and the intuition of coupled optimization. 1. Individual KL Divergence: For each i, consider the derivative of DKL(PiM ) = (j) log (cid:88) (j) (j) (49) . If we ignore the dependence of on Pi (as an approximation), the derivative with respect to (j) is DKL(PiM ) (j) log (j) (j) + 1 The condition for minimum (when this derivative is zero for all j) is then log (j) (j) + 1 = 0 = (j) (j) = e1 which alone does not yield Pi = ; however, when accounting for the normalization constraint (cid:80) = 1 and the simultaneous optimization across all Pi, the equilibrium is reached only when (j) (j) = (j) for every and j. 2. Coupled Optimization: Since is the average of all Pi, any deviation in one Pi from the others will increase its corresponding KL divergence. Thus, the overall optimization drives all predictions to align with each other. 23 (50) (51) Interpretation and Discussion By minimizing LsKL, we force the different predictions (from various granularities) to become consistent. Every Pi is toward the common mean , ensuring that the predictions from different parts of the model or from different feature granularities agree with each other. This consistency also contributes to more stable and robust feature space, as the model learns to reconcile variations in prediction across granularities. The Smooth KL Divergence Loss LsKL = (cid:80)m i=1 DKL(PiM ) is fundamentally designed to enforce that all predicted probability distributions Pi (across different granularities) become identical by driving them toward the mean distribution ."
        },
        {
            "title": "B DATASET DETAILS",
            "content": "B.1 PRETRAIN DATASETS MGLL-Fundus: We develop MGLL-Fundus, comprehensive multi-granularity fundus image-text dataset comprising 246,389 image-text pairs. This dataset integrates fundus images from 49 public datasets, encompassing more than 50 disease categories. The data distribution of the MGLL-Fundus dataset is presented in Table 9. The textual descriptions in MGLL-Fundus are structured across two distinct granularity levels: disease category and clinical explanation. The disease-level granularity includes normal/abnormal classification and specific disease categorization, while the clinical explanation granularity provides detailed textual descriptions derived from dataset label explanations and EyeWiki EyeWiki (2024). Some multi-granularity textual description examples from the dataset are illustrated in Table 10. Table 9: The data distribution of MGLL-Fundus dataset. Dataset HRF Budai et al. (2013) RITE Hu et al. (2013) ORIGA Zhang et al. (2010) PALM Fang et al. (2024) APTOS Karthik et al. (2019) IDRID Porwal et al. (2018) Num 45 40 650 1200 3662 516 MESSIDOR-2 Abr`amoff et al. (2013) 1748 4854 2838 463 12522 9939 485 175 2757 1369 1120 LAG Li et al. (2019a) BiDR Darabi (2024) E-ophta Decenciere et al. (2013) OIA-DDR Li et al. (2019b) DGOCF Takahashi et al. (2017) RIM-ONE Batista et al. (2020) LSD Wei et al. (2019) DHRF 202 (2022) Papilledema Kim (2018b) ROI Adal et al. (2015) Summary Dataset INSPIRE-AVR Niemeijer et al. (2011) G1020 Bajwa et al. (2020) REFUGE Orlando et al. (2020) RFMiD Pachade et al. (2021) DeepDRiD Liu et al. (2022a) ADAM Fang et al. (2022) JSIEC Cen et al. (2021) PARAGUAY Benıtez et al. (2021) FIVES Jin et al. (2022) BRSET Nakayama et al. (2023) SUSTech-SYSU Lin et al. (2020) BoVW Pires et al. (2014) CHAKSU Kumar et al. (2023) GNG Nandi (2022) VietAI vie (2020) BEH Islam et al. (2021) Dataset IOSTAR Zhang et al. (2016) GAMMA Wu et al. (2023) ODIR Larxel (2021) RFMiDv2 Panchal et al. (2023) EyePACS Dugas et al. (2015) Num 40 1020 1200 3200 1600 1200 ACRIMA Diaz-Pinto et al. (2019) 1000 AIROGS De Vente et al. (2023) PAPILA Kovalyk et al. (2022) 757 800 FUND Hassan et al. (2022) 16266 MuReD Rodrıguez et al. (2022) 1219 2013 HarvardGlaucoma Kim (2018a) DiaRetDB Kauppi et al. (2007) 1345 AOD 202 (2021) 400 ToxoFundus Alam et al. (2024) 3435 ROD Binu (2023) 634 Cataract 202 (2020) Num 30 100 7000 860 35126 705 101442 488 179 2208 601 1544 89 14813 411 281 246,389 images MGLL-Xray: To enhance data compatibility, we assembled 190,882 X-ray images from the MIDRC repository MIDRC (2024). We transformed these images from DICOM to PNG format while preserving essential metadata. The extracted multi-granularity textual information encompasses three levels: modality, study description, and series description. The modality category distinguishes between CR (Computed Radiography), characterized by relatively lower resolution and signal-tonoise ratio (SNR), and DX (Digital Radiography), which employs flat-panel detectors to achieve superior image quality. Study Description provides examination-level context (e.g., Chest X-ray), while Series Description specifies imaging protocols such as PA View (posteroanterior) or Lateral View. These hierarchical textual elements constitute the textual component of our MGLL-Xray dataset. Some multi-granularity textual description examples from this dataset are presented in Table 11. MIMIC-CXR: To further evaluate the generalization capability of our approach, we conducted supplementary experiments using multi-granular labels from MIMIC-CXR dataset Johnson et al. (2019), with performance assessed on the ChestX-ray14 benchmark Wang et al. (2017a). The MIMIC-CXR dataset represents one of the largest publicly available collections of chest radiographs, comprising 377,110 images associated with 227,835 imaging studies. This dataset encompasses 14 common thoracic pathologies, including atelectasis, cardiomegaly, consolidation, edema, 24 Table 10: The multi-granularity textual description examples of MGLL-Fundus dataset. Disease Category Abnormal, Mild Non-Proliferative Diabetic Retinopathy Abnormal, Moderate Non-Proliferative Diabetic Retinopathy Abnormal, Severe Non-Proliferative Diabetic Retinopathy Abnormal, Proliferative Diabetic Retinopathy Abnormal, Cataract Abnormal, Myopia Abnormal, Media Haze Abnormal, Branch Retinal Vein Occlusion Abnormal, Tessellation Abnormal, Laser Scars Abnormal, Central Serous Retinopathy Abnormal, Optic Disk Cupping Abnormal, Central Retinal Vein Occlusion Abnormal, Tortuous Vessels Abnormal, Asteroid Hyalosis Abnormal, Optic Disc Pallor Abnormal, Optic Disc Edema Abnormal, Optociliary Shunt Abnormal, Anterior Ischemic Optic Neuropathy Abnormal, Parafoveal Telangiectasia Abnormal, Retinal Traction Abnormal, Retinitis Abnormal, Chorioretinitis Abnormal, Macular Hole Abnormal, Retinitis Pigmentosa Abnormal, Cotton Wool Spots Abnormal, Coloboma Abnormal, Preretinal Hemorrhage Abnormal, Myelinated Nerve Fibers Abnormal, Hemorrhagic Retinopathy Abnormal, Central Retinal Artery Occlusion Abnormal, Tilted Disk Abnormal, Cystoid Macular Edema Abnormal, Post-traumatic Choroidal Rupture Abnormal, Choroidal Folds Abnormal, Vitreous Hemorrhage Abnormal, Macroaneurysm Abnormal, Vasculitis Normal, Healthy Clinical Explanation Only microaneurysms observed Retinal hemorrhages or hard exudates observed Many intraretinal hemorrhages or definite venous beading observed Neovascularization or vitreous/preretinal hemorrhage Opacification of crystalline lens observed Leopard fundus observed Opacity of media observed Occlusion of the central retinal vein The choroidal vessels are visible due to the reduced density of the pigments Circular or irregular shaped scars on the retinal surface observed Fluid accumulation under the retina observed The thinning of neuroretinal rim such that optic disc appears excavated Occlusion of the central retinal vein and the presence of flame-shaped hemorrhages Marked tortuosity of the retinal blood vessels Numerous astroid bodies are dispersed in vitreous Pale yellow discoloration of the optic disc Swelling of the optic disc Presence of prepapillary vascular loops or optociliary shunt vessels Optic disc swelling and pallor Yellow, lipid-rich exudation or parafoveal graying or tortuous blood vessels Presence of traction and retinal traction detachment Presence of vitreous inflammation or intraretinal hemorrhage The hard exudates observed small retinal break located in the center of the fovea observed The presence of bone-spicule deposits and arterial narrowing The presence of soft exudates The missing of portion of tissue in both the choroid and retina Boat-shaped hemorrhage which obscures the underlying retina Gray-white opaque lesions with feathery edges observed The presence of flame-shaped hemorrhages The presence of pale, whitening, and retinal swelling The tilting presence of the oval optic disc The presence of multiple cystoid areas in the macula and causes retinal edema The breaks in the choroid, Bruchs membrane, and RPE The presence of folds in the choroid The presence of extravasated blood in one of the spaces created around the vitreous body Fusiform or round dilation of the retinal arterioles which occur in the temporal retina observed The presence of inflammation of retinal blood vessels Clear optic disk boundaries, Normal fundus color, No apparent retinopathy Table 11: The multi-granularity textual description examples of MGLL-Xray dataset. Modality CR CR CR CR CR CR CR CR CR CR DX DX DX DX DX DX DX DX DX DX DX Study Description CHEST PORT 1 VIEW (RAD)-CS XR CHEST AP PORTABLE XR RIBS RIGHT WITH CHEST AP OR PA - SINGLE VIEW XR PORT CHEST 1V XR CHEST 2 VIEWS XR CHEST AP PA LATERAL 2 VW XR PORT CHEST 1V XRAY CHEST ONE VIEW XR RIGHT HIP 2+ VIEWS ORTHOPEDICS PRE OPERATIVE XR CHEST PA AND LATERAL XR CHEST 2 VIEWS, FRONTAL AND LATERAL XR WRIST LEFT (ROUTINE: AP,LAT,OBL) XR CHEST 2 VIEWS PA AND LATERAL XR THORACOLUMBAR SPINE STANDING 2 OR 3 VIEWS XR THORACIC SPINE AP AND LATERAL XR SCOLIOSIS STUDY 2 OR 3 VIEWS (NEURO INTERPRETATION) XR RIGHT RIBS 2 VIEWS UNILATERAL XR RIGHT SHOULDER 1 VIEW XR RIGHT KNEE 4+ VIEWS (NON-TRAUMA, PAIN/ARTHRITIS) XR RIGHT HIP 2-3 VIEWS (UNILATERAL) WITH PELVIS WHEN PERFORMED XR CHEST 1 VW, FRONTAL Series Description AP(shutter) AP PA Ribs LOWER CXR AP GRID Chest Lat. Lateral ClearRead Bone Suppression XRAY CHEST FRONTAL AND LATERAL VIEWS HIP X-Table Lat Chest a.p. PA XR WRIST LEFT (ROUTINE: AP,LAT,OBL) Chest Thoraco Lumbar Thoracic-spine DR Long Spine Rib AP Ext Rot(shutter) Patella Hip-joint PA CHEST LANDSCAPE effusion, emphysema, fibrosis, hernia, infiltration, mass, nodule, pleural thickening, pneumonia, and pneumothorax, along with No Finding category for normal cases. For our multi-granularity framework, we leveraged two distinct levels of textual information available in MIMIC-CXR: the structured disease labels (coarse granularity) and the detailed radiology reports (fine granularity). The disease labels provide categorical classification, while the reports offer comprehensive clinical interpretations with anatomical specificity, disease progression details, and differential diagnoses. This hierarchical representation allows our model to simultaneously process high-level disease categorization and nuanced clinical descriptions. Some examples of these multi-granularity textual descriptions are presented in Table 12. Table 12: The multi-granularity textual description examples of MIMIC-CXR dataset. Disease Labels Radiology Reports Edema Atelectasis Lung Opacity Cardiomegaly Pleural Effusion New tracheostomy is midline. The approximate diameter of the tube, 11 mm, compares to the diameter of the trachea, 27 mm. This sizing should be evaluated clinically. Pneumomediastinum outlines the tracheal wall and extends into deep subcutaneous emphysema in the neck, presumably function of tracheostomy. Followup advised. There is no pneumothorax or pleural effusion. Lungs are clear. Heart size is normal. The small right apical pneumothorax is stable and unchanged. The right chest tube is in stable position. Unchanged parenchymal opacity at the left lung base. Unchanged size of the cardiac silhouette and stable position of the right internal jugular vein catheter. Monitoring and support devices are in stable position. Stable left retrocardiac atelectasis and right basal parenchymal opacity. No pulmonary edema. No larger pleural effusions. No pneumothorax. Support lines and tubes are unchanged in position. The left-sided pleural effusion continues to decrease in size. There is improved aeration at the left base. Partially layering right-sided pleural effusion is again seen. There is new small left-sided apical pneumothorax. Slight worsening of cardiomegaly and mild-to-moderate pulmonary edema, accompanied by increasing moderate left pleural effusion and persistent small right pleural effusion. Indwelling support and monitoring devices are unchanged in position, including proximally located left PICC, terminating at the junction of the left axillary and subclavian veins. Single portable upright AP image of the chest. There are low lung volumes. There is an interval increase in the alveolar opacities bilaterally, consistent with moderate to severe new onset pulmonary edema. The cardiomediastinal silhouette appears to be somewhat enlarged from prior exam, particularly in the right mediastinum. There is no large pleural effusion or pneumothorax. pacer is seen overlying the left anterior chest with intact leads in appropriate position. Portable AP radiograph of the chest was reviewed with no prior studies available for comparison. Heart size is top normal. Mediastinum is grossly unremarkable. Lungs are essentially clear except for right basal opacity which unclear if represents true lesion or summation of shadows. Repeated radiograph preferably with full inspiration is required. If finding is persistent, assessment with chest CT would be necessary. Tracheostomy tube is in satisfactory position with the tip 4.5 cm above the carina. The right internal jugular central line and nasogastric tube are unchanged in position. The heart remains stably enlarged. Lung volumes are markedly reduced and there is small layering left effusion with persistent retrocardiac consolidation likely reflecting partial lower lobe atelectasis. No pulmonary edema. No obvious pneumothorax. Lung Lesion No Finding Pneumonia B.2 DOWNSTREAM DATASETS We evaluate our proposed MGLL alongside several baseline methods on multiple downstream datasets. The details of these datasets are presented in Table 13. The multiple-choice evaluation benchmark details are in Table 14. Fundus Imaging Datasets: FIVES Jin et al. (2022): collection of 800 retinal images categorized into four diagnostic classes: normal, age-related macular degeneration, diabetic retinopathy, and glaucoma. IDRiD Porwal et al. (2018): Contains 516 retinal images with annotated severity grades for diabetic retinopathy (DR) and diabetic macular edema (DME). OIA-DDR Li et al. (2019b): Comprises 12,523 fundus images labeled with diabetic retinopathy severity classifications. ADAM Fang et al. (2022): dataset of 1,200 fundus images specifically designed for age-related macular degeneration detection. PALM Fang et al. (2024): Consists of 1,200 fundus images annotated for pathological myopia diagnosis. REFUGE Orlando et al. (2020): Includes 1,200 retinal images with binary classification for glaucomatous and non-glaucomatous. RIM-ONE Batista et al. (2020): retinography collection of 485 images developed for glaucoma evaluation. RFMiD Pachade et al. (2021): Encompasses 3,200 retinal images with multi-label annotations across 45 categories. Our evaluation focuses on 12 labels where positive cases exceed 2% prevalence. Radiographic Imaging Datasets: MIDRC-XR MIDRC (2024): dataset contains 111,816 X-ray images across 14 LOINC-coded categories (including XR Chest AP views). 26 Table 13: The details of downstream datasets. Name FIVES Jin et al. (2022) Numer (Train : Val : Test) 480 : 120 : 200 IDRiD Porwal et al. (2018) 319 : 94 : OIA-DDR Li et al. (2019b) 6260 : 2503 : 3759 ADAM Fang et al. (2022) 400 : 400 : 400 PALM Fang et al. (2024) REFUGE Orlando et al. (2020) RIM-ONE Batista et al. (2020) RFMiD Pachade et al. (2021) 400 : 400 : 400 400 : 400 : 400 270 : 69 : 146 1920 : 640 : MIDRC-XR MIDRC (2024) 89453 : 11182 : 11181 MIDRC-XR-Portable MIDRC (2024) 63253 : 7906 : 7907 ChestX-ray14 Wang et al. (2017a) 77872 : 8652 : Label Categories Normal, Age-related Macular Degeneration, Diabetic retinopathy, and Glaucoma Severity levels of Diabetic Retinopathy (no apparent, mild non-proliferative, moderate non-proliferate, severe non-proliferate, proliferative) and Diabetic Macular Edema (no apparent, mild, moderate, severe) Severity levels of Diabetic Retinopathy (no apparent, mild non-proliferative, moderate non-proliferate, severe non-proliferate, proliferative) Age-related Macular Degeneration and no Age-relatedd Macular Degeneration Pathological myopia and Healthy Glaucoma and Healthy Glaucoma and Healthy Diabetic Retinopathy, Age-related Macular Degeneration, Media haze, drusens, Myopia, Branch Retinal Vein Occlusion, Tessellation, Laser scars, Optic disc cupping, Optic disc pallor, Optic disc edema, and Retinitis XR Chest AP, XR Chest 2 Views, XR Unspecified body region Views, XR Chest Single view, XR Chest PA and Lateral, XR Chest Views, XR Chest AP and Lateral, XR Chest and Abdomen Single view, XR Ribs Views, XR Abdomen AP, XR Abdomen Single view, XR Chest View and Abdomen Supine and Upright, XR Abdomen Supine and Upright, XR Ribs Views and Chest PA Portable XR Chest AP single view, Portable XR Chest Views AP, Portable XR Abdomen AP, Portable XR Chest Views, Portable XR Chest Views inspiration and expiration, Portable XR Abdomen Supine and Upright Atelectasis, Cardiomegaly, Effusion, Infiltration, Mass, Nodule, Pneumonia, Pneumothorax, Consolidation, Edema, Emphysema, Fibrosis, Pleural Thickening, Hernia 27 MIDRC-XR-Portable MIDRC (2024): Focuses on portable radiography with 79,066 X-ray images across 6 LOINC-coded categories (including Portable XR Chest AP single views). ChestX-ray14 Wang et al. (2017a): comprehensive medical imaging repository contains 112,120 frontal-view chest radiographs annotated with 14 labels. Labels are extracted from corresponding radiological reports. Evaluation Benchmark on MLLMs (Multiple-choice Benchmark): comprehensive ophthalmic evaluation dataset comprising 2,233 images across 25 distinct diagnostic categories. The label distribution of the multiple-choice evaluation benchmark is in Table 14. Table 14: The label distribution of the multiple-choice evaluation benchmark. Label Health Myopia Retinitis Diabetic Retinopathy (DR) Media Haze (MH) Cataract Optic Disk Cupping (ODC) Optic Disc Pallor (ODP) Branch Retinal Vein Occlusion (BRVO) Num 890 226 9 149 31 20 32 2 16 Label Other disease (Other) Tessellation Chorioretinitis Drusen Central Serous Retinopathy (CSR) Arteriosclerotic Retinopathy (AR) Optic Disc Edema (ODE) Hypertensive Retinopathy (HR) Central Retinal Vein Occlusion (CRVO) Age-related Macular Degeneration (AMD) 171 No Age-related Macular Degeneration (No AMD) Diabetic Macular Edema (DME) Glaucoma Choroidal Neovascularization (CN) 58 162 No Diabetic Macular Edema (No DME) No Glaucoma Summary Num 50 12 3 30 7 2 11 3 11 311 11 12 2233 images"
        },
        {
            "title": "C SETUP DETAILS",
            "content": "C.1 EVALUATION METRICS In our quantitative evaluation, we employ Area Under the Receiver Operating Characteristic Curve (AUC), mean Average Precision (MAP), and Accuracy (ACC) as assessment metrics. Among these, AUC serves as our primary evaluation metric, as it reflects overall model performance. MAP is particularly useful for handling long-tailed label distributions. To better assess performance on the imbalanced multi-label dataset such as RFMiD Pachade et al. (2021), we report the category-wise average accuracy instead of overall accuracy. As for the accuracy on the multiple-choice benchmark, we implement four-option forced-choice classification approach utilizing the MGLL-Fundus dataset. For each fundus image presented, the model must select the most probable diagnostic classification from among four distinct disease labels. These options comprise the correct diagnostic label along with three additional labels randomly sampled from the complete disease names available in the dataset. The randomized inclusion of incorrect options helps evaluate model performance in distinguishing the correct diagnosis from plausible alternatives, which also enables quantitative assessment of diagnostic accuracy. C.2 IMPLEMENTATION DETAILS We adopt ViT-L/14 Dosovitskiy et al. (2020) as the image encoder and BiomedicalBERT Alsentzer et al. (2019) as the text encoder. All images are resized to 224 224, and data preprocessing includes random flipping (probability = 0.5) and color jittering (factor = 0.1). We set the batch size to 32, the feature vector dimension to 768, and the temperature coefficient to 0.07. Optimization is performed using AdamW Loshchilov & Hutter (2017) with learning rate of 1e-4, weight decay of 0.0001, and hyperparameters β1 = 0.9, β2 = 0.98, and ϵ = 1e 6. All experiments are conducted with the NVIDIA RTX A6000 GPU."
        },
        {
            "title": "D MORE DETAILED EXPERIMENTAL RESULTS",
            "content": "D.1 DETAILED RESULTS ON RETINAL FUNDUS DATASETS We present the complete experimental results for performance comparison with eight baselines (Radford et al. (2021); Yao et al. (2023); Zhou et al. (2023b); Silva-Rodriguez et al. (2025); Wu et al. (2024); Du et al. (2024); Khattak et al. (2024); Qiu et al. (2024)) on downstream retinal fundus datasets in Table 15 to Table 23. The experimental results demonstrate that the MGLL consistently outperforms existing approaches across nine retinal fundus datasets in both linear probing and full fine-tuning evaluation settings. Notably, MGLL demonstrates particularly strong gains in the linear probing setting, where it demonstrates substantial improvements over second-best methods (e.g., achieving 90.02% AUC in ADAM compared to UniMed-CLIPs 79.33%, and 92.42% AUC in REFUGE versus RET-CLIPs 84.59%). To further analyze these results, we observe consistent performance improvements across multiple evaluation metrics. For instance, in the FIVES dataset, MGLL achieved 89.73% AUC, 52.00% ACC, and 75.32% mAP in linear probing, significantly outperforming RETFound (88.09% AUC, 49.00% ACC, 72.55% mAP). When fully fine-tuned on this dataset, MGLL maintained its advantage with 94.98% AUC, 72.00% ACC, and 86.34% mAP. The robust performance of MGLL can be attributed to its multi-granularity learning approach, which effectively captures both local and global features in retinal fundus images. This architectural advantage enables MGLL to identify subtle pathological patterns that may be overlooked by conventional methods. For example, in the REFUGE dataset (glaucoma detection), MGLL achieved 7.83% improvement in AUC over the second-best method in linear probing setting. Additionally, the exceptional performance on the PALM dataset (99.66% AUC, 96.00% ACC, and 99.72% mAP in linear probing) demonstrates MGLLs capacity to achieve near-perfect diagnostic accuracy in certain retinal conditions. When compared to previous state-of-the-art methods such as VisionFM (97.12% AUC) and RET-CLIP (95.25% AUC), MGLL offers clinically significant improvements in detection reliability. This superior performance indicates excellent feature representation quality and transferability of our pretrained MGLL, enabling effective adaptation to diverse diagnostic tasks with fine-tuning. Table 15: The performance evaluation on FIVES. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 76.22 81.25 76.53 81.63 81.36 88.09 77.14 84.24 77.96 82.31 79.89 86.74 79.23 82.75 82.76 85.98 86.34 89.73 64.21 64.72 72.55 67.17 64.98 69.35 65.46 68.73 75.32 64.00 64.50 69.50 66.00 67.00 68.50 68.00 70.50 72.00 37.00 39.50 49.00 43.50 41.00 47.50 41.50 45.00 52. 88.96 89.16 92.83 89.85 90.62 92.04 91.59 93.11 94.98 29 Table 16: The performance evaluation on IDRiD (DR). Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 44.01 70.83 44.86 72.34 47.05 73.51 48.54 74.62 45.32 71.81 49.42 78.18 52.84 77.39 51.38 77.52 54.30 80.28 34.44 36.29 37.48 39.16 35.37 47.42 45.75 46.22 51.19 44.66 46.60 53.40 55.34 48.54 56.31 58.25 57.28 60.19 35.92 40.78 43.69 45.63 37.86 52.43 49.51 51.46 58.25 76.74 76.81 78.14 78.82 77.13 79.32 80.72 79.95 82.57 Table 17: The performance evaluation on IDRiD (DME). Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 57.87 71.81 50.11 60.33 55.39 65.92 51.83 62.85 54.97 68.71 55.75 64.13 56.85 70.59 62.23 73.53 67.80 78.41 56.67 48.86 52.05 50.31 54.76 51.28 56.04 59.98 64.72 77.67 65.05 69.90 66.99 70.87 67.96 74.76 78.64 79. 76.70 66.02 72.82 68.93 71.84 73.79 75.73 79.61 80.58 73.34 62.78 69.26 64.52 69.03 70.14 71.81 77.95 86.17 Table 18: The performance evaluation on OIA-DDR. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 49.91 73.30 45.92 72.09 52.21 80.77 50.27 82.48 44.53 72.68 48.05 77.43 46.88 74.67 48.77 78.85 56.67 86.28 39.21 37.87 46.94 48.09 38.72 43.31 40.18 44.27 50.92 55.41 53.68 61.13 63.36 54.46 58.18 56.19 59.35 72.09 71.75 65.47 72.12 70.63 64.03 68.82 66.69 69.46 73.13 85.29 80.39 85.96 85.54 79.42 83.68 81.23 84.25 88.85 Table 19: The performance evaluation on ADAM. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 65.48 52.41 60.54 52.13 65.86 59.34 66.91 63.82 52.26 56.97 79.92 77.48 69.22 79.33 75.89 73.56 90.08 90.02 25.05 24.36 33.53 39.27 29.88 53.76 55.63 51.42 62.40 76.50 74.75 78.25 79.50 77.75 82.25 82.50 81.75 85. 83.00 82.75 83.50 84.75 82.00 86.25 85.25 85.50 90.00 86.70 84.42 88.02 90.16 74.88 93.27 90.64 92.43 96.30 30 Table 20: The performance evaluation on PALM. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 99.58 93.94 94.81 87.94 96.39 92.02 95.24 89.42 94.18 88.21 98.42 95.25 97.02 90.31 97.84 97.12 99.76 99.66 94.62 88.11 92.87 90.03 88.46 95.89 91.12 97.45 99.72 91.75 88.25 90.50 89.25 88.50 92.50 89.75 94.25 96.00 99.51 94.21 95.75 94.88 93.34 98.67 96.82 97.73 99.72 96.00 92.00 93.00 92.25 91.50 95.25 93.75 94.50 95. Table 21: The performance evaluation on REFUGE. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 58.37 65.33 49.07 60.69 63.84 79.67 56.82 70.59 55.78 63.04 70.45 84.59 52.84 61.25 57.68 73.21 80.99 92.42 15.68 11.52 45.39 27.82 14.15 55.06 12.87 33.42 75. 93.00 90.75 93.50 92.25 91.50 93.75 91.00 92.50 94.75 88.75 86.25 90.50 89.25 88.25 91.25 87.50 89.50 94.50 86.96 81.45 89.02 85.67 84.89 90.46 83.11 86.13 93.90 Table 22: The performance evaluation on RIM-ONE. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (NEJM AI-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 84.00 65.96 85.88 74.34 90.35 89.79 92.41 81.83 85.22 67.35 89.19 84.42 78.14 69.87 87.21 72.97 94.97 94.39 54.11 63.45 83.92 70.14 55.24 79.85 58.43 61.86 86.68 66.44 73.97 83.56 79.45 67.81 82.19 70.55 72.60 87.67 82.88 84.25 86.99 88.36 83.56 86.30 81.51 84.93 89.73 88.38 90.39 94.22 94.93 89.91 92.58 83.37 91.27 97. Table 23: The performance evaluation on RFMiD. Bold indicates best performance and underline shows second-best. Method CLIP (ICML-21) KgCoOp (CVPR-23) RETFound (Nature-23) FLAIR (MedIA-25) KeepFIT (MICCAI-24) RET-CLIP (MICCAI-24) UniMed-CLIP (arXiv-24) VisionFM (arXiv-24) MGLL Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP 17.31 44.66 21.49 50.82 50.21 60.16 23.24 56.11 42.39 51.48 51.27 58.94 22.54 53.44 48.92 63.38 64.99 79.62 7.28 7.96 16.37 14.85 10.24 16.02 13.68 17.84 34. 92.53 92.21 92.55 92.38 92.19 92.46 92.25 92.59 92.84 92.86 92.28 93.48 92.62 93.09 93.92 92.57 93.17 95.48 65.10 70.36 84.62 75.43 81.52 86.12 72.59 82.78 92.83 31 D.2 ZERO-SHOT COMPARISONS ACROSS MEDICAL AND NATURAL DOMAINS As summarized in Table 24, we evaluate the zero-shot classification performance of MGLL across three representative datasets to demonstrate its strong generalization ability across diverse modalities. On the COVIDx dataset Wang et al. (2020), MGLL achieves 39.0% accuracy, outperforming strong medical vision-language baselines such as CheXAgent Chen et al. (2024b) (34.3%) and MedVersa Zhou et al. (2024) (35.5%), as well as recent contrastive methods including FGCLIP Xie et al. (2025), MGCA Wang et al. (2022a), RetiZero Wang et al. (2025), MAVL Phan et al. (2024), and Ark+ Ma et al. (2025). For anatomical recognition on the CT-based OrganAMNIST dataset Yang et al. (2023), MGLL again surpasses FG-CLIP Xie et al. (2025) with significant margin (52.7% vs. 47.9%). Moreover, MGLL achieves the best performance on the natural image dataset CC3M Sharma et al. (2018) (evaluated on ImageNet), outperforming FG-CLIP Xie et al. (2025) with an accuracy of 23.5%. These results collectively highlight MGLLs flexibility and universal applicability across both medical and natural image domains in zero-shot settings. Table 24: Comparisons of Zero-Shot classification on MGLL and others methods. * denotes using published pretrained model. Method Pretrain Data Downstream Data ACC (%) FG-CLIP (ICML-25) MGLL FG-CLIP (ICML-25) MGLL CC3M CC3M PMC-OA PMC-OA ImageNet ImageNet OrganAMNIST OrganAMNIST CheXAgent (Arxiv-24) MedVersa (Arxiv-24) FG-CLIP (ICML-25) MGCA (NIPS-22) * * MIMIC-CXR MIMIC-CXR RetiZero (Nat. Com-25) MIMIC-CXR MIMIC-CXR MIMIC-CXR MIMIC-CXR MAVL (CVPR-24) Ark+ (Nature-25) MGLL COVIDx COVIDx COVIDx COVIDx COVIDx COVIDx COVIDx COVIDx 21.4 23.5 47.9 52.7 34.3 35.5 36.3 37.3 35.8 37.0 37.8 39.0 D.3 PERFORMANCE EVALUATION ON REGION SEGMENTATION We have evaluated MGLL on region segmentation tasks, and the results are reported in Table 25. Medical image segmentation is inherently challenging due to the subtle differences between adjacent pixels of heterogeneous classes. We compared our MGLL with several excellent methods such as GLoRIAHuang et al. (2021), CLIPRadford et al. (2021), LAVTYang et al. (2022), UniLSegLiu et al. (2024b), and STPNetShan et al. (2025). By incorporating multi-level semantic alignment, MGLL enhances the models language-guided spatial understanding and achieves the best performance among compared methods. Table 25: Comparisons of COVID-19 lesion segmentation between MGLL and others methods on COVID-Xray dataset Degerli et al. (2021)."
        },
        {
            "title": "Dataset",
            "content": "Dice (%) IoU(%) GLoRIA (ICCV-21) CLIP (ICML-21) LAVT (CVPR-22) UniLSeg (CVPR-24) STPNet (TIP-25) MGLL COVID-Xray COVID-Xray COVID-Xray COVID-Xray COVID-Xray COVID-Xray 79.94 79.81 79.28 79.99 80.63 81.69 70.68 70.66 69.89 70.29 71.42 73. 32 D.4 MORE DETAILED RESULTS OF MGLL IN MLLMS To evaluate MGLLs impact on multimodal large language models diagnostic capabilities, we conduct comprehensive multiple-choice evaluations across 25 distinct ophthalmological conditions as shown in Table 27 to Table 34. This expanded analysis provides more insights into MGLLs performance beyond the ten primary conditions highlighted in previous experiments. Table 26: Brief summary of recent vision-language models. Model Name InstructBLIPDai et al. (2023) Mini-GeminiLi et al. (2024b) Qwen-VLBai et al. (2023) InternVLChen et al. (2024a) LLaVALiu et al. (2024a) LLaVA-MedLi et al. (2024a) Med-FlamingoMoor et al. (2023) Janus-ProChen et al. (2025) Key Features and Description Combines vision and language with instruction tuning to enable versatile zero-shot performance across tasks. lightweight and efficient multimodal model designed for fast inference and strong performance. Supports multimodal reasoning with strong focus on Chinese vision-language understanding. Achieves strong cross-modal alignment and generalization across image-text benchmarks. Integrates CLIP and LLaMA for open-ended visual question answering and dialogue. Adapts LLaVA for medical vision-language tasks including medical image question answering. Extends Flamingo to the medical domain with few-shot learning capabilities. Uses bidirectional multimodal modeling to enhance multi-turn visual-language interactions. Our detailed analysis reveals that MGLL integration yields significant improvements across all InstructBLIP demonstrates 9.76% overall accuracy improveseven multimodal architectures. ment (55.17% to 64.94%), with particularly notable enhancements in challenging conditions such as Retinitis (11.11% to 44.44%) and Media Haze (16.13% to 45.16%). These improvements highlight MGLLs capacity to enhance feature extraction for complex ophthalmological pathologies. MGLL integration showcases substantial performance gains across multiple diagnostic tasks and architectures. Qwen-VL with MGLL integration exhibits 6.58% overall improvement (76.80% to 83.39%), with remarkable advances in low-prevalence conditions including Astigmatic Refractive Error (0.00% to 50.00%) and Hypertensive Retinopathy (0.00% to 66.67%). Even high-performing models benefit significantly, InternVL achieves 5.19% improvement with MGLL integration, enhancing diagnostic accuracy particularly for conditions such as Optic Disc Edema (45.45% to 63.64%) and Central Retinal Vein Occlusion (36.36% to 63.64%). LLaVA exhibits similarly robust baseline performance (85.22%), yet MGLL integration yields 5.55% improvement, achieving near-perfect accuracy in several categories including No Diabetic Macular Edema (90.91% to 100.00%) and miscellaneous conditions (90.00% to 100.00%). The most dramatic improvements occur in medical-specialized models. Med-Flamingo demonstrates substantial 21.76% improvement (49.17% to 70.94%), with particularly significant gains in Glaucoma (24.07% to 61.11%) and Diabetic Retinopathy (36.91% to 80.54%). Similarly, LLaVA-Med shows 20.78% improvement (56.47% to 77.25%), with exceptional gains in AMD (16.37% to 58.48%) and Diabetic Retinopathy (26.85% to 77.18%). Across all evaluated models, we observe consistent patterns of improvement that highlight MGLLs particular efficacy with complex retinal conditions, vascular pathologies, and conditions requiring fine-grained feature discrimination. The results demonstrate that MGLL provides substantial benefits regardless of the underlying model architecture. The multiple-choice evaluation framework presented models with standardized diagnostic queries. Some prompt examples of the multiple-choice evaluation benchmark are as follows: Question 1: What is the most reasonable diagnosis? A. Glaucoma B. Drusen C. Chorioretinitis D. Hypertensive Retinopathy Answer with the options letter from the given choices directly. Answer 1: A. Question 2: What diagnosis is most likely? A. Central Serous Retinopathy B. Media Haze C. Diabetic Retinopathy D. Age-related Macular degeneration Answer with the options letter from the given choices directly. Answer 2: D. Question 3: What diagnosis is most probable? A. Optic Disk Cupping B. Mild Non-Proliferative Diabetic Retinopathy C. Central Serous Retinopathy D. Central Retinal Vein Occlusion Answer with the options letter from the given choices directly. Answer 3: B. 33 AR + MGLL BRVO Cataract Chorioretinitis Table 27: Comparison of multiple-choice accuracy with MGLL in InstructBLIP on the multiplechoice evaluation benchmark. Label Name AMD 80.17% 0.00% 50.00% 80.00% InstructBLIP Dai et al. (2023) 83.63% 50.00% 62.50% 85.00% Drusen Glaucoma Health 36.67% 59.30% 51.57% 0.00% 50.00% 65.43% 60.22% 33.33% No DME 63.64% 65.63% 27.27% 50.00% 72.73% 75.00% 45.45% 50.00% CRVO CN 75.00% 45.45% 75.00% 63.64% Myopia MH 16.13% 44.25% 45.16% 52.65% Retinitis Tessellation 11.11% 44.44% 58.33% 64.94% (9.76% ) Diabetic Retinopathy 76.51% 82.55% No Glaucoma 41.67% 58.33% CSR 0.00% 28.57% No AMD 54.34% 70.10% Overall 55.17% 0.00% 33.33% DME 63.79% 74.14% Other 58.00% 70.00% 41.67% ODC ODE ODP HR AR + MGLL BRVO Cataract Chorioretinitis Table 28: Comparison of multiple-choice accuracy with MGLL in Mini-Gemini on the multiplechoice evaluation benchmark. Label Name AMD 76.61% 0.00% 43.75% 85.00% Mini-Gemini Li et al. (2024b) 82.46% 0.00% 56.25% 85.00% Drusen Glaucoma Health 23.33% 67.90% 61.46% 0.00% 46.67% 72.22% 68.99% 33.33% No DME 72.73% 62.50% 36.36% 50.00% 81.82% 78.13% 45.45% 50.00% CRVO CN 25.00% 27.27% 50.00% 54.55% MH Myopia 58.41% 38.71% 58.06% 64.16% Retinitis Tessellation 33.33% 55.56% 41.67% 70.58% (7.93% ) Diabetic Retinopathy 79.87% 84.56% No Glaucoma 66.67% 75.00% CSR 14.29% 42.86% No AMD 63.02% 72.99% Overall 62.65% 0.00% 66.67% DME 60.34% 65.52% Other 66.00% 74.00% 33.33% ODC ODE ODP HR Table 29: Comparison of multiple-choice accuracy with MGLL in Qwen-VL on the multiple-choice evaluation benchmark. Label Name Qwen-VL Bai et al. (2023) BRVO Cataract Chorioretinitis AR AMD 81.87% 0.00% 43.75% 75.00% 85.96% 50.00% 62.50% 80.00% Drusen Glaucoma Health 26.67% 78.40% 79.89% 0.00% 43.33% 87.04% 85.39% 66.67% No DME 72.73% 56.25% 27.27% 50.00% 72.73% 68.75% 54.55% 100.00% ODC ODE ODP HR 0.00% 33.33% DME 84.48% 89.66% Other 84.00% 86.00% CRVO CN 25.00% 9.09% 75.00% 36.36% Myopia MH 54.84% 76.55% 70.97% 80.97% Retinitis Tessellation 22.22% 33.33% 41.67% 83.39% (6.58% ) CSR 28.57% 42.86% No AMD 82.96% 87.14% Overall 76.80% 25.00% + MGLL Diabetic Retinopathy 80.54% 89.93% No Glaucoma 75.00% 83.33% Table 30: Comparison of multiple-choice accuracy with MGLL in InternVL on the multiple-choice evaluation benchmark. Label Name InternVL Chen et al. (2024a) BRVO Cataract Chorioretinitis AR AMD 81.29% 0.00% 37.50% 85.00% 86.55% 50.00% 56.25% 90.00% Drusen Glaucoma Health 43.33% 89.51% 85.73% 33.33% 53.33% 90.74% 91.46% 66.67% No DME 81.82% 87.50% 45.45% 50.00% 90.91% 93.75% 63.64% 50.00% ODC ODE ODP HR 0.00% 0.00% DME 87.93% 94.83% Other 90.00% 96.00% CRVO CN 25.00% 36.36% 50.00% 63.64% Myopia MH 64.52% 88.05% 67.74% 91.15% Retinitis Tessellation 44.44% 55.56% 75.00% 89.52% (5.19% ) CSR 71.43% 71.43% No AMD 85.85% 90.68% Overall 84.33% 66.67% + MGLL Diabetic Retinopathy 94.63% 96.64% No Glaucoma 91.67% 100.00% Table 31: Comparison of multiple-choice accuracy with MGLL in LLaVA on the multiple-choice evaluation benchmark. Label Name LLaVA Liu et al. (2024a) AR BRVO Cataract Chorioretinitis AMD 0.00% 83.04% 0.00% 50.00% 90.00% 33.33% 84.80% 100.00% 68.75% 90.00% DME Drusen Glaucoma Health 93.10% 36.67% 91.36% 88.65% 0.00% 96.55% 50.00% 91.98% 94.38% 33.33% Other No DME 90.91% 62.50% 18.18% 50.00% 90.00% 100.00% 62.50% 45.45% 100.00% 100.00% 66.67% 66.67% 90.77% (5.55% ) CRVO CN 25.00% 9.09% 25.00% 45.45% Myopia MH 88.50% 48.39% 61.29% 90.71% Retinitis Tessellation 44.44% CSR 42.86% 57.14% No AMD 90.68% 96.78% Overall 85.22% 58.33% ODC ODE ODP HR + MGLL Diabetic Retinopathy 87.25% 93.96% No Glaucoma 100.00% 100.00% 34 AR + MGLL BRVO Cataract Chorioretinitis Table 32: Comparison of multiple-choice accuracy with MGLL in LLaVA-Med on the multiplechoice evaluation benchmark. Label Name AMD 16.37% 100.00% 31.25% 15.00% LLaVA-Med Li et al. (2024a) 58.48% 100.00% 50.00% 65.00% Drusen Glaucoma Health 16.67% 25.31% 91.46% 33.33% 40.00% 59.26% 97.42% 33.33% No DME ODP 36.36% 21.88% 27.27% 0.00% 72.73% 40.63% 63.64% 50.00% CRVO CN 0.00% 45.45% 25.00% 63.64% Myopia MH 25.81% 23.89% 51.61% 57.08% Retinitis Tessellation 33.33% 44.44% 58.33% 77.25% (20.78% ) Diabetic Retinopathy 26.85% 77.18% No Glaucoma 16.67% 41.67% CSR 42.86% 57.14% No AMD 66.56% 78.46% Overall 56.47% 0.00% 66.67% DME 16.67% 55.17% Other 28.00% 62.00% 16.67% ODC ODE HR AR + MGLL Table 33: Comparison of multiple-choice accuracy with MGLL in Med-Flamingo on the multiplechoice evaluation benchmark. Label Name AMD 25.73% 100.00% 31.25% 30.00% Med-Flamingo Moor et al. (2023) 69.01% 100.00% 56.25% 75.00% Drusen Glaucoma Health 20.00% 24.07% 74.16% 33.33% 43.33% 61.11% 83.37% 66.67% ODP No DME 36.36% 43.75% 36.36% 0.00% 63.64% 59.38% 63.64% 50.00% CRVO CN 25.00% 63.64% 50.00% 72.73% Myopia MH 22.58% 18.58% 54.84% 45.58% Retinitis Tessellation 22.22% 44.44% 33.33% 70.94% (21.76% ) Diabetic Retinopathy 36.91% 80.54% No Glaucoma 16.67% 50.00% CSR 57.14% 71.43% No AMD 52.73% 69.13% Overall 49.17% 0.00% 33.33% DME 24.14% 51.72% Other 28.00% 70.00% BRVO Cataract Chorioretinitis 8.33% ODC ODE HR Table 34: Comparison of multiple-choice accuracy with MGLL in Janus-Pro on the multiple-choice evaluation benchmark. Label Name Janus-Pro Chen et al. (2025) BRVO Cataract Chorioretinitis AR AMD 88.30% 50.00% 56.25% 75.00% 90.64% 100.00% 62.50% 85.00% Drusen Glaucoma Health 40.00% 90.74% 96.40% 33.33% 53.33% 95.06% 96.63% 66.67% No DME 72.73% 81.25% 36.36% 50.00% 90.91% 87.50% 54.55% 50.00% ODC ODE ODP HR 33.33% 66.67% DME 62.07% 70.69% Other 82.00% 92.00% CRVO CN 25.00% 36.36% 75.00% 63.64% Myopia MH 58.06% 87.17% 67.74% 90.27% Retinitis Tessellation 33.33% 55.56% 75.00% 91.85% (3.31% ) CSR 42.86% 71.43% No AMD 92.28% 94.21% Overall 88.54% 58.33% + MGLL Diabetic Retinopathy 93.29% 96.64% No Glaucoma 58.33% 83.33% 35 Figure 5: Case Studies (Top: Case 1, Bottom: Case 2) Demonstrating MGLL Integration Impact on Diagnostic Accuracy of Different Multimodal Large Langue Models (MLLMs). 36 The Fig. 5 presents two representative case studies demonstrating the diagnostic impact of MGLL integration across multiple MLLMs. Case 1 displays fundus image with characteristic features of choroidal neovascularization (CNV), including well-defined yellowish lesion with surrounding subretinal hemorrhage in the macula. Only InstructBLIP correctly identifies this pathology in its baseline configuration, whereas five models with MGLL integration provide accurate diagnoses, demonstrating MGLLs capacity to enhance the detection of vascular abnormalities. Case 2 exhibits subtle inflammatory changes consistent with chorioretinitis, characterized by chorioretinal infiltrates against background of mild vitreous haze, which is condition none of the baseline models correctly identify. Following MGLL integration, six models accurately diagnose this inflammatory condition, with responses shifting from incorrect options (Arteriosclerotic Retinopathy, Retinitis, or Healthy) to the correct identification. D.5 MORE CAMS ON RETINAL FUNDUS DATASETS We present additional class activation maps (CAMs) from CLIP and MGLL on downstream retinal datasets in Fig. 6. These images include cases of diabetic retinopathy, diabetic macular edema, and glaucoma. Through both linear probing and full fine-tuning approaches, MGLL consistently demonstrates more precise lesion localization than CLIP, specifically highlighting pathological features rather than producing diffuse, non-specific activations. In diabetic retinopathy cases spanning mild to proliferative stages, MGLL accurately identifies microaneurysms, hemorrhages, venous beading, and neovascularization sites. While in diabetic macular edema, it effectively localizes retinal thickening and exudate formation with activation intensity proportional to disease severity. For glaucoma, MGLL appropriately focuses on optic disc abnormalities, cup enlargement, and neural rim thinningcritical diagnostic markers often missed by CLIP, which tends to highlight anatomical landmarks regardless of pathological relevance. These findings demonstrate MGLLs advantages for ophthalmological applications, offering more robust performance for clinical feature detection that supports diagnostic confidence. D.6 ABLATION STUDY ON WEIGHT FACTORS OF LOSS To investigate the impact of different weight factors in our composite loss function, we conducted an extensive ablation study using the RFMiD dataset as shown in Table 35. Our loss function incorporates three components with corresponding weight factors (α1, α2, and α3), and the results demonstrate that weight selection significantly influences model performance across all metrics. Compared to the baseline CLIP model, all our weight configurations show substantial improvements, with the optimal configuration (α1 = 0.5, α2 = 1.0, and α3 = 1.0) achieving the best performance in both linear probing (79.62% AUC, 92.84% ACC, 34.08% mAP) and full fine-tuning (92.83% AUC, 95.48% ACC, 64.99% mAP) scenarios. Notably, reducing the weight of the first component improved performance, while reducing either the second or third component weights resulted in performance degradation, suggesting that the information captured by these components is particularly valuable for medical image classification tasks and should be emphasized during training. These findings highlight the importance of appropriate loss weighting in multi-component objective functions and provide empirical evidence for the optimal configuration selection. Table 35: Ablations of Weight Factors on RFMiD. Bold indicates best performance and underline shows second-best."
        },
        {
            "title": "Method",
            "content": "Linear Probe (%) Fully Fine-tune (%) AUC ACC mAP AUC ACC mAP CLIP Radford et al. (2021) 44.66 92.53 7.28 65.10 92.86 17.31 79.29 92.83 33.82 92.51 95.35 64.57 79.62 92.84 34.08 92.83 95.48 64.99 78.11 92.79 32.42 91.46 94.99 63.28 78.85 92.80 33.39 92.01 95.18 63.77 α1: 1.0, α2: 1.0, α3: 1.0 α1: 0.5, α2: 1.0, α3: 1.0 α1: 1.0, α2: 0.5, α3: 1.0 α1: 1.0, α2: 1.0, α3: 0.5 37 D.7 ABLATION STUDIES ON THE TEMPERATURE COEFFICIENT (τ ) We have conducted the ablation studies of temperature coefficient and observe that the performance first improves and then drops as the temperature coefficient τ increases, as shown in Table 36. smaller τ sharpens the similarity distribution, enhancing discrimination but causing training instability. Conversely, larger τ produces smoother gradients but weakens alignment. The best results are achieved when τ = 0.07, which provides good balance between discriminative alignment and stable optimization."
        },
        {
            "title": "Method",
            "content": "Linear Probe (%) Table 36: Ablations of temperature coefficient (τ ) on MIDRC-XR-Portable. Fully Fine-tune (%) ACC mAP AUC 83.94 90.08 91.83 89.71 98.67 99.60 87.92 97.26 97.89 85.95 94.57 95.53 89.87 98.80 99.75 ACC mAP 22.31 78.22 30.49 88.91 28.93 87.02 27.74 86.19 30.62 89.06 CLIP, τ = 0.07 MGLL, τ = 0.05 MGLL, τ = 0.20 MGLL, τ = 0.50 MGLL, τ = 0.07 AUC 71.43 83.55 81.89 79.52 83.86 D.8 PERFORMANCE ON DATASETS WITH ARTIFICIALLY INTRODUCED NOISE We evaluated the robustness of MGLL under varying levels of artificially introduced noise, where 10%30% of granularity labels were randomly removed. As shown in Table 37, even with 30% missing labels, MGLL achieves AUCs of 79.61% (linear probing) and 96.74% (full fine-tuning), which remain substantially higher than CLIP trained with complete labels (71.43% and 91.83%, respectively). These results demonstrate that MGLL maintains strong robustness against incomplete or noisy granularity supervision. Method Table 37: Ablations of missing granularity labels on MIDRC-XR-Portable. Fully Fine-tune (%) Linear Probe (%) ACC mAP AUC 83.94 90.08 91.83 89.30 98.39 99.31 88.74 97.95 98.62 87.28 96.02 96.74 89.87 98.80 99.75 CLIP, No Missing MGLL, 10% Missing MGLL, 20% Missing MGLL, 30% Missing MGLL, No Missing ACC mAP 22.31 78.22 29.97 88.15 28.86 87.25 27.75 86.23 30.62 89.06 AUC 71.43 82.58 81.14 79.61 83. D.9 PERFORMANCE ON DATASETS WITH MIXING GRANULARITY LEVELS We have evaluated MGLL on datasets with mixed granularity levels, and the results are reported in Table 38. Specifically, the pretraining dataset was randomly divided into two subsets of equal size: Set with two levels of granularity and Set with single level. MGLL achieves comparable performance across both subsets and their combination, demonstrating its robustness to heterogeneous annotation structures and its applicability to real-world scenarios with mixed-granularity data. Table 38: Ablations of mixing granularity levels on MIDRC-XR-Portable. The dataset is randomly divided into two subsets of equal size for the ablation study, Set (50% data) and Set (50% data). Study Desc. Series Desc. Set Set Set + Set Set Set + Set Set MGLL (Ours) Linear Probe (%) AUC 78.93 80.25 80.92 83.86 ACC mAP 27.04 85.47 27.97 86.46 28.63 87.07 30.62 89.06 Fully Fine-tune (%) ACC mAP AUC 85.14 93.58 94.81 86.72 94.87 96.21 87.45 96.56 97.03 89.87 98.80 99."
        },
        {
            "title": "E DISCUSSION AND FUTURE WORK",
            "content": "Our investigation into Multi-Granular Language Learning (MGLL) reveals several important insights about vision-language alignment in complex domains. The consistent performance improvements across various medical imaging datasets demonstrate that hierarchical textual information substantially enhances visual understanding, particularly when images correspond to multiple clinical findings at different levels of specificity. The ablation studies confirm that performance gains scale with both the number of granularity levels and the quality of input data, suggesting that MGLL effectively leverages the complementary information contained in multi-granular textual descriptions. While MGLL achieves simultaneous multi-label and cross-granularity alignment without additional computational cost, further optimization could potentially improve its generality. Future work should explore several directions: (1) extending MGLL to incorporate multimodal inputs beyond images and text, such as patient metadata or temporal information; (2) investigating domain adaptation techniques to improve generalization to unseen medical conditions or imaging modalities; and (3) exploring the integration of MGLL with large language models to generate more nuanced textual descriptions at multiple granularities. Additionally, applying MGLL to other domains with inherently hierarchical structures, such as satellite imagery or scientific visualization, could further validate its broader applicability beyond medical imaging. These extensions would strengthen MGLLs position as generalizable framework for improved multimodal understanding. 39 Figure 6: More Class Activation Maps from CLIP and Proposed MGLL."
        }
    ],
    "affiliations": [
        "Duke University",
        "University of Washington"
    ]
}