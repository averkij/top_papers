{
    "paper_title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
    "authors": [
        "Xin Yu",
        "Xiaojuan Qi",
        "Zhengqi Li",
        "Kai Zhang",
        "Richard Zhang",
        "Zhe Lin",
        "Eli Shechtman",
        "Tianyu Wang",
        "Yotam Nitzan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation."
        },
        {
            "title": "Start",
            "content": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation Xin Yu1,2 Xiaojuan Qi1* Zhengqi Li2 Kai Zhang2 Richard Zhang2 Zhe Lin2 Eli Shechtman2 Tianyu Wang2 Yotam Nitzan2 1The University of Hong Kong 2Adobe Research 5 2 0 2 6 2 ] . [ 1 4 7 3 2 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce the Self-Evaluating Model (Self-E), novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to Flow Matching model, while simultaneously employing novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering unified framework for efficient and scalable generation. 1. Introduction Diffusion models [19, 53, 54] and flow matching models [26, 29] currently dominate text-to-image generation due to their stability, scalability, and strong visual fidelity [2, 40, 60, 61]. These models are trained to approximate local supervision from data either the score function or the instantaneous velocity field which specifies how noisy sample should infinitesimally move toward the data manifold at each timestep. Because this supervision is inherently *Corresponding author. Project lead. local, it provides only short-range guidance: each update corrects small deviations but lacks holistic global view of the target distribution. Consequently, diffusion and flowbased models typically require dozens of sequential steps to reliably traverse the curved reverse trajectory from noise to data, making inference computationally expensive and limiting their use in time-sensitive applications. dominant strategy for reducing inference steps is distillation, where pretrained teacher supervises student model [25, 34, 4346, 63, 64]. Although these methods differ technically, they share core principle: the student is optimized with global objectives that match the teachers distributions or trajectories, rather than data-derived local velocities, so that it can perform few-step inference. key limitation, however, is the reliance on strong pretrained teacher. This has recently motivated growing interest in self-contained, from-scratch training frameworks that natively yield few-step models. prominent line of work is consistency-based methods [4, 15, 22, 30, 42, 55, 59], which essentially learn the underlying flow maps [4] or, equivalently, the average velocity [15] between two points along the reverse trajectory so that, in principle, the model can follow one-step shortcut instead of integrating many instantaneous velocities at test time. However, these objectives are typically unstable to optimize from scratch [59, 72] or suffer from quality degradation [70], and have so far scaled reliably only on simpler benchmarks such as ImageNet [8], while large text-to-image systems that do succeed in this regime still rely heavily on distillation [42, 70], undermining the original teacher-free motivation. In this paper, we present the Self-Evaluating Model (SelfE, pronounced like selfie) novel, self-contained, fromscratch training framework enabling any-step text-to-image inference. The model learns simultaneously from data, which provides local velocity supervision, and through novel self-evaluation mechanism supervising the global distribution. The core idea is conceptually simple yet powerful: the model evaluates its own generated samples using its current local score estimate, effectively serving as dynamic self-teacher. This self-evaluation becomes an increasingly 1 Figure 1. Qualitative Any-Step Generation. We showcase diverse text-to-image results from our model at different inference step counts, demonstrating coherent semantics, strong text alignment. Text prompts are provided in the supplementary material. accurate guidance signal allowing the model to improve itself. By combining instantaneous local learning with selfdriven global matching, Self-E naturally bridges the gap between flow-based and distillation-based paradigms. As result, it can be trained entirely from scratch while supporting any-step text-to-image inference, generating high-quality images even at very low step counts (see Fig. 1). To the best of our knowledge, Self-E is the first native any-step text-to-image model, concurrent with TiM [59]. We conduct extensive experiments on large-scale text-to-image generation and show that Self-E achieves both strong fewstep quality and graceful scaling across inference budgets. In the few-step (< 8) setting, Self-E surpasses the performance of diffusion and flow-based models including FLUX1-dev [2], SDXL [37], SANA [61], the distillation-based LCM [33], and concurrent any-step model TiM [59]. Remarkably, although targeting this few-step generation, Self-E is also competitive or even surpasses some state-of-the-art flow-based methods at the 50 step setting. We also note that Self-Es performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within single unified model. 2. Background: Flow Matching Generative models aim to learn parameterized distribution pθ(x0c) that approximates the real data distribution q(x0c) where is condition such as text prompt. Flow matching and diffusion models achieve this by learning the instantaneous velocity field, or equivalently the score function, induced by continuous-time forward diffusion process. Specifically, given real data samples x0 q(x0c), these models define trajectory indexed by [0, 1]: xt = αtx0 + σtϵ, ϵ (0, I), (1) where coefficients (αt, σt) form noise scheduler and defines the noisy distribution q(xtc). Flow matching specifically refers to particular parameterization that explicitly matches the velocity field: vt(xt) := dxt dt = dαt dt x0 + dσt dt ϵ, (2) where coefficients (αt, σt) = (1 t, t). Conditional Flow Matching (CFM) trains neural network Vθ(xt, t, c) to predict the marginal velocity field (i.e., the expectation of instantaneous velocity) [26] by minimizing the mean squared error between predicted and conditional velocities: LCFM(θ) = Et,x0,ϵ (cid:2)Vθ(xt, t, c) vt(xt)2(cid:3) . (3) At inference time, we use the predicted velocity to follow the trajectory and generate samples. However, because the velocity is local quantity, single-step estimate of the original sample x0 often falls short. Intuitively, single step 2 Figure 2. Self-Evaluating Model. (a) Overview. The model is trained with two complementary objectives: learning from data (b) and self-evaluation (c). (b) Learning from data. Given real sample x0, we add noise to obtain xt and train Gts θ with an x0-prediction loss, providing local trajectory supervision. (c) Self-evaluation with classifier score. When < t, we re-noise the generated ˆx0 to ˆxs and run the same network in evaluation mode (stop-gradient) twice: once with condition and once with the null prompt ϕ. The difference between these outputs yields self-evaluation score, which is treated as feedback gradient on ˆx0 and back-propagated through the denoising path, enforcing global distribution matching in teacher-free manner. only captures the immediate direction and cannot account for the curvature of the trajectory, so it typically recovers just the average of the possible original samples. Formally, naive one-step estimate is ˆx0 = xt Vθ (xt, t, c) E[x0xt, c], (4) where θ minimizes Eq. (3). 3. Self-Evaluating Model We introduce the Self-Evaluating Model, novel text-toimage pretraining approach enabling flexible, any-step inference. As illustrated in Fig. 2(a), the core idea is simple yet effective: the model simultaneously learns from data while performing self-evaluation. Conceptually, the loss function of our model is formulated as: L(θ) = Ldata(θ) + λLself-evaluate(θ). (5) The learning-from-data component Ldata(θ) (Sec. 3.1) provides local trajectory supervision, effectively estimating the conditional expectation E[x0xt, c]. Meanwhile, the selfevaluation component Lself-evaluate (Sec. 3.2) targets global distribution matching, encouraging the model-generated output ˆx0 to be realistic sample drawn from the true distribution q(x0). We demonstrate that surprisingly, this can be achieved through self-evaluation of the generated images by the model itself. Model Parametrization. Formally, given noisy input xt, we train model Gθ(xt, t, s, c) to predict the clean data sample ˆx0 = Gθ(xt, t, s, c), parameterized as: ˆx0 = Gθ(xt, t, s, c) = xt Vθ(xt, t, s, c), (6) where Vθ(xt, t, s, c) denotes neural network analogous to Vθ from Eq. (3), but noted distinctively as it takes in two time variables. Our two time variables intuitively remind of self-consistency-based models [12, 15, 22, 42], but here they serve fundamentally different purpose. Self-consistency methods essentially learn specific underlying Flow Map [4] or an average velocity [15] along the reverse trajectory, i.e., the integral of local velocities. In contrast, our goal is to directly predict samples whose marginal distribution pθ(xsc) matches the real distribution q(xsc), without constraining the reverse transition to follow any particular trajectory. 3.1. Learning from Data Our model is always trained on real data using the conditional flow matching loss in Eq. (3) which is equivalent to learning expectation of x0 prediction from Gθ(xt, t, s, c) through Ldata(θ) = Es,t,x0,ϵ (cid:2)Gθ(xt, t, s, c) x02(cid:3) . (7) where are randomly sampled during training. In particular, when = t, our model is optimized solely by this loss. The optimally trained Gθ(xt, t, t, c) serves as an estimate of the conditional expectation E[x0xt, c]. However, the expectation itself may not be meaningful sample in q(x0c). Since this supervision is derived from the data distribution, we refer to this process as learning from data. 3.2. Learning by Self-Evaluation When < t, we introduce another objective which targets at global distribution matching. We interpret ˆx0 = Gθ(xt, t, s, c) as sample from an implicit distribution pθ(x0xt, t, s, c). Our goal is then to ensure that the marginal distribution: (cid:90) (cid:90) pθ(xsc) = q(xsx0)pθ(x0xt, t, s, c)q(xtc)dxtdx0 (8) closely matches the real distribution q(xsc). To accomplish this, we consider the reverse KL divergence between pθ(xsc) and q(xsc): DKL (cid:0)pθ(xsc)q(xsc)(cid:1) = Exspθ(xsc) [log pθ(xsc) log q(xsc)] . (9) 3 The gradient of this KL divergence for per-sample optimization involves the difference between corresponding score functions: δ(ˆxs) = ˆxs log pθ(ˆxsc) ˆxs log q(ˆxsc), (10) where we denote ˆxs as sample from pθ(xsc). Key Observation. Both score functions in Eq. (10) are intractable in practice. Specifically, ˆxs log q(ˆxsc) represents the real-data score, which serves as the key driving force directing the sample towards regions of higher data density. In contrast, ˆxs log pθ(ˆxsc) is termed the fake score, guiding the sample away from its current position and typically preventing mode collapse. To make optimization possible, prior methods use pre-trained diffusion to model real-data score, i.e., distill from teacher model [58, 63, 64]. We argue that, obtaining perfect real score is unnecessary; instead, we leverage the currently trained model Gθ(xs, s, s, c) to provide feedback for global distribution matching, which is self-evaluation process. Formally, according to Tweedies formula [6, 10, 39], the score function is related to the conditional expectation: xs log q(xsc) = αsE[x0xs, c] xs σ2 . (11) Note that our current model Gθ(xs, s, s, c) progressively learns the expectation E[x0xs, c] from the data, so we can use it to approximate the real score. Although this estimate is not fully accurate before convergence, it can still effectively guide training, since the student model itself is also far from converged in the early stages. Moreover, in practice the real score is typically evaluated under classifier-free guidance (CFG), and the reverse KL objective is inherently mode-seeking. Together, these properties provide stronger guidance for model optimization. Self-Evaluation Score. We now concretely describe how we use the in-training model Gθ(xs, s, s, c), which progressively learns the expectation E[x0xs, c], to approximate the score terms in Eq. (10) and Eq. (11). In common practice [63 65], the real score is evaluated within conditionally sharpened distribution via classifier-free guidance (CFG) [18], defined as: ˆxs log qw(ˆxsc) = ˆxs log q(ˆxsc) + (ω 1)ˆxs log (12) q(ˆxsc) q(ˆxsϕ) , where ω is guidance scale, ϕ is null prompt, denoting the unconditional distribution. By subtracting the fake score and applying appropriate transformations, we rewrite Eq. (10) into two distinct terms: which we call classifier score term and an auxiliary term, i.e.: 4 δ(ˆxs) = (ω 1) (cid:0)ˆxs log q(ˆxsϕ) ˆxs log q(ˆxsc)(cid:1) (cid:125) (cid:124) (cid:123)(cid:122) Classifier score term + (cid:0)ˆxs log pθ(ˆxsc) ˆxs log q(ˆxsc)(cid:1) (cid:125) (cid:123)(cid:122) Auxiliary term (optional) (cid:124) . (13) Empirically, we observe that using only the classifier score term is sufficiently effective and even improves convergence (see Tab. 2). This observation is consistent with prior work [65], which also found classifier scores effective when performing score distillation for 3D generation [38]. Consequently, we omit the auxiliary term and thereby avoid co-training an additional model for the fake score during the early stages of training. Although this setting no longer corresponds to exact distribution matching, it still provides meaningful learning signal: intuitively, the classifier score encourages the model to generate samples that align with an implicit classifier q(cx) [18, 65]. The fake score primarily helps prevent mode collapse; in our case, we note that the learning-from-data component can already fulfill this role. When the model is close to convergence, i.e., in later training stages, we can optionally re-introduce the auxiliary term to perform more accurate distribution matching, which helps reduce artifacts (see Fig. 6). Even then, we do not require an additional copy of the model; instead, we simply utilize specialized prompt to estimate the generated score. We provide more details about this case in the supplementary material. We now formally describe our practical implementation of the self-evaluation score using the in-training network. The detailed procedure of self-evaluation with only the classifier score is illustrated in Fig. 2(c). We employ two stopgradient forward passes with self-generated samples as input. In particular, we add noise to the generated sample ˆx0: ˆxs = αs ˆx0 + σsϵ, and define pseudo-target as: xself := sg[ˆx0 [Gθ(ˆxs, s, s, ϕ) Gθ(ˆxs, s, s, c)]], (14) where sg denotes the stop-gradient operation. Minimizing the mean squared error (MSE) between this pseudo-target and our models prediction induces gradient with respect to ˆxs that precisely matches the desired direction, i.e., the classifier score in Eq. (13). We provide the proof in the appendix. Thus, our self-evaluation loss is expressed as: Lself-evaluate(θ) = Et,s,x0,ϵ (cid:2)Gθ(xt, t, s, c) xself2(cid:3) . (15) 3.3. Final Objective Our final per-sample objective is hybrid loss function that combines data-driven supervision with global distribution matching via self-evaluation. Formally, it is defined as: 4. Experiment Ls,t(θ) = ˆx0 x02 2 + λs,t ˆx0 xself2 2, (16) where the weight λs,t controls the relative contribution of the self-evaluation term and is given by: λs,t = σt αt σs αs . (17) Note that λs,t = 0 when = s, in which case the objective reduces to purely data-driven reconstruction loss. In practice, large values of λs,t can overpower the datadriven loss, leading to undesired color bias. To mitigate this effect, we introduce an energy-preserving normalization of the effective training target, inspired by Zhang et al. [66], which addresses similar issue with high CFG values. From the gradient of Eq. (16), the implicit regression target can be expressed as: xtar = x0 + λs,txself 1 + λs,t . (18) We normalize this target to preserve the energy of the clean sample x0: xrenorm = x0 + λs,txself x0 + λs,txself2 x02. (19) Empirically, this normalization yields slightly improved visual quality and stability (see Tab. 2). Replacing xtar with xrenorm, our practical per-pair objective becomes: Ls,t(θ) = ˆx0 xrenorm2 2. (20) Finally, the overall training loss is obtained by averaging over all possible timestep pairs: L(θ) = Es,t [ws,t Ls,t(θ)] , (21) where ws,t denotes the sampling weight for each pair (s, t). 3.4. Inference Our model supports inference with an arbitrary number of steps by iteratively removing noise, similar to diffusion and flow matching models. Given predefined inference step budget and corresponding time scheduler {tk}, where 1 = t1 > t2 > > tN = 0, we sequentially predict denoising direction at each timestep tk and take step towards the next timestep tk+1. Formally, each inference step is defined as: xtk+1 = xtk (tk tk+1)Vθ(xtk , tk, sk, c). (22) By default, we set the target timestep sk to be the next timestep tk+1. Nevertheless, we find that setting the timestep sk to other values in the interval [tk+1, tk] might lead to improved results in some cases. We demonstrate this phenomenon and provide some suggestions to setting this hyperparameter in the supplementary material. We employ energy-preserving classifier-free guidance [66] with ω = 5. 5 We conduct two complementary sets of experiments to validate our approach. First, we train 2B-parameter model with 512 512-resolution images and compare against stateof-the-art text-to-image models spanning the landscape of training paradigms (Sec. 4.1). Second, we perform controlled ablation studies with 0.5B-parameter models under identical training conditions to isolate the contributions of key design choices (Sec. 4.2). In both, we adopt latent transformer architecture similar to FLUX [2, 11], with minor modifications to accommodate the additional timestep input s, which mirrors the typical handling of timestep input t. Additional details about architecture, data, and hyperparameters are provided in the supplementary material. 4.1. Comparison with Prior Work We compare our method with several state-of-the-art textto-image approaches spanning the landscape of training paradigms. Most closely related to our setting are fromscratch any-step methods. Since all published works in this category have been demonstrated only on small-scale datasets, such as CIFAR10 [24] and ImageNet [15, 72], we compare our model with the concurrent Transition Models (TiM) [59], which are the first to scale this family of approaches to text-to-image generation. In addition, we include standard flow-matching and diffusion baselines FLUX.1dev [2], SDXL [37], and SANA-1.5 [61]. Finally, we compare with Latent Consistency Models (LCM) [33], SDXLTurbo [48], and SD3.5-Turbo [47], which employ different distillation methods from the pretrained Stable Diffusion model [40] for few-step sampling. Note that these models, as well as other distillation-based approaches [63, 64], are not trained from scratch and require pretrained teacher. Following the evaluation protocol of Deng et al. [7], we report quantitative results on the GenEval benchmark [16]. As shown in Tab. 1, our method consistently outperforms other methods across all inference step counts, achieving notably higher scores overall. In the few-step regime, our model outperforms the second-best method by large margin. In Fig. 3, we visualize generated images from representative methods across 2, 4, 8, and 50 inference steps. Our approach consistently produces high-quality, detailed, and text-coherent images at all step counts. In the extreme few-step setting (2 steps), FLUX, SANA, and SDXL fail to generate meaningful images, while LCM and TiM produce recognizable objects but suffer from significant degradation in structure and semantic coherence. In contrast, our method yields clear, semantically aligned, and visually detailed results even under this challenging configuration. As the number of inference steps increases to 4 and 8 steps, all methods progressively improve, yet our approach maintains clear advantage in both detail and text alignment. At 50 steps, our model attains image quality comparable or better than SANA, and SDXL, Figure 3. Qualitative Any-Step Comparison. Generated images from all methods at various inference steps. Our approach consistently produces detailed, semantically accurate, and visually appealing images aligned with textual prompts at all step counts. In extremely few-step scenarios (e.g., 2-step), FLUX, SANA, and SDXL fail to generate recognizable results, while LCM and TiM exhibit semantic and structural degradation. When using more inference steps, all methods improve, but our method retains superior quality, realism, and text alignment. At 50 steps, normal Flow Matching realm, our method is competitive with FLUX, despite FLUX being much larger model. while LCM and TiM exhibit saturation artifacts. 4.2. Ablation Studies To isolate the advantages of our approach over alternatives and to assess the effects of key design choices, we conduct controlled ablation experiments. We use 0.5B-parameter models trained on identical datasets under consistent conditions, enabling direct comparison without confounding factors. All ablations use 256 256 resolution and batch size 1024. Other settings follow the setup in Sec. 4.1 and are detailed in the supplementary material. of this family the recently proposed, Inductive Moment Matching (IMM) [72], as our baseline. IMM can be viewed as an extension of trajectory-based models to the distribution level via moment matching. We report GenEval results in Tab. 2 and provide corresponding qualitative comparisons in Figure 4. Our approach consistently outperforms both Flow Matching and IMM across all step counts. In Figure 5, we further plot GenEval scores for our method and Flow Matching throughout training, clearly demonstrating that our approach not only converges to superior performance but also maintains this advantage throughout the entire training. Comparison with Pretraining Alternatives. Here we compare our approach with two paradigms: standard Flow Matching and native few-step methods. For native few-step methods, we choose to experiment with second method Design Choices. We further investigate two design choices and analyze their individual effects by training each variant for 100k iterations. In Tab. 2 we present results comparing models trained without energy-preserving target normal6 Table 1. Quantitative Comparison on GenEval [16]. Our method is consistently SOTA across all step counts and improves monotonically with more steps on GenEval Overall (24850: 0.7530.7810.7850.815). Notably, we achieve large margins in the few-step regime (e.g., +0.12 at 2-step over the best prior methods), while remaining the top performer at 8 and 50 Steps. Steps Method Overall Single Object Two Object Attribute Binding Colors Counting Position 2 Steps 4 Steps 8 Steps 50 Steps SDXL [37] FLUX.1-Dev [2] LCM [33] SANA-1.5 [61] TiM [59] SDXL-Turbo [48] SD3.5-Turbo [47] Ours SDXL [37] FLUX.1-Dev [2] LCM [33] SANA-1.5 [61] TiM [59] SDXL-Turbo [48] SD3.5-Turbo [47] Ours SDXL [37] FLUX.1-Dev [2] LCM [33] SANA-1.5 [61] TiM [59] SDXL-Turbo [48] SD3.5-Turbo [47] Ours SDXL [37] FLUX.1-Dev [2] LCM [33] SANA-1.5 [61] TiM [59] SDXL-Turbo [48] SD3.5-Turbo [47] Ours 0.0021 0.0998 0.2624 0.1662 0.6338 0.4622 0.3635 0.7531 0.1576 0.3198 0.3277 0.5725 0.6867 0.4766 0.7194 0.7806 0.3759 0.5893 0.3398 0.7788 0.7143 0.4652 0.7071 0.7849 0.4601 0.7966 0.3303 0.8062 0.7797 0.3983 0.6114 0.8151 0.0130 0.2969 0.7937 0.5531 0.9469 0.9781 0.7125 0. 0.5281 0.6469 0.9344 0.9219 0.9531 0.9781 0.9344 0.9688 0.8812 0.8844 0.9281 0.9812 0.9656 0.9688 0.9437 0.9688 0.9688 0.9781 0.8938 0.9844 0.9656 0.9156 0.8656 0.9875 0.0000 0.0227 0.0985 0.0707 0.7071 0.3308 0.2879 0.8838 0.0758 0.2955 0.1667 0.6313 0.7601 0.4040 0.8510 0.9141 0.2702 0.7298 0.1818 0.8864 0.8232 0.3763 0.8232 0. 0.4217 0.9318 0.2247 0.9192 0.8864 0.2980 0.7449 0.9394 0.0000 0.0025 0.0050 0.0075 0.4375 0.1500 0.1650 0.5900 0.0125 0.0550 0.0150 0.2525 0.5225 0.1400 0.5650 0.6250 0.0675 0.2175 0.0300 0.5800 0.5750 0.1300 0.5450 0.6225 0.1300 0.5600 0.0075 0.7175 0.7300 0.0700 0.4050 0.6700 0.0000 0.1835 0.4761 0.2234 0.8723 0.7527 0.5691 0. 0.2606 0.4202 0.5372 0.6968 0.9016 0.7713 0.7952 0.8936 0.6569 0.7314 0.5319 0.9202 0.8936 0.7500 0.8271 0.8830 0.8138 0.9096 0.5319 0.9229 0.9069 0.6702 0.6995 0.8910 0.0000 0.0656 0.1812 0.1125 0.4188 0.4594 0.2812 0.6094 0.0437 0.2437 0.2656 0.5125 0.5031 0.4562 0.5656 0.6219 0.2594 0.4625 0.3094 0.6750 0.5156 0.4562 0.5312 0. 0.3312 0.7500 0.2812 0.7031 0.6344 0.3563 0.4281 0.7000 0.0000 0.0275 0.0200 0.0030 0.4200 0.1025 0.1650 0.6325 0.0250 0.2575 0.0475 0.4200 0.4800 0.1100 0.6050 0.6600 0.1200 0.5100 0.0575 0.6300 0.5125 0.1100 0.5725 0.6525 0.0950 0.6500 0.0425 0.5900 0.5550 0.0800 0.5250 0.7025 ization (i.e. using Eq. (16) instead of Eq. (20)) and models trained with the auxiliary term from Eq. (13) included throughout all training iterations. We observe that target normalization generally improves performance, except in the extreme two-step inference setting, so we adopt this strategy for all our experiments. In contrast, introducing the auxiliary term from scratch significantly degrades performance. Therefore, we rely primarily on the classifier score in the early stages of training, which both reduces computational cost and stabilizes optimization. However, we find that incorporating the auxiliary term in later training stages is beneficial, notably mitigating oversaturated stripe artifacts in two-step generations, as shown in Fig. 6. Consequently, we adopt this hybrid schedule classifier-score-only early, with including auxiliary term for refinement later as our final training strategy in the main experiments. 5. Related Work Diffusion and Flow Matching. Diffusion models [19, 49, 52, 54] and flow-matching models [1, 2628] have become two of the most popular frameworks for generative modeling in recent years. These models are trained to learn either score function or velocity field that reverses noising process, transporting samples from the clean data distribution back to simple prior distribution such as Gaussian. Both diffusion and flow-matching approaches have been successfully scaled to wide range of generative tasks, including 7 Figure 4. Controlled Ablation Study. We compare our method to alternative pretraining methods - Flow Matching and IMM. Full prompts appear in supplementary. Our method produces favorable results across all step budgets. Table 2. Controlled Ablation Study. We report overall scores on GenEval [16]. The upper block compares our method with two alternative design choices of omitting the target normalization or incorporating the auxiliary term throughout all training steps. Reported after 100K iterations. The bottom block compares our method with alternative pretraining methods - Flow Matching and IMM. Reported after 300K iterations. Method 2 Steps 4 Steps 8 Steps 50 Steps 100k Iterations w/o target norm. w/ aux. term Ours 300k Iterations Flow Matching [26] IMM [72] Ours 0.5555 0.3307 0.5439 0.2523 0.2617 0.6097 0.6156 0.4304 0.6381 0.6075 0.5994 0.7121 0.6521 0.5153 0. 0.7155 0.7112 0.7490 0.7018 0.6166 0.7160 0.7311 0.7472 0.7543 text-to-image synthesis [2, 5, 11, 37, 40, 71], text-to-video generation [3, 13, 23, 36, 56, 62], and large language modeling [35]. Despite their impressive performance, diffusion and flow-matching models are fundamentally designed to predict local properties of the data distribution. As result, they typically require many iterative denoising steps to produce high-quality samples, which can pose significant computational challenges during inference. Accelerating Diffusion/Flow Matching. There has been rich body of literature focused on reducing the number of denoising steps required by diffusion and flow matching. Training-free approaches typically employ high-order solvers to better approximate the underlying differential equations [9, 21, 31, 32, 41, 67, 69], but these methods still struggle to achieve high-quality samples within ten denoising steps. Another major line of work aims to accelFigure 5. Training Progress Comparison. GenEval scores across different inference steps (2, 4, 8, and 50) for our method and Flow Matching over training iterations (from 50k to 300k). Our approach consistently outperforms Flow Matching at all inference steps, indicating its superior effectiveness and robustness. Figure 6. (Left) Models trained only with the classifier score component from Eq. (13) have clear checkerboard artifacts in extreme few-step regime, 2 steps in this example. (Right) Incorporating the auxiliary term from Eq. (13) in later stages of training helps mitigating these artifacts. Results are from our 2B model. erate diffusion models through distillation. Early distillation techniques train student model to match the long-step transitions along the trajectory produced by multi-step teacher [26, 43]. Consistency Models (CMs) and their variants [14, 30, 51, 55] instead learn direct flow map that transports noisy input directly to its corresponding clean sample by following the PF-ODE trajectory. Flow-map models further generalize this paradigm by learning mappings between arbitrary pairs of points (s, t) along the PF-ODE trajectory [4, 12, 17, 20, 22, 42, 57, 68]. More recent work, such as TiM [59] and MeanFlow [15], attempts to learn such flow-map models through large-scale pre-training; however, we observe that these techniques remain difficult to scale effectively to text-to-image generation. Another approach to obtain few-step models is distribution-matching distillation [4446, 63, 64, 73, 74], where different divergence metrics are employed as training losses and applied to samples at different noise levels to move student generated sample towards teachers learned distribution. Our work is inspired by the distribution-matching viewpoint, but differs in that we apply this idea during the pre-training stage of text-to-image models. 8 6. Conclusion In this paper, we introduce the Self-Evaluating Model (SelfE), novel pretraining framework for text-to-image generation capable of flexible, any-step inference entirely from scratch. Departing from prior approaches dependent on pretrained teacher models, Self-E leverages dynamically learned local scores to self-assess generated samples, establishing an internal feedback loop that seamlessly integrates local trajectory learning with global distribution matching. Comprehensive evaluations on the GenEval benchmark demonstrate Self-Es state-of-the-art performance across diverse inference budgets, particularly excelling in few-step generation scenarios. Furthermore, Self-Es performance monotonically improves with increased inference steps, indicating its capability to scale from rapid generation to high-quality long-trajectory sampling. We hope Self-E offers fresh perspective on designing teacher-free pretraining methods for any-step image generation and inspires future work on transferring such self-evaluating models to downstream tasks."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 7 [2] Black-Forest-Labs. Flux.1 [dev], 2024. 12B parameter rectified flow transformer, text-to-image. 1, 2, 5, 7, 8, 13 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 8 [4] Nicholas M. Boffi, Michael S. Albergo, and Eric VandenEijnden. Flow map matching with stochastic interpolants: mathematical framework for consistency models. Trans. Mach. Learn. Res., 2025, 2024. 1, 3, 8 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 8 [6] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. Advances in Neural Information Processing Systems, 35:2568325696, 2022. [7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 5 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 1 [9] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. Advances in Neural Information Processing Systems, 35:3015030166, 2022. 8 [10] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. 4 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 5, 8, 13 [12] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. 3, 8 [13] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [14] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. 8 [15] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. 1, 3, 5, 8 [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 5, 7, 8 [17] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. ArXiv, abs/2403.06807, 2024. 8 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 7 [20] Zheyuan Hu, Chieh-Hsin Lai, Yuki Mitsufuji, and Stefano Ermon. Cmt: Mid-training for efficient learning of consistency, mean flow, and flow map models. ArXiv, abs/2509.24526, 2025. 8 [21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [22] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 1, 3, 8 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 8 [24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. 5 [25] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 1 9 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 2, 7, 8 [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [28] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [29] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 1 [30] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 1, 8 [31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022. 8 [32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. 8 [33] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2, 5, 7 [34] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1429714306, 2023. [35] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 8 [36] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in 200k. arXiv preprint arXiv:2503.09642, 2025. 8 [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, 5, 7, 8 [38] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 4 [39] Herbert Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388394. Springer, 1992. 4 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 5, [41] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your steps: Optimizing sampling schedules in diffusion models. arXiv preprint arXiv:2404.14507, 2024. 8 [42] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. ArXiv, abs/2506.14603, 2025. 1, 3, 8 [43] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 1, 8 [44] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. ArXiv, abs/2406.04103, 2024. 8 [45] Axel Sauer, Dominik Lorenz, A. Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, 2023. [46] Axel Sauer, Frederic Boesel, Tim Dockhorn, A. Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. SIGGRAPH Asia 2024 Conference Papers, 2024. 1, 8 [47] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 5, 7 [48] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2024. 5, 7 [49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. 7 [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 15 [51] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. Improved techarXiv preprint [52] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 7 [53] Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution, 2020. 1 [54] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 7 [55] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 1, 8 [70] Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation via score-regularized continuous-time consistency. arXiv preprint arXiv:2510.08431, 2025. 1 [71] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 8 [72] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. 1, 5, 6, 8 [73] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. ArXiv, abs/2410.14919, 2024. 8 [74] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. ArXiv, abs/2404.04057, 2024. 8 [56] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 8 [57] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li, and Xiaogang Wang. Phased consistency models. In Neural Information Processing Systems, 2024. [58] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 4 [59] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, and Lei Bai. Transition models: Rethinking the generative learning objective. arXiv preprint arXiv:2509.04394, 2025. 1, 2, 5, 7, 8 [60] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 1 [61] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 1, 2, 5, 7 [62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 8 [63] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:47455 47487, 2024. 1, 4, 5, [64] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 1, 4, 5, 8 [65] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, SongHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023. 4 [66] Kai Zhang, Fujun Luan, Sai Bi, and Jianming Zhang. Ep-cfg: Energy-preserving classifier-free guidance. arXiv preprint arXiv:2412.09966, 2024. 5 [67] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. 8 [68] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. ArXiv, abs/2402.19159, 2024. 8 [69] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpmsolver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36:5550255542, 2023."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides additional details and results complementing the main paper. In Sec. S.1, we provide proofs showing that our proposed self-evaluation loss correctly induces the desired optimization gradients. We distinguish two scenarios: deriving the classifier-score gradient and deriving the full reverse KL divergence gradient. For the latter, we include additional implementation details on training. Sec. S.2 contains extended information on our training and inference implementation. In Sec. S.3, we present additional experimental results and further discussions regarding the choice of the second timestep input. Prompts corresponding to image examples shown in the main paper are provided in Sec. S.4. Finally, in Sec. S.5, we discuss limitations of our method and propose directions for future work. S.1. Derivation of the Self-Evaluation Loss Setup. We follow the forward noising in Eq. (1) and the model parameterization in Eqs. (6)(7) (main paper). Throughout, we use the same network head Gθ as in the main text, and stop-gradient is denoted by sg[]. All gradients are taken w.r.t. xs that is obtained by re-noising the model prediction ˆx0 as in Sec. 3.2. Posterior means. By Tweedies formula applied to Eq. (1), the (data) conditional and unconditional posterior means, and the (model) conditional posterior mean, satisfy Eq[x0xs, c] = Eq[x0xs] = Epθ [x0xs, c] = 1 αs 1 αs 1 αs (cid:0)xs + σ2 xs log q(xsc)(cid:1) , (cid:0)xs + σ2 xs log q(xs)(cid:1) , (s.1) (cid:0)xs + σ2 xs log pθ(xsc)(cid:1) . Subtracting the first two lines of (s.1) gives Eq[x0xs] Eq[x0xs, c] = σ2 αs (xs log q(xs) xs log q(xsc)) , and subtracting the first and third lines yields Epθ [x0xs, c] Eq[x0xs, c] = σ2 αs (xs log pθ(xsc) xs log q(xsc)) . (s.2) (s.3) S.1.1. Self-evaluation without auxiliary term We use the self-evaluation pseudo-target from the main paper, Eq. (14), xself := sg(cid:2)ˆx0 (cid:0)Gθ(ˆxs, s, s, ϕ) Gθ(ˆxs, s, s, c)(cid:1)(cid:3) , and the per-sample squared loss (whose expectation over (t, s, x0, ε) gives Eq. (15)): Lself := ˆx0 xself 2 2 . (s.4) Result 1. Under the posterior-mean approximation Gθ(xs, s, s, c) Eq[x0xs, c] and Gθ(xs, s, s, ϕ) Eq[x0xs], the gradient of (s.4) w.r.t. ˆxs is ˆxs Lself = (cid:16) ˆx0 ˆxs (cid:17) ˆx0 Lself = 2 αs (ˆx0 xself) (Gθ(ˆxs, s, s, ϕ) Gθ(ˆxs, s, s, c)) (Eq[x0ˆxs] Eq[x0ˆxs, c]) = = 2 αs 2 αs 2σ2 α2 (ˆxs log q(ˆxs) ˆxs log q(ˆxsc)) , (s.5) where the last equality uses (s.2). Hence gradient descent on Eq. (15) moves ˆxs in the direction of the classifier score ˆxs log q(cˆxs). S.1.2. Self-evaluation with auxiliary term We optionally add branch prompted by cfake to estimate the model posterior mean Gθ(xs, s, s, cfake) Epθ [x0xs, c]. Define θ(xs, c) := k(Gθ(xs, s, s, ϕ) Gθ(xs, s, s, c)) + (1 k) (Gθ(xs, s, s, cfake) Gθ(xs, s, s, c)) , and the target xself sample squared loss Lself := ˆx0 xself 2 2 . (s.6) := sg[ˆx0 θ(ˆxs, c)], and the perResult 2. Proceeding as in (s.5), ˆxs Lself = 2 αs θ(ˆxs, c) 2 αs [k(Eq[x0ˆxs] Eq[x0ˆxs, c]) + (1 k) (Epθ [x0ˆxs, c] Eq[x0ˆxs, c])] = [k(ˆxs log q(ˆxsϕ) ˆxs log q(ˆxsc)) 2σ2 α2 + (1 k) (ˆxs log pθ(ˆxsc) ˆxs log q(ˆxsc))] , (s.7) where we used (s.2) and (s.3). Equation (s.7) is proportional to the full ideal vector field in Eq. (13) once we set = (w 1)/w. In practice, we set = 0.9. Training. To realize Gθ(xs, s, s, cfake) Epθ [x0xs, c], we use model samples and reuse the same conditional FM loss as Eq. (7): draw ˆx0 pθ(x0c) and ˆxs pθ(xsx0, c), 12 and cfake is constructed by concatenating the phrase fake image with the original prompt, and then minimize (cid:104) Lfake = Gθ(sg[ˆxs], s, s, cfake) sg[ˆx0] 2 2 (cid:105) . (s.8) In practice we follow the training schedule in Sec. 4 of the main paper: use only the classifier term early, and enable the auxiliary term later to refine artifacts, while keeping the overall objective identical to Eqs. (16)(21). S.2. Implementation Details We adopt latent transformer architecture similar to FLUX [2, 11] for our experiments, with minor modifications to accommodate our new s-input. Specifically, the design of the modules handling mirrors those handling t. We employ 2B-parameter model trained on mixedresolution and varying aspect-ratio text-to-image datasets. Initially, the model is trained at an approximate resolution of 2562 pixels for 500k iterations with batch size of 1024. Subsequently, we introduce higher-resolution data of 5122 pixels, maintaining balanced batch proportion (1:1) between the lower-resolution and higher-resolution data, with total batch size of 768, continuing training until reaching 710k iterations. At iteration 550k, we additionally introduce training with the auxiliary term. We use the Adam optimizer with β1 = 0.9, β2 = 0.95, learning rate warmup for 1000 iterations, and linearly decay the learning rate from 3 104 to 1 105. For model evaluation, we maintain an exponential moving average (EMA) with decay rate of 0.9999. Additionally, during training in the self-evaluation forward pass, the conditional branch utilizes the EMA model, while the unconditional branch employs the non-EMA model. Architecture. We adopt FLUX-style latent transformer and keep the notation consistent with the main paper: the denoisers raw prediction is Vθ() and the sample head is Gθ(xt, t, s, c) = xt Vθ(xt, t, s, c) (cf. Eq. (6)(7) in the main paper). Our implementation consists of four modules: (a) VAE, (b) patchifier, (c) frozen text encoders, and (d) dual-time denoiser. (a-b) VAE and patch tokens. We use the FLUX.1-dev auto-encoder with z-channels = 16, and compression factors [1, 8, 8] for [frames, H, W]. Images are tokenized by patchifier with patch size [1, 2, 2]. Thus, each image produces sequence of Limg = (H/16) (W/16) tokens, each of dimension dimg = 16 2 2 = 64. (c) Text and global conditioning. We use frozen T5XXL encoder to obtain token embeddings of dimension dtxt = 4096. Additionally, we compute global pooled CLIP embedding (ViT-L/14) of dimension dvec = 768. Both encoders are kept frozen during training, and their outputs are linearly projected to Rdmodel before entering the denoiser. (d) Denoiser. Our 2B model has model width dmodel = 2048, head size dhead = 128 (thus 16 heads), and total of 8 Double-Stream blocks followed by 16 Single-Stream blocks. Positional encoding uses multi-axis RoPE over (t, y, x) with axis dimensions [16, 56, 56], whose sum matches dhead and whose three axes correspond to time and the two spatial directions. Inputs are linearly projected to dmodel: text via R4096 R2048, image tokens via R64 R2048, and the global CLIP vector via two-layer MLP R768 R2048. For ablations, we also train smaller 0.5B variant with dmodel = 1024, dhead = 64, and RoPE axis dimensions [8, 28, 28], while keeping all other components identical. Particularly, the denoiser has two time inputs: the primary time and an auxiliary time used by the self-evaluation mechanism (Sec. 3.2). In practice, we encode and the gap with sinusoidal features followed by small MLPs: et = MLPt(Sinusoid(t)), es = MLPs(Sinusoid(t s)), (s.9) and form combined time embedding et = et + es. (s.10) This combined embedding et simply replaces the original single-time embedding in the backbone: every module that previously consumed et now receives et. Consequently, the only architectural change relative to FLUX is the additional auxiliary term es added on top of et, while all downstream conditioning and modulation remain unchanged. Timestep Scheduler. We first sample the primary time from logit-normal distribution defined on (0, 1): traw = σ(z), (0, 1), (s.11) where σ() denotes the sigmoid function. This raw time is further adjusted by length-dependent warping function. Specifically, given the latent patch length L, we define linear shift µ(L) interpolating between 0.5 at length 512 and 1.15 at length 4096, then compute the warped primary time as: eµ(L) eµ(L) + (1/traw 1) For the secondary time s, we set = with probability = 0.5. For the remaining half of the cases, we sample uniformly from the interval: (s.12) = . U((1 τ ) t, t), (s.13) where τ is linear annealing weight, transitioning from 0 to 1 over the first 300, 000 training iterations. As result, the effective lower bound (1 τ )t decreases gradually from approximately towards 0 during training. For the weighting function ws,t in Eq. (20), we set it to 1/t2. 13 Figure S.1. More results with 2 and 4 steps. We showcase diverse text-to-image results from our model at 2 and 4 inference step counts, demonstrating coherent semantics, strong text alignment. 14 Figure S.2. More results with 8 and 50 steps. We showcase diverse text-to-image results from our model at 8 and 50 inference step counts, demonstrating coherent semantics, strong text alignment. Inference. For inference, we employ an initially linear timestep scheduler with length-dependent warping function, same with Eq. (s.12). We use DDIM-style update with an η-controlled noise level, following Song et al. [50]; 15 setting η = 0 recovers deterministic DDIM, while η = 1 corresponds to the original DDPM ancestral sampling. In our case, we use η = 1. S.3. Additional Experimental Results S.3.1. Alternative s-Scheduler We investigate alternative strategies for selecting the secondary timestep sk during inference, given transition from tk to tk+1. During training, the selection of sk affects two aspects simultaneously: it determines the noise level for the smoothed data distribution used in the reverse KL divergence, and it specifies the self-evaluation weighting factor λs,t. These dual roles suggest alternative choices for sk might yield intermediate and potentially improved behaviors. An intriguing direction for future work would be decoupling the dependence between sk and the weighting factor λs,t, making λs,t independently tunable. We illustrate our empirical observations in Fig. S.3, highlighting two notable special cases: 1. When sk = tk, the model utilizes only the flow matching loss. Consequently, its behavior closely resembles standard Flow Matching, performing poorly at very low inference steps but improving significantly with more steps. 2. When sk = tk+1, the model excels in few-step generation. However, as the number of inference steps increases (e.g., at 50 steps), we occasionally observe it underperforms compared to sk = tk (see the last two examples in Fig. S.3). Additionally, we explore special inference settingonestep generation without classifier-free guidance. As shown in Fig. S.4, we interpolate between sk = tk (represented as = 1) and sk = tk+1 (represented as = 0). Both extreme cases fail to yield meaningful images, whereas the midpoint choice = 0.5 achieves favorable balance between texture detail and overall image coherence. S.3.2. More Results We present more results at different inference budgets in Fig. S.1 and Fig. S.2. S.4. Prompts of Results We provide the text prompts used for the qualitative results shown in the main paper. S.4.1. Prompts of Figure 1. 2-step: The word Self-E appearing faintly through condensation on train window, blurred landscape passing behind, city lights refracting, cinematic melancholic tone. Figure S.3. Visualization of two special cases for choosing the secondary timestep sk during inference. Top rows: sk = tk, bottom rows: sk = tk+1. Portrait of wolf under snowfall, frost collecting on its muzzle and fur, visible texture and natural grain, calm expression, photoreal cold-environment realism. An oil painting of woman with her hair turning into waves, seascape blending with portrait, tactile brushwork, painterly surreal tone. 4-step: volcano erupting with petals instead of lava, clouds of color drifting across the sky, surreal cinematic beauty. cat composed of smoke sitting on rooftop, its form dissolving into the night air, glowing eyes reflecting city lights, detailed cinematic surrealism. plate of pastries beside teacup, sunlight highlighting golden crusts, powdered sugar shimmering under warm 16 horizon, warm afternoon light highlighting color contrast, photoreal cinematic realism. S.4.2. Prompts of Figure 4. colorful chalkboard artwork spelling SELFE in bright pastel colorsblue, pink, yellow, and greeneach letter outlined softly, chalk dust particles floating through air, faint eraser marks around, warm nostalgic classroom atmosphere. small home bar setup with wine bottles, glass of whiskey half full, sliced lemon on napkin, reflections on wooden counter, photoreal cinematic tone. cat sleeping on cloud drifting above mountain range, soft pink sunrise illuminating fur, photoreal dreamlike realism. royal guard in ornate jade armor, sword reflecting sunlight, palace gardens behind full of flowers and fountains, silk banners waving in soft breeze, cinematic elegant realism. S.4.3. Prompts of Figure 5. high-altitude thunderhead above wheat plain; sculpted cumulonimbus, sunlit anvil, tiny barn for scale, global contrast, 24mm vastness, dramatic meteorological realism. house constructed from luminous jelly bricks glowing at night, detailed transparency and refraction, cinematic realism. S.5. Limitations and Future Work While our method significantly surpasses existing fromscratch training methods in few-step generation, it still has some limitations. Notably, our current approach, although effective in significantly reducing the number of inference steps, cannot fully compete with the quality obtained by 50-step inference when employing extremely few steps (e.g., 12 steps). In these cases, the generated images may lack sufficiently sharp details. Additionally, given that our proposed paradigm fundamentally differs from existing consistency-based methods, it remains at an early stage of exploration. Several critical design choices, such as loss weighting schemes and inference strategies, have not yet been thoroughly optimized. We believe further systematic exploration of these aspects could lead to considerable improvements. Nonetheless, we emphasize that our method introduces genuinely novel training paradigm, distinct from the consistency-training family. Empirically, we observe that our method inherently produces robust structure and semantic coherence, exhibiting clear trend of generating coherent structures first, followed by iterative refinement of details. Looking forward, we identify several promising avenues for future work: Figure S.4. One-step generation without classifier-free guidance. We show results of when selecting different s. glow, photoreal comforting realism. 8-step: Portrait of jungle guardian with vine tattoos and greengold war paint, wet skin glistening under filtered sunlight, 85mm, macro detail on skin texture, cinematic naturalism. bison standing in foggy grassland at dawn, dew on tall grass, sun barely visible through haze, fur glistening with moisture, cinematic atmospheric realism. cozy cottage built entirely from red and white yarn, knitted walls and woven roof shingles, soft texture visible in each thread, golden sunlight casting gentle shadows, photoreal tactile realism. 50-step: human face emerging from cracked porcelain, half side smooth and half crumbled revealing crystalline interior, emotional surreal realism. queen in jeweled crown standing under golden archway, sunlight refracting through gems, detailed embroidery on gown, distant cityscape visible behind, regal photoreal tone, 9:16. rabbit made of transparent glass jumping across shallow creek, sunlight refracting rainbow light through its body, ripples and stones visible beneath, forest on both sides, 16:9 photoreal wide scene. close-up underwater portrait of woman leaning forward on large rectangular glowing sign that reads SelfE, the sign filling the lower part of the frame like real physical board. Neon hues of cyan, pink, and gold from the illuminated surface ripple through the clear turquoise water, casting colorful reflections across her face. She smiles brightly, blue eyes open with confidence, freckles and natural skin texture visible under shifting light. Transparent fish swim nearby among coral branches, tiny bubbles rising through the calm cinematic 9:16 scene. valley full of blooming lupines and daisies, 16:9 panoramic view, rolling hills leading toward mountain 17 1. Improving training strategies and inference-time scheduling to further enhance generation quality. 2. Investigating the efficacy of our approach for downstream task fine-tuning. 3. Exploring scalability and potential adaptations of the proposed paradigm to video generative models. 4. Extending our method to unconditional generative settings, as the current approach relies on conditional guidance to derive the classifier scores."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "The University of Hong Kong"
    ]
}