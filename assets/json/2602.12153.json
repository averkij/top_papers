{
    "paper_title": "dVoting: Fast Voting for dLLMs",
    "authors": [
        "Sicheng Feng",
        "Zigeng Chen",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting"
        },
        {
            "title": "Start",
            "content": "DVOTING: Fast Voting for dLLMs Sicheng Feng 1 Zigeng Chen 1 Xinyin Ma 1 Gongfan Fang 1 Xinchao Wang 1 *"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (dLLMs) represent new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce DVOTING, fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. DVOTING is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, DVOTING performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that DVOTING consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting 6 2 0 2 2 1 ] . [ 1 3 5 1 2 1 . 2 0 6 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Diffusion large language models (dLLMs) (Yi et al., 2024; Zhang et al., 2025; Nie et al., 2025; Ye et al., 2025; Yu et al., 2025b; Bie et al., 2025) have recently emerged as competitive alternative to autoregressive LLMs (Achiam et al., 2023; Bai et al., 2023; Dubey et al., 2024), demonstrating strong 1Department of Electrical and Computer Engineering, National University of Singapore, Singapore. Correspondence to: Xinchao Wang <xinchao@nus.edu.sg>. Preprint. February 13, 2026. 1 performance and extending beyond open-source settings to closed-source models such as Gemini-Diffusion, SeedDiffusion (Song et al., 2025), and Mercury (Khanna et al., 2025). Through an iterative unmask-and-remask process, dLLMs enable parallel decoding in flexible order beyond left-to-right token generation in autoregressive LLMs, offering substantial flexibility and potential at test time. In dLLMs, recent efforts (Zhao et al., 2025a; Tang et al., 2025; Zhao et al., 2025b; Wang et al., 2025a; Yang et al., 2025a; Huang et al., 2025) for reasoning enhancement mainly focus on training time, primarily through reinforcement learning (RL) (Ouyang et al., 2022; Shao et al., 2024; Feng et al., 2025b). These RL approaches introduce novel training schemes and achieve notable performance improvements. Complementary to these training-time approaches, another line of work explores enhancing reasoning by operating directly on the decoding process at test time. Inspired by the success of test-time scaling in LLMs (Muennighoff et al., 2025; Snell et al., 2024; Wang et al., 2025b; Sun et al., 2024; Xu et al., 2025) and recent findings that RL primarily improves sampling efficiency rather than intrinsic capability (Yue et al., 2025; Chen et al., 2025c), we instead focus on inference. To date, few works have explored test-time scaling for reasoning enhancement in dLLMs. HEX (Lee et al., 2025) activates implicit semi-autoregressive experts by varying block sizes, while RFG (Chen et al., 2025a) guides generation at the logit level using an additional fine-tuned model. Although these methods can boost reasoning performance, they typically involve increased inference-time computation. Our goal is to reduce the substantial redundancy inherent in the voting paradigm. Based on the consistency analysis, we further identify simple yet crucial observation that repeated tokens frequently appear across multiple samples for the same input, as illustrated in Figure 2(b) and quantified in Table 1. Furthermore, we connect this observation with the remasking mechanism of dLLMs which allows the model to mask and regenerate an arbitrary number of decoded tokens at arbitrary positions within sequence1, making it well-suited to reduce the redundancy revealed by this observation. Accordingly, we 1e.g., dLLMs are very efficient at test time. dLLMs [MASK] [MASK] [MASK] at [MASK] time. dLLMs can be fast at inference time. DVOTING : Fast Voting for dLLMs Figure 1. Overview of DVOTING. For each prompt, our DVOTING preserves consistent tokens in previous generations and remasks the remaining tokens to initiate subsequent sampling, and terminates the process early when candidate answers satisfy consistent criteria. propose simple yet fast voting strategy (Figure 1). Specifically, we identify uncertain tokens based on token consistency, iteratively remask and regenerate them for refinement, and finally aggregate candidate answers via voting. We conduct extensive experiments across various reasoning benchmarks on two popular dLLMs (LLaDA (Nie et al., 2025) & Dream (Ye et al., 2025)) to evaluate the effectiveness of our method. The results demonstrate that our method boosts the performance across all benchmarks covering mathematical, scientific, and general reasoning. For instance, our method yields performance gains of 6.22%7.66% on GSM8K (Cobbe et al., 2021) and 4.40%7.20% on MATH500 (Lightman et al., 2023) on LLaDA. Our method further achieves leading performanceefficiency trade-off. Additionally, we present the robustness of our method under various configurations. In conclusion, we propose DVOTING, training-free, simple yet effective voting strategy that boosts performance. Based on the empirical study, we identify and quantify key observation that many tokens are repeatedly generated across multiple sampling runs, and relate this to the remasking mechanism of dLLMs. Accordingly, we introduce the remask sampling strategy, which iteratively remasks and regenerates selected tokens to obtain multiple candidate generations and aggregates via voting to enhance performance. Extensive evaluations demonstrate the effectiveness of DVOTING. Together, DVOTING establishes the first baseline and provides foundation for efficient test-time scaling in dLLMs, further unlocking their potential at test time."
        },
        {
            "title": "2 Related Work",
            "content": "Overview of Diffusion Language Models. Diffusion models (Ho et al., 2020; Song & Ermon, 2019; Song et al., 2020) have shown strong generative capabilities in continuous domains (e.g., image (Rombach et al., 2022; Peebles & Xie, 2023), video (Ho et al., 2022; Brooks et al., 2024), and audio (Liu et al., 2023; Evans et al., 2024)), while their extension to language modeling remains challenging due to the discrete nature of text. Recent works (Austin et al., 2021; Sahoo et al., 2024; Lou et al., 2023; Zheng et al., 2024; Cheng et al., 2025) address this issue by formulating diffusion processes directly over token spaces, typically through masked token prediction, which enables parallel generation and relaxes the strict autoregressive constraint. Building on this paradigm, dLLMs (Nie et al., 2025; Ye et al., 2025; Khanna et al., 2025; Song et al., 2025; Bie et al., 2025) have demonstrated competitive performance compared to autoregressive models at the billion-parameter scale, indicating the practical viability of diffusion for language generation. Moreover, diffusion language models have been increasingly explored in advanced settings such as reasoning (Zhu et al., 2025; Zhao et al., 2025a; Tang et al., 2 DVOTING : Fast Voting for dLLMs Figure 2. Empirical study on LLaDA-8B-Instruct. We report the performance of five strategies: (1) Pass@1; (2) Pass@3; (3) Pass@5; (4) Majority voting (5 samples), which denotes the results of standard test-time scaling strategies; and (5) d1, which represents the performance of RL-enhanced models. We report the results of GSM8K, MATH500, and ARC-C in (a), (b), and (c), respectively. 2025; Lin et al., 2025), multimodal generation (Yang et al., 2025b; Li et al., 2025b; Yu et al., 2025b; You et al., 2025), and code synthesis (Gong et al., 2025; Khanna et al., 2025; Peng et al.), reflecting the growing scope and maturity of this research direction (Yu et al., 2025a; Li et al., 2025c). Test-Time Scaling in Language Models. Test-time scaling (Welleck et al., 2024; Snell et al., 2024; Muennighoff et al., 2025; OpenAI, 2024) has emerged as an effective alternative to training-time scaling, aiming to elicit stronger reasoning capabilities by allocating additional computation during inference. In autoregressive language models, testtime scaling has been extensively studied through techniques such as chain-of-thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022; Zhou et al., 2022), best-of-N sampling (Sun et al., 2024; Wang et al., 2025b; Xu et al., 2025), and self-consistency (Wang et al., 2022; 2024; Aggarwal et al., 2023). However, test-time scaling in dLLMs remains relatively underexplored. Recent studies provide initial evidence of its potential: HEX (Lee et al., 2025) shows that aggregating diverse masking schedules during inference can substantially improve performance, while RFG (Chen et al., 2025a) introduces guidance from an additional fine-tuned model by operating at the logits level. These methods provide valuable references for test-time scaling in dLLMs. However, test-time scaling efficiency (Sun et al., 2024; Xu et al., 2025; Feng et al., 2025a; Ma et al., 2024; Zhu et al., 2024) is equally critical, as it directly addresses the substantial inference cost and determines the practicality of such approaches in real-world settings. To date, efficient test-time scaling for dLLMs remains largely unexplored; this gap is the primary focus of our work."
        },
        {
            "title": "3 Preliminaries",
            "content": "We anchor our preliminaries in continuous-time diffusion language models defined over discrete vocabularies, with masked diffusion language models (MDLMs) serving as the main instantiation. MDLMs generate text by iteratively reconstructing partially corrupted sequence rather than producing tokens in an autoregressive order. Starting from an initial clean sequence x0, the model constructs noised version xt by independently masking each position with intensity [0, 1]. Formally, the corruption distribution is q(xt x0) = (cid:89) i=1 (cid:34)(1 t) δ(xi + δ(xi = xi 0) = [MASK]) (cid:35) . Once corrupted sequence is obtained, denoising network pθ attempts to infer the original tokens at the masked locations. Since all masked positions are conditionally independent given xt, the recovery model factorizes as pθ(x0 xt) = (cid:89) pθ(xi 0 xt). i:xi t=[MASK] Training focuses on masked tokens, with the model learning to reconstruct original content from corrupted sequences. The resulting reconstruction loss serves as surrogate objective that upper-bounds the negative log-likelihood. Properties of dLLMs Fit Parallel Test-Time Scaling. During inference, dLLMs update all masked positions in parallel, producing token predictions for the entire sequence at each iteration while selectively remasking uncertain positions. In contrast to prior work that focuses on single-run decoding (Chen et al., 2025b; Wei et al., 2025), our work primarily targets parallel test-time scaling, where multiple outputs are generated in parallel and subsequently aggregated into final prediction. Across multiple sampling runs, the remasking mechanism enables the model to selectively reuse reliable context from previous iterations, refining earlier decisions. This property makes dLLMs natural and scalable backbone for parallel test-time scaling. Additionally, the inherently parallel decoding process of dLLMs further reduces inference latency. 3 DVOTING : Fast Voting for dLLMs (a) Distribution of voting consistency level (b) Cases on token consistency Figure 3. (a) We report the distribution of voting consistency levels across different sample categories, defined by the correctness of the baseline and voting predictions. (b) We present two cases illustrating token-level redundancy in dLLM sampling (5 samples), drawn from GSM8K and ARC-C, respectively (zoom-in for more details). Parallel Test-Time Scaling Can Work as an Alternative to Reinforcement Learning in dLLMs. RL can be viewed as selecting higher-quality trajectories from multiple samples and increasing their sampling probability, thereby improving sampling efficiency rather than fundamentally enhancing the models capacity. This perspective is also supported by recent work (Yue et al., 2025) in LLMs. As major strategy of parallel test-time scaling, voting performs multiple sampling runs and selects the most consistent one as the final prediction. From this viewpoint, voting can be regarded as training-free alternative that approximates the effect of RL. As shown in Figure 2, we conduct experiments and further validate this claim in dLLMs: scaling test-time computation achieves performance comparable to advanced RL-based methods across both reasoning and general tasks. However, these significant performance gains come at the cost of increased inference overhead, which motivates us to improve efficiency."
        },
        {
            "title": "4 Methods",
            "content": "In this section, we introduce DVOTING to demonstrate the potential of simple voting strategies as cost-effective alternative to RL in dLLMs. We first present the critical observation and then introduce the key designs of our DVOTING. 4.1 Key Observations We conduct an empirical study using majority voting with entropy-threshold parallel decoding on LLaDA with GSM8K. Based on the results, we present the key observation that motivates us to leverage the remasking capability of dLLMs to further improve efficiency. Observation A. We partition samples into four categories based on whether the baseline and voting predictions are correct. We propose metric called voting consistency level, defined as the fraction of votes received by the most Table 1. Results of NUPR@k on various benchmarks under two generation lengths. To be specific, we record the tokens of 5 samples and calculate the metric for each question. Metric / Seq. Len. GSM8K MATH500 ARC-C 128 256 256 128 256 NUPR@2 NUPR@3 0.6077 0.5473 0.4894 0.4383 0.4612 0.4812 0.2109 0.1953 0.1327 0.1142 0.1562 0.2134 frequent answer. As shown in Figure 3(a), we observe that samples correctly solved by both the baseline and voting methods are highly concentrated at high consistency levels, with 84.58% falling into the 4/5 or 5/5 bins. In contrast, the remaining categories are predominantly associated with low consistency levels, with 66.07% of samples receiving only 1/5 or 2/5 votes. In summary, easier questions tend to exhibit higher voting consistency, indicating that sampling redundancy predominantly arises in such cases and can be reduced with minimal impact on performance. Observation B. We further observe that for given question, many token positions remain identical across multiple samples, as illustrated in Figure 3(b). This token-level redundancy indicates substantial overlap among sampled sequences and reveals additional opportunities to reuse confident predictions while focusing resampling on small subset of uncertain token positions. We propose new metric termed Non-Unique Position Rate at (NUPR@k) to quantify how frequently this phenomenon occurs. Given sampled answers of equal length for the same question, NUPR@k measures the fraction of token positions at which at least out of samples share an identical token. Formally, token position is considered non-unique if at least out of the samples share the same token at that position, and NUPR@k is computed as the average fraction of such positions across all tokens and questions. As shown in Table 1, we report results on GSM8K, MATH500, and ARC-C under different generation lengths. We observe that NUPR@2 is consistently around 4 DVOTING : Fast Voting for dLLMs Table 2. Results on mathematical reasoning benchmarks with LLaDA-8B-Instruct. represents the results of reimplementation. indicates that RFG requires an extra instruction-tuned or RL-enhanced model as the policy model. We report Pass@1 accuracy along with the corresponding step count. The involved RL-enhanced methods require in-domain training data. Method / Gen. Len. Training Necessity GSM8K (0-shot) MATH500 (0-shot) 128 256 512 256 512 LLaDA-8B-Instruct Pre-training 70.58% / 128.0 76.72% / 256.0 81.50% / 512. 31.40% / 128.0 35.00% / 256.0 38.20% / 512.0 RL-Enhanced Models (in-domain) d1 wd1 IGPO SFT + RL RL RL 73.20% / 64.0 - - 81.10% / 128.0 80.80% / 128.0 83.60% / 256.0 82.10% / 256.0 82.30% / 256.0 - 33.80% / 64.0 - - 38.60% / 128.0 34.40% / 128.0 42.80% / 256.0 40.20% / 256.0 39.00% / 256.0 - Test-Time Scaling Strategies Majority Voting HEX RFG DVOTING (ours) Training-free Training-free FT or RL Training-free 76.72% / 320.0 36.80% / 640.0 44.00% / 1280.0 80.14% / 1600.0 85.75% / 3200.0 88.78% / 6400.0 39.60% / 1600.0 43.60% / 3200.0 47.40% / 6400.0 82.33% / 640.0 86.28% / 1280.0 33.00% / 320.0 - 78.24% / 170. 81.30% / 512.0 83.78% / 237.1 - 87.72% / 289.6 - 34.80% / 292.1 39.60% / 512.0 40.20% / 473.7 - 45.40% / 701.2 Table 3. Results on scientific and general reasoning benchmarks with LLaDA-8B-Instruct. represents the results of reimplementation. We report Pass@1 accuracy along with the corresponding step count. Method / Gen. Len. Training Necessity ARC-C (0-shot) GPQA (0-shot) MMLU (0-shot) 128 128 256 128 256 LLaDA-8B-Instruct Pre-training 66.13% / 128. 74.66% / 256.0 25.00% / 128.0 23.66% / 256.0 57.13% / 128.0 58.04% / 256.0 Test-Time Scaling Strategies Majority Voting HEX DVOTING (ours) Training-free Training-free Training-free 63.11% / 640.0 77.65% / 320.0 82.67% / 1600.0 83.87% / 3200.0 25.67% / 1600.0 28.34% / 3200.0 64.28% / 1600.0 64.75% / 3200.0 62.87% / 391.4 80.97% / 265.4 62.87% / 222.1 83.36% / 415.6 28.57% / 219. 28.39% / 555.7 81.31% / 640.0 22.54% / 320.0 25.45% / 640.0 62.27% / 320.0 50%, while NUPR@3 remains around 20%, indicating substantial token-level redundancy. old α at each denoising step. In our main experiments, we set α = 0.3 and provide an ablation study on this threshold. 4.2 Remask Sampling Based on these observations (A & B), we naturally connect the empirical findings with the inherent remasking capability of dLLMs and propose simple yet effective remask sampling strategy (as shown in Figure 1). We present the detailed implementation pipeline in Appendix A.1. Remask Sampling Strategy. Inspired by observation A, we stop sampling on time based on answer consistency (e.g., the first several generations yield the same answer). Furthermore, inspired by observation B, we propose our core design to further improve efficiency. Instead of generating full response at each sampling action, we selectively remask tokens and continue sampling based on the consistency analysis. In particular, we retain tokens that exhibit agreement across samples, as well as tokens from samples with highly consistent predicted answers, and perform subsequent sampling conditioned on the preserved tokens. Finally, we stop sampling once the voting-based answer converges and adopt it as the final prediction."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setups Baselines. We compare against four sampling baselines: the original results, majority voting, and two related methods, HEX (Lee et al., 2025) and RFG (Chen et al., 2025a). Specifically, for the original performance, we adopt the semi-autoregressive decoding strategy following the original paper; for majority voting (5 samples), we follow the configuration used in HEX. For HEX and RFG, we report either our reimplementations or the results from the original papers; the two methods require 25 and 2 samples, respectively. Additionally, we include the original results reported in the paper for three RL methods: d1 (Zhao et al., 2025a), wd1 (Tang et al., 2025), and IGPO (Zhao et al., 2025b). Additionally, in dLLMs, parallel decoding has been extensively explored and is natural advantage of this paradigm. Following prior work (Ben-Hamu et al., 2025; Chen et al., 2025b; Wei et al., 2025), we adopt simple entropythreshold parallel decoding scheme that commits all token positions whose entropy falls below predefined threshInference Details. We follow the official inference settings to evaluate the original models from both the LLaDA series (Nie et al., 2025; Zhu et al., 2025) and Dream (Ye et al., 2025). For the baselines, we report the results from the original paper or our reimplementation, with the latter following the configurations specified in the original paper. We apply 5 DVOTING : Fast Voting for dLLMs Table 4. Results on Dream-7B-Instruct. represents the results of reimplementation. indicates that RFG requires an extra instructiontuned or RL-enhanced model as the policy model. We set top-p to 0.6 for the majority voting baseline. We report Pass@1 accuracy along with the corresponding step count. Method / Gen. Len. Training Necessity GSM8K (0-shot) MATH500 (0-shot) ARC-C (0-shot) MMLU (0-shot) 256 128 256 128 128 Dream-Base-7B - 68.92% / 128.0 81.80% / 256.0 33.00% / 128.0 42.20% / 256.0 66.63% / 128. 64.83% / 128.0 Test-Time Scaling Strategies Majority Voting HEX RFG DVOTING (ours) Training-free Training-free FT or RL Training-free 72.33% / 320.0 68.51% / 320.0 77.48% / 1600.0 87.04% / 3200.0 43.60% / 1600.0 50.60% / 3200.0 88.14% / 1600.0 69.87% / 1600.0 77.73% / 320. 84.53% / 640.0 44.00% / 640.0 34.20% / 320.0 - 75.44% / 206.2 82.10% / 512.0 86.96% / 290.4 - 42.20% / 320. 46.40% / 512.0 48.80% / 498.5 - 86.09% / 221.9 - 69.80% / 119.6 Figure 4. Comparison of performance-efficiency trade-off between DVOTING and other test-time scaling baselines. We present the results on the LLaDA model for GSM8K and MATH500 in (a) and (b), respectively. We present the results on the Dream model for GSM8K, MATH500, and ARC-C under 128 generation length in (c). We mark our method with star to distinguish it from other methods. full denoising steps (N ) for evaluating the original models and half denoising steps (N/2) for baselines (e.g., majority voting and HEX) to generate sequence of length . We set the temperature to 0.6 for our proposed DVOTING and conduct experiments under standard generation lengths (128, 256, 512), with corresponding block sizes of 8, 16, and 32, respectively, following the semi-autoregressive strategy for models from the LLaDA series. For the Dream model, we additionally implement semi-autoregressive decoding strategy, as it is not supported by the original generation pipeline, while all other settings follow those used for the LLaDA series. We set an upper bound = 5 on the total number of samples for the proposed DVOTING. Evaluation Details. We conduct extensive experiments across various benchmarks covering broad range of reasoning tasks, including mathematical reasoning, scientific reasoning, and general high-level reasoning, using GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), ARC-C (Clark et al., 2018), MMLU (Hendrycks et al., 2021), and GPQA (Rein et al., 2024). Additionally, for models from the LLaDA (LLaDA-8B-Instruct and LLaDA-1.5) and Dream (Dream-7B-Instruct), we follow the simple-eval framework2 for zero-shot evaluation and 2https://github.com/openai/simple-evals prompt the models to generate step-by-step reasoning. We implement the voting procedure within the same framework to process all candidate answers. 5.2 Main Results Results on the Native dLLM (LLaDA). We report the results of LLaDA-8B-Instruct in Tables 2 and 3. Compared to the original model and most in-domain RL methods, our DVOTING achieves consistent and substantial performance gains across different benchmarks and generation lengths: 6.22%7.66% on GSM8K, 4.40%7.20% on MATH500, 3.16%-14.84% on ARC-C, 3.57%-4.73% on GPQA, and 4.83%-5.74% on MMLU. Against test-time scaling baselines, DVOTING attains strong performance with the fewest steps, delivering 1.14.4 speedup over majority voting, 1.12.2 over RFG, and 5.522.1 over HEX. Results on the AR-initialized dLLM (Dream). As shown in Table 4, our DVOTING also consistently achieves significant performance improvements on the Dream model across all benchmarks while using the fewest steps among all test-time scaling baselines. Specifically, DVOTING delivers 1.02.7 speedup over majority voting, 1.01.8 speedup over RFG, and 5.013.4 speedup over HEX. DVOTING : Fast Voting for dLLMs Figure 5. Case visualization. We present two cases where the original model yields correct and an incorrect answer, respectively. Prompt-2 fails by incorrectly assuming five-day week, while our method iteratively refines this part to produce the correct answer. Table 5. Results on RL-enhanced model (LLaDA-1.5). represents the results of reimplementation. indicates that RFG requires an extra instruction-tuned or RL-enhanced model as the policy model. We report Pass@1 accuracy along with the corresponding step count. Method / Gen. Len. Training Necessity GSM8K (0-shot) MATH500 (0-shot) ARC-C (0-shot) MMLU (0-shot) 128 256 128 128 128 LLaDA-1.5 - 72.48% / 128.0 78.01% / 256. 30.60% / 128.0 33.40% / 256.0 68.52% / 128.0 58.27% / 128.0 Test-Time Scaling Strategies Majority Voting Training-free HEX (Lee et al., 2025) Training-free RFG (Chen et al., 2025a) FT or RL DVOTING (ours) Training-free 78.47% / 320.0 62.91% / 320.0 82.41% / 1600.0 86.50% / 3200.0 39.60% / 1600.0 43.40% / 3200.0 86.69% / 1600.0 64.47% / 1600.0 79.69% / 320.0 41.00% / 640.0 84.69% / 640.0 32.00% / 320. - 79.15% / 158.8 82.10% / 512.0 84.53% / 223.1 - 34.40% / 280.1 - 41.20% / 443.1 - 82.63% / 267.7 - 63.10% / 224. DVOTING Pushes PerformanceEfficiency Pareto Frontier Forward. We propose unified metric, denoted as benefits per cost (BPC), to quantify the performanceefficiency trade-off in the context of test-time scaling. Specifically, the metric measures the performance gain per unit of additional test-time cost, computed as the improvement over the original model normalized by the corresponding increase in the total number of denoising steps3. As shown in Figure 4, our DVOTING achieves superior performanceefficiency trade-off across all settings. Based on the LLaDA results, our DVOTING consistently achieves larger BPC improvements than other test-time scaling baselines as the generation length increases, demonstrating its clear advantage in long-generation length settings. 3We present an example (GSM8K on LLaDA-8B-Instruct under 128 generation length): 78.24%70.58% 170.4 / 128.0 = 5.75. Generalization of DVOTING on RL-enhanced Models. We next evaluate the generalization of DVOTING on RLenhanced models, such as LLaDA-1.5 fine-tuned with Variance-Reduced Preference Optimization (VRPO) (Zhu et al., 2025). As shown in Table 5, we conduct experiments across four benchmarks. The results demonstrate that DVOTING consistently brings significant performance improvements on RL-enhanced models while incurring only minimal additional inference cost, further confirming the generalization and effectiveness of our method. 5.3 Visualization Figure 5 illustrates two GSM8K cases where LLaDA produces correct and an incorrect prediction, respectively. For Prompt 1, the base model already yields the correct answer, and our DVOTING quickly identifies that no further remasking is needed, terminating sampling early. For Prompt 2, the 7 DVOTING : Fast Voting for dLLMs Table 6. Ablation on sampling upper bound n. We evaluate sampling upper bounds of 1, 5, 9, 13, and 17 while keeping all other hyperparameters fixed, and report Pass@1 accuracy and corresponding step count. We provide the majority voting baseline as reference. Dataset / Gen. Len. Baselines DVOTING LLaDA-8B-Instruct Majority Voting n=1 n=5 n=9 n= n=17 GSM8K / 128 GSM8K / 256 MATH500 / 128 MATH500 / 256 70.58% / 128.0 76.72% / 256.0 31.40% / 128.0 35.00% / 256.0 76.72% / 320.0 82.33% / 640. 70.13% / 58.2 78.24% / 170.4 79.68% / 348.6 80.89% / 515.5 77.55% / 85.3 83.78% / 237.1 85.97% / 477.3 86.13% / 708.6 80.89% / 683.2 86.20% / 937.7 33.00% / 320.0 27.60% / 72.0 34.80% / 292.1 36.40% / 573.3 37.60% / 837.9 39.20% / 1105.0 36.80% / 640.0 36.20% / 120.5 40.20% / 473.7 42.20% / 928.2 43.20% / 1350.7 43.20% / 1779.3 Table 7. Ablation on block size. We evaluate block sizes of 4, 8, 16, 32, and 64 while keeping all other hyperparameters fixed, and report Pass@1 accuracy along with the corresponding step count. We provide the majority voting baseline as reference. Dataset / Gen. Len. Baselines DVOTING LLaDA-8B-Instruct Majority Voting block size=4 block size=8 block size=16 block size= block size=64 GSM8K / 128 GSM8K / 256 MATH500 / 128 MATH500 / 256 70.58% / 128.0 76.72% / 256.0 31.40% / 128.0 35.00% / 256.0 76.72% / 320.0 79.23% / 168.6 78.24% / 170.4 77.71% / 176.1 78.01% / 177.8 75.82% / 184.3 82.33% / 640.0 83.70% / 227.2 84.23% / 231.5 83.78% / 237.1 84.23% / 238.8 84.76% / 238. 33.00% / 320.0 35.00% / 281.0 34.80% / 292.1 34.00% / 296.4 34.00% / 297.9 34.80% / 308.8 36.80% / 640.0 40.00% / 448.1 40.00% / 469.8 40.20% / 473.7 40.40% / 478.5 40.40% / 463.7 Table 8. Ablation on entropy threshold α. We evaluate entropy thresholds of 0.1, 0.3, 0.5, 0.7, and 0.9 while keeping all other hyperparameters fixed, and report Pass@1 accuracy and corresponding step count. We provide the majority voting baseline as reference. Dataset / Gen. Len. Baselines DVOTING LLaDA-8B-Instruct Majority Voting α=0.1 α=0.3 α=0.5 α=0.7 α=0.9 GSM8K / 128 GSM8K / MATH500 / 128 MATH500 / 256 70.58% / 128.0 76.72% / 256.0 31.40% / 128.0 35.00% / 256.0 76.72% / 320.0 78.24% / 213.7 78.24% / 170.4 77.48% / 152.6 77.18% / 140.4 74.91% / 136.4 82.33% / 640.0 84.76% / 299.9 83.78% / 237.1 83.70% / 206.1 82.48% / 188.6 80.14% / 189.4 33.00% / 320.0 34.80% / 345.7 34.80% / 292.1 33.80% / 260.9 33.60% / 242.5 31.40% / 221.4 36.80% / 640.0 42.60% / 565.4 40.20% / 473.7 40.80% / 411.0 39.20% / 378.0 35.20% / 361.5 base model fails, while our method performs five sampling iterations and aggregates the results via voting to recover the correct answer. Overall, our proposed DVOTING can adapt its computation according to problem difficulty automatically, efficiently solving simple cases while allocating more inference effort to harder ones to improve both efficiency and performance. We provide more cases in Appendix B. 5.4 Ablation Study We provide comprehensive ablation studies on sampling upper bound n, block size, and entropy threshold α on LLaDA, evaluated on GSM8K and MATH500. Ablation on Sampling Upper Bound. We first ablate the sampling upper bound under two generation lengths (128 and 256). As shown in Table 6, performance consistently improves with larger across both benchmarks and both generation lengths, until reaching saturation point. This demonstrates that DVOTING can effectively scale test-time computation to boost performance, consistent with the testtime scaling law stated in prior work (Snell et al., 2024). Ablation on Block Size. We next ablate the block size, with results summarized in Table 7. Across block sizes ranging from 4 to 64, DVOTING consistently outperforms both the original results and the majority voting baseline, indicating that its performance is robust to this hyperparameter and further demonstrating the effectiveness of our approach. Ablation on Entropy Threshold. We further conduct an ablation study on the entropy threshold α. As shown in Table 8, DVOTING maintains strong performance and favorable efficiency trade-off across broad range of threshold values (e.g., 0.1-0.7). noticeable performance drop is observed only when the threshold is set excessively high, such as α=0.9, where DVOTING still outperforms the original results. Moreover, the results follow test-time scaling trend, with increased computation leading to improved performance. Overall, this ablation study further demonstrates the robustness and effectiveness of our proposed method."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the challenge of the high inference cost incurred by test-time scaling in dLLMs. We first identify key empirical insight: repeated tokens frequently emerge across multiple samples for the same prompt. Building on this observation, we leverage the remasking mechanism of dLLMs and propose DVOTING, fast voting strategy tailored to dLLMs. Extensive evaluations demonstrate that our method consistently enhances reasoning performance with modest extra inference cost, achieving leading per8 DVOTING : Fast Voting for dLLMs formanceefficiency trade-off. Overall, our work provides foundation for future test-time scaling and unlocks the inference-time potential of dLLMs."
        },
        {
            "title": "Impact Statement",
            "content": "This work investigates efficient test-time scaling for dLLMs by proposing simple voting strategy that leverages the remasking mechanism to focus computation on uncertain tokens. The approach improves reasoning performance while significantly reducing inference cost, making test-time scaling more practical under limited computational budgets. By improving inference efficiency, the proposed approach reduces computational overhead and resource consumption, thereby making large-scale model deployment more feasible in practice. The method does not introduce additional ethical or societal concerns beyond those commonly associated with large language models."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Aggarwal, P., Madaan, A., Yang, Y., et al. Lets sample step by step: Adaptive-consistency for efficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860, 2023. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, 2021. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Ben-Hamu, H., Gat, I., Severo, D., Nolte, N., and Karrer, B. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Bie, T., Cao, M., Chen, K., Du, L., Gong, M., Gong, Z., Gu, Y., Hu, J., Huang, Z., Lan, Z., et al. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745, 2025. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. Chen, T., Xu, M., Leskovec, J., and Ermon, S. Rfg: Test-time scaling for diffusion large language model reasoning with reward-free guidance. arXiv preprint arXiv:2509.25604, 2025a. Chen, Z., Fang, G., Ma, X., Yu, R., and Wang, X. dparallel: Learnable parallel decoding for dllms. arXiv preprint arXiv:2509.26488, 2025b. Chen, Z., Qin, X., Wu, Y., Ling, Y., Ye, Q., Zhao, W. X., and Shi, G. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025c. Cheng, S., Bian, Y., Liu, D., Zhang, L., Yao, Q., Tian, Z., Wang, W., Guo, Q., Chen, K., Qi, B., et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Evans, Z., Carr, C., Taylor, J., Hawley, S. H., and Pons, J. Fast timing-conditioned latent audio diffusion. In ICML, 2024. Feng, S., Fang, G., Ma, X., and Wang, X. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903, 2025a. Feng, S., Tuo, K., Wang, S., Kong, L., Zhu, J., and Wang, H. Rewardmap: Tackling sparse rewards in fine-grained visual reasoning via multi-stage reinforcement learning. arXiv preprint arXiv:2510.02240, 2025b. Feng, S., Wang, S., Ouyang, S., Kong, L., Song, Z., Zhu, J., Wang, H., and Wang, X. Can mllms guide me home? benchmark study on fine-grained visual reasoning from transit maps. arXiv preprint arXiv:2505.18675, 2025c. Gong, S., Zhang, R., Zheng, H., Gu, J., Jaitly, N., Kong, L., and Zhang, Y. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. 9 DVOTING : Fast Voting for dLLMs Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In ICLR, 2021. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In NeurIPS, 2022. Huang, Z., Chen, Z., Wang, Z., Li, T., and Qi, G.-J. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025. Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., Ermon, S., et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Lee, J., Moon, H., Zhai, K., Chithanar, A. K., Sahu, A. K., Kar, S., Lee, C., Chakraborty, S., and Bedi, A. S. Test-time scaling in diffusion llms via hidden semiautoregressive experts. arXiv preprint arXiv:2510.05040, 2025. Li, Q., Yu, R., Lu, H., and Wang, X. Every step counts: Decoding trajectories as authorship fingerprints of dllms. arXiv preprint arXiv:2510.05148, 2025a. Li, S., Kallidromitis, K., Bansal, H., Gokul, A., Kato, Y., Kozuka, K., Kuen, J., Lin, Z., Chang, K.-W., and Grover, A. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025b. Li, T., Chen, M., Guo, B., and Shen, Z. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025c. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. In ICLR, 2023. Ma, C., Zhao, H., Zhang, J., He, J., and Kong, L. Nonmyopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195, 2024. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In EMNLP, 2025. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. OpenAI. OpenAI o1. https://openai.com/o1/, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Peng, F. Z., Zhang, S., and Tong, A. contributors. open-dllm: Open diffusion large language models. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In COLM, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. In NeurIPS, 2024. Lin, N., Zhang, J., Hou, L., and Li, J. Boundary-guided policy optimization for memory-efficient rl of diffusion large language models. arXiv preprint arXiv:2510.11683, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., and Plumbley, M. D. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. DVOTING : Fast Voting for dLLMs Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. Song, Y., Zhang, Z., Luo, C., Gao, P., Xia, F., Luo, H., Li, Z., Yang, Y., Yu, H., Qu, X., et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Sun, H., Haider, M., Zhang, R., Yang, H., Qiu, J., Yin, M., Wang, M., Bartlett, P., and Zanette, A. Fast best-of-n decoding via speculative rejection. In NeurIPS, 2024. Tang, X., Dolga, R., Yoon, S., and Bogunovic, I. wd1: Weighted policy optimization for reasoning in diffusion arXiv preprint arXiv:2507.08838, language models. 2025. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wang, X., Feng, S., Li, Y., Yuan, P., Zhang, Y., Tan, C., Pan, B., Hu, Y., and Li, K. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. arXiv preprint arXiv:2408.13457, 2024. Wang, Y., Yang, L., Li, B., Tian, Y., Shen, K., and Wang, M. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025a. Wang, Y., Zhang, P., Huang, S., Yang, B., Zhang, Z., Huang, F., and Wang, R. Sampling-efficient test-time scaling: Self-estimating the best-of-n sampling in early decoding. arXiv preprint arXiv:2503.01422, 2025b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Wei, Q., Zhang, Y., Liu, Z., Liu, D., and Zhang, L. Accelerating diffusion large language models with slowfast: The three golden principles. arXiv preprint arXiv:2506.10848, 2025. Welleck, S., Bertsch, A., Finlayson, M., Schoelkopf, H., Xie, A., Neubig, G., Kulikov, I., and Harchaoui, Z. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Xu, F., Yan, H., Ma, C., Zhao, H., Liu, J., Lin, Q., and Wu, Z. ϕ-decoding: Adaptive foresight sampling for balanced inference-time exploration and exploitation. arXiv preprint arXiv:2503.13288, 2025. Yang, J., Chen, G., Hu, X., and Shao, J. Taming masked diffusion language models via consistency trajectory reinforcement learning with fewer decoding step. arXiv preprint arXiv:2509.23924, 2025a. Yang, L., Tian, Y., Li, B., Zhang, X., Shen, K., Tong, Y., and Wang, M. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025b. Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Yi, Q., Chen, X., Zhang, C., Zhou, Z., Zhu, L., and Kong, X. Diffusion models in text generation: survey. PeerJ Computer Science, 2024. You, Z., Nie, S., Zhang, X., Hu, J., Zhou, J., Lu, Z., Wen, J.-R., and Li, C. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Yu, R., Li, Q., and Wang, X. Discrete diffusion in large language and multimodal models: survey. arXiv preprint arXiv:2506.13759, 2025a. Yu, R., Ma, X., and Wang, X. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025b. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? In NeurIPS, 2025. Zhang, L., Fang, L., Duan, C., He, M., Pan, L., Xiao, P., Huang, S., Zhai, Y., Hu, X., Yu, P. S., et al. survey on parallel text generation: From parallel decoding to diffusion language models. arXiv preprint arXiv:2508.08712, 2025. Zhao, S., Gupta, D., Zheng, Q., and Grover, A. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025a. Zhao, S., Liu, M., Huang, J., Liu, M., Wang, C., Liu, B., Tian, Y., Pang, G., Bell, S., Grover, A., et al. Inpaintingguided policy optimization for diffusion large language models. arXiv preprint arXiv:2509.10396, 2025b. Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al. 11 DVOTING : Fast Voting for dLLMs Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. Zhu, F., Wang, R., Nie, S., Zhang, X., Wu, C., Hu, J., Zhou, J., Chen, J., Lin, Y., Wen, J.-R., et al. Llada 1.5: Variancereduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. Zhu, J., Shen, Y., Zhao, J., and Zou, A. Path-consistency: Prefix enhancement for efficient inference in llm. arXiv preprint arXiv:2409.01281, 2024."
        },
        {
            "title": "Appendix",
            "content": "DVOTING : Fast Voting for dLLMs In Appendix A.1, we present the detailed process of our DVOTING. Appendix includes additional case analysis to further demonstrate the effectiveness of our DVOTING. We further discuss future works in Appendix C. Finally, we present the LLM usage statement in Appendix D."
        },
        {
            "title": "A The Details of DVOTING",
            "content": "A.1 The Detailed Algorithm of the Consistency-Guided Designs We provide the detailed algorithm for the pipeline of our DVOTING in Algorithm 1. The entire pipeline is simple and easy to implement. We implement this pipeline with the PyTorch library (Paszke et al., 2019). Compute the token consistency score at position from previous samples if the score at position does not satisfy the remasking criterion then Initialize sequence with prompt tokens and masked generation positions Initialize Boolean array {0, 1}L representing whether remask or not if Xall = then else for each generation position do Fix the token at position Set m[t] False Algorithm 1 Detailed Process of dVoting Require: Prompt p, dLLM fθ, maximum samples n, generation length 1: Xall {store all sampled generations} 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end if if all entries of are False then break {Stop sampling here!} Fix all tokens Set m[t] False for all {1, . . . , L} end if if there exist masked tokens in then Mask the token at position Set m[t] True end if end if end for Evaluate global token-level consistency across all positions if the consistency scores indicate that no token requires further remasking then Perform entropy-threshold-based parallel decoding with semi-autoregressive strategy {Details omitted as this is typical strategy!} end if Append the completed sequence to Xall 27: 28: 29: end for 30: return Xall 31: Perform voting on candidate answers in Xall to get final prediction {Voting part is here!}"
        },
        {
            "title": "B Case Analysis",
            "content": "As shown in Figure A1, we present additional examples from ARC-C and MATH500, further demonstrating that our method quickly produces correct answers on simple problems and refines initially incorrect answers on more challenging ones. Prompt 1 is an easy ARC-C question, where DVOTING early-stops after two samplings upon obtaining consistent answers. In contrast, Prompt 2 is harder MATH500 problem. While the base model outputs an incorrect answer in the first sampling, 13 DVOTING : Fast Voting for dLLMs Figure A1. Additional case visualization. DVOTING refines key tokens through multiple samplings and stops after four sampling runs when consistent answer is observed, yielding the correct result."
        },
        {
            "title": "C Future Work",
            "content": "Our method and experiments primarily focus on the language modality. As an emerging architecture, diffusion language models have demonstrated performance comparable to autoregressive models in language tasks, while also showing promise for extension to other modalities and tasks (Li et al., 2025a), such as multimodal question answering (Yu et al., 2025b; You et al., 2025; Yang et al., 2025b; Feng et al., 2025c; Yu et al., 2025a). Exploring parallel test-time scaling in these broader settings is promising direction for future work. Moreover, parallel test-time scaling is simple to implement (easy to code) and computationally lightweight (easy to run), making it easy to adopt and follow. In practice, DVOTING can be applied to resource-constrained and cost-sensitive deployment settings, such as on-device reasoning and large-scale inference services."
        },
        {
            "title": "D Large Language Model Usage Statement",
            "content": "Large language models were used only for minor language editing, such as improving grammar, clarity, and formatting. They did not contribute to the development of ideas, methods, algorithms, code, experiments, figures, or analyses. All technical work and experimental results were produced by the authors. Any model-assisted text was manually reviewed, and the use of LLMs does not affect the reproducibility of the work."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, National University of Singapore, Singapore"
    ]
}