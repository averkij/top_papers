{
    "paper_title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors",
    "authors": [
        "Yimeng Chen",
        "Piotr Piȩkos",
        "Mateusz Ostaszewski",
        "Firas Laakom",
        "Jürgen Schmidhuber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 5 5 5 1 . 7 0 5 2 : r PHYSGYM: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors Yimeng Chen1, Piotr Piekos1, Mateusz Ostaszewski1, Firas Laakom1, Jürgen Schmidhuber1,2,3 1Center of Excellence for Generative AI, KAUST 2The Swiss AI Lab, IDSIA-USI/SUPSI 3NNAISENSE"
        },
        {
            "title": "Abstract",
            "content": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PHYSGYM, novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PHYSGYMs primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PHYSGYM provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmarks utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity."
        },
        {
            "title": "Introduction",
            "content": "Automating aspects of the scientific discovery process holds immense promise for accelerating research across physics, chemistry, and biology [1, 2, 3, 4, 5, 6]. central capability for artificial intelligence (AI) agents in this pursuit is scientific reasoning, i.e. the ability to explore environments, gather evidence, form hypotheses and uncover underlying mechanisms [3, 7, 8, 9]. The idea of an autonomous AI Scientist dates back at least more than three decades to early work on artificial curiosity [10, 11, 12], which proposed agents that learn to explore in order to improve their internal world models. Recent advances in Large Language Models (LLMs) [13, 14], trained on vast corpus of scientific literature and equations, have sparked enthusiasm about their potential as generalpurpose scientific agents [14, 9, 15, 16]. Systems like the AI Scientist [9, 17] demonstrate closed-loop scientific workflows: generating hypotheses, designing experiments, analyzing results, and even writing papers. Other systems like SCIMON [18] and SCIAGENTS [19] leverage literature-based reasoning and multi-agent coordination to discover hypotheses and explore novel research directions. Yet despite these advances, fundamental question remains: how do these models actually reason about physical systems? The answer is obscured by critical limitation in how we evaluate these models. Existing evaluation frameworks [20, 21, 22, 23, 24] rely on static datasets or expose fixed sets of priors, lacking finegrained control over what contextual knowledge is available to the model. However, scientific reasoning depends critically on context. Consider simple pendulum experiment: if an agent is told that the system is harmonic oscillator and sees variables like \"length\" or \"gravity\" it can trivially Correspondence to: yimeng.chen@kaust.edu.sa Preprint. Figure 1: Controlled levels of prior knowledge. PHYSGYM lists three types of prior knowledge: Context - textual description of the environment, Variable Descriptions and Variable Names. The levels of prior start with full information disclosed to the model at Level 1 and then gradually strip the information from the model - removing the Context at Level 2, Variable Descriptions at Level 3 and then finally removing Variable Names as well by anonymizing variables at Level 4. match patterns to recover the canonical solution. However if those variables are anonymized or the description is hidden, the agent must engage in experimental probing. It has to vary inputs, observe outputs, and form structural hypotheses to discover the relationships. These scenarios involve identical physics but radically different cognitive demands. This distinction between pattern matching and mechanistic discovery lies at the heart of scientific reasoning, yet current benchmarks fail to disentangle them. This motivates the need for interactive, controllable benchmarks that evaluate AI models on how they adapt to unfamiliar problem settings, balance prior knowledge with posterior exploration (leveraging existing priors or compensating for missing ones through experimentation), and their ability to construct and modify physical models. To address this gap, we introduce PHYSGYM, novel benchmark suite and simulation platform designed for the assessment of agent-based scientific reasoning within interactive physics environments with controllable priors. At its core, physics discovery can be abstracted as curiosity-driven search for action sequences (experiments) that generate data containing previously unknown yet learnable regularity or compressibility [8], eventually leading to minimal descriptions of the data that explain the observations [25, 26, 27, 28, 29]. PHYSGYM operationalizes this abstract process within series of carefully designed, interactive physics environments. In each environment, an agents task is to discover the equation relating target observation to set of controllable variables. To achieve this, agents iteratively design experiments by proposing values for the controllable variables and receiving the corresponding outcomes as feedback. central feature of PHYSGYM is its fine-grained control over the prior information available to the agent. The platform provides structured environmental descriptions that detail the experimental setup, the physical meaning of variables, and their symbolic representations. By selectively revealing or masking these information (as illustrated in Figure 1), we can systematically investigate how varying degrees of prior knowledge affect an agents problem-solving and reasoning capabilities. Through comprehensive interface, agents in PHYSGYM interact with the environment, control experiments, and systematically document their discovery process. This interface also enforces realistic constraints that mirror actual scientific practice, most notably limited experimental budget. To assess performance, PHYSGYM measures both the fidelity between discovered hypothesis and the ground-truth physics equation, as well as how well the hypothesis fits the observed data. Together, these components provide robust insights into an agents scientific reasoning capabilities under realistic conditions. To demonstrate the utility of PHYSGYM and establish initial performance levels, we present baseline results using representative LLMs with direct prompting. Our results showcase that increasing benchmarking granularity into the inspection of prior knowledge allows to find distinctive performance pattern, leading to more hollistic understanding of models performance in different scenarios. These findings highlight the challenges posed by PHYSGYM and serve as valuable starting point for future research that aims to develop more robust AI agents for scientific discovery. In summary, this paper makes the following contributions: Figure 2: Overview of the PHYSGYM suite. PHYSGYM: novel benchmark suite and simulation platform for evaluating interactive LLM-based scientific reasoning, designed for systematic control over task complexity and prior knowledge. Tailored evaluation protocols and metrics for assessing performance and reasoning processes within the PHYSGYM framework. Experimental validation of PHYSGYM using representative LLMs, demonstrating its effectiveness in differentiating their capabilities and responses to varied conditions. The remainder of this paper is organized as follows: Section 3 details the design principles, simulation environment, and evaluation metrics of PHYSGYM. Section 4 presents the experimental setup and provides our observations from analyzing models in the PHYSGYM benchmark. Section 5 summarizes the paper, discusses the conclusions and limitations of the PHYSGYM."
        },
        {
            "title": "2 Related Works",
            "content": "AI Scientists. Early research in computational scientific discovery conceptualized discovery as heuristic search process, predicated on the view that creativity is an emergent property of specific computational mechanisms rather than an ineffable quality [30, 3, 31]. Systems from this era, such as BACON [1] and STAHL [32], demonstrated the capacity to rediscover known scientific laws from structured data, while others like AM [33] and Eurisko [34] applied similar heuristic methods to explore mathematical concepts. Later, Schmidhuber [5, 35, 8] established formal, computable theory of artificial curiosity and creativity [10, 11, 12] grounded in intrinsic motivation. This framework views inductive science as an active process of data collection and compression, where an agent actively generates experiments resulting in data that contains previously unknown patterns that allow for better compressing the observations through shorter programs. The architecture involves reinforcement learning-based controller that selects actions to generate new data for an adaptive world model which, in turn, learns to predict or compress the agents sensory history. The intrinsic reward that drives the controller is the measured learning progress of the world model as its compression of the perceptual data improves. Recent advances in large language models (LLMs) have revived these visions, producing autonomous scientific agents that can generate hypotheses, design and run experiments, and iterate on findings. Systems like the AI SCIENTIST [9] and its successor [17] demonstrate closed-loop scientific workflows: generating hypotheses, designing experiments, analyzing results, and even writing papers. Other platforms like SciAgents [19] and SciMuse [36] coordinate multiple agents or leverage massive knowledge graphs to uncover non-obvious research directions and interdisciplinary insights. The ability to explore and refine ideas has been recognized as central ability of AI scientists. For instance, SciMON [18] optimizes hypotheses for originality using literature-grounded novelty scoring, while ResearchAgent [37] simulates peer-review loops to refine research ideas iteratively. Ma et al. [38] proposed bilevel optimization-based framework, where the outer-level leverages LLMs to conduct 3 reasoning and generate hypotheses, while the inner-level employs simulations to provide feedback and perform numerical optimization. Emerging work also explores how agents can generate or evolve themselves: ADAS [39] evolves agent designs via meta-level optimization, while OMNI-EPIC [40] proposes tasks with increasing interestingness, forming self-curated curriculum for exploration. Designing benchmarks to evaluate AI capabilities in scientific research has become crucial for inspiring and advancing the development of AI scientists. Existing research has proposed several benchmarks to assess AI performance in scientific tasks. Current benchmarks primarily focus on coding [41, 42], problem-solving [43], and research engineering abilities within specific scientific domains for predefined tasks and datasets [44, 45], including training models, preparing datasets, and conducting experiments. However, benchmarks for fundamental research capabilities, such as experimental design and the formulation of reasonable hypotheses, remain largely underexplored. Equation discovery. Scientific inquiry often begins with the classification of objects and the construction of taxonomies, but as field matures, it increasingly emphasizes quantitative modeling through mathematical equations. This progression has led to the formalization of equation discoverythe task of uncovering underlying mathematical relationships from data or interactive environmentsas key research direction [1, 3, 46]. Closely related is the field of symbolic regression, which aims to infer interpretable mathematical expressions from data [47, 48, 49]. LLMs have recently emerged as promising paradigm for scientific equation discovery, offering new capabilities grounded in their broad scientific pretraining [50, 23, 51, 9]. Several LLM based methods have been developed for equation discovery. For instance, In-Context Symbolic Regression [52] leverages LLMs in an iterative error-correction loop, yielding simpler expressions with enhanced generalization capabilities. LLM-SR [50] frames the task as program synthesis, combining LLMgenerated symbolic skeletons with evolutionary search and parameter optimization to outperform traditional methods across multiple scientific domains. LLM4ED [53] guides equation generation and optimization through natural language prompts, employing self-improvement and evolutionary strategies to uncover governing equations in nonlinear dynamical systems. LLM-Feynman [54] integrates LLMs with Monte Carlo tree search and self-evaluation, showing the ability to rediscover over 90% of the Feynman physics equations. However, as current approaches are tailored to static datasets [22], the ability of models to generate hypotheses and design experiments remains inadequately assessed. Interactive benchmarks. In order to assess the full capabilities of different agents in complete scientific discovery process, there has been growing interest in developing interactive benchmarks that go beyond static datasets. These environments aim to capture the dynamic nature of scientific inquiry, where agents must iteratively design experiments, collect data, and revise hypotheses based on feedback. Science-Gym [20] provides 5 simple physics simulation environments where agents must achieve desired states by modifying the parameters of physical objects. However, as it was not designed for large language models, it does not consider the novelty of physics problems that presents unique challenges for LLM evaluation. CodeARC [55] focuses on inductive program synthesis discovery. Discovery World [21] offers 24 simulation environments structured across 8 distinct domains and 3 difficulty levels. Agents are required to perform various actions within these environments to accomplish specified task objectives. The environmental interactions typically involve common actions such as taking or moving objects. In contrast, PHYSGYM leverages 97 novel physics problem settings spanning 6 distinct physics domains. Our framework establishes more abstract and simplified environment with enhanced specificity, while providing fine-grained control over the prior knowledge embedded in linguistic descriptionsa core differentiating factor when evaluating LLMs compared to other model architectures. This design enables more targeted assessment of language models scientific reasoning capabilities within controlled experimental conditions."
        },
        {
            "title": "3 The PHYSGYM Benchmark",
            "content": "A key process in scientific discovery consists of formulating hypotheses based on prior knowledge and observational frameworks, carrying out experimental validation, and accordingly refining and developing theoretical models. PHYSGYM is designed to simulate this process in controllable abstract setting. 4 Specifically, we simulate an environment whose characteristics are described textually and which encompasses both controllable and observable variables. The latent relationship among these variables is governed by fixed, albeit unknown, mechanism function . The task of the AI physicist is to deduce hypothesis ˆf that is consistent with the true underlying mechanism of this environment. This must be accomplished based on the available information and conducting experiments within the environment, subject to limited budget. An experiment consists of specific assignment of values = (x1, . . . , xn), to controllable variables {xi}n i=1. Subsequently, the AI physicist can observe the environmental outcomes, i.e., the corresponding values of the observable results (x). The rest of this section introduces the benchmark developed for our study. We detail the structure of the dataset, the accompanying simulation environment designed to facilitate experimentation, and the evaluation metrics. 3.1 Dataset Construction Our current dataset is constructed from 97 physics problems selected from PHYBench [56]. These problems feature comprehensive descriptions of their physical context, and their solutions are presented as symbolic equations that define the relationship between target physical quantity and other relevant physical quantities. Furthermore, complete derivation process, grounded in physics principles, is provided for each problem. Consequently, this collection offers robust foundation, facilitating the development of our abstract environment. The construction of the data for each problem was meticulous process involving the assistance of LLMs followed by rigorous manual verification and refinement by domain experts. This hybrid approach allowed us to efficiently process the initial information while ensuring accuracy and relevance. Each data instance in our benchmark is structured with the following fields: context: This field contains the textual description of the physics problem. It serves as the contextual or environmental description from which model must derive understanding. solution: manually provided, step-by-step reasoning process that solves the problem based on established physics principles. This offers gold-standard derivation. equation: This represents the core physical relationship of the problem, expressed as an equation linking input and output physical quantities. It acts as the ground-truth model of the environment. python_code: An executable Python script that implements the equation. This code can simulate the environments behavior by taking numerical inputs and producing the corresponding output. Crucially, it also includes checks for the numerical validity of inputs and outputs. input_variables: list of variables that serve as inputs to the equation. Each variable is accompanied by its physical description and its corresponding unit (e.g., \"mass\" in \"kilograms\"). output_variable: The primary output variable that the equation predicts. Similar to input variables, it includes detailed physical description and its unit (e.g., \"velocity\" in \"meters per second\"). dummy_variables: This field lists physical quantities that are mentioned within the context (problem description) but are not causally related to the output_variable through the core equation. 3.2 The Simulation Environment Beyond the dataset itself, PHYSGYM includes comprehensive simulation environment designed to work with this benchmark. The primary architecture of PHYSGYM is depicted in Figure 2. At its heart, PHYSGYM integrates the environments derived from our dataset, including all associated metadata described in Section 3.1. The core components of the PHYSGYM simulation environment are introduced as follows. Environments. These are directly constructed from the 97 curated physics problems, each encapsulating the environment content, equation, and variable definitions. The associated code is transformed into an executable function which accepts numerical inputs for the defined input variables and executes the code to simulate the environments response, returning the corresponding outputs. Interface. This serves as the control panel for experiments and provides several key functionalities: Quota Management: Allows for setting limitations on experimental resources, such as the number of experiments or simulation steps. Environmental Prior Knowledge Control: Enables varying the level of prior information about the environment that is exposed to the model under test. Historical Data Management: Tracks and stores the history of interactions, observations, and model hypotheses. Automatic Evaluation Component: Integrates the evaluation metrics to provide automated assessment of model performance. 3.3 Sophisticated Prior Control key feature of PHYSGYM is its capacity for sophisticated control over the environmental prior knowledge available to an agent. By meticulously managing the availability of the environments metadata, we can precisely adjust the level of prior information an agent possesses when tackling problem. This fine-grained control allows for the systematic observation of differences in agent behavior and performance under varying degrees of physical priors. Importantly, this enables us to investigate how agents balance deductive reasoning (leveraging prior knowledge) with inductive learning (from new interactions), and how they navigate the interplay between theoretical understanding and experimental exploration. For instance, control can be exerted over: The visibility of the textual problem description (context). The availability of detailed physical descriptions for input_variables and the output_variable. Whether variable names adhere to common-sense physical conventions (e.g., for mass, for velocity) or are obfuscated (e.g., var1, var2). Figure 1 illustrates examples of four distinct levels of prior knowledge that can be configured within PHYSGYM: Level 1: In this setting, the task is simplified to reasoning problem where full observational access and testing capabilities are permitted. The agent is provided with rich contextual information, and solutions can, in theory, be derived entirely through deduction based on the provided priors. Level 2 & 3: These levels represent spectrum where partial information is provided. For example, in Level 2, variable descriptions are available but not the full problem context, challenging the agent to integrate incomplete priors with active exploration. Level 4: At this end of the spectrum, PHYSGYM-L4 degenerates into an interactive equation discovery environment. Most explicit prior knowledge (like textual descriptions or meaningful variable names) is withheld. The primary implicit prior is the understanding that the underlying relationships are governed by equations pertinent to the physical world. It is important to note that these four levels are illustrative examples and do not encompass all possible configurations. By selectively masking or revealing different combinations of metadata fields and variable information, more extensive array of nuanced prior knowledge settings can be created to probe specific aspects of agent learning and reasoning. This flexibility is paramount for studying how agents adapt to and leverage different informational landscapes. 3.4 Evaluation Metrics The primary evaluation metric for model performance is the Success Rate. This binary metric awards one point for each correctly solved task, with the final score representing the percentage of successfully completed tasks across the entire test set. Success Rate serves as the foundation for both comparative analysis between models and for measuring performance degradation when varying the prior information provided to the models. Each task is considered solved if the equation proposed by the model is equivalent to the groundtruth equation of the task. We employ two evaluation methods to determine this equivalence. The primary method, SymPy-based Symbolic Evaluation, rigorously verifies the mathematical equivalence between candidate and ground-truth equations. However, this method occasionally produces false negatives when encountering LLM-generated notations (e.g., np.pi) that SymPy fails to properly parse. To mitigate this limitation, we supplement with LLM-based Equivalence Assessment, which 6 Table 1: Results overview of LLMs by Prior Level. Acc denotes the success rate. The values in the table show the average number of experiments, tests, runs, unique hypotheses proposed ((U) Hyps), and total hypotheses proposed in successful examples. Model Mode Acc (%) Experiments Tests Turns (U)Hyps Total Hyps Gemini-2.5-flash OpenAI-o4-mini Claude-3.7-Sonnet L1 L2 L3 L4 L1 L2 L3 L4 L1 L2 L3 L4 65.98 49.48 36.08 30. 62.89 41.24 31.96 27.84 22.68 20.62 21.65 12.37 10.62 12.92 15.51 20.63 7.21 12.58 13.90 20.11 7.50 12.60 12.00 10.92 1.08 1.25 1.23 1. 1.21 1.15 1.19 1.26 1.27 1.30 1.29 1.00 2.28 2.60 2.77 3.43 2.28 3.02 3.10 3.96 2.36 3.20 2.86 2.33 1.77 2.06 1.54 1. 1.72 1.72 1.48 1.52 2.00 1.70 1.67 1.17 2.28 2.54 1.86 2.37 2.21 2.12 1.68 1.70 2.36 2.55 1.95 1.17 leverages an LLM to judge equation equivalence. task is considered successfully solved if either evaluation method confirms equivalence. Additionally, PHYSGYM incorporates suite of auxillary evaluation metrics to assess the performance of models in details. Consistency Metrics. fundamental sanity check for our evaluation framework is whether the models proposed equation remains consistent with previously observed data points. Since the model generates hypotheses based on its proposed variable valuations and corresponding outputs, we expect mathematically sound models to formulate equations that accurately fit these observations. To quantify this consistency, we employ multiple statistical metrics that measure the alignment between the proposed equations and the observed data points. These include the Coefficient of Determination (R2), Mean Squared Error (MSE), Kendalls Tau (τ ) rank correlation, and Mean Absolute Percentage Error (MAPE). Task Difficulty Metrics. To facilitate detailed analysis of the performance of the model, we incorporate quantitative metrics of the difficulty of the task. We employ two complementary heuristics to assess the complexity of ground-truth equations: equation length (measured by character count) and variable count. Our approach rests on two key observations about mathematical complexity: First, when comparing equations of equal length, those containing more variables typically present greater estimation challenges due to increased dimensionality of the problem space. Second, when the number of variables is constant, longer equations generally indicate more complex mathematical relationships, involving additional operations or nested functions that are inherently more difficult to model."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate representative LLMs in the PHYSGYM benchmark and analyze the results. 4.1 Experimental Setup We implement basic prompt-based method in which the prompt describes the objective of the task and specifies the required input and output data structures. The LLM is instructed to generate three key components: (1) specification of proposed experiments consisting of list of parameter valuations; (2) current hypothesis formulated by the model; and (3) Boolean flag that determines whether the current hypothesis warrants testing, which shows the models confidence in how well its hypothesis captures the underlying mechanism. The complete prompt is provided in the Appendix. 7 Oracle Test. During the experimental phase, the model is permitted to conduct one oracle test and receive its results. These results include fitness metric and judgment of symbolic equivalence. The interface will invoke the metrics module to return the test results, which are then incorporated into the experimental records and fed back to the model for the subsequent turn. Input Information. In each turn, the LLM is provided with the following information: (1) the physics context, (2) the symbolic names and natural language descriptions of controllable variables and the observable variable, (3) history of prior experimental records, and (4) the remaining quotas for experiments and oracle hypothesis tests. The content for (1) and (2) varies across different prior levels. Observational history is included in the experimental records. Prior Levels. We implement the 4 different prior levels as introduced in Section 3.3, denoted as L1 to L4. In L1 we provide the original context and full description of the variables. In L2 substitute context with \"Unknown context.\". In L3 we further use the meaningless description for the variables. In L4 we change the namespace of the variables to var1, var2, etc. Models & Configurations. We evaluated three distinct commercial models from three different companies: Gemini-2.5-flash:thinking [57], Claude 3.7 Sonnet [58], and OpenAI o4 mini [59]. Among these, Gemini-2.5-flash:thinking and OpenAI o4 mini are configured as reasoning models. By selecting models from diverse sources, we aim to observe variations in their inherent inductive biases. The total quota for experiments was set to 100. The temperature coefficient for the LLMs was configured to 0.3. The maximum token limit for Gemini-2.5-flash:thinking and OpenAI o4 mini was set to 50,000. The \"thinking\" level for OpenAI o4 mini was set to \"high\". The token limit for the Claude 3.7 Sonnet was set to 5,000. 4.2 Results and Observations Impact of Prior Knowledge on Model Performance. Intuitively, reducing prior knowledge should increase task difficulty and lower success rates for models that can effectively leverage such information. Our systematic evaluation across four prior knowledge levels confirms this expectation while revealing distinct patterns across LLMs. As shown in Figure 3, the thinking models, o4-mini and Gemini, both exhibit monotonic decrease in success rates as prior knowledge diminishes. This performance decline was substantial across all models; for instance, o4-minis accuracy dropped from 62.89% to 41.24% with the removal of contextual information, and performance at the lowest prior level (L4) was roughly half of that at the highest (L1) for all models  (Table 1)  . The non-thinking model, Claude 3.7 Sonnet, showed particularly interesting profile. It suffered the most pronounced accuracy drop from L3 to L4, indicating high sensitivity to variable nomenclature. However, it gained minimal benefit from the richer information in levels L3 to L1, suggesting its ability to leverage complex prior knowledge for reasoning is limited. Figure 3: Success rates of different models by prior level. For individual problems, success patterns are not monotonic. more detailed analysis (Appendix B.3) reveals non-monotonic inclusion relationship between the sets of problems solved at different prior knowledge levels. Specifically, some problems that are successfully solved at lower prior levels (e.g., L3 or L4) fail at higher prior levels (e.g., L1 or L2), and vice versa. This counterintuitive pattern indicates fundamental limitation in current models ability to consistently and rationally utilize prior knowledge across varying information contexts. These findings underscore the complex and non-linear effects of prior knowledge on model reasoning capabilities and validate that our 8 Figure 4: Model success rate as function of prior knowledge for tasks grouped by dimensionality. Figure 5: The change of numbers of unique hypotheses proposed at different prior levels. experimental setup provides valuable insights into how models process and integrate information when understanding physical environments. Prior is More Important For Difficult Task. To investigate the relationship between prior knowledge and model performance, we conducted more granular analysis across varying task difficulty levels. We hypothesized that task difficulty would significantly influence how models leverage prior knowledge when formulating experiments and environmental hypotheses. For quantitative assessment, we use the number of controllable variables as heuristic measure of task difficulty. We adopted it as reasonable proxy following the established principle that higherdimensional problems present greater modeling challenges. We categorized task dimensionality into four distinct groups: problems with 1-3 variables, 4-6 variables, 7-9 variables, and those with 10 or more variables. Figure 4 illustrates the success rate for each model as function of prior knowledge and difficulty level. Our analysis revealed several noteworthy patterns. First, performance consistently decreased as difficulty increased, validating our difficulty heuristic. Furthermore, we observed that tasks of varying difficulty responded differently to prior knowledge levels. This distinction was particularly evident in the performance transition from L3 to L4. For moderate difficulty tasks (4-6 variables), this transition produced minimal performance degradation. However, for higher difficulty tasks (7-9 variables), we observed substantial performance drop between these same prior levels. For environments with 10+ variables, o4 and Gemini only solved the task at L1. This suggests that current models rely on prior knowledge to solve more complex environments, lacking the ability to design effective experiments. Unique hypotheses vs prior levels. The number of distinct hypotheses can reflect the models ability to dynamically adjust its conjectures in response to experimental outcomes. Figure 5 shows the evolution of the number of unique hypotheses synthesized by the model throughout the entire 9 process. We capture all hypotheses from the models output, including those flagged as not to be tested. Overall, Gemini demonstrates the highest ability for hypothesis adjustment. In particular, under conditions of greater uncertainty, it exhibits significant ability to adapt its strategies. In successful instances, the diversity of hypotheses proposed by Claude and o4 mini diminishes as the prior level increases. This observation further substantiates that, when prior knowledge is limited, Claudes success is not substantially influenced by posterior observations, relying instead heavily on its inherent biases. In contrast, Gemini maintains comparatively higher diversity of hypotheses at level 4, suggesting that its ability to revise conjectures based on observations possesses stronger generalization capabilities across varying prior levels. Exploration Dynamics in Data Collection Under Varying Priors. As prior knowledge is reducedfrom L1 (full prior) to L4 (no prior)all models exhibit clear increase in the number of interaction experiments, indicating greater reliance on active experimentation. For example, Gemini-2.5-flash increases its experiment count from 10.6 at L1 to 20.6 at L4, while OpenAI-o4-mini increases from 7.2 to 20.1. This trend reflects shift from low-effort recognition strategies to more exploratory behaviors as the prior structure is removed. In particular, these increases are accompanied by an increase in turns, tests, and generated hypotheses, suggesting that models not only interact more frequently but also perform more complex reasoning cycles under uncertainty. The contrast across models is equally informative. While Gemini and OpenAI-o4-mini both increase their testing and hypothesis-generation behavior with reduced priors, Claude-3.7-Sonnet shows more erratic pattern. Although it increases its samples from L1 to L2, it does not scale its interaction efforts as sharply at L3 and L4. In fact, its sample count slightly declines at L4, and its accuracy remains considerably lower across all levels (e.g., 22.7% at L1 and only 12.4% at L4). This may indicate that Claude is less capable of using feedback loops or fails to appropriately escalate its exploration strategy when prior structure is unavailable. Overall, these results highlight the importance of interactive evaluation settings that vary prior information. They reveal not just whether models can succeed in task, but how they approach scientific reasoning. In this context, the number of interaction samples becomes proxy for epistemic engagement: models that scale their behavior with uncertainty are arguably demonstrating more principled and scientific approach to discovery. This underscores the central motivation behind Physicist: to move beyond static benchmarks and provide controlled, interactive environment where both reasoning strategies and discovery dynamics can be rigorously assessed."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduced PHYSGYM, test suite designed for benchmarking the scientific discovery capabilities of LLM-based agents within simulated interactive environment. Specifically, we evaluated the ability of LLMs to formulate hypotheses based on varying levels of prior knowledge and observations, design useful experiments, and iteratively refine these hypotheses to approximate the true latent mechanism. We constructed structured data derived from existing physics problem benchmarks and provided simulation environment with control interface and evaluator to support our interactive test protocol. Our experiments on representative LLMs revealed distinct behaviors and capabilities. We confirmed the intuitive finding that increasingly detailed prior knowledge generally leads to higher task success rates, validating that models can effectively leverage background information. Furthermore, our analysis elucidated that prior knowledge becomes most critical as task difficulty increases and highlighted stark differences between model types: reasoning-enabled LLMs effectively leveraged priors for dynamic hypothesis adjustment and strategic exploration, especially under uncertainty. In contrast, non-reasoning models often displayed erratic, sometimes detrimental, responses to priors and relied more on inherent biases with limited information. The interactive nature of PHYSGYM was crucial for observing these nuanced strategies and how the models intensify experimentation with diminishing priors, demonstrating its value in rigorously assessing LLM scientific discovery processes beyond static benchmarks. These findings underscore the importance of such interactive environments for understanding and advancing AI in science, suggesting that fostering robust reasoning and adaptive exploration is key to developing LLMs as effective scientific partners. 10 Limitations. current limitation is the static nature of the problem set within the PHYSGYM benchmark. The existing dataset is fixed and is based on manual construction. key area for future work is the development of methods for the automated generation of new physics environments and problem instances. Moreover, the existing approach to quantifying task difficulty relies on relatively simple heuristics, such as equation length and the number of variables. While these serve as useful proxies, they are approximations of the true underlying complexity. more precise and fundamental set of metrics for characterizing task difficulty should be designed."
        },
        {
            "title": "References",
            "content": "[1] Pat Langley. Bacon: production system that discovers empirical laws. In IJCAI, page 344. Citeseer, 1977. [2] Edward Feigenbaum, Bruce Buchanan, and Joshua Lederberg. On generality and problem solving: case study using the dendral program. Machine Intelligence, 6, 09 1970. [3] Patrick W. Langley, Herbert A. Simon, Gary Bradshaw, and Jan M. Zytkow. Scientific Discovery: Computational Explorations of the Creative Process. The MIT Press, 1987. [4] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):4760, 2023. [5] Jürgen Schmidhuber. Driven by compression progress: simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. In Workshop on anticipatory behavior in adaptive learning systems, pages 4876. Springer, 2008. [6] Nathan Brown, Peter Ertl, Richard Lewis, Torsten Luksch, Daniel Reker, and Nadine Schneider. Artificial intelligence in chemistry and drug design, 2020. [7] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. science, 324(5923):8185, 2009. [8] Jürgen Schmidhuber. Artificial scientists & artists based on the formal theory of creativity. In 3d Conference on Artificial General Intelligence (AGI-2010), pages 148153. Atlantis Press, 2010. [9] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The arXiv preprint AI scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292, 2024. [10] Jürgen Schmidhuber. possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, pages 222227. MIT Press, 1991. [11] Jürgen Schmidhuber. Adaptive confidence and adaptive curiosity. Technical Report FKI-149-91, Institut für Informatik, Technische Universität München, 1991. [12] Jürgen Schmidhuber. Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, volume 2, pages 14581463. IEEE, 1991. [13] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [14] Daniil Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023. [15] Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. comprehensive survey of scientific large language models and their applications in scientific discovery. arXiv preprint arXiv:2406.10833, 2024. 11 [16] Qiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al. Scientific large language models: survey on biological & chemical domains. ACM Computing Surveys, 57(6):138, 2025. [17] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. [18] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. SciMON: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [19] Alireza Ghafarollahi and Markus J. Buehler. SciAgents: Automating scientific discovery through bioinspired multi-agent intelligent graph reasoning. Advanced Materials, 2024. Early Access. [20] Mattia Cerrato, Nicholas Schmitt, Lennart Baur, Edward Finkelstein, Selina Jukic, Lars Münzel, Felix Peter Paul, Pascal Pfannes, Benedikt Rohr, Julius Schellenberg, et al. Science-gym: simple testbed for ai-driven scientific discovery. In International Conference on Discovery Science, pages 229243. Springer, 2024. [21] Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents. Advances in Neural Information Processing Systems, 37:1008810116, 2024. [22] Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D. Doan, and Chandan K. Reddy. LLM-SRBench: new benchmark for scientific equation discovery In Proceedings of the International Conference on Machine with large language models. Learning (ICML), 2025. [23] Chandan Reddy and Parshin Shojaee. Towards scientific discovery with generative ai: Progress, opportunities, and challenges. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2860128609, 2025. [24] Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, and Yoshitaka Ushiku. Rethinking symbolic regression datasets and benchmarks for scientific discovery. arXiv preprint arXiv:2206.10540, 2022. [25] Ray Solomonoff. preliminary report on general theory of inductive inference, 1960. [26] Ray Solomonoff. formal theory of inductive inference. part i. Information and control, 7(1):122, 1964. [27] R. J. Solomonoff. Complexity-based induction systems. IEEE Transactions on Information Theory, IT-24(5):422432, 1978. [28] C. S. Wallace and D. M. Boulton. An information theoretic measure for classification. Computer Journal, 11(2):185194, 1968. [29] Christopher Wallace. Statistical and inductive inference by minimum message length. Springer Science & Business Media, 2005. [30] Herbert Simon. Scientific discovery and the psychology of problem solving. In Models of discovery: And other topics in the methods of science, pages 286303. Springer, 1977. [31] Pat Langley. Heuristics for scientific discovery: The legacy of herbert simon. Models of Man: Essays in Memory of Herbert A. Simon, MIT Press, Cambridge, pages 461472, 2004. [32] Donald Rose and Pat Langley. Stahlp: Belief revision in scientific discovery. In Proceedings of the Fifth AAAI National Conference on Artificial Intelligence, pages 528532, 1986. [33] Douglas Lenat. The ubiquity of discovery. Artificial Intelligence, 9(3):257285, 1977. 12 [34] Douglas Lenat. Eurisko: program that learns new heuristics and domain concepts: the nature of heuristics iii: program design and results. Artificial intelligence, 21(1-2):6198, 1983. [35] Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (19902010). IEEE Transactions on Autonomous Mental Development, 2(3):230247, 2010. [36] Xuemei Gu and Mario Krenn. Interesting scientific idea generation using knowledge graphs and LLMs: Evaluations with 100 research group leaders. arXiv preprint arXiv:2405.17044, 2025. [37] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2024. [38] Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik. LLM and Simulation as Bilevel Optimizers: New Paradigm to Advance Physical Scientific Discovery. In International Conference on Machine Learning, 2024. [39] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. [40] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. OMNI-EPIC: Open-endedness via models of human notions of interestingness with environments programmed in code. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. [41] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: research coding benchmark curated by scientists. Advances in Neural Information Processing Systems, 37:3062430650, 2024. [42] Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138, 2025. [43] Ludovico Mitchener, Jon Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi Wellawatte, Andrew White, Lorenzo Sani, and Samuel Rodriques. Bixbench: comprehensive benchmark for llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025. [44] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [45] Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against human experts. arXiv preprint arXiv:2411.15114, 2024. [46] Sašo Džeroski and Ljupˇco Todorovski. Discovering dynamics. In Proc. tenth international conference on machine learning, pages 97103, 1993. [47] John Koza. Genetic programming as means for programming computers by natural selection. Statistics and computing, 4:87112, 1994. [48] Brenden Petersen, Mikel Landajuela, Nathan Mundhenk, Claudio Santiago, Soo Kim, and Joanne Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871, 2019. [49] Terrell Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio Santiago, Brenden Petersen, et al. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. Advances in Neural Information Processing Systems, 34:2491224923, 2021. [50] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan Reddy. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400, 2024. [51] Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, and Dongxiao Zhang. Large language models for automatic equation discovery of nonlinear dynamics. Physics of Fluids, 36(9), 2024. [52] Matteo Merler, Katsiaryna Haitsiukevich, Nicola Dainese, and Pekka Marttinen. In-context symbolic regression: Leveraging large language models for function discovery. arXiv preprint arXiv:2404.19094, 2024. [53] Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, and Dongxiao Zhang. Llm4ed: Large language models for automatic equation discovery. arXiv preprint arXiv:2405.07761, 2024. [54] Zhilong Song, Minggang Ju, Chunjin Ren, Qiang Li, Chongyi Li, Qionghua Zhou, and Jinlan Wang. LLM-feynman: Leveraging large language models for universal scientific formula and theory discovery. arXiv preprint arXiv:2503.06512, 2025. [55] Anjiang Wei, Tarun Suresh, Jiannan Cao, Naveen Kannan, Yuheng Wu, Kai Yan, Thiago SFX Teixeira, Ke Wang, and Alex Aiken. Codearc: Benchmarking reasoning capabilities of llm agents for inductive program synthesis. arXiv preprint arXiv:2503.23145, 2025. [56] Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. [57] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [58] Anthropic. Claude 3.7, Sonnet, https://claude.ai/, 2025. [59] OpenAI. o4 mini, https://chatgpt.com/, 2025."
        },
        {
            "title": "A Dataset Statistics",
            "content": "This table presents the statistical overview of our physics dataset, which contains 97 samples distributed across six different fundamental physics domains. Each sample is characterized by an average of 4.7 input variables and 1.0 dummy variables. The context length varies considerably across domains, with Optics having the longest average context (1241.4 characters) and Mechanics having the shortest (845.9 characters). Domain # Samples # Input Variables # Dummy Variables Length of Context (char) Table 2: Dataset Statistics Optics Mechanics Electricity Thermodynamics Modern Advanced All 7 39 31 10 7 3 97 4.9 4.1 5.5 4.8 4.3 5. 4.7 0.6 1.2 1.0 1.1 0.4 0.3 1.0 1241.4 845.9 1026.8 885.1 857.4 1217.7 948."
        },
        {
            "title": "B Additional Results Analysis",
            "content": "B.1 Efficiency Metrics Beyond accuracy and task complexity, we evaluate how economically the model arrives at its hypotheses. We therefore introduce three complementary efficiency metrics. Iteration efficiency quantifies the average number of experimental rounds (i.e., proposals of variable valuations and corresponding evaluations) required for the model to reach predefined performance threshold on consistency metrics. Sample efficiency measures the total count of experiment samples the model consumes before achieving the success, highlighting its ability to learn from limited observations. Hypothesis efficiency captures the number of candidate equations the model generates and discards prior to converging on final hypothesis; lower values indicate more targeted exploration of the hypothesis space. Together, these metrics provide holistic view of the models resource requirements in terms of iterations, data samples, and hypothesis evaluations. Figure 6: Performance metrics for different models. Figure 6 presents several performance metrics averaged over all tasks, regardless of level, for various models. The results show that Gemini Flash demonstrates strong performance across all dimensions, while Claude exhibits particular strength in the sample efficiency dimension. B.2 Fitness fundamental requirement for any scientific reasoning model is that its predictions align closely with past observations. In the ideal case, every data point lies exactly on the hypothesis curve. More realistically, we demand that the model incur low empirical riskfor example, small mean squared error or high coefficient of determination R2 on these observations. We assess the goodness-of-fit of the model proposed hypothesis functions by measuring the R2 metric on the experiments observed from previous executions. higher value of R2 indicates better fit to the data, reflecting stronger hypothesis-data consistency. We report the maximum Fit Quality across all hypotheses for each models complete experimental history at the point when each hypothesis was tested. The results are presented in Figure 7. Several key observations emerge from this analysis: 15 (a) Level 1 (b) Level 2 (c) Level 3 (d) Level Figure 8: Venn diagrams illustrating the overlap of problems solved by 3 models at each level. (a) Gemini-2.5-flash:think (b) OpenAI o4-mini-high (c) Claude-3.7-Sonnet Figure 9: UpSet plots illustrating the intersection of problems solved by the model at four levels. Superior performance of thinking models. Consistent with the overall success rates, thinking models demonstrate significantly superior Fit Quality compared to the non-thinking model across all prior knowledge levels. This advantage is expected, as thinking models can verify their hypotheses during the reasoning process and adapt them to better align with experimental constraints. Prior Knowledge vs. Observations Balance. Thinking models effectively balance prior knowledge and observations. As prior knowledge is reduced, their fit quality shows only modest decrease, indicating they can incorporate experimental evidence to form accurate hypotheses. In contrast, the non-thinking Claude model displays counterintuitive trend: its fit quality worsens with more prior knowledge. This suggests the model anchors too heavily on prior information, failing to integrate conflicting experimental data. B.3 Overlaps between the solved sets Figure 7: The quality of fit of models hypotheses and historical experimental results, grouped by prior level. In this section, we aim to explore the following questions: How significantly do different models differ in the sets of problems they can solve? How much overlap exists in the problem sets that the same model can solve under different prior knowledge levels? If problem is solved at level with less information, is it more likely to be solved at levels with more information? Figure 8 shows the relationships between problem sets solved by three different LLMs at each given level. We use Venn diagrams to illustrate the intersections between these sets. Overall, the problem sets solved by Gemini-flash and o4-mini become more divergent as the level increases. At levels 1, 2, and 4, Claudes solved problem set is contained within those of the other two models. Level 3 is an exception. Combined with Figure 7, we observe that Claudes behavior at level 3 differs significantly from the other two models, which may indicate that Claude has stronger symbolic priors. Figure 9 demonstrates the relationships between success sets of the same model across different levels. We employ UpSet plots, where solid dots below the graph represent included sets and hollow dots represent excluded sets. Several interesting observations emerge: 4 problems were solved only at levels 1, 2, and 4; 3 problems were solved only at levels 1 and 4; and another 3 problems were 16 Figure 10: Venn diagram showing the overlap of problems solved by the three models across all levels. Figure 11: The change of numbers of unique hypotheses proposed at different prior levels. solved only at levels 2, 3, and 4. The existence of these cases indicates that our assumption does not hold universallyprior knowledge does not always facilitate problem solving. We will conduct detailed case analyses in Section to examine possible reasons. Comparing across models, we find that Gemini-flash and o4-mini exhibit similar distribution patterns, while Claude shows significant differences, with many problems solved only at specific levels. This may suggest that Claudes ability to reason based on existing theoretical knowledge is limited, which aligns with expectations given that it is not reasoning model. Figure 10 further illustrates the differences between models. Even for the set of problems that can be solved across all prior knowledge levels, there are significant differences among the different models. B.4 Results with Gemini 2.5 Pro We further incorporated one of the current strongest reasoning models, Gemini 2.5 Pro (gemini-2.5-pro-preview, which we refer to as Gemini-pro), and conducted tests on Physicist using the basic agent prompt. The test results for success rates across different prior levels are shown in Figure 12. Notably, Gemini-pro significantly outperforms Gemini-flash in L1 and L2 scenarios, which rely more heavily on linguistic prior knowledge. However, in L3 and L4 scenarios, which are more mathematics-oriented and emphasize posterior interaction, Gemini-pros performance is comparable to that of Gemini-flash, showing no significant improvement. We compare the number of unique hypotheses generated by Gemini-pro (Figure 11). On correct examples at L1, Geminipro produces the fewest unique hypotheses, demonstrating its superior capability for physics-based prior reasoning. At L2, it generates the most unique hypotheses, which corresponds 17 Figure 12: Success rates of different models by prior level. to the scenario where it shows the most significant improvement in success rate compared to Flash, indicating its enhanced ability to adjust hypotheses based on experimental feedback. For failed cases, Gemini-pro proposes significantly more unique hypotheses than other methods, suggesting its stronger exploratory capabilities."
        },
        {
            "title": "C Case study",
            "content": "In this chapter, we discuss results from specific case studies. These examples demonstrate behavioral differences across various prior knowledge levels and illustrate how prior knowledge influences hypothesis formation and experimental design. They also reveal different capability deficiencies exhibited by models under current baseline implementations. In the first example, Env. 310, we observe that models with incomplete prior knowledge lack the ability to reason about causal relationships between variables and to explore the sample space effectively. In Env. 409, we find that variable namespaces significantly influence the models hypothesis formation. In Env. 716, prior knowledge actually impairs model performance, as experimental design is heavily influenced by preconceptions about physical quantities. These findings indicate substantial room for improvement in current methods, particularly in effectively leveraging prior knowledge and balancing reasoning with exploration. For convenience, in the following discussion, we will use these abbreviated names: Gemini-2.5flash:think will be referred to as Gemini-flash, Gemini-2.5-Pro as Gemini-pro, OpenAI-o4-mini-high as o4-mini, and Claude-3.7-Sonnet as Claude. C.1 Environment 310: only solved at Level 1 for all models Data context: Consider an ideal mirror moving at relativistic velocity, with mass and area S0. (The direction of photon incidence is the same as the direction of the mirrors motion.) Now consider the case where the mirror is moving with an initial velocity β0c. In this situation, the mirror is unconstrained by external forces, and photons are incident on it with constant power for certain period of time, with energy E. Assuming the mirrors velocity after irradiation is β1c, find the expression for β1. equation (in LaTeX format): β1 = (cid:16)(cid:113) 1+β0 1β0 (cid:16)(cid:113) 1+β0 1β0 + 2E m2997924582 + 2E (cid:17)2 (cid:17)2 1 + 1 input_variables: β0: Initial velocity of the mirror as fraction of the speed of light (dimensionless, between -1 and 1). E: Energy of the incident photons (in joules, J). m: Mass of the mirror (in kilograms, kg). output_variable: β1: Final velocity of the mirror as fraction of the speed of light (dimensionless, between -1 and 1). dummy_variables: S0: Area of the mirror (in square meters, m2). 18 Discussion As provided by PHYBench, the answer can be derived from the following two equations: + (cid:112) mc2 1 β0 2 = + mc (cid:112) 1 β"
        },
        {
            "title": "E\nc",
            "content": ", 2 + mcβ0 (cid:112)1 β0 2 = mcβ1 (cid:112)1 β1 2 which is straightforward with the knowledge of relativistic effects. The Level 2 configuration of this problem presents significant test of the models ability to utilize existing knowledge for reasoning and experimental design. The key issue lies in the relativistic correction term present in the solution. In Geminis initial experimental setup, all observations appeared to conform to the pattern β1 = β0. This occurred because the ratio of to was too small, resulting in output values virtually identical to β0. Throughout subsequent reasoning, the model struggled to overcome the influence of the β1 = β0 hypothesis, which repeatedly emerged during Geminis thinking process. This reveals several critical issues: First, regarding experimental design, the model did not explore sufficiently extreme scenarios, instead adopting relatively conservative parameter ranges. Second, the model failed to investigate potential relationships between controllable variables or attempt to discover underlying causality through experimentation. The model should have been able to infer possible relationships between incident photons and mirror velocity by examining logical associations in the variable hypotheses, including how energy and mirror mass affect the final outcome. By connecting this to relevant relativistic knowledge, the model should have recognized the probable inclusion of light-speed term, prompting the use of significantly larger E/m values in experimental settings. Concerning the inaccuracy of the β1 = β0 hypothesis, the model merely assumed the existence of correction term without reasoning through its underlying cause. This indicates substantial room for improvement in the models ability to reason about potential relationships between known entities in unfamiliar environments. Furthermore, this example illustrates the fundamental distinction between generating data points through interaction and inductively reasoning from existing data points. The sampling scope employed during data construction naturally incorporates prior knowledge, which is therefore inconsistent with the assumption-free problem setting we establish in Level 4. C.2 Environment 409: only solved at Level 1 & 2 In the case of Environment 409, both Gemini-flash and o4-mini successfully derived the correct equation only at levels 1 and 2. Notably, we observed that model performance at level 3 was inferior to that at level 4, suggesting that prior knowledge from symbolic namespaces can sometimes impede the models posterior observations. We also experimented with Gemini-pro, which similarly failed at level 3 but arrived at the correct conclusion at level 4. Data context: In electromagnetism, we often study the problem of electromagnetic field distribution in region without charge or current distribution. In such cases, the electromagnetic field will become tubular field. The so-called tubular field is named because of the nature of the velocity field of an incompressible fluid at every instant. Its characteristic can be described using the language of vector analysis as the fields divergence being equal to zero. Another perspective is that if we take the field lines of vector field FFF (the tangent line at each point being the direction of field strength) and select flux tube along set of field lines passing through certain cross-section, then at any cross-section of the flux tube, the field flux: (cid:90) dS = Φ will be conserved quantity. This problem will investigate special case of tubular field that is rotationally symmetric around the axis: using oblate spheroidal coordinates to construct unique tubular field as an electric field or magnetic field. The so-called oblate spheroidal coordinate system is similar to spherical coordinates, and it is generated by the following coordinate transformations: = achµ cos ν cos φ = achµ cos ν sin φ = ashµ sin ν We want the shape of the field lines of an electric field or magnetic field to precisely follow its generatrix direction, i.e., the tangent direction of the curve where field strength changes with µ while ν, φ remain fixed (upwards when > 0). Find the charge distribution in the xy plane that can produce this electric field distribution. Assume the field strength near the origin is E0. The dielectric constant is ε0. equation (in LaTeX format): input_variables: σ = 2ε0E0a a2 r2 ε0: Dielectric constant (permittivity of free space) (in farads per meter, F/m). E0: Electric field strength near the origin (in volts per meter, /m). a: Parameter of the oblate spheroidal coordinate system (in meters, m). r: Radial distance from the origin in the xy plane (in meters, m). output_variable: σ: Surface charge density in the xy plane (in coulombs per square meter, C/m2). dummy_variables: None Discussion In this case, we examine the detailed responses of Gemini-flash at different levels to analyze the discrepancies in its behavior. Level 1 We present Geminis level 1 reasoning process in the subsequent colorbox. Through the models returned reasoning process, we observe extensive iterative repetition in the models reasoning. Compared to the reference solution, it actually omits several key concepts, such as equipotential surfaces and symmetry. Nevertheless, the final answer is correct, indicating that the interpretability of the reasoning process remains insufficient. Level 2 In the level 2 reasoning process, critical step occurs when the model hypothesizes correct form based on physical knowledge: σ = ϵ0 E0 (a, r). By considering the presence of a2 r2 in the result, thus obtaining hypothesis that the disk, it infers the existence of the term differs from the correct formula by only constant in the first test. However, it should be noted that in the experimental design at level 2, the model selects the true physical value for ϵ0. This behavior actually provides no assistance in formula inference and represents an example of excessive reliance on prior knowledge. Level 3 At level 3, none of the models successfully completed the task, including Gemini-pro. However, the data fitting quality of their proposed models was consistently high. The answer of the Gemini-flash at the final test, after 20 observations: 8 3 (cid:114) ϵ0E0 0.2 + 0. The answer of o4-mini, after 35 observations: (cid:18) ϵ0E0 2 + 3.48 (cid:17)2.81(cid:19) (cid:16) 20 They both fit the observed data very well, with MSE less than 1 1017. The answer of Gemini-pro after 33 observations: (cid:16) ϵ0E0 2 4 3 (cid:17) + (cid:0)4 1 2(cid:1) 3 6 The MSE is also small, less than 1 104. Through observation, we find significant variation in the hypotheses proposed by different models, yet interestingly, they predominantly conform to the form ϵ0 E0 (a/r). This is likely bias introduced by the physical variable naming conventions. Level 4 At level 4, the hypotheses proposed by the models differ substantially from those at level 3, no longer containing the (a/r) term. For example, after 16 observations, Gemini-flash reached the following final hypothesis: 2 var3 (cid:112)var2 3 which achieves an MSE of less than 1 105. If we further allow it continue the experiment, when the number of samples is extended to 26, the model successfully derives the correct function. Notably, Gemini-pro demonstrates superior efficiency, obtaining the correct answer after only 14 observations. C.3 Environment 716: only solved at Level 3 & 4 We observed particularly notable case, Environment 716, where both Gemini-flash and o4-mini successfully solved the problem only at the lower-prior settings of levels 3 and 4. This indicates that the environmental context and semantic descriptions of variables actually impaired model performance. Data context: The principle of rotational speed measurement and control device is as follows. At point O, there is positive charge with an electric quantity of Q. lightweight, smooth-walled insulating thin tube can rotate around vertical axis through point in the horizontal plane. At distance from point inside the tube, there is photoelectric trigger control switch A. lightweight insulating spring with free length of L/4 is fixed at the end, and the other end of the spring is connected to small ball with mass and positive charge q. Initially, the system is in static equilibrium. The thin tube rotates about fixed axis under the action of an external torque, allowing the small ball to move within the thin tube. When the rotational speed ω of the thin tube gradually increases, the small ball reaches point in the thin tube and just achieves radial equilibrium relative to the thin tube, triggering the control switch. The external torque instantaneously becomes zero, thus limiting excessive rotational speed; at the same time, the charge at point becomes an equal amount of negative charge -Q. By measuring the position of the radial equilibrium point of the small ball relative to the thin tube thereafter, the rotational speed can be determined. If the distance OB is measured to be L/2, determine the rotational speed ω of the thin tube when the ball is at point B. Express the result using the following physical quantities: Electric charge Q, balls electric charge q, mass m, length L, and Coulombs constant k. equation (in LaTeX format): Heres your reformatted content: input_variables: (cid:114) ωB = 4 13kqQ 23mL3 k: Coulombs constant (in newton-meters squared per coulomb squared, m2/C 2). q: Electric charge of the small ball (in coulombs, C). Q: Electric charge at point (in coulombs, C). 21 m: Mass of the small ball (in kilograms, kg). L: Distance from point to point in the tube (in meters, m). output_variable: ωB: Rotational speed of the thin tube when the ball is at point (in radians per second, rad/s). dummy_variables: None Discussion The answers obtained by Gemini-pro at levels 1 and 2 are as follows: (cid:114) 3 kqQ mL3 (cid:114) 3.0072 kqQ mL We observe that the experimental data designed for levels 1 and 2 exhibit particular characteristic: the numerical values chosen for the variables closely approximate their actual values in the physical context. For example, in the following response: { } \"next_experiments\": [ {\"k\": 8000000000.0, \"q\": 1e-06, \"Q\": 1e-06, \"m\": 0.01, \"L\": 0.1}, {\"k\": 9000000000.0, \"q\": 0.5e-06, \"Q\": 0.5e-06, \"m\": 0.01, \"L\": 0.1}, {\"k\": 9000000000.0, \"q\": 1e-06, \"Q\": 1e-06, \"m\": 0.005, \"L\": 0.1}, {\"k\": 9000000000.0, \"q\": 1e-06, \"Q\": 1e-06, \"m\": 0.01, \"L\": 0.3}, {\"k\": 10000000000.0, \"q\": 3e-06, \"Q\": 3e-06, \"m\": 0.002, \"L\": 0.05} ], \"test_hypothesis_flag\": true, \"current_hypothesis_formula\": \"3.0072 * (np.abs(k * * Q) / (m * L**3))**0.5\" This choice of numerical values is unnecessary for formula inference. While such settings may be more likely to yield valid numerical results initially, once hypothesis has been formed, the model should explore broader range of variable configurations. This demonstrates that prior knowledge can sometimes constrain the models capacity for exploration and innovation. 22 Env 409 Level 1 (1st Response of Gemini-2.5-Flash) Thinking... Evaluating the Physics Im now considering the users request, they need me to perform in my areas of expertise: Physics and Experimental Design. This is good challenge. Lets see how to best approach this. Deconstructing the Problem need to understand the users question about σ. have to find formula relating σ to ϵ0, E0, a, and based on the given electric field. The geometry uses oblate spheroidal coordinates, which adds layer of complexity, but think can handle it. Lets break down the coordinates in the xy-plane. Pinpointing the Geometry Im analyzing the users requirements, specifically the request for the electric field in the xy-plane, examining the oblate spheroidal coordinates. am zeroing in on the cases where the z-coordinate is zero, and the relationships between µ, ν, and r. This will clarify the charge distribution. Refining the Analysis Im re-examining the geometry, particularly how the xy-plane is represented with oblate spheroidal coordinates, and how the origin (r=0) is approached. Considering mu and nu, and their ranges for the xy-plane, will determine what the formula σ should depend on, relating to ϵ0, E0, a, and r. Conceptualizing the Approach must use my knowledge of physics and experimental design, starting with the users request. need to derive σ based on the input variables (ϵ0, E0, a, r) and the context of the xy-plane. have to keep in mind, that based on the problem statement, the xy-plane represents boundary with zero divergence. Calculating Surface Charge Density must use the users specifications: my expertise, electric fields, oblate spheroidal coordinates, and z=0 plane charge density. Considering the divergence-free field, am focusing on how the electric field behaves at the origin (r=0) as zero in on the geometry. Im relating the xy-plane to the coordinate ranges of µ. Formulating the Equations must understand the users instructions about σ in the xy-plane. have to connect σ to the given variables, ϵ0, E0, a, and r, using the electric field described in oblate spheroidal coordinates. Lets see about the implications for the boundary conditions. Re-evaluating Coordinate Ranges Im using the users constraints, particularly the requirement to determine σ in the xy-plane. Understanding the coordinate system is vital. Im refining the coordinate mappings for the xy-plane, focusing on the radial distance concerning µ and ν, and how they relate to the disk and the exterior plane. Revising the Approach Im analyzing the users request, where they need me to calculate σ. Ive to consider the xy-plane as boundary with zero divergence, alongside the given field strength and the variables. Im now refining my coordinate analysis, looking for the origin. Addressing the Boundary Im now focusing on the users request, determining the surface charge σ in the xy-plane using the given variables. Analyzing the coordinate system reveals two cases: inside the disk, and outside the disk, > a. The geometry dictates that the coordinate ranges will be crucial for determining the charge density. Focusing on Boundary Conditions Im checking the users request, which requires me to derive σ. Im thinking about the problem in the xy-plane, considering the divergence-free nature of the electric field. This informs how the electric field at the origin must align. Reframing the Question Im focusing on the users request: finding the surface charge σ in the xy-plane. Im relating it to the known variables: ϵ0, E0, a, and r. The goal is to find physical model that satisfies all constraints. 23 Defining Surface Charge Density must use my physics expertise, including the users input, such as σ, to determine the surface charge density. The problem states an electric field with zero divergence, especially within the xy-plane. This, alongside the given variables, sets the stage for calculations. Decomposing the Field Equations Im carefully analyzing the users query: am requested to find σ as it relates to the given E0, ϵ0, a, and r. need to connect it to the z=0 plane by assessing the boundary behavior of electric fields. This problem involves the xy-plane. Rethinking the Problem Im analyzing the users request, their question about finding surface charge density, σ, and Im trying to link this to the xy-plane. Focusing on how the E0, a, and are behaving within the constraints of the problem. Reviewing Coordinate Systems Im re-evaluating the users request to find sigma and the given variables: ϵ0, E0, a, and r. need to relate them to the electric field in the xy-plane and consider coordinate system variations. Then, will check for boundary conditions. Calculating the Charge Density must apply my knowledge of physics and experimental design to determine the surface charge density, σ. Ill focus on the users input, the variables ϵ0, E0, a, and r, keeping in mind the z=0 plane and the divergence-free nature of the electric field. Verifying Field Direction Im considering the users instructions, to construct tubular field and to find σ at z=0. Im now re-evaluating the coordinate system to confirm the field direction and how it relates to the xy-plane. Rethinking Boundary Conditions Im analyzing the users request, reviewing the xy-plane, and the need to find σ. need to apply boundary conditions at z=0, to the given variables and the electric fields behavior. also must keep the tubular field constraint in mind. Focusing on the Solution Im re-examining the users prompt, needing σ in the xy-plane using the given parameters. Analyzing the coordinate system reveals two cases: inside the disk (r a) and outside the disk (r > a). My next step is to investigate the field direction relative to the z-axis. Finalizing Charge Distribution need to connect the users parameters with σ. need to determine the electric fields normal component near the disk. am now finalizing how the divergence-free constraint affects the boundary conditions at z=0. will apply Gausss law near the disk. Rethinking the Problems Core am re-evaluating the users query and its constraints. need to determine sigma in the xyplane using the electric field and the given variables. must now carefully consider boundary conditions, and the implications of divergence. This may impact how analyze the geometry. Pinpointing Key Parameters Im re-examining the users query and the data. Considering the boundary conditions, field strength, and the goal of finding sigma in the xy-plane. The origin and how to model the disk is also important. Calculating the Charge Distribution need to find σ given the physics context and the users request. With the electric field equations established, must now focus on applying the correct boundary conditions at the z=0 plane. am seeking to relate the given values for ϵ0, E0, a, and to the surface charge density. Ill start by applying Gauss law. Deriving Surface Charge Density Im breaking down the users request: to find the formula for σ, relating it to the given variables. have an electric field with zero divergence, therefore will consider how the fields direction aligns with the z-axis in both regions, considering their individual coordinate ranges. Finalizing the Calculation need to derive σ given the data, analyzing the electric field, the xy-plane, and boundary conditions. am now working through the disk (r a) and the plane (r > a). will apply Gauss Law to find final formula. 24 Revising the Approach Based on the users prompt, Im re-examining how to calculate σ within the xy-plane. must consider the field strength and the specified variables. Also need to account for the boundary conditions at z=0, using coordinate systems. Solving for Density Im working on the users question, focused on calculating the surface charge density σ in the xy-plane. The electric field and the problem geometry, involving the parameters ϵ0, E0, a, and r, and the boundary conditions, are all key to the solution. Im now analyzing and comparing the electric fields above and below the plane. Deriving Solution must understand the users prompt to connect σ to the knowns: the electric field, and parameters. will use my physics expertise, especially Gausss law and the electric fields boundary conditions. Im starting by finding σ in the z=0 plane and its relationship to the disk. Deriving Field Model Im currently studying the users details: their request to determine the surface charge density, σ, in the xy-plane using the electric field and ϵ0, E0, a, and r. must assess boundary conditions at z=0, while considering coordinate systems. will use Gauss law for this calculation. Pinpointing Surface Charge must solve for σ within the xy-plane using the known values and my expertise. Considering the tubular electric field, the origin, and the xy-plane boundary, need way to apply Gauss law. Formulating Solution need to relate the users inputs (ϵ0, E0, a, r) to σ within the xy-planes conditions. The boundary behavior of the electric field at z=0 is also relevant, alongside the specified electric field. Lets apply Gausss law. Refining the Calculation need to use my knowledge of physics and experimental design, considering the users request for σ and given variables. Calculating the surface charge density in the xy-plane given the field. This also involves understanding the geometry and boundary conditions in order to formulate the solution. Calculating the Charge Im evaluating the users prompt about σ, tying it to my expertise and the given variables. The challenge is to derive correct formula for the xy-plane. Focusing on boundary conditions at z=0 and the electric fields divergence. The origin (r=0) and how it approaches µ is also important. Formulating Model need to find formula for σ in the xy-plane using ϵ0, E0, a, and r. Im focused on how the xy-plane behaves, and the electric field near the origin, considering both cases, when < a, and when > a. The field lines must satisfy zero divergence, which impacts my understanding of the coordinate system. Deriving Final Solution Im finally focused on answering the user. The question is σ, and will use the constraints: the electric field, the given variables, boundary conditions, and especially the geometry in order to determine final formula. will focus on the xy-plane, particularly applying Gauss Law, to determine the charge. Finalizing the Calculation need to derive σ in the xy-plane given the specified values, analyzing the electric field, and using the boundary conditions. am now working through the inside and outside cases to apply Gauss Law and finalize the formula. Formulating Final Solution will use my knowledge of physics and experimental design to address the users request. Im to use the electric field and the known variables in order to find σ on the xy-plane. The disk and exterior plane have separate formulas for this value. Calculating the Surface Charge 25 must analyze the users input. am to use the equations to find σ in the xy-plane, applying the electric field and the known variables. The approach will involve applying boundary conditions at z=0. will look for the discontinuity to apply Gauss law. Confirming Charge Distribution need to find the formula for σ in the xy-plane based on the users electric field description and the variables. am focusing on Gauss law and am considering how the boundary conditions at z=0 affect the electric field and the surface charge. Deriving Formula must fulfill the users request for σ, linked to the provided parameters. am now deriving the field equations and boundary conditions at = 0, and Im focusing on applying Gauss law to the xy-plane. This will lead to formula for my tests. Finalizing the Formula must derive the formula for σ in the xy-plane with my physics knowledge, the electric field and boundary conditions. The formula, derived from the known variables, will define the charge. Formulating Hypothesis need to derive formula for σ based on the users request, using the electric field, boundary conditions, and the given values. After careful analysis, will test my understanding and refine the model. Refining the Calculation need to derive formula for σ based on the users request, using the given electric field and variables. The xy-plane and its implications for the boundary conditions will guide me in this. Considering the known values, Im aiming for precise equation. Formulating Final Solution have to find σ from the users request by assessing the xy-plane and utilizing my physics knowledge. Gausss law will be essential. Ill derive formula, ensuring it aligns with the electric fields behavior and boundary conditions, starting with the well-known charged disk formula. will now test the values, to verify my hypothesis. Deriving Solution must create formula for σ tied to ϵ0, E0, a, and r, while keeping the xy-plane and its special boundary in mind based on the users request. will be using the information to produce the solution. will use the electric field and boundary conditions. Formulating Final Solution have to use the users instructions to determine σ using my physics knowledge. Ill determine the formula, considering the electric field, xy-plane, and all given variables and boundary conditions. Gausss law will be useful. have determined the proper charge density formula. Formulating the Formula Im analyzing the users input, considering the desired formula relating σ to ϵ0, E0, a, and r. The xy-plane, with its specific boundary conditions, is crucial. Now, am focusing on the final formula, key step. Confirming the Field Model Im now ready to provide the formula for the user. My analysis has led to formula for the charge density σ as function of the given values. The electric field, with its boundary conditions, and the xy-plane has been key, especially by applying Gauss law. Ill now test this formula. Refining the Equation need to calculate σ given the users specifications, considering that Im an expert in Physics and Experimental Design, and the σ formula must be based on the electric field. Ive chosen formula for the observable values, using ϵ0, E0, a, and r. Deriving Formula Im analyzing the users input, requesting σ for the xy-plane based on the given values. Im relating this to the boundary conditions and the z=0 plane to define the charge density. will use Gausss Law in my final step. Refining the Calculation 26 need to derive formula for σ based on the users input, using the given electric field and variables. Im finding the formula relating σ to ϵ0, E0, a, and r, with the xy-planes geometry in mind. am now preparing to test and evaluate my model, using the given information. Formulating Plan am evaluating the users prompt about calculating σ, given the parameters, and am considering the conditions of the xy-plane. must use my understanding of physics and experimental design. Lets apply Gausss Law. Refining the Derivation Im reviewing the users request, focused on the calculation of surface charge density in the xy-plane. Im connecting this to the variables given, and making sure the boundary conditions are considered and accurately represent the physics. Then will formulate the correct formula. Calculating the Formula Im working on the users specifications, particularly σ in the xy-plane. Ill be using the formula, relating to ϵ0, E0, a, and r, and the condition < for this calculation. After assessing the theory Im ready to propose the formula. Response: { } \"next_experiments\": [ {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 1, \"r\": 0}, {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 1, \"r\": 0.5}, {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 1, \"r\": 0.8}, {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 1, \"r\": 0.9}, {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 2, \"r\": 1}, {\"epsilon_0\": 1, \"E_0\": 1, \"a\": 0.5, \"r\": 0.25}, {\"epsilon_0\": 1, \"E_0\": 2, \"a\": 1, \"r\": 0.5}, {\"epsilon_0\": 1, \"E_0\": 0.5, \"a\": 1, \"r\": 0.5}, {\"epsilon_0\": 2, \"E_0\": 1, \"a\": 1, \"r\": 0.5}, {\"epsilon_0\": 0.5, \"E_0\": 1, \"a\": 1, \"r\": 0.5} ], \"test_hypothesis_flag\": false, \"current_hypothesis_formula\": \"2 * epsilon_0 * E_0 / (1 - (r/a)**2)**0.5\" Env 409 Level 2 (1st Response of Gemini-2.5-Flash) Deducing Variable Relationships Im analyzing the users request, specifically the problem_description lack of context. The goal is to determine the formula connecting σ to ϵ0, E0, a, and r. Considering that ϵ0 is fundamental and E0 is electric field strength. The parameter seems significant. Hypothesizing & Unit Checks Analyzing the users parameters to extract formula relating σ to the other variables. Using unit analysis, see that multiplying ϵ0 by E0 results in the correct units for σ. Next, Ill check how and might fit. Revisiting Formula Derivation Given the users input on σ, ϵ0 and E0, Ill focus on refining the formula. From the units, multiplying ϵ0 and E0 gives us σ, so this part is correct. Now, Ill consider how and fit the formula for the specific geometry. Final Formula Consideration The user is asking for the formula relating σ to the provided variables. My initial hypothesis is σ = ϵ0 E0 (a, r). Analyzing conducting disk in an electric field, suspect a2 r2 component. Time to refine the equation. Refining & Initial Hypothesis Considering the users parameters. Given the electric field E0, dielectric constant ϵ0, characteristic length a, and radial distance and knowing that the observable variable σ is surface 27 charge density. Combining ϵ0 and E0 yields σ. hypothesize that σ = ϵ0 E0 (a, r), considering conducting disk. Re-evaluating Hypothesis Formulation Based on the users inputs, specifically ϵ0, E0, and r, plus my initial hypothesis. Ill try refining my approach, as the initial hypothesis may be too simplistic. Using the units from earlier, Ill reformulate to create the expected observable, σ. Confirming Initial Equation Im analyzing the relationships between σ, ϵ0, E0, a, and r, using ϵ0 E0. My hypothesis: σ = ϵ0 E0 a2 r2 (1) as first pass. Ill use this to guide experimental design. Validating & Experiment Planning Given the users variables, including σ, ϵ0, and E0. Considering the geometry mentioned, a2 r2. Ill set the test flag to false and will use the initial formula σ = ϵ0 E0 a/ design 5 initial experiments. Formulating the Equation Given the users variables, particularly σ, ϵ0, and E0, and incorporating the geometry of conducting disk with radius and distance r, my initial hypothesis is: σ = ϵ0 E0 a2 r2 (2) Response: { } \"next_experiments\": [ {\"epsilon_0\": 8.854e-12, \"E_0\": 500, \"a\": 0.2, \"r\": 0.1}, {\"epsilon_0\": 8.854e-12, \"E_0\": 1000, \"a\": 0.2, \"r\": 0.1}, {\"epsilon_0\": 8.854e-12, \"E_0\": 500, \"a\": 0.4, \"r\": 0.2}, {\"epsilon_0\": 8.854e-12, \"E_0\": 500, \"a\": 0.2, \"r\": 0.18}, {\"epsilon_0\": 8.854e-12, \"E_0\": 500, \"a\": 0.2, \"r\": 0.02} ], \"test_hypothesis_flag\": false, \"current_hypothesis_formula\": \"epsilon_0 * E_0 * / np.sqrt(a**2 - r**2)\""
        },
        {
            "title": "D Prompt",
            "content": "Researcher Prompt # Core Task Based on the provided problem description, controllable physical quantities, observable physical quantities, and historical experimental records, your primary goal is to: 1. Deduce the mathematical expression (formula) that describes how the observable quantity changes with the controllable physical quantities 2. Propose the next set of experimental parameters to test 3. Determine if its appropriate to formally test your current hypothesis # Inputs The input will be JSON dictionary with the following five parts: 28 1. **problem_description**: (String) * brief description of the physical phenomenon or system under study * Example: \"Investigating the relationship between the extension of spring and the applied force.\" 2. **controllable_variables**: (Dictionary) * Physical quantities that can be actively changed in an experiment * The dictionary contains: * Keys: The name of the physical quantity (e.g., \"F\", \"k\") * Values: Detailed description of the physical quantity * Example: json {\"F\": \"The applied force on the spring in Newtons (N)\", \"k\": \"The spring constant in Newtons per meter (N/m)\"} 3. **observable_variable**: (Dictionary) * The physical quantity measured in the experiment * This quantity changes in response to changes in controllable variables * Example: json {\"x\": \"The extension of the spring in meters (m)\"} 4. **historical_experiments**: (List of Dictionaries) * Data from previously conducted experiments * Each dictionary represents single experimental record * Contains all controllable variables and their set values, plus observed results * Example: json [ {\"F\": 0.5, \"k\": 10, \"x\": 0.05}, {\"F\": 1.0, \"k\": 10, \"x\": 0.10} ] * May be an empty list [] for the first run 5. **quota**: (Dictionary) * Defines the remaining budget for experimentation and hypothesis testing. * experiments_quota: (Integer) The maximum total number of remaining experimental runs you can try. * test_quota: (Integer) The remaining number of times you can set the test_hypothesis_flag to true. * Example: json {\"experiments_quota\": 10, \"test_quota\": 2} # Outputs You must output JSON dictionary with the following three parts: 1. **next_experiments**: (List of Dictionaries) 29 * Your designed input variable combinations for the next experiment(s) * Each dictionary represents set of experimental parameters * Keys are the name of controllable physical quantities * Values are your proposed settings within allowed ranges. Use floats or integers. * Strategy: * For empty historical data: Provide exploratory initial experimental points * For existing data: Select the most informative points to test your hypothesis * Ensure experimental diversity and coverage 2. **test_hypothesis_flag**: (Boolean) * true: Your current hypothesis is mature and next experiments aim to validate it * false: Youre in exploratory phase with low confidence, seeking more data 3. **current_hypothesis_formula**: (String) * Your conjectured mathematical expression relating observable quantity to controllable quantities (in Python syntax with basic operators) * Use the exact variable names defined in the input * IMPORTANT: Use \"**\" operator for powers/exponents. Only if it is necessary, you can use \"np.xxx\" to implement special functions, e.g. \"np.cos\". * Examples: \"F / k\", \"a * (F ** 2) + * + c\" * Return None or \"\" if data is too scarce to form meaningful hypothesis # Example (First interaction without historical data) **Input:** json { \"problem_description\": \"Investigating the relationship between the extension of an ideal spring (within its elastic limit) and the applied force F. The spring constant is k.\", \"controllable_variables\": {\"F\": \"The applied force on the ideal spring in Newtons (N).\",\"k\": \"The spring constant in Newtons per meter (N/m).\"}, \"observable_variable\": {\"x\": \"The extension of the spring in meters (m).\"}, \"historical_experiments\": [], \"quota\": {\"experiments_quota\": 10, \"test_quota\": 2} } **Expected Output:** json { \"next_experiments\": [ {\"F\": 0.5, \"k\": 10}, {\"F\": 1.0, \"k\": 10}, {\"F\": 2.0, \"k\": 10}, {\"F\": 1.0, \"k\": 20}, 30 {\"F\": 1.0, \"k\": 5} ], \"test_hypothesis_flag\": false, \"current_hypothesis_formula\": \"F / k\" }"
        }
    ],
    "affiliations": [
        "Center of Excellence for Generative AI, KAUST",
        "NNAISENSE",
        "The Swiss AI Lab, IDSIA-USI/SUPSI"
    ]
}