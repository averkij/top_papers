{
    "paper_title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "authors": [
        "Akhiad Bercovich",
        "Mohammad Dabbah",
        "Omri Puny",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Ehud Karpas",
        "Itay Levy",
        "Zach Moshe",
        "Najeeb Nabwani",
        "Tomer Ronen",
        "Itamar Schen",
        "Elad Segal",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 0 9 8 1 . 3 0 5 2 : r FFN FUSION: RETHINKING SEQUENTIAL COMPUTATION IN LARGE LANGUAGE MODELS Akhiad Bercovich*, Mohammad Dabbah*, Omri Puny*, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, and Ran El-Yaniv NVIDIA, {abercovich, mdabbah, opuny, relyaniv}@nvidia.com ABSTRACT We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253BBase (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves 1.71 speedup in inference latency and 35 lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have emerged as one of the most transformative technologies of our time, revolutionizing how we approach artificial intelligence and computation. From powering sophisticated virtual assistants (Guan et al., 2023) to enabling breakthrough scientific research (AI4Science & Quantum, 2023; Taylor et al., 2022), these models have evolved from academic curiosities (Radford et al., 2019) into indispensable tools that are reshaping entire industries. This transformation has been driven by rapid scaling to hundreds of billions of parameters (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), enabling unprecedented capabilities in reasoning, generation, and complex problem-solving (DeepSeek-AI et al., 2025; Guan et al., 2025). However, this extraordinary growth in scale and capability comes at critical cost: the computational demands of running these models have become fundamental bottleneck. As these models push the boundaries of what is possible with artificial intelligence, their deployment costs and resource requirements severely limit their accessibility, creating an urgent need for innovations that can make their capabilities widely available. Research in LLM runtime optimization explored diverse approaches to managing computational demands. Traditional techniques like quantization (Dettmers et al., 2022; Frantar et al., 2022; Dettmers & Zettlemoyer, 2023; Xiao et al., 2023)which reduce memory footprint and accelerate inference through lower-precision arithmeticand pruning (LeCun et al., 1989; Hassibi & Stork, 1992; Han et al., 2015; Li et al., 2016; Frankle & Carbin, 2018)which removes *Equal Contribution. redundant parametershave become standard tools. more recent innovation, Mixture-of-Experts (MoE) (Shazeer et al., 2017; Jiang et al., 2024), has demonstrated remarkable potential by dynamically activating only small subset of model parameters during inference. The DeepSeek-V3 architecture (DeepSeek-AI et al., 2024) pushes this approach to new extremes, employing 256 expert FFN modules at each layer while activating only 8 experts per token (plus one shared expert), effectively achieving the capabilities of much larger model while maintaining reasonable computational costs through sparse activation. However, each of these approaches faces distinct challenges: quantization encounters precision-accuracy trade-offs at small bit numbers, pruning struggles to identify significant additional redundancy without compromising accuracy performance (and pruning can be harnessed to efficiency mainly when it is structured), and MoE architectures, while efficient for single-token inference or for very large batches, do not provide optimal throughput at small and intermediate batch sizes due to under-utilization of compute resources, as discussed in Appendix D. Given the above limitations, there is an urgent need for complementary approaches that can unlock new dimensions of efficiency improvement while maintaining the simplicity and predictable scaling characteristics of dense architectures. In this work, we introduce two major contributions that fundamentally advance the state of LLM efficiency. First, we present FFN Fusion, novel architectural optimization technique that challenges the conventional sequential nature of transformer computation, and is illustrated in Figure 1. By identifying and exploiting patterns of computational independence in FFN layers, our approach enables parallel execution across multiple GPUs while preserving model functionality. This parallelization is particularly effective on modern GPU nodes, where tensor-parallel implementations often suffer from synchronization delays between consecutive layers. By concentrating the computation into fewer layers and reducing cross-device communication, our method significantly improves hardware utilization. Our findings reveal that substantial portions of LLM computation can be parallelized with minimal accuracy impact, complementing existing runtime optimization techniques like pruning and quantization. Second, we demonstrate the practical impact of these insights through Ultra-253B-Base, state-of-the-art and publicly available 253B parameter model derived from Llama-3.1-405B-Instruct (henceforth Llama-405B), derived using FFN Fusion and attention pruning. This model not only showcases the scalability of our approach, but also achieves remarkable efficiency gains while maintaining or exceeding its parent models capabilities across comprehensive suite of benchmarks. The applicability of FFN Fusion rests on fundamental insight about modern LLM architectures. Recent work has shown that LLM display structural redundancy (Voita et al., 2019; Michel et al., 2019; Fan et al., 2019; Zhang & He, Figure 1: An overview of our FFN Fusion approach. Step 1: We apply Puzzle to partially remove FFN layers and remove entire attention layers. Step 2: We fuse consecutive FFN layers into single wide FFN layer. 2 2020; Sajjad et al., 2023; Lad et al., 2024; Gromov et al., 2024), particularly attention mechanisms, which can be selectively removed with minimal accuracy impact (Bercovich et al., 2024; He et al., 2024), often leaving models with extended sequences of consecutive FFN layers. We demonstrate that these FFN sequences exhibit surprisingly low inter-layer dependencies, enabling novel transformation: multiple sequential FFN layers can be fused into single, wider layer that enables simple parallel execution. This transformation is particularly powerful, eliminating synchronization points while allowing for more efficient hardware utilization without requiring architectural redesign (as visualized in Figure 10). Through analysis and empirical validation, we argue for the conditions under which this fusion preserves model behavior, showing that these conditions are commonly satisfied in practice, especially in larger models where the potential efficiency gains are most significant. Most surprisingly, our preliminary investigations suggest that even complete transformer blockscontaining both attention and FFN componentscan sometimes be parallelized, pointing to potential new directions in neural architecture design (see Section 6). Leveraging these insights, we develop Ultra-253B-Base, derived from Llama-3.1-405B through combination of attention pruning and FFN Fusion. This model achieves remarkable efficiency gains while maintaining or exceeding its parent models capabilities: 1.71 speedup in inference latency and 35 lower per-token cost at batch size 32. State-of-the-art performance on key benchmarks, including 84.92% on Arena Hard, 86.58% on HumanEval, 87.54% on MMLU Instruct, 72.25% on MMLU-Pro, and 9.19 on MT-Bench. Improving memory footprint with 2x reduction in kv-cache memory, and reduced parameter count from 405B to 253B. Our key contributions are: The discovery that some Feed-Forward Networks in transformer architectures can often be parallelized with minimal accuracy loss, challenging the conventional wisdom that sequential processing is necessary for these layers. comprehensive methodology for implementing FFN Fusion at scale, validated through extensive experiments across model scales from 49B to 253B parameters, demonstrating that fusion effectiveness actually increases with model size and can complement existing optimization techniques. The creation of Ultra-253B-Base, powerful 253B parameter model that matches or exceeds Llama-3.1-405Bs capabilities while offering substantially improved efficiency, which we release to the public domain. Preliminary evidence that even complete transformer blockscontaining both attention and FFN componentscan sometimes be parallelized, opening new possibilities for architectural innovations in large language models. Beyond their immediate impact, our contributions lay groundwork for future optimization: FFN Fusion operates orthogonally to techniques like quantization, pruning, and sparse inference, suggesting the potential for multiplicative efficiency gains when these approaches are combined and offering new perspectives on transformer architecture design."
        },
        {
            "title": "2 Preliminaries",
            "content": "Transformer-based LLMs. The structure of LLMs is typically based on the widely used transformer architecture (Vaswani et al., 2023). It is generally composed of series of sequential blocks, denoted as 1, . . . , m, each containing an attention layer and an FFN layer. The attention layers capture the contextual relationships between input tokens, while the FFN layers apply transformations independently to each token. Given an input Rnd, where represent the sequence length and the embedding dimension, transformer block : Rnd Rnd is defined by the following equations: (1) (2) xi sk with xi Rd being the where ηk is token-level normalization module, defined by the equation ηk(xi) = xi i-th token, sk Rd being learnable scale factor, and is the element-wise product. The FFN layer used in the models we experiment with is the SwiGLU module (Shazeer, 2020), which is defined as follows: g(X) = + Attention(η1(X)) (X) = g(X) + FFN(η2(g(X))), (3) where σ is the SiLU activation (Hendrycks & Gimpel, 2016) function, W1, W2 Rdhde and W3 Rdedh. Here, de represents the embedding dimension, and dh denotes the hidden dimension of the FFN. Other components of the SwiGLU(X) = (σ(XW 2 ) XW 1 )W 3 , 3 LLM include an embedding function at the models input, which maps tokens to vectorized embeddings, an additional normalization function, and linear function at the output, which normalizes the final block outputs and projects them to the vocabulary dimension. Puzzle. Puzzle (Bercovich et al., 2024) is neural architecture search (NAS) framework that optimizes trained LLM for inference efficiency. Starting with pre-trained parent model, it systematically prunes or reconfigures each transformer blockoften reducing or removing attention layerswhile preserving model quality through distillation process. In particular, applications of Puzzle often show that many attention layers can be removed with minimal accuracy loss, thus leaving sequences of FFNs uninterrupted by attention. Additionally, certain FFN layers experience significant channel pruning, leading to reduction in their hidden dimension. Step 1 in Figure 1 illustrates hypothetical outcome of this process."
        },
        {
            "title": "3 FFN Fusion",
            "content": "As part of the model optimization done by the puzzle algorithm, many of the base models attention layers are removed. An attention-removed transformer block is defined by the equation ˆf (X) = + FFN(η2(X)). Given consecutive sequence of attention-removed blocks ˆf i, . . . , ˆf i+c we define the parallel version of this sequence ˆf [i,i+c] as (4) ˆf [i,i+c](X) = + FFNi+j(η2(X)), (5) (cid:88) j=0 where FFNi is the FFN layer of block and η2 is taken from the last layer in the sequence. While in the sequential form the input for every FFN depends on the output of the previous layers, in this form all the FFN layers share the same input, which makes their computation independent. Another useful property of this formulation is that equation 5 is equivalent to equation 4 with single wider FFN (in case it is implemented as in Equation 3) when the weights are concatenated. Theorem 3.1. Let N, and let FFN1, . . . , FFNn be sequence of FFN functions, where the weights of FFNi are 3. Then, the sum of these FFN functions (equation 5) is equivalent to single FFN function FFN, with the weight matrices given by: 2, 1, 1 = (cid:2)(W 1 2 = (cid:2)(W 1 3 = (cid:2)(W 1 )T (cid:3)T 1 )T , . . . , (W 2 )T (cid:3)T 2 )T , . . . , (W 3 )(cid:3) , 3 ), . . . , (W where [, . . . , ] denotes the concatenation of matrices along the second axis and the dimensions of the matrices are 2 Rndhde , 3 Rdendh . 1 , The theorem (and proof in Appendix A) are written for the simple case where dh is equal for all the FFNs. The extension for the case where the dimension is different for each layer is straightforward. While written in the context of Equation 3, the theorem holds for other common FFN variants (Shazeer, 2020; So et al., 2022). Efficiency motivation and analysis. LLMs are ubiquitously designed in sequential blocks, with the block sizes and the number of blocks increasing as the models grow in size. For bigger models, parallelization techniques such as tensor parallel (TP) are utilized to split work across GPUs and reach acceptable inference latencies. However, this strategy does not result in linear latency improvements with the number of GPUs applied. The first reason, which is easily apparent, is the communication time required for the allreduce that follows each TP block. The second reason is more subtle: GPUs are at their best for very large operations. As each atomic operation (GEMM) becomes smaller, low level overheads (GPU wave quantization) become more apparent and take larger portion of the latency budget. Therefore, increasing the computation per GPU (by increasing the size of block) while reducing the number of synchronizations needed (by using fewer blocks) is an effective strategy for low latencies in TP setting. Drawing from this motivation, we attempt to fuse many sequential blocks/FFNs in LLMs into fewer, larger blocks/FFNs. Each single reduction in the depth of the computational graph removes one unit of time spent on synchronization, and the bigger fused blocks also enable operating at higher TP numbers with enough computation assigned to each GPU. In Figure 5 we show the effects described in this analysis using measurements of different model architectures in the TP setting. 4 Pairwise block dependency. It is reasonable to hypothesize that not every transformer block in sequential model is equally dependent on all its predecessors. In particular, given block may depend only on subset of the blocks that precede it. To investigate this, we perform pairwise dependency analysis between blocks. Let us define h(X) = (X) as the contribution of to X, and similarly, hi as the contribution of i. We define hi as the contribution of block when block is removed from the model. We construct dependency matrix Rmm by computing the following cosine distance: Mij = CosineDist(hj(X), hj (X)). That is, Mij quantifies the dependency of block on block i. Thus, small cosine distance indicates that dropping block has little effect on block j, suggesting relative independencea characteristic that can be exploited for increasing parallel computation. Conversely, large cosine distance corresponds to strong dependency, implying that sequential processing is more critical to maintain performance. We illustrate this approach in Figure 2 by constructing dependency matrix, , for Ultra-PreFusion (prior to FFN Fusion; see Section 4). This matrix visually encodes the interdependencies among the models layers: darker blue hues indicate weaker dependenciessignaling promising opportunities for FFN Fusionwhile darker red hues denote strong dependencies that would hinder parallelization or fusion. For example, Figure 2 shows that all layers in the model depend on layers 0 and 5, causing their entire rows to be colored in dark red hues. The dark blue region, marked with dashed square, shows sequence of FFNs with low interdependency were selected for FFN Fusion. Although we use Ultra-PreFusion as an exemplar here, the same method can be applied to any model. Due to GPU memory constraints, we are unable to fuse them all at once, so we divide them into fusion sequences based on the maximum size that fits within our devices. Additionally, we applied this metric to explore block parallelization, aiming to parallelize general LLM blocks rather than just FFNs. The dataset that was used for this evaluation is Distillation Mix (Bercovich et al., 2024). Further details and results can be found in Section 6. Figure 2: Block-wise dependency heatmap for Ultra-PreFusion (log-scale). Each coordinate (i, j) encodes how much block depends on block i, measured by the cosine distance between hj(X) and hj (X). Darker blue hues indicate weaker dependencies. The attention-removed region (dotted box) shows consistently lower values, suggesting greater potential for parallelization. Darker red hues indicate stronger dependencies. Further analysis of this Figure can be found in Appendix E."
        },
        {
            "title": "4 Producing Large-Scale Models with FFN Fusion",
            "content": "In this section, we describe how FFN Fusion is applied at large scale to transform 405B-parameter Llama-based model into our more compact Ultra-253B-Base model. Specifically, we show how identifying and fusing long sequential feed-forward blocks reduces depth without sacrificing accuracy. We utilize lightweight refinement KD and alignment phase to ensure the fused model retains or even improves upon the performance of its larger predecessor. Finally, we present comprehensive evaluation of the resulting Ultra-253B-Base model, demonstrating significant speedups and strong results on standard benchmarks. Base 405B derivative. We first run the standard Puzzle search on Llama-405B, specifying that the derivative model must obtain 1.5 latency speedup and fit within single NVIDIA 8H100 node (640 GB total), and in single B100 5 Figure 3: Comparison of Ultra-253B-Base before and after applying an additional longer continual pretraining. GPU (192 GB) . This yields 253B-parameter baseline whose overall configuration is shown in Appendix C. Many attention layers were removed, resulting in 50 blocks contiguously arranged without interleaved attention layers. FFN Fusion. Next, we apply FFN Fusion (3) to 49 of the 50 consecutive FFN layers (See Section 5.3 for ablation on leaving the last layer). Due to per-GPU memory limits, we split the layers into four sequences [66, 73], [74, 85], [86, 100], [101, 114], each fused into single FFN. Before fusion, the baseline model achieved 84.23 on MMLU and 8.83 on MT-Bench. Remarkably, after fusing all 49 layerslayers that, if removed entirely, would damage performance severelythe model still maintains similar MMLU accuracy (82.76) and achieves 8.35 on MT-Bench before additional training. Additional Training. To recover performance, we used KD as described in (Bercovich et al., 2024). This involved multi-stage distillation process: 54B tokens at 8k context, followed by 5B tokens each at 16k and 32k, and finally 0.8B tokens at 128k. The KD process improved MMLU and MT-bench scores to 85.17 and 9.10, respectively. Further optimization was explored through two methods. First, inexpensive alignment via instruction-tuning and RLHF (Wang et al., 2024b). Table 1 compares Ultra-253B-Base before and after alignment. Table 3 demonstrated that Ultra-253BBase surpassed Llama-405Bs capabilities, particularly on the Arena Hard benchmark. Second, we applied only longer continual pretraining (CPT), without alignment, following the initial KD. This involved 73B tokens at context length of 8k and another 15B tokens at context length of 258k, and also yielded strong performance, even before instruction-based tuning (Figure 3). Table 1: Ultra-253B-Base performance before and after alignment. Model Alignment MMLU MMLU-Pro Arena Hard HumanEval MT-Bench Ultra-253B-Base Ultra-253B-Base 85.17 85.13 71.11 72.25 71.81 84.92 84.14 86.58 9.10 9.19 Table 2: User latency under Tensor Parallel (TP) 8 on single H100 node. higher tokens/second value indicates lower latency for single user. Efficiency Improvements. Ultra-253B-Base achieves 1.71 speedup in user latency  (Table 2)  , 35 lower per-token cost at batch size 32, and reduced memory footprint with half the attention layers and 253B parameters (down from 405B). Ultra-253B-Base breaks the efficient frontier on single H100 node, offering state-of-the-art accuracy and latency under the similar hardware constraints (see Figure 4). Table 2 details the user latency (tokens/second) achieved by Llama-405B, by Ultra-253B (with and without FFN Fusion), and by Llama3.3-70B, all under identical tensor parallel settings on single 8H100 node. Notably, Ultra-253B-Base is 1.71 faster than the parent for single-user decoding. On NVIDIA H200, its rate increases to 90.05 tokens/second. With speculative decoding (Leviathan et al., 2023)using Llama-3.2-1B-Instruct as draft model with no extra trainingUltra-253B-Base reaches 202 tokens/s on H200. This speculative decoding latency is averaged over all MT-Bench completions. Notably, 41.44 63.38 (1.53 Speedup) 70.92 (1.71 Speedup) 94.03 Llama-405B Ultra-PreFusion Ultra-253B-Base Llama-3.3-70B Tokens/Second Model as TP size increases, the efficiency of the fused FFN layers will increase further, and thus ready to benefit from improving hardware designs, such as GB200 NVL72 nodes. Overall, Ultra-253B-Base demonstrates that FFN Fusion coupled with the Puzzle algorithm can greatly reduce models depth at large scale. Figure 4: Accuracy vs. latency performance of Ultra-253B-Base. Latency is measured on single NVIDIA H100 node with tensor parallel (TP) 8, running in FP8. The red line represents the efficient frontier, highlighting models with the best accuracy-to-throughput tradeoff. Accuracy = (MT-Bench10+MMLU+MMLU-Pro+Arena Hard+HumanEval)/5. Table 3: Comparison of Ultra-253B-Base and its parent Llama-405B after applying FFN Fusion, KD, and alignment. Metric Llama-405B Ultra-253B-Base MMLU Instruct (Nvidia et al., 2024) MMLU-Pro (Wang et al., 2024a) Arena Hard (Li et al., 2024) HumanEval (Chen et al., 2021) MT-Bench (Zheng et al., 2023) 86.53 71.92 72.56 85.97 9.06 87.54 72.25 84.92 86.58 9."
        },
        {
            "title": "5 Additional Empirical Studies",
            "content": "In this section, we present additional ablation studies to further validate our approach and explore alternative strategies. In Section 5.1 we present results for FFN Fusion on 70B-scale model, confirming that the technique also generalizes to smaller architectures. Next, in Section 5.2, we examine the consequences of removing FFN layers rather than fusing them, demonstrating that these layers are vital to preserving model quality. In Section 5.3, we investigate the sensitivity of certain FFNs to fusion. Finally, in Section 5.4 we provide hypothesis as to why FFN Fusion works. 5.1 FFN Fusion in 70B Scale Model Description. We consider derivative of Llama-3.1-70B-Instruct created using the Puzzle algorithm (Bercovich et al., 2024), which reduces the model to 49B parameters while matching the original accuracy. The Puzzle process removes attention in specific layers, leaving two main sequences of consecutive FFN layers: layers 4251 (10 layers) and layers 5370 (18 layers). Results. We evaluate FFN Fusion at four progressively increasing levels of intensity, each reducing the number of FFN layers more than the previous: Step 1: Fuse adjacent pairs of FFNs within each sequence of consecutive FFNs. Step 2: Merge neighboring pairs from Step 1 to form longer fused blocks of length 4 or 5. 7 Step 3: Fuse the entire first sequence (layers 4250) into single FFN, and split the second sequence (5369) into two fused blocks. Step 4: Fuse the entire second sequence (5369) as well, reducing each main FFN run to single layer. Similarly to Ultra-253B-Base, we chose to exclude the last FFN in each sequence. We perform no additional training when applying these fusions. Table 4 reports each variants performance on MMLU and MT-Bench, as well as combined score Accuracy = (MMLU + 10 MT-Bench)/2. We observe that step-1 fusion reduces depth with only 1% overall accuracy drop, while step-4 fusion (collapsing each sequence to single layer) sees roughly 4% drop. Table 4: Evaluation of FFN Fusion. detailed description of each fusion step (step x) is provided in the main text. The second column from the left indicates how many FFN layers were replaced by the corresponding number of new layers. Model Baseline (49B Model) Step 1 Step 2 Step 3 Step 4 Step 3 + KD Fusion MMLU MT-Bench Accuracy - 80.73 26 12 80.64 80.29 26 6 80.39 26 3 79.98 26 2 26 80.56 8.87 8.72 8.54 8.30 8.25 9.00 84.71 83.92 82.84 81.69 81.24 85.28 5.2 Removing FFNs vs. FFN Fusion An intuitive alternative to fusing FFNs is to remove the same FFN layers outright. However, as shown below, large-scale removal generally leads to significant accuracy degradation, whereas fusion preserves representational capacity by retaining all parameters in single, parallel module. To determine which FFNs to drop, we rely on Puzzles blockimportance scores and remove the least important FFNs first. We restrict removal only to the FFN-fusion regions. For the 49B model, we observe clear advantage for fusion over removal. Figure 5 compares the accuracy-latency trade-off for two removal levels (15 or 20 FFNs removed) versus the two fusion steps. As we gradually remove more FFNs, we decrease latency but see larger accuracy drops. By contrast, fusing step 3 yields 10% latency improvement with only 3.5% accuracy drop. Removing 15 or 20 FFNs yields comparable or slightly higher latency gains (7.5% or 11%), but at steeper accuracy declines (5% or 8.7%). Moreover, brief knowledge-distillation phase (25B tokens) on the step-3 fused model actually surpasses the baseline (MMLU: 80.56, MT-Bench: 9.00). Figure 5: Accuracy vs. Latency for FFN Removal vs. Fusion. 5.3 The Final FFN in Each Sequence is Sensitive to Fusion In the 49B model, we observe that fusing the final FFN in long sequence often degrades accuracy more than fusing earlier FFNs. Below, we summarize an ablation study to highlight this phenomenon. Table 5 examines various ways of fusing the two main FFN sequences (layers 4251 and 5370) in the Puzzle-49B model. The first part of the table compares fusing the entire second sequence ([53, 69] vs. [54, 70]) and the entire first sequence ([42, 50] vs. [43, 51]) in isolation, indicating that layer 70 is especially sensitive. In the second part, we fuse both sequences simultaneously. 8 When either layer 51 or layer 70 is included in these large fused blocks, we see additional performance drops. Notably, skipping the final FFN (e.g. fusing [42, 50] , [53, 69]) yields higher MT-Bench scores than including layer 51 or 70. The final FFN in each attention-removed sequence appears uniquely important to the models representations. While most layers can be safely fused, incorporating the last FFN often triggers significant accuracy drop. Consequently, omitting this final FFN from the fused groups is typically more reliable choice for efficient fusion with minimal performance loss. Table 5: Evaluating the impact of fusing the final FFN in Puzzle-49B. Fusing these final layers often causes more accuracy loss. Fused FFN Sequence(s) MMLU MT-Bench Puzzle-49B (No Fusion) 80.73 8."
        },
        {
            "title": "Fusing a Single Sequence",
            "content": "[53, 69] [54, 70] [42, 50] [43, 51] 80.57 79.89 80.49 80."
        },
        {
            "title": "Fusing Both Sequences",
            "content": "[42, 50], [53, 69] [42, 51], [53, 69] [43, 51], [54, 70] [42, 51], [53, 70] 79.98 80.05 79.92 79.89 8.49 8.12 8.39 8.42 8.25 7.64 7.38 7.30 5.4 Fusion Explainability Sequence [42, 49] [53, 60] [63, 67] [63, 70] Fusion Accuracy 83.74 83.24 84.22 84.00 Reverse Accuracy 83.52 83.25 84.10 83.88 Table 6: Reverse order vs Fusion experiment on Puzzle-49B. In this section, we aim to further explain the phenomenon of FFN Fusion. We examine the functional structure of LLMs that allows for the fusing of layers, specifically focusing on the consecutive FFN layers after attention removal. First, we rewrite the token-level normalization equation in an alternative form: ηk(xi) = Dsk xi/xi, where we replace the normalization parameters sk Rd with diagonal matrix Dsk Rdd, with sk on the diagonal. This reformulation allows us to push the matrix into its corresponding moduleeither attention or FFNand consider equations 1 and 4 as functions of the token direction xi = xi/xi, while ignoring the magnitude. Figure 6 (a and b), Similar to the measurements in (Liu et al., 2023), shows the cosine distance between and (X) for each layer of both models. The plot reveals that the FFN fusing areas[42, 51] , [53, 70] for the 49B model and [66, 115] for the 253B modelexhibit lower cosine distance compared to other regions in the model. Assuming that small changes in the input to an FFN layer lead to small changes in its output, we can conclude that fusing the FFNs with low cosine distance (except for the last one) will not cause significant changes to the model. Specifically, by fusing the FFN layers, we are altering the input for each layer, but the directional difference between the original input (according to the original models structure) and the new input (resulting from fusion) remains small, and the outputs should remain similar under the smoothness assumption. Another experiment  (Table 6)  that strengthens this claim shows that even if we reverse the FFNs order instead of fusing them, we get similar performances. The results in Figure 6 (c and d) present the relationship between the layer input and h(X) = (X) X. The small change in token directions may be linked to the low cosine distance between h(X) and X, but they are nearly orthogonal (with distance of around 0.95 for both models across all layers). However, more suitable explanation comes from the ratio h(X) / X, which is smaller in the fused regions. This suggests that h(X) is small compared to that it does not significantly affect the direction of X. Although this section can also serves as motivation for FFN removal, we demonstrated in Figure 5 that removing these layers significantly harms the models performance, to much greater extent than fusion does. Both metrics were evaluated using the Distillation Mix dataset. 9 Figure 6: Per-layer metrics. Upper row is the cosine distance between (X) and for the (a) The 49B model and (b) Ultra-253B-Base model. Bottom row represents the ratio between h(X) and for the (c) The 49B model and (d) Ultra-253B-Base model."
        },
        {
            "title": "6 Block Parallelization",
            "content": "In all previous experiments, we focused on fusing sequences of FFNs in attention-removed layers (see Section 3). In that setting, the homogeneous structure of FFN-only layers permits straightforward weight concatenation and fusion. However, when considering full block parallelizationwhere entire Transformer blocks, each comprising both an attention module and an FFN, are run in parallelsuch fusion is not possible. This is because each full block consists of two distinct components, with the attention output explicitly directed to its paired FFN. Running entire blocks completely in parallel is not currently natively supported by heavily optimized inference frameworks such as TensorRTLLM or vLLM (Kwon et al., 2023). Nevertheless, one can envision assigning each full block to different GPU, thereby maximizing parallelism and potentially achieving significant speedups. We use our block-wise dependency analysis to examine the dependency structure between blocks and identify candidate groups whose outputs are relatively independent. Such groups are ideal for parallelization with minimal accuracy loss and could enable higher degrees of tensor parallelismeach full block operating on its own set of GPUsthereby improving inference throughput. In our experiments, we implement these full block parallelization strategies using more flexible environment (e.g., HuggingFace Transformers (Wolf et al., 2020)), albeit with reduced optimization for large-scale deployments. thorough investigation of the actual speedups in specialized frameworks is left for future work. 10 Method. We first compute the block-wise dependency matrix exactly as in Section 3, We then search for candidate sequences of 4 consecutive blocks to fuse. The choice of this number of blocks is mostly related to hardware constraints. Arbitrary sizes will be considered in our future work. For each such sequence [i, + 3], [m 4], we extract the corresponding [i,i+3] R44 submatrix from the block dependency matrix and compute two statistics: [i,i+3] max = max k,j[4] [i,i+3] kj , [i,i+3] sum = (cid:88) [i,i+3] kj k,j[4] lower Mmax, and Msum indicates weaker dependencies among those blocks, suggesting they may be more amenable to parallelization. We exclude any blocks that lack attention (shaded regions in Figure 7b) because our focus here is on full Transformer blocks that contain both attention and FFN modules. We employ simple greedy algorithm to choose the best subsequences: 1. Choose the length-4 sequence starting index = arg mini [i,i+3] . 2. If the argmin is not unique, break the tie using = arg mini [i,i+3] sum 3. Parallelize the blocks according to the chose sequence [i, + 3]. max 4. Remove any overlapping sequences. 5. Repeat until no valid sequences remain. (restricted to the set from step 1). The motivation behind this algorithm is to avoid choosing sequences with high dependency pairs. The sum tie breaking is due to step-like characteristic of Mmax statistic (see Figure 7b). Table 7 reports the downstream performance (MMLU and MT-Bench) after fusing the sequences chosen by 4 algorithm steps. Results. From Table 7, we observe that parallelizing the first and second sequence of four blocks leads to only modest drop in MMLU, but once the third sequence is added, the decline becomes much more pronounced. This suggests that full block parallelization is more challenging than FFN Fusion. These observations are supported by the block-wise dependency heatmap and the submatrix statistics (Figure 7), both of which indicate stronger interblock dependencies and higher Mmax and Msum values in full block sequences compared to attention removed sequences. In addition, Figure 6 shows that the full transformer blocks alter significantly the tokens directions. Table 7: Comparison of MMLU and MT-Bench scores across different parallelization strategies . Model Name Puzzle-49B MMLU MT-Bench 80. 8.87 Parallel Sequences [38, 41] 80.51 [38, 41] , [71, 74] 80.04 [38, 41] , [71, 74] , [32, 35] 77.86 [38, 41] , [71, 74] , [32, 35] , [26, 29] 56.12 8.67 8.67 8.20 6.62 (a) (b) Figure 7: (a) Block-wise Dependency Heatmap of the 49B model (log-scale). Darker blue hues indicate weaker dependencies, darker red hues indicate stronger dependencies. (b) Mmax and Msum values for 4-Block Sequences of the 49B model. Lower values indicating more promising candidates for parallelization."
        },
        {
            "title": "7 Concluding Remarks",
            "content": "This work introduces FFN Fusion, novel optimization technique that demonstrates how sequential computation in large language models can be effectively parallelized. Through comprehensive experiments at scales from 49B to 253B parameters, we showed that sequences of FFN layers can be fused with minimal impact on model capabilities. Our work revealed couple of key insights: (1) FFN layers in attention-removed regions exhibit remarkably low inter-layer dependencies, suggesting these computations may be more independent than the sequential architecture implies; and (2) empirically, final FFN layers in parallelizable sequences show higher sensitivity to fusionan observation that points to potentially meaningful structural patterns in how these models process information. The practical impact of our work is demonstrated through Ultra-253B-Base, which achieves 1.71 speedup in inference latency compared to its parent Llama-3.1-405B model, while requiring 35 lower per-token cost at batch size 32. Most remarkably, these efficiency gains come with minimal degradation in model capabilityin fact, Ultra253B-Base matches or exceeds its parents performance across key benchmarks despite using only 253B parameters compared to the original 405B. Our findings open several promising research directions. First, the clear patterns in inter-layer dependencies we observed could provide new lens for model interpretability research. The ability to identify which FFN layers operate independently versus those that require sequential processing may offer insights into how these models structure and transform their internal representations. Second, our preliminary finding that even full transformer blocks can sometimes be parallelized suggests possibilities for new architectural designs explicitly optimized for parallel execution. Third, future work could extend FFN Fusion to models incorporating Mixture-of-Experts (MoE) layers. Finding way to fuse MoE layers while maintaining sparse activation patterns and efficient expert routing could lead to efficiency improvements. Finally, the strong performance of FFN Fusion at larger scales raises intriguing questions about the relationship between model size and natural parallelization."
        },
        {
            "title": "References",
            "content": "AI4Science, M. R. and Quantum, M. A. The impact of large language models on scientific discovery: preliminary study using gpt-4, 2023. URL https://arxiv.org/abs/2311.07361. Anil, R., Borgeaud, S., Wu, Y., Alayrac, J., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https: //doi.org/10.48550/arXiv.2312.11805. Bercovich, A., Ronen, T., Abramovich, T., Ailon, N., Assaf, N., Dabbah, M., Galil, I., Geifman, A., Geifman, Y., Golan, I., Haber, N., Karpas, E., Koren, R., Levy, I., Molchanov, P., Mor, S., Moshe, Z., Nabwani, N., Puny, O., Rubin, R., Schen, I., Shahaf, I., Tropp, O., Argov, O. U., Zilberstein, R., and El-Yaniv, R. Puzzle: Distillation-based nas for inference-optimized llms, 2024. URL https://arxiv.org/abs/2411.19146. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., and 12 Zeng, W. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL https://doi.org/10.48550/arXiv.2412.19437. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pp. 77507774. PMLR, 2023. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Fan, A., Grave, E., and Joulin, A. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and Roberts, D. A. The unreasonable ineffectiveness of the deeper layers, 2024. URL https://arxiv.org/abs/2403.17887. Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519. Guan, Y., Wang, D., Chu, Z., Wang, S., Ni, F., Song, R., Li, L., Gu, J., and Zhuang, C. Intelligent virtual assistants with llm-based process automation, 2023. URL https://arxiv.org/abs/2312.06677. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Hassibi, B. and Stork, D. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992. He, S., Sun, G., Shen, Z., and Li, A. What matters in transformers? not all attention is needed, 2024. URL https://arxiv.org/abs/2406.15786. 13 Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.08415. Hive-Digital-Technologies. https://huggingface.co/datasets/H-D-T/Buzz-V1.2. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lad, V., Gurnee, W., and Tegmark, M. The remarkable robustness of llms: Stages of inference?, 2024. URL https://arxiv.org/abs/2406.19384. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Advances in neural information processing systems, 2, 1989. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1927419286. PMLR, 2023. URL https://proceedings.mlr.press/v202/leviathan23a. html. Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016. Li, T., Chiang, W., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. CoRR, abs/2406.11939, 2024. doi: 10.48550/ARXIV.2406.11939. URL https://doi.org/10.48550/arXiv.2406.11939. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen, B. Deja vu: Contextual sparsity for efficient llms at inference time, 2023. URL https://arxiv.org/abs/2310.17157. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. Nvidia, :, Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., Das, S., Dattagupta, A., Delalleau, O., Derczynski, L., Dong, Y., Egert, D., Evans, E., Ficek, A., Fridman, D., Ghosh, S., Ginsburg, B., Gitman, I., Grzegorzek, T., Hero, R., Huang, J., Jawa, V., Jennings, J., Jhunjhunwala, A., Kamalu, J., Khan, S., Kuchaiev, O., LeGresley, P., Li, H., Liu, J., Liu, Z., Long, E., Mahabaleshwarkar, A. S., Majumdar, S., Maki, J., Martinez, M., de Melo, M. R., Moshkov, I., Narayanan, D., Narenthiran, S., Navarro, J., Nguyen, P., Nitski, O., Noroozi, V., Nutheti, G., Parisien, C., Parmar, J., Patwary, M., Pawelec, K., Ping, W., Prabhumoye, S., Roy, R., Saar, T., Sabavat, V. R. N., Satheesh, S., Scowcroft, J. P., Sewall, J., Shamis, P., Shen, G., Shoeybi, M., Sizer, D., Smelyanskiy, M., Soares, F., Sreedhar, M. N., Su, D., Subramanian, S., Sun, S., Toshniwal, S., Wang, H., Wang, Z., You, J., Zeng, J., Zhang, J., Zhang, J., Zhang, V., Zhang, Y., and Zhu, C. Nemotron-4 340b technical report, 2024. URL https://arxiv.org/abs/2406.11704. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https: //doi.org/10.48550/arXiv.2303.08774. Penedo, G., Kydlıcek, H., Allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., von Werra, L., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data at scale. CoRR, abs/2406.17557, 2024. doi: 10.48550/ARXIV. 2406.17557. URL https://doi.org/10.48550/arXiv.2406.17557. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI, 2019. URL https://cdn.openai.com/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf. Accessed: 2024-11-15. Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. Shazeer, N. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=B1ckMDqlg. So, D. R., Manke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Primer: Searching for efficient transformers for language modeling, 2022. URL https://arxiv.org/abs/2109.08668. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K. R., Dumas, J., Elazar, Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, E. P., Zettlemoyer, L., Smith, N. A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Ku, L., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1572515788. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.840. URL https://doi.org/10.18653/v1/2024.acl-long.840. Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: large language model for science, 2022. URL https://arxiv.org/abs/2211.09085. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762. Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024a. doi: 10.48550/ARXIV.2406.01574. URL https://doi.org/10.48550/arXiv.2406.01574. Wang, Z., Bukharin, A., Delalleau, O., Egert, D., Shen, G., Zeng, J., Kuchaiev, O., and Dong, Y. Helpsteer2-preference: Complementing ratings with preferences. CoRR, abs/2410.01257, 2024b. doi: 10.48550/ARXIV.2410.01257. URL https://doi.org/10.48550/arXiv.2410.01257. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingfaces transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/abs/1910.03771. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. CoRR, abs/2406.08464, 2024. doi: 10.48550/ARXIV.2406.08464. URL https://doi.org/10.48550/arXiv.2406.08464. Zhang, M. and He, Y. Accelerating training of transformer-based language models with progressive layer dropping. Advances in neural information processing systems, 33:1401114023, 2020. Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 15 Proof of Theorem 3.1 Proof. The proof will proceed by induction. For the base case, we consider two FFN layers, FFN1 and FFN2. We will demonstrate that FFN = FFN1 + FFN2. The induction step follows naturally from the associative property of addition. (FFN1(X) + FFN2(X)) =(σ(X(W 1 = (cid:2)σ(X(W 1 =((cid:2)σ(X(W 1 =(σ(X(W =FFN(X) 2 )T ) X(W 1 2 )T ) X(W 1 2 )T ), σ(X(W 2 2 )T ) X(W 1 )T )(W 1 1 )T , σ(X(W 2 2 )T )(cid:3) (cid:2)X(W 1 3 )T 1 )T )(W 3 )T + (σ(X(W 2 2 )T ) X(W 2 1 )T (cid:3) (W 3 )T 1 )T (cid:3))(W 3 )T 2 )T ) X(W 2 1 )T , X(W 2 1 )T )(W 2 3 )T where [, ] is the concatenation of matrices along the second dimension. Figure 8: visualization of FFN Fusion applied to SwiGLU. Two FFNs (left) are fused into single FFN (right)."
        },
        {
            "title": "B Datasets",
            "content": "For applying Puzzle throughout our experiments, we used the same dataset mixture used in (Bercovich et al., 2024), termed Distillation Mix. This mixture includes source code repositories, Wikipedia articles, books, news websites, and several other domains. The dataset comprises 224 billion tokens collected from three public datasets: FineWeb (Penedo et al., 2024), Dolma (Soldaini et al., 2024), and Buzz-V1.2 (Hive-Digital-Technologies). For Ultra-253B-Base KD training we used the same data reinforced with synthetic data generated with Llama-3.1-405B-Instruct, following the (Xu et al., 2024) approach, to help further align Ultra-253B-Base with its parent model. 16 Ultra-253B-Base and Puzzle-49B Architecture Overview In this section, we detail the configuration of each block in the Ultra-253B-Base model and highlight several unique design choices introduced by the Puzzle framework on top of Llama-3.1-405B. Notably, the model employs variable FFN widths, with scaling multipliers ranging from 0.4875 up to 4.875 across its 126 blocks. This variability allows the model to dynamically adjust capacity at different depths, balancing performance with computational efficiency. Furthermore, it includes sequence of 50 consecutive layers that bypass the attention mechanism (denoted by no op), dedicating these layers entirely to FFN processing and creating an ideal region for FFN Fusion. We observe similar pruning pattern in the Puzzle-49B model, which is derived from the Llama-3.1-70B model using the Puzzle framework. Like its larger counterpart, Puzzle-49B also features variable FFN multipliers and blocks where attention is removed, although at smaller scale. Figure 9 compares both architectures. (a) Ultra-PreFusion Attention Layout (b) Ultra-PreFusion FFN Layout (c) Puzzle-49B Attention Layout (d) Puzzle-49B FFN Layout Figure 9: 2x2 overview of Ultra-253B-Base (top row) and Puzzle-49B (bottom row). Subfigures (a) and (b) illustrate the attention and FFN configurations, respectively, for the 253B model. Subfigures (c) and (d) show the corresponding layouts for the 49B model. Both architectures feature variable FFN widths and regions where attention has been removed, although at different scales."
        },
        {
            "title": "D MoE Inference Performance",
            "content": "MoEs show great promise in resucing inference costs and are currently spearheaded by DeepSeek-V3 (DeepSeek-AI et al., 2024), highly sparse MoE with great performance. However, MoEs have their own problems in inference. 17 D.1 Problems with Small Blocks An underlying problem for MoEs paradoxically arises from the small size of their sub-modules. Smaller layers incur large latency than expected for two reasons: GPU utilization - low level GPU overheads such as wave quantization produce latency problems that become stronger as the module becomes smaller. Communication overhead - The All-Reduce operation that is typical for tensor parallelism creates an overhead for communication. For small modules this overhead increases in proportion. MoEs experts are smaller than dense MLPs and hence suffer more strongly from the above effects. In addition the routing mechanism of MoEs is another small module that suffers accordingly. As result, dense models scale better with the number of GPUs while MoEs reach an early saturation. It is worth to notice that this is in complete contrast to our FFN Fusing method that creates larger modules that parallelize better, as visualized in Figure 10. Figure 10: FFN Fusion helps reduce latency by increasing GPU utilization and by reducing syncs D.2 Batch Size Effects MoEs behave very well for very large batch sizes. For such large workloads load balancing issues are less dominant and low level overheads become negligible. However, smaller batch sizes are not as efficient. The problem is that as more tokens are passed through the model, more experts are activated, thus increasing the latency. This is in sharp contrast to dense model which have similar latency from batch one to batch 64. As the batch size increases above 1, the number of experts activated first increases linearly, leading to the same throughput but with lower latency. When all experts are already activated, any further increase of batch size does not increase the latency. While this may appear beneficial at first glance, it is important to recognize that the MoE is still operating in suboptimal region while dense model is already in its efficient operation for the same batch size. Unfortunately for MoEs, this also means that speculative decoding is not efficient for batch size 1, as verifying the speculated tokens falls within the aforementioned inefficient regime associated with processing only few tokens. Effectively, this provides dense models with means to offset the MoEs initial advantage at batch size 1. We can thus conclude that MoEs only really live up to their promise for very large batch sizes. For more commonly used intermediate batch sizes they suffer from bad scaling with the number of tokens, larger low level overheads, and worse parallelization scaling than dense models."
        },
        {
            "title": "E Further Pairwise Block Dependency Analysis",
            "content": "Figure 11 offers more detailed view of the block-wise dependency structure in Ultra-PreFusion. Below, we highlight several notable observations: Overall Color Scale. Each entry (i, j) in the heatmap corresponds to the cosine distance between the outputs of block when block is dropped versus when the model is intact. dark red hue signifies strong dependency, meaning that omitting block substantially alters block j. Conversely, dark blue indicates weak dependency, suggesting that dropping block does not notably affect block j. Attention-Removed Region (Dotted Box). The dotted box highlights region of consistently low dependency values, where most blocks exhibit minimal mutual influence. This area arises from attention pruning in those layers and is particularly well-suited for FFN Fusion, since layers here can be more easily parallelized without significantly harming accuracy. 18 Figure 11: Replication of the block-wise dependency heatmap for Ultra-PreFusion, shown here for convenience. Each coordinate (i, j) represents the cosine distance between hj(X) and hj (X), quantifying how much block depends on block i. Darker blue indicates weaker dependencies, while darker red indicates stronger dependencies. The dotted box marks an attention-removed region with generally low dependency values, suggesting high parallelization potential. Crucial Early Blocks. Some blocks in the upper rows of the matrix (toward the top of the y-axis) display consistently red cells across many columns. This indicates that certain early layers have strong influence on wide range of subsequent blocks, making them less amenable to parallelization. Sub-region with Sequential Reliance. noticeable diagonal band of higher values appears in some subregions, implying that each block in that band strongly depends on its immediate predecessor. Such sequential reliance reduces the potential for parallelization in those areas, since omitting any single block significantly affects the next one. Global Sinks at Later Layers. As we move toward deeper layers (near the bottom-right corner), some blocks again show stronger dependencies, acting as global sinks that consolidate information from many earlier blocks. Although the importance of certain blocks may wane in mid-network regions, it can resurface in the final layers. In summary, the dependency matrix reveals both strongly and weakly coupled sub-regions. High-dependency zones require careful consideration to avoid accuracy loss if parallelization is attempted. Conversely, low-dependency zones, such as the attention-removed region, offer promising avenue for efficient FFN Fusion."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}