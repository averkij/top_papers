{
    "paper_title": "Draft and Refine with Visual Experts",
    "authors": [
        "Sungheon Jeong",
        "Ryozo Masukawa",
        "Jihong Park",
        "Sanggeon Yun",
        "Wenjun Huang",
        "Hanning Chen",
        "Mahdi Imani",
        "Mohsen Imani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems. Code is available at https://github.com/EavnJeong/Draft-and-Refine-with-Visual-Experts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 2 5 0 0 1 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Draft and Refine with Visual Experts",
            "content": "SUNGHEON JEONG1 RYOZO MASUKAWA1 JIHONG PARK3 SANGGEON YUN1 WENJUN HUANG1 HANNING CHEN1 MAHDI IMANI2 MOHSEN IMANI1 1University of California, Irvine, 2Northeastern University, 3MOLOCO sungheoj@uci.edu"
        },
        {
            "title": "Abstract",
            "content": "While recent Large VisionLanguage Models (LVLMs) exhibit impressive multimodal reasoning abilities, they often produce ungrounded, hallucinated responses by overrelying on linguistic priors rather than visual evidence. This critical limitation arises from the lack of quantitative measure of how much these models actually rely on visual inputs during reasoning. We propose Draft and Refine (DnR), an agent framework driven by novel question-conditioned utilization metric. This metric quantifies the models actual reliance on visual evidence by first constructing queryconditioned relevance map to localize question-specific evidence, and then assessing dependence through relevancebased probabilistic masking. Guided by this metric, the DnR agent refines its initial draft through targeted feedback from external visual experts. Each experts output (e.g., boxes, masks) is rendered as visual cues on the image, and the LVLM is re-queried to select the response that yields the greatest improvement in utilization. This process strengthens visual grounding of predictions without retraining or architectural changes. Experiments across broad range of VQA and captioning benchmarks demonstrate consistent accuracy gains and reduced hallucination. These results show that quantifying visual utilization provides principled path for designing more interpretable and evidence-driven multimodal agent systems that effectively leverage visual experts. Code is available at Github. 1. Introduction Large vision language models (LVLMs) have begun to interface with diverse visual tools and experts, enabling complex reasoning that combines perception and language [13, 27, 56, 64, 7072]. However, existing methods mainly rely on language-driven control, prompting models to call experts based on chain-of-thought or textual confidence [53, 56, 70, 71]. Such mechanisms inherit the biases and unreliability of the language model itself and seldom account for how effectively visual information is actually utilized [4, 10]. Learning-based coordination frameworks, on the other hand, require costly and inflexible joint optimization across multiple experts and tasks [7, 26, 31, 32]. This raises fundamental question: can visionlanguage model autonomously determine when and which visual expert to invokeguided by its own perceptual needs rather than linguistic biases? To address this challenge, we reconsider expert coordination from the perspective of visual utilization. Our initial intuition is simple: LVLMs should be guided to more effectively utilize visual information when forming its predictions [28, 75]. However, not all visual cues are equally useful, because different tasks and questions require attention to different parts of the image [22, 40]. Therefore, the key is not to maximize visual dependence globally but to evaluate how well the model utilizes the regions relevant to the given situation [5, 50]. Once the model identifies which visual information is most critical, the selected expert can provide additional evidence to refine the models reasoning and improve its decision. Building on this idea, we develop Draft and Refine (DnR) framework that measures visual utilization and uses it as quantitative criterion for selecting and incorporating visual experts. This formulation enables adaptive expert choice grounded in the models actual perceptual behavior rather than relying on linguistic priors or heavy joint supervision. Existing LVLMs can describe what they see but cannot determine which visual elements are truly important or why they matter for given task [55, 66]. Because there is no explicit label or metric to assess such dependence, we first define measurable criterion that quantifies how much the model relies on visual information through relevance-based perturbation process. This enables us to evaluate visual utilization without additional supervision. After computing this utilization, we incorporate external 1 visual experts to complement perceptual gaps by providing additional evidence. To align their outputs with the models perception, we introduce visual rendering mechanism that highlights essential regions while suppressing irrelevant content. This controllable rendering mechanism enables flexible integration of diverse visual experts into general-purpose multimodal reasoning systems without retraining. Through this formulation, our approach offers practical framework to quantify and leverage visual information within multimodal reasoning. It allows models to evaluate how effectively they use visual evidence and refine reasoning based on perceptual relevance rather than linguistic confidence. The rendering mechanism serves as flexible interface for integrating diverse visual experts without retraining, enabling agent-style coordination that generalizes across domains. Although rendering styles and parameters require datasetand model-specific tuning, the framework consistently improves performance across VQA and captioning benchmarks, showing strong correlation with task accuracy and substantial reduction in hallucination. Collectively, these findings suggest that measurable visual grounding offers principled basis for developing interpretable and evidence-driven AI agent systems that leverage visual tools. 2. Related Work Multimodal Large Language Models. Recent advances in multimodal large language models (MLLMs) have significantly expanded the capacity of language models to reason over visual inputs [3, 8, 29, 35, 38, 45, 47, 51, 73]. By coupling pretrained vision encoders with powerful language backbones, these systems achieve strong zero-shot performance on captioning, visual question answering, and general visual reasoning tasks [35, 61]. However, their reasoning process remains predominantly language-driven, relying heavily on internal linguistic priors rather than on visual grounding [4, 10, 61]. When the visual representation is coarse or misaligned, such models tend to hallucinate plausible yet unsupported content [4, 10, 61]. This imbalance between linguistic reasoning and perceptual understanding has been repeatedly observed across recent evaluations, which report limited grounding fidelity and reduced robustness on fine-grained or localized visual reasoning benchmarks [4, 35, 61]. These findings underscore the need for frameworks that can explicitly assess, verify, and refine visual evidence throughout the reasoning process [4, 10, 35]. Tool-Augmented and Agentic LLMs. Recent research has transformed large language models into agentic systems that autonomously plan, execute, and integrate external tools or APIs for reasoning and perception [36, 53, 56, 70, 71]. These approaches span programmatic reasoning agents that compose visual functions via code execution [13, 64, 65], tool-calling coordinators that orchestrate multiple pretrained experts in modular pipeline [27, 34, 56, 70, 72], and multimodal planners that integrate vision encoders for grounded decision-making [3, 8, 29, 38, 45, 47, 51, 62]. Despite their success, these systems generally rely on predefined or heuristic tool invocation, often selecting experts based on the LLMs textual outputs or embedding similarities rather than measurable visual evidence [7, 26, 27, 30 32, 34, 53, 56, 70, 72], and thus lack quantitative criteria for determining when visual assistance is necessary or which expert should be called. In contrast, our framework formulates expert invocation as measurable decision process, guided by the models actual utilization of visual evidence, thereby enabling an adaptive and systematic bridge between perception and reasoning. Hallucination Mitigation and Visual Grounding. Addressing hallucination and weak grounding has become central challenge in both language and visionlanguage modeling [4, 10, 35]. Existing approaches attempt to improve factual or visual alignment through faithfulnessoriented supervision [16, 33], retrieval-augmented reasoning [6, 68], or uncertainty-driven selection and abstention strategies [15, 20, 21, 59]. While these methods reduce hallucination, they generally operate as post-hoc corrections or apply global feature amplification without explicitly conditioning on the query, often leading to over-attention on irrelevant regions and diluted reasoning fidelity [1, 74]. In contrast, our framework introduces query-conditioned relevance map that directly links visual importance to question semantics, enabling selective and evidence-grounded refinement rather than uniform enhancement. 3. Draft and Refine with Visual Experts The Draft-and-Refine (DnR) framework improves LVLM by refining its draft response using specialized visual experts. Given an image and question q, the LVLM first drafts an initial response based on its existing visual understanding. DnR then analyzes the to identify regions most relevant to q, evaluates how the models reasoning depends on them, and selectively engages the expert that provides complementary visual evidence. By integrating this evidence, DnR turns the LVLMs passive description into an active, evidence-anchored reasoning process that steers its answers toward visually grounded decisions rather than incidental linguistic context. 3.1. Query-Conditioned Relevance Map To assess how effectively the LVLM grounds its reasoning in visual evidence, we focus on identifying the regions of image that truly matter for given question q. Rather than assuming that all parts of the image contribute equally, we recognize that only certain regions serve as meaningful evidence. Therefore, we introduce query-conditioned 2 Figure 1. Overview of the Draft-and-Refine (DnR) framework. Given an image and question q, the LVLM first generates an initial draft answer ˆy 1. The question is decomposed by fLLM into query set = {qi}, and each query is grounded by fg to produce spatial relevance maps, aggregated into r(x q) 2. Gumbel-k sampling masks Top-k and Bottom-k regions for perturbation, and semantic encoder g() measures similarity shifts between ˆy and perturbed predictions yτ to compute the utilization score base 3. Expert models (e.g., CLIP, SAM, OCR) render structured visual evidence onto the image, producing refined outputs with updated utilization (j) . The expert with the largest gain (j) is selected for refinement 4. base relevance map r(x q), which localizes and highlights the areas of the most informative for answering q. Extracting query terms. We first transform the freeform question into set of explicit visual queries = {q1, q2, . . . , qm}. Directly using is often suboptimal, as natural questions may include abstract or relational expressions (e.g., Is the person hungry?) that are difficult for vision encoders to interpret. To bridge this gap, large language model fLLM reformulates into visually grounded queries = fLLM(q), where each qi denotes concrete, visually identifiable concept such as an object or attribute. For example, for the question What is the man wearing on his feet?, the fLLM generates queries like shoes, feet, and clothing. These queries serve as explicit textual anchors that guide visionlanguage alignment, as illustrated in Fig. 2. Computing query-conditioned regions. Given the extracted queries Q, each qi is used to localize its corresponding visual evidence within x. We employ CLIPbased visual grounding model fg, where each text query qi guides the decoder to produce spatial relevance map R(x qi) = fg(x, qi) [0, 1]HW , representing pixelwise relevance to qi. This retrieval-like grounding process aligns textual concepts with their spatial counterparts. r(x q) = 1 (cid:88) qiQ R(x qi), = fLLM(q). (1) Averaging across all query terms ensures consistent activation of semantically related regions while suppressing spurious responses as defined in Eq. (1). The resulting map r(x q) provides the spatial foundation for evaluating how effectively the LVLM leverages relevant evidence in the subsequent stages of DnR. 3.2. Question-Conditioned Utilization After obtaining r(x q), the next objective is to assess how effectively the model utilizes the critical visual regions indicated by r(x q). The question-conditioned utilization Uq(x) quantifies how the models prediction varies when these regions, either highly relevant (Top-k) or irrelevant (Bottom-k), are perturbed. higher Uq(x) indicates that the model responds sensitively to question-critical evidence while remaining stable against distractive regions, indicating more faithful and evidence-grounded reasoning. 3.2.1. Relevance-based probabilistic masking To quantitatively evaluate how much the models prediction depends on the regions identified as relevant, we construct probability distribution over the spatial regions of the image proportional to their relevance scores. Let U(x) denote the set of candidate regions within x. The probability of each region U(x) is computed by normalizing its relevance value obtained from the relevance map. (u x, q) = r(u x, q) uU (x) r(u x, q) (cid:80) . (2) Figure 2. Illustration of the query-conditioned relevance map. For the same image (top row), different questions lead to distinct relevance regions aligned with the extracted query terms. Conversely, for the same question (bottom row), the relevance map varies with the image content, localizing evidence that matches the queried concept. This normalized distribution, Eq. (2), provides probabilistic weighting over image regions according to their question-conditioned importance. Based on this distribution, two complementary masking strategies are employed, as illustrated in Fig. 3. Top-k masking occludes highly relevant regions (high (u x, q)) to measure degradation in model prediction, while Bottom-k masking occludes less relevant regions (low (u x, q)) to assess prediction stability. Sampling from these two strategies yields set of stochastic binary masks, denoted as Mq = τ1, . . . , τM , each covering approximately ρ fraction of the image area. To ensure stochastic region selection, we adopt Gumbel-k sampling [23, 60], which enables diverse Top-k/Bottom-k mask generation from the relevance distribution. Adaptive combination of Top-k and Bottom-k masking. While Top-k and Bottom-k perturbations reveal complementary aspects of model behavior, their relative influence should depend on the sharpness and distribution of the relevance map r(x q). We therefore propose an adaptive weighting factor α [0, 1] per sample by analyzing the information structure of r(x q). Specifically, α is obtained through (cid:0)r(x q)(cid:1) that integrates two cues function α = Φadapt (entropy and contrast) reflecting the focus and separability of the relevance distribution. Eq. (3) defines this adaptive function as weighted combination of the normalized entropy Hnorm and contrast C: Figure 3. Question-conditioned utilization computation. Given question and image x, the relevance map r(x q) guides Gumbel Top-k/Bottom-k masking over ratio ρ of the image. Masked inputs τ (x) are fed into the LVLM to obtain perturbed predictions yτ , compared with the original ˆy via semantic encoder g(), and aggregated with the adaptive factor α to compute the final utilization score Uq(x). higher α corresponds to sharper and more distinct relevance map, assigning greater weight to Top-k masking, whereas lower α emphasizes Bottom-k masking to assess stability under diffuse relevance. This adaptive mechanism allows Uq(x) to dynamically decide which perturbation to emphasize based on the input, focusing on decisive evidence when the relevance map is confident and prioritizing stability when it is uncertain. 3.2.2. Semantic deviation measurement. For each mask τ Mq, masked image τ (x) is generated by removing the selected regions. Let ˆy = fVLM(x, q) denote the original prediction and yτ = fVLM(τ (x), q) the masked prediction. Both are projected into an embedding space by semantic encoder g(), yielding zˆy = g(ˆy) and zyτ = g(yτ ). The semantic deviation for each mask is defined differently depending on its type (top-k or bottom-k). (cid:40)1 cos(cid:0)zˆy, zyτ cos(cid:0)zˆy, zyτ dτ (ˆy, yτ ) = if τ Mtop , if τ Mbottom (cid:1), (cid:1), (4) . α = βentHnorm + βctrC βent + βctr . (3) As shown in Eq. (4), Top-k masking measures the drop in semantic similarity after masking highly relevant regions, 4 LVLM cannot directly process. To make these outputs compatible, they are rendered onto the original image to produce ˆx(j) = R(x, hj), forming rendered visual evidence. This rendering converts the experts structural predictions into visual cues (e.g., gray, blur, or highlight regions) that the model can process through its existing vision encoder without any architectural modification  (Fig. 4)  . y(j) = fVLM(ˆx(j), q) = fVLM(R(x, hj), q) (6) Each y(j) represents the models refined response obtained by re-querying fVLM with the rendered input ˆx(j) = R(x, hj), where the rendering R() visually encodes the experts structural output onto the original image to serve as explicit visual evidence. Subsequently, the framework computes the queryconditioned utilization (j) for each refined response y(j) and compares it to the baseline utilization base obtained from the initial draft ˆy  (Fig. 1)  . For each expert-rendered input ˆx(j) and its prediction y(j), the utilization score (j) is re-computed by applying the original masks τ Mq to ˆx(j) and using y(j) as the new baseline for Eq. (4). The expert that yields the largest improvement is then selected: (cid:0)U (j) = arg max base (7) (cid:1) +, If no expert increases Uq(x) beyond base , the system concludes that the draft already captures sufficient visual grounding and skips further refinement, where ()+ ensures that only positive improvements over the baseline are considered. 3.4. Learned Expert Selection Running all experts for Eq. (7) is computationally expensive. To make DnR practical, lightweight selector network Sθ(j s) is trained to predict the optimal expert as defined in Eq. (7) directly from the state s. The state represents the query-conditioned context observed before expert invocation: = (x, Q, ˆy, r(x q)), (8) where is the image, the query set, ˆy the draft prediction, and r(x q) the relevance map. The selector Sθ is trained with the loss = EsD [log Sθ(j s)] . (9) This setup enables scalable expert coordination as the number of experts increases, where direct evaluation becomes linearly expensive since each candidate requires separate rendering and utilization computation. Leveraging the utilization difference (U (j) base ) defined in DnR, the selector learns an approximate rule that converts the refinement process from exhaustive search to direct decision. Figure 4. Comparison of rendering strategies across different experts. Each column corresponds to an experts, and each row represents rendering style. whereas Bottom-k masking quantifies the stability of predictions when irrelevant regions are masked. 3.2.3. Utilization score. Building upon the semantic deviations defined in Eq. (4), the final utilization score Uq(x) aggregates the semantic deviations from both Top-k and Bottom-k perturbations through the adaptive factor α. It quantifies how the models prediction changes when query-relevant or irrelevant regions are masked, providing balanced measure of evidence dependence and robustness. Uq(x) = α τ Mtop [dτ (ˆy, yτ )] + (1 α) Eτ Mbottom [dτ (ˆy, yτ )]. (5) τ Mtop In Eq. (5), the expectation operator [] denotes the mean semantic deviation over all masks in each subset. Owing to the complementary definitions in Eq. (4), higher Uq(x) consistently indicates more faithful and visually grounded reasoning process. Specifically, the Top-k term captures how strongly the model relies on question-critical regions, where larger deviations imply stronger evidence dependence, whereas the Bottom-k term measures how stable the prediction remains when irrelevant regions are perturbed. Their contributions are adaptively balanced by the factor α, yielding unified measure of both dependence and robustness. 3.3. Expert Selection and Incorporation DnR leverages Uq as quantitative measure to guide the selection of visual experts. Given multiple candidates {h1, h2, . . . , hK}, each expert generates structured outputs such as bounding boxes, masks, or depth maps, which the 5 VLM Backbone IDEFICS [24] (Draft / DnR) Revision Rate Correction / Degradation Pearson/Spearman InstructBLIP [8] Revision Rate Correction / Degradation Pearson/Spearman MiniGPTv2 [77] Revision Rate Correction / Degradation Pearson/Spearman LLaVA 1.6 [38] Revision Rate Correction / Degradation Pearson/Spearman PaliGemma [9] Revision Rate Correction / Degradation Pearson/Spearman CogVLM [17] Revision Rate Correction / Degradation Pearson / Spearman Qwen2.5-VL [49] Revision Rate Correction / Degradation Pearson/Spearman VQAv2 [12] GQA [18] VizWiz [14] TextVQA [57] OCR-VQA [44] COCO [37] NoCaps [2] Flickr [48] VCR [76] VSR [67] OK-VQA [43] A-OKVQA [54] ScienceQA [41] MME [11] MMBench [39] SEED-Bench [25] VQA Image Captioning Visual Reasoning Knowledge VQA Comprehensive Benchmarks 37.8 / 47.85 29.8 46.2 / 14.3 0.143 / 0.064 76.4 / 77.75 7.2 9.1 / 5.4 0.243 / -0.024 32.6 / 34.1 17.5 12.9 / 2.9 0.026 / 0.051 80.9 / 82.81 5.1 66.7 / 9.2 0.509 / 0. 73.2 / 75.2 9.2 36.4 / 27.3 0.292 / 0.291 82.05 / 82.85 5.1 66.7 / 2.3 0.684 / 0.735 83.95 / 85.45 10.6 40 / 4.3 0.389 / 0.509 24.1 / 25.5 1.5 51.3 / 1.1 0.449 / 0.364 38.24 / 39.77 13 35.3 / 1.2 0.290 / 0.286 25.3 / 27.6 3.1 8.3 / 0.5 0.194 / 0. 61.5 / 64.2 4.5 33.3 / 3.1 0.288 / 0.328 58.3 / 59.9 1.5 16.7 / 0.8 0.276 / 0.399 56.13 / 57.74 2.5 32.2 / 1.3 0.223 / 0.357 57.02 / 58.31 8.8 16.7 / 8.3 0.136 / 0.222 43.33 / 36.67 19.8 6.3 / 24.1 0.248 / 0.259 37.83 / 38.67 24.2 16.1 / 2 0.290 / 0. 59.17 / 60.67 3.8 41.2 / 6.7 0.304 / 0.234 76.83 / 76.99 1.8 14.3 / 2.1 0.156 / 0.154 79.17 / 80.17 2.8 82.1 / 2.3 0.307 / 0.251 48.5 / 50.33 4.8 5.7 / 0.4 0.220 / 0.136 73.01 / 73.83 1.2 41.3 / 1.0 0.19 / 0.224 30.15 / 30.32 2 2.3 / 0.3 0.026 / 0. 52.43 / 54.1 2.1 52.3 / 25.25 0.066 / 0.076 36.67 / 36.68 15.5 1.6 / 0.1 0.492 / 0.129 64.49 / 64.59 1 25 / 0.7 0.155 / 0.231 65.83 / 66.5 1.2 42.3 / 21.3 0.136 / 0.03 68.51 / 69.68 2.30 42.9 / 28.6 0.318 / 0.11 83.92 / 84.25 0.20 33.3 / 5.7 0.230 / 0. 47.24 / 47.74 3.2 12.5 / 2.1 0.129 / 0.222 80.4 / 81.91 1.2 24.5 / 3.4 0.426 / 0.415 56.78 / 58.79 1.8 11.1 / 0.1 0.065 / 0.124 73.37 / 74.87 3.9 66.7 / 16.7 0.626 / 0.849 68.34 / 70.35 3.5 28.6 / 0.5 0.321 / 0.329 82.91 / 82.91 0.20 1.3 / 0.1 0.263 / 0. 72.36 / 74.86 2.5 89.3 / 3.4 0.499 / 0.425 135.66 / 137.7 17.8 - - 114.31 / 118.7 26.2 - - 106.53 / 108.5 17 - - 107.8 / 109.1 31.1 - - 74.2 / 80.4 18.4 - - 72.7 / 79.8 36 - - - - - - 126.54 / 138.6 22.8 - - 137.99 / 144.7 22.6 - - 86.57 / 94.8 13.4 - - 61.96 / 64.7 27.6 - - - - - - 70.18 / 75.5 37.1 - - 83.81 / 85.99 38.2 - - 80.3 / 81.7 11.1 - - 71.8 / 74.2 37.5 - - - - - - 77.9 / 79.6 23 - - 92.9 / 101.9 53.2 - - 67.7 / 70.5 30.6 - - 57.3 / 55.9 42.1 - - 15.58 / 21.11 18.3 56.3 / 7.1 0.38 / 0.421 13.07 / 12.56 4.1 13.2 / 21.3 0.2 / 0. 13.07 / 15.58 7.2 49.9 / 3.4 0.292 / 0.224 18.59 / 18.69 0.5 25.3 / 11.4 0.772 / 0.778 13.57 / 15.53 2.2 43.4 / 5.4 0.253 / 0.265 12.56 / 13.29 2.5 56.3 / 19.4 0.110 / 0.117 30.65 / 31.66 5.5 45.5 / 18.2 0.637 / 0.631 52.76 / 54.27 2.5 52.9 / 32.3 0.158 / 0. 61.31 / 61.81 1.2 59.9 / 33.1 0.444 / 0.361 40.2 / 43.72 0.8 65.1 / 20.8 0.155 / 0.165 64.82 / 65.83 0.2 95.3 / 0.3 0.272 / 0.272 66.83 / 68.84 0.9 75.5 / 11.1 0.341 / 0.275 62.81 / 65.33 0.5 99.7 / 0.1 0.458 / 0.456 79.9 / 80.4 0.2 93.9 / 0.3 0.25 / 0. 43.5 / 44.4 2.9 33.3 / 1.4 0.351 / 0.210 49.2 / 50.2 4.5 39.5 / 18.4 0.168 / 0.09 18.1 / 19.8 4.5 11.1 / 0.1 0.206 / 0.168 55.1 / 56.1 2 37.5 / 24.9 0.297 / 0.161 57.2 / 58.5 3.2 60.15 / 23.1 0.165 / 0.128 58.6 / 58.6 0.8 33.1 / 4.6 0.500 / 0. 58.2 / 58.3 3.8 13.3 / 6.7 0.02 / 0.023 69 / 69.5 0.2 8.6 / 1.9 0.1 / 0.166 79.11 / 81.09 1.8 42.9 / 28.6 0.136 / 0.104 38.58 / 41.01 6.8 14.8 / 7.2 0.108 / 0.122 73.5 / 73.5 0.8 33.3 / 7.8 0.110 / 0.128 85.2 / 85.5 1 12.3 / 0.3 0.441 / 0. 84.5 / 84.5 0.7 3.4 / 2.1 0.163 / 0.14 74.4 / 76.4 1.5 33.2 / 16.8 0.43 / 0.467 43.95 / 44.02 0.2 94.3 / 0.4 0.351 / 0.277 50.5 / 52.5 1.2 77.6 / 4.3 0.608 / 0.421 28.77 / 29.41 0.2 94.3 / 0.3 0.719 / 0.807 72.9 / 73.8 1.4 49.8 / 0.4 0.454 / 0. 89.1 / 89.7 0.5 92.1 / 1.3 0.803 / 0.853 61.5 / 62.22 1 75.43 / 21.33 0.499 / 0.412 86.5 / 87.1 0.4 82.4 / 5.3 0.644 / 0.703 1392.11 / 1431.58 0.6 80.3 / 16.6 0.12 / 0.148 1294.74 / 1295.31 0.1 92.1 / 4.3 0.152 / 0.128 878.95 / 910.53 0.2 50.32 / 12.1 0.347 / 0. 1694.74 / 1721.05 0.4 98.3 / 0.6 0.270 / 0.307 1434.21 / 1444.74 0.2 51.1 / 12.5 0.08 / 0.07 1384.21 / 1423.68 0.6 66.6 / 31.1 0.420 / 0.470 2268.42 / 2276.32 0.3 65.6 / 32.1 0.5 / 0.5 50.01 / 50.26 0.4 43.8 / 12.2 0.12 / 0.06 51.84 / 52.89 1.6 55.3 / 4.4 0.01 / 0. 37.63 / 39.21 0.4 66.7 / 1.2 0.422 / 0.360 76.32 / 77.89 0.5 23.3 / 7.3 0.109 / 0.116 70.09 / 71.05 0.5 23.5 / 2.1 0.09 / 0.1 76.84 / 77.89 0.3 70.1 / 2.9 0.01 / 0.03 86.05 / 86.84 1.2 55.5 / 42.3 0.188 / 0.083 32.16 / 32.75 0.3 33.3 / 19.6 0.354 / 0. 51.46 / 53.8 3.2 36.4 / 9.1 0.062 / 0.145 29.82 / 31.58 7.6 30.8 / 15.7 0.155 / 0.131 66.08 / 66.67 0.3 95.5 / 0.4 0.112 / 0.01 57.89 / 59.65 4.1 64.3 / 31.7 0.129 / 0.191 58.48 / 59.06 0.3 98.1 / 0.3 0.153 / 0.227 80.12 / 81.29 0.5 29.4 / 17.6 0.170 / 0. Table 1. Comprehensive evaluation of diverse LVLMs on 16 multimodal benchmarks. Each cell reports Draft / DnR performance. Additional rows show revision rates (percentage of refined), correction vs. degradation (FalseTrue / TrueFalse) transitions, and correlation coefficients (Pearson / Spearman) measuring alignment between confidence or utilization scores and accuracy changes. Cells in green denote improvements after refinement, and those in blue indicate strong correlations (r > 0.2). 4. Experiments We evaluate DnR to validate the proposed Uq and its role in multimodal reasoning. The framework performs singlestep selection by choosing the candidate that maximizes Uq, and experiments examine how this choice influences accuracy, reduces hallucination, and promotes evidencegrounded reasoning. We use LLaMA-3-70B [58] as the language backbone fLLM for query decomposition, CLIP-L/14 [50] and SentenceTransformer all-MiniLM-L6-v2 [52] as the semantic encoder g(), and CLIPSeg [42] as the visual grounding model fg. Four visual experts (GroundingDINO [40], SAM [22], DepthAnything [69], and mDETR [19]) are chosen from distinct backbone families to provide complementary cues. For the utilization computation, we sample = 16 stochastic masks per image using Gumbel-k sampling, with the masking ratio ρ set to 0.25 for Top-k, 0.75 for Bottom-k, and adaptively adjusted in Hybrid mode. 4.1. Comprehensive Evaluation We evaluate DnR across five categories: VQA, image captioning, visual reasoning, knowledge VQA, and comprehensive benchmarks, each examining distinct aspect of multimodal reasoning with subsets. All experiments use the GRAY rendering strategy and Hybrid masking mode for consistency. The utilization metric Uq acts as unified indicator connecting visual evidence to model behavior, and its validity is tested through Pearson and Spearman correlations between utilization scores and accuracy gains. 4.1.1. VQA DnR improves visual reasoning across diverse LVLMs, but its effect varies with each models baseline groundsuch as ing. Models with weaker visual reliance, IDEFICS [24] and MiniGPT-v2 [77], show higher revision rates (8.311.3%) and larger gains (+1.43.0%), indicating that DnR supplies missing visual feedback. Stronger systems like LLaVA 1.6 [38], CogVLM [17], and Qwen2.5VL [49] revise less often (3.04.6%) yet still improve (+1.11.5%), suggesting stabilizing effect that refines already coherent reasoning. This contrast shows that DnR enhances consistency while moderating excessive or insufficient visual dependence during inference. DnR scales with each tasks visual demand. Perceptionfocused benchmarks such as VQAv2 [12] and VizWiz [14] show larger gains (1.82.7%), while text-centric tasks like OCR-VQA [44] and TextVQA [57] show smaller but consistent improvements (1.41.6%). An exception appears in IDEFICSVizWiz, the drop occurs because DnR pushes the model to answer cases it previously avoided by saying unanswerable, and its limited capability on these queries naturally leads to more mistakes. high correction-todegradation ratio (31.86.5%) and positive Pearson and Spearman correlations (0.275/0.273) indicate that DnR systematically shifts predictions toward more evidencegrounded reasoning rather than random variation. 4.1.2. Image Captioning In image captioning, where no explicit is provided, DnR constructs object-centric to explore the scene and guide visual reasoning. The strength of improvement depends on each models initial captioning bias. Models producing linguistically generic descriptions, such as IDEFICS and InstructBLIP, show moderate CIDEr gains (+2.04.4) at low medium revision rates (1736%), suggesting that DnR primarily supplements missing scene-specific details. In contrast, PaliGemma and LLaVA 1.6 yield larger gains (+6.6 12.1), especially on COCO [37] and Flickr [48], where 6 object-level grounding is emphasized. Overall, DnR promotes more discriminative and visually grounded phrasing even when baseline captions are already coherent. COCO and Flickr show high CIDEr gains supported indicating that by high revision frequencies (2253%), DnR shifts captions away from generic phrasing toward grounded, object-centric descriptions. NoCaps [2], with weaker visual constraints, yields smaller but steady gains (+1.75.3). minor drop appears for Qwen2.5-VL on Flickr (57.355.9) because its global pooling, amplified by DnRs focus on high-confidence cues, suppresses finegrained details. Overall, the positive correlation between revision rate and CIDEr gain shows that DnR steers captions toward visual fidelity rather than altering them arbitrarily. 4.1.3. Visual Reasoning Across reasoning benchmarks, DnR strengthens both semantic and spatial reasoning by aligning model decisions with visual evidence. On VCR [76], visually weaker models such as IDEFICS and MiniGPT-v2 revise more often (718%) and gain larger improvements (+2.05.5), showing that DnR supplies missing visual cues. More visually coherent models (LLaVA 1.6, CogVLM, Qwen2.5VL) revise infrequently (<6%) yet still improve (+0.11.0), with high Pearson correlations (0.630.77) indicating stable alignment between utilization shifts and accuracy. InstructBLIP shows slight drop (-0.5%) when grounding fails to influence its reasoning stage. Overall, DnR corrects visually under-grounded models and refines visually strong ones without destabilizing performance. Meanwhile, in VSR [67], where spatial relationships dominate, revisions occur rarely (<1%) but consistently yield measurable gains (+0.52.5%), showing that DnR corrects spatial misinterpretations while maintaining overall stability. CogVLM and LLaVA 1.6 achieve near-perfect correction ratios (99.7% and 95.3%) and moderate correlations (0.270.46), demonstrating that DnR fine-tunes spatial grounding with minimal perturbation. Overall, DnR acts as dual-function mechanism reinforcing under-grounded reasoning and regularizing over-grounded logic, thereby improving both semantic and spatial fidelity across models. 4.1.4. Knowledge-based VQA Across knowledge-based benchmarks, DnR reinforces perceptual grounding but cannot substitute for missing conceptual knowledge. This limitation is evident from the low revision rates (6.8%) and small accuracy gains (+0.52.0%), indicating that additional visual cues cannot recover facts the model does not know. Weak grounded models, such as IDEFICS and MiniGPT-v2, achieve higher correction ratios (1133%) despite minimal revisions, showing that DnR helps retrieve existing knowledge previously inaccessible due to poor visual grounding. In contrast, stronger models like PaliGemma, LLaVA 1.6, and Qwen2.5-VL maintain steady improvements (+0.61.3%) with high utilizationaccuracy correlation (Pearson/Spearman0.70.85 in ScienceQA [41]), confirming that their factual reasoning is already visually aligned. A-OKVQA [54], however, shows near-zero correlation (0.2) and minimal accuracy change, demonstrating that when conceptual knowledge is missing, DnR offers no benefit. In summary, the low revision activity, stable gains, and strong correlations together confirm that DnR improves access to existing knowledge through perceptual reinforcement, not knowledge augmentation. 4.1.5. Comprehensive Benchmark Comprehensive benchmarks evaluate overall multimodal consistency rather than task-specific accuracy. Because these datasets contain stable and unambiguous inputs, DnR yields few revisions (2%) and small but consistent gains, indicating shift from error correction to stability refinement. Moderate utilizationaccuracy correlations (0.30.5) confirm that such adjustments are selective rather than random. Across models, IDEFICS and MiniGPT-v2 gain mainly in perception-oriented metrics (+3139 in MME [11]) as DnR compensates weak grounding, whereas stronger systems such as LLaVA 1.6, CogVLM, and Qwen2.5-VL show minimal revision but sustained coherence (>95% correction). Overall, once multimodal grounding stabilizes, DnR works primarily to preserve consistency rather than alter predictions, defining its operating range as stability refinement. 4.2. Hallucination We evaluate hallucination under the highlight rendering configuration, which emphasizes salient regions while preserving context. Four benchmarks are used: HaloQuest [63] and MMHal-Bench [75] for hallucination-oriented VQA, VizWiz for real-world unanswerable cases, and COCO Caption for free-form captioning. These benchmarks are chosen because they provide detailed hallucination annotations rather than binary labels. Each response was categorized as Hallucination (H), Misperception (M), Grounded (G), or Correct (C) by ChatGPT 4o-mini [46]. DnR consistently reduced hallucinations across all benchmarks, with absolute decreases of 19 pp and percentage drops of 829%. InstructBLIP (6.78 pp, 26.7%) and Qwen2.5-VL (5.96 pp, 35.0%) showed the strongest declines, while LLaVA 1.6 (29.1%) achieved the largest proportional reduction. Average Misperception decreased by 13 pp. InstructBLIP (-3.97 pp, -13%) and LLaVA 1.6 (-1.53 pp, -10%) showed the strongest declines, while in CogVLM and PaliGemma, some samples shifted from hallucination to misperception. Average grounding increased by 0.65.9 pp across datasets. CogVLM (+3.94 pp, +33.6%) and Qwen2.5-VL (+6.23 pp, +22.0%) achieved the largest gains. LLaVA 1."
        },
        {
            "title": "Model",
            "content": "HaloQuest [63] MMHal-Bench [75] VizWiz [14] COCO Caption [37] M H C M H IDEFICS [24] 43.34/40.87 22.05/23.76 34.62/35.37 13.54/11.50 44.79/44.75 12.50/11.46 29.17/32.29 12.06/17.09 32.16/30.65 19.10/22.11 36.68/30.15 28.14/22.11 32.66/37.69 39.20/40.20 InstructBLIP [8] 33.73/33.02 20.48/21.54 45.79/45.44 35.42/28.12 38.54/45.83 9.38/8.33 16.67/17.71 23.62/18.59 23.62/26.13 20.60/22.11 32.16/33.17 37.69/23.62 29.15/34.17 33.16/42. MiniGPT-v2 [77] 20.22/19.42 29.75/29.52 50.03/51.06 35.42/27.08 48.96/54.17 6.25/8.33 9.38/10.42 11.56/10.55 26.13/24.85 10.05/12.06 52.26/52.54 LLaVA 1.6 [38] 26.33/25.65 13.96/14.57 59.71/59.79 8.33/4.17 26.04/26.04 21.88/23.96 43.75/45.83 1.01/0.53 13.07/17.06 20.10/16.58 65.83/65.83 24.62/20.60 29.15/30.65 46.24/48. PaliGemma [9] 20.67/16.41 14.96/16.63 64.37/66.96 12.50/8.33 31.25/33.33 14.58/9.38 41.67/48.96 2.01/1.11 12.56/10.45 14.57/17.09 70.85/71.36 30.65/28.14 26.63/26.13 42.72/45.73 CogVLM [17] 19.24/17.82 15.60/16.72 65.16/65. 6.25/4.17 52.08/48.96 9.38/17.71 32.29/29.17 20.60/16.08 15.08/17.59 22.61/24.12 41.71/42.21 41.21/36.18 21.61/24.62 37.19/39.19 Qwen2.5-VL [49] 3.48/3.07 11.90/12.30 84.63/84.63 4.17/3. 20.83/14.58 31.25/35.42 43.75/46.88 0.49/0.21 10.55/12.86 25.14/22.10 63.82/64.83 48.24/26.13 24.62/29.15 27.14/44.72 Table 2. Hallucination-benchmark comparison. Each cell reports Draft/DnR results across four hallucination-oriented datasets. Green cells indicate reduced hallucination (H), while blue cells highlight improved grounding or correctness (G or C). Values denote the percentage proportion of each category, where Hallucination+Misperception+Grounded+Correct=100(%)."
        },
        {
            "title": "Metric",
            "content": "Exhaustive Policy-Driven Perf. Cost (%) Acc. (%) VQAv2 [12] CIDEr COCO [37] VSR [67] Acc. (%) ScienceQA [41] Acc. (%) MME [11]"
        },
        {
            "title": "Average",
            "content": "75.2 144.7 68.8 89.7 1444.7 364.6 74.9 143.5 68.7 89.5 1424.4 360.2 0.3 1.2 0.07 0.2 20. 4.42 72.64 52.72 69.97 78.16 78.48 70.39 Table 3. Performance and cost comparison between Exhaustive and Policy-Driven selection for PaliGemma. Both values denote relative differences, with cost reduction reported in percentage. and Qwen2.5-VL showed the clearest shifts from hallucination to grounded responses on MMHal-Bench, while PaliGemma exhibited similar transition on HaloQuest. Average correctness increased by 0.52 pp across benchmarks. PaliGemma (+3.46 pp, +7.4%) and MiniGPT-v2 (+0.78 pp, +4.56%) showed the most notable gains. These results indicate modest yet consistent improvements in response accuracy across all models. Across all models, hallucinated responses were largely redirected toward Misperception and Grounded categories, indicating shift from unfounded to visually supported reasoning. minor exception occurred on VizWiz, where IDEFICS previously overused unanswerable responses; after applying DnR, its behavior became less conservative, leading to fewer such cases and more contextually grounded answers, albeit with slightly lower raw scores. LLaVA 1.6 and Qwen2.5-VL showed the strongest transitions toward grounded reasoning, while CogVLM and PaliGemma demonstrated moderate yet consistent shifts. 4.3. Policy-Driven Expert Selection We train an expert selector Sθ to replace exhaustive expert evaluation. The selector is three-layer MLP defined over the state in Eq. (8) and optimized with the loss in Eq. (9). Using the query set instead of the raw question improves performance through stronger alignment with visual representations. Given s, Sϕ(s) = arg maxj (j) predicts which expert or initial draft yields the highest Uq. PaliGemma [9] was selected for efficiency analysis as it offered the fastest and most reliable inference across datasets. The performance difference remained minimal across four benchmarks (excluding MME with distinct scoring scale), averaging around -0.4 pp, while computational cost decreased by approximately 70% on average. COCO exhibited the smallest cost reduction (-52.7%) since it inherently involves more experts during generation, leaving less room for pruning. Occasional mispredictions occurred where the selector chose the Refine stage instead of Draft, but these cases typically satisfied ˆy = y(j) and base < ϵ, indicating that such swaps had negligible impact on the final outcome in Table 3. (j) These results demonstrate the potential of framing expert selection as learnable task guided by Uq. Rather than remaining an interpretive metric, Uq becomes concrete objective for deciding when and how an expert should intervene. This allows the framework to scale to larger and more diverse expert pools under unified selection criterion. 5. Discussion In our experiments, performance gains show clear linear correlation with utilization, confirming that higher Uq reflects stronger visual grounding. Yet both the absolute level of Uq and its change Uq vary across datasets, architectures, and even individual inputs. Some models sustain high but stable Uq with little variation, indicating consistent yet less adaptive grounding, whereas others display larger Uq shifts linked to stronger gains. These behaviors reveal distinct reasoning patterns and attention distributions among LVLMs. Developing adaptive normalization across domains may further standardize the interpretability and stability of Uq without manual tuning. Rendering is the most flexible yet sensitive component of DnR, and the current hybrid masking remains heuristic whose effectiveness varies with the dataset, masking ratio (ρ), rendering style, and expert setup. policy-driven mechanism that adaptively adjusts masking density, visual emphasis, and expert combinations per input would provide 8 more principled alternative. Such framework can extend rendering beyond fixed single-step heuristic toward multistep adaptive refinement, and can incorporate expert textual outputs hi to jointly enhance both visual and linguistic consistency under unified policy."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the DARPA Young the National Science Foundation (NSF) Faculty Award, under Grants #2127780, #2319198, #2321840, #2312517, and #2235472, the Semiconductor Research Corporation (SRC), the Office of Naval Research through the Young Investigator Program Award and Grants #N00014-21-1-2225 and #N00014-24-1-2547, and the Army Research Office under Grant #W911NF2410360. Additional support was provided by the Air Force Office of Scientific Research under Award #FA9550-22-1-0253. 6. Conclusion We presented Draft and Refine (DnR), scalable agent-style framework that coordinates multiple visual experts to quantify and improve how LVLMs use visual evidence. DnR employs relevance map to assess visual reliance and selects expert-guided refinements through lightweight, modalityagnostic interface rather than heuristic control. Experiments across diverse benchmarks demonstrate consistent accuracy gains, reduced hallucination, and clearer attribution of visual reasoning. Overall, DnR provides principled criterion for evaluating and leveraging visual experts, creating scalable foundation for systematically integrating and expanding expert-driven multimodal reasoning."
        },
        {
            "title": "References",
            "content": "[1] Aishwarya Agrawal, Rishi Nand, and Dhruv Batra. Dont just assume; look and answer: Overcoming priors for visual question answering. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2 [2] Harsh Agrawal, Karan Desai, Yufei Wang, Jiasen Lu, Vlad Morariu, Stefan Lee, Mark Yatskar, Devi Parikh, and Dhruv Batra. nocaps: novel object captioning at scale. In ICCV, 2019. 6, 7 [3] Jean-Baptiste Alayrac et al. Flamingo: visual-language model for generalist vision + language understanding, 2022. Preprint. 2 [4] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. 1, 2 [5] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1 [6] Hao Chen, Jiayi Liu, and Rui Feng. Groundingqa: Enhancing visual question answering via evidence retrieval and verification. arXiv preprint arXiv:2402.09412, 2024. [7] Zhen Chen, Rui Han, Xin Zhao, et al. Vcoder: Versatile vision encoders for multimodal large language models. arXiv preprint arXiv:2403.09820, 2024. 1, 2 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tan, Shafiq Joty, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 6, 8 [9] Sagar Desai, Carlos Riquelme, Basil Mustafa, and et al. Paligemma: unified multimodal model for visionlanguage understanding. arXiv preprint arXiv:2405.14289, 2024. 6, 8 [10] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 2 [11] Chaoyou Fu, Wenhao Zhang, Yichong Zhang, Dongdong Chen, Shuhui Han, Jing Liao, Errui Ding, and Hui Zeng. Mme: comprehensive evaluation benchmark arXiv preprint for multimodal large language models. arXiv:2403.05532, 2024. 6, 7, 8 [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 6, [13] Tanmay Gupta, Aniruddha Kamath, Cordelia Schmid, et al. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 1, 2 [14] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 6, 8 [15] Fabiola Gurrola-Kim, Sanja Fidler, and Yanwei Fu. Reliable vqa: Abstain rather than answer. In European Conference on Computer Vision (ECCV), 2022. 2 [16] Jun Han, Yifan Li, Ziyu Zhao, et al. Grounding by attention: strong baseline for faithful multimodal reasoning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023. 2 [17] Shizhe Hong, Weihao Yu, Yuntao Chen, and et al. Cogvlm: arXiv Visual language models for vision-centric tasks. preprint arXiv:2403.00504, 2024. 6, [18] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 6 [19] Aishwarya Kamath, Mannat Singh, Gabriel Ilharco, Pratyay Banerjee, Tanmay Kumar, Sayna Ebrahimi, Yong Jae Lee, Marcus Rohrbach, Douwe Kiela, and Aniruddha Kembhavi. Mdetr: Modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 6 [20] Kashif Khan and Yanwei Fu. Consistency and uncerIdentifying unreliable responses for selective vqa. tainty: In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [21] Minseok Kim, Hyojin Kim, and Chang Yoo. Single-modal entropy-based active learning for visual question answering. In British Machine Vision Conference (BMVC), 2021. 2 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and In Proceedings of the Ross Girshick. Segment anything. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 1, 6 [23] Wouter Kool, Herke van Hoof, and Max Welling. Stochastic beams and gumbel-top-k sampling for diverse generation. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 4 [24] Hugo Laurencon, Anton Lozhkov, Lucile Saulnier, and et al. Idefics 2: Image-aware decoder-only transformers. Hugging Face Technical Report, 2024. 6, 8 [25] Bowen Li, Yichong Zhang, Bowen Zhao, Tianyu Lin, Xinyue Wang, Dongdong Chen, and Hui Zeng. Seed-bench: Benchmarking multimodal large language models. arXiv preprint arXiv:2403.17839, 2024. 6 [26] Chen Li, Wei Zhang, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2405.06560, 2024. 1, [27] Dongxu Li, Bo Zhao, et al. Visionllm v2: An end-to-end generalist multimodal large language model. arXiv preprint arXiv:2406.01479, 2024. 1, 2 [28] Haotian Li, Pengchuan Liu, Yue Li, Silvio Savarese, and Steven C. H. Hoi. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. 2 [30] Jiacheng Li, Hangjie Deng, et al. Hyperclip: Adapting vision-language models with hypernetworks. arXiv preprint arXiv:2404.01379, 2024. 2 [31] Jinyu Li, Chen Zhang, Jiaqi Xu, and Ziwei Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2402.05000, 2024. 1 [32] Qiang Li, Jiaming Zhao, et al. Hyperllava: Dynamic visual and language expert tuning for multimodal large language models. arXiv preprint arXiv:2404.08580, 2024. 1, 2 [33] Zujie Li, Zhecan Wang, Yue Wu, and Wen Gao. Faithfulvqa: benchmark for faithful visual question answering. In European Conference on Computer Vision (ECCV), 2022. 2 [34] Zihan Li, Yifan Yuan, Jing Han, et al. Meta-prompting for automating zero-shot visual recognition with llms. arXiv preprint arXiv:2401.02345, 2024. [35] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. arXiv preprint, 2025. 2 [36] Hao Liang et al. Octotools: An agentic framework with arXiv preprint extensible tools for complex reasoning. arXiv:2405.08588, 2024. 2 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. ECCV, 2014. 6, 8 [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Llava-1.6: Improved multimodal reasoning with visual instruction tuning. arXiv preprint arXiv:2401.02418, 2024. 2, 6, 8 [39] Haotian Liu, Chengzhi Li, Ming Zhang, Wenqi Shao, Bo Li, Ziyu Wang, Qingyang Ye, et al. Mmbench: Is your arXiv preprint multi-modal model an all-around player? arXiv:2307.06281, 2024. [40] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Peng Gao, Yong-Lu Li, Hongyang Zhang, Tianrui Hui, Qiang Li, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 6 [41] Pan Lu, Swaroop Mishra, Tony Xia, Lin Qiu, Kai-Wei Chang, Song-Chun Zhu, Peter Clark, and Yizhou Liang. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 6, 7, 8 [42] Timo Luddecke and Alexander Ecker. Clipseg: Image segIn CVPR, 2022. mentation using text and image prompts. 6 [43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, 2019. 6 [44] Ankan Kumar Mishra, Karteek Alahari, and CV Jawahar. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 6 [45] OpenAI. Gpt-4v(ision): Multimodal capabilities of gpt-4. OpenAI Technical Report, 2023. 2 [46] OpenAI. Chatgpt 4o-mini. https://chat.openai. com/, 2025. Used as automatic evaluator for multimodal hallucination analysis. 7 [47] Zhiliang Peng, Wenhui Wang, Dong Li, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2 [48] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015. 6 [49] Qwen Team. Qwen2.5-vl: strong multimodal foundation model. Technical report, Alibaba Group, 2025. 6, [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 6 10 [51] Michael Reid et al. modal understanding across domains. arXiv:2403.05530, 2024. 2 Gemini 1.5: Unlocking multiIn arXiv preprint [52] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP, 2019. [53] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761, 2023. 1, 2 [54] Dustin Schwenk, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, and Aniruddha Kembhavi. A-okvqa: benchmark for visual question answering using world knowledge. In CVPR, 2022. 6, 7 [55] Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking hint: Leveraging explanations to make vision and language models more grounded. In International Conference on Computer Vision (ICCV) Workshop on Debugging Machine Learning Models, 2019. 1 [56] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. CoRR, abs/2303.17580, 2023. 1, 2 [57] Amanpreet Singh, Vivek Natarajan, Yu Jiang, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Towards vqa models that can read. In CVPR, 2019. 6 [58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.12360, 2024. [59] Shreyas Tuli, Xiang Liang, Hyun Lee, et al. Vision-amplified semantic entropy for hallucination detection in multimodal large language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [60] Nicolas Vieillard, Olivier Pietquin, and Matthieu Geist. Diverse sampling with gumbel-top-k. In International Conference on Machine Learning (ICML), 2020. 4 [61] Andres Villa, Juan Carlos Leon Alcazar, Alvaro Soto, and Bernard Ghanem. Behind the magic, merlim: Multimodal evaluation benchmark for large image-language models. arXiv preprint arXiv:2312.02219, 2023. 2 [66] Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Wang, and Changsheng Xu. Towards visual grounding: survey. arXiv preprint arXiv:2412.20206, 2024. 1 [67] Zhiqiang Xie, Zhiyong Huang, Wenqi Shi, and Yuxin Liu. Visual spatial reasoning with graph-structured representation. In AAAI, 2023. 6, 7, 8 [68] Chen Yang, Yifan Wu, Ming Xu, et al. Retrieval-augmented visual question answering for grounded reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. 2 [69] Xiaokang Yang, Yixiao Ge, and Ying Shan. Depth anything v2: Unleashing the power of large-scale pretraining. arXiv preprint arXiv:2406.11886, 2024. [70] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. CoRR, abs/2303.11381, 2023. 1, 2 [71] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. 1, 2 [72] Jing Ye et al. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2402.13273, 2024. 1, 2 [73] Tao Yin et al. survey of multimodal large language models: From visual embed to language reasoning. arXiv preprint, 2024. 2 [74] Xi Yin, Yuchen Zhou, Jianfeng Gao, et al. Cq-vqa: Visual question answering on categorized questions. In European Conference on Computer Vision (ECCV), 2020. 2 [75] Xiang Yin, Lei Zhu, Dan Xu, Hengshuang Zhao, and Jing Shao. Mmhal-bench: benchmark for multimodal hallucination evaluation. arXiv preprint arXiv:2310.01817, 2023. 1, 7, 8 [76] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019. 6, [77] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohan S. Kankanhalli. Minigpt-v2: Efficient vision-language In Advances in Neural Information Processing alignment. Systems (NeurIPS), 2023. 6, 8 [62] Yuhang Wang et al. ing with grounded chain-of-thought. arXiv:2403.09719, 2024. 2 Argus: Vision-centric reasonarXiv preprint [63] Zhecan Wang, Garrett Bingham, Adams Wei Yu, Quoc Le, Thang Luong, and Golnaz Ghiasi. Haloquest: visual hallucination dataset for advancing multimodal reasoning. In European Conference on Computer Vision, pages 288304. Springer, 2024. 7, 8 [64] Tianhao Wu, Nikhil Nair, and Jiajun Xu. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2302.14838, 2023. 1, [65] Tianhao Wu et al. Visual chatgpt: Talking, drawing and arXiv preprint editing with visual foundation models. arXiv:2303.04671, 2023."
        }
    ],
    "affiliations": [
        "MOLOCO",
        "Northeastern University",
        "University of California, Irvine"
    ]
}