{
    "paper_title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models",
    "authors": [
        "Guo Chen",
        "Zhiqi Li",
        "Shihao Wang",
        "Jindong Jiang",
        "Yicheng Liu",
        "Lidong Lu",
        "De-An Huang",
        "Wonmin Byeon",
        "Matthieu Le",
        "Tuomas Rintamaki",
        "Tyler Poon",
        "Max Ehrlich",
        "Tuomas Rintamaki",
        "Tyler Poon",
        "Tong Lu",
        "Limin Wang",
        "Bryan Catanzaro",
        "Jan Kautz",
        "Andrew Tao",
        "Zhiding Yu",
        "Guilin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 7 2 5 1 . 4 0 5 2 : r 2025-4Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Guo Chen1*, Zhiqi Li1*, Shihao Wang2*, Jindong Jiang3*, Yicheng Liu1, Lidong Lu1, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, Tyler Poon, Max Ehrlich, Tong Lu1, Limin Wang1, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, Guilin Liu"
        },
        {
            "title": "Abstract",
            "content": "We introduce Eagle 2.5, family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B. Links: Project Page 1. Introduction Despite the significant advances in multimodal learning Bai et al. (2023); Chen et al. (2023); Li et al. (2023,); Wang et al. (2024), many visionlanguage models (VLMs) remain focused on shortcontext tasks, with long-context understanding under-explored. This gap is particularly evident in both long video comprehension and highresolution image/video understanding, where the processing of extended visual contexts remains an open challenge. Such extended contexts encompass multiple images, extended video sequences, high-resolution media, or combinations thereof. However, the development of long-context VLMs is still in its early stages, hindered by fundamental challenges in dataset construction, architecture design, training strategies, and computation/memory bottlenecks. Figure 1: Performance comparison of Eagle 2.5 with leading vision-language models on the Video-MME benchmark. Eagle 2.5 demonstrates consistent improvement as the number of input frames increases. To enable long-context visual understanding, several approaches have been proposed to address the challenge of processing extended visual inputs by designing specialized compression or selection modules Jin et al. (2024); Korbar et al. (2024); Li et al. (2023,, 2024); Shen et al. (2024); Weng et al. (2024); Yu et al. (2024). While these methods effectively circumvent the need * Work Done during an internship at NVIDIA. Equal advising and corresponding authors: guilinl@nvidia.com, zhidingy@nvidia.com. Additional affiliations: 1 Nanjing University, 2 The Hong Kong Polytechnic University, 3 Rutgers University. 2025 NVIDIA. All rights reserved. Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models to extend the context length of VLMs, they often introduce additional computational overhead or capacity limitations, potentially constraining model performance. promising research direction is to extend the context length of LLMs to enable native long-context understanding. While prior studies Shen et al. (2025); Xue et al. (2024); Zhang et al. (2024) have explored this direction, challenges and key limitations still remain. First, the performance of existing methods is often suboptimal, generally falling behind proprietary models. Second, these approaches struggle to achieve consistent improvements as the amount of visual input increases. Lastly, the optimal training strategies for state-of-the-art long-context VLMs remain unclear, given the complex interplay of factors such as training strategies and data recipes. To this end, we present Eagle 2.5, versatile multimodal model designed to efficiently process extensive contextual information. Unlike models solely optimized for handling long multimodal sequences without improving performance, Eagle-2.5 benefits from increased input length, leading to consistent performance gains besides merely accommodating longer inputs. As shown in Fig. 1, our model achieves superior context coverage and exhibits consistent performance scaling with increasing frame counts. Notably, it attains competitive results compared to larger models such as GPT-4o OpenAI (2023) and Qwen2.5-VL-72B Bai et al. (2025), while maintaining significantly smaller parameter footprint. Eagle 2.5 is driven by both the advanced training strategy and data recipe. For training strategy, we introduce two core components for effective long-context learning: information-first sampling and progressive training. Information-first sampling. The information-first sampling strategy ensures the preservation of essential visual and semantic information through two mechanisms: (1) Image Area Preservation, which optimizes tiling to retain the majority of the original image area while maintaining aspect ratio fidelity, avoiding rigid aspect ratio constraints; and (2) Automatic Degradation Sampling (ADS), which dynamically balances visual and textual inputs by prioritizing complete text retention while adaptively optimizing visual content to maximize context length utilization and preserve multimodal information. Progressive training. We employ progressive mixed post-training approach, wherein context length is incrementally expanded during training, enhancing the models ability to process inputs of varying sizes. This integrated strategy significantly improves information density over static sampling methods while ensuring consistent performance across diverse input types and lengths. For data recipe, we embrace the diversity first, then quality principle in curating the training data pool. Our data recipe combines open-source data (including human-annotated data as well as synthetic video data) with our self-curated Eagle-Video-110K dataset, specifically designed to enhance long video understanding capabilities. We adopt diversity-driven collection strategy, using multiple video sources and similarity thresholding method to identify novel clips that maximize content diversity. Our dataset is distinguished by its dual annotation approach: top-down story-level method that leverages human-annotated chapters as meaningful segments instead of traditional shot-level segmentation, producing dense captions that form the basis for comprehensive long-form QA pairs capturing the entire videos narrative structure; complementary bottom-up clip-level approach that generates focused QA pairs for short clips using GPT-4o with diverse question types. To address the challenge of extending localized clip annotations to full-length videos, we implement anchors that incorporates temporal references and contextual elements without revealing answers, thereby letting models understand both overarching narratives and precise spatio-temporal details within videos. Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models 2. Related Work Vision-language models. Advancements in large language models (LLMs) Achiam et al. (2023); Dubey et al. (2024); OpenAI (2023) have significantly propelled visual understanding by integrating visual features, leading to the creation of Visual Language Models (VLMs) Li et al. (2023); Liu et al. (2023); OpenAI (2023); Zhu et al. (2024). Open-source VLMs Dai et al. (2024); Li et al. (2024,); Liu et al. (2023, 2024, 2025); Team (2024); Wang et al. (2024) continue to achieve breakthroughs, often matching or exceeding the performance of state-of-the-art commercial models like GPT-4V/4o OpenAI (2023) and Gemini-1.5 Reid et al. (2024). The release of open-source VLMs IDEFICS (2023); Li et al. (2024); Tong et al. (2024), complete with its training data and code base, has further accelerated research in this area. However, most current VLMs primarily focus on short-context understanding, handling only few images or short video clips at time. Eagle 2.5 advances this field by concentrating on long-context visual understanding through comprehensive exploration and development of training strategies and data recipes. Long-context VLMs. Long-context VLMs were developed to address the challenges of processing large multimodal sequences. Currently, methods for long-context VLMs fall into two main categories. The first category involves specialized modules designed for context compression. Question-guided compressions Korbar et al. (2024); Li et al. (2024); Shen et al. (2024) or selection Li et al. (2025); Yu et al. (2023, 2024) methods extract question-related visual cues through an additional module, while various token reduction techniques Jin et al. (2024); Li et al. (2023, 2024); Wang et al. (2024); Weng et al. (2024); Yu et al. (2024); Zhang et al. (2024) aim to minimize the visual representation before LLM processing. The other category attempts to directly extend the context of LLMs. Works like LongVA Zhang et al. (2024), LongVILA Xue et al. (2024), and LongViTA Shen et al. (2025) directly extend the context length of LLMs to accommodate longer multimodal sequences. While promising, these approaches often underperform proprietary models, fail to show consistent performance improvements with increasing visual input, and have underexplored constraints on training strategies and data recipes. Our approach focuses on developing native long-context capabilities that enhance VLMs by exploring training data, formulations, and without introducing additional compression modules or suffering from performance inconsistencies observed in previous expansion attempts. Long-context multimodal data. To enhance VLMs long-context multimodal understanding capabilities, various datasets have been proposed. Some datasets focus on multimodal understanding of long documents Pramanick et al. (2025); Tanaka et al. (2023); Tito et al. (2023); Van Landeghem et al. (2023), such as slides and papers. However, they often lack temporal understanding. Other datasets Ghermi et al. (2025); Huang et al. (2020); Rawal et al. (2024); Song et al. (2024,); Wu and Krahenbuhl (2021); Yue et al. (2023, 2024) emphasize the temporal coherence and information retrieval across long spans inherent in movies. Additionally, recent datasets Chen et al. (2024); Han et al. (2023); Miech et al. (2019); Zhang et al. (2024) covering domains further enhance VLMs long-context multimodal understanding. Regarding the annotation methods for longcontext multimodal datasets, early works Huang et al. (2020); Miech et al. (2019); Song et al. (2024); Tanaka et al. (2023); Tito et al. (2023); Van Landeghem et al. (2023)relied on manual efforts. To reduce costs, some methods Chen et al. (2024); Ghermi et al. (2025); Han et al. (2023); Pramanick et al. (2025); Rawal et al. (2024); Song et al. (2024); Yue et al. (2023, 2024); Zhang et al. (2024) use tools like GPT-4V OpenAI (2023) and Gemini Team et al. (2023) for automated or semi-automated annotation. Recent advancements in data construction emphasize hierarchical annotation strategies Han et al. (2023), which can preserve narrative structure in long videos. These advancements reflect trend towards creating balanced datasets that effectively assess long-context multimodal understanding while managing creation costs. 3. Eagle 2.5 In this section, we present Eagle 2.5 from three parts: model architecture, training strategies, and data recipe. 3.1. Model Architecture 3 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models We design our proposed model as versatile multimodal system capable of efficiently processing long-context information, rather than specialized model solely optimized for handling extended multimodal inputs. To ensure adaptability and generalization across diverse tasks, we deliberately avoid incorporating tailored compression modules that might constrain the models flexibility. Following the architecture of LLaVA Liu et al. (2023), we employ an MLP projection layer to align vision embeddings from SigLIP Zhai et al. (2023) with the LLM representation space, as shown in Fig. 2. In this work, we utilize the Qwen2.5 series models Team (2024). To effectively handle any-resolution images, we adopt the image tiling strategy, inspired by LLaVA-1.5 Liu et al. (2024) and InternVL Chen et al. (2023). Figure 2: Tiling-based general multimodal system. 3.2. Training Strategy Our approach contains two key components to achieve effective long-context training: first, an information-first sampling strategy that establishes optimal sampling criteria; and second, progressive training schedule based on this strategy, which directs the entire model training process. 3.2.1. Information-First Sampling In multimodal training, the sampling of visual content is essential. Multi-image documents typically comprise dozens of pages with ultra-high-resolution images, while video content can vary drastically in length - from mere seconds to hours. To effectively manage this diversity, we present information-first sampling to promote information preservation from both visual and semantic dimensions. Image area preservation (IAP). Traditional tiling methods divide an image of size ğ‘Š ğ» into rigid ğ‘Ÿğ‘¤ ğ‘Ÿâ„ grid of ğ‘  ğ‘  tiles. While effective for handling high-resolution inputs, these approaches often distort the original image geometry through improper aspect ratio handling. For example, InternVL Chen et al. (2023) imposes strict aspect ratio constraints that force image downsampling, undermining the purpose of tiling. To address this, we propose an area-prioritized tiling strategy that optimizes two key objectives: Area Preservation: Encourage maintaining at least 60% of the original area (ğ´orig = ğ‘Š ğ») in the tiled version (ğ´new = ğ‘Ÿğ‘¤ğ‘Ÿâ„ğ‘ 2). Aspect Ratio Fidelity: Align the tiling ratio ğ‘Ÿğ‘¡ = ğ‘Ÿğ‘¤/ğ‘Ÿâ„ with the original aspect ratio ğ‘Ÿorig = ğ‘Š/ğ». Figure 3: Image area preservation. Compared to the tiling strategy (a) from InternVL Team (2024), our method (b) effectively retains larger portion of the original image, especially for high-resolution inputs. This ensures that more comprehensive visual information is preserved, benefiting tasks that require fine-grained details. For candidate tiling ratios {(ğ‘Ÿğ‘¤, ğ‘Ÿâ„) ğ‘Ÿğ‘¤ ğ‘Ÿâ„ ğ‘ }, we select the optimal configuration by: arg max (ğ‘Ÿğ‘¤,ğ‘Ÿâ„) min ) , 0. ( ğ´new ğ´orig Area penalty ) min ( ğ‘Ÿğ‘¡ ğ‘Ÿorig Aspect ratio alignment ğ‘Ÿorig ğ‘Ÿğ‘¡ , (1) This formulation imposes penalties when ğ´new < 0.6ğ´orig but avoids over-rewarding configurations where ğ´new > 0.6 ğ´orig. The aspect ratio term reaches maximum value 1 when ğ‘Ÿğ‘¡ = ğ‘Ÿorig, decaying symmetrically 4 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models for deviations. comparison between the strategies is shown in Fig. 3. Automatic degradation sampling. VLMs require careful allocation of sequence length budgets between visual and textual inputs. Conventional vision-context-centric approaches sample visual content (e.g., video frames) at fixed rates or with predetermined counts, risking text truncation and suboptimal token allocation. We propose Automatic Degradation Sampling (ADS), an all-context-centric strategy that dynamically optimizes this balance. Given training sample ğ’® = {ğ‘†visual, ğ‘†text} with max sequence length â„’max, where ğ‘†visual contains arbitrary combinations of images, videos, and multi-page documents: 1. Compute fixed text token length â„’text. 2. Derive fixed visual token budget: â„’visual = â„’max â„’text. Thus, we keep the complete textual information by constricting the visual token budget. For visual content optimization under â„’visual, we distinguish two types and optimize two key variables: Images: Optimize maximal tile count per image ğ‘¡ to maximize spatial information of ğ‘€ images. Temporal content (videos/documents): Optimize sampling count ğ‘› to maximize temporal coverage. The constrained optimization problem is formulated as: maximize ğ‘¡,ğ‘› subject to ğ‘€ ğ‘–= ğ‘€ ğ‘–=1 ğ¿(ğ‘¡, ğ¼ğ‘–) + 256 ğ‘› ğ¿(ğ‘¡, ğ¼ğ‘–) + 256 ğ‘› â„’vis (2) 1 ğ‘¡ 12, { ğ‘max = 1 ğ‘› ğ‘max 2 duration (video) pages (document) Where optimization variables are the tile count per image (ğ‘¡) and temporal sampling count (ğ‘›), with fixed parameters including: total image instances ğ‘€ (calculated from input), token function ğ¿(ğ‘¡, ğ¼ğ‘–) used to calculate the tokens of ğ‘–-th image ğ¼ğ‘– under maximal tiling number ğ‘¡, predefined upper bounds ğ‘‡max = 12 (max tiles per image) and ğ‘max = 2 duration/1 pages (video/doc constraints). For temporal content, we do not use image tiling, thus the token quantity per temporal unit (frame/page) is ğ¿(1, ) = 256. Given that training samples typically exhibit mutually exclusive composition (predominantly images or temporal content), ADS employs dual-phase degradation process to address the above optimization problem: Temporal degradation: Initially, we fix the max tile number ğ‘¡ = 1 and focus on temporal sampling. We target sampling rate of 2 FPS for videos, and the usage of all images for multi-image documents. We also require that each visual input has at least ğ‘min frames; if this minimum cannot be met within the visual context budget, the sample is discarded. Formally, the maximally sampled temporal units is: ğ‘›* = â„’visual ğ‘€ 256 Tiling degradation: After deciding the number of frames, we dynamically adjust the tiling to maximize the use of available context. Let ğ’¯ = {12, 8, 6, 4, 2, 1} represent the possible tile configurations in decreasing Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models order. We choose the highest tile configuration ğ‘¡* such that: { ğ‘¡* = max ğ‘¡ ğ’¯ : ğ‘š ğ‘–=1 ğ¿(ğ‘¡, ğ¼ğ‘–) (â„’ğ‘‰ ğ‘›* 256) } This strategy preserves as much visual detail as possible while ensuring the full textual input is retained, thereby optimizing the overall learning signal. This dual-phase approach guarantees complete text preservation while dynamically adapting visual resolution to available context budget, achieving superior information density compared to static sampling strategies. 3.2.2. Post-Training Schedule We introduce comprehensive post-training framework consisting of two complementary strategies. First, we establish foundational mixed post-training approach, upon which we develop an enhanced progressive mixed post-training strategy to substantially improve model performance across varying context lengths. Mixed post-training. Since the model needs to efficiently process multimodal inputs of diverse lengths, maintaining consistent performance across variable context sizes is essential. Our ADS method adaptively adjusts each training sample to the maximum sequence length â„’max, providing frame-agnostic training paradigm. We implement mixed training strategy with length-balanced packing Authors (2025) to optimize performance uniformly across the entire spectrum of context lengths. Progressive mixed post-training. For scenarios with large â„’max values, balancing the distribution of long and short sequences becomes computationally intensive, and achieving optimal performance through single training iteration proves challenging. To address this limitation, we propose progressive mixed training methodology that gradually exposes the model to increasingly larger â„’max values, systematically enhancing its capacity to process extended contexts. Compared to conventional mixed training, our method more effectively preserves the models capabilities across different sequence lengths while safely generating diverse model variants at intermediate training stages. In our exeriment, we sequentially set â„’max to 32K, 64K and 128K. 3.3. Data Recipe Our data recipe begins with open-source data. We embrace the diversity first, then quality principle and gather data from various open sources. This data mainly comprises high-definition multi-image/short videos, long videos, multi-page documents, and extensive text data. We also find that current open-source video data often lacks sufficient length. We thus propose novel dataset, Eagle-Video-110K, to complement the length, as shown in Fig. 4. 3.3.1. Open-Source Long-Context Data models capability is intrinsically linked to the diversity of its training data. Therefore, gathering the most diverse data possible represents fundamental principle of this work, leading to two main strategies: Human-annotated Data: We integrate various open-source human-annotated datasets, including established video and image-document collections such as COIN Tang et al. (2019) and SlideVQA Tanaka et al. (2023), which can be directly considered as high-quality data. Synthetic Video Data: Considering that videos naturally contain long-context information, we incorporate open-source synthetic video data, such as LLaVA-Video Zhang et al. (2024). These datasets are primarily annotated automatically using state-of-the-art models including GPT-4V/4o OpenAI (2023,?), Claude3 Anthropic (2024), and Gemini-1.5 Pro Reid et al. (2024). Combined with short-context data, all collected open-source datasets are summarized in Tab. 1. For convenience, we refer to this collective dataset as Open-Data. 6 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Category Video Classification Temporal Action Localization Video Temporal Grounding Dense Video Captioning Temporal Segmentation Temporal Reasoning General Video QA Multi-Page Document Video Captioning Long Text Dataset Kinetics710 Carreira and Zisserman (2017); Wang et al. (2024), Something-Something-v2 Goyal et al. (2017), ActivityNet Caba Heilbron et al. (2015), HACS Segment Zhao et al. (2019), COIN Tang et al. (2019), HIREST Zala et al. (2023), FineAction Liu et al. (2022), PortraitMode-400 Han et al. (2024) ActivityNet Caba Heilbron et al. (2015), HACS Segment Zhao et al. (2019), FineAction Liu et al. (2022), Ego4D-MQ Grauman et al. (2022), COIN Tang et al. (2019), HIREST Zala et al. (2023), PerceptionTest Patraucean et al. (2023) Charade-STA Gao et al. (2017), QVHighlight Lei et al. (2021), Ego4D-NLQ Grauman et al. (2022), Didemo He et al. (2020), QueryD Oncescu et al. (2021), MedVidQA Gupta et al. (2023), Youcook2 Zhou et al. (2018), FineVideo FarrÃ© et al. (2024), ActivityNet Caba Heilbron et al. (2015), HACS Segment Zhao et al. (2019), FineAction Liu et al. (2022), Ego4D-MQ Grauman et al. (2022), COIN Tang et al. (2019), HIREST Zala et al. (2023), Perception-Test Patraucean et al. (2023), EgoExoLearn Huang et al. (2024) ActivityNet Caba Heilbron et al. (2015), Youcook2 Zhou et al. (2018), EgoExoLearn Huang et al. (2024), ViTT Huang et al. (2020), HIREST Zala et al. (2023), COIN Tang et al. (2019) Breakfast Kuehne et al. (2014), ViTT Huang et al. (2020) ActivityNet-RTL Huang et al. (2024) TVQA Lei et al. (2018), CLEVRER Yi et al. (2020), NextQA Xiao et al. (2021), SportsQA Li et al. (2024), LLaVA-Video Zhang et al. (2024), FineVideo FarrÃ© et al. (2024), VideoGPT+ Maaz et al. (2024), Oops Epstein et al. (2020), Perception-Test Patraucean et al. (2023), EgoTaskQA Jia et al. (2022), CinePile Rawal et al. (2024), STAR Wu et al. (2024) SlideVQA Tanaka et al. (2023), DUDE Van Landeghem et al. (2023), MP-DocVQA Tito et al. (2023) ActivityNet Caba Heilbron et al. (2015), Youcook2 Zhou et al. (2018), Shot2story Han et al. (2023), Vript Yang et al. (2025), LLaVA-Video Zhang et al. (2024), Momentos Wang et al. (2024), FunQA Xie et al. (2024), S-MiT Monfort et al. (2021), LLaVA-Hound Zhang et al. (2024), Ego4D-HCap Islam et al. (2024), EgoExoLearn Huang et al. (2024) LongAlign Bai et al. (2024), LongReward Zhang et al. (2024) Table 1: Video, multi-page document, and long text dataset used in Eagle-2.5. 3.3.2. Eagle-Video-110K We curate Eagle-Video-110K to enhance long video understanding capabilities. Specifically, we first collect videos using diversity-driven strategy. We then automatically annotate these videos using both top-down and bottom-up approaches to generate comprehensive story-level and fine-grained clip-level annotations, as shown in Fig. 5. Diversity-driven video collection. We utilize several data sources for our video collection: Vidchapters Yang et al. (2023), MiraData Ju et al. (2025), InternVid-10M Wang et al. (2024), Panda-70M Chen et al. (2024), Vript Yang et al. (2025), Shot2story Han et al. (2023), ViTT Huang et al. (2020), and WebVid10M Bain et al. (2021), collectively referred to as ğ´. Our approach prioritizes diversity, focusing on gathering wide range of video content. For the current training dataset ğµ, we use CLIP Radford et al. (2021) to extract temporal features at rate of 1 frame per second. Videos from both ğ´ and ğµ are segmented into 10-second clips. We perform pooling operation on each clips frames to derive representative feature vector. Let {ğ‘ğ‘–}ğ‘ğµ ğ‘–=1 and {ğ‘ğ‘—}ğ‘ğ´ ğ‘—=1 For each clip ğ‘ğ‘— in ğ´, we identify its maximum similarity with any clip in ğµ: Figure 4: Comparison of video duration between open-source data and Eagle-Video-110K. represent the clips from ğµ, represent those from ğ´. We calculate the pairwise cosine similarity between clips from ğµ and ğ´. We then introduce similarity threshold ğœ = 0.5. Clips in ğ´ with ğ‘†max(ğ‘ğ‘—) below this threshold are considered ğ‘†max(ğ‘ğ‘—) = max 1ğ‘–ğ‘ğµ ğ‘†(ğ‘ğ‘–, ğ‘ğ‘—) Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Figure 5: Overview of our video annotation framework combining bottom-up clip-level and top-down story-level approaches. The diagram illustrates our dual annotation strategy. In the bottom-up approach (left), short video clips are processed by GPT-4o to generate clip-level QA pairs enhanced with time anchors and textural context anchors. In the top-down approach (right), human annotators create story-level segmentations of longer videos, which are then captioned and processed by GPT-4 to generate comprehensive story-level QA pairs. This hierarchical methodology enables both fine-grained temporal understanding and high-level semantic comprehension of video content. most novel relative to ğµ: ğ´novel = {ğ‘ğ‘— ğ´ ğ‘†max(ğ‘ğ‘—) < ğœ } The clips in ğ´novel and their corresponding original videos are selected to enhance the diversity of our collection. Story-level video data. We construct story-level annotations for long videos using top-down approach. Unlike existing video datasets such as Shot2story Han et al. (2023), which employs shot detection to segment videos and construct storylines across shots, our methodology differs fundamentally. Shot-level segmentation often results in over-segmentation, producing excessively detailed annotations that are suboptimal for constructing coherent story-level text. Instead, we leverage human-annotated chapters as video segments, which provide more semantically meaningful annotations. We incorporate content from ViTT Huang et al. (2020) and Vidchapters Yang et al. (2023) among the selected videos and filter out any videos with fewer than two chapters to ensure they serve as effective story-level sources. Chapter-level dense caption. For video divided into ğ‘ clips, where each clip spans from timestamp ğ‘ to ğ‘, we perform visual captioning for each segment individually. For each segment, frames are sampled at rate of up to 2 frames per second, with maximum of 50 frames. These sampled frames, together with user-provided segment titles, guide GPT-4o OpenAI (2023) in generating detailed visual descriptions focused on the content indicated by the titles. Long-form QA generation. Once visual descriptions for all segments are completed, we compile the captions for the entire video along with their corresponding time intervals and chapter titles. This aggregated information is provided to GPT-4 Achiam et al. (2023), which generates diverse question-answer pairs covering multiple question types. Clip-level video data. Story-level video data typically emphasizes high-level semantic information that unfolds over extended periods. However, for general queries, it is often necessary to focus on localized spatiotemporal details. To address this need, we propose bottom-up, computationally efficient automatic annotation method. This approach enables the generation of short clip annotations and facilitates the conversion of segment-level annotations into video-level ones by incorporating temporal and contextual anchors. Clip-level video QA generation. We generate QA pairs for each short clip in dataset ğ´ based on various question types. Specifically, we sample frames from each short clip at rate of up to 2 frames per second and input them into GPT-4o. From predefined question type pool, we randomly select five question types and prompt the model to generate corresponding question-answer pairs. Clip-to-video QA conversion. Since annotations for individual clips are designed for localized queries, conflicts Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Model Closed-Source Models GPT-4o-0806 OpenAI (2023) Claude-3.5-Sonnet Anthropic (2024) Gemini-1.5-Pro Reid et al. (2024) Publicly Available Models MiniCPM-V2.6-8B Yao et al. (2024) LongVILA-8B Chen et al. (2024) InternVL2.5-8B Chen et al. (2024) LLaVA-Video-8B Zhang et al. (2024) Qwen2.5-VL-8B Bai et al. (2025) VideoChat-Flash-8B Li et al. (2024) InternVL2.5-78B Chen et al. (2024) Qwen2.5-VL-72B Bai et al. (2025) LLaVA-Video-72B Zhang et al. (2024) Eagle2.5-8B MVBench Perception_test EgoSchema MMB-Video MLVU LVBench Video-MME CG-Bench HourVideo Charade-STA - - - - - 67.1 72.0 58.6 69.6 74.0 76.4 70.4 64.1 74.8 Val fullset - Val Val w/o subtitle subtitle Clue Long Open mIoU Dev Test mIoU - - - - 58.1 - 67.9 70.5 76.2 - 73.2 74.3 82. - - 72.2 - 67.7 - 57.3 65.0 - - 76.2 65.6 72.2 1.63 - 1.30 - - 1.68 - 1.79 - 1.97 2.02 - 1.94 - - - - - 68.9 70.8 70.2 74.6 75.7 74.6 74. 77.6 66.7 - 64.0 - 57.1 60.0 58.2 56.0 64.7 63.6 60.7 61.9 66.4 71.9 60.0 75.0 60.9 60.1 64.2 63.3 65.1 65.3 72.1 73.3 70.6 72.4 77.2 62.9 81. 63.7 65.1 66.9 69.7 71.6 69.7 74.0 79.1 76.9 75.7 58.6 44.9 39.2 5.73 56.5 40.3 35.6 4.17 50.9 37.8 28.7 3.85 37.2 37.4 - - - - 44.4 29.9 26.3 2.27 47.5 34.3 26.6 - - - - - - - - - 44.5 35.5 24.1 2.48 52.8 43.1 37.5 1.49 59.5 44.2 34.2 3.90 - - - - - - - - - - - - - - - - - - - - - - - - - - 55.8 46.6 45.6 13.4 44.5 41.8 35.7 - - - - - - 43.6 - - 50.9 - 65.9 Table 2: Comparison with SoTA models on Various Video Benchmarks. We sample each video at 2 FPS by default and disable tiling, and limit the minimum sampling frame number to 8 frames. Among them, the maximum frame number of Video-MME is 512, and the others are 256. Perception-Test turns on tiling to enable high-resolution testing. Model DocVQA ChartQA InfoVQA TextVQA OCRBench MMstar RWQA AI2D MMMU MMB1.1 MMVet HallB MathVista Avg Test Test-Mini Score Test Test Test Test Test Test Test Test Test Val Val Closed-Source Models GPT-4o-0806 OpenAI (2023) Claude-3.5-Sonnet Anthropic (2024) Gemini-1.5-Pro Reid et al. (2024) Publicly Available Models MiniCPM-V2.6-8B Yao et al. (2024) LLaVA-One-Vision-8B Li et al. (2024) InternVL2.5-8B Chen et al. (2024) Qwen2.5-VL-8B Bai et al. (2025) LLaVA-One-Vision-72B Li et al. (2024) LLaMa-3.2-90B-Vision Dubey et al. (2024) Eagle2.5-8B 92.8 95.2 93.1 90.8 87.5 93.0 95.7 91.7 90.1 94.1 85.7 90.8 87.2 82.4 80.0 84.8 87.3 83.7 85.5 87.5 79.2 74.3 81.0 - 68.8 77.6 82.6 74.9 - 80. 77.4 74.1 78.8 80.1 - 79.1 84.9 - - 83.7 736 788 754 852 622 822 864 741 783 869 64.7 65.1 59.1 57.5 61.7 62.8 63.9 66.1 55.3 66. 75.4 84.6 60.1 81.2 67.5 79.1 69.1 68.3 62.2 65.0 82.1 66.3 81.4 70.1 84.5 68.5 83.9 71.9 85.6 - - 76.7 84. 49.8 48.8 56.0 58.6 56.6 60.3 55.8 83.1 80.9 74.6 78.0 80.9 83.2 82.6 84.5 77.3 81.7 69.1 70.1 64.0 55.0 55.5 45.6 60.0 57.5 62.8 67.1 60.6 64.1 62. 48.1 31.6 50.1 52.9 47.5 44.1 54.7 63.8 67.7 63.9 60.6 63.2 64.4 68.2 68.4 57.3 67.8 74.9 74.0 71.7 - - 73.1 75.6 - - 75.6 Table 3: Comparison with SoTA models on Various Image Benchmarks. The avg score is computed as the average of all benchmark scores, with OCRBench score divided by 10. may arise when these queries are extended to the entire video. To address this issue, we introduce two types of anchors for each clip-question pair: (1) We directly incorporate time intervals into questions to establish temporal references; (2) Using GPT-4o, we generate textual context anchors that provide additional information without revealing the answers. 4. Experiments 4.1. Comparison with State-of-the-Art VLMs Video benchmarks. As shown in Tab. 2, Eagle2.5-8B demonstrates strong performance across multiple video understanding benchmarks. It achieves 74.8 on MVBench Li et al. (2024), 82.0 on Perception_test Patraucean et al. (2023) and 72.2 on EgoSchema, outperforming similar-sized models like InternVL2.5-8B Team (2024) (72.0, -, -) and Qwen2.5-VL-8B Wang et al. (2024) (69.6, 70.5, 65.0). Eagle2.5-8B excels in MLVU Zhou et al. (2024) (77.6) and LongVideobench Wu et al. (2025) (66.4), surpassing even InternVL2.5-78B (75.7, 63.6). For VideoMME (w/o subtitle), the performance of Eagle 2.5 (72.4) significantly surpasses models of the same size and is extremely close to the 72B parameter model. On CG-Bench Chen et al. (2024), it scores 55.8, 46.6, 45.6, 13.4 across metrics, exceeding Claude-3.5-Sonnet Anthropic (2024) (56.5, 40.3, 35.6, 4.17) and Gemini1.5-Pro Reid et al. (2024) (50.9, 37.8, 28.7, 3.85). With 44.5 on HourVideo Chandrasegaran et al. (2025) dev set and 41.8 on test set, all surpassing Gemini-1.5-Pro Reid et al. (2024). Finally, on Charade-STA Gao et al. (2017), Eagle 2.5 outperforms other models significantly, demonstrating strong temporal perception capabilities. 9 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Training & Data recipe DocVQA ChartQA InfoVQA TextVQA OCRBench MMstar RWQA AI2D MMMU MMB1.1 MMVet HallB MathVista Avg Test Test-Mini Score Test Test EN-Val Test Test Test Val Val Val Val Val 92.6 Eagle2.5-S2 92.3 Eagle2.5-S2+Eagle2.5-S2, â„’max = 32ğ¾ 92.5 Eagle2.5-S2+Eagle2.5-S2, â„’max = 64ğ¾ Eagle2.5-S2+Eagle2.5-S2, â„’max = 128ğ¾ 93. 88.3 86.6 87.0 87.5 78.8 77.6 78.4 78.5 84.6 82.8 83.9 83.7 868 861 865 869 66.5 66.7 66.8 66.2 74.4 85.5 54.0 75.9 83.7 55.5 76.8 83.9 55.7 76.7 84.5 55. 85.5 84.8 85.2 85.5 57.3 53.4 63.6 55.4 63.3 55.2 62.9 54.7 65.1 68.3 67.3 67.8 74.8 75.3 75.6 75.7 Table 4: Impact of long-context data on performance of image benchmarks. Training & Data recipe MVBench MLVU Video-MME Val w/o subtitle - Recipe InfoVQA DocVQA TextVQA Perception_test MLVU Video-MME Val w/o subtitle Val Val Val Val S1S2 S1S1.5S2 (Open-Data+EV-110K) S1S1.5S2 (Image+Open-Data+EV-110K) 70.4 72.9 73.1 67.4 70.9 71.5 64.9 65.2 65. baseline w/o IAP w/o ADS 77.6 76.2 77.0 92.3 91.9 92.1 82.8 82.4 82.8 76.3 73.3 75.5 71.5 71.2 70. 65.4 64.9 65.0 Table 5: The impact of image data and pretraining on the performance of video benchmarks. S1/S1.5 denotes the stage-1 and stage-1.5 similar to Eagle2 Authors (2025). Table 6: The impact of information-first sampling on performance of image and video benchmarks. The baseline is equipped with IAP and ADS strategy. Eagle2.5-8B shows effective long-form video understanding, highlighting its robust visual reasoning using less parameters. Image benchmarks. As shown in Tab. 3, Eagle2.5-8B demonstrates competitive performance across diverse image understanding benchmarks. It achieves strong results on document understanding (94.1 on DocVQA Mathew et al. (2021)), chart interpretation (87.5 on ChartQA Masry et al. (2022)), and general information extraction (80.4 on InfoVQA Mathew et al. (2022), 83.7 on TextVQA Singh et al. (2019)). The model also performs well in optical character recognition with 869 on OCRBench Liu et al. (2024), comparable to other models in its category. Eagle2.5-8B shows balanced capabilities across multimodal general perception and reasoning tasks, scoring 66.2 on MMstar Chen et al. (2024), 76.7 on RWQA X.ai (2024), and 81.7 on MMB1.1 Liu et al. (2024), and 62.9 on MMVet Yu et al. (2023). Its performance extends to knowledge domain (55.8 on MMMU Yue et al. (2024), 84.5 on AI2D Hiippala et al. (2021)), visual hallucination benchmark (54.7 on HallB Guan et al. (2023)), and mathematical reasoning (67.8 on MathVista Lu et al. (2023)). Overall, Eagle2.5-8B achieves competitive 75.6 average score, demonstrating its effectiveness as versatile vision-language model that balances performance across various visual understanding tasks. 4.2. Ablation Studies In this section, we conduct experiments on various benchmarks to evaluate our method. We mainly design experiments to study the following questions. Q1: How do video and image data influence each others benchmarks? Tab. 4 studies the impact of long context data on the image benchmark performance. We compare the image benchmark performance without training with long-context data and with training long-context data under different â„’max. The results show that increasing the long-context data, under our training recipe, does not harm the short-context images and even slightly benefits it. To assess the impact of image data and pre-training on video benchmarks, we conduct comparison using the â„’max = 32ğ¾ model. For each benchmark, we sampled at 2FPS, ensuring maximum of 32 frames. As shown in Tab. 5, extensive image pre-training significantly enhances performance on short video benchmarks like MVBench, as well as on the relatively simple long video benchmark, MLVU. However, for the more challenging and held-out long video benchmark, Video-MME, the improvements are less pronounced. Q2: The effect of information-first sampling on performance? Tab. 6 illustrates the impact of the informationfirst sampling strategy on image and video tasks. Without the Image Area Preservation strategy, high-resolution image benchmarks like InfoVQA and fine-grained video benchmarks such as Perception-test suffer significant performance degradation. The effect on other benchmarks is less pronounced. While the Automatic Degradation 10 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Sampling strategy offers convenience for processing various visual inputs, experiments indicate that omitting it poses risk. The vision-context-centric strategy may truncate supervision signals, leading to performance loss. Training & Data recipe MVBench MLVU Video-MME Q3: The impact of different post-training schedules? Tab. 7 illustrates the performance impact of progressive mixed training from 32K to 64K compared to direct 64K mixed training on the video benchmarks. The results demonstrate that progressive training outperforms direct 64K mixed training, possibly due to two reasons: 1) Direct 64K hybrid training disperses samples across the 64K space, diluting the focus on shorter contexts. 2) Some longer samples are challenging to learn without gradual learning process that transitions from easy to difficult. Fig. 6 shows the effect of progressive mixed training on the Video-MME benchmark. It reveals that as progressive training advances, the models capacity to process more frames is gradually enhanced. Table 7: The impact of Eagle-Video-110K dataset and different post-training schedules on the performance of video benchmarks. 32K64K, Open-Data 64K, Open-Data 32K64K, Open-Data + Eagle-Video-110K Val w/o sbutitle 75. 74.0 71.3 73.0 74.5 73.9 68. 67.9 68.1 - Q4: The impact of Eagle-Video-110K data on performance? Finally, we examine the impact of Eagle-Video-110K on the models performance. Tab. 7 indicates that EagleVideo-110K enhances several mainstream long and short video benchmarks. Fig. 6 illustrates the effect of using EagleVideo-110K for training 64K context on Video-MME. Notably, it significantly boosts the models capability to handle numerous frames ( 128 frames) by focusing on the inclusion of long videos that were absent in the Open-Data training set. 5. Conclusion In this work, we introduce Eagle 2.5, family of advanced vision-language models for long-context multimodal understanding. Benefiting from the information-first sampling strategy, the progressive mixed post-training schedule, and the Eagle-Video-110K dataset with dual-level annotations, we boost the long-context understanding capabilitlies significantly, especially on video understanding. Our results show that Eagle 2.5 achieves state-of-the-art performance across benchmarks, including both long video comprehension and high-resolution image understanding. Notably, Eagle 2.5 is comparable to larger frontier models like GPT-4o and Gemini 1.5 Pro on video understanding despite having fewer parameters. With advanced training strategies and diverse data, Eagle 2.5 sets strong foundation for future research, paving the way for efficient and versatile VLMs in complex real-world scenarios. Figure 6: The impact of Eagle-Video-110K dataset and different post-training schedules on the performance of Video-MME. 11 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models A. Acknowledgments The team would like to thank the valuable discussions and input from Wei Ping, Zhuolin Yang, Wenliang Dai, Nayeon Lee, Boxin Wang, Karan Sapra, Amala Sanjay Deshmukh, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, Timo Roman, Jon Barker, Yao Lu, Hongxu Yin, Mike Ranzinger, Greg Heinrich, Xin Dong, Shizhe Diao, Hanrong Ye, Pavlo Molchanov, Song Han, Yi Dong, Hao Zhang, Xiaolong Li, Subhashree Radhakrishnan, Ratnesh Kumar, Sean Cha, Zaid Pervaiz Bha, Parthasarathy Sriram, Guanzhi Wang, Johan Bjorck, Linxi Fan, Yuke Zhu, Yan Wang, Min-Hung Chen, Ryo Hachiuma, Yao Xu, Osvald Nitski, Elena Lantz, Qing Miao, Ofri Masad, Ofer Baratz, Benedikt Schifferer, Mengyao Xu, Nave Algarici, Chintan Shah, Padmavathy Subramanian and Kari Briski. We would also like to thank the NVIDIA infrastructure team for their prompt and helpful assistance. B. Training and Inference B.1. Framework We integrate and develop multiple technologies to optimize long-context training framework, encompassing GPU memory optimization, distributed context parallelism, and video processing acceleration. GPU Memory Optimization. We integrate Triton-based fused operators replacing PyTorchs MLP, RMSNorm, and RoPE Su et al. (2024) implementations. We employ operators that fuse linear layers with cross-entropy loss to eliminate intermediate logit storage, and utilize CPU-offloading of hidden states to further reduce GPU memory usage. Distributed Context Parallelism. Building on USP Fang and Zhao (2024), we adopt two-layer communication group based on Ulysses and Ring Liu et al. (2023). Rather than using zigzag ring-attention, we implement zigzag Llama3-style Dubey et al. (2024) context parallelism with all-gather KV to reduce communication latency. Video Decoding Acceleration. Training often requires sampling specific sparse video frames, which can cause frame seek latency or memory management issues. We optimize this process through rapid video metadata parsing, improving long video decoding while minimizing memory consumption. Inference Acceleration. We deploy VLLM Kwon et al. (2023) for model serving and evaluation, significantly reducing memory requirements and accelerating inference speed. B.2. Training Settings To maximize resource utilization, we use the model weight from the Stage-1.5 training of Eagle-2 and extend long-context training in stage 2. Finally, our training pipeline is as follows Tab. 8. Vision Data Model Training Resolution Tokens Dataset #Samples Trainable Qwen2.5-7B Batch Size Learning Rate Max Length Eagle2.5-Stage-1 Eagle2.5-Stage-1.5 Eagle2.5-Stage-2 Eagle2.5-Stage-3 Eagle2.5-Stage448 {(ğ‘–, ğ‘—) ğ‘–, ğ‘— Z+, ğ‘– ğ‘— 12} (ğ‘– ğ‘— + 1) 256 ALLaVA 1.2M Rich Diverse Data 21.6M Short+Long Data 4.6M+4.6M Short+Long Data 4.6M+4.6M Short+Long Data 4.6M+4.6M MLP Connector 40.0M 1024 2104 4096 Full Model 8B 1024 8192 32768 2105 128 65536 128 128K Table 8: The proposed progressive training settings. C. Additional Benchmarks The performance of our model is also evaluated on the SlideVQA and MMLongBench-Doc benchmarks. The test results are shown in the Table 9 and 10: 12 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Subset ANLS Score Exact Match Score F1 Score Dev Test 73.8 72.7 67.7 63.2 74.7 72. Metric Overall F1 Overall Acc Score 29.4 27.7 Table 9: Performance on the SlideVQA benchmark. The ANLS Score refers to Approximate Normalized Levenshtein Similarity. Table 10: Performance on the MMLongBench-Doc. D. Training Data The training of Eagle2.5 is divided into multiple stages, including stage 1 for MLP alignment, and the pretraining stage 1.5, similar to Eagle-2 Authors (2025). It also includes the progressive long-context training proposed by Eagle 2.5. The training data used in these stages are as follows: Eagle2.5-Stage1: ALLaVA. Eagle2.5-Stage1.5: Table 11, including Eagle2.5-Image-SFT (Table 11a) and additional pretraining data (Table 11b) in Eagle2.5. Eagle2.5-Stage2: Mixture of shortand long-context data, including Eagle2.5-Image-SFT (Table 11a), Open-Data, and Eagle-Video-110K. Eagle2.5-Stage3: Mixture of shortand long-context data, including Eagle2.5-Image-SFT (Table 11a), Open-Data, and Eagle-Video-110K. Eagle2.5-Stage4: Mixture of shortand long-context data, including Eagle2.5-Image-SFT (Table 11a), Open-Data, and Eagle-Video-110K. E. Eagle-Video-110K E.1. Data Curation Prompts In this section, we will introduce the prompts utilized for curating dataset Eagle-Video-110K. They consist of prompts for generating captions and textual context anchors, prompts for generating clip-level QA, and prompts for generating video-level QA. E.1.1. Prompts for Generating Captions and Anchors You are an expert in understanding visual content in video clips. You are requested to create both brief and detailed captions for the current video clip titled \"{title}\". #### Guidelines For Brief Caption: - Create concise summary (15-30 words) that captures the essential action, setting, and participants - Focus on the most visually or narratively significant elements of the scene - Use clear, direct language - **IMPORTANT** Treat the video as complete clip rather than sequence of frames #### Guidelines For Detailed Caption: - Begin with thorough analysis of the visual content shown in the clip - **IMPORTANT** Pay special attention to the progression of actions and movements: * Break down complex actions into their component steps * Use transitional words (then, next, afterward, etc.) to show the flow of actions * Describe how one action leads to or connects with the next * Capture the natural sequence of movements and gestures - Note that while the clip may be shown as multiple frames, it should be described as continuous piece of footage 13 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models - For text that appears clearly in the clip: describe it in its original language and provide an English translation in parentheses). For example: [book in Chinese] [book]. Additionally, explain the meaning of the text within its context - **IMPORTANT** If any text is unclear, partially visible, or too blurry to read with confidence, simply mention the presence of text without attempting to specify its content - When referring to people, use their characteristics, such as clothing, to distinguish different people - **IMPORTANT** Please provide as many details as possible in your caption, including colors, shapes, and textures of objects, actions and characteristics of humans, as well as scenes and backgrounds - Consider how the visual content provides context and meaning Only output your response in the following format without any additional text, explanations or notes: json {\"Brief Caption\":\"concise summary of the video\", \"Detailed Caption\":\"The clip begins with..., progresses by..., and concludes with...\"} Here, the brief caption will be used for textual contextual anchors. E.1.2. Prompts for generating clip-level QA ### Task: You are tasked with generating question-answer pairs based on detailed video caption and given question categories. Try to create challenging questions when possible, but simpler questions are also acceptable when the details are limited. #### Input Information: The detailed caption of the video clip is: {caption} brief version of the caption is also provided: {brief_caption} #### Question Generation Guidelines: - Read the caption carefully - Generate question-answer pair for each category if possible - Make sure the question-answer pair cannot be fully answered using only the brief caption - Create two components for each pair: * question that is unambiguous within the current clip * An answer (can be word, phrase, or sentence) #### Question Categories: {question_type_pool} #### Important Criteria: - Questions should be unambiguous within the current clip - Create challenging questions when the details allow - Ensure answers require information beyond whats in the brief caption - Generate questions for as many categories as possible - Mark category as null only if: Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models * The question-answer would be fully answerable using just the brief caption * The category cannot be addressed using any information from either caption Only output your response in the following format without any additional text, explanations or notes: json {\"question_type_1\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null, \"question_type_2\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null, ..., \"question_type_n\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null} The question type pool\" is defined in E.2. And the question_type_x\", in (1, 2, ...n) are the types selected from the \"question type pool\". E.1.3. Prompts for generating generating video-level QA ### Task: You are tasked with generating question-answer pairs based on video caption and given question categories. Try to create challenging questions when possible, but simpler questions are also acceptable when the details are limited. #### Input Information: The caption of the video clip is: {caption} #### Question Generation Guidelines: - Read the caption carefully - Generate question-answer pair for each category if possible - Create two components for each pair: * question that is unambiguous within the current clip * An answer (can be word, phrase, or sentence) #### Question Categories: {question_type_pool} #### Important Criteria: - Questions should be unambiguous within the current clip - Create challenging questions when the details allow - Generate questions for as many categories as possible - Mark category as null only if: * The category cannot be addressed using any information from the caption Only output your response in the following format without any additional text, explanations or notes: json {\"question_type_1\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null, \"question_type_2\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null, ..., \"question_type_n\":{\"Q\":\"question specific to this video\",\"A\":\"answer\"} or null} 15 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Here, caption\" is composed of clip-level caption, and its format is: start_1 end_1: caption_1 start_2 end_2: caption_2 ... The start_x\" and end_x\", (x in 1, 2, ...) are in seconds. E.2. Question type pool As shown in table 12, we list the category names and category descriptions in the question type pool. We ask the model to generate qa pairs according to these categories. 16 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Category Dataset Captioning & Knowledge Mathematics Science Chart & Table Naive OCR OCR QA ShareGPT4o OpenGVLab (2024), KVQA Shah et al. (2019), Movie-Posters skvarre (2024), GoogleLandmark Weyand et al. (2020), WikiArt HugGAN (2024), Weather-QA Ma et al. (2024), Coco-Colors hazal karakus (2024), music-sheet EmileEsmaili (2024), SPARK Yu et al. (2024), Image-Textualization Pi et al. (2024), SAM-Caption PixArt-alpha (2024), Tmdb-Celeb-10k Ashraq (2024), PixMo Deitke et al. (2024) GeoQA+ Cao and Xiao (2022), MathQA Yu et al. (2023), CLEVR-Math/Super Li et al. (2023); LindstrÃ¶m and Abraham (2022), Geometry3K Lu et al. (2021), MAVIS-math-rule-geo Zhang et al. (2024), MAVIS-mathmetagen Zhang et al. (2024), InterGPS Lu et al. (2021), Raven Zhang et al. (2019), GEOS Seo et al. (2015), UniGeo Chen et al. (2022) AI2D Kembhavi et al. (2016), ScienceQA Lu et al. (2022), TQA Kembhavi et al. (2017), PathVQA He et al. (2020), SciQA Auer et al. (2023), Textbooks-QA, VQA-RAD Lau et al. (2018), VisualWebInstruct TIGER-Lab (2024) ChartQA Masry et al. (2022), MMC-Inst Liu et al. (2023), DVQA Kafle et al. (2018), PlotQA Methani et al. (2020), LRV-Instruction Liu et al. (2023), TabMWP Lu et al. (2022), UniChart Masry et al. (2023), Vistext Tang et al. (2023), TAT-DQA Zhu et al. (2022), VQAonBD VQAonDB, FigureQA Kahou et al. (2017), Chart2Text Kantharaj et al. (2022), RobuT-{Wikisql, SQA, WTQ} Zhao et al. (2023), MultiHiertt Zhao et al. (2022) SynthDoG Kim et al. (2022), MTWI He et al. (2018), LVST Sun et al. (2019), SROIE Huang et al. (2019), FUNSD Jaume et al. (2019), Latex-Formula OleehyO (2024), IAM Marti and Bunke (2002), HandwritingLatex aidapearson (2023), ArT Chng et al. (2019), CTW Yuan et al. (2019), ReCTs Zhang et al. (2019), COCOText Veit et al. (2016), SVRD Yu et al. (2023), Hiertext Long et al. (2023), RoadText Tom et al. (2023), MapText Li et al. (2024), CAPTCHA parasam (2024), Est-VQA Wang et al. (2020), HME-100K TAL (2023), TAL-OCR-ENG TAL (2023), TAL-HW-MATH TAL (2023), IMGUR5K Krishnan et al. (2023), ORAND-CAR Diem et al. (2014), Invoicesand-Receipts-OCR mychen76 (2024), Chrome-Writting MouchÃ¨re et al. (2016), IIIT5k Mishra et al. (2012), K12-Printing TAL (2023), Memotion Ramamoorthy et al. (2022), Arxiv2Markdown, Handwritten-MathematicalExpression Azu (2023), WordArt Xie et al. (2022), RenderedText wendlerc (2024), Handwriting-Forms ift (2024) DocVQA Clark and Gardner (2018), InfoVQA Mathew et al. (2022), TextVQA Singh et al. (2019), ArxivQA Li et al. (2024), ScreencQA Hsiao et al. (2022), DocReason mPLUG (2024), Ureader Ye et al. (2023), FinanceQA Sujet AI (2024), DocMatrix LaurenÃ§on et al. (2024), A-OKVQA Schwenk et al. (2022), Diagram-Image-To-Text Kamizuru00 (2024), MapQA Chang et al. (2022), OCRVQA Mishra et al. (2019), ST-VQA Biten et al. (2019), SlideVQA Tanaka et al. (2023), PDF-VQA Ding et al. (2023), SQuAD-VQA, VQA-CD Mahamoud et al. (2024), Block-Diagram shreyanshu09 (2024), MTVQA Tang et al. (2024), ColPali Faysse et al. (2024), BenthamQA Mathew et al. (2021) Grounding & Counting TallyQA Acharya et al. (2019), OODVQA Tu et al. (2023), RefCOCO/+/g (en) Mao et al. (2016); Yu et al. (2016), GroundUI Zheng et al. (2024) General VQA Text-only LLaVA-150K Liu et al. (2023), LVIS-Instruct4V Wang et al. (2023), ALLaVA Chen et al. (2024), Laion-GPT4V LAION (2023), LLAVAR Zhang et al. (2023), SketchyVQA Tu et al. (2023), OminiAlign-V Zhao et al. (2025), VizWiz Gurari et al. (2018), IDK Cha et al. (2024), AlfworldGPT, LNQA Pont-Tuset et al. (2020), Face-Emotion FastJobs (2024), SpatialSense Yang et al. (2019), Indoor-QA keremberke (2024), Places365 Zhou et al. (2017), MMinstruct Liu et al. (2024), DriveLM Sima et al. (2023), YesBut Nandy et al. (2024), WildVision Lu et al. (2024), LLaVACritic-113k Xiong et al. (2024), RLAIF-V Yu et al. (2024), VQAv2 Goyal et al. (2017), MMRA Wu et al. (2024), KONIQ Hosu et al. (2020), MMDU Liu et al. (2024), Spot-The-Diff Jhamtani and Berg-Kirkpatrick (2018), Hateful-Memes Kiela et al. (2020), COCO-QA Ren et al. (2015), NLVR Suhr et al. (2017), Mimic-CGD LaurenÃ§on et al. (2024), Datikz Belouadi et al. (2023), Chinese-Meme Contributors (2024), IconQA Lu et al. (2021), Websight LaurenÃ§on et al. (2024) Orca Lian et al. (2023), Orca-Math Mitra et al. (2024), OpenCodeInterpreter Zheng et al. (2024) MathInstruct Yue et al. (2023), WizardLM Xu et al. (2023), TheoremQA Chen et al. (2023), OpenHermes2.5 Teknium (2023), NuminaMath-CoT LI et al. (2024), Python-Code-25k flytech (2024), Infinity-Instruct BAAI (2024), PythonCode-Instructions-18k-Alpaca iamtarun (2024), Ruozhiba LooksJuicy (2024), InfinityMATH Zhang et al. (2024), StepDPO Lai et al. (2024), TableLLM Zhang et al. (2024), UltraInteract-sft Yuan et al. (2024) (a) Summary of the collected Eagle 2.5 SFT datasets Category Dataset Captioning & Knowledge CC3M Sharma et al. (2018), TextCaps Sidorov et al. (2020), ShareGPT-4V Chen et al. (2023), DenseFusion-1M Li et al. (2024) Grounding & Counting Object 365 Shao et al. (2019) Text-only OpenMathInstruct Toshniwal et al. (2024) (b) Summary of the additional Stage 1.5 datasets Table 11: Dataset used in Eagle 2.5 for Stage 1 and Stage1.5. Dataset in Magenta is internal data. 17 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Index 1 2 3 4 5 6 7 8 9 10 11 Category object_recognition object_properties object_count object_state object_location object_presence human_attributes human_pose human_appearance human_identity human_cognitive_process 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 human_location human_emotion scene_description text_recognition text_count text_location single_object_event_recognition single_object_event_count single_object_state_change single_object_quantity_change single_object_location_change single_object_trajectory single_object_speed single_object_presence_change human_object_interaction_recognition human_object_interaction_count human_human_interaction_recognition object_interaction abnormal_event_detection domain_medical domain_education domain_sports domain_movies domain_gaming domain_technology domain_arts video_editing_effects camera_movement spatial_relationship property_comparison quantity_comparison state_comparison human_object_relationship human_human_relationship scene_sequence event_sequence event_causality counterfactual_reasoning trajectory_tracking speed_comparison event_prediction anomaly_reasoning planning navigation human_action dialogue_content event_summary object_ordering event_location process_description video_topic anomaly_recognition Description Questions about what an object is Questions about object properties, such as color, shape, material, texture Questions about the number of objects Questions about object states, such as stretched, compressed, cut, stationary Questions about where an object is located Questions about object existence Questions about human attributes, such as height, body type, build Questions about human posture Questions about human external appearance, such as clothing and makeup Questions about human identity Questions about human mental processes, including intentions, motivations, decision-making rationale, problem-solving approaches, and reasoning methods Questions about human location Questions about human emotional state Questions about overall scene description Questions about text content appearing in the video Questions about frequency of text appearances in the video Questions about location of text in the video Questions about events involving single object Questions about frequency of single-object events Questions about changes in single object state Questions about changes in single object quantity Questions about changes in single object location Questions about single object motion trajectory Questions about single object movement speed Questions about changes in single object presence Questions about types of human-object interaction Questions about frequency of human-object interactions Questions about types of human-human interaction Questions about objects states, actions, interactions, changes, identifications (including brands), and how objects affect or interact with other objects Questions about presence of abnormal events Questions about medical-related professional knowledge Questions about education-related professional knowledge Questions about sports-related professional knowledge Questions about movie-related professional knowledge Questions about gaming-related professional knowledge Questions about technology-related professional knowledge Questions about arts-related professional knowledge Questions about video editing effects, including shot transitions, editing effects, transition effects, etc. Questions about camera motion Questions about spatial relationships between objects Questions about comparison of multiple object properties Questions about comparison of multiple object quantities Questions about comparison of multiple object states Questions about human-object relationships Questions about human-human relationships Questions about temporal relationships between scenes Questions about temporal relationships between events Questions about causal relationships between events, including both human-initiated actions and their consequences, as well as cause-effect relationships in natural or systematic processes Questions about counterfactual reasoning Questions about tracking object or human positions Questions about speed comparison between multiple objects or humans Questions about future event prediction Questions about causes of anomalous phenomena Questions about planning for specific tasks Questions about navigation to destinations Questions about actions, behaviors, movements or activities performed by humans, including analysis of techniques, efficiency, and patterns of behavior Questions about spoken dialogue, verbal content, or conversations between characters/people Questions about overall event summary Questions about the sequence or order in which objects are placed, arranged, or handled by individuals Questions about where events or activities take place Questions about identifying key components, steps, or progression in process involving objects and/or humans Questions about the main subject, focus, or theme covered in the video Questions about identifying and interpreting anomalies Table 12: Question type categories and their descriptions 18 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models References [1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. TallyQA: Answering complex counting questions. In AAAI, 2019. 17 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv:2303.08774, 2023. 3, 8 [3] aidapearson. Aida calculus math handwriting recognition dataset. https://www.kaggle.com/ datasets/aidapearson/ocr-data, 2023. [4] Anthropic. The claude 3 model https://www.anthropic. com, 2024. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. 6, 9 family: Opus, sonnet, haiku. [5] Ashraq. TMDB-Celeb-10K Dataset. https://huggingface.co/datasets/ashraq/tmdb-celeb-10k, 2024. [6] SÃ¶ren Auer, Dante AC Barone, Cassiano Bartz, Eduardo Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, et al. The sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13(1):7240, 2023. 17 [7] Anonymous Authors. Eagle 2: Building post-training data strategies from scratch for frontier visionlanguage models. In Submission, 2025. 6, 10, 13 [8] Azu. Handwritten-mathematical-expression-convert-latex. https://huggingface.co/datasets/Azu/ Handwritten-Mathematical-Expression-Convert-LaTeX, 2023. 17 [9] BAAI. Infinity-instruct dataset. https://huggingface.co/datasets/BAAI/Infinity-Instruct, 2024. 17 [10] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv:2308.12966, 2023. 1 [11] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv:2502.13923, 2025. 2, 9 [12] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models. arXiv:2401.18058, 2024. [13] Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 7 [14] Jonas Belouadi, Anne Lauscher, and Steffen Eger. Automatikz: Text-guided synthesis of scientific vector graphics with tikz. arXiv:2310.00367, 2023. 17 [15] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, MarÃ§al Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, 2019. [16] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In CVPR, 2015. 7 [17] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In COLING, 2022. 17 19 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [18] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In CVPR, 2017. 7 [19] Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong Yang. Visually dehallucinative instruction generation. In ICASSP, 2024. 17 [20] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, CristÃ³bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. In NeurIPS, 2025. 9 [21] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv:2211.08545, 2022. 17 [22] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. ALLaVA: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv:2402.11684, 2024. 17 [23] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding. arXiv:2412.12075, 2024. 9 [24] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv:2212.02746, 2022. 17 [25] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improving large multi-modal models with better captions. arXiv:2311.12793, 2023. 17 [26] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. [27] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv:2406.04325, 2024. 3 [28] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In CVPR, 2024. 7 [29] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In EMNLP, 2023. 17 [30] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv:2408.10188, 2024. 9 [31] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv:2312.14238, 2023. 1, [32] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv:2412.05271, 2024. 9 20 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [33] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. ICDAR2019 robust reading challenge on arbitrary-shaped text (RRC-ArT). In ICDAR, 2019. 17 [34] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In ACL, 2018. [35] LLM-Red-Team Contributors. emo-visual-data: Emotion and visual data analysis project. https: //github.com/LLM-Red-Team/emo-visual-data, 2024. 17 [36] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. NVLM: Open frontier-class multimodal llms. arXiv:2409.11402, 2024. 3 [37] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv:2409.17146, 2024. 17 [38] Markus Diem, Stefan Fiel, Florian Kleber, Robert Sablatnig, Jose Saavedra, David Contreras, Juan Manuel Barrios, and Luiz Oliveira. Icfhr 2014 competition on handwritten digit string recognition in challenging datasets (hdsrc 2014). In International Conference on Frontiers in Handwriting Recognition, 2014. 17 [39] Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren Han. VQA: new dataset for real-world vqa on pdf documents. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2023. [40] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv:2407.21783, 2024. 3, 9, 12 [41] EmileEsmaili. sheet music clean ataset. https://huggingface.co/datasets/EmileEsmaili/sheet_ music_clean, 2024. 17 [42] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! predicting unintentional action in video. In CVPR, 2020. 7 [43] Jiarui Fang and Shangchun Zhao. Usp: unified sequence parallelism approach for long context generative ai. arXiv:2405.07719, 2024. 12 [44] Miquel FarrÃ©, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https://huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. 7 [45] FastJobs. Visual emotional analysis dataset. https://huggingface.co/datasets/FastJobs/Visual_ Emotional_Analysis, 2024. 17 [46] Manuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. arXiv:2407.01449, 2024. 17 [47] flytech. Python codes 25k dataset. https://huggingface.co/datasets/flytech/python-codes-25k, 2024. 17 [48] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. 7, 9 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [49] Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Long story short: Story-level video understanding from 20k short films, 2025. URL https://arxiv.org/abs/2406.10221. 3 [50] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In ICCV, 2017. 7 [51] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. [52] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. 7 [53] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. HallusionBench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv:2310.14566, 2023. 10 [54] Deepak Gupta, Kush Attal, and Dina Demner-Fushman. dataset for medical instructional video classification and question answering. Scientific Data, 10(1):158, 2023. 7 [55] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. VizWiz Grand Challenge: Answering visual questions from blind people. In CVPR, 2018. 17 [56] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv:2312.10300, 2023. 3, 7, 8 [57] Mingfei Han, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang, and Heng Wang. Video recognition in portrait mode. In CVPR, 2024. 7 [58] hazal karakus. mscoco-controlnet-canny-less-colors dataset. https://huggingface.co/datasets/ hazal-karakus/mscoco-controlnet-canny-less-colors, 2024. 17 [59] Mengchao He, Yuliang Liu, Zhibo Yang, Sheng Zhang, Canjie Luo, Feiyu Gao, Qi Zheng, Yongpan Wang, Xin Zhang, and Lianwen Jin. ICPR2018 contest on robust reading for multi-type web images. In ICPR, 2018. [60] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. PathVQA: 30000+ questions for medical visual question answering. arXiv:2003.10286, 2020. 7, 17 [61] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. 10 [62] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:40414056, 2020. 17 [63] Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Victor Carbune, Jason Lin, Maria Wang, Srinivas Sunkara, Yun Zhu, and Jindong Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots. arXiv:2209.08199, 2022. 17 [64] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. LITA: Language instructed temporal-localization assistant. In ECCV, 2024. 7 22 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [65] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. CoRR, abs/2011.11760, 2020. 7, 8 [66] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In ECCV, 2020. 3 [67] Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, et al. Egoexolearn: dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world. In CVPR, 2024. 7 [68] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. ICDAR 2019 robust reading challenge on scanned receipts ocr and information extraction. In ICDAR, 2019. 17 [69] HugGAN. WikiArt Dataset. https://huggingface.co/datasets/huggan/wikiart, 2024. 17 [70] iamtarun. Python code instructions 18k alpaca dataset. https://huggingface.co/datasets/ iamtarun/python_code_instructions_18k_alpaca, 2024. 17 [71] IDEFICS. Introducing IDEFICS: An open reproduction of state-of-the-art visual language model. https: //huggingface.co/blog/idefics, 2023. 3 [72] ift. Handwriting forms dataset. https://huggingface.co/datasets/ift/handwriting_forms, 2024. [73] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. In CVPR, 2024. 7 [74] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. FUNSD: dataset for form understanding in noisy scanned documents. In ICDAR Workshops, 2019. 17 [75] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. arXiv:1808.10584, 2018. [76] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human tasks in egocentric videos. In NeurIPS, 2022. 7 [77] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, 2024. 1, 3 [78] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. In NeurIPS, 2025. 7 [79] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. DVQA: Understanding data visualizations via question answering. In CVPR, 2018. [80] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ãkos KÃ¡dÃ¡r, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv:1710.07300, 2017. 17 [81] Kamizuru00. Diagram image to text dataset. https://huggingface.co/datasets/Kamizuru00/ diagram_image_to_text, 2024. 17 [82] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. arXiv:2203.06486, 2022. 17 23 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [83] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 17 [84] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017. 17 [85] keremberke. Indoor scene classification dataset. https://huggingface.co/datasets/keremberke/ indoor-scene-classification, 2024. 17 [86] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In NeurIPS, 2020. [87] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. OCR-free document understanding transformer. In ECCV, 2022. 17 [88] Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, and Federico Tombari. Text-conditioned resampler for long form video understanding. In ECCV, 2024. 1, 3 [89] Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vassilev, and Tal Hassner. Textstylebrush: transfer of text aesthetics from single example. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):91229134, 2023. 17 [90] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. 2014. [91] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 12 [92] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv:2406.18629, 2024. 17 [93] LAION. gpt4v-dataset. https://huggingface.co/datasets/laion/gpt4v-dataset, 2023. 17 [94] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 17 [95] Hugo LaurenÃ§on, AndrÃ©s Marafioti, Victor Sanh, and LÃ©o Tronchon. Building and better understanding vision-language models: insights and future directions. arXiv:2408.12637, 2024. 17 [96] Hugo LaurenÃ§on, LÃ©o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building visionlanguage models? arXiv:2405.02246, 2024. 17 [97] Hugo LaurenÃ§on, LÃ©o Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv:2403.09029, 2024. 17 [98] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. arXiv:1809.01696, 2018. 7 [99] Jie Lei, Tamara L. Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. 2021. 7 24 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [100] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. arXiv:2408.03326, 2024. 3, [101] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. 3 [102] Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, and Chen Chen. Sports-qa: large-scale video question answering benchmark for complex and professional sports. arXiv:2401.01505, 2024. 7 [103] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https:// github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. 17 [104] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In ICML, 2023. 1, 3 [105] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. VideoChat: Chat-centric video understanding. arXiv:2305.06355, 2023. 1, 3 [106] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 9 [107] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv:2403.00231, 2024. 17 [108] Pengyi Li, Irina Abdullaeva, Alexander Gambashidze, Andrey Kuznetsov, and Ivan Oseledets. Maxinfo: training-free key-frame selection method using maximum volume for enhanced video understanding. arXiv:2502.03183, 2025. 3 [109] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv:2407.08303, 2024. [110] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 9 [111] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV. Springer, 2024. 1, 3 [112] Zekun Li, Yijun Lin, Yao-Yi Chiang, Jerod Weinman, Solenn Tual, Joseph Chazalon, Julien Perret, Bertrand DumÃ©nieu, and Nathalie Abadie. ICDAR 2024 competition on historical map text detection, recognition, and linking. In ICDAR, 2024. 17 [113] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-CLEVR: virtual benchmark to diagnose domain robustness in visual reasoning. In CVPR, 2023. 17 [114] Lian, Goodson, Pentland, et al. OpenOrca: An open dataset of gpt augmented flan reasoning traces, 2023. 17 25 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [115] Adam Dahlgren LindstrÃ¶m and Savitha Sam Abraham. CLEVR-Math: dataset for compositional language, visual and mathematical reasoning. arXiv:2208.05358, 2022. 17 [116] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multimodal model with robust instruction tuning. arXiv:2306.14565, 2023. 17 [117] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv:2311.10774, 2023. 17 [118] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv:2310.01889, 2023. 12 [119] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 3, 4, [120] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 4 [121] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVANeXT: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github. io/blog/2024-01-30-llava-next/. 3 [122] Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, et al. Mminstruct: high-quality multi-modal instruction tuning dataset with extensive diversity. arXiv:2407.15838, 2024. 17 [123] Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao. Fineaction: fine-grained video dataset for temporal action localization. IEEE transactions on image processing, 31:69376950, 2022. [124] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. 10 [125] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 10 [126] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. NVILA: Efficient frontier visual language models. In CVPR, 2025. 3 [127] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv:2406.11833, 2024. 17 [128] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis. ICDAR 2023 competition on hierarchical text detection and recognition. In ICDAR, 2023. [129] LooksJuicy. Ruozhiba dataset. https://huggingface.co/datasets/LooksJuicy/ruozhiba, 2024. 17 [130] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv:2105.04165, 2021. 17 26 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [131] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv:2105.04165, 2021. 17 [132] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. IconQA: new benchmark for abstract diagram understanding and visual language reasoning. arXiv:2110.13214, 2021. [133] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 17 [134] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv:2209.14610, 2022. 17 [135] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv:2310.02255, 2023. 10 [136] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv:2406.11069, 2024. 17 [137] Chengqian Ma, Zhanxiang Hua, Alexandra Anderson-Frey, Vikram Iyer, Xin Liu, and Lianhui Qin. WeatherQA: Can multimodal language models reason about severe weather? arXiv:2406.11217, 2024. 17 [138] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv:2406.09418, 2024. 7 [139] Ibrahim Souleiman Mahamoud, MickaÃ«l Coustaty, AurÃ©lie Joseph, Vincent Poulain dAndecy, and JeanMarc Ogier. CHIC: Corporate document for visual question answering. In ICDAR, 2024. 17 [140] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 17 [141] U-V Marti and Horst Bunke. The iam-database: an english sentence database for offline handwriting recognition. International journal on document analysis and recognition, 5:3946, 2002. 17 [142] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. 10, 17 [143] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. UniChart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv:2305.14761, 2023. 17 [144] Minesh Mathew, Lluis Gomez, Dimosthenis Karatzas, and CV Jawahar. Asking questions on handwritten document collections. International Journal on Document Analysis and Recognition, 24(3):235249, 2021. 17 [145] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: dataset for vqa on document images. In WACV, 2021. 10 [146] Minesh Mathew, Viraj Bagal, RubÃ¨n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. InfographicVQA. In WACV, 2022. 10, 17 27 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [147] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. PlotQA: Reasoning over scientific plots. In WACV, 2020. 17 [148] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In ICCV, 2019. 3 [149] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text recognition using higher order language priors. In BMVC, 2012. 17 [150] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual question answering by reading text in images. In ICDAR, 2019. [151] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-Math: Unlocking the potential of slms in grade school math. arXiv:2402.14830, 2024. 17 [152] Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva. Spoken moments: Learning joint audio-visual representations from video descriptions. In CVPR, 2021. 7 [153] Harold MouchÃ¨re, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr2016 crohme: Competition on recognition of online handwritten mathematical expressions. In International Conference on Frontiers in Handwriting Recognition, 2016. 17 [154] mPLUG. Docreason25k dataset. https://huggingface.co/datasets/mPLUG/DocReason25K, 2024. [155] mychen76. Invoices and receipts ocr v1 dataset. https://huggingface.co/datasets/mychen76/ invoices-and-receipts_ocr_v1, 2024. 17 [156] Abhilash Nandy, Yash Agarwal, Ashish Patwa, Millon Madhur Das, Aman Bansal, Ankit Raj, Pawan Goyal, and Niloy Ganguly. Yesbut: high-quality annotated multimodal dataset for evaluating satire comprehension capability of vision-language models. arXiv:2409.13592, 2024. 17 [157] OleehyO. Latex formulas dataset. https://huggingface.co/datasets/OleehyO/latex-formulas, 2024. [158] Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video dataset with high-quality text and audio narrations. In ICASSP, 2021. 7 [159] OpenAI. Gpt-4v(ision) system card, 2023. URL https://openai.com/index/gpt-4v-system-card/. 3, 6 [160] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2023. Accessed: 2024-11-12. 2, 3, 6, 8, [161] OpenGVLab. ShareGPT-4o dataset. https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o, 2024. 17 [162] parasam. Captcha Dataset. https://www.kaggle.com/datasets/parsasam/captcha-dataset, 2024. 17 [163] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. 9 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [164] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. 7 [165] Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. Image Textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv:2406.07502, 2024. 17 [166] PixArt-alpha. SAM-LLaVA-Captions10M Dataset. https://huggingface.co/datasets/PixArt-alpha/ SAM-LLaVA-Captions10M, 2024. 17 [167] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. 17 [168] Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. Spiqa: dataset for multimodal question answering on scientific papers. In NeurIPS, 2025. 3 [169] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML. PmLR, 2021. 7 [170] Sathyanarayanan Ramamoorthy, Nethra Gunti, Shreyash Mishra, Suryavardan, Aishwarya Reganti, Parth Patwa, Amitava DaS, Tanmoy Chakraborty, Amit Sheth, Asif Ekbal, et al. Memotion 2: Dataset on sentiment and emotion analysis of memes. In Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection, CEUR, 2022. 17 [171] Ruchit Rawal, Khalid Saifullah, Miquel FarrÃ©, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv:2405.08813, 2024. 3, [172] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. 3, 6, 9 [173] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering. In NeurIPS, 2015. 17 [174] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: benchmark for visual question answering using world knowledge. In ECCV, 2022. 17 [175] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In EMNLP, 2015. [176] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. KVQA: Knowledge-aware visual question answering. In AAAI, 2019. 17 [177] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, 2019. 17 [178] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. [179] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv:2410.17434, 2024. 1, 3 29 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [180] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, et al. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuray. arXiv:2502.05177, 2025. 2, 3 [181] shreyanshu09. Block diagram dataset. https://huggingface.co/datasets/shreyanshu09/Block_ Diagram, 2024. [182] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. TextCaps: dataset for image captioning with reading comprehension. In ECCV, 2020. 17 [183] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens BeiÃŸwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. arXiv:2312.14150, 2023. 17 [184] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In CVPR, 2019. 17 [185] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 10 [186] skvarre. Movie posters-100k dataset. https://huggingface.co/datasets/skvarre/movie_ posters-100k, 2024. 17 [187] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, 2024. 3 [188] Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing long video understanding with ai-generated movies. arXiv:2403.01422, 2024. 3 [189] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 12 [190] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual reasoning. In ACL, 2017. 17 [191] Hamed Rahimi Sujet AI, Allaa Boutaleb. for financial document vqa, 2024. Sujet-Finance-QA-Vision-100k. 17 Sujet-finance-qa-vision-100k: large-scale dataset URL https://huggingface.co/datasets/sujet-ai/ [192] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. ICDAR 2019 competition on large-scale street view text with partial labeling RRC-LSVT. In ICDAR, 2019. 17 [193] TAL. TAL open dataset. https://ai.100tal.com/dataset, 2023. 17 [194] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In AAAI, 2023. 3, 6, 7, 17 [195] Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv:2307.05356, 2023. 17 [196] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv:2405.11985, 2024. 17 30 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [197] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019. 6, 7 [198] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [199] OpenGVLab Team. Internvl2: Better than the bestexpanding performance boundaries of opensource multimodal models with the progressive scaling strategy. https://internvl.github.io/blog/ 2024-07-02-InternVL-2.0/, 2024. 3, 4, 9 [200] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. 4 [201] Teknium. OpenHermes 2.5: An open dataset of synthetic data for generalist llm assistants. https: //huggingface.co/datasets/teknium/OpenHermes-2.5, 2023. [202] TIGER-Lab. VisualWebInstruct Dataset. https://huggingface.co/datasets/TIGER-Lab/ VisualWebInstruct, 2024. 17 [203] RubÃ¨n Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. 3, [204] George Tom, Minesh Mathew, Sergi Garcia-Bordils, Dimosthenis Karatzas, and CV Jawahar. ICDAR 2023 competition on roadtext video text detection, tracking and recognition. In ICDAR, 2023. 17 [205] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv:2406.16860, 2024. 3 [206] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. OpenMathInstruct-1: 1.8 million math instruction tuning dataset. arXiv:2402.10176, 2024. 17 [207] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv:2311.16101, 2023. 17 [208] Jordy Van Landeghem, RubÃ¨n Tito, Åukasz Borchmann, MichaÅ‚ Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, MickaÃ«l Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In ICCV, 2023. 3, 7 [209] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. COCO-Text: Dataset and benchmark for text detection and recognition in natural images. arXiv:1601.07140, 2016. [210] VQAonDB. Vqaondb dataset. https://ilocr.iiit.ac.in/vqabd/. 17 [211] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv:2311.07574, 2023. 17 [212] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024. 1, 3, 9 [213] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and bilingual scene-text visual question answering. In CVPR, 2020. 17 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [214] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv:2401.10529, 2024. 7 [215] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al. InternVid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2024. 7 [216] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. InternVideo2: Scaling video foundation models for multimodal video understanding. arXiv:2403.15377, 2024. 7 [217] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurrent memory bridges. arXiv:2409.01071, 2024. [218] wendlerc. Renderedtext dataset. https://huggingface.co/datasets/wendlerc/RenderedText, 2024. 17 [219] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In ECCV. Springer, 2024. 1, 3 [220] Tobias Weyand, AndrÃ© Araujo, Bingyi Cao, and Jack Sim. Google Landmarks Dataset v2 - Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In CVPR, 2020. [221] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. STAR: benchmark for situated reasoning in real-world videos. CoRR, abs/2405.09711, 2024. 7 [222] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In CVPR, 2021. 3 [223] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2025. 9 [224] Siwei Wu, Kang Zhu, Yu Bai, Yiming Liang, Yizhi Li, Haoning Wu, Jiaheng Liu, Ruibo Liu, Xingwei Qu, Xuxin Cheng, et al. Mmra: benchmark for multi-granularity multi-image relational association. arXiv:2407.17379, 2024. [225] X.ai. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v, 2024. 10 [226] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. 2021. 7 [227] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. In ECCV, 2024. 7 [228] Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, and Xiang Bai. Toward understanding wordart: Corner-guided transformer for scene text recognition. In ECCV. Springer, 2022. 17 [229] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv:2410.02712, 2024. 17 [230] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv:2304.12244, 2023. 17 [231] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv:2408.10188, 2024. 2, 3 32 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [232] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. In NeurIPS, 2023. 7, 8 [233] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. In NeurIPS, 2025. 7 [234] Kaiyu Yang, Olga Russakovsky, and Jia Deng. Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition. In ICCV, 2019. [235] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv:2408.01800, 2024. 9 [236] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv:2310.05126, 2023. 17 [237] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. CLEVRER: Collision events for video representation and reasoning. In ICLR, 2020. 7 [238] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. 17 [239] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv:2309.12284, 2023. 17 [240] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. In NeurIPS, 2023. 3 [241] Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, et al. Frame-voyager: Learning to query frames for video large language models. arXiv:2410.03226, 2024. 1, 3 [242] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv:2405.17220, 2024. [243] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv:2308.02490, 2023. 10 [244] Wenwen Yu, Chengquan Zhang, Haoyu Cao, Wei Hua, Bohan Li, Huang Chen, Mingyu Liu, Mingrui Chen, Jianfeng Kuang, Mengjun Cheng, et al. ICDAR 2023 competition on structured text extraction from visually-rich document images. In ICDAR, 2023. 17 [245] Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, and Yong Man Ro. SPARK: Multi-vision sensor perception and reasoning benchmark for large-scale vision-language models. arXiv:2408.12114, 2024. 17 [246] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv:2404.02078, 2024. 17 [247] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, and Shi-Min Hu. large chinese text dataset in the wild. Journal of Computer Science and Technology, 34, 2019. 33 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [248] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv:2309.05653, 2023. 17 [249] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 10 [250] Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, and Qin Jin. Movie101: new movie understanding benchmark. arXiv:2305.12140, 2023. [251] Zihao Yue, Yepeng Zhang, Ziheng Wang, and Qin Jin. Movie101v2: Improved movie narration benchmark. arXiv:2404.13370, 2024. 3 [252] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In CVPR, 2023. 7 [253] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [254] Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu. Infinitymath: scalable instruction tuning dataset in programmatic mathematical reasoning. In ACM International Conference on Information and Knowledge Management, 2024. 17 [255] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: dataset for Relational and Analogical Visual rEasoNing. In CVPR, 2019. 17 [256] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv:2406.08085, 2024. 3 [257] Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. Longreward: Improving long-context large language models with ai feedback. arXiv:2410.21252, 2024. [258] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv:2406.16852, 2024. 2, 3 [259] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. MAVIS: Mathematical visual instruction tuning. arXiv:2407.08739, 2024. 17 [260] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. ICDAR 2019 robust reading challenge on reading chinese text on signboard. In ICDAR, 2019. 17 [261] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv:2404.01258, 2024. 7 [262] Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, et al. Tablellm: Enabling tabular data manipulation by llms in real office usage scenarios. arXiv:2403.19318, 2024. 17 [263] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: Enhanced visual instruction tuning for text-rich image understanding. arXiv:2306.17107, 2023. 17 34 Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [264] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024. 6, 7 [265] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024. 3, 9 [266] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV, 2019. 7 [267] Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, et al. Omnialign-v: Towards enhanced alignment of mllms with human preference. arXiv preprint arXiv:2502.18411, 2025. 17 [268] Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv:2206.01347, 2022. [269] Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and Dragomir Radev. Robut: systematic study of table qa robustness against human-annotated adversarial perturbations. arXiv:2306.14321, 2023. 17 [270] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: toolkit for building general virtual agents. arXiv:2403.17918, 2024. 17 [271] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating code generation with execution and refinement. arXiv:2402.14658, 2024. 17 [272] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40 (6):14521464, 2017. 17 [273] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv:2406.04264, 2024. [274] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. 7 [275] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In ICLR, 2024. 3 [276] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In ACMMM, 2022."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Nanjing University",
        "Rutgers University",
        "The Hong Kong Polytechnic University"
    ]
}