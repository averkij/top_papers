{
    "paper_title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
    "authors": [
        "Jooyeol Yun",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics."
        },
        {
            "title": "Start",
            "content": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure Jooyeol Yun,"
        },
        {
            "title": "Jaegul Choo",
            "content": "KAIST AI {blizzard072, jchoo}@kaist.ac.kr 5 2 0 2 6 1 ] . [ 1 6 3 3 4 1 . 2 1 5 2 : r I want the emoji to look to the left and right. want the elements smoothly pop up in lively manner. want the compass needle to quickly spin around once. want the buttons to bounce in one by one. Figure 1. Animations generated by Vector Prism. Please view them in Adobe Acrobat or the Firefox browser for the best experience. An HTML version is available in the project page."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for visionlanguage models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics. Scalable Vector Graphics (SVG) has become increasingly central to modern web experiences, prized for its portability across devices and infinite scalability without quality loss. This popularity is driven by their vector-based design, which describes graphics through geometric primitives rather than pixels, resulting in compact and resolutionindependent files. As modern web interfaces evolve tothe demand ward dynamic and interactive experiences, for expressive animation techniques has become essential, since SVG animations can deliver rich visual motion where videos would be prohibitively heavy for web delivery. Recent advances in vision-language models (VLMs) [16, 20, 37] offer tempting possibility, which is generating animations simply by instructing VLM given the SVG file. At first glance, this seems to be straightforward, since modern vision-language models can already plan animation sequences [32] and generate code [7, 39]. In practice, VLMgenerated SVG animations rarely succeed, often resulting in visually broken animations. The problem lies not in the planning or coding capabilities, but in how SVGs are structured, as SVGs are optimized for rendering efficiency rather than semantic clarity. For example, as seen in Figure 2, visually coherent elements (e.g., bunny ears and nose) are 1 often fragmented or grouped by draw order, obscuring the higher-level semantics needed for animation. In this paper, we address the overlooked step of restructuring SVGs so that vision-language models can reason about meaningful parts during animation. Our aim is to reveal an internal structure for SVGs that allows model to reference semantic units and attach motion to correct semantic units. The native SVG hierarchy rarely provides this structure, which motivates method that can reliably recover the semantics required for animation. We introduce Vector Prism, frame work that performs this recovery by stratifying noisy visual cues into coherent semantic groups, much like prism for vector graphics. Each SVG primitive (i.e., basic shapes) is rendered through several focused views (e.g., highlighting, isolation, zoomin, outlining, and bounding boxes) and VLM predicts its semantic meaning, producing set of weak, tentative semantic labels. Instead of aggregating these predictions using simple majority voting, Vector Prism interprets these predictions through the lens of statistical inference process [9]. Specifically, our method analyzes agreement patterns across weak labels and infers the underlying semantic signal with high stability. Bayes decision rule then selects labels that minimize expected classification error and recover the most plausible true part structure. These refined labels form the basis for the final stage, where Vector Prism restructures SVG primitives into coherent, animation-ready hierarchies. This restructuring bridges the gap between the visual semantics of the artwork and the syntactic organization of the SVG file, aligning the representation with how VLMs perceive and manipulate visual concepts. As result, VLMs can animate graphics at the level of meaningful parts rather than low-level shapes, producing motions that are both visually stable and semantically consistent. Our contributions are threefold: We formalize the overlooked challenge that SVG files are structured for rendering efficiency rather than semantic clarity, making them ill-suited for animation. We introduce the problem of semantic SVG restructuring and propose principled methodology for recovering animationrelevant part structure. We propose Vector Prism, statistical inference framework that transforms noisy view-dependent predictions from visionlanguage models into reliable semantic labels. By combining weak labels from multiple focused visualizations, our method infers robust underlying semantics. Our experiments demonstrate significant improvements over state-of-the-art methods in animation quality and instruction faithfulness, even outperforming commercial services such as Sora 2. Figure 2. Unstructured SVG contains fragmented elements and unclear tags, while structured SVG organizes parts with descriptive tags, ensuring alignment between SVG syntax and user instructions. 2. Related Work Vector Graphics Animation One line of work generates or animates vector graphics by optimizing vector (or animation) parameters using gradients from pre-trained image/video diffusion priors [17, 31, 35, 42, 43], typically via score distillation sampling (SDS) [1, 22, 26]. Since the SDS objective acts on rasterized renderings rather than vector structure, it encourages appearance preserving changes and resists large part rearrangements that animation often needs. Without explicit temporal regularization, the optimization often settles into short repetitive motions with visible jitter. Another active stream fine-tunes LLMs to directly produce vector graphics parameters or animation commands [25, 29, 40], enabled by large paired datasets of vector graphics and human instructions. Because LLMs carry little understanding for vector geometry and scene hierarchies [19, 46], performance scales primarily with data, often requiring millions of examples. Orthogonal to data scaling, we focus on recovering element-level semantics in the input SVGs, so that downstream LLMs/VLMs can robustly plan motions and generalize to diverse, in-the-wild graphics. Semantic Understanding of Vector Graphics Raw symbolic representations of vector graphics (e.g., shape coordinates and translation matrices) are designed for rendering and programmatic manipulation rather than human reading or editing, which makes them inherently difficult for humans to directly inspect and understand [2, 4, 36, 44]. This limitation has been highlighted as the research community seeks to teach LLMs, which often rely on perceptual cues Figure 3. (a) Animation pipeline overview. We first create detailed animation plan, then , then create the animation code for the structured SVG. (b) Vector Prism overview. We collect agreement patterns of response from different rendering methods and similar to humans, to understand and edit vectorized formats [15, 19, 23, 46]. ference framework that makes reliable semantic inferences from inherently noisy model predictions. Although VLMs tend to understand rasterized renderings of simple and well-separated vector graphics [44], we find that they quickly fail to understand individual parts of complex real-world cases. In this paper, we take significant step in vector graphics understanding by aiming not only to target complex real-world SVGs but to identify and label individual SVG primitives, which is exactly the capability required for animation. To do so, we present statistical inference framework that makes unreliable and noisy LLM outputs into robust decisions, enabling animation possible even without finetuning VLMs. 3. Method 3.1. The overview As illustrated in Section 1, the pipeline begins with animation planning (3.2), where visionlanguage model (VLM) interprets the visual content and generates detailed scheme of how each semantic components should be animated. It then proceeds to semantic wrangling (3.3), where the SVG is restructured into semantically meaningful and animatable form through statistical inference, and finally to animation generation (3.4), which produces executable animation code. The planning stage provides semantic understanding of the scene, while the animation stage operates directly on the SVG code. Our restructuring stage bridges this gap by injecting semantic meaning into the SVG, enriching its structure with interpretable tags that connect visual reasoning to code-level representation. The core contribution of our approach lies in this stage, where we introduce statistical in3.2. Animation planning The planning stage uses VLM to reason about the scene at semantic level. The SVG is first rendered into raster image so it can be understood by the VLM, which offers strong visual signals compared to the original SVG code representations. The VLM is then instructed to produce high-level animation plans given the rendered image and the users animation description, identifying which semantic components should move and how they relate to one another. For example, when prompted to make the sun rise, the VLM identifies the circular yellow region as the sun and the blue background as the sky, proposing that the sun should move upward while the sky gradually brightens. Since VLMs lack an understanding of the symbolic structures (i.e., SVG syntax), they have no way to directly implement those plans into the SVGs syntactic hierarchy. Bridging this semanticsyntactic divide is precisely the role of the restructuring stage. 3.3. Vector Prism Problem setup and notations Given SVG file, let be the set of all the primitives, which are basic shapes such as <path>, <rect>, <circle>, <ellipse>, <line>, <polyline>, and <polygon>. Every primitive should fall into the one of the semantic categories = {1, . . . , k} fixed from the planning stage. For each primitive there is an unknown true label y(x) that we want to predict. For SVG primitive to be visually interpreted by VLM, it first needs to be rendered into raster image. Deciding 3 how to render is non-trivial, and thus we use different rendering methods indexed by {1, . . . , }. This provides complementary views of the target primitive, which helps us safely collect different weak labels of the same primitive. Examples include highlight on the original canvas, tight bounding-box overlay, zoomed crop, and isolation on blank background. When we use method to render primitive x, the VLM returns label si(x) Y. We assume Dawid-Skene model [9] for each rendering method, Pr[si = ℓ] = (cid:40) pi, 1pi k1 , ℓ = y, ℓ = y. where rendering method has as accuracy pi and fails uniformly over the other 1 labels. We will recover the reliability pi of each strategy. to reliability Under From pairwise agreement the model above, VLM responses from two different renderings and would agree either because both are correct or because both pick the same wrong label. Thus, the probability of agreement is Aij = Pr[si = sj] = pipj + (1 pi)(1 pj) 1 . (1) Since two random guesses could still agree by chance with probability 1/k, we write δi = pi and Aij = 1 + 1 δiδj (i = j). (2) to separate chance from skill. Subtracting the chance term gives centered agreement matrix with Bij = Aij 1 for = and Bii = 0. Matrix is rank one on the offdiagonals E[B] = δδ, (3) 1 which is the outer product of δ. Let λ and be the top eigenvalue and eigenvector of B, then (cid:114) δ = λ(k 1) v, pi = 1 + δi, with the sign of chosen so that (cid:80) ˆδi 0. In this way, given the agreement matrix A, we can recover the initially unknown reliability of each VLM response i. The agreement matrix can be empirically estimated by burn-in pass, traversing the SVG primitives and collecting the agreement patterns ˆAij = 1 (cid:88) xX 1[si(x) = sj(x)]. Following Equation (2) and Equation (3), we can obtain ˆδ and consequently reliability ˆpi for each rendering method. From reliability to semantic labels With reliabilities ˆpi in hand, we score each candidate label for given element using Bayes decision rule with uniform prior log (y s) = const + (cid:88) log ˆpi + (cid:88) i: si=y i: si=y log 1 ˆpi 1 ."
        },
        {
            "title": "This is equivalent to a weighted vote with",
            "content": "wi = log (k 1)ˆpi 1 ˆpi , ˆy = arg max (cid:88) wi. i: si=y When all VLMs are equally reliable, all wi are equal and the decision rule reduces to majority voting. probability bound comparing this rule to majority voting and showing strict advantage whenever VLM reliabilities differ, is provided in the Appendix. From semantic labels to new structure Once reliable semantic labels are available, restructuring the SVG becomes straightforward step that turns meaning into organization without changing appearance. Although SVGs are usually grouped for rendering efficiency, not semantics, this step only needs to reorganize existing elements rather than reinterpret them. For example, shapes that share similar transformations may be grouped together even if they represent different objects, causing unrelated parts to move together. With correct labels, this can be easily fixed. Our restructuring algorithm attaches each label as class attribute and flattens the hierarchy so that all visual properties are applied directly to each primitive, preserving appearance. Primitives are then regrouped by label while maintaining the original paint order. Overlaps between different labels are checked to prevent rendering changes. The resulting SVG looks identical but is organized into meaningful parts ready for animation. Full pseudocode are provided in the Appendix and the code will be released upon acceptance. 3.4. Animation generation The LLM is instructed to animate the restructured SVG file according to the animation plan using CSS. While the earlier pipeline steps do not restrict generating animations to the CSS markup type, CSS was chosen for its simplicity, and our method has the capability to extend to complex animations using JavaScript or specialized libraries. Animation code can become lengthy, often exceeding the token generation limits of many models. To address this constraint, we adopt an iterative generation strategy [13, 18], where CSS animations are generated separately for each semantic category, with previously completed animations retained in the context for subsequent generations. To prevent conflicting animations, we enforce strict animation rules that ensure mutual exclusivity between generated effects. Complete prompts are provided in the Appendix. 4. Experiments 4.1. Dataset Our test dataset consists of 114 carefully curated animation instructions and SVG pairs, designed to test variety of SVG animation techniques. The instruction set covers broad range of animation tasks, from simple movements to complex actions such as 3D rotations and synchronized transitions. The SVG files were sourced from SVGRepo, ensuring diverse collection of objects and scenes, including animals, logos, buildings, and natural elements like fire, clouds, and water. The goal of this dataset is to evaluate the performance of SVG animation tools and systems by providing clear, detailed animation instructions that simulate real-world use cases in web environments. The animation categories and their performance are discussed in detail in the appendix. 4.2. Baselines AniClipart AniClipart [35] represents the optimizationbased animation methods, which optimizes animation parameters such as keypoint movements, using the Score Distillation Sampling loss [22]. While AniClipart does not output standard animation formats, it defines Bezier curves for keypoints within SVG files, enabling direct vector graphics animation. GPT-5 GPT-5 is reported to have one of the best understanding of symbolic representation among LLMs [20]. However, we observe that naive prompting of LLMs to generate animation code rarely produces meaningful motion. Therefore, we augment GPT-5 with the same highlevel planning and animation generation pipeline employed in our framework to ensure fair comparison. In this configuration, GPT-5 generates CSS animations in vector format. Video generation models We include two video generation models, the open-sourced Wan2.2 14B model [28] and OpenAIs Sora2 service [21]. Although these models produce rasterized video output (.mp4) and cannot generate the desired vector files, we include them to cover wide scope of animation generation technique, especially as these models demonstrate high performances in instruction following and video quality. 4.3. Implementation Details We use GPT-5-nano, which is 25 more cost-efficient than GPT-5, as the underlying visionlanguage model for planning and semantic labeling, while GPT-5 is used for animation generation. Our semantic labeling stage is statistically robust to noise and operates with minimal computational overhead, enabling lightweight models to perform reliably CLIP-T2V GPT-T2V DOVER Vector AniClipart [35] GPT-5 [20] Wan 2.2 [28] Sora 2 [21] Ours 15.66 20.67 21.14 20.29 21.55 23.96 40.92 65.21 69.08 76. 3.35 4.92 3.72 4.19 4.97 Table 1. Animation quality and instruction-following scores across different methods. The checkmark indicating whether each method generates vector-based animations. without sacrificing accuracy. All SVG primitives are rendered at 512 512 resolution when given as VLM input for analysis. We do not share the agreement matrix across SVGs, since we find that the reliability of each rendering method can vary depending on the visual complexity and structure of the SVG. During the burn-in stage, where agreement patterns are collected, single full pass over all primitives within each SVG provides good balance between estimation stability and computational efficiency. 4.4. Quantitative Evaluation We evaluate the generated animations using two instructionfollowing metrics and one perceptual quality metric. Following InternSVG [29], we measure the correspondence between animation instructions and rendered videos using video-pretrained CLIP model [24, 30], referred to as CLIPT2V. To complement this, we introduce the GPT-T2V score, where GPT grades each video based on how accurately its motion follows the given instruction. This follows the growing use of LLM-based evaluators for instruction following and multimodal reasoning [45]. Finally, we assess perceptual quality with DOVER [33], an off-the-shelf video quality assessment model that captures both technical fidelity and visual aesthetics. Also, trade-off between perceptual quality and instruction following can easily occur, as limiting motion often leads to higher visual quality, whereas enforcing movement to meet the instruction can reduce perceptual fidelity. As shown in Table 1, our method achieves the best scores across all metrics, demonstrating clear advantages in both motion realism and instruction faithfulness. This improvement comes from the ability to expose meaningful parts of the SVG prior to animation, allowing the model to attach coherent motions to relevant semantic parts. It is also important to note that vector-based animations typically struggle with instruction-following compared to video generation models, as video models are heavily trained on videotext pairs. However, this limitation is not inherent to the vector-based format, but rather stems from the lack of semantic understanding of vectors. Our method overcomes this and outperforms video models as well, without training on large-scale video video-text datasets. 5 want an opening animation for the SVG, starting from the bottom and moving up to the top. AniClipart GPT-5 Wan 2.2 Sora 2 Ours want the lightning bolt to glow softly and the raindrops to fade in and out gently. AniClipart GPT-5 Wan 2.2 Sora 2 Ours want the stars and planets first to emerge gently and then the rings to appear in stroke effect. AniClipart GPT-5 Wan 2.2 Sora 2 Ours want the hexagon to appear first, and then the sign to enter by spinning in. AniClipart GPT-5 Wan 2.2 Sora 2 Ours Figure 4. Animations generated by each method. Please use Adobe Acrobat, the Firefox browser, or the PDF. js extension on Chromium browsers for the best experience [10]. An HTML version is available in the project page. 6 Figure 5. Human preference results comparing our method with baseline approaches. Pink segments represent preferences towards our method, and orange or purple segments represent the competing baseline. 4.5. Qualitative Analysis AniClipart and GPT-5 often fail to produce meaningful motion since they lack explicit semantic understanding. These approaches interpret semantics implicitly, AniClipart through the diffusion prior and GPT-5 through internal representations, without explicit part labels or hierarchy. As result, they tend to produce uniform motion across entire figures, leading to swaying or barely moving animations. Video generation models, Wan 2.2 and Sora 2, generate richer motion than the above methods but often collapse into static frames or distorted scenes when given dynamic, animation-focused instructions such as An opening scene of the SVG. Note that these are rasterized videos rather than vector graphics, which makes them unsuitable for webbased animation tasks where lightweight rendering is essential. In contrast, our method translates instructions into motion entirely through the language domain, avoiding the limitations of multimodal training and dataset dependence. We showcase examples of the generated animations in Figure 4, where the differences discussed above are clearly visible. Additional qualitative results and extended comparisons are provided in the project page for further reference. 4.6. User Study To complement the quantitative evaluation, we conducted user study to assess how well each generated animation aligns with the given instructions from human perspective. total of 760 pairwise comparisons were collected from 19 participants. In each trial, participants were shown two videos generated from the same instruction, each produced by different method, and asked to select the one that better followed the instruction. The aggregated preferences are summarized in Figure 5, showing consistent favorability toward our method even when compared against state-of-the-art video generation models such as Sora 2 and Wan 2.2. We report the alignment between the user study outcomes and the GPT-T2V metrics in the Appendix. Figure 6. Dual-axis bar chart comparing compression ratio (left y-axis) and animation fidelity (right y-axis). Compression ratios are depicted by solid bars, and Animation Fidelity is shown with hatched bars. 5. Analysis 5.1. Encoding efficiency of vector-based animations We demonstrate the effectiveness of vector-based animations by comparing the compression ratio compared to Sora 2 and the animation fidelity in Figure 6. Typically, as the raster video resolution increases for quality (e.g., from 480p to 720p), the file size increases and the video less compressed. In contrast, the SVG animations generated by our approach describe motion through compact, symbolic CSS keyframes applied to geometric primitives. The resulting file size is primarily dependent on the complexity of the SVG structure (number of primitives) and the length of the animation code, not the output resolution or frame rate. This leads to significant improvement in encoding efficiency compared to video models like Wan 2.2 and Sora 2, which generate every pixel of the animation, even when vector representation is possible. Sora 2, for instance, results in an average file size that is 54 larger than those produced by our approach, with this gap widening as video resolution and duration increase. This makes our approach particularly well-suited for modern web environments, where lightweight assets are essential for fast loading times, responsive UI/UX, and reduced data consumption across networks. 5.2. Stability compared to majority voting Evaluating the quality of semantic groupings in SVGs is challenging without ground truth labels, yet crucial for understanding whether our statistical inference produces coherent clusters. We treat each semantic group as cluster and measure clustering quality using the Davies-Bouldin 7 Figure 7. Example case of when Bayes decision rule can consistently make robust decisions even with noisy signals. index (DBI) [8], metric that quantifies the ratio of withincluster scatter to between-cluster separation. We compute distances in the feature space of DINO v3 [27], which provides semantically meaningful visual embeddings. SVG files with their original, rendering-oriented groupings yield an average DBI of 33.8, reflecting the semantic incoherence of primitives grouped solely for drawing efficiency. Majority voting with the same multi-view rendering techniques improves this to 12.6, demonstrating that aggregating multiple views helps, but still produces noisy groupings. In contrast, Vector Prism achieves DBI of 0.82, indicating near-perfect semantic clustering. The key advantage of our approach over majority voting is illustrated in Figure 7. When one rendering method produces unreliable predictions, correct only by chance, majority voting treats it equally with other reliable methods. This equal weighting allows the weakest reliable responses to occasionally flip the predicted label for certain primitives, creating inconsistent groupings that fragment semantically coherent parts. Since animation quality depends on all primitives being correctly grouped, even small fraction of mislabeled elements can break the visual logic of motion. By estimating reliability scores ˆpi for each rendering method, Vector Prism consistently downweights noisy VLM responses throughout the entire labeling process, ensuring stability across the full set of primitives. 5.3. Failure Cases Even with semantic groupings and well-structured hierarchy, our method operates at the level of SVG primitives defined in the original file. We treat primitives as atomic units and do not subdivide or decompose them further, which limits animation flexibility when the input SVG lacks granularity. For example, as shown in Figure 8, the lightning shape is written as single large <path> element, while the instruction requires this to shatter into pieces. The method fails to animate this part of the instruction, as the pieces Figure 8. Failure case. Since the lightning bolt is defined as single atomic <path> primitive (left), our approach cannot execute the operation beyond the input SVGs granularity (right). themselves do not exist as independent primitives. This limitation could be addressed if users can refine their SVG files using vectorization tools such as VTracer [11] or recent image-to-SVG models [25, 40], which generate SVGs with controllable levels of detail. Alternatively, future work could explore automatic primitive subdivision strategies that identify and split overly coarse elements based on the animation requirements. 6. Conclusion In this paper, we introduced Vector Prism, novel framework designed to overcome the critical semantic-syntactic gap that prevents modern vision-language models (VLMs) from successfully animating Scalable Vector Graphics (SVGs). Our core insight is that by enriching the native SVG structure with coherent semantic anchors, VLMs can reason about meaningful parts and reliably generate targeted motion. The foundation of our approach is multiview statistical inference mechanism utilizing the DawidSkene model, which effectively transforms noisy, weak predictions from VLM into robust, high-confidence semantic labels, eliminating the need for extensive, domain-specific VLM fine-tuning. Through rigorous quantitative and qualitative evaluations, we demonstrated that our method achieves unmatched improvements in animation quality and instruction fidelity, surpassing both existing vector animation techniques and state-of-the-art raster video generation models. We believe that bridging the semantic-syntactic gap is vital, generalizable step for unlocking the full potential of VLMs across various symbolic domains. Whether for vector graphics or for 3D assets and scenes, methods that align human semantic intent with machine-readable structure will significantly broaden the capabilities of language models, transforming them from passive code generators into robust, context-aware animation and design agents."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [2] Mu Cai, Zeyi Huang, Yuheng Li, Utkarsh Ojha, Haohan Wang, and Yong Jae Lee. Leveraging large language models for scalable vector graphics-driven image understanding. arXiv preprint arXiv:2306.06094, 2023. 2 [3] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. NeurIPS, 2020. 2 [4] Sumit Chaturvedi, Michal Lukaˇc, and Siddhartha Chaudhuri. Regroup: Recursive neural networks for hierarchical grouping of vector graphic primitives. arXiv preprint arXiv:2111.11759, 2021. 2 [5] Chen Chen, Bongshin Lee, Yunhai Wang, Yunjeong Chang, and Zhicheng Liu. Mystique: Deconstructing svg charts for layout reuse. IEEE TVCG, 2023. 2 [6] Chen Chen, Hannah Bako, Peihong Yu, John Hooker, Jeffrey Joyal, Simon Wang, Samuel Kim, Jessica Wu, Aoxue Ding, Lara Sandeep, et al. Visanatomy: An svg chart arXiv preprint corpus with fine-grained semantic labels. arXiv:2410.12268, 2024. [7] Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1 [8] David Davies and Donald Bouldin. cluster separation measure. IEEE TPAMI, 2009. 8 [9] Alexander Philip Dawid and Allan Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series (Applied Statistics), 1979. 2, 4 [10] Alexander Grahn. The animate package, 2011. 6 [11] Vision Cortex Group. Vtracer, 2020. 8 [12] HsiaoYuan Hsu and Yuxin Peng. Postero: Structuring layout trees to enable language models in generalized contentaware layout generation. In CVPR, 2025. [13] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competitionlevel code generation with alphacode. Science, 378(6624): 10921097, 2022. 4 [14] Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, JianGuang Lou, and Dongmei Zhang. Layoutprompter: Awaken the design ability of large language models. NeurIPS, 2023. 2 [15] Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, and Alex Jinpeng Wang. Vcode: multimodal coding benchmark with svg as symbolic visual representation. arXiv preprint arXiv:2511.02778, 2025. 3 [16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2023. 1 Bringing text to life via video diffusion prior. In ICCV, 2025. [18] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In ICLR, 2023. 4 [19] Kunato Nishina and Yusuke Matsui. Svgeditbench v2: benchmark for instruction-based svg editing. arXiv preprint arXiv:2502.19453, 2025. 2, 3 [20] OpenAI. Gpt-5, 2025. 1, 5 [21] OpenAI. Sora2 system card, 2025. 5 [22] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, 5 [23] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Xiao, Katherine Collins, Joshua Tenenbaum, Adrian Weller, Michael Black, and Bernhard Scholkopf. Can large language models understand symbolic graphics programs? ICLR, 2025. 3 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PmLR, 2021. 5 [25] Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images. In CVPR, 2025. 2, [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [27] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. 8 [28] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 5 [17] Zichen Liu, Yihao Meng, Hao Ouyang, Yue Yu, Bolin Zhao, Daniel Cohen-Or, and Huamin Qu. Dynamic typography: [29] Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting 9 [44] Tong Zhang, Haoyang Liu, Peiyan Zhang, Yuxuan Cheng, and Haohan Wang. Beyond pixels: Exploring humanreadable svg generation for simple images with vision language models. arXiv preprint arXiv:2311.15543, 2023. 2, [45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 2023. 5 [46] Bocheng Zou, Mu Cai, and Jianrui Zhang Yong Jae Lee. Vgbench: Evaluating large language models on vector graphics understanding and generation. EMNLP, 2024. 2, 3 Internsvg: Towards unified svg Zhang, Yuanqi Li, et al. tasks with multimodal large language models. arXiv preprint arXiv:2510.11341, 2025. 2, 5 [30] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2024. 5 [31] Zhenyu Wang, Jianxi Huang, Zhida Sun, Yuanhao Gong, Daniel Cohen-Or, and Min Lu. Layered image vectorization via semantic simplification. In CVPR, 2025. 2 [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 1 [33] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In ICCV, 2023. [34] Ronghuan Wu, Wanchao Su, and Jing Liao. Layerpeeler: Autoregressive peeling for layer-wise image vectorization. SIGGRAPH Asia, 2025. 2 [35] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Aniclipart: Clipart animation with text-to-video priors. IJCV, 2025. 2, 5 [36] Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. Empowering llms to understand and generate complex vector graphics. In CVPR, 2025. 2 [37] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1 [38] Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, et al. Structeval: Benchmarking llms capabilities to generate structural outputs. arXiv preprint arXiv:2505.20139, 2025. 2 [39] John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? In ICLR, 2025. 1 [40] Yiying Yang, Wei Cheng, Sijin Chen, Xianfang Zeng, Fukun Yin, Jiaxu Zhang, Liao Wang, Gang Yu, Xingjun Ma, and Yu-Gang Jiang. Omnisvg: unified scalable vector graphics generation model. In NeurIPS, 2025. 2, [41] Zhiqiang Yuan, Ting Zhang, Ying Deng, Jiapei Zhang, Yeshuang Zhu, Zexi Jia, Jie Zhou, and Jinchao Zhang. Rdtf: Resource-efficient dual-mask training framework for arXiv preprint multi-frame animated sticker generation. arXiv:2503.17735, 2025. 2 [42] Peiying Zhang, Nanxuan Zhao, and Jing Liao. Text-to-vector generation with neural path representation. ACM TOG, pages 113, 2024. 2 [43] Peiying Zhang, Nanxuan Zhao, and Jing Liao. Style customization of text-to-vector generation with image diffusion priors. In SIGGRAPH, pages 111, 2025. 2 10 Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure"
        },
        {
            "title": "Contents",
            "content": "A Extended Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Confidence Bounds for Reliability Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 B.1 Decomposing Agreements and Rank-One Structure . . . . . . . . . . . . . . . . . . . . . . . . . 2 B.2 Reliability Estimation via Eigenvector Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 B.3 Bayes Rule vs. Majority Voting: Error Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 GPT-Human Alignment on VideoText Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . Dataset Composition and Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 VLM Prompts for Planning and Animation Generation . . . . . . . . . . . . . . . . . . . . . . 8 Restructuring SVG Files with Semantic Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1 A. Extended Related Work A.1. Vector graphics generation and decomposition Beyond animation, the broader field of vector graphics generation has mainly focused on vectorizing raster images. DeepSVG [3] introduced hierarchical generative network that jointly models both the structure and appearance of vector graphics, enabling controllable generation through learned latent representations. More recently, LayerPeeler [34] proposed an autoregressive approach to decompose raster images into layered vector representations, demonstrating that careful layer-wise decomposition can produce more interpretable and editable vector graphics. Yuan et al. [41] extended these ideas to animated stickers, introducing resource-efficient dual-mask training framework that generates multi-frame animations while maintaining computational efficiency. While our method assumes user-provided SVG as input, it can be seamlessly combined with image-to-SVG or text-to-SVG models to synthesize SVG animations directly from images or text, eliminating the need for manually authored SVGs. A.2. Domain-specific SVG understanding The challenge of understanding structured visual representations extends to specialized domains such as data visualization, such as graphs and charts. Chen et al. [5] developed Mystique, system for deconstructing SVG charts to enable layout reuse, demonstrating that reverse-engineering the semantic structure of charts requires domain-specific parsing strategies. Building on this, VisAnatomy [6] provided large-scale corpus of SVG charts with fine-grained semantic labels, establishing benchmarks for chart understanding tasks. These works highlight that even within the SVG domain, different application areas (charts vs. illustrations vs. icons) require tailored approaches to semantic understanding. Our framework focuses on general-purpose illustrations and icons, where semantic parts correspond to visual objects rather than data encodings. A.3. Language models for design tasks Recent work has explored leveraging large language models for various design tasks beyond vector graphics. LayoutPrompter [14] demonstrated that LLMs can be awakened to perform layout design through carefully crafted prompts that encode spatial relationships and design principles. PosterO [12] extended this to content-aware layout generation by structuring layout trees in way that enables language models to reason about hierarchical spatial arrangements. These works share our core insight that restructuring visual representations to align with how language models process information is crucial for enabling reliable generation. Taken together, these insights suggest that LLMs are not inherently incapable of design [38], and rather, their potential emerges when design representations are aligned with the natural language structures they are trained to process. B. Confidence Bounds for Reliability Estimation In this section, we provide the formal justification for the statistical inference framework described in Section 3.3. We first show how to derive the underlying reliabilities from pairwise agreement (B.1, B.2) and then prove that using these reliabilities in Bayes-weighted vote is provably superior to standard majority vote (B.3). B.1. Decomposing agreements and deriving the rank-one structure The goal here is to formalize the relationship between the observable quantity (the agreement patterns Aij) and the hidden quantity we care about (the individual reliability of each method, pi). Lemma B.1 (Agreement). Under the symmetric Dawid-Skene model, Aij = pipj + (1pi)(1pj ) with δi = pi 1 . k1 δiδj for = j, + = k1 Proof. The probability of agreement Aij = Pr[si = sj] is the sum of two mutually exclusive cases. One case where both methods are correct (pipj), and another case where both methods are incorrect but agree on the same wrong label, which sums to (1pi)(1pj ) δi into the first expression and simplifying the resulting algebra. . The second expression is obtained by substituting pi = 1 + δi and 1 pi = k1 k1 Proposition 1 (Rank one). Let Bij = Aij (i = j) and Bii = 0. Then E[B] = k1 δδ on off-diagonals (rank one). Proof. By definition, the centered agreement matrix entry is Bij = Aij 1 S.1 for Aij: for = j. Substituting the result from Lemma Bij = (cid:18) 1 + 1 1 = 1 δiδj (cid:19) δiδj 2 This is the (i, j)-th entry of the matrix assuming δ is non-zero. k1 δδ. Since δδ is the outer product of the vector δ with itself, its rank is one, B.2. Reliability estimation via eigenvector analysis This is the recovery part of our proof. Now that we have established link between agreement and skill (B and δ), we need to show we can actually solve for δ. Theorem S.3 confirms that we can exploit this rank-one structure to reliably estimate the skill vector δ from our empirical data ˆB using standard linear algebra techniques, which is finding the top eigenvector. Theorem B.2 (Quality Guarantee for Estimated Skill). Let ˆB RM be the empirical centered agreement matrix built from cases and rendering methods. If each pairwise agreement is estimated within ε, then with probability 1 η, (cid:13) ˆδ δ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:32)(cid:114) (cid:33) + ε ,"
        },
        {
            "title": "M\nn",
            "content": "for some confidence bound η, scale > 0, and constant depending only on k. Furthermore, = (cid:112)λ1(k 1)/k where λ1 is the top eigenvalue of ˆB.* Proof. By definition, the centered agreement matrix entry is Bij = Aij 1 S.1 for Aij: for = j. Substituting the result from Lemma (cid:18) 1 This means the matrix E[B] is constant factor k1 times the matrix δδ. The outer product of any vector with itself, δδ, is mathematically rank-one matrix. This rank-one property is critical because it means the vector δ (our reliability or skill vector) must be proportional to the dominant eigenvector of B, enabling its recovery in Theorem S.3. 1 Bij = δiδj δiδj 1 + = (cid:19) B.3. Bayes decision rule and error bounds vs. Majority voting This section proves that weighting VLM responses by their inferred reliability is statistically superior to simple majority voting. Corollary B.5 shows that the Bayes-weighted method achieves strictly better error exponent whenever VLM reliabilities differ. B.3.1. Setup and the log-likelihood ratio Fix the true label {1, 2, . . . , k} and any competitor label = y. For each method i, recall that pi is the probability of correct classification. Define: qi 1pi di pi qi = kpi1 wi = log pi qi k1 (probability of any specific wrong label) k1 (discrimination parameter) (Bayes weight, or log-likelihood-ratio) = log (k1)pi 1pi For each observation si (method is output), define the log-likelihood ratio: Zi = wi 1[si = y] wi 1[si = y] = +wi wi 0 if si = if si = otherwise The Bayes decision rule prefers over when (cid:80) Zi > 0. An error occurs when (cid:80) Zi 0 despite being true. Lemma B.3 (Properties of Zi). Under the true label y: (1) Zi [wi, wi], (2) E[Zi y] = widi, and (3) the Zi are independent across methods. Proof. Boundedness is immediate from the definition. For the expected value, given y, method outputs with probability pi (giving Zi = wi) and outputs with probability qi (giving Zi = wi). Thus: E[Zi y] = pi wi + qi (wi) = wi(pi qi) = widi. Independence follows from the conditional independence assumption in the Dawid-Skene model. *The underlying mathematics, based on the Davis-Kahan theorem, provides strong upper limit on the error between our calculated skill vector (ˆδ) and the true skill vector (δ). This error is confirmed to decrease as we process more SVG primitives (n). 3 B.3.2. Error bound for Bayes decision rule Theorem B.4 (Hoeffding bound for Bayes LLR). (cid:34) (cid:88) Pr i=1 Zi 0 (cid:35) (cid:12) (cid:12) (cid:12) (cid:12) (cid:32) exp ((cid:80)m 2 (cid:80)m i=1 widi)2 i=1 w2 (cid:33) . Proof. We apply Hoeffdings inequality for bounded independent random variables. Since Zi [wi, wi] with range (bi ai) = 2wi, Hoeffdings inequality gives: Pr (cid:34) (cid:88) i=1 (Zi E[Zi]) (cid:35) (cid:18) exp 2t2 i=1 4w2 (cid:80)m (cid:19) . The error event (cid:80) Zi 0 can be rewritten as (cid:80)(Zi E[Zi]) (cid:80) E[Zi]. Setting = (cid:80)m substituting: i=1 widi = (cid:80)m i=1 (cid:35) (cid:18) Pr Zi 0 exp (cid:34) (cid:88) i= 2((cid:80)m 4 (cid:80)m i=1 widi)2 i=1 w2 (cid:19) (cid:18) = exp ((cid:80)m 2 (cid:80)m i=1 widi)2 i=1 w2 (cid:19) . E[Zi] and The exponent ((cid:80) widi)2 2 (cid:80) w2 error rates. quantifies how fast the error probability decays. Larger exponents mean exponentially smaller B.3.3. Comparison with majority voting and proof of superiority Majority voting uses uniform weights wMV strictly better when reliabilities differ. 1, yielding error exponent ((cid:80) di)2 2m . We now show that Bayes weighting is Theorem B.5 (Improvement over majority voting). In the small-error regime where pi 1 wi k2 ) imply wi kdi. The Bayes error exponent then satisfies: k1 δi (where δi = pi 1 k1 δi and di 1, the approximations ExponentBV 1 2 (cid:88) i=1 d2 = (cid:2)(Mean(d))2 + Var(d)(cid:3) ExponentMV = ((cid:80) di)2 2m , with equality if and only if all di are equal. The improvement factor is ExponentBV ExponentMV 1 + Var(d) (Mean(d))2 , quantifying the benefit of exploiting heterogeneity in method reliabilities. Proof. First, we derive the weight approximation. For pi = 1 + δi with small δi: wi = log (k 1)pi 1 pi = log + δi) (k 1)( 1 k1 δi = log 1 + kδi 1 kδi k1 . Using Taylor expansions log(1 + x) and (1 y)1 1 + y: wi kδi + kδi 1 = k2δi 1 . Similarly, di = kpi1 k1 = kδi k1 , so wi kdi. Now compute the Bayes exponent using wi kdi: ExponentBV = ((cid:80) widi)2 2 (cid:80) w2 (k (cid:80) d2 )2 2k2 (cid:80) d2 = (cid:80) d2 2 . 4 Using the variance decomposition (cid:80) i = m(Mean(d))2 + Var(d): 2 ExponentBV (cid:2)(Mean(d))2 + Var(d)(cid:3) . For majority voting with wi = 1: ExponentMV = ((cid:80) di)2 2m = m2(Mean(d))2 2m = m(Mean(d))2 2 . The difference is ExponentBV ExponentMV mVar(d) improvement factor is: 2 0, with equality only when Var(d) = 0 (all di equal). The"
        },
        {
            "title": "ExponentBV\nExponentMV",
            "content": "= 2 [(Mean(d))2 + Var(d)] m(Mean(d))2 2 = 1 + Var(d) (Mean(d))2 . Remark B.6. The improvement factor 1 + Var(d) (Mean(d))2 shows that Bayes weighting provides the most benefit when method reliabilities are heterogeneous. If all methods have identical reliability, both approaches are equivalent. The more diverse the reliabilities, the greater the advantage of properly weighting methods by their estimated skill. 5 C. GPT-Human Alignment on Video-Text Alignment We assess the alignment between our user study and the GPT-T2V metric in order to validate the reliability of the GPT based evaluation. In particular, we compare pairwise preferences and measure how often the metric selects the same animation as human participants. We find that GPTs preferences (i.e., cases where GPT assigns higher score to one animation than the other) agree with user preferences in 83.4% of the pairs, which indicates strong correspondence between the automatic and human judgments. In comparison, the CLIP-T2V metric, which operates without any external API services, reaches only 53.4% agreement with the user study responses. This substantial gap suggests that GPT-T2V captures human perceptual preferences much more faithfully and therefore provides more reliable proxy for human evaluation in our setting. We observe that state-of-the-art LLMs demonstrate robust ability to interpret simple animations and reason about their motion. When guided by clear evaluation criteria, such as those provided in Figure 9, these models exhibit high degree of alignment with human judgments. Although GPT-5 is also the model used to generate our animations, its role as an evaluator is fundamentally different. In practice, using LLMs as judges is often more straightforward and reliable than using them as generators, as evaluation requires consistency and comparative reasoning rather than creative synthesis. Figure 9. Prompt templated used for GPT-T2V evaluation. 6 D. Dataset Composition and Coverage Our test dataset comprises 114 hand-crafted animation instructions across 57 unique SVG files, with each SVG file receiving an average of two distinct animation scenarios. These examples were meticulously designed to reflect the diverse animation needs encountered in modern web development. As shown in Table 2, our dataset spans six thematic categories, with particularly strong representation in Nature/Environment (31.6%) and Objects/Miscellaneous (26.3%), ensuring broad coverage of visual content types commonly found in web interfaces. From tech logos and brand animations to natural phenomena and user interface elements, our dataset encompasses the full spectrum of SVG animation use cases. Furthermore, Table 3 demonstrates our intentional focus on varied interaction patterns, with Appearance/Reveal animations (28.1%) and State Transition effects (13.2%) representing critical components of modern web user experiences. The substantial presence of Organic/Natural Movement (12.3%) and Rotational Movement (8.8%) patterns reflects our commitment to including both subtle, life-like animations and dynamic, attention-grabbing effects. This careful curation ensures that our test dataset not only provides comprehensive coverage but also accurately represents the practical animation requirements of contemporary web applications, from loading indicators and state feedback to decorative enhancements and interactive storytelling. Table 2. Distribution of Subject Themes in Test Dataset Table 3. Distribution of Interaction Patterns in Test Dataset Subject Theme Nature/Environment Objects/Miscellaneous UI/Interface Elements Tech Logos/Brands Animals/Characters Faces/Emojis Total Count % 31.6 26.3 15.8 10.5 8.8 7.0 100.0 36 30 18 12 10 8 114 Interaction Pattern Other/Mixed Appearance/Reveal State Transition Organic/Natural Movement Rotational Movement Total Count % 37.7 28.1 13.2 12.3 8.8 100.0 43 32 15 14 10 114 E. VLM Prompts for Planning and Animation Generation Our animation pipeline relies on two complementary VLM prompts, one for planning and one for per-class animation generation. The model is instructed to avoid generic SVG terms and to instead use intuitive, role-based identifiers, and to write short, human-interpretable description of the intended motion of each part. This prompt focuses entirely on semantic intent and it does not require the model to understand SVG syntax, only to reason visually and symbolically about what should happen. The second prompt is invoked once for each semantic class produced during restructuring. It receives three ingredients, the restructured SVG, all previously generated CSS (so it can remain consistent), and the animation plan for that particular class. Its role is purely syntactic and translate one actors high-level plan into concrete, production-safe CSS. To avoid conflicts across iterative generations, the prompt enforces strict lanes convention in which each motion component (translation, rotation, scale, opacity, blur, etc.) is expressed through typed CSS custom properties rather than direct transform declarations in keyframes. single composer rule per class then assembles these properties into the final transform. This ensures that new animations never overwrite existing ones, allowing independent motions to compose reliably across multiple generation passes. The two prompts divide responsibilities cleanly, the planner performs semantic reasoning, and the per-class generator performs structured code synthesis. This separation avoids the common failure modes where single prompt must juggle visual interpretation, HTML/SVG structure, and CSS constraints simultaneously. The lanes system further guarantees that iterative code generation remains stable, that different motions do not collide, and that long CSS files can be produced incrementally without exceeding model context limits. Figure 10. Prompt template used for planning animations. The output is JSON formatted dictionary of semantic categories and their animation plan. 8 Figure 11. Prompt template used for generating animations. CSS codes are generated in cascaded manner to bypass generation token length limits. 9 F. Restructuring SVG Files with Semantic Labels Once we obtain semantic labels for all primitives, we reorganize the SVG structure by regrouping primitives according to their labels wherever possible. This is non-trivial because existing SVG groupings are tightly coupled to the rendering order, and naively introducing new groups can disrupt this order and alter the final appearance. Nevertheless, restructuring is crucial for enabling meaningful motion, as primitives that belong to the same semantic group can share attributes such as rotation axes, timing, and other animation parameters. To safely regroup primitives, we first flatten the SVG structure and ungroup all nested groups, while transferring group properties to the child primities, so that the rendering appears identical to the original. Then, we estimate the spatial extent (area) occupied by each primitive and use this to detect conflicting merges. Nest, we merge primitives with the same semantic label only when doing so introduces no conflicts with any primitives in between them in the rendering order. Finally, we augment each resulting group with metadata, including its bounding box, geometric center, and parent-child relationships, which we later use to drive animation. We describe the steps in Algorithm 1 and plan to make all the implementation fully public upon acceptance. Algorithm 2 Pseudocode for the SVG file restructuring process using the predicted semantic labels. 1: Inputs SVG S, predicted label ˆy(x) for each primitive 2: Output regrouped SVG 3: Flatten 4: Traverse in original paint order and build list = {(e, idx, ℓ, B)} is cloned primitive with inherited properties baked in idx is the original paint index ℓ = ˆy(e) is appended as the final class token is screen-space bounding box 5: Regroup by label with barrier test 6: for each label ℓ do 7: 8: Iℓ indices of with label ℓ in ascending paint order Greedily form groups G[ℓ] over Iℓ using the rule: candidate can join current group if no element of different label whose index lies between min(G {j}) and max(G {j}) overlaps any member of {j} in screen space 9: end for 10: Compose regrouped SVG 11: Create with original attributes and non-drawables copied verbatim 12: For each group in order of its earliest index: emit <g> with class ℓ-group or ℓ-group-k append members in original relative order write light metadata: bounds, geometric center, paint-order index optionally add parent and children links from the plan 13: return"
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}