{
    "paper_title": "Efficient Test-Time Scaling for Small Vision-Language Models",
    "authors": [
        "Mehmet Onurcan Kaya",
        "Desmond Elliott",
        "Dim P. Papadopoulos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Small Vision-Language Models (VLMs) provide a computationally efficient alternative to larger models, at the cost of weaker generalization abilities and downstream task performance. These shortcomings could be addressed by test-time scaling techniques, but existing methods are typically computationally demanding, contradicting the resource-efficient design goals of small models. To address these limitations, we propose two novel and efficient test-time scaling strategies that leverage the model-internal features rather than external supervision: (i) Test-Time Augmentation (TTAug), which generates multiple augmented inputs and aggregates outputs at the token level without parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model parameters during inference using consensus-based pseudolabels from TTAug. Through extensive experiments across nine benchmarks, we demonstrate consistent performance improvements while maintaining computational efficiency suitable for resource-constrained environments. The generality of our approach is demonstrated both within models at different scales and across different VLMs without additional tuning."
        },
        {
            "title": "Start",
            "content": "Efficient Test-Time Scaling for Small Vision-Language Models EFFICIENT TEST-TIME SCALING FOR SMALL VISION-LANGUAGE MODELS Mehmet Onurcan Kaya1,2 Desmond Elliott3,2 Dim P. Papadopoulos1,2 1 Technical University of Denmark monka@dtu.dk, 2 Pioneer Center for AI dimp@dtu.dk de@di.ku.dk, 3 University of Copenhagen 5 2 0 2 3 ] . [ 1 4 7 5 3 0 . 0 1 5 2 : r (cid:128) Project Page: https://monurcan.github.io/efficient_test_time_scaling"
        },
        {
            "title": "ABSTRACT",
            "content": "Small Vision-Language Models (VLMs) provide computationally efficient alternative to larger models, at the cost of weaker generalization abilities and downstream task performance. These shortcomings could be addressed by test-time scaling techniques, but existing methods are typically computationally demanding, contradicting the resource-efficient design goals of small models. To address these limitations, we propose two novel and efficient test-time scaling strategies that leverage the model-internal features rather than external supervision: (i) Test-Time Augmentation (TTAug), which generates multiple augmented inputs and aggregates outputs at the token level without parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model parameters during inference using consensus-based pseudolabels from TTAug. Through extensive experiments across nine benchmarks, we demonstrate consistent performance improvements while maintaining computational efficiency suitable for resource-constrained environments. The generality of our approach is demonstrated both within models at different scales and across different VLMs without additional tuning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Small Vision-Language Models (VLMs) offer computational efficiency and accessibility, yet their performance frequently degrades under domain shift due to inherent biases and limited generalization capabilities (Marafioti et al., 2025; Lu et al., 2025). While test-time scaling methods can, in principle, improve their performance, there are several critical limitations that undermine their practicality for small models in resource-constrained settings. First, many test-time scaling methods rely on external verification models or computationally intensive reranking strategies, making them unsuitable for deployment on resource-constrained consumer GPUs (Zhang et al., 2024; Singh et al., 2025). This contradicts the resource-efficient design goals of small VLMs. Second, existing approaches that avoid external verifiers, such as sampling multiple candidate responses and aggregating them into final prediction using the models internal signals (Wang et al., 2023b; Adiwardana et al., 2020; Chen et al., 2024a), remain unsatisfactory because they typically operate only at the answer level, ignoring local signals for aggregation. Global measures like average confidence obscure token-level fluctuations that signal response quality, and averaging across entire sequences masks reasoning breakdowns at intermediate steps. Moreover, these methods require complete response generation before evaluation, preventing early termination and wasting computation. Finally, many existing methods are restricted to tasks with extractable final answers (e.g., multiple-choice or numerical reasoning), limiting their applicability to open-ended tasks such as visual question answering and captioning (Zhang et al., 2025a; Chen et al., 2024a). In this paper, we leverage model-internal representations to overcome these limitations. Our goal is to improve the robustness and accuracy of small VLMs at inference time through efficient, lightweight, and practical test-time scaling strategies that require no external models or additional training data. We introduce two methods in unified framework: Test-Time Augmentation and Test-Time Adaptation. Test-Time Augmentation generates multiple responses by applying input-level augmentations to both images and text. Crucially, it aggregates outputs at the token-level rather than the answer-level, 1 Efficient Test-Time Scaling for Small Vision-Language Models (a) Our efficient test-time scaling framework (b) Performance comparison Figure 1: Our framework consists of two main pipelines: (1) Test-Time Augmentation: Given an input image and text prompt, we apply various transformations to create multiple augmented versions. VLM processes each augmented input to produce next token probability distributions, which are then aggregated at the token level to generate the final response. (2) Test-Time Adaptation: We create pseudolabels through test-time augmentation and fine-tune the VLM parameters, then repeat the process. Both methods demonstrate effectiveness across nine diverse benchmarks as shown in (b). which allows the model to quickly detect low-quality responses, and allows for more fine-grained exploitation of the model-internal signals. This method requires no parameter updates, making it both simple and efficient. Our second method, Test-Time Adaptation, extends this idea by adapting model parameters during inference. It leverages consensus signals from TTAug as pseudolabels, which guide lightweight fine-tuning on test samples without any labeled data. This enables the model to dynamically adjust to domain-specific characteristics while retaining computational efficiency. Our approach consistently outperforms existing test-time scaling methods, such as selfconsistency (Wang et al., 2023b), sample-and-rank (Adiwardana et al., 2020), self-selector (Chen et al., 2024a; Parmar et al., 2025), and self-synthesizer (Li et al., 2025d; Jiang et al., 2023; Wang et al., 2025a; Li et al., 2025b). Furthermore, these improvements do not come with heavy computational cost, allowing our approach to be used in resource-constrained settings. Beyond performance gains, our study reveals two important general insights for test-time scaling: (1) generating multiple candidate answers through input augmentations with greedy decoding is more effective than the commonly-used temperature sampling strategy, and (2) token-level aggregation provides stronger signals than aggregating only at the final-answer level. These findings highlight practical principles for scaling VLMs efficiently at inference. Our experiments across nine diverse benchmarks and multiple VLM architectures confirm that these insights translate into consistent improvements and broad generalization, underscoring the effectiveness and generality of our framework. Our contributions are threefold: (1) We present two efficient test-time scaling methods for small VLMs deployable on consumer GPUs. (2) We provide the first comprehensive analysis of TestTime Augmentation for VLMs, investigating augmentation strategies, aggregation methods, and optimal aggregation layers. Despite being simple and easily integrable technique, its application to multimodal settings remains surprisingly underexplored. (3) We introduce the first Test-Time Adaptation method for multimodal language models, whereas prior work on VLM test-time adaptation has focused primarily on CLIP-based models (Liang et al., 2025; Dong et al., 2025; Ji et al., 2025)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Test-time scaling is paradigm in which current large language models increasingly achieve superior performance by allocating substantial computational resources during inference (Zhang et al., 2025a; Ji et al., 2025). popular test-time scaling strategy is parallel sampling, which generates multiple 2 Efficient Test-Time Scaling for Small Vision-Language Models outputs simultaneously and aggregates them. However, existing parallel sampling methods face several critical limitations that make them impractical for resource-constrained deployments. Most approaches rely on external verifier models or compute-heavy strategies, making them incompatible with the small model paradigm (Zhang et al., 2024; Singh et al., 2025). We address these limitations by proposing two lightweight but effective methods via test-time augmentation. Test-time augmentation (TTAug) improves model robustness and generalization by averaging predictions across augmented views (Shorten & Khoshgoftaar, 2019). Recent work extends TTAug through learnable policies (Lyzhov et al., 2020; Kim et al., 2020; Shanmugam et al., 2021) by optimizing augmentation selection and weighting. However, these active methods typically require labeled datasets to learn optimal policies, limiting their practical applicability. Prior TTAug research for (multimodal) language models (Mashrur et al., 2022; Kamoda et al., 2023) mainly addresses hallucination detection and robustness, not accuracy improvement, and does not treat TTAug as systematic test-time scaling method. Our work closes this gap by extending both non-learnable and learnable TTAug strategies to Vision-Language Models (VLMs), systematically evaluating how augmentation design, aggregation, and scaling affect performance across tasks, and leveraging self-supervised objectives from test-time adaptation literature to avoid reliance on labeled data. Test-time adaptation (TTAdapt) is an emerging paradigm for adapting pretrained models to new data batches during inference by updating model weights or inputs to maximize prediction accuracy without ground-truth labels (Xiao & Snoek, 2024). The choice of optimization target and objective is crucial for adaptation effectiveness. In multimodal learning, most prior TTAdapt work focuses on CLIP-based VLMs (Liang et al., 2025; Dong et al., 2025; Ji et al., 2025), with entropy minimization as the optimization strategy (Shu et al., 2022) and widespread use of self-training with pseudolabels. In language models, TTAdapt is less explored (Dong et al., 2025; Ji et al., 2025). Hübotter et al. (2025) require training datasets and is not source-free, while Huang et al. (2025) extend an existing test-time scaling method called self-consistency (Wang et al., 2023b) for better confidence calibration but suffers from the same limitations of applicability and generalization. Akyürek et al. (2025) explore test-time training with methods similar to ours (i.e., aggregated predictions via hierarchical voting and per-instance adaptation); however, their method is specifically designed for the ARC benchmark and lacks broader applicability. Our universal and source-free TTAdapt method overcomes these limitations by leveraging consensus-based pseudolabeling from our TTAug method."
        },
        {
            "title": "3 METHODS",
            "content": "We propose comprehensive framework for test-time scaling of small Vision-Language Models (VLMs) through two complementary approaches: test-time augmentation (TTAug) and test-time adaptation (TTAdapt). Fig. 1a illustrates our framework, which addresses the fundamental challenge of improving model performance and robustness without requiring additional training data or substantial computational overhead. 3.1 TEST-TIME AUGMENTATION (TTAUG) Our approach leverages input diversity to improve model robustness through systematic aggregation of predictions from semantically equivalent inputs. Given an input consisting of an image and text prompt t, we generate set of augmented versions {(Ii, ti)}N i=1 through semantic-preserving transformations (Sec. 4.3). Each transformation preserves the semantic content essential for multimodal understanding while introducing controlled textual and visual diversity (Sec. 4.4 and 4.5). Our token generation process follows an autoregressive approach where aggregation occurs at each step during generation. Starting with an empty sequence = {}, we iteratively generate tokens. At generation step j, for each augmented input (Ii, ti), the VLM computes the probability distribution over the vocabulary conditioned on the current shared context: pi,j(v) = p(vIi, ti, y<j) = softmax(fθ(Ii, ti, y<j)) (1) where fθ represents the VLM with parameters θ, and y<j = {y1, . . . , yj1} denotes the shared sequence of previously generated tokens. We then aggregate the probability distributions across all augmented inputs through token-level averaging: 1 pj(v) = pi,j(v) (cid:88) (2) 3 Efficient Test-Time Scaling for Small Vision-Language Models The next token is selected greedily from this aggregated distribution: yj = arg max vV pj(v) (3) This selected token yj is then appended to the shared context = {yj}, and the process repeats for the next step. This autoregressive aggregation ensures that each token decision leverages the collective confidence from all augmented views while maintaining single coherent output sequence. This token-level aggregation strategy enables the model to leverage local confidence signals from multiple augmented views at each generation step, combining the strengths of different input representations (Sec. 4.2). Moreover, semantic-preserving input perturbations with greedy decoding yield superior diversity than temperature sampling used in prior test-time scaling methods (Sec. 4.1)."
        },
        {
            "title": "3.2 TEST-TIME ADAPTATION (TTADAPT)",
            "content": "We also introduce learnable variant that adapts model parameters during inference through iterative pseudolabel generation and fine-tuning. Our TTAdapt method operates without requiring labeled data by leveraging the consensus from TTAug as supervision signal. The TTAdapt process optimizes the entire VLM parameter set θ through consensus-driven supervision in an iterative three-stage loop: (1) generate high-confidence pseudolabels using the current model state with TTAug consensus, (2) fine-tune model parameters using these pseudolabels as supervision through efficient training with gradient checkpointing or parameter-efficient methods, and (3) reset to initial weights before processing each new question to prevent catastrophic forgetting. This iterative process allows the model to progressively adapt to the test distribution while maintaining stability through consensus-based pseudolabeling. See Appendix I.7 for implementation details. The TTAug method generates multiple predictions for each test input and aggregates them using token-level averaging to create high-confidence pseudolabels. These pseudolabels represent the collective wisdom of the augmented predictions and serve as training targets for model adaptation. By iteratively refining predictions through TTAug consensus and parameter updates, we enable the model to adapt to test-time distribution shifts while preserving its core capabilities. Through this iterative process, we adapt the model parameters to achieve locally-optimal performance for the specific question type encountered during inference (Sec. 4.6). Our unified framework provides flexibility for different deployment scenarios: TTAug offers immediate improvements without parameter updates, while TTAdapt enables more substantial gains when brief optimization is feasible. We systematically evaluate both approaches across diverse benchmarks and models to understand their effectiveness and computational trade-offs (Sec. 4.7)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct comprehensive experiments to validate the test-time scaling framework presented in the previous section. Each major design decision is explored here, e.g. how can we generate highquality diverse answers, or should we perform aggregation at the level of the final answer or at the token-level, using the SmolVLM2-2.2B (Marafioti et al., 2025) model as the baseline. Our experiments encompass 9 benchmarks covering various task types: visual question answering (VQA) including ChartQA (Masry et al., 2022), OCRBench (Liu et al., 2024), OCRVQA (Mishra et al., 2019), GQA (Hudson & Manning, 2019), and TextVQA (Singh et al., 2019); multiple-choice questions (MCQ) with AI2D (Kembhavi et al., 2016) and MME-RealWorld (Zhang et al., 2025b); yes/no questions using AMBER (Wang et al., 2023a); and image captioning with COCO Captions (Lin et al., 2014). We utilize the evaluation protocols provided by VLMEvalKit (Duan et al., 2024) to ensure standardized and reproducible results. For computational efficiency and fair comparison across all methods, we sample 1000 samples from each benchmark using uniform intervals to maintain representative coverage of the original data distribution while enabling extensive ablation studies. The evaluation metric is accuracy for most benchmarks, with ROUGE-L used specifically for COCO Captions. For comprehensive description of the evaluation metrics, refer to Appendix J. 4 Efficient Test-Time Scaling for Small Vision-Language Models"
        },
        {
            "title": "4.1 DIVERSITY-INDUCEMENT METHODS",
            "content": "Generating diverse, high-quality candidate answers is critical for test-time scaling. We compare two approaches for inducing diversity: Temperature Sampling, and Input Perturbations combined with greedy decoding. Temperature Sampling introduces randomness into the process by sampling from softened probability distribution, while Input Perturbations applies classic semantic-preserving augmentations to inputs (Sec. 4.4 and 4.5), and then decodes greedily. We augment the inputs = 8 times. To evaluate which method is more effective, we consider two test-time scaling strategies: 1 Self-Consistency aggregates candidate answers via majority voting across multiple sampled outputs (Wang et al., 2023b). While effective for tasks where final answers can be parsed, it struggles in creative or open-ended settings where the final answer is not easy to parse. 2 Self-Selector uses the VLM itself as verifier to select one response among the candidates (Chen et al., 2024a; Parmar et al., 2025). This approach extends applicability beyond tasks suited to majority voting. See Appendix I.1 for implementation details. Tab. 1 shows that Input Perturbations with Greedy Decoding outperform Temperature Sampling for generating high-quality candidate responses under both the Self-Consistency and Self-Selector strategies. This approach achieves the largest gains on OCRVQA and GQA, where temperature sampling fails. The theoretical analysis in Appendix shows that greedy decoding with input perturbations maintains higher correlation and better alignment with the model training objective, making it more effective for test-time scaling. Takeaway: Input Perturbations with greedy decoding ultimately performs better than the Baseline or Temperature Sampling. This fundamental insight forms the basis of our method throughout the remainder of the paper. 4.2 AGGREGATION LEVELS Table 1: Comparison of diversity-inducement methods compared to the Baseline. Input Perturbation outperforms Temperature Sampling. Temperature e Sampling 1 2 ChartQA 74.2 OCRBench 72.9 0.0 OCRVQA 0.0 GQA 73.2 TextVQA AI2D 68.5 MME-RW 27.8 68.7 AMBER 9.1 COCO Mean 43.8 74.4 72.6 0.0 0.0 72.6 3.1 26.2 70.4 8.2 36.4 73.4 71.9 0.0 0.0 71.6 69.2 26.4 64.5 8.4 42. Input Perturbation 1 74.8 72.7 12.0 7.6 72.3 3.6 30.8 72.7 21.2 40.9 2 70.9 73.1 4.5 3.7 72.9 66.6 29.6 67.0 13. 44.6 We now compare different aggregation levels for test-time scaling: Answer-level versus Token-level aggregation. Existing test-time scaling methods predominantly employ answer-level aggregation with temperature sampling for diversity inducement (Zhang et al., 2025a). However, given that input perturbations with greedy decoding provide superior diversity inducement, we evaluate answer-level versus token-level aggregation using this improved diversity-inducement method for comparison. Building on the 1 Self-Consistency and 2 Self-Selector test-time scaling strategies evaluated in Sec. 4.1, we introduce two additional Answer-level aggregation methods to complete our comparison of four answer-level approaches against our Token-level approach. We compare our approach against existing test-time scaling methods that can potentially operate without external model dependencies. 3 Sample-and-Rank. Self-Consistency ignores the models internal signals for selection; majority voting treats all reasoning traces equally, ignoring quality variations (Wang et al., 2025b). Sampleand-Rank (Adiwardana et al., 2020), leverages next-token distribution statistics to assess response quality by selecting the response with the highest log probability, arg max log p(y). 4 Self-Synthesizer. The selection of only one answer, as in previous strategies, ignores information from other responses. To combine potentially correct parts from different responses, we use the tested VLM to aggregate responses into one coherent final answer (Li et al., 2025d; Jiang et al., 2023; Wang et al., 2025a; Li et al., 2025b). See Appendix I.2 for implementation details. Nevertheless, all of these answer-level aggregation methods have critical limitations. First, global measures like confidence obscure confidence fluctuations at local reasoning steps, which can provide valuable signals for estimating response quality. Averaging across entire sequences masks critical reasoning breakdowns that occur at intermediate steps. Additionally, global measures require generat5 Efficient Test-Time Scaling for Small Vision-Language Models ing complete responses before calculation, preventing early stopping of low-quality generations and resulting in computational inefficiency. They generate constant number of responses per question rather than adaptively distributing computational budget based on response agreement. Moreover, small VLMs also often lack sufficient synthesis capabilities for reliable response combination. 5 Token-level aggregation with simple averaging. Our Token-level approach aggregates the predictions at each step using token-level aggregation of the final logits, as defined in Eq. 2. l B Answer-level Table 2: Comparison of Answer-level versus Token-level aggregation methods. Token-level aggregation outperforms all other approaches. Tab. 2 demonstrates the effectiveness of Token-level aggregation compared to the Answer-level methods. This consistent advantage validates our hypothesis that token-level aggregation preserves valuable local confidence information that global answer-level methods discard. Particularly notable are the improvements on OCRVQA, GQA, and COCO, where the baseline model struggles, indicating that tokenlevel aggregation effectively leverages augmentation diversity to recover from initial prediction failures. The methods ability to outperform answer-level approaches, such as Self-Synthesizer, despite their access to the full models reasoning capabilities, underscores the fundamental advantage of preserving local confidence signals during generation rather than attempting post-hoc response combination. Appendix provides mathematical analysis of this phenomenon. Appendix presents experiments using different Token-level aggregation methods, including entropy-weighted averaging, majority voting, and most confident token. Finally, Appendix shows that aggregation of earlier layer outputs can produce better results for some tasks. ChartQA 74.2 74.8 70.9 61.1 72.8 75.6 OCRBench 72.9 72.7 73.1 60.9 71.1 73.4 OCRVQA 0.0 11.8 5.8 0.0 GQA TextVQA 73.2 72.3 72.9 61.6 71.6 72.8 AI2D 66.6 69.9 68.0 68.8 MME-RW 27.8 30.8 29.6 29.0 29.2 31.1 68.7 72.7 67.0 58.9 75.8 75.4 AMBER 29.5 15.9 9.1 COCO 43.8 40.9 44.6 38.9 46.8 47.9 21.2 13.0 12.0 7. 0.2 0.0 3.3 0.0 4.5 3.7 Token Mean 68. 8.6 3.6 1 2 3 5 Takeaway: Token-level aggregation consistently outperforms Answer-level aggregation. This validates our test-time augmentation method as more practical alternative to existing test-time scaling approaches that rely on Answer-level approaches based on Selection or Synthesis. We use Token-level aggregation with simple averaging at the final logits for all subsequent experiments. 4.3 NUMBER OF AUGMENTATIONS We study how performance scales with the number of augmented inputs to understand the optimal balance between computational cost and accuracy. Augmentation counts range from 1 (baseline) to 64 with simple averaging aggregation. This analysis clarifies diminishing returns in test-time scaling and provides practical guidance for deployment scenarios with varying computational budgets. Fig. 2 reveals diverse scaling behaviors across benchmarks, reflecting task-specific characteristics. Benchmarks showing monotonic improvement with saturation (OCRVQA, AMBER, MME-RealWorld) Figure 2: Performance scaling as function follow established test-time scaling patterns (Snell of the number of augmentations. Performance et al., 2025; Brown et al., 2025; Wu et al., 2024), gains generally plateau after 16 augmentations. with performance increasing steadily before plateauing. In contrast, several benchmarks exhibit non-monotonic curves (ChartQA, COCO, GQA) where performance peaks at intermediate augmentation counts before declining due to the consistencydiversity tradeoff (Geiping et al., 2023). This decline probably occurs because excessive augmentation introduces outlier predictions and semantic drift that degrade aggregated signal quality, as simple token-level averaging assumes equal validity across augmented predictions. Mixed behaviors (OCRBench, TextVQA, AI2D) show irregular patterns with task-specific characteristics. Efficient Test-Time Scaling for Small Vision-Language Models Takeaway: The average performance curve (dashed line) indicates peak performance at 16 augmentations, which we adopt for subsequent experiments. This translates to peak GPU memory usage of 8.75 GB (1.9 increase from 4.60 GB baseline) and an inference time of 4.77 per query (3.33 increase from 1.43 baseline), when using parallel batch inference on an NVIDIA A100 GPU. For detailed computational overhead analysis across different augmentation counts, refer to Appendix E."
        },
        {
            "title": "4.4 TEXT AUGMENTATION METHODS",
            "content": "We now compare different text augmentation strategies to understand the trade-offs between quality, practicality, and computational overhead in our resource-constrained setting. 0 Image-only uses classical image augmentations (Sec. 4.5) without text augmentation, serving as control. 1 4 apply the same image augmentations along with their respective text strategies. 1 AugGPT uses ChatGPT (Achiam et al., 2023) to generate high-quality paraphrases, to evaluate the ability of high-capacity finetuned paraphraser models distilled for our scenario (Dai et al., 2025). This high-quality paraphrasing augmentation using state-of-the-art external models, but it is not practical as it requires external models in resource-constrained deployment scenarios. 2 Self-paraphrasing uses the LLM of the VLM to paraphrase the input prompt. Since small VLMs cannot reliably do this in one shot, we split the prompt into sentences and paraphrase each with structured generation to obtain fixed number of variants. The final paraphrased prompt is the concatenation of these outputs. This approach maintains consistency with the target models internal linguistic patterns while remaining self-contained. See Appendix I.3 for implementation details. 3 Classical augmentation uses simple and fast semantic-preserving augmentations sequentially and randomly with minimal cost (Ma, 2019; Aepli & Sennrich, 2022). Keyboard errors simulate common typing mistakes by replacing characters with nearby keys leveraging one-key distance to generate realistic character substitutions. Word splitting introduces spacing variations within compound words. Word deletion removes individual words. Sentence reordering swaps adjacent sentences. Consistency enforcement is applied by appending the original prompt after each augmented version, structured as \"In other words,\" followed by the original prompt, mirroring the alpha blending technique in AugMix (Hendrycks et al., 2020). We report ablation study results without consistency enforcement using the classical augmentation method in the 4 column of Tab. 3; all other columns ( 1 , 2 , 3 ) employ consistency enforcement technique. Tab. 3 shows that self-paraphrasing achieves superior performance by leveraging model-aligned augmentations, as the models own weights influence how prompts are generated, resulting in augmentations that exhibit superior alignment with the models internal representations. This approach creates linguistic patterns within the training manifold, leading to better-calibrated confidence estimates during token-level aggregation. Consistency enforcement proves critical for semantic coherence, with large drops observed in the ablation study, though notable exceptions occur in GQA and MME-RealWorld where diversity outweighs consistency. Classical augmentations remain competitive with minimal computational overhead, making them the most practical choice for resourceconstrained deployment. Their similar performance to self-paraphrasing suggests simple perturbations provide sufficient diversity for our purposes. Table 3: Comparison of text augmentation strategies. Self-Paraphrasing 2 and Classical Augmentations 3 consistently perform best. l B 0 2 3 4 0.0 0.0 ChartQA 74.2 74.7 76.9 76.6 76.1 71.4 OCRBench 72.9 73.3 73.5 72.8 73.7 70.6 0.0 OCRVQA 0.0 GQA 31.2 0.0 TextVQA 73.2 74.2 73.5 74.0 72.4 63.9 AI2D 68.5 69.8 69.9 68.4 69.6 63.9 MME-RW 27.8 26.6 30.0 25.9 31.9 32.1 68.7 64.7 68.8 72.9 75.9 60.0 AMBER 20.6 46.2 16.9 13.2 9.1 COCO 12.6 5. 0.0 10.0 2.6 0.0 8.4 Mean 43.8 43.5 46.2 48.5 48.3 45.1 Takeaway: Self-paraphrasing Classical AugGPT. Consistency enforcement is critical for reliable performance. For the remaining experiments, we use classical augmentation with consistency enforcement to balance accuracy and efficiency. 7 Efficient Test-Time Scaling for Small Vision-Language Models 4."
        },
        {
            "title": "IMAGE AUGMENTATION METHODS",
            "content": "We evaluate three different image augmentation strategies to understand their effectiveness for multimodal test-time scaling: classical transformations, established methods, and generative approaches. 0 Text-only uses classical text augmentation (Sec. 4.4) without image augmentation, serving as control. 1 3 apply the same text augmentation along with their respective image strategies. 1 Classical augmentations apply traditional computer vision transformations including brightness/contrast adjustments, rotation, blurring, noise injection, and geometric distortions, shown useful in other vision-language tasks (Vendrov et al., 2016). We test three augmentation intensity levels: Low (conservative), Medium (moderate), and High (aggressive) to explore the diversity-consistency trade-off. See Appendix I.4 for detailed implementation specifications. 2 AugMix (Hendrycks et al., 2020) employs mixing strategy that combines multiple augmentation chains with convex combinations, originally designed for robustness in image classification tasks. 3 Generative augmentations use FLUX.1-dev (Labs et al., 2025) to create semantically similar but visually distinct image variants. However, this approach excludes text-containing images to prevent OCR corruption. Also, it requires external diffusion models, making it impractical for resource-constrained deployments. See Appendix I.5 for detailed implementation specifications. Table 4: Comparison across different image augmentation strategies. Classical Augmentations , , perform the best. Tab. 4 reveals several key insights about image augmentation strategies. Classical augmentations with high and low strengths marginally outperform medium strength augmentations. This nonmonotonic relationship reflects the fundamental diversity-consistency trade-off: low strength preserves semantic coherence but provides limited diversity; high strength introduces beneficial variance without excessive semantic drift (Geiping et al., 2023); while medium strength falls into suboptimal region where augmentations disrupt model confidence without compensating diversity benefits. AugMix performs competitively but falls short of classical methods, suggesting that the principled mixing strategy designed for unimodal classification may not align with the token-level aggregation in VLMs. Generative augmentations underperform despite their semantic richness, primarily because text-containing images must be excluded, reducing the effective augmentation coverage. ChartQA 74.2 75.8 77.0 76.4 76.1 74.1 75.7 OCRBench 72.9 73.1 73.7 73.3 73.7 72.4 65.3 OCRVQA 0.0 13.5 12.1 10.6 12.6 12.9 12.0 GQA 0.0 2.0 4.1 3.7 5.5 3.1 2.5 TextVQA 73.2 73.0 72.6 73.3 72.4 72.4 71.6 AI2D 68.5 68.1 69.1 69.0 69.6 68.9 67.0 MME-RW 27.8 31.6 31.8 32.5 31.9 32.1 31.1 AMBER 68.7 77.3 77.0 75.9 75.9 77.3 76.2 9.1 19.0 17.8 17.1 16.9 17.8 18.0 COCO 43.8 48.2 48.3 48.0 48.3 47.9 46.6 e e 3 Mean 0 For modality-wise decomposition of TTAug performance gains, see Appendix F; for representative samples of augmented inputs with classical methods and corresponding outputs, see Appendix K. Takeaway: Classical high/low strength augmentations outperform AugMix and generative approaches; with medium strength falling into suboptimal diversity-consistency trade-off. Thus, we use high-strength classical augmentations for all subsequent experiments. 4.6 TEST-TIME ADAPTATION METHODS While TTAug provides improvements, test-time adaptation (TTAdapt) extends this framework by optimizing learnable components during inference. Unlike conventional test-time scaling that generates and selects among multiple candidate responses, our approach directly optimizes model behavior using selfor semi-supervised objectives. We investigate two different adaptation strategies targeting distinct components of the aggregation pipeline, each with unique optimization objectives. 1 Aggregation weights optimization learns adaptive token-wise weights wi,j to replace the uniform averaging scheme in Eq. 2. At each generation step j, we initialize learnable parameters as wj RN and optimize them through gradient descent to minimize the marginal entropy H(pj) = (cid:80) vV pj(v) log pj(v) of the weighted aggregated distribution by performing multiple micro-steps per token to achieve convergence. This approach requires minimal computational overhead with 8 Efficient Test-Time Scaling for Small Vision-Language Models compact computational graph, making it suitable for real-time deployment. Marginal entropy minimization represents the dominant optimization paradigm in test-time adaptation for CLIP-based models (Shu et al., 2022; Liang et al., 2025). We include this method as an ablation study and as computationally efficient alternative to our main adaptation approach. See Appendix I.6 for details. 2 Model parameter adaptation implements the iterative pseudolabel generation and fine-tuning framework detailed in Sec. 3.2. 2 1 Baseline TTAug Table 5: Performance comparison of test-time adaptation strategies. Model parameter adaptation 2 yields the best performance. Tab. 5 shows clear performance differences among adaptation methods with distinct efficiencyperformance trade-offs. Aggregation weights optimization performs on par with TTAug, mainly improving benchmarks that require precise confidence calibration (e.g., AMBER, TextVQA), where adaptive weighting highlights high-quality predictions. This supports findings that TTAdapt via marginal entropy minimization is not more effective than TTAug for CLIP-based VLMs (Farina et al., 2024). Its average performance matches TTAug, but it notably fixes simple averagings underperformance on TextVQA, outperforming the baseline model on all benchmarks. Model parameter adaptation delivers the strongest overall gains, particularly excelling on COCO. Given its superior performance, we refer to model parameter adaptation as TTAdapt throughout this paper. However, performance occasionally degrades on specialized benchmarks with strong baseline capabilities, indicating that aggressive parameter adaptation can disrupt carefully calibrated domain-specific knowledge. This pattern suggests that adaptation intensity should be task-dependent, conservative for well-calibrated domains where the base model already performs well, and more aggressive for challenging distributions where consensus-based supervision provides reliable guidance. 74.2 ChartQA 72.9 OCRBench 0.0 OCRVQA 0.0 GQA 73.2 TextVQA AI2D 68.5 MME-RW 27.8 68.7 AMBER 9.1 COCO 76.7 70.5 13.8 13.5 70.5 67.4 31.4 72.8 35.9 76.1 73.7 12.6 5.5 72.4 69.6 31.9 75.9 16.9 76.1 73.0 11.9 5.2 74.2 69.7 30.9 76.9 16.4 Mean 48.3 48. 43.8 50.3 Takeaway: Model adaptation achieves superior gains through consensus-based learning. Aggregation weight optimization provides an efficient middle ground with minimal computational overhead. 4.7 CROSS-MODEL GENERALIZATION Finally, we test our methods generalization to other VLMs by applying the SmolVLM2-2.2B configuration (greedy decoding, 16 classical augmentations, token-level averaging) to diverse architectures and parameter scales. See Appendix for more details. Fig. 3 shows TTAug gains across model families and parameter scales. The best performance gains are found for SmolVLM2-2.2B, but we find consistent improvements across different architectures and scales. Key findings include: (1) Although optimal hyperparameters vary across models due to differences in training data, architecture, and training augmentations, our method generalizes well and Figure 3: Improvements across different modprovides improvements. (2) Contrary to expectaels, demonstrating cross-model generalization. tions, TTAug effectiveness does not correlate with model size, but rather with model family and architectural similarity. Transfer is stronger within model families and among models with similar parameter counts, indicating that both architecture and capacity matter. Even with suboptimal hyperparameters, our methods yield robust improvements, though dedicated tuning is recommended for best results. See Appendix for more detailed results. Takeaway: Despite hyperparameters being optimized for SmolVLM2-2.2B, our methods provide consistent improvements across diverse models, though transferability varies by family and size. 9 Efficient Test-Time Scaling for Small Vision-Language Models"
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose two efficient test-time scaling methods, Test-Time Augmentation and Test-Time Adaptation. Our comprehensive experiments demonstrate that both methods consistently improve performance by outperforming existing test-time scaling approaches with minimal overhead, making them suitable for resource-constrained environments. Our work provides systematic way to tune hyperparameters for given model, though optimal strategies remain taskand model-dependent."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "Dim P. Papadopoulos was supported by the DFF Sapere Aude Starting Grant \"ACHILLES\". Desmond Elliott was supported by the European Unions Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM) and research grant (VIL53122) from Villum Fonden. This work was supported by the Pioneer Centre for AI, DNRF grant number P1. We would like to thank Marco Schouten, Thanos Delatolas and Stella Frank for proofreading and insightful discussions."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the replicability of our findings, we release our code, allowing the research community to reproduce our results and build upon our contributions. Our experimental setup exclusively employs publicly accessible models, ensuring that all resources are readily obtainable by other researchers. We provide comprehensive details regarding all prompts and hyperparameters utilized across our experiments in Appendix I. Additionally, Appendix contains thorough descriptions of the benchmarks and evaluation metrics employed in our study. All evaluation benchmarks utilized in this work are established and widely-used standards within the field. We include references to these resources to facilitate easy access for interested researchers. Our commitment to transparency extends beyond code release, as we meticulously detail every aspect of our experimental methodology to enable faithful reproduction of our work."
        },
        {
            "title": "AUTHOR STATEMENT ON THE USE OF LARGE LANGUAGE MODELS",
            "content": "During the preparation of this paper, large language models were used solely for minor grammar and language polishing. They were not used for research ideation, experiment design, analysis, or writing of scientific content. All research contributions are the sole responsibility of the authors."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. Daniel Adiwardana, Minh-Thang Luong, David So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards human-like opendomain chatbot. arXiv:2001.09977, 2020. Noëmi Aepli and Rico Sennrich. Improving zero-shot cross-lingual transfer between closely related languages by injecting character-level noise. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 40744083, Dublin, Ireland, May 2022. Association for Computational Linguistics. Unsloth AI, Daniel Han-Chen, and Michael Han-Chen. Unsloth. https://github.com/ unslothai/unsloth, 2025. Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. In Forty-second International Conference on Machine Learning, 2025. 10 Efficient Test-Time Scaling for Small Vision-Language Models Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai, 2025. Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, ShaoYen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal. Lvlm-intrepret: An interpretability tool for large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81828187, 2024. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. Zenab Bosheah and Vilmos Bilicki. Challenges in generating accurate text in images: benchmark for text-to-image models on specialized content. Applied Sciences, 15(5), 2025. Bradley Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information, 11 (2), 2020. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv:2501.17811, 2025. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language models. In ICML 2024 Workshop on In-Context Learning, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024b. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024. Sewhan Chun, Jae Young Lee, and Junmo Kim. Cyclic test time augmentation with entropy weight method. In Uncertainty in Artificial Intelligence, pp. 433442. PMLR, 2022. Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Fang Zeng, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li. AugGPT: Leveraging ChatGPT for Text Data Augmentation . IEEE Transactions on Big Data, 11(03):907918, June 2025. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 91104, 2025. 11 Efficient Test-Time Scaling for Small Vision-Language Models Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, and Olga Fink. Adapting vision-language models without labels: comprehensive survey. arXiv:2508.05547, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1119811201, 2024. Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano Mancini, and Elisa Ricci. Frustratingly easy test-time adaptation of vision-language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Goldstein, and Andrew Gordon Wilson. How much data are augmentations worth? an investigation into scaling laws, invariance, and implicit regularization. In The Eleventh International Conference on Learning Representations, 2023. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 13211330. PMLR, 2017. Dan Hendrycks and Kevin Gimpel. baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017. Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. spacy: Industrialstrength natural language processing in python, 2020. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via self-calibration. arXiv:2503.00031, 2025. Jonas Hübotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. Efficiently learning at testIn The Thirteenth International Conference on Learning time: Active fine-tuning of LLMs. Representations, 2025. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Yixin Ji, Juntao Li, Yang Xiang, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, and Min Zhang. survey of test-time compute: From intuitive inference to deliberate reasoning, 2025. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1416514178, Toronto, Canada, July 2023. Association for Computational Linguistics. Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, and Kentaro Inui. Test-time augmentation In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the for factual probing. Association for Computational Linguistics: EMNLP 2023, pp. 36503661, Singapore, December 2023. Association for Computational Linguistics. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pp. 235251. Springer, 2016. 12 Efficient Test-Time Scaling for Small Vision-Language Models Ildoo Kim, Younghoon Kim, and Sungwoong Kim. Learning loss for test-time augmentation. Advances in neural information processing systems, 33:41634174, 2020. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025a. Cheryl Li, Tianyuan Xu, and Steven Y. Guo. Reasoning-as-logic-units: Scaling test-time reasoning in large language models through logic unit alignment. In Forty-second International Conference on Machine Learning, 2025b. Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, and Dimitris N. Metaxas. The hidden life of tokens: Reducing hallucination of large vision-language models via visual information steering. In Forty-second International Conference on Machine Learning, 2025c. Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, and Tuo Zhao. Llms can generate better answer by aggregating their own responses. arXiv:2503.04104, 2025d. Jian Liang, Ran He, and Tieniu Tan. comprehensive survey on test-time adaptation under distribution shifts. International Journal of Computer Vision, 133(1):3164, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), December 2024. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, et al. Ovis2.5 technical report. arXiv:2508.11737, 2025. Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Molchanov, and Dmitry Vetrov. Greedy policy search: simple baseline for learnable test-time augmentation. In Conference on uncertainty in artificial intelligence, pp. 13081317. PMLR, 2020. Edward Ma. Nlp augmentation. https://github.com/makcedward/nlpaug, 2019. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve noyan, Elie Bakouch, Pedro Manuel Cuenca Jiménez, Cyril Zakka, Loubna Ben allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. SmolVLM: Redefining small and efficient multimodal models. In Second Conference on Language Modeling, 2025. 13 Efficient Test-Time Scaling for Small Vision-Language Models Akib Mashrur, Wei Luo, Nayyar A. Zaidi, and Antonio Robles-Kelly. Semantic multi-modal reprojection for robust visual question answering. In 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA), pp. 16, 2022. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pp. 1688816905. PMLR, 2022. Nostalgebraist. Interpreting gpt: LessWrong, August 31, 2020, https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ the logit lens. 2020. interpreting-gpt-the-logit-lens. Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, et al. Plangen: multi-agent framework for generating planning and reasoning trajectories for complex problem solving. arXiv:2502.16111, 2025. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Divya Shanmugam, Davis Blalock, Guha Balakrishnan, and John Guttag. Better aggregation in test-time augmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12141223, 2021. Connor Shorten and Taghi Khoshgoftaar. survey on image data augmentation for deep learning. Journal of big data, 6(1):148, 2019. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:1427414289, 2022. Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 83178326, 2019. Joykirat Singh, Tanmoy Chakraborty, and Akshay Nambi. Self-evolved preference optimization for enhancing mathematical reasoning in small language models. arXiv:2503.04813, 2025. Ray Smith. An overview of the tesseract ocr engine. In ICDAR 07: Proceedings of the Ninth International Conference on Document Analysis and Recognition, pp. 629633, Washington, DC, USA, 2007. IEEE Computer Society. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. Gemma Team. Gemma: Open models based on gemini research and technology. arXiv:2403.08295, 2024. InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. 14 Efficient Test-Time Scaling for Small Vision-Language Models Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 45934601, Florence, Italy, July 2019. Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR, 2016. Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances In The Thirteenth International Conference on Learning large language model capabilities. Representations, 2025a. Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv:2311.07397, 2023a. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 72017211, 2022. Weiqin Wang, Yile Wang, and Hui Huang. Ranked voting based self-consistency of large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1441014426, Vienna, Austria, July 2025b. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. Brandon Willard and Rémi Louf. Efficient guided generation for large language models. arXiv:2307.09702, 2023. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Zehao Xiao and Cees GM Snoek. Beyond model adaptation at test time: survey. arXiv:2411.03687, 2024. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Selfevaluation guided beam search for reasoning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, ChiaChih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, and Ran Xu. xgen-mm (blip-3): family of open large multimodal models. CoRR, abs/2408.08872, 2024. 15 Efficient Test-Time Scaling for Small Vision-Language Models Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, et al. survey on test-time scaling in large language models: What, how, where, and how well? arXiv:2503.24235, 2025a. YiFan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, and Rong Jin. MME-realworld: Could your multimodal LLM challenge high-resolution real-world scenarios that are difficult for humans? In The Thirteenth International Conference on Learning Representations, 2025b. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1563715653, Bangkok, Thailand, August 2024. Association for Computational Linguistics. 16 Efficient Test-Time Scaling for Small Vision-Language Models THEORETICAL ANALYSIS OF DIVERSITY-INDUCEMENT METHODS Formally, let each candidate response have latent quality Q(y). The model also assigns an internal signal, such that the confidence score, S(y), which is used for candidate selection. In practice, since the true quality Q(y) is unknown at test time, the practical selector chooses the candidate = arg max yY S(y). We can approximate the joint distribution of (Q, S) as bivariate distribution. This distribution has means µQ and µS, variances σ2 S, and correlation ρ = Corr(Q, S). The expected quality of the selected candidate can then be expressed as: and σ2 E[Q(y)] µQ + ρσQkN , where kN = (cid:82) φ(z) Φ(z)N 1 dz is the expected maximum of standard normal variables. Here, φ(z) is the standard normal probability density function, and Φ(z) is the standard normal cumulative distribution function. Notably, kN grows slowly as the candidate pool size increases. Temperature sampling generates candidates with high variance in quality, σQ. However, these samples are often drawn from low-likelihood regions, where the models internal confidence S(y) is poorly aligned with the true quality Q(y). As result, the correlation ρ between and is small, which leads to weak scaling as more candidates are added. In contrast, input perturbations combined with greedy decoding produce candidates with lower variance but higher mean quality µQ. More importantly, the correlation ρ is stronger, because these responses remain on the likelihood manifold where the model was trained to assign high confidence. This difference arises from the training objective of language models: next-token prediction under maximum likelihood estimation. During training, the model is optimized for greedy decoding, and temperature sampling is not simulated (e.g., there is no Gumbel-softmax trick in training), making temperature sampling less natural for the model. Furthermore, language models are often miscalibrated, especially after post-training (Achiam et al., 2023). This miscalibration further reduces the correlation ρ for candidates from temperature sampling. Under confidence-based selection, the product ρσQ is provably larger for greedy decoding with input perturbations than for temperature sampling. This establishes greedy decoding with augmented inputs as superior mechanism for generating diverse candidates in test-time scaling. THEORETICAL ANALYSIS OF TOKEN-LEVEL AGGREGATION VS. ANSWER-LEVEL AGGREGATION Consider generating response of length tokens. Let pt denote the probability of the base model generating the correct token at step given the correct prefix, with 0 < pmin pt pmax < 1. Token-level selection. At each step t, 2 candidate tokens are generated. selector with accuracy st (probability of selecting the correct token if available) yields correctness probability qt = st (cid:2)1 (1 pt)N (cid:3). Thus, the overall correctness probability is Ptoken = (cid:89) t= qt. Answer-level selection. independent responses are generated. selector with accuracy (probability of selecting the fully correct response if available) yields correctness probability given by (cid:32) Panswer = 1 1 (cid:33)N . (cid:89) t=1 pt Theorem. Assume there exists δ > 0 such that qt (1 + δ)pt for all t. Then for sufficiently large , token-level selection achieves higher expected correctness probability, Ptoken > Panswer. 17 Efficient Test-Time Scaling for Small Vision-Language Models Proof. From the assumption qt (1 + δ)pt: Ptoken (1 + δ)T (cid:89) t= pt = (1 + δ)T Pcorrect where Pcorrect = (cid:81)T t=1 pt. For answer-level selection: since 1 (1 x)N for [0, 1]. Comparing the two: Panswer Pcorrect"
        },
        {
            "title": "Ptoken\nPanswer",
            "content": "(1 + δ)T Pcorrect sN Pcorrect = (1 + δ)T sN . Since δ > 0, (1 + δ)T grows exponentially with , while sN is constant. Therefore, for sufficiently large : (1 + δ)T sN > 1 = Ptoken > Panswer. Feasibility of qt (1 + δ)pt. The condition holds if: st (1 + δ) pt 1 (1 pt)N Since 1 (1 pt)N > pt for 2 and pt < 1, the right-hand side < 1. Thus, there exists st < 1 satisfying the inequality. For typical pt (0.5, 0.99) and 2, reasonable st ( 0.7 0.95) suffice. Conclusion. Token-level selection achieves superior performance because it corrects errors immediately at each generation step, preventing error propagation through the sequence. The per-token improvement factor (1 + δ) compounds multiplicatively across steps. In contrast, answer-level selection suffers from exponential decay in correctness probability ((cid:81) pt) and provides only constant-factor improvement (sN ) through response selection. This exponential scaling with sequence length means that token-level aggregation provides rapidly growing advantage as responses become longer, making it especially effective for reasoning tasks such as chain-of-thought and thinking models. In these settings, each token represents step in the reasoning process, so the ability to correct errors at every step prevents error accumulation and leads to much higher overall correctness compared to answer-level selection, whose benefits do not scale with sequence length. Also, the superiority of increased granularity aligns with empirical observations that process reward models outperform outcome reward models (Lightman et al., 2023), and reasoning step-wise approaches like step-level self-evaluation (Xie et al., 2023) and REBASE (Wu et al., 2024) surpass answer-level methods. However, these reasoning step-wise strategies remain limited to problems where reasoning steps can be clearly defined and still fall short of token-level granularity. But, they exemplify general trend: increased granularity yields better performance in test-time scaling. For autoregressive generation with imperfect selectors, token-level selection achieves higher expected correctness than answer-level selection when the token selectors provide consistent multiplicative improvement over base probabilities and the response length is sufficiently large. The critical advantage comes from per-step error correction that mitigates compounding errors."
        },
        {
            "title": "C AGGREGATION METHODS",
            "content": "We compare different token-level aggregation methods for test-time augmentation. Simple averaging uniformly weights all augmented predictions by computing the arithmetic mean of probability distributions across all augmented inputs, as in Eq. 2, pj(v) = 1 i=1 pi,j(v). (cid:80)N Entropy-weighted averaging assigns higher weights to more confident predictions by computing the entropy Hi,j = (cid:80) vV pi,j(v) ln pi,j(v) for each augmented input at step j, deriving weights Efficient Test-Time Scaling for Small Vision-Language Models wi,j = eHi,j / (cid:80)N (cid:80)N i=1 wi,jpi,j(v) (Chun et al., 2022). k=1 eHk,j through softmax over negative entropy, and aggregating as pj(v) = Majority voting aggregates predictions by selecting the token that receives the most votes across augmented inputs. For each vocabulary token at step j, we compute the vote count cj(v) = (cid:80)N I[arg maxuV pi,j(u) = v], where I[] is the indicator function. The final token is selected as yj = arg maxvV cj(v), choosing the vocabulary token with the highest vote count across all augmented predictions (Farina et al., 2024). i= Most confident token method selects the token with the highest predicted probability across all augmented inputs, yj = arg maxi,v pi,j(v). Since the predicted probability offers noisy proxy for confidence as shown by Guo et al. (2017), this approach effectively chooses the most confident token across all augmentations (Hendrycks & Gimpel, 2017). EW Av. No TTA Maj. Vote Most Conf. Simple Av. Table 6: Comparison of token-level aggregation methods for test-time augmentation. The experimental results in Tab. 6 reveal that averaging-based methods consistently outperform discrete voting approaches, challenging the widespread adoption of majority voting in established test-time scaling methods like self-consistency (Wang et al., 2023b). This performance hierarchy reflects fundamental differences in handling prediction uncertainty and model calibration: averaging-based approaches leverage continuous probability distributions from all augmented inputs, preserving valuable confidence information that discrete methods discard, while the majority voting and the most confident selection rely on discrete decisions from poorly calibrated predictions (Achiam et al., 2023). Simple averaging demonstrates superior robustness compared to entropy-weighted averaging, suggesting that equal weighting provides better stability than confidencebased weighting given the miscalibration issues in language models. But, confidence-based weighting can be beneficial when the models internal confidence aligns well with true prediction quality. 74.2 ChartQA 72.9 OCRBench 0.0 OCRVQA 0.0 GQA 73.2 TextVQA AI2D 68.5 MME-RW 27.8 68.7 AMBER 9.1 COCO 76.6 73.4 11.4 4.3 73.3 68.8 31.0 74.6 14.6 75.6 73.4 11.8 5.8 72.8 68.8 31.1 75.4 15.9 73.6 72.0 3.5 6.1 70.5 68.7 29.5 72.3 14. 74.8 72.2 9.0 3.4 71.5 68.7 30.4 71.4 18.4 Mean 47.9 45.6 47.6 46. 43.8 Takeaway: Averaging-based aggregation outperforms discrete selection methods, with simple averaging achieving the best overall performance. Continuous probability aggregation preserves valuable uncertainty information that discrete voting methods discard."
        },
        {
            "title": "D AGGREGATION IN EARLY LAYERS",
            "content": "To understand the optimal point for feature aggregation within the model architecture, we systematically evaluate aggregation at different transformer layers rather than exclusively at the final output logits. Instead of averaging probability distributions from the final layer, we aggregate hidden representations from intermediate layers and continue forward propagation through the remaining layers using the aggregated features. Formally, for aggregation at layer ℓ, we compute the averaged hidden states hℓ,j = 1 i=1 hi,ℓ,j across all augmented inputs at generation step j, then feed this aggregated representation through layers ℓ + 1 to to produce the final token probabilities. This approach investigates whether early semantic representations or late linguistic features provide superior aggregation targets for multimodal understanding. (cid:80)N 19 Efficient Test-Time Scaling for Small Vision-Language Models Figure 4: Performance across aggregation layers. Each subplot shows accuracy as function of the transformer layer where feature aggregation occurs. Different benchmarks exhibit distinct optimal aggregation points: later layers favor language-heavy tasks (ChartQA, TextVQA), while earlier layers benefit visual reasoning tasks (OCRVQA, GQA). The experimental results in Fig. 4 reveal task-dependent variations in optimal aggregation layers, exposing fundamental differences in how VLMs process multimodal information across different reasoning types. Three distinct patterns emerge that reflect the hierarchical nature of multimodal understanding in transformer architectures. Late-layer preference for linguistic reasoning. Language-heavy benchmarks, including ChartQA, OCRBench, and TextVQA, consistently achieve optimal performance when aggregating at later layers (layers 18-24), with monotonic improvement as aggregation approaches the final output. This pattern aligns with established findings from logit lens analysis (Nostalgebraist, 2020), where later layers increasingly specialize in linguistic refinement and task-specific formatting. Recent work by Chuang et al. (2024) demonstrates that factual knowledge progressively accumulates in higher transformer layers, with later layers exhibiting stronger factual representations than earlier ones when contrasted through layer-wise decoding strategies. This hierarchical knowledge encoding suggests that deeper layers contain more refined and task-specific information essential for accurate linguistic reasoning. For tasks requiring precise text extraction and numerical reasoning, the specialized linguistic representations in deeper layers provide more reliable aggregation targets than earlier semantic features. Early-layer advantage for visual reasoning. Conversely, visually-intensive benchmarks like OCRVQA and GQA demonstrate superior performance when aggregating at earlier layers (layers 6-12), with performance degrading as aggregation moves toward final layers. This counterintuitive finding reflects the models information processing hierarchy: early layers capture rich multimodal semantic representations before aggressive compression into linguistic tokens. Recent work on visual information steering by Li et al. (2025c) reveals that visual information gradually attenuates through transformer layers, with genuine visual tokens losing prominence as language priors dominate in deeper layers. This gradual visual information loss phenomenon explains why early aggregation preserves critical visual details that become diluted in later layers optimized for autoregressive text generation. The early excitation pattern observed in multimodal models (Li et al., 2025c) further supports this finding, showing that semantically meaningful visual tokens achieve peak activation in penultimate or earlier layers rather than the final output layer. For tasks requiring complex visual understanding and spatial reasoning, these early semantic representations retain critical visual details that are progressively lost in later transformer layers. Task-specific optimal points. Benchmarks like AI2D, AMBER, and COCO Captions exhibit intermediate optimal points around layers 10-16, suggesting these tasks benefit from balanced multimodal-linguistic representations. This intermediate optimum reflects the complex interplay between visual understanding and linguistic expression required for these tasks. The non-monotonic 20 Efficient Test-Time Scaling for Small Vision-Language Models patterns observed in several benchmarks indicate that aggregation timing must carefully balance semantic richness against linguistic specificity. This finding resonates with the token ranking dynamics identified by Li et al. (2025c), who demonstrate that different token types (genuine visual vs. hallucinated linguistic) achieve peak confidence at different layer depths, suggesting that optimal aggregation strategies should account for the hierarchical emergence of multimodal information processing patterns. The observed layer preferences can be attributed to fundamental architectural properties of VLMs and align with recent discoveries about information flow in transformer-based multimodal models. Early layers primarily encode multimodal semantic relationships and spatial structures, while later layers increasingly focus on autoregressive text generation and task-specific output formatting (Tenney et al., 2019). This hierarchical specialization creates trade-off: early aggregation preserves rich semantic diversity, but may introduce inconsistencies in linguistic expression, while late aggregation ensures coherent text generation, but may lose crucial semantic nuances. The dynamic contrastive decoding work of Chuang et al. (2024) provides additional theoretical support for our findings, demonstrating that factual knowledge evolves systematically across transformer layers, with different types of information reaching peak reliability at distinct layer depths. Our layer-dependent aggregation results extend these insights to the multimodal domain, revealing that visual and linguistic information follow distinct developmental trajectories through the network architecture. From theoretical perspective, these findings suggest that optimal aggregation requires matching the representational granularity to the task demands. Visual reasoning tasks benefit from the semantic spaces of early layers, where diverse augmented views can provide complementary visual interpretations. Conversely, linguistic tasks require the refined representations of later layers, where augmented inputs converge toward consistent textual expressions. The practical implications are significant for deployment optimization. Rather than universally aggregating at final layers, practitioners can achieve substantial improvements by selecting task-appropriate aggregation points. This layer-aware aggregation strategy could be implemented adaptively, with the aggregation layer selected based on task classification or learned through validation performance. However, the computational overhead of this approach remains modest, as early aggregation actually reduces computation by bypassing later layers for individual augmented inputs. Notably, the average performance trend shows late-layer aggregation as generally superior, but this global pattern obscures important task-specific exceptions where early aggregation provides substantial benefits. This finding challenges the common assumption that final-layer representations are universally optimal for test-time scaling and suggests that hierarchical aggregation strategies could unlock further improvements in multimodal understanding. Takeaway: Optimal aggregation layers depend critically on task type: language-heavy tasks benefit from late-layer aggregation that preserves linguistic refinement, while visual reasoning tasks achieve superior performance through early-layer aggregation that retains semantic richness. Task-adaptive layer selection can provide substantial improvements over universal late-layer aggregation."
        },
        {
            "title": "E COMPUTATIONAL OVERHEAD OF TTAUG",
            "content": "A critical consideration for deploying TTAug on resource-constrained devices is the computational overhead introduced by processing multiple augmented inputs. We analyze two implementation strategies that offer different trade-offs between memory usage and inference latency, enabling practitioners to select the most suitable approach based on their hardware constraints and requirements. Parallel implementation. In the parallel strategy, all augmented inputs are processed simultaneously within single forward pass by concatenating them into larger batch. This approach maximizes GPU utilization and minimizes wall-clock time by leveraging parallel computation capabilities. The memory overhead scales linearly with the number of augmentations, as the model must store activations for all inputs concurrently. Peak memory consumption increases substantially due to the need to maintain intermediate representations for the entire augmented batch during forward propagation. Sequential implementation. The sequential approach processes each augmented input independently in separate forward passes, accumulating token-level probability distributions for subsequent 21 Efficient Test-Time Scaling for Small Vision-Language Models aggregation. While this strategy significantly reduces peak memory requirements by processing only one augmentation at time, it incurs higher latency due to the sequential nature of computation. The modest memory increase observed in sequential processing primarily stems from the accumulation of key-value cache states across multiple forward passes, which must be retained for consistent generation. Figure 5: Overhead in peak GPU memory usage and runtime for different numbers of augmentations, comparing parallel and sequential implementation strategies. The experimental results in Fig. 5 demonstrate distinct scaling behaviors for the two strategies, measured on an NVIDIA A100 GPU. Parallel implementation exhibits substantial memory overhead that grows approximately linearly with the number of augmentations, while sequential implementation maintains relatively constant memory usage with only minor increases due to key-value cache accumulation. Conversely, runtime overhead follows the opposite pattern: parallel processing achieves near-constant inference time regardless of augmentation count, while sequential processing incurs linear time penalties proportional to the number of augmentations. These complementary trade-offs enable flexible deployment across diverse hardware configurations. For applications with abundant GPU memory but strict latency constraints, parallel implementation provides optimal performance. Conversely, memory-constrained environments benefit from sequential processing, which maintains feasible memory footprints at the cost of increased inference time. Practitioners can select the appropriate strategy based on their specific resource limitations and performance requirements, with both approaches representing practical extremes of the memorylatency trade-off spectrum. Takeaway: Parallel implementation minimizes latency but requires substantial memory, while sequential implementation conserves memory at the cost of increased runtime. The choice between strategies depends on hardware constraints and application priorities."
        },
        {
            "title": "F MULTIMODAL AUGMENTATION DECOMPOSITION",
            "content": "To understand the individual contributions of different modality-specific augmentations to our TTAug framework, we conduct an ablation study that isolates the effects of text-only, image-only, and combined multimodal augmentations. This analysis addresses fundamental question in multimodal test-time scaling: whether the benefits of joint augmentation can be decomposed into additive components from individual modalities, or whether multimodal synergies introduce non-linear interactions that exceed the sum of single-modal improvements. We design three experimental conditions to systematically evaluate modality-specific contributions. In the text-only condition, we apply classical textual augmentations while keeping the input image identical across all augmented samples. Conversely, the image-only condition applies classical visual transformations while maintaining identical text prompts. The both condition applies augmentations to both modalities simultaneously, representing our full TTAug framework. This decomposition enables us to quantify the relative importance of each modality and assess whether multimodal interactions produce emergent benefits beyond simple additive effects. 22 Efficient Test-Time Scaling for Small Vision-Language Models Figure 6: Performance comparison across different augmentation strategies showing the relative contributions of text-only, image-only, and combined multimodal augmentations. Each benchmark demonstrates different sensitivity patterns to modality-specific augmentations, with text augmentations consistently providing larger improvements than image augmentations across most tasks. The experimental results in Fig. 6 reveal several critical insights about multimodal augmentation dynamics. First, combined multimodal augmentation consistently outperforms both single-modality approaches across all benchmarks, demonstrating the value of joint augmentation strategies. However, the magnitude of improvement varies substantially across different task types, suggesting that multimodal synergies are task-dependent rather than universally additive. Second, text-only augmentation emerges as the dominant contributor to performance gains, substantially outperforming image-only augmentation across most benchmarks. This asymmetry is particularly pronounced on language-heavy benchmarks, where textual diversity appears more critical for robust understanding than visual transformations. Third, our analysis reveals that the combined effect exhibits non-linear characteristics that cannot be predicted by simply summing the individual contributions of text-only and image-only augmentations. On several benchmarks, the joint augmentation achieves improvements that exceed the arithmetic sum of single-modality gains, indicating positive synergistic interactions between visual and textual diversity. This non-linearity suggests that multimodal augmentation creates richer semantic spaces that enhance the models ability to extract consistent signals across diverse input representations. The observed modality asymmetry can be attributed to several fundamental architectural and representational factors inherent to multimodal language models. First, multimodal language models typically employ heavily compressed visual representations to maintain computational efficiency, often reducing high-resolution images to low-dimensional feature vectors through aggressive pooling or patch-based tokenization (Marafioti et al., 2025). These compression operations inherently filter out fine-grained visual details that our image augmentations target, rendering subtle transformations like brightness adjustments or minor rotations largely imperceptible to the models internal representations. Consequently, visual augmentations operate in severely constrained semantic space where meaningful diversity is difficult to achieve. Second, our findings align with recent interpretability research demonstrating that when one modality dominates the reasoning process, variations in the subordinate modality become largely irrelevant to model outputs (Ben Melech Stan et al., 2024). In many of our benchmarks, the textual component carries the primary semantic load, specifying the question type, reasoning requirements, and output format, while the visual component provides supplementary information. This inherent task structure naturally amplifies the impact of textual diversity while diminishing the influence of visual variations. Third, the token-level architecture of multimodal language models creates an additional bias toward textual processing. Since both visual and textual inputs are eventually projected into shared token space for text generation, the models training predominantly optimizes for linguistic coherence and next-token prediction accuracy. This architectural choice inherently favors modalities that directly influence the language generation process, explaining why textual augmentations, which directly modify the prompt structure and linguistic context, yield more substantial improvements than visual transformations that must traverse multiple encoding layers before affecting token-level decisions. The observed modality asymmetry has important implications for practical deployment. Since text augmentation provides disproportionate benefits while requiring minimal computational overhead compared to image processing, resource-constrained applications might prioritize textual diversity generation over complex visual transformations. However, the non-additive nature of multimodal interactions suggests that completely eliminating visual augmentation would sacrifice valuable 23 Efficient Test-Time Scaling for Small Vision-Language Models synergistic effects, supporting our unified approach that leverages both modalities while emphasizing textual diversity. Future work might explore augmentation strategies that operate directly in the compressed visual feature space or develop modality-aware weighting schemes that account for task-specific dominance patterns. Takeaway: Combined multimodal augmentation outperforms single-modality approaches through non-linear synergistic effects. Text augmentations contribute more substantially than image augmentations, but their combination produces emergent benefits that exceed simple additive predictions."
        },
        {
            "title": "G DETAILED RESULTS FOR DIFFERENT MODELS",
            "content": "Table 7: Performance comparison across SmolVLM2 family models (256M, 500M, 2.2B parameters) with no TTA, TTAug, and TTAdapt approaches. SmolVLM2-256M SmolVLM2-500M SmolVLM2-2.2B No TTA 65.1 56.7 0.2 0.1 47.8 37.0 21.0 29.5 29. 31.8 TT Aug 59.4 53.3 0.4 5.8 45.1 35.4 21.4 53.3 40.6 35.0 TT Adapt 55.1 50.3 0.3 18.4 40.1 34.0 20.7 43.0 38. 33.4 No TTA 64.1 61.0 0.0 0.0 59.9 56.6 27.6 55.3 6.2 36.7 TT Aug 64.8 60.0 4.6 0.0 58.0 55.3 27.6 56.1 9. 37.3 TT Adapt 65.5 57.6 5.2 0.9 57.7 52.1 27.2 52.8 31.6 38.9 No TTA 74.2 72.9 0.0 0.0 73.2 68.5 27.8 68.7 9. 43.8 TT Aug 76.1 73.7 12.6 5.5 72.4 69.6 31.9 75.9 16.9 48.3 TT Adapt 76.7 70.5 13.8 13.5 70.5 67.4 31.4 72.8 35. 50.3 ChartQA OCRBench OCRVQA GQA TextVQA AI2D MME-RW AMBER COCO Mean Table 8: TTAug performance across Ovis2 model family (1B, 2B, 4B, 9B) and InternVL2-1B. Ovis2-1B Ovis2-2B Ovis2-4B Ovis2-9B InternVL2-1B No TTA 80.4 88.8 74.3 30.0 79.2 76.5 35.5 76.1 22.7 62. TT Aug 81.6 84.9 70.5 54.3 77.2 73.3 35.6 73.8 13.7 62.8 No TTA 86.6 87.3 76.7 34.5 78.8 81.9 38.6 84.9 17.3 65. TT Aug 85.9 86.0 73.1 58.7 79.5 82.2 40.5 85.9 13.1 67.2 No TTA 87.6 91.2 80.2 40.5 83.5 84.9 45.7 87.4 14.0 68. TT Aug 87.8 89.2 76.9 55.7 83.9 84.5 44.1 87.4 12.5 69.1 No TTA 87.4 89.2 79.3 59.4 83.1 87.1 45.7 87.3 13.8 70. TT Aug 87.9 87.2 78.7 64.2 84.0 87.2 46.5 89.8 13.3 71.0 No TTA 72.1 75.7 43.3 52.0 69.6 52.8 13.5 72.6 17.2 52. TT Aug 72.1 75.1 42.0 51.3 67.6 52.6 13.3 75.7 24.6 52.7 ChartQA OCRBench OCRVQA GQA TextVQA AI2D MME-RW AMBER COCO Mean Note that TTAdapt method is not implemented for Ovis2 and InternVL model families due to practical constraints; the Unsloth library does not currently support those model families yet. 24 Efficient Test-Time Scaling for Small Vision-Language Models Table 9: Evaluation on diverse baseline models for reference. These models are evaluated without our methods just to establish performance baselines across different architectures. Baseline results are shown in Fig. 7. Pali Gemma xGen -MM LLaVA -OV Molmo -D Idefics 2 Janus -Pro ChartQA 40.7 OCRBench 61.4 OCRVQA 61.2 61.5 GQA 70.7 TextVQA AI2D 67.9 MME-RW 25.4 84.9 AMBER 45.9 COCO Mean 57. 65.0 55.5 70.7 60.2 72.8 73.5 35.1 82.1 15.7 59.0 72.3 61.2 69.5 62.5 60.8 78.2 31.1 84.4 13.9 59.3 85.8 66.3 44.9 55.1 81.5 80.7 36.8 85.0 12.1 60. 31.6 63.4 0.0 0.0 72.6 72.2 34.3 85.4 24.4 42.7 31.0 58.9 2.5 13.7 55.0 67.5 23.4 74.8 18.0 38.3 SMALL VISION-LANGUAGE MODELS Figure 7: Performance improvements across different models. Each point represents different model-strategy pair; x-axis shows model parameter size (B) using asinh scaling, and y-axis shows accuracy (%). The Transformer architecture (Vaswani et al., 2017) revolutionized language modeling, enabling models like BERT (Devlin et al., 2019) through bidirectional pretraining and GPT (Radford et al., 2019; Brown et al., 2020) via autoregressive generation. These foundational advances led to largescale models such as GPT-3 (Brown et al., 2020) with human-like text generation abilities. More recent developments have emphasized efficiency, with LLaMA (Touvron et al., 2023) demonstrating that smaller, well-trained models can outperform earlier, larger counterparts. Open-source families including Qwen (Bai et al., 2023), InternLM (Team, 2023), and Gemma (Team, 2024) further expanded access to capable language models. In the multimodal domain, CLIP (Radford et al., 2021) introduced contrastive vision-language pretraining, facilitating strong zero-shot visual understanding. This inspired the integration of vision encoders with LLMs to produce multimodal large language models, such as GPT-4V (Achiam et al., 2023), LLaVA (Liu et al., 2023), Qwen-VL (Bai et al., 2023), and InternVL (Chen et al., 2024b). Notably, Molmo (Deitke et al., 2025) provides transparency by releasing full training data and evaluation protocols. Recently, the emergence of small vision-language models or multimodal small language models, models under 10B parameters, has shifted attention toward efficient, accessible architectures suitable for edge deployment. Examples include Ovis2 (Lu et al., 2025), InternVL2 (Chen et al., 2024b), Janus-Pro (Chen et al., 2025), Idefics2 (Laurençon et al., 2024), LLaVA-OneVision (Li et al., 2025a), Molmo (Deitke et al., 2025), XGen-MM (Xue et al., 2024), PaliGemma (Beyer et al., 2024), and the SmolVLM family (Marafioti et al., 2025), with models as small as 256M parameters. These compact models achieve competitive performance on vision-language benchmarks while significantly reducing computational cost, making them attractive for real-world, resource-constrained applications. They offer compelling advantages for practical deployment: they enable inference on consumer GPUs and edge devices, support privacy-preserving local processing, and demonstrate superior cost-performance ratios for specialized tasks (Belcak et al., 2025). But, their limited parameter capacity makes them particularly vulnerable to domain shifts, various biases, and distribution mismatches at inference time. I"
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "I.1 SELF-SELECTOR Self-selector uses the tested VLM itself as verifier to select one response among the candidates (Chen et al., 2024a; Parmar et al., 2025). We enforce the VLM to choose between available indices ranging from 0 to the number of augmentations. Since small VLMs are not capable of reliably following this constrained output behavior through prompt engineering alone, we employ structured generation techniques to guarantee valid responses. We use the Outlines library (Willard & Louf, 2023) for structured generation. We use the prompt given below: 25 Efficient Test-Time Scaling for Small Vision-Language Models Prompt \"{input_question}\" Different people answered this question in different ways. Select the best response from these candidate answers: {responses} Just return the index of the best response. Return an integer between 0 and {n_aug}. I.2 SELF-SYNTHESIZER Self-synthesizer method uses the tested VLM to aggregate responses into one coherent final answer (Li et al., 2025d; Jiang et al., 2023; Wang et al., 2025a; Li et al., 2025b). We use the prompt given below: Prompt \"{input_question}\" Different people answered this question in different ways. Combine these responses into single, coherent and accurate answer: {responses} Just return the final answer. I.3 SELF-PARAPHRASING Self-paraphrasing uses the text backbone of the tested VLM to paraphrase the input prompt. Since the model is not good enough to do this in one shot, we split the prompt into sentences and feed each sentence to the model to paraphrase using structured generation to get fixed number of paraphrases. After that, we concatenate all paraphrased sentences to get the final paraphrased prompt. This approach maintains consistency with the target models internal linguistic patterns while remaining self-contained. We use the prompt given below: Prompt You are an expert paraphraser. Your task is to paraphrase input text without changing its meaning. Keep the details and core content. Generate {n_aug} paraphrased versions. Return your output as JSON object with the key \"paraphrases\", mapped to list of {n_aug} unique paraphrased versions. Now, paraphrase the following text: Since small VLMs are not capable of paraphrasing complex long prompts reliably in one shot, we first split the input text into individual sentences using spaCy (Honnibal et al., 2020) for sentence splitting. We then paraphrase each sentence independently. Also, since small VLMs are not capable of reliably following constrained output behavior, outputting the exact number of paraphrases, through prompt engineering alone, we employ structured generation techniques to guarantee valid responses. We use JSON schema that enforces an output with exactly desired number of paraphrases. After obtaining paraphrases for each sentence independently, we compute the Cartesian product across all sentence-level paraphrase sets to generate diverse combinations of the complete prompt. This approach produces final paraphrased prompts by systematically combining different paraphrased versions of each sentence, ensuring both local sentence-level diversity and global prompt-level variation while maintaining semantic consistency. 26 Efficient Test-Time Scaling for Small Vision-Language Models I.4 CLASSICAL IMAGE AUGMENTATIONS We implement classical image augmentations using the Albumentations library (Buslaev et al., 2020). For each input image, we randomly select three transformations from our predefined set and apply them sequentially through composed transformation pipeline. This random selection approach ensures diverse augmentation combinations while maintaining computational efficiency. The predefined sets for different augmentation strengths are given below. High fill=144), A.RandomBrightnessContrast(p=0.6), A.SafeRotate(limit=20, p=0.6, border_mode=cv2.BORDER_CONSTANT, (cid:44) A.GaussianBlur(blur_limit=(3, 7), p=0.6), A.CLAHE(p=0.5), A.RandomGamma(p=0.6), A.HueSaturationValue(p=0.6), A.RandomScale(scale_limit=0.1, p=0.6), A.RGBShift(p=0.6), A.MedianBlur(blur_limit=3, p=0.6), A.ImageCompression(quality_range=(85, 95), p=0.45), A.Sharpen(p=0.6), A.PlanckianJitter(), A.RandomFog(alpha_coef=0.15), A.RandomToneCurve(), A.Emboss(), A.GridDistortion(), A.Perspective(scale=0.05, fit_output=True), A.GridDropout(ratio=0.25, random_offset=True, fill=144, p=0.66), A.CoarseDropout(fill=144, p=0.7), Medium val_shift_limit=15, p=0.5), A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2), A.SafeRotate(limit=15, border_mode=cv2.BORDER_CONSTANT, fill=144), A.GaussianBlur(blur_limit=(3, 7), p=0.5), A.CLAHE(clip_limit=3.0, p=0.4), A.RandomGamma(gamma_limit=(80, 120), p=0.5), A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=15, (cid:44) A.RandomScale(scale_limit=0.08, p=0.5), A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15), A.MedianBlur(blur_limit=3, p=0.5), A.ImageCompression(quality_range=(85, 95), p=0.35), A.Sharpen(alpha=(0.2, 0.5), lightness=(0.6, 1.0), p=0.5), A.PlanckianJitter(p=0.5), A.RandomFog(alpha_coef=0.1, p=0.3), A.RandomToneCurve(scale=0.2, p=0.5), A.Emboss(alpha=(0.2, 0.5), strength=(0.5, 0.7), p=0.5), A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.5), A.Perspective(scale=0.03, fit_output=True, p=0.5), A.GridDropout(ratio=0.25, random_offset=True, fill=144, p=0.6), A.CoarseDropout(fill=144, p=0.5), 27 Efficient Test-Time Scaling for Small Vision-Language Models Low p=0.3), fill=144), val_shift_limit=10, p=0.3), A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, (cid:44) A.SafeRotate(limit=10, p=0.3, border_mode=cv2.BORDER_CONSTANT, (cid:44) A.GaussianBlur(blur_limit=(3, 5), p=0.3), A.CLAHE(clip_limit=2.0, p=0.3), A.RandomGamma(gamma_limit=(90, 110), p=0.3), A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, (cid:44) A.RandomScale(scale_limit=0.05, p=0.3), A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, (cid:44) A.MedianBlur(blur_limit=3, p=0.3), A.ImageCompression(quality_range=(85, 95), p=0.25), A.Sharpen(alpha=(0.1, 0.3), lightness=(0.7, 1.0), p=0.3), A.PlanckianJitter(p=0.3), A.RandomFog(alpha_coef=0.05, p=0.2), A.RandomToneCurve(scale=0.1, p=0.3), A.Emboss(alpha=(0.1, 0.3), strength=(0.3, 0.5), p=0.3), A.GridDistortion(num_steps=5, distort_limit=0.1, p=0.3), A.Perspective(scale=0.02, fit_output=True, p=0.3), p=0.3), I.5 GENERATIVE IMAGE AUGMENTATIONS Generative augmentations use FLUX.1-dev (Labs et al., 2025) to create semantically similar but visually distinct image variants. We employ an image-to-image pipeline that, unlike traditional flow matching which starts from random noise, begins denoising from fixed intermediate timestep with noisy version of the input image. This approach preserves semantic similarity to the original while introducing visual diversity through the prompt \"realistic image.\" However, even recent generative models struggle with creating images containing textual elements (Bosheah & Bilicki, 2025). Therefore, our approach excludes text-containing images to prevent OCR corruption, using Tesseract for text detection (Smith, 2007). Two key hyperparameters control the generation process: strength (chosen as 0.25) determines the initial denoising timestep, lower values preserve more of the original image structure, and guidance scale (chosen as 3.0) controls the classifier-free guidance parameter. While this method produces diverse and consistent image variations, it requires external diffusion models and significant computation budget. It is not practical for resource-constrained deployments. I.6 AGGREGATION WEIGHTS OPTIMIZATION Aggregation weights optimization learns adaptive token-wise weights wi,j to replace the uniform averaging scheme in Eq. 2. At each generation step j, we initialize learnable parameters as wj RN and optimize them through gradient descent to minimize the marginal entropy H(pj) = (cid:80) vV pj(v) log pj(v) of the weighted aggregated distribution. The optimization employs AdamW with adaptive learning rates and gradient clipping for stability, performing multiple micro-steps per token to achieve convergence. This approach requires minimal computational overhead with compact computational graph, making it suitable for real-time deployment. Optimization Parameters. We use the AdamW optimizer with an initial learning rate of 1 102 and weight decay of 1 104. The aggregation weights wj are initialized uniformly as wi,j = 1/N where is the number of augmentations. We perform 20 optimization micro-steps per token generation step to ensure convergence of the entropy minimization objective. We reinitialize the aggregation weights before processing each new question to ensure independent optimization across different inputs. Gradient Clipping. To maintain training stability, we apply gradient clipping with maximum norm of 1.0. This prevents gradient explosion during the iterative optimization process. Efficient Test-Time Scaling for Small Vision-Language Models Numerical Stability. We add small epsilon value of 1 1012 to the logarithm computation in the entropy calculation to prevent numerical instabilities when probabilities approach zero. The softmax temperature is kept at the default value of 1.0. At each optimization step, we apply softmax normalization to the raw learnable parameters to ensure the weights sum to 1: wi,j = where θi,j are the raw learnable parameters. exp(θi,j ) k=1 exp(θk,j ) (cid:80)N Computational Efficiency. The optimization process uses detached probability distributions from the forward pass to prevent gradients from flowing back through the entire model, maintaining the compact computational graph. I.7 MODEL PARAMETER ADAPTATION Model parameter adaptation (TTAdapt) performs iterative fine-tuning during inference using pseudolabels generated from TTAug consensus. The method employs full parameter fine-tuning with gradient checkpointing for memory efficiency and implements three-stage iterative loop: pseudolabel generation, parameter updates, and weight reset between questions. Training Configuration. We use the AdamW optimizer in the Unsloth (AI et al., 2025) library. The learning rate is set to 2 106 with cosine learning rate scheduler and 5 warmup steps. We apply weight decay of 0.01 for regularization and perform 6 training steps per pseudolabel iteration with batch size of 64 and gradient accumulation steps of 2. Iterative Adaptation Process. We perform 3 pseudolabel iterations per question. Each iteration generates pseudolabels using the current model state with TTAug consensus (average aggregation), then fine-tunes the model parameters using these pseudolabels as supervision. The final iteration generates the output without additional training to prevent overfitting. Resetting Weights. fundamental challenge in continual test-time adaptation is catastrophic forgetting (Niu et al., 2022; Wang et al., 2022), where models suffer severe performance degradation on original training samples after adaptation. During sample-by-sample adaptation to test streams, models can lose important information through unsupervised learning, causing rare domains to disappear while abundant ones dominate. One solution involves episodic adaptation, which means restarting from the original model for each sample rather than continual learning. Thus, in our method, model parameters are reset to their initial state before processing each new question to prevent catastrophic forgetting. I.8 IMPLEMENTATION DETAILS FOR TTAUG TTAug implementation requires careful integration with the models generation pipeline to enable efficient token-level aggregation while preserving KV caching and other optimization features. We achieve this through dynamic method patching that intercepts the sampling process without disrupting the underlying generation mechanics. Monkey patching is critical for KV cache compatibility. We override the models _sample method to inject our aggregation logic while maintaining compatibility with existing optimizations. The patched method preserves the original sampling interface but intercepts logits before token selection to perform aggregation across augmented inputs: The modified sampling method extracts logits from multiple augmented forward passes, applies the specified aggregation strategy (uniform averaging, learned weights, or entropy optimization), and returns aggregated token selections. This approach enables seamless integration with existing generation pipelines, including beam search, nucleus sampling, and temperature scaling. Our implementation leverages KV caching by processing augmented inputs in batches and sharing cached key-value pairs across the prefix tokens. The aggregation computation adds minimal overhead as it operates only on the final logits rather than intermediate representations, maintaining the models inference speed while enabling test-time adaptation. The patched method maintains full compatibility with the Transformer (Wolf et al., 2020) librarys generation utilities, preserving advanced sampling techniques such as top-k, top-p, and temperature scaling. The aggregation occurs at the logit level before these sampling strategies are applied, ensuring that the enhanced diversity from TTAug benefits from sophisticated decoding procedures. Efficient Test-Time Scaling for Small Vision-Language Models"
        },
        {
            "title": "J EVALUATION METRICS DETAILS",
            "content": "We provide detailed mathematical formulations for the evaluation metrics used across all benchmarks in our study. J.1 EXACT STRING MATCHING (OCRVQA, GQA) For datasets requiring exact string correspondence, we define the accuracy metric as: Accuracy ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 I[ˆyi = yi] (4) where ˆyi is the predicted answer and yi is the ground truth answer. J.2 VQA SCORE WITH INTER-ANNOTATOR AGREEMENT (TEXTVQA) Following the standard VQA evaluation protocol that accounts for multiple valid answers and inter-annotator variability: VQA Score = min 1, 1 N (cid:88) i=1 1 3 Ai (cid:88) j=1 I[ˆyi = yi,j] (5) where Ai is the set of ground truth answers for question i, yi,j represents the j-th ground truth answer, and the factor 1 3 reflects the standard VQA scoring that considers an answer correct if at least 3 annotators agree. J.3 RELAXED STRING MATCHING (CHARTQA) For numerical and chart-based questions requiring approximate matching: Relaxed Accuracy = 1 (cid:88) i=1 max jAi R(ˆyi, yi,j) where Ai is the set of acceptable answers for question i, and R(ˆy, y) is defined as: (cid:104) ˆyvy (cid:40)I (cid:105) vy 0.05 R(ˆy, y) = if both are numeric I[ˆy = y] otherwise (6) (7) where vˆy and vy represent the numerical values extracted from the predicted and ground truth answers, respectively. J.4 SUBSTRING CONTAINMENT MATCHING (OCRBENCH) OCRBench evaluates text recognition performance using substring containment matching: Accuracy = I[yi,j ˆyi] (8) (cid:88) 1 max jAi i=1 where Ai represents the set of acceptable answers for question i, and denotes substring containment. J.5 MULTIPLE-CHOICE AND YES/NO EXTRACTION (MME-REALWORLD, AI2D, AMBER) For multiple-choice and yes/no questions, we extract the choice label from predictions and perform exact matching: Accuracy = 1 (cid:88) i=1 I[li = ci] (9) where ci {A, B, C, D, ...} or {yes, no} is the correct choice label for question i, and li is the extracted choice label from the predicted answer ˆyi. 30 Efficient Test-Time Scaling for Small Vision-Language Models J.6 ROUGE-L EVALUATION (COCO CAPTIONS) We evaluate captioning quality using ROUGE-L, which measures the longest common subsequence between predicted and reference captions: ROUGE-L = 2 PLCS RLCS PLCS + RLCS where the precision and recall are defined as: PLCS = RLCS = LCS(ˆy, y) ˆy LCS(ˆy, y) (10) (11) (12) and LCS(ˆy, y) computes the longest common subsequence between the predicted caption ˆy and reference caption y, with denoting sequence length. For multiple reference captions, we compute ROUGE-L against each reference and take the maximum score: ROUGE-Lmulti = max jRi ROUGE-L(ˆyi, yi,j) (13) where Ri is the set of reference captions for image i. J. IMPLEMENTATION NOTES All text preprocessing follows consistent normalization procedures: (1) converting to lowercase, (2) stripping leading and trailing whitespace, (3) replacing multiple consecutive spaces with single spaces, and (4) removing newline characters where appropriate. For mathematical expressions in OCRBench, additional preprocessing removes all whitespace to handle formatting variations. For ChartQA relaxed matching, numerical values are extracted by handling percentage symbols (converting \"X%\" to X/100) and parsing floating-point numbers. For multiple-choice extraction in MME-RealWorld and AI2D, choice labels are identified using regular expressions that match single uppercase letters (A-Z) appearing in isolation or with minimal surrounding punctuation. For detailed implementation specifics and evaluation protocols, refer to VLMEvalKit (Duan et al., 2024)."
        },
        {
            "title": "K QUALITATIVE RESULTS",
            "content": "Both classical text augmentations (Sec. 4.4) and classical image augmentations with high strength (Sec. 4.5) are applied, with 16 augmentations per sample. Thus, the shown cases correspond to samples underlying the quantitative results in Sec. 4.6. 31 Efficient Test-Time Scaling for Small Vision-Language Models ChartQA Original Inputs Which country had the most visitors to Italy in 2018? Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>For the question below, follow the following instructions: -The answer should contain as few words as possible. -Don't paraphrase or reformat the text you see in the image. -Answer binary question with Yes or No. -When asked to give numerical value, provide number like 2 instead of Two. -If the final answer has two or more items, provide it in the list format like [1, 2]. -When asked to give ratio, give out the decimal value like 0.25 instead of 1:4. -When asked to give percentage, give out the whole value like 17 instead of decimal like 0.17%. -Don't include any units in the answer. -Do not include any full stops at the end of the answer. -Try to include the full label from the graph when asked about an entity. Question: Whish countrg had the mo st vi sitors to ta>y in 2 018? Ans wer the question using ingle ord or phrase. In other words, Which country had the most visitors to Italy in 2018? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 1: [... truncated, same as Prompt 0 ...] Question: Which couJtry had the ost vis itors to Italy in 2 018? Answ er the question usi ng si nble ord or pbrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 2: [... truncated, same as Prompt 0 ...] Question: Which coun try had the mo st isitors to It aly in 20 18? Ans wer the quest ion usiMg s7ngle woTd or phrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 3: [... truncated, same as Prompt 0 ...] Question: Whi ch co untry had the mLst visitors to taly in 2 018? Answer the uesyion using eing le word or phr ase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 4: [... truncated, same as Prompt 0 ...] Question: Wh ich country had the most visitors to Ita ly in 20 18? Answer the que ction usinn inglW ord or hrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 5: [... truncated, same as Prompt 0 ...] Question: Wh ich country had the mo st vi siRors to Italy in 2 018? jswer the question sing eingle word or phr ase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 6: [... truncated, same as Prompt 0 ...] Question: Wh ich ountry had the mo st visi tors to Italy in 2 018? Answer the quesRion usi ng sing?e Dord or hrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 7: [... truncated, same as Prompt 0 ...] Question: Which country had the mo st isitors to Ita ly in W018? AnWwer the quest ion ising ingle ord or phr ase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 8: [... truncated, same as Prompt 0 ...] Question: Which country had the mo st visitors to raly in 20 18? Ajsw er the questi on usinb sin gle wo rd or phrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 9: [... truncated, same as Prompt 0 ...] Question: Wh ich cou ntry had the most visitors to Ita ly in 2018? Answer the qu estipn us(ng eing le ord or hrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 10: [... truncated, same as Prompt 0 ...] Question: Wnich cou ntry had the ost visitors to Italy in 20 18? An sAer the question usiBg si ngle ord or hrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 11: [... truncated, same as Prompt 0 ...] Question: Wbich co8nt ry had the most visiYors to It aly in 2 018? An swer the question using si ngle wo rd or ph rase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 12: [... truncated, same as Prompt 0 ...] Question: Which ountry had the ost visJtors to taly in 2018? An swer the question us ing ingle aord or ph ras4. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 13: [... truncated, same as Prompt 0 ...] Question: Whi ch country had the mo st visit ors to ItZly in 2 018? Answ er the ques$ion sing si ngle wo%d or phrase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 14: [... truncated, same as Prompt 0 ...] Question: Wgich count ry had the most visito rs to Ita ly in 201*? Ans3er the qu estion us ing si ngle word or ph rase. In other words, Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Prompt 15: [... truncated, same as Prompt 0 ...] Question: Which country had the most visitors to Italy in 2018? [... truncated, same as Prompt 0 ...] Baseline Output Answer: France Accuracy: 0.0% TTAug Output Answer: Germany Accuracy: 100.0% TTAdapt Output Answer: Germany Accuracy: 100.0% 32 Efficient Test-Time Scaling for Small Vision-Language Models OCRBench Original Inputs Augmented Prompts what is the total amount of this receipt? Answer this question using the text in the image directly. Augmented Input Images Prompt 0: <im_start>User:<image>what the amount thks receipt? this question the text the imagw directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>what is the total amount of? AGswer this the in image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 2: <im_start>User:<image>wNat is the toyal amount? this question using in the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 3: <im_start>User:<image>whWt total amount of receipt? AnsAer this text in the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 4: <im_start>User:<image>what is the smount of this? quesGion using the text in image. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 5: <im_start>User:<image>what is the total amount receipt? question the text the image di5ectly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 6: <im_start>User:<image>what is total amount of? Answer this question in the image dieectly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 7: <im_start>User:<image>is the total of tbis receipt? Answer thJs question using text in the. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 8: <im_start>User:<image>is the total amount of this reVeipt? Answer questiLn text the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 9: <im_start>User:<image>what the total aJount of this rdceipt? using the the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 10: <im_start>User:<image>is amouHt of this? Answer th * question text in the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 11: <im_start>User:<image>what is the total amount of this receipt? thix using in iKage. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 12: <im_start>User:<image>is total amounY of this? Answer this question using text in the. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 13: <im_start>User:<image>is the amount of receipt? ques5ion using the text in the direVtly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 14: <im_start>User:<image>wYat the of this receipt? thie question the in the image directly. In other words, what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Prompt 15: <im_start>User:<image>what is the total amount of this receipt? Answer this question using the text in the image directly. Give very brief answer.<end_of_utterance> Assistant: Baseline Output Answer: 100.00 Accuracy: 0.0% TTAug Output Answer: 71.10 Accuracy: 100.0% 33 TTAdapt Output Answer: 71.10 Accuracy: 100.0% Efficient Test-Time Scaling for Small Vision-Language Models OCRVQA Original Inputs Who is the author of this book? Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>Answer the question using single word or phGase. Who is the author of thiC book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>qnswer the question using single word or pTrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 2: <im_start>User:<image>Answer the question usjng single word or phraDe. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 3: <im_start>User:<image>Answer the question usinT single Sord or phrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 4: <im_start>User:<image>Answer the question using Eingle word or (hrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 5: <im_start>User:<image>Answer the question using sinNle word or phrase. Who is the author of tgis book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 6: <im_start>User:<image>Answer the quest(on using s&ngle word or phrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 7: <im_start>User:<image>AnsweF the question ^sing single word or phrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 8: <im_start>User:<image>Answer the question uAing single word or phrase. Who is the authoT of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 9: <im_start>User:<image>AnsweF the qHestion using single word or phrase. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 10: <im_start>User:<image>Answe$ the question using single word or phrase. Who is the author of this Hook? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 11: <im_start>User:<image>Answer the question using single word or phrase. Who is the autho# of thks book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 12: <im_start>User:<image>Answer the question using single word or ph#ase. Who is the author of %his book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 13: <im_start>User:<image>AnAwer the question using single word or phrase. Who is the autTor of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 14: <im_start>User:<image>Answer the question using s7ngle word or phrQse. Who is the author of this book? In other words, Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Prompt 15: <im_start>User:<image>Who is the author of this book? Answer the question using single word or phrase. Give very brief answer.<end_of_utterance> Assistant: Baseline Output Answer: Brushy. Accuracy: 0.0% TTAug Output Answer: Brush Dance. Accuracy: 100.0% TTAdapt Output Answer: Brush Dance. Accuracy: 100.0% 34 Efficient Test-Time Scaling for Small Vision-Language Models GQA Original Inputs Whats in front of the window? Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>W hat ' in feont of the win dow? Answer the que stiin sing si ngle wo rd or phrWse. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>Wh at ' in fro nt of the window? Ansder the qu estlon usi ng sing le word or ph rqse. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 2: <im_start>User:<image>W hat ' in fro nt of the win dow? Abswer the question uxing sihg le wo rd or phra se. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 3: <im_start>User:<image>Wh at ' in fro nt of the wi ndow? AnXwer the queAtion us ing si ngle w)rd or phr ase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 4: <im_start>User:<image>Wh at ' in fr ont of the wi ndod? AnsAer the queWt ion using ingle wo rd or phrase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 5: <im_start>User:<image>W hat ' in romt of the window? Answ er the qu estion us ing sin gle Sord or lhrase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 6: <im_start>User:<image>W hat ' in fro nt of the windo#? Answer the ques tion using s&ng le wo rd or hGase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 7: <im_start>User:<image>Wmat ' in fro nt of the wi ndow? Ans wer the quest ion usigg sin gle Sord or ph rase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 8: <im_start>User:<image>What ' in ront of the !ind ow? Answer the Wuesti on usi ng single wo rd or hrQse. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 9: <im_start>User:<image>Wh at ' in fr0nt of the wi ndow? An swer the qu estKon using sin gle w9rd or phr ase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 10: <im_start>User:<image>Wh at ' in front of the qind ow? Ans wer the Testion usi ng ingle ord or phraAe. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 11: <im_start>User:<image>Wh at ' in f%ont of the win dow? Ajswer the que stion usi ng simg le word or hrase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 12: <im_start>User:<image>W hat ' in front of the wi ndow? Ans wer the questi on usinN ingIe ord or phraEe. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 13: <im_start>User:<image>W hat ' in front of the winE ow? Answer the qu 4stion using sing le ord or Urase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 14: <im_start>User:<image>dhat ' in fro nt of the win dow? An sw2r the Aues tion using ingle word or phr ase. In other words, What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 15: <im_start>User:<image>What's in front of the window? Answer the question using single word or phrase.<end_of_utterance> Assistant: Baseline Output Answer: Blinds. Accuracy: 0.0% TTAug Output Answer: Desk. Accuracy: 100.0% TTAdapt Output Answer: Desk. Accuracy: 100.0% 35 Efficient Test-Time Scaling for Small Vision-Language Models TextVQA Original Inputs which of these books was recently adapted by netflix? Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>Answer the following question about the image using as few words as possible. Follow these additional instructions: -Always answer binary question with Yes or No. -When asked what time it is, reply with the time seen in the image. -Do not put any full stops at the end of the answer. -Do not put quotation marks around the answer. -An answer with one or two words is favorable. -Do not apply common sense knowledge. The answer can be found in the image. Question: which of ese ooks re adapted? que stion usi ng single word or phra. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 1: [... truncated, same as Prompt 0 ...] Question: of these boo was recen adapted by netflix? the ques tion using sing le word. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 2: [... truncated, same as Prompt 0 ...] Question: of hese was cently apted by netflix? question using sing le wo or ase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 3: [... truncated, same as Prompt 0 ...] Question: of these was recen adapted netfl? nswer questi on us ing word or se. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 4: [... truncated, same as Prompt 0 ...] Question: of hese boo was by? An swer the question usi ng single word or se. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 5: [... truncated, same as Prompt 0 ...] Question: which of hese bo oks recent apted netflix? the ques tion ng word phrase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 6: [... truncated, same as Prompt 0 ...] Question: ch th ese ooks was adapted by tflix? Answer question sin gle ord or. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 7: [... truncated, same as Prompt 0 ...] Question: ich books recently by netf lix? Ans wer the using le word or phrase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 8: [... truncated, same as Prompt 0 ...] Question: whi ch of the se books was adapted by? nswer the sing wo hrase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 9: [... truncated, same as Prompt 0 ...] Question: of bo oks was recently by? using si ngle wo rd or phra se. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 10: [... truncated, same as Prompt 0 ...] Question: whi ch books recent ly adapted by ix? wer the qu using le word phrase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 11: [... truncated, same as Prompt 0 ...] Question: ich se books was pted by netflix? Answer the ion using wo or phra se. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 12: [... truncated, same as Prompt 0 ...] Question: hese books was recently adapted n? Answer que stion using wo or ase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 13: [... truncated, same as Prompt 0 ...] Question: wh was rece ntly ad apted netflix? nswer using word or ph rase. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 14: [... truncated, same as Prompt 0 ...] Question: which of the books by netfl ix? Answer the questi using single ord or phr. In other words, which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Prompt 15: [... truncated, same as Prompt 0 ...] Question: which of these books was recently adapted by netflix? Answer the question using single word or phrase.<end_of_utterance> Assistant: Baseline Output TTAug Output Answer: broken angels Accuracy: 0.0% Answer: altered carbon Accuracy: 100.0% TTAdapt Output Answer: altered carbon Accuracy: 100.0% 36 Efficient Test-Time Scaling for Small Vision-Language Models AI2D Original Inputs Augmented Prompts What would happen if the cricket population decreased? The choices are listed below: (A) lizards would decrease (B) eagle would increase (C) king brown snake would increast (D) salt bush would decrease Augmented Input Images Prompt 0: <im_start>User:<image>Ques tion: What wo^ld appen if the ricket popula tion deSreased? In other words, Question: What would happen if the cricket population decreased?Options: A. lizards would decrease B. eagle would increase C. king brown snake would increast D. salt bush would decrease Answer with the letter.<end_of_utterance> Assistant: Answer: Prompt 1: <im_start>User:<image>Que stion: hat wo uld happen if the cri cket p)pulation decr2ased? In other words, [... truncated, same as Prompt 0 ...] Prompt 2: <im_start>User:<image>Question: Wh at woulS happen if the cr icke^ pop ulation decrea sed? In other words, [... truncated, same as Prompt 0 ...] Prompt 3: <im_start>User:<image>Question: What wo uld wppen if the cri cket opulation Eecreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 4: <im_start>User:<image>Qu3s tion: Dhat wo uld hap pen if the cri cket population decreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 5: <im_start>User:<image>Quwstion: WhQt wou ld happen if the cr icket populati on ecreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 6: <im_start>User:<image>Que sti*n: hat wou;d happen if the cri cket populati on decreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 7: <im_start>User:<image>Question: Wuat would hap) en if the crick et populati on decreas ed? In other words, [... truncated, same as Prompt 0 ...] Prompt 8: <im_start>User:<image>Question: hat wPuld happ en if the cr7c ket population decrea sed? In other words, [... truncated, same as Prompt 0 ...] Prompt 9: <im_start>User:<image>Quest ion: hat wouId happen if the cr icket pop7lation decr eased? In other words, [... truncated, same as Prompt 0 ...] Prompt 10: <im_start>User:<image>QueC tion: Wh at wou ld xppen if the cricket population decreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 11: <im_start>User:<image>Quest ion: What wo uld appen if the crifket populayion dec reased? In other words, [... truncated, same as Prompt 0 ...] Prompt 12: <im_start>User:<image>QuFst ion: What wo uld happen if the cricket pop u;ation decrea sed? In other words, [... truncated, same as Prompt 0 ...] Prompt 13: <im_start>User:<image>Qu estJon: hat wou ld happen if the ricket Lopulation decreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 14: <im_start>User:<image>Question: hat oulE happ en if the cricket popula ti)n decreased? In other words, [... truncated, same as Prompt 0 ...] Prompt 15: <im_start>User:<image>Question: What would happen if the cricket population decreased?Options: A. lizards would decrease B. eagle would increase C. king brown snake would increast D. salt bush would decrease Answer with the letter.<end_of_utterance> Assistant: Answer: Baseline Output Answer: Accuracy: 0.0% TTAug Output Answer: Accuracy: 100.0% TTAdapt Output Answer: Accuracy: 100.0% 37 Efficient Test-Time Scaling for Small Vision-Language Models MME-RealWorld Original Inputs This image shows the front view of the ego car. What is the future state of the black pants pedestrian in the middle? The choices are listed below: (A) Turn left. (B) Stationary. (C) Keep going straight. (D) Turn right. (E) The image does not feature the object. Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>This image shows the front view the ego car. What is the future state of black pants pedestrian middle? The are listed below: (A) Turn. (B) Stationary. (C) Keep going straight. (D) Turn right. (E) The image does not feature the object. Respond only letter (A, B, C, D, or E) of the correct option. Select the best answer to the above multiple - choice question based on the image. The answer: In other words, This image shows the front view of the ego car. What is the future state of the black pants pedestrian in the middle? The choices are listed below: (A) Turn left. (B) Stationary. (C) Keep going straight. (D) Turn right. (E) The image does not feature the object. Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option. The best answer is:<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>This image shows front view of the ego. The choices are listed: (A) Turn. (B ). What is future state of the black pants in the middle? (C) Keep going straight. (D) Turn right. (E) The does not feature object. the best answer to the above multiple - choice question based on the image. The best answer is: Respond with only the letter (A, B, C, D, or E) of the correct option. In other words, [... truncated, same as Prompt 0 ...] Prompt 2: <im_start>User:<image>This image the view the car. What is the state of the black pants pedestrian in the middle? The choices are listed below: (A) Turn left. (C) Keep going straight. (B) Stationary. () The image does not feature the object. () Turn right. Respond with only the letter (A, B, C, D, or E) of the option. the best answer to above multiple - choice question based on the image. The best answer is: In other words, [... truncated, same as Prompt 0 ...] Prompt 3: <im_start>User:<image>The is: What is the future state of the black pants pedestrian in the middle? (B ). The choices are listed below: () Turn left. (C) Keep going straight. () right. Select the best answer to the above multiple - choice question based on the image. (E) image does not feature the object. Respond with only the letter (, , C, D, E) of the correct option. This image shows the front view of the ego car. In other words, [... truncated, same as Prompt 0 ...] Prompt 4: <im_start>User:<image>What is the future state of the black pants pedestrian in the? This shows the front view of the ego car. The choices are listed below: (A) left. (C) Keep going straight. (B ). (D) Turn right. (E) The image does not feature the object. the answer to the multiple - choice question based on the image. The best answer is: Respond only the letter (, B, C, D, or E) of the option. In other words, [... truncated, same as Prompt 0 ...] Prompt 5: <im_start>User:<image>This image shows the front view of the ego car. What the future state of the black pedestrian in the middle? The choices are listed below: () Turn left. (C) Keep going straight. (B ). () The image does not feature the object. (D) Turn right. Select the best answer to above multiple - choice question on the image. The answer is: Respond with only the letter (A, , C, D, or) of the correct option. In other words, [... truncated, same as Prompt 0 ...] Prompt 6: <im_start>User:<image>The best answer is: The choices are listed below: (A) Turn left. What is the state the black pants in the middle? (B) Stationary. (C) Keep going. (D) Turn right. the best answer to the above multiple - choice question based on the image. (E) The image does not feature the object. Respond with only the letter (A, , C, , or) of the correct option. This image shows front view of the ego. In other words, [... truncated, same as Prompt 0 ...] Prompt 7: <im_start>User:<image>This image shows the front view of the ego car. What is the future state of the black pants in the? (B) Stationary. The choices are listed below: (A) Turn. (D) Turn right. (C) Keep going straight. (E) The image does not the object. Respond with only the letter (A, B, C, D, or) of correct. Select the best answer the above multiple - choice question based the image. The best answer: In other words, [... truncated, same as Prompt 0 ...] Prompt 8: <im_start>User:<image>This image shows the front view the ego car. The choices are listed below: (A) Turn left. (B) Stationary. What is the future state of black pedestrian in middle? (C) Keep going. (D) Turn right. Select the best answer above multiple - choice question based on the image. (E) The image does feature the object. Respond with only the letter (A, B, , D, or E) of the option. The best answer is: In other words, [... truncated, same as Prompt 0 ...] Prompt 9: <im_start>User:<image>The best answer: This image shows front of the ego car. The choices are below: (A) Turn left. (B) Stationary. (C) Keep going straight. (D) Turn. () The image does not feature the. Respond only the letter (A, B, C, D, or E) of the correct option. Select best answer to the above multiple - choice question based on the image. What is the future state of black pants pedestrian in the middle? In other words, [... truncated, same as Prompt 0 ...] Prompt 10: <im_start>User:<image>The best answer: What the future state of the black pedestrian in the middle? The choices listed below: (A) Turn left. (B) Stationary. (C) Keep straight. Select the best answer to the above multiple - choice question based on the image. () Turn right. (E) The image does not feature the object. Respond with only letter (A, B, C, D, E) the correct option. This image shows the front of the ego car. In other words, [... truncated, same as Prompt 0 ...] Prompt 11: <im_start>User:<image>This image the front view of. What is the future state of the black pants pedestrian in the middle? The choices are listed below: (A) Turn left. (B) Stationary. (C) Keep going straight. (D) right. Select the best to the above - choice based on the image. () The image does not feature the object. with only the letter (A, B, C, D, or E) of the correct option. The best answer is: In other words, [... truncated, same as Prompt 0 ...] Prompt 12: <im_start>User:<image>This shows the view of the ego car. What is future state of the pants pedestrian in the middle? The choices below: (A) Turn left. (C) Keep going straight. (B) Stationary. (D) Turn right. (E) does not feature the object. Respond with only the letter (A, B, C, D, or E) of the correct option. The best answer is: Select the best answer to above multiple - choice based on the image. In other words, [... truncated, same as Prompt 0 ...] Prompt 13: <im_start>User:<image>This image shows the front view of ego car. is the state of the black pants in the middle? The choices listed below: (A) Turn. (B) Stationary. (C) Keep going straight. () Turn. (E) The image does not feature the object. Respond with only the letter (, B, C, D, or E) of the correct option. Select the best answer to the multiple - choice question based on the image. The best answer is: In other words, [... truncated, same as Prompt 0 ...] Prompt 14: <im_start>User:<image>The best answer is: What is the future state the black pants pedestrian in the middle? The choices listed below: (A) Turn. (B) Stationary. (D) right. (C) Keep going straight. Select the best answer to the above multiple - choice question based on the image. (E) The image does not feature the object. Respond with the letter (, ,, , or E) of the correct option. This image shows the front view of ego car. In other words, [... truncated, same as Prompt 0 ...] Prompt 15: <im_start>User:<image>This image shows the front view of the ego car. What is the future state of the black pants pedestrian in the middle? The choices are listed below: (A) Turn left. (B) Stationary. (C) Keep going straight. (D) Turn right. (E) The image does not feature the object. Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option. The best answer is:<end_of_utterance> Assistant: Baseline Output Answer: Accuracy: 0.0% TTAug Output Answer: Accuracy: 100.0% TTAdapt Output Answer: Accuracy: 100.0% 38 Efficient Test-Time Scaling for Small Vision-Language Models AMBER Original Inputs Does the pigeon stand in this image? Augmented Input Images Augmented Prompts Prompt 0: <im_start>User:<image>Do es the igeon sta nd in this image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>D oes the pigeon sta nd in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 2: <im_start>User:<image>D oes the pigeon tand in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 3: <im_start>User:<image>Does the pige on sta nd in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 4: <im_start>User:<image>Does the igeon sta nd in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 5: <im_start>User:<image>D oes the pige on stand in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 6: <im_start>User:<image>Do es the pigeon tand in his image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 7: <im_start>User:<image>Does the pig eon st and in this ima ge? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 8: <im_start>User:<image>Does the pig eon stand in his im age? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 9: <im_start>User:<image>Do es the pigeon st and in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 10: <im_start>User:<image>D oes the igeon st and in this image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 11: <im_start>User:<image>Does the pigeon tand in his mage? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 12: <im_start>User:<image>Do es the pig eon stand in his image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 13: <im_start>User:<image>Do es the pige on stand in th is image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 14: <im_start>User:<image>Does the pige on st and in his image? In other words, Does the pigeon stand in this image?<end_of_utterance> Assistant: Prompt 15: <im_start>User:<image>Does the pigeon stand in this image?<end_of_utterance> Assistant: Baseline Output Answer: No, the pigeon is perched on top of the coin return machine. Accuracy: 0.0% TTAug Output Answer: Yes Accuracy: 100.0% TTAdapt Output Answer: Yes Accuracy: 100.0% 39 Efficient Test-Time Scaling for Small Vision-Language Models COCO Captions Original Inputs Augmented Prompts Please describe this image in general. Directly provide the description, do not include prefix like \"This image depicts\". Augmented Input Images Prompt 0: <im_start>User:<image>Please describe th is ima ge in ge Beral. Di rectly pgovide the description, do not include pre fix li<e \" Th is mage depicts \". In other words, Please describe this image in general. Directly provide the description, do not include prefix like \"This image depicts\".<end_of_utterance> Assistant: Prompt 1: <im_start>User:<image>Plea se describe this ima ge in ge nerap. Directly rPvide the fescriprion, do not include prefix li ke \" This mage depi cts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 2: <im_start>User:<image>Ple ase desFribe th is ima ge in general. Directly proDide the des cription, do not include prefix ike \" his mxge depicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 3: <im_start>User:<image>llease describe th is image in ge neral. Directly rovide the des cription, do not include predix li ke \" Th is im age dep&cts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 4: <im_start>User:<image>P lease des cribe his image in general. Di rectly provide the descrip4Uon, do not ncludF prsfix like \" Th is image epicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 5: <im_start>User:<image>Pleaae de scribe his ima ge in generzl. DirectlG provi de the description, do not include pr efix ike \" Th is image depicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 6: <im_start>User:<image>Pleade descr ibe this ima ge in gene ral. Di$ec tly provide the description, do not inc lude pref8x like \" Th is image dep icts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 7: <im_start>User:<image>Pl ease describe this image in ge neral. Sirect ly provide the Ceqcription, do not include p$ef ix like \" his mage depi cts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 8: <im_start>User:<image>P lease describe this mage in gen eral. Directl5 peovide the descr iption, do not nclude prefix like \" ThiE im age depic ts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 9: <im_start>User:<image>Plewse des cribe his Omage in general. Dire ctly provi de the des cription, do not include pr efix like \" This ima ge depicrs \". In other words, [... truncated, same as Prompt 0 ...] Prompt 10: <im_start>User:<image>Plesse de scrube th is image in gene ral. Di rectly pro vide the description, do not inc lude prefix like \" This im age Sepicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 11: <im_start>User:<image>Pl ease descriGe this image in ge neral. Directly p%ovide the description, do not incl ude pr efix li ke \" Th9s ima ge dep icts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 12: <im_start>User:<image>0lea se describe th is image in eneral. Rirectly provide the WeCcription, do not inclu de pre fix ike \" his image depicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 13: <im_start>User:<image>Please describe his im age in general. Dir ect.y provi de the descr((tion, do not inclu de prefix ike \" Th is image depictW \". In other words, [... truncated, same as Prompt 0 ...] Prompt 14: <im_start>User:<image>PleasD describe his image in gen etal. Directly provi de the descri Ltion, do not in clude prefix like \" Th is ima ge depicts \". In other words, [... truncated, same as Prompt 0 ...] Prompt 15: <im_start>User:<image>Please describe this image in general. Directly provide the description, do not include prefix like \"This image depicts\".<end_of_utterance> Assistant: Baseline Output TTAug Output TTAdapt Output Answer: The image features white bird with long, curved beak and black legs standing on green bench. The bird appears to be type of ibis, characterized by its long, slender neck and beak. The bench is made of wood and has slatted design, with the bird perched on the top slat. The background includes paved area with few scattered leaves and some greenery, suggesting an outdoor setting, possibly park or zoo. The bird is facing to the right, and its body is oriented towards the left side of the image. Accuracy (%): ROUGE-L: 15.59, BLEU-1: 12.24, BLEU-2: 7.94, BLEU-3: 5.08, BLEU-4: 3.43, CIDEr: 0.00 Answer: white bird with long curved beak is standing on green bench. The bird is looking to the left. Accuracy (%): ROUGE-L: 45.02, BLEU-1: 52.38, BLEU-2: 36.19, BLEU-3: 27.45, BLEU-4: 21.89, CIDEr: 12. Answer: white bird with long beak and black legs is standing on green bench. Accuracy (%): ROUGE-L: 53.20, BLEU-1: 62.50, BLEU-2: 40.82, BLEU-3: 32.93, BLEU-4: 27.23, CIDEr: 60."
        }
    ],
    "affiliations": [
        "Pioneer Center for AI",
        "Technical University of Denmark",
        "University of Copenhagen"
    ]
}