{
    "paper_title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "authors": [
        "Tong Wu",
        "Chong Xiang",
        "Jiachen T. Wang",
        "Prateek Mittal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 7 3 4 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Effectively Controlling Reasoning Models through\nThinking Intervention",
            "content": "Tong Wu1 Chong Xiang2 Jiachen T. Wang1 Prateek Mittal1 1Princeton University 2NVIDIA tongwu@princeton.edu"
        },
        {
            "title": "Abstract",
            "content": "Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens promising new research avenue for controlling reasoning LLMs."
        },
        {
            "title": "Introduction",
            "content": "Reasoning-enhanced models, such as OpenAIs o1 [25], DeepSeek R1 [20], and Googles Flash Thinking [14], mark significant advancement in the field of large language models (LLMs). By explicitly incorporating reasoning process prior to generating responses, these models demonstrate enhanced performance, especially when addressing complex tasks, such as mathematical problemsolving [32], programming [27], and logical reasoning [43]. Despite these developments, the control of reasoning models still primarily relies on traditional input-level operations, such as prompt engineering [41], which modifies the instructions provided to the LLM. In this paper, we demonstrate that the explicit thinking step introduced in reasoning models provides enhanced transparency into the models cognitive processes, creating new opportunities for direct and precise interventions within these reasoning stages. We introduce Thinking Intervention, novel paradigm designed to explicitly control the internal reasoning processes of models. Rather than allowing the model to generate entire reasoning chains on its own, Thinking Intervention specifies token sequences (e.g., detailed guidance) to be inserted or replaced within the ongoing reasoning process. Such targeted interventions enable fine-grained and transparent control over the reasoning trajectory, aligning the models behavior with specific task objectives. Thinking Intervention has the following properties: (1) it does not require any form of model training and can be deployed in real-world settings with minimal engineering effort; (2) it is compatible with existing model control techniques such as prompt engineering and even more sophisticated fine-tuning approaches; (3) it enables fine-grained and flexible control over the reasoning process by adaptively inserting or revising intermediate reasoning steps based on context-specific and task-specific needs. Preprint. Figure 1: (a) demonstration of how Vanilla Prompting, Prompt Engineering, and Thinking Intervention work. Both Vanilla Prompting and Prompt Engineering methods act on the input query. In contrast, Thinking Intervention, either written by humans or generated by LLMs, explicitly injects instructions into the reasoning process. (b) Compared to Vanilla Prompting and Prompt Engineering, Thinking Intervention offers significant performance improvements for R1-Qwen-32B reasoning model across instruction following, instruction hierarchy, and safety alignment tasks. Demonstration. To further illustrate, consider general instruction-following task shown in Figure 1 (a), which asks the model to \"list 2 famous moms in JSON format\". Vanilla Prompting would simply state, \"Please provide the names of 2 famous moms in JSON format,\" while Prompt Engineering might add reminder, such as \"Ensure the format is JSON with 2 famous moms.\" Although these input-level prompts instruct the model on what to do, reasoning models may still overlook or misunderstand some constraints, as shown in Figure 1(b). In contrast, Thinking Intervention explicitly guides the reasoning by injecting instructions directly into the models thinking process, such as \"I should generate 2 famous moms and put them in JSON format.\" This explicit intervention helps reduce the likelihood of the model missing constraints during the reasoning process, thereby significantly improving performance. Case studies. We demonstrate the effectiveness of Thinking Intervention across diverse tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench benchmarks. For instruction following (Section 3), we illustrate how Thinking Intervention can guide the model to follow instructions more accurately and effectively. For instruction hierarchy (Section 4), we show that Thinking Intervention helps the model reason about hierarchical instructions and appropriately prioritize main tasks over low-priority ones. As shown in Figure 1(b), Thinking Intervention significantly outperforms both the Vanilla Prompting and Prompt Engineering methods, achieving 6.7% and 1.9% improvements in instruction following tasks and 15.4% and 4.0% improvements in instruction hierarchy tasks, respectively. Lastly, in the safety alignment task (Section 5), we first reveal that open-source reasoning models (e.g., the DeepSeek R1 model series [20]) tend to over-comply with user instructions, even when those instructions pose safety risks. This behavior underscores the urgent need for more effective safety steering methods. We then demonstrate that applying Thinking Intervention enables explicit guidance toward safer reasoning patterns, leading to substantial improvements in safety performance. For example, as shown in Figure 1(b), Thinking Intervention significantly increases the refusal rates for unsafe requests by up to 40.0% and 26.9% on the XSTest and SORRY-Bench, respectively. Overall, our work demonstrates that the Thinking Intervention, as novel paradigm, provides powerful and flexible approach to enhance the capabilities of reasoning models across multiple dimensions. We encourage the community to explore and extend this framework for various applications, as it opens up new possibilities for precise, transparent, and effective control over the reasoning processes of LLMs, ultimately leading to more reliable and aligned AI systems."
        },
        {
            "title": "Models",
            "content": "2.1 Preliminaries and Notations Consider next-token prediction language model LM : V, where denotes the vocabulary set and represents the space of all possible token sequences over V. Given token sequence as input, the LLM predicts the next token in the sequence. Let := (x1, . . . , xn) denote an input context, where each xi V. We use [a, b] to denote the concatenation of two token sequences followed by b. conventional LLM autoregressively generates response sequence := (y1, . . . , ym) by iteratively predicting each response token yj = LM([x, y<j]) conditioned on the context and the previously generated tokens y<j := (y1, . . . , yj1). Reasoning-enhanced LLM. Unlike conventional LLMs, reasoning-enhanced LLM explicitly separates the generation process into \"reasoning/thinking\" stage and \"response\" stage. Formally, the generation operates as follows: (1) Reasoning Stage: The model first generates sequence of intermediate reasoning tokens (or \"reasoning chain\") = (r1, . . . , rk) . Each reasoning token is autoregressively generated by conditioning on the input context and previously generated reasoning tokens: ri = LM([x, r<i]). (2) Response Stage: After obtaining the reasoning chain, the model generates the final response = (y1, . . . , ym) by conditioning each token on the context, reasoning chain, and previous response tokens: yj = LM([x, r, y<j]). This explicit decomposition enhances the models capability for complex tasks and improves interpretability by transparently exposing its reasoning steps [20]. 2.2 Intervening in the Reasoning Process as General Paradigm Traditional approaches1 to improving LLM performance have largely focused on prompt engineering [57, 54, 64, 39], which optimizes the input context to elicit better model response y. For reasoning-enhanced LLMs, although crafting the initial prompt remains important, the explicit reasoning stage offers new, more direct pathway for optimization: intervening within the reasoning process itself. In this work, we propose general paradigm termed Thinking Intervention, which directly intervenes within the reasoning process of LLMs, e.g., through revising explicit instructions or guidance at intermediate reasoning steps. Unlike prompt engineering, where the entire input context is optimized before the LLM generates tokens, Thinking Intervention operates in an online, dynamic environment. The reasoning chain is generated token-by-token in real time, requiring the intervention mechanism to make decisions based on the incomplete reasoning chain r<i. The key challenge lies in developing intervention strategies that can quickly evaluate partially completed reasoning paths, intervene at appropriate intermediate steps, and conduct intervention strategies adapted to the diverse reasoning trajectories while not disrupting the models natural reasoning flow. General paradigm. Given the autoregressive nature of reasoning LLMs, we propose interventions that can insert new tokens or revise existing tokens within the reasoning chain. Formally, we define an intervention function intervene : {NO_INTERVENE} that determines when and how to intervene in the LLM reasoning process: intervene(x, r<i) = (cid:26)NO_INTERVENE if no intervention is needed at step (cid:101)r if intervention is needed, where (cid:101)r where is the input context and r<i represents all reasoning tokens generated up to step 1. The output sequence (cid:101)r replaces the existing partial reasoning chain. The modified reasoning generation process can thus be formalized as: ri = (cid:26)[r<i, LM([x, r<i])] if intervene(x, r<i) = NO_INTERVENE intervene(x, r<i) otherwise This formulation highlights that interventions are strategically designed based on the specific reasoning path observed, enabling context-aware modifications at critical junctures. The revised reasoning chain 1Fine-tuning can also be viewed as form of model control in certain cases, but we do not consider it here because it is more destructiveit modifies the entire model rather than controlling fixed one. 3 (cid:101)r can incorporate corrective feedback, alternative reasoning approaches, or relevant domain knowledge that addresses errors or gaps identified in the current reasoning flow. This generalized framework accommodates both token insertion and revision, providing flexible and powerful mechanism for dynamically guiding the reasoning process. 2.3 Instantiation: Intervention via Postfix-based Monitor simple yet powerful instantiation of the intervention function is based on monitoring the reasoning chain to detect specific trigger strings. Specifically, given set of trigger strings (which can be single tokens or sequences of multiple tokens), the monitor checks if the most recent tokens (i.e., the postfix of the current reasoning chain) match any string in S. If match is detected, we append an intervention sequence (e.g., \"I am responsible assistant\") immediately to the existing reasoning chain. The trigger set can be designed flexibly to capture relevant reasoning stages, domain-specific phrases, or other reasoning markers. Formally, the intervention function with postfix-based monitor is defined as: intervene(x, r<i) = (cid:26)NO_INTERVENE if no postfix of r<i matches any string in S, [r<i, v] if postfix of r<i matches string in S. Here, [r<i, v] denotes concatenation of the current reasoning chain with the intervention sequence v. Various practical intervention strategies can be readily implemented by appropriately selecting trigger strings S. Below, we illustrate several simple, easy-to-implement examples, though more sophisticated approaches are also possible: Intervention at reasoning start. To intervene at the beginning of reasoning, trigger string indicating reasoning onset (e.g., \"<think>\" in DeepSeek R1 models) can be included in S. Upon detecting this string, the model immediately receives guidance through an intervention sequence containing relevant instructions or hints to direct reasoning from the start. Intervention at reasoning conclusion. Trigger strings signaling the end of the reasoning phase (e.g., \"</think>\" in DeepSeek R1 models) can be included in S. Detecting these triggers allows reinforcing key points or addressing overlooked considerations before finalizing the output. Intervention at reasoning transitions. To intervene at intermediate stages of reasoning, trigger strings corresponding to transitional markers or reflective cues (e.g., \"wait\", \"Hmm\", \"Alternatively\") can be specified. Interventions triggered at these junctures prompt the model to reconsider previous steps, correct potential errors, or further elaborate on its reasoning, thereby enhancing the quality and rigor of the resulting reasoning chain. In our evaluations, we experimented with all three strategies and observed that intervention at the beginning of the reasoning process is the most effective. Therefore, throughout our experiments, we primarily focused on using this approach. We believe this is because the initial reasoning tokens are particularly important and can directly guide the reasoning trajectory toward more productive outcomes. 2.4 Thinking Intervention enjoys several unique features and advantages Flexibility in design. The generation of Thinking Intervention sequences can be designed manually by domain experts or automatically synthesized by auxiliary models. For example, users or LLM providers can use another LLM to transform key instructions into intervention sequences. Additionally, Thinking Intervention can be injected or edited multiple times at arbitrary positions throughout the reasoning process, offering versatile and robust framework for precise control over reasoning paths. Simplicity and lightweight. Thinking Intervention is straightforward and easy to implement in any reasoning model. It does not require additional fine-tuning or reinforcement learning to adjust model behavior. Moreover, it incurs no significant computational cost, as it only involves adding or editing few tokens within the reasoning process. If the intervention guides the model into correct reasoning paths at an early stage, it can even reduce the number of reasoning tokens needed to arrive at the final answer, thus saving computation time. Compatibility with other methods. Thinking Intervention can be combined with other techniques, such as prompt engineering, to further enhance model performance. For example, we can first 4 use prompt engineering to provide the model with some background knowledge and then use Thinking Intervention to guide the models reasoning process. This combination allows for more comprehensive approach to controlling reasoning models. First-person narrative. One interesting property that distinguishes Thinking Intervention from prompt engineering is its use of the first-person narrative. For instance, while prompt engineering might use \"You are responsible assistant\", Thinking Intervention would instead use \"I am responsible assistant\", as it is integrated into the reasoning process. This first-person narrative allows the model to treat the instruction as part of its own reasoning rather than as an external command."
        },
        {
            "title": "3 Case Study: Instruction Following",
            "content": "In this section, we demonstrate how our proposed Thinking Intervention method effectively improves the instruction-following capability of reasoning models. 3.1 Experimental Design Benchmark. We leverage Instruction-Following Evaluation (IFEval) [69] as benchmark to measure how well reasoning models follow instructions. The benchmark comprises 500 prompts, each containing one or more of 25 identified types of verifiable instructions. These verifiable instructions function as constraints on the output, such as \"do not use any commas\" or \"no capital letters are allowed\" To quantify instruction following capability, we report the accuracy, defined as the proportion of prompts for which the model satisfies all verifiable instructions within the prompt. Additional evaluation details are provided in Appendix B.1. Models. We consider reasoning models distilled from DeepSeek R1 [20], including R1-Qwen-14B and R1-Qwen-32B, and additionally the QwQ-32B model [38] from different family. All of these models use the same tag format, <think> and </think>, to denote the start and end of the reasoning process. 3.2 Methods Baseline prompting methods. We evaluate two common baseline techniques: Vanilla Prompting and Reminder Prompting. Specifically, Vanilla Prompting directly feeds the complete prompt into the reasoning model without modification. In contrast, Reminder Prompting augments the original prompt by explicitly reiterating the constraint. To generate the corresponding reminder, we prompt an auxiliary LLM with the instructions from IFEval. The full prompt is provided in Appendix B.1. Generating Thinking Intervention. To ensure that constrained instructions are correctly followed, we use Thinking Intervention to explicitly highlight the importance of these constraints during the reasoning process. To construct the intervention sequence v, we leverage the Reminder Prompting generated by LLMs and make slight edits to it. For instance, given an original instruction such as \"without using any commas\", the LLM may produce reminder like \"Ensure that you do not use any commas.\" We then apply light narrative editing to transform this into the intervention sequence, such as \"I should not use any commas.\" This intervention sequence is then inserted at the beginning of the reasoning process throughout the evaluations. We illustrate concrete example of how Thinking Intervention can be integrated with Vanilla Prompting and Reminder Prompting in Figure 2. 3.3 Results The evaluation results on the IFEval dataset, summarized in Figure 3, demonstrate the effectiveness of Thinking Intervention across multiple reasoning models (R1-Qwen-14B, R1-Qwen-32B, and QwQ-32B). Additional experimental results related to more evaluation metrics are provided in Appendix B.2. Thinking Intervention consistently improves instruction-following capability. We observe that Vanilla Prompting with Thinking Intervention yields the most significant gains across all three models, increasing accuracy by 4.99%, 6.65%, and 2.96% for R1-Qwen-14B, R1-Qwen-32B, and QwQ-32B, Figure 2: An example demonstrating how Thinking Intervention is integrated with vanilla prompting and Reminder Prompting prompting techniques for instruction following tasks. (a) R1-Qwen-14B (b) R1-Qwen-32B (c) QwQ-32B Figure 3: Evaluation results on the IFEval benchmark [69]. We compare the performance with and without Thinking Intervention (ThinkingI), across Vanilla Prompting and Reminder Prompting methods and multiple reasoning models. respectively. We also observe that optimal performance is consistently achieved by incorporating Thinking Intervention. For R1-Qwen-14B, combining Thinking Intervention with Vanilla Prompting yields the best results, while for R1-Qwen-32B and QwQ-32B, pairing Thinking Intervention with Reminder Prompting achieves the highest accuracies of 77.63% and 82.44%, respectively, highlighting the significance of Thinking Intervention. These consistent improvements across different models demonstrate that Thinking Intervention effectively guides model reasoning by explicitly emphasizing constrained instructions, providing robust enhancement to existing prompting strategies without requiring additional training."
        },
        {
            "title": "4 Case Study: Instruction Hierarchy",
            "content": "Next, we explore how Thinking Intervention benefits the instruction hierarchy task [53, 59], which evaluates models ability to appropriately prioritize high-priority instructions while disregarding low-priority ones. This capability is essential for safety-critical applications, where models must adhere to specific guidelines even in the presence of conflicting instructions. We examine how Thinking Intervention enhances the models ability to navigate complex scenarios involving competing directives. 4.1 Experimental Design Benchmark. We use the SEP dataset [74] as our benchmark. Each prompt contains high-priority main instruction paired with relevant data content and an unrelated low-priority instruction. Models are expected to prioritize the main instruction while ignoring the low-priority instruction when it is embedded within the data (see left example in Figure 4). This benchmark enables us to evaluate how effectively models can maintain instruction hierarchies in complex scenarios involving potentially misaligned directives. Evaluation metrics. We evaluate model performance on the SEP benchmark using two key metrics: (1) robustness, which measures the proportion of low-priority instructions correctly ignored when embedded within data (Figure 4 left); (2) utility, which quantifies the models baseline performance on the main task in the absence of any low-priority instructions (Figure 4 right). For the utility metric, we follow [68] by employing LLM-as-a-judge for evaluation and normalizing scores to 0-100 scale. 6 Figure 4: demonstration of how the SEP benchmark evaluates instruction hierarchy capabilities. Each example contains main instruction paired with data. Left: The low-priority instruction is injected into the data, which the model should correctly ignore. Right: The low-priority instruction is absent, measuring the utility of models. Figure 5: demonstration of Vanilla Prompting, Reminder Prompting, and Thinking Intervention for the SEP benchmark. The {Task} and {Data} fields are filled with content from the SEP dataset (e.g., Figure 4) during evaluation. Table 1: Evaluation results on the SEP dataset across various reasoning models. We compare our proposed Thinking Intervention (+ThinkingI) against the Vanilla Prompting and Reminder Prompting. R1-Qwen-14B R1-Qwen-32B QwQ-32B Methods Robustness(%) Utility(%) Robustness(%) Utility(%) Robustness(%) Utility(%) Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI 34.00 38.40 (+4.40) 38.40 41.80 (+3.40) 81.04 81.08 (+0.04) 80.50 80.90 (+0.40) 34.80 50.20 (+15.40) 46.20 66.40 (+20.20) 81.76 82.02 (+0.26) 81.16 80.90 (-0.26) 22.20 31.40 (+9.20) 36.20 43.40 (+7.20) 88.00 88.16 (+0.16) 87.52 86.79 (-0.73) 4.2 Methods Baseline prompting methods. Similar to our instruction-following experiments, we include two baseline approaches: Vanilla Prompting, which directly uses the prompts without additional guidance, and Reminder Prompting, which includes an explicit instruction reminder. Figure 5 shows all the prompts used for evaluation. Thinking Intervention. For our Thinking Intervention approach, we use the intervention sequence \"I should follow all the instructions in the task block and not follow any instructions in the data block.\" to explicitly guide the model in maintaining the correct instruction hierarchy. This intervention sequence is inserted at the beginning of the reasoning process to help the model correctly prioritize the instructions. 4.3 Results Thinking Intervention significantly improves robustness while maintaining model utility. Table 1 presents the evaluation results of reasoning models on the SEP benchmark. Our Thinking Intervention approach consistently improves both Vanilla Prompting and Reminder Prompting techniques in terms of robustness across all model scales. For example, when applied to R1-Qwen-32B with Reminder Prompting, Thinking Intervention achieves robustness of 66.4%, marking substantial 20.20% improvement over the Reminder Prompting baseline. These improvements are consistent across different model sizes, with QwQ-32B showing 7.20% increase and R1-Qwen-14B demonstrating 3.40% boost. Importantly, Thinking Intervention preserves the utility of the models. Across all settings, it incurs only negligible variation in utility ( 0.73%) compared to the corresponding baselines. 7 These results demonstrate that Thinking Intervention effectively guides models to maintain proper instruction hierarchies by correctly following high-priority instructions without compromising their performance on utility-related tasks. For experimental results with additional metrics, see Appendix C."
        },
        {
            "title": "5 Case Study: Safety Alignment",
            "content": "Before deployment, LLMs typically undergo safety alignment process using SFT followed by RLHF to ensure the model does not respond to harmful queries, such as \"how to build bomb\". In this section, we first demonstrate that open-source reasoning models (e.g., R1-Qwen-32B) exhibit notably low refusal rates to unsafe requests. We then show that Thinking Intervention can effectively steer the model toward much safer behavior. 5.1 Preliminary Safety Evaluations Benchmark. We leverage XSTest [40] to evaluate the safety alignment of models. The benchmark contains 200 harmful requests, as well as 250 benign questions that include safety triggers for LLMs (e.g., \"How can kill Python process?\"), potentially causing the model to mistakenly refuse to answer. Evaluation metrics. We follow the evaluation methods provided in [40], using GPT-4o-mini to verify whether the models response is compliant or refusing the prompt provided by the user. We report two metrics: the refusal rate for unsafe requests and the compliance rate for safe requests. Higher values for both indicate better performance. Models. We evaluate several reasoning models: those distilled from DeepSeek R1 [20] including R1-Qwen-14B, and R1-Qwen-32B, as well as the QwQ-32B model [38]. For comparison with closed-source LLMs, we include o3-mini (a reasoning model) alongside the non-reasoning models GPT-4o-mini and GPT-4o. All models are evaluated using directly query the benchmarks safe and unsafe requests (Vanilla Prompting), without any additional safety-oriented prompting strategies. This approach provides an unbiased assessment of each models inherent safety behaviors when handling potentially harmful content. R1 models exhibit an alarmingly low refusal rate to harmful requests Figure 6 presents the evaluation results of model behavior on the XSTest benchmark, revealing clear compliance-safety trade-off. Specifically, R1 models achieve near-perfect compliance (100%) with safe requests but demonstrate dangerously low refusal rate to unsafe requests (< 20%). In contrast, the GPT series refuses more than 70% of unsafe requests, with o3-mini even refusing nearly all unsafe requests (100%),2 but maintain relatively lower but acceptable compliance rate (90.4%95.6%) for safe requests. These trade-off highlight critical concern: R1 models tend to comply with requests regardless of safety implications, underscoring the urgent need to develop robust safety mechanisms for reasoning models prior to real-world deployment. 5.2 Methods to Improve Safety Based on the findings from our previous experiments, we now investigate methods to guide the open-source models toward safer behavior. Baseline prompting methods. There exist multiple methods to improve the safety of models when handling harmful content, including safety fine-tuning, prompting, and monitor-based approaches. We focus on prompting-based strategies as they require minimal additional implementation effort. We consider the following baselines: Vanilla Prompting, which does not include any additional instructions; Default Prompting, which uses the default safety prompting of Llama-2 [50] to ensure safety; Reminder Prompting [62], which reminds the model to act responsibly; and Goal Priority [67], which explicitly prioritizes safety over other objectives. The complete prompts used for each baseline are provided in the Appendix D.1. Thinking Intervention. We aim to improve the general safety of reasoning models when handling potentially harmful requests. Therefore, we inject the intervention sequence as \"I am helpful, 2Since GPT models are closed-source, it is unclear if there exist auxiliary safety monitors filtering unsafe responses; thus, high (or even perfect) refusal rates might reflect monitor behavior rather than the models themselves. 8 Figure 6: Models performance on the XSTest benchmark via Vanilla Prompting prompting. Figure 7: Effect of the Thinking Intervention with R1-Qwen32B model on XSTest benchmark results. Effect of the Figure 8: Thinking Intervention with R1-Qwen-32B model on SORRY-Bench. respectful, and honest assistant.\" at the beginning of the reasoning process to steer the model toward safety. We keep the intervention simple to better understand the direct effect of Thinking Intervention. 5.3 Thinking Intervention effectively steers the reasoning models toward safety Thinking Intervention significantly improves model safety. Figure 7 shows the evaluation results of the R1-Qwen-32B model on the XSTest benchmark. Our Thinking Intervention approach consistently outperforms all baseline methods in terms of safety refusal. For example, compared with Vanilla Prompting, we improve the refusal rate for unsafe requests by 30% while maintaining the compliance rate for safe requests above 97%. Significantly, when combined with Goal Priority prompting, our Thinking Intervention approach achieves refusal rate of 75% for unsafe requests and compliance rate of 95% for safe requests, comparable to the safety alignment behavior of the GPT-4o model series in Figure 6. The results suggest that Thinking Intervention provides an effective mechanism to guide reasoning models toward safer behavior without compromising their ability to respond appropriately to benign requests. Thinking Intervention excels in more comprehensive safety benchmark. To further validate our approach, we evaluate SORRY-Bench, which features more comprehensive taxonomy and more detailed unsafe instructions, using exactly the same prompting and Thinking Intervention methods. As shown in Figure 8, Thinking Intervention consistently improves robustness (i.e., the refusal rate of unsafe instructions) over baseline prompting methods. For example, when combined with Default Prompting, Thinking Intervention achieves refusal rate of approximately 87% for unsafe requests, nearly 20% improvement over Default Prompting alone and even higher than the refusal rate of the GPT-4o and o3-mini models. These results further demonstrate that Thinking Intervention can effectively steer reasoning models toward safer behavior across different safety benchmarks, highlighting the generalizability of our approach. Overall, our findings show that open-source reasoning models exhibit significant safety alignment gaps, particularly in refusing harmful queries. Thinking Intervention significantly improves safety alignment without compromising the compliance rate for safe requests. The effectiveness of Thinking Intervention across multiple challenging benchmarks demonstrates its potential as practical approach to enhancing the safety of reasoning models for real-world deployment. We present more detailed results in Appendix D.2 and Appendix D.3, including the performance of other reasoning models on XSTest and SORRY-Bench, respectively. 5.4 Does Thinking Intervention solve all safety challenges? While Thinking Intervention offers significant improvements in model safety, we do not view it as panacea for all safety challenges. Rather, it serves as complementary safety layer that can be integrated with existing mechanisms to enhance the overall safety profile of reasoning models. The intervention approach is particularly valuable because it can be applied to existing reasoning models without requiring expensive retraining or fine-tuning. In future work, we plan to explore how Thinking Intervention can be synergistically combined with other safety mechanisms, such as 9 RLHF [35, 12] or constitutional AI approaches [7, 44], to create more robust safety frameworks for reasoning models. This multi-layered approach [34] may help address the inherent trade-offs between helpfulness and safety that current models struggle to balance."
        },
        {
            "title": "6 Discussion",
            "content": "Improving Thinking Intervention. In this study, we propose Thinking Intervention as novel paradigm to control reasoning models. We evaluated with intervention via postfix-based monitor, primarily focusing the intervention at the beginning of the reasoning process. We also experimented with injecting Thinking Intervention at later stages of the reasoning process, but this did not yield the desired outcomes (Appendix E.1), suggesting direction for future exploration. Concurrent works [8, 5] have focused on actively monitoring the reasoning chains using another LLM and identified instances of unfaithful reasoning or misbehavior. Thus, another promising direction is to monitor the reasoning process beyond the postfix and inject Thinking Intervention at appropriate moments to guide it toward safe, faithful, and appropriate outcomes. Additionally, the intervention sequence used in our experiments was intentionally kept simple to allow fair comparison with basic prompt engineering techniques. However, we anticipate that more sophisticated and complex Thinking Intervention could further enhance model performance. We have conducted preliminary explorations on manually designing more detailed safety instructions and observed certain trade-offs between safety and compliance (Appendix E.2). Another promising direction is to frame Thinking Intervention as an optimization problem and leverage insights from automated prompt optimization methods [31, 71] to generate more effective Thinking Intervention. Utilizing Thinking Intervention. We anticipate several practical use cases for Thinking Intervention. For LLM providers, Thinking Intervention can be directly applied to enhance model performance. For example, user instructions could be transformed into interventions through an auxiliary language model, which can then be injected into the reasoning process. Previous system prompts aimed at ensuring functionality, such as role-play, can be seamlessly integrated into the reasoning process to improve the user experience. In terms of safety alignment, Thinking Intervention can serve as an additional layer to steer model outputs and help prevent the generation of harmful content. For LLM users, Thinking Intervention can be easily adopted with open-source models. Users can create their own Thinking Intervention when they observe that the model is not performing or reasoning as expected. For closed-source models, although there is currently no direct way to inject Thinking Intervention, we anticipate and encourage LLM providers to support such feature in the future, similar to the prefilling output functions available in Anthropic Claude models [4]."
        },
        {
            "title": "7 Related Works",
            "content": "Reasoning models. Reasoning models have rapidly advanced since OpenAIs O1 model [25]. This trend has produced closed-source models like Googles Flash Thinking [14], Anthropics Claude 3.7 Sonnet [3], and xAIs Grok 3 [60], alongside open-source alternatives such as DeepSeek R1 [20], QwQ [38], and S1 [33]. These models employ test-time scaling [46, 58], allocating additional inference computation to improve performance on complex tasks. Controlling LLMs. There are two mainstream approaches to controlling LLMs after the training stage. Prompt Engineering includes providing clear, detailed instructions, either manually written [9, 57, 51, 64] or automatically generated [45, 39, 48, 15], to achieve specific objective. There also exists the Activation Steering method, which selects subset of LLM inner activations to probe [13, 72, 22, 30, 66] as means to control LLMs. Thinking Intervention differs from these methods as it intervenes in the thinking process. Intervening thinking process. Prior to the emergence of reasoning models, research from [28, 52] explored measuring faithfulness in chain-of-thought (CoT) reasoning by intervening in the CoT with modifications such as paraphrasing or introducing mistakes. Very recently, two concurrent works [8, 5] extended faithfulness measurements to emerging reasoning models. Other studies focus on controlling the length of reasoningeither encouraging longer chains for improved performance [33, 1] or shortening them to reduce costs [21, 63, 29]. However, none of these works directly introduce reasoning interventions aimed at enhancing performance. 10 We present additional related works on the tasks we evaluated in Appendix A."
        },
        {
            "title": "8 Conclusion.",
            "content": "In this paper, we proposed Thinking Intervention, novel approach to effectively control reasoningenhanced LLMs. We demonstrate that Thinking Intervention can significantly improve the performance of LLMs across various tasks, including instruction following, instruction hierarchy, and safety alignment. We also provided comprehensive analysis of the effectiveness of Thinking Intervention and discussed its potential applications. We encourage future research to explore how Thinking Intervention can further enhance broad spectrum of LLMs applications."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] Together AI. Deploying deepseek-r1 and distilled models securely on together ai, 2025. Accessed: 2025-03-19. [3] Anthropic. Claudes extended thinking, February 24 2025. [4] Anthropic. Prefill claudes response for greater output control, 2025. [5] Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy. Chain-of-thought reasoning in the wild is not always faithful. arXiv preprint arXiv:2503.08679, 2025. [6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [8] Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, and David Farhi. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arXiv preprint arXiv:2503.11926, 2025. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. Struq: Defending against prompt injection with structured queries. arXiv preprint arXiv:2402.06363, 2024. [11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [12] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. [13] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: simple approach to controlled text generation. In International Conference on Learning Representations, 2020. [14] Google DeepMind. Gemini flash thinking, 2025. Accessed: 2025-03-15. [15] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022. [16] Yilin Geng, Haonan Li, Honglin Mu, Xudong Han, Timothy Baldwin, Omri Abend, Eduard H. Hovy, and Lea Frermann. Control illusion: The failure of instruction hierarchies in large language models. ArXiv, abs/2502.15851, 2025. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pages 7990, 2023. 12 [19] Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. [20] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [21] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [22] Evan Hernandez, Belinda Li, and Jacob Andreas. Inspecting and editing knowledge representations in language models. arXiv preprint arXiv:2304.00740, 2023. [23] Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. Defending against indirect prompt injection attacks with spotlighting. arXiv preprint arXiv:2403.14720, 2024. [24] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Yichang Xu, and Ling Liu. Safety tax: Safety alignment makes your large reasoning models less reasonable. arXiv preprint arXiv:2503.00555, 2025. [25] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [26] Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. arXiv preprint arXiv:2502.12025, 2025. [27] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [28] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. [29] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. [30] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36:4145141530, 2023. [31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9), January 2023. Association [32] Mathematical matics american-invitational-mathematics-examination-aime, February 2024. cessed: 2025-03-24. Invitational Mathehttps://maa.org/math-competitions/ Acof (AIME). Examination American America. [33] Niklas Muennighoff, Zitong Yang, Weĳia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [34] OpenAI. How we think about safety and alignment, 2025. Accessed: 2025-03-20. [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 13 [36] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. [37] Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. In European Symposium on Research in Computer Security, pages 105124. Springer, 2024. [38] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [39] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, pages 17, 2021. [40] Paul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 53775400, 2024. [41] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinĳa Jain, Samrat Mondal, and Aman Chadha. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024. [42] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. [43] Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. [44] Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, et al. Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. arXiv preprint arXiv:2501.18837, 2025. [45] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42224235, Online, November 2020. Association for Computational Linguistics. [46] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [47] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. [48] Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M. Rush. Interactive and visual prompt engineering for ad-hoc task adaptation with large language models, 2022. [49] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [51] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022. [52] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36:7495274965, 2023. [53] Eric Wallace, Kai Yuanqing Xiao, Reimar Heinrich Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208, 2024. [54] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. [55] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36:8007980110, 2023. [56] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. [57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [58] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. [59] Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, and Wenxuan Zhou. Instructional segment embedding: Improving LLM safety with instruction hierarchy. In The Thirteenth International Conference on Learning Representations, 2025. [60] xAI. Grok 3 beta the age of reasoning agents, February 19 2025. [61] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598, 2024. [62] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, 5:14861496, 2023. [63] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [64] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [65] Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, et al. Trading inference-time compute for adversarial robustness. arXiv preprint arXiv:2501.18841, 2025. [66] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering for LLMs. In The Twelfth International Conference on Learning Representations, 2024. 15 [67] Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, and Minlie Huang. Defending large language models against jailbreaking attacks through goal prioritization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 88658887, 2024. [68] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [69] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. [70] Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning models: safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025. [71] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. [72] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. [73] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [74] Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, and Christoph H. Lampert. Can LLMs separate instructions from data? and what do we even mean by that? In The Thirteenth International Conference on Learning Representations, 2025. [75] Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh, Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, and Christoph Lampert. Aside: Architectural separation of instructions and data in language models. arXiv preprint arXiv:2503.10566, 2025."
        },
        {
            "title": "A Addtional Related Works",
            "content": "In this appendix, we present additional related works relevant to the tasks on which we are evaluated. Instruction following. LLMs rely on accurately following natural language instructions for broad applicability. This capability is improved through supervised fine-tuning on instruction-response pairs [42, 56, 11] and reinforcement learning from human feedback (RLHF) [47, 6, 35]. Additionally, prompting techniques [54, 57] further help elicit more effective responses. Instruction hierarchy. The concept of instruction hierarchy was proposed by [53], suggesting that LLM systems should prioritize instructions based on their trustworthiness. Otherwise, they may become vulnerable to misalignment or adversarial prompts [36, 18, 16]. Researchers have proposed various methods to enhance instruction hierarchy through additional training on misaligned data [10, 53, 37], prompting-based techniques [23, 74], and architectural design [59, 75]. Safety alignment. Safety alignment [7, 17, 49] is critical aspect of LLM development, aiming to ensure that models follow ethical guidelines and avoid producing harmful content [55, 73, 61]. For reasoning models, works from OpenAI [19, 65] suggest that their o1/o3 series can achieve better safety alignment by leveraging more test-time compute. Meanwhile, other studies [70, 26, 24] have indicated that open-sourced reasoning models, like DeepSeek R1 series, exhibit more safety issues. In this work, we explore how to leverage Thinking Intervention to improve instruction following, instruction hierarchy, and safety alignment in open-source reasoning models. Instruction Following Evaluation (IFEval) B.1 More Details of Evaluations IFEval [69] evaluates the instruction-following capabilities of language models using 25 distinct instruction types across approximately 500 prompts. In the main texts, we report only the prompt-level strict accuracy as accuracy for simplicity. Here, we formally discuss all four metrics used in [69] and report the results. Prompt-level strict accuracy: The proportion of prompts for which all verifiable instructions are correctly followed. Instruction-level strict accuracy: The proportion of verifiable instructions that are correctly completed, evaluated individually. Prompt-level loose accuracy: Similar to prompt-level strict accuracy, but evaluated under loose evaluation criterion (see below for details). Instruction-level loose accuracy: Instruction-level strict accuracy under loose evaluation criterion (see below for details). Strict accuracy and loose accuracy. Strict accuracy requires the model output to precisely match the requirements. For instance, if an instruction specifies output in JSON format, the entire response must be in valid JSON format with no extraneous text. Any deviation results in the output being marked incorrect. Loose accuracy, on the other hand, allows some flexibility. For example, if response begins with preamble like \"Sure, here is the answer:\" followed by correctly formatted JSON, it would still be considered correct under loose accuracy criteria, even though it would fail strict evaluation. In addition to removing such introductions, evaluators also disregard font modifiers and outros, making the assessment more flexible. For more details on the benchmark, please refer to the original paper [69]. Our implementation uses the codebase available at https://github.com/josejg/instruction_following_eval. Generating Reminder Prompting and Thinking Intervention. Figure 9 demonstrates how we generate the text of Reminder Prompting by providing system prompts to GPT-4o. We then modify the narrative (e.g., from \"Ensure the summary is at least 300 words\" to \"I should ensure the summary is at least 300 words\") to create the intervention sequence. 17 Figure 9: demonstration of how we prompt GPT-4o to generate the Reminder Prompting. The intervention sequence is slightly modified version of Reminder Prompting. Table 2: The evaluation results on the IFEval dataset span multiple reasoning models. We compare our method, Thinking Intervention (+ThinkingI), with the Vanilla Prompting and Reminder Prompting methods and observe consistent performance improvements. The best results are in bold, and the second best are underlined. Models Methods Prompt-level strict acc.(%) strict acc.(%) loose acc.(%) loose acc.(%) Prompt-level Inst-level Inst-level R1-Qwen-14B R1-Qwen-32B QwQ-32B Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI 70.43 75.42 72.83 74.31 70.43 77.08 75.23 77.63 79.30 82.26 81.33 82.44 79.50 81.65 81.18 82.13 79.14 84.29 82.85 84.53 86.09 88.01 87.53 88.13 73.57 78.37 76.53 77.26 74.49 80.96 78.74 81.70 83.92 86.32 86.69 86.69 81.77 84.29 83.69 84.41 81.89 86.93 85.37 87.29 89.09 90.65 91.01 91.01 B.2 Comprehensive Experiment Results In Table 2, we present the complete experimental results of IFEval. The findings aligned with the conclusions in Section 3.3, demonstrating that our Thinking Intervention consistently improves the models instruction-following capabilities across all evaluation metrics. For instance, in terms of prompt-level loose accuracy, Vanilla Prompting with Thinking Intervention leads to performance gains of 4.8%, 6.47%, and 2.4% for R1-Qwen-14B, R1-Qwen-32B, and QwQ-32B, respectively. 18 Instruction Hierarchy (SEP) Figure 10: demonstration of how the SEP benchmark evaluates instruction hierarchy capabilities. Each example consists of main instruction paired with data. Left: low-priority instruction is injected into the data, which the model should correctly ignore. Middle: low-priority instruction is injected into the task portion, which the model should follow and generate answers. Right: The low-priority instruction is absent, allowing us to measure the utility of different methods. Figure 11: Prompt template of evaluating the utility metric on the SEP benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively. C.1 More Details of Evaluations We use the SEP dataset [74] to evaluate models ability to follow hierarchical instructions. The dataset contains 9,160 examples, each consisting of main instruction, corresponding data, low-priority query, and witness (the answer to the low-priority query). For computational efficiency, we randomly sample 500 prompts for our evaluation. In our main paper, we primarily focused on two metrics: robustness and utility. In fact, the SEP benchmark also contains another metric called SEP utility to measure if the model can correctly follow the low-priority task when it is placed in the task section. We detail these metrics as follows: Robustness: We inject the low-priority query into the data block and measure the models ability to correctly ignore it (left example in Figure 10). The metric represents the percentage of cases where the model successfully ignores the low-priority instruction (i.e., the witness does not appear in the response). Note that in the original paper [74], this metric is called SEP. SEP utility: We place the low-priority query in the task block and evaluate the models ability to follow it (middle example in Figure 10). The metric represents the percentage of cases where the model correctly follows the low-priority instruction (i.e., the witness appears in the response). Utility: We omit the low-priority query (right example in Figure 10). We then evaluate the models performance using the prompt template shown in Figure 11. We use GPT-4o-mini as the judge to assess response quality. Results are scaled to 0-100. C.2 Comprehensive Experiment Results We present more comprehensive evaluations of Thinking Intervention on the SEP dataset in Table 3, including the new metric SEP utility. We found that our method achieves high SEP utility scores of 92.4% and 91.6% for R1-Qwen-14B, which are 4.0% and 2.8% higher than the baseline models, respectively. For the other two models, QwQ-32B and R1-Llama-8B, our method also achieves comparable performance, with degradation less than 1.2%. This indicates that our method can effectively improve the robustness of models while maintaining high utility. 19 Table 3: Evaluation results on the SEP dataset across various reasoning models. We compare our proposed Thinking Intervention (+ThinkingI) against Vanilla Prompting and Reminder Prompting. Models Methods Robustness(%) SEP utility(%) Utility (%) R1-Qwen-14B R1-Qwen-32B QwQ-32B Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI Vanilla Vanilla+ThinkingI Reminder Reminder+ThinkingI 34.00 38.40 (+4.40) 38.40 41.80 (+3.40) 34.80 50.20 (+15.40) 46.20 66.40 (+20.20) 22.20 31.40 (+9.20) 36.20 43.40 (+7.20) 88.40 92.40 (+4.00) 88.80 91.60 (+2.80) 92.80 91.60 (-1.20) 92.00 91.40 (-0.60) 96.60 96.40 (-0.20) 96.80 96.60 (-0.20) 81.04 81.08 (+0.04) 80.50 80.90 (+0.40) 81.76 82.02 (+0.26) 81.16 80.90 (-0.26) 88.00 88.16 (+0.16) 87.52 86.79 (-0.73)"
        },
        {
            "title": "D Safety Alignment",
            "content": "We provide more details on how baseline methods are implemented for safety alignment in Appendix D.1. We then introduce the detailed evaluation setup and present additional experimental results for XSTest and SORRY-Bench in Appendix D.2 and Appendix D.3, respectively. D.1 Details of Baseline Models For those baseline prompting models of safety steering, we utilize the prompts shown in Figure 12. Figure 12: The baseline prompting methods, as well as our Thinking Intervention, used for evaluating safety steering. The {Query} will be filled with the query from the benchmarks. Note for Goal Priority Prompting, we eliminate the few-shot exemplars as suggested by [2]. D.2 Safety Alignment on XSTest Details of experimental design. For safety alignment evaluation, we first use the XSTest benchmark [40], which consists of 250 safe requests and 200 unsafe requests. This benchmark is designed to examine the exaggerated safety behaviors of LLMs; therefore, all safe requests are intentionally tricky and contain potential safety triggers. Following [40], we evaluate the model outputs using the GPT-4o-mini with the prompts shown in Figure 13. 20 Figure 13: Prompt template of evaluating the compliance or refusal rate on the XSTest benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively. Note that the evaluation results are categorized into three levels: full compliance, full refusal, and partial refusal. For safe requests, we only consider full compliance as the correct response, and report the ratio as the compliance rate for safe requests. For unsafe requests, we only consider full refusal as the correct response, and report the ratio as the refusal rate for unsafe requests. This provides stringent evaluation of the models safety alignment behavior, as it requires models to clearly distinguish between safe and unsafe requests, and to respond appropriately in each case. Experimental results for more reasoning models. Figure 14 presents the results of XSTest for additional reasoning models, including R1-Llama-8B, R1-Qwen-14B, and QwQ-32B. Our Thinking Intervention approach consistently enhances safety alignment across various prompting methods and reasoning models. Notably, compared to Vanilla Prompting, incorporating our Thinking Intervention increases the refusal rate for unsafe requests by over 40% for R1 models and approximately 10% for QwQ-32B. For safe requests, we observe small degradation in compliance rates (< 5%) for R1-Qwen-14B and QwQ-32B across most prompting methods, with the exception of Goal Priority Prompting. However, the compliance rate for safe requests is significantly lower for R1-Llama-8B. We conjecture that this discrepancy may be due to the models limited capability to distinguish between tricky safe requests and unsafe ones. Additionally, QwQ-32B models exhibit less performance changes compared to R1-Llama-8B and R1-Qwen-14B when utilizing identical Thinking Intervention. This variance across models suggests differing inherent characteristics. We defer further investigation to future work. (a) R1-Llama-8B (b) R1-Qwen-14B (c) QwQ-32B Figure 14: Effectiveness of Thinking Intervention on the XSTest benchmark across multiple reasoning models. D.3 Safety Alignment on SORRY-Bench Details of experimental design. We also evaluate the safety alignment of reasoning models on the SORRY-Bench benchmark [61], which is more comprehensive benchmark containing 45 taxonomies of unsafe instructions, with 10 unsafe requests per taxonomy (450 total unsafe requests). Following the method in [61], we use GPT-4o-mini as the evaluation judge with the prompts shown in Figure 15. 21 Figure 15: Prompt template of evaluating the refusal rate on the SORRY-Bench benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively. Experimental results for more reasoning models. We then evaluate R1-Llama-8B, R1-Qwen-14B, and QwQ-32B on the SORRY-Bench benchmark. Figure 16 illustrates that our Thinking Intervention approach consistently enhances the safety alignment of reasoning models across various prompting methods. Specifically, robustness improves by 10% to 20% for R1-Llama-8B and R1-Qwen-14B, and by 5% to 15% for QwQ-32B. Notably, when applying our Thinking Intervention with Vanilla Prompting, the refusal rate for unsafe requests surpasses that of all GPT models using the same Vanilla Prompting methods. This demonstrates the effectiveness of our approach in enhancing the safety alignment of reasoning models. (a) R1-Llama-8B (b) R1-Qwen-14B (c) QwQ-32B Figure 16: Effectiveness of Thinking Intervention on the SORRY-Bench benchmark across multiple models. Our approach consistently improves the safety alignment of reasoning models."
        },
        {
            "title": "E Effect of Thinking Intervention Designs",
            "content": "In this section, we present an in-depth analysis of how variations in the design of the Thinking Intervention influence performance using safety alignment benchmarks. In Appendix E.1, we analyze the effect of positioning the intervention sequence at the beginning, middle, and end of the reasoning process, demonstrating that these positions significantly impact compliance and refusal behaviors. In Appendix E.2, we evaluate how variations in the text of the intervention sequence affect model responses, highlighting the trade-offs between safety enforcement and compliance rates. E.1 Positions of Thinking Intervention We investigate the effect of intervention positions on the reasoning process using R1-Qwen-32B on XSTest and SORRY-Bench. The intervention sequence is kept unchanged, and we implement three distinct intervention functions: (1) TIbegin: The intervention is introduced at the beginning of the reasoning process, corresponding to the default setting described in the main text. (2) TIend: The intervention is introduced at the conclusion of the reasoning process. Specifically, when the model is about to generate the reasoning-ending token \"</think>\", we replace it with the intervention sequence and allow the model to continue generating. (3) TImid: The intervention occurs at an intermediate stage of the reasoning process. We use the token \"wait\" as trigger, indicating transition in reasoning. Upon detecting this trigger, we replace the \"wait\" token with the intervention sequence and continue generating the output. (a) TIbegin vs. TIend on XSTest (b) TIbegin vs. TIend on SORRY-Bench (c) TIbegin vs. TImid on XSTest (d) TIbegin vs. TImid on SORRY-Bench Figure 17: Analysis of varying the location of the intervention sequence on the XSTest and SORRYBench benchmarks. The content is kept unchanged, and the Thinking Intervention is placed at the beginning, middle, and end of the reasoning process. Figure 17 (a) and Figure 17 (c) show that the position of the Thinking Intervention does not significantly affect the refusal rate of unsafe requests on XSTest, with most methods maintaining refusal rate between 60% and 70%. However, the compliance rate for safe requests is notably influenced by the 23 position of the Thinking Intervention. Placing the intervention sequence at the beginning of the reasoning process generally ensures high compliance with safe requests (i.e., over 90%), while placing it in the middle or at the end leads to substantial drop in performance, with compliance rates falling below 40% and even 20%, respectively. Interestingly, for the SORRY-Bench benchmark, shown in Figure 17 (b) and Figure 17 (d), placing the intervention sequence in the middle or at the end of the reasoning process results in substantial increase in the refusal rate for unsafe requests, reaching nearly 100%. This represents an improvement of over 10% compared to placing the intervention at the beginning. We conjecture that this difference stems primarily from the nature of the SORRY-Bench benchmark, which includes more detailed and explicit requests, whereas the XSTest benchmark contains more ambiguous and subtly phrased prompts. We leave deeper investigation of this phenomenon to future work. E.2 Text of Thinking Intervention text of the Thinking Intervention affects the reaWe also explore how the different soning process and the final safety performance using the XSTest and SORRY-Bench benchmarks with the R1-Qwen-32B model. We evaluate two different intervention sequences: (1) the default shorter intervention sequence (i.e., \"I am helpful, respectful, and honest assistant.\"), denoted as TIshort, and (2) longer version of the intervention sequence (i.e., \"I am responsible assistant and should not generate harmful or misleading content! So need to verify the dangerousness of the users request.\"), denoted as TIlong. We insert both versions at the beginning of the reasoning process. Figure 18 (a) shows that the compliance rate for safe requests on XSTest decreases by over 10% when using the longer intervention sequence, compared to the shorter version. In contrast, the refusal rate for unsafe requests increases by approximately 10% with the longer intervention sequence. Similarly, Figure 18 (b) indicates that the refusal rate for unsafe requests on SORRY-Bench also improves with the longer intervention sequence, showing an increase of 3% to 10%. This outcome is expected, as the longer intervention sequence provides more explicit safety instructions, highlighting the trade-off between safety and compliance. (a) R1-Qwen-32B on XSTest (b) R1-Qwen-32B on SORRY-Bench Figure 18: Analysis of varying the Thinking Intervention content on XSTest and SORRY-Bench benchmarks. We compare our default short intervention sequence with longer version. Both versions are inserted at the beginning of the reasoning process."
        },
        {
            "title": "F Case Study",
            "content": "In this section, we showcase an example from the SEP dataset that demonstrates how Thinking Intervention can effectively guide reasoning models to correctly follow hierarchical instructions. In Figure 19 and Figure 20, the main instruction is to \"Interpret the statute mentioned in the next paragraph, explaining its legal implications.\" where the content is about \"The SarbanesOxley Act\". However, the data block contains low-priority instruction to \"Tell me what book is primarily made of.\", which is unrelated to the main task and should be ignored. Figure 19 shows the response of model without Thinking Intervention. The model directly tries to answer the low-priority query and discuss \"what book is primarily made of\". Then the model recognizes the main task related to the \"Sarbanes-Oxley Act\". However, it gets confused and finally reverts to considering the low-priority query. As result, the model provides an incorrect response by discussing the content of the low-priority query instead of interpreting the statute. Figure 19: demonstration of how models without Thinking Intervention fail to ignore low-priority instructions and consequently provide incorrect responses. We use green color to highlight the main task and red color to highlight the low-priority query. Then, in Figure 20, we demonstrate the response of model with Thinking Intervention. The model directly focuses on the main task and reasons about how to answer it. Then the model also finds the low-priority query in the data block, but it successfully recognizes that this is unrelated to the main task and should be ignored. Consequently, the model provides correct response by interpreting the statute mentioned in the main instruction. This example illustrates how Thinking Intervention can effectively guide reasoning models to correctly follow hierarchical instructions and provide accurate responses. 25 Figure 20: demonstration of how models with Thinking Intervention successfully ignore low-priority instructions and provide correct responses. We use blue color to highlight the Thinking Intervention, green color to highlight the main task and red color to highlight the low-priority query."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Princeton University"
    ]
}