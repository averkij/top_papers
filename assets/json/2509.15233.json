{
    "paper_title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents",
    "authors": [
        "Xueqiao Zhang",
        "Chao Zhang",
        "Jingtao Xu",
        "Yifan Zhu",
        "Xin Shi",
        "Yi Yang",
        "Yawei Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs."
        },
        {
            "title": "Start",
            "content": "Video2Roleplay: Multimodal Dataset and Framework for Video-Guided Role-playing Agents Xueqiao Zhang, Chao Zhang, Jingtao Xu, Yifan Zhu, Xin Shi, Yi Yang, Yawei Luo* Zhejiang University {xueqiaozhang, yaweiluo}@zju.edu.cn 5 2 0 2 7 1 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 3 3 2 5 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Roleplaying-Video60k, large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during finetuning, and (2) summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) (Zhao et al., 2023) have spurred significant research interest in RPAs (Chen et al., 2024b), which simulate interactive characters through the integration of diverse modality data to create realistic user experiences. However, real-world human perception is inherently multifaceted and dynamic. The current reliance primarily on static modalities like text and images limits the ability of * Corresponding author 1Our data and code are available at https://github. com/zxqSled/Video2Roleplay. 1 Figure 1: Examples illustrating our RPAs performance compared to general baselines. More examples are provided in Appendix. these agents to fully satisfy the growing demand for highly immersive and expressive role-playing experiences. Video, as powerful multimodal medium (Song et al., 2024; Yang et al., 2024; Lian et al., 2024; Mou et al., 2024), offers rich array of dynamic details related to characters, such as emotional states, physical actions, scene transitions, and narrative experiences. This information is highly valuable for pioneering dynamic role-playing profiles. For example, lives showcase character dynamic motions in authentic scenarios. Vlogs and role documentaries capture individuals expressions and daily activities, effectively conveying complex emotions and personality traits for detailed character portrayals. Consequently, integrating the video modality into RPAs equips agents with more comprehensive and detailed dynamic information, improving role-playing performance and user engagement. Currently, despite some promising results of the existing work (Dai et al., 2024; Wang et al., 2025b) in the field of RPAs, there is still lack of exploration in data resources and effective methods of video modality. How to effectively integrate video modality information with existing static modalities and leverage its unique dynamic information advantages for RPAs remains challenging problem. Furthermore, the long length of some videos often introduces considerable redundant information, leading to high memory resource consumption and inefficient video information representation. To fill these gaps, this study introduces the concept of dynamic role-playing to integrate video modality into the RPAs for the first time, constructs large-scale and high-quality dataset tailored to the requirements of dynamic profile representation in RPAs, and proposes comprehensive framework that effectively incorporates video modality with static modalities. Specifically, we construct large-scale and highquality dataset sourced from various social media platforms like Xiaohongshu, Douyin, Weibo, and Bilibili. The dataset comprises daily lives, lifestyle vlogs, and personal documentaries from diverse groups, accompanied by corresponding video captions and related dialogues, providing rich resources for the development of RPAs. Additionally, we propose novel multimodal RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. To construct the dynamic role profile, we adaptively sample video frames based on their duration and provide them to the LLM in their original order. In parallel, the static role profile captures character information with two main components: (1) character-specific dialogues related to training videos, which are used to guide the base model during fine-tuning, and (2) high-level summary generated from the input video during inference, which provides concise but accurate description of the video scene and character presentation. By integrating both dynamic and static role profiles, our framework enables RPAs to generate responses that are highly consistent with the characters identity and the narrative context. Moreover, we design series of evaluation metrics and experiments to validate the effectiveness of our framework. Extensive experiments demonstrate the superior performance of our framework Table 1: Comparison between different role-playing datasets. Our work is the first role-playing dataset that introduces the video. Dataset Dialogues Video ChatHaruhi (Li et al., 2023a) Character-LLM (Shao et al., 2023) RoleLLM (Wang et al., 2024a) CharacterGLM (Zhou et al., 2024) Character100 (Wang et al., 2024b) DITTO (Lu et al., 2024) CharacterEval (Tu et al., 2024) LifeChoice (Xu et al., 2024) RolePersonality (Ran et al., 2024) MMRole (Dai et al., 2024) CharacterBench (Zhou et al., 2025) OpenCharacter (Wang et al., 2025a) RoleMRC (Lu et al., 2025) CoSER (Wang et al., 2025b) 54,726 14,300 168.1k 1,034 10,609 7,186 1785 1,462 87,345 14,346 13,162 306k 39.3k 29,798 Role-playing-Video60k(Ours) 700k (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) on RPAs. It establishes compelling trade-off between parameter size and overall performance while achieving SOTA for human-likeness. In summary, our contributions are threefold: We are the first to integrate the video modality into RPAs, introducing the concept of dynamic role-playing and enabling the creation of rich dynamic role profiles. We construct large-scale and high-quality dataset for the development of RPAs, including 60k videos and 700k dialogues across various categories, durations, and scenarios. We develop novel and comprehensive RPA framework that integrates adaptive temporal sampling with both dynamic and static role profiles. Extensive experiments and analyses demonstrate its outstanding performance."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Static Role Playing ChatHaruhi (Li et al., 2023a) provides dataset of over 54k simulated dialogues for 32 characters spanning Chinese, English, and anime. CharacterGLM (Zhou et al., 2024) allows for personalizing diverse range of agent personas and social agents through customizable attributes and behaviors. CharacterLLM (Shao et al., 2023) builds dataset detailing specific character exthen fine-tunes base model with periences, 2 the dataset to achieve target character portrayal. RoleLLM (Wang et al., 2024a) improves LLM roleplaying via multi-component framework (e.g., role profile construction, role-GPT, role-bench). Ditto (Lu et al., 2024) introduces self-alignment method to enhance LLM role-playing capabilities through knowledge augmentation and dialogue simulation. MMrole (Dai et al., 2024)introduces the concept of multimodal role-playing agents and offers comprehensive framework for their development and evaluation. RoleMRC (Lu et al., 2025) provides fine-grained composite benchmark for role-playing and instruction-following, revealing activation patterns linked to these distinct abilities. CoSER (Wang et al., 2025b) provides dataset comprising 29, 798 authentic conversations and comprehensive data from 771 renowned books and proposes given-circumstance acting method for training and evaluating role-playing LLMs. Figure 2: The video types and examples of our dataset."
        },
        {
            "title": "2.3 Multimodal Large Language Model",
            "content": "GPT4Video (Wang et al., 2024e) proposes unified framework for video understanding and generation via pre-trained model integration and develops simple text-only fine-tuning method for instruction following and safety alignment. LongVLM (Weng et al., 2024) introduces VideoLLM for long-term video understanding, achieving affordability via segment decomposition, feature extraction, token merging, and global semantics. Video-LLaVA (Lin et al., 2024) maps visual signals to the language feature space to achieve unified visual representations, introducing method for aligning features prior to projection. VideoAgent (Wang et al., 2024c) proposes an agent-based system that iteratively extracts and compiles key information for question answering, using vision-language models for visual translation and retrieval. VidRecap (Islam et al., 2024) proposes hierarchical caption generation method that creates CLIP captions, segment descriptions, and video summaries, trained using coarse-to-fine approach to learn the structure of video. LongVU (Shen et al., 2024) preserves frame information for lengthy videos by compressing tokens based on similarity and selecting relevant visual tokens for text queries. InternVideo2.5 (Wang et al., 2025c) introduces length-adaptive token approach to process videos, integrating visual perception with MLLM for fine-grained analysis. CLIP (Radford et al., 2021) achieves cross-modal understanding and unified representation by applying contrastive learning to unlabeled image-text pairs, eliminating the need for task-specific annotation. Flamingo (Alayrac et al., 2022) inserts new gated cross-attention layers into the LLMs to inject visual features and pre-trains the new layers on billions of image-text pairs. Emu (Sun et al., 2024) extends the approach of Flamingo (Alayrac et al., 2022) by integrating additional modalities to model generation and the corresponding training corpus. BLIP-2 (Li et al., 2023b) introduces Q-Former for visual and linguistic representation learning, achieving zero-shot image-text generation and strong performance on visual language tasks with more efficient parameterization. InternVL (Chen et al., 2024c) presents the first alignment of large-scale vision encoder with LLMs and introduces progressive image-text alignment strategy, enabling efficient training of large-scale visionlanguage foundation models. InstructBLIP (Dai et al., 2023) introduces an instruction-aware feature extraction method for vision-language instruction tuning, significantly enhancing multimodal model performance. LLaVA-NeXT (Li et al., 2024) enhances visual detail capture via improved input image resolution and refines its data mix through adapted visual instructions."
        },
        {
            "title": "3 Dataset Curation",
            "content": "To ensure richness and diversity of video content, we curate large-scale and high-quality video dataset sourced from various social media platforms, including Xiaohongshu, Douyin, Weibo, and Bilibili. This dataset comprises daily lives, lifestyle vlogs, and personal documentaries from diverse groups, accompanied by corresponding captions and related dialogues, providing comprehensive resources for the development of RPAs. More details can be found in the Appendix A.2. 3.1 Video Type We divide the videos into three categories by their content and duration, as shown in Figure 2. Live. This type of video captures few seconds before and after specific moment, focusing on close-up details that highlight the characters related motions. Notably, unlike static images, which freeze single frame, these videos offer continuous narrative by incorporating both preceding and following frames. This dynamic continuity enables deeper understanding of the role-related motion in the scene, reducing the bias of isolated moments. Vlog. Unlike traditional blogs, this category of video uses dynamic visuals to document daily life, typically capturing daily moments from individuals. Their vivid filming style, distinct character portrayals, and strong self-expression lend them unique individuality, effectively conveying positive character profiles to LLMs. Documentary. This type of video documents the life journeys or period-specific experiences of individuals, often featuring frequent scene transitions. Drawing from life footage that includes various personal events, these videos construct cohesive storyline that presents the deeper character traits. 3.2 Video Caption Video captions serve as critical bridge linking textual information with visual content. Therefore, ensuring these captions are rich, diverse, and comprehensive is essential for subsequent effective integration. Our preliminary strategy for annotating the videos entailed per-second frame descriptions aggregated by an LLM into complete caption. However, this approach requires substantial resource consumption and costs, and is further constrained by the input size of the LLM, preventing full frame processing. Thus, we design the staged annotation Figure 3: The illustration of video caption. We uniformly divide the video into segments and annotate each segment with frame description, then we summarize these descriptions as video caption and employ it during the fine-tuning and inference phase. Notably, video captions are utilized distinctly across the two phases, originating from different videos and serving distinct purposes. Specifically, during the fine-tuning phase, captions are employed to generate questionanswer pairs. In contrast, during the inference phase, captions are used to develop the role context. approach illustrated in Figure 3 which generates captions in two distinct phases, detailed below. Uniform Segmentation Sampling. To effectively capture the diverse scenes within each video while optimizing annotation efficiency, we employ temporal segmentation strategy. Each video is uniformly divided into multiple segments based on its length. From each segment, single frame is sampled as its representative. Based on case results and manual comparisons, we divide each video into 64 segments, thereby achieving trade-off between representational quality and annotation efficiency. Segment-Based Annotation and Summarization. For each representative frame selected from the segments, we use an LLM to generate detailed description. Following this, we introduce summary agent, which takes the descriptions of the frames in video order as context and produces comprehensive video summary using Chain-of-Thought (CoT) (Wei et al., 2022) and In-Context Learning (ICL) (Brown et al., 2020). 3.3 Dialogue Generation and Filtering Given detailed video caption, we use an LLM to generate question-answer pairs for each video. Following existing video works (Chen et al., 2024a; Zhang et al., 2024; Liu et al., 2024), the instruction prompt includes: (1) The role definition of the video scene. (2) The detailed video description. (3) In-context examples that include questionanswer pairs from the real comments in social me4 Figure 4: Our framework consists of three key components: (1) Adaptive Temporal Sampling: This module adaptively samples video frames based on the input videos length. (2) Dynamic Role Profile Representation: This module constructs dynamic role profiles from the sampled video frame. (3) Static Role Profile Representation: This module extracts static role profiles from dialogue and summary contexts. Further, we propose comprehensive evaluation approach incorporating eight metrics. dia. (4) Instruction order about the specific generation of question-answer pairs. Also, we instruct GPT-4o to return None if it is unable to generate question-answer pairs in the case of bad context. Additionally, to improve the quality of the generated question-answer pairs, we filter out the generated question-answer pairs by discarding answers that begin with phrases like As an AI language model, does not present, does not show, does not demonstrate, or other errors."
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we propose the overall framework as illustrated in Figure 4, which can be divided into three key parts: (1) Adaptive Temporal Sampling: We adapt an adaptive temporal sampling strategy tailored to the various lengths of video input. (2) Dynamic Role Profile Representation: We represent the samplings from the video as dynamic role profile. (3) Static Role Profile Representation: We represent the static role information from the dialogues obtained from Section 3.3 and the summary context of the input video. We provide detailed explanation of these processes as follows."
        },
        {
            "title": "4.1 Adaptive Temporal Sampling\nFor video V ∈ RT ×H×W ×3, we implement a\ncontext-aware sampling mechanism that adapts to\nthe video length, forming the video frame sequence\nV ′ ∈ Rt×H×W ×3.",
            "content": "For shorter videos like lives (0-5 seconds), where fine-grained motion details are essential, we employ dense temporal sampling by capturing every frame of the video. For medium-length videos like vlogs (5 seconds - 10 minutes), where the coherence of events is more important, we apply sparse sampling, taking one frame per 5 seconds uniformly. In contrast, for longer videos like documentaries (longer than 10 minutes) that focus on event-level understanding, we sample frames representing key scene events. The specific keyframe sampling process is detailed below. Step 1. Collect candidate frames by uniformly sampling one frame per second from the long video. Compute the frame difference 5 k=1 k , where D(i, j) = (cid:80)M is the k-th pixel value of the i-th frame, and is the total number of pixels. frame is added to the candidate set = {f1, f2, f3, ..., fm} if its difference score D(i 1, i) exceeds threshold . Step 2. Divide the candidate set into uniform groups, each containing frames. For each group g, compute the intra-group variation (g) = maxi,jg D(i, j). Select the frame with the maximum (g) as the representative frame for each group, forming refined candidate set = {f1, f2, f3, ..., fn}. Step 3. For adjacent frames and j, calculate the similarity S(i, j) = Clip(i, j) using CLIP. Merge frame into frame if S(i, j) > τ , where τ is similarity threshold. Repeat until all adjacent frames have S(i, j) τ , resulting in the final key frame set Ck = {f1, f2, f3, ..., fk}. Due to restrictions on computational resources, we cap the maximum number of frame samples at 128."
        },
        {
            "title": "4.3 Static Role Profile Representation",
            "content": "In this section, we fine-tune the base model to learn the static role profile from the dialogue related to the video scenes and characters, as discussed in Section 3.3. During the inference stage, we also employ summary agent to capture the global information of the video. This agent uses CoT process to generate video summary, which is presented as static character context to guide role-playing. Character Dialogue. RPAs are designed to simulate characters and engage in immersive dialogues with users. While these agents acquire dynamic role information from the process described in Section 4.2, our approach further integrates the static role information through role-related dialogues. The approach presented in Section 3.3 ensures the training dialogues are centered on and informed by 6 Figure 5: The example of fine-tune data format, the special token <video> indicates the position where the video is inserted. the roles and scenes within the videos. The integration can be achieved through supervised fine-tuning (SFT), with its specific data format shown in Figure 5. Video Summary. After the SFT of the base model, we introduce summary agent to capture global information of the video during the inference phase. For the input video with length L, we divide it into successive segments uniformly and caption the corresponding description for all segments, = {d1, d2, d3, ..., dk}, = L/n. Additionally, we introduce summary agent with CoT approach to summarize these descriptions into an entire video summary S, which is used as the context to guide the LLM in performing role-playing with the ICL approach."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "For the experimental dataset, we randomly shuffle our dataset into 57k training sets and 3k inference sets. Our test samples consist of 328 questions that are manually selected from social media platforms. To minimize the bias introduced by the model itself during evaluation, we employ GPT-4o and GPTo3-mini as LLM evaluators, averaging their assessments for more balanced perspective. Additionally, to enhance the reliability of our results, we set the API temperature to 0.0 and conduct three rounds of judgments per sample, averaging the results to further reduce variance. 5.2 Evaluation Metric Following the existing works (Dai et al., 2024; Tu et al., 2024; Zhou et al., 2025; Wang et al., 2024d), we evaluate the performance of RPAs including eight metrics. The specific metrics are as follows. Table 2: Main results of our framework and baselines. Model LLM-based Metrics Cons. Hall. Adh. Flu. Hum. Acc. Ton. Avg. llama3.1-8B-Instruct qwen3-8B InternVL2.5-8B Yi-Large GPT3.5 Turbo GPT-4-Turbo GPT-4.1 GPT-4o GPT-4o Mini GPT-o4 Mini GPT-o1 Gemini-2.5-Pro-Exp Claude3.5 Sonnet Claude3.7 Sonnet-thinking Deepseek-V3 Deepseek-R1 Qwen-max Doubao-1.5-pro Baichuan-4-Turbo General Baselines 64.48 60.46 53.12 74.38 68.75 75.73 79.31 76.74 74.73 81.12 78.48 82.12 80.87 83.66 72.38 80.68 81.89 71.19 73.03 53.93 55.27 51.56 68.40 66.22 70.76 74.56 71.42 67.27 74.12 74.44 75.48 74.33 78.31 67.95 78.69 70.75 70.74 68.75 47.67 37.36 37.43 61.91 57.34 60.34 71.91 68.77 62.15 74.17 72.98 80.85 60.27 77.93 65.22 77.13 66.17 65.11 56.33 72.04 79.24 71.40 84.15 84.55 86.38 88.05 86.31 85.91 85.03 87.57 88.11 85.23 86.80 86.04 86.58 88.44 83.29 83.46 46.72 48.20 32.46 51.23 52.16 54.67 58.27 49.94 46.90 49.85 62.93 62.70 49.32 59.19 43.09 47.86 57.56 46.12 51.33 48.24 52.98 44.48 63.58 58.61 63.08 68.89 64.87 60.13 66.94 69.86 69.14 64.53 71.73 60.29 69.47 64.29 59.94 60. 46.96 50.72 36.25 66.41 59.75 63.62 71.45 65.98 62.39 66.51 71.88 78.26 69.22 78.03 66.28 74.33 71.43 57.15 61.34 54.29 54.89 46.67 67.15 63.91 67.79 73.21 69.14 65.64 71.11 74.02 76.67 69.11 76.52 65.89 73.53 71.50 64.79 64.92 Role-playing Expertise Models CharGLM4 Ernie-char-8k Qwen-plus-character InternVL2.5-8B w/ Video SFT (Ours) 71.80 72.18 76.52 72.17 69.51 65.13 70.30 74. 60.45 58.26 63.11 70.52 86.22 84.68 87.57 87.93 52.87 54.28 54.29 69.98 59.88 56.09 60.28 69.26 61.31 63.48 62.76 61.75 66.01 64.87 67.83 72. Character Consistency. Do the responses maintain character consistency throughout interactions, rather than exhibiting random behavioral changes? Knowledge Hallucination. Do the responses prioritize factual grounding over fake assumptions when virtual knowledge conflicts with reality? Utterance Fluency. Do the responses maintain grammatical correctness and exhibit smooth readability in utterance expression? Tone Consistency. Do the responses match the characters typical tone patterns and catchphrases? Instruction Adherence. Do the responses adhere to instructions by strictly keeping in character without added explanation? Response Accuracy. Do the responses accurately address the question or appropriately engage in conversation based on the context? Human Likeness. Do the responses convey sense of human rather than presenting an AI style? Video-Text Relevance.2 Do the responses closely 2Due to the limitations of direct video input for most baselines, we evaluate this metric only during the ablation study. correlate with the content depicted in the video? Notably, we conduct user study to evaluate the models performance with human judgment. Participants are asked to compare responses from our model and the closed-source SOTA model (Gemini 2.5 Pro Preview 0325) across 21 diverse questions covering health, pets, fitness, learning, etc. Additionally, we verify the alignment between the LLM judge and human perception. Further details are provided in the Appendix A.3. 5.3 Baseline (1) Yi-Large, (6) GPT-4o We select sixteen well-known advanced LLMs (2) as general baselines: GPT-3.5-Turbo, (3) GPT-4-Turbo, (4) GPT-4.1, (5) GPT-4o, (7) GPT-o4 (9) Gemini2.5-Pro-Exp, Mini, (10) Claude (11) Claude (12) Deepseek-V3, 3.7 (13) (15) Doubao-1.5-Pro, (16) Baichuan-4-Turbo. Sonnet-thinking, Deepseek-R1, (8) GPT-o1, 3.5 Qwen-max, Sonnet, Mini, (14) We also use three role-playing expertise (1) CharGLM-4, (2) LLMs as robust baselines: Erine-char-8k, (3) Qwen-plus-character. Table 3: The ablation studies of the video SFT and the summary context. Method Cons. Hall. Adh. Flu. Hum. Acc. Ton. Rel. Avg. W/ Video Inference + W/ Summary Context 8B w/ Video SFT 8B w/ Text SFT 8B w/o SFT 72.17 69.41 53.12 74.38 67.56 51.56 70.52 68.09 37. 87.93 82.37 71.40 69.98 65.17 32.46 69.26 60.41 44.48 61.75 58.74 36.25 23.43 14.20 11.61 66.18 60.74 42. W/ Video SFT + W/ Video Inference 8B w/ Summary Context 8B w/o Summary Context 72.17 70.38 74.38 72.46 70.52 69.66 87.93 85. 69.98 68.51 69.26 65.89 61.75 61.03 23.43 19.37 66.18 64.13 5.4 Comparative Studies As shown in Table 2, we report the performance of two types of baselines and our framework on LLM-based metrics. Analyzing the generated responses, we observe that, in contrast to untrained RPAs, fine-tuned RPAs tend to generate shorter and more concise responses without additional explanation. These responses more closely align with human conversational patterns, rather than exhibiting the heavily formatted and AI styles often found in the outputs of untrained RPAs. The comprehensive experimental results demonstrate that our framework achieves superior performance in RPAs, realizing compelling trade-off between parameter size and effectiveness. Our model demonstrates comparable performance across all metrics against baselines with significantly larger parameters, and even presents SOTA on the human-likeness metric."
        },
        {
            "title": "5.5 Analysis",
            "content": "Large-Scale and High-Quality Dataset. We curate large-scale dataset comprising 60k videos and 700k conversations from various groups, featuring synthetic dialogues grounded in real-world social media scenarios. This large-scale, high-quality dataset is designed to improve the performance of RPAs. To validate its effectiveness, we compare our framework with the base model InternVL2.58B. As shown in Table 3, our framework significantly outperforms the base model across all metrics. The base model presents poor performance on RPA tasks without any SFT method, underscoring the necessity of SFT. Notably, benefiting from our datasets highly human-like style, text-only or both image and text SFT approaches demonstrate comparably strong enhancements in human-likeness and instruction adherence. conduct ablation experiments comparing our framework to the two approaches without video modality: 1) model fine-tuned only on dialogues. 2) model fine-tuned on single frame randomly sampled from videos and dialogues. As shown in Table 3, our framework, fine-tuned on our dataset with video modality, significantly outperforms models fine-tuned only on dialogues or on both images and dialogues. We observe that introducing the video modality leads to substantial improvements in almost all metrics. These improvements demonstrate the significant potential of integrating the video modality for developing RPAs that are more expressive and consistent, thus contributing to more engaging and immersive user experience. Additionally, despite some improvements in video-text relevance from incorporating video modality, the score still remains low, suggesting significant potential for further development of RPAs with more effective video modality integration. Summary Context Ablation. To evaluate the effect of the summary context derived from video captions on the performance of RPAs, we conduct an ablation study. Specifically, we replace the summary context with the full long descriptions for all sampled frames. As shown in Table 3, the model with summary context presents better performance. Notably, despite providing the LLM with more detailed information, the full long descriptions did not improve performance on any metric, including video-text relevance. In contrast, compared to lengthy contexts, the summary context generated under the CoT guidance is more concise and effectively captures the key points of the long description. This allows the model to have more accurate understanding of the input video, thus improving the performance of RPAs. Video Modality Ablation. To verify the impact of the video modality on the performance of RPAs, we Inference Time and Computational Resources. As shown in Table 4, we measure inference time Table 4: The results on inference time and computational resources. Table 5: The alignment tax of SFT and the generalization capabilities of the model after SFT. Frame Time(s) GPU0(MiB) GPU1(MiB) Benchmark W/ SFT W/O SFT 0 (Text) 1 (Image) 8 16 32 64 1.95 2.72 5.05 5.87 7.58 17.49 7,825 7,899 8,509 8,733 10,637 13,625 9,097 9,123 9,359 9,593 10,037 11,377 and computational resources on single case, using two NVIDIA RTX A6000 GPUs with FlashAttention (v2.7.4). For inference time, it is generally acceptable. When the input contains fewer than 32 frames, the inference time remains nearly constant and does not significantly exceed that of singleimage and text input. As the number of frames increases from 32 to 64, the inference time grows approximately linearly. For computational resources, we use FlashAttention to accelerate inference and reduce the attention memory from O(N²) to O(N), which is especially helpful for our linear inputs. The Alignment Tax of Fine-tuning. As shown in Table 5, we evaluate the model after SFT on several general benchmarks outside the role-playing domain. Based on our experimental results, we observe that while role-playing capabilities have improved substantially, the alignment tax introduced by SFT presents, resulting in some performance decrease across various general benchmarks and potential reduction in generalization ability. Despite the existing SFT tax, we believe that the notable gains in role-playing effectiveness outweigh the relatively minor alignment tax, which does not lead to collapse in generalization. Additionally, we note that SFT has not caused significant degradation in the models multimodal understanding ability, which we believe will better support the work on multimodal role-playing agent research. MMLU SuperGLUE-WiC SuperGLUE-WSC TriviaQA GSM8K RACE-Middle RACE-High MMMLU-Lite 73.27 73.20 70.19 60.76 75.36 92.76 90.91 48.92 73.67 73.82 73.08 62.07 76.27 93.04 90.85 49.89 results and analyses demonstrate the great effectiveness of our framework. Our work can advance the progress of RPAs, providing novel perspective for this field. In the future, we believe that engaging roles constructed from dynamic and static perspectives can benefit the various social applications and introduce promising connection with digital humans, leading to better user interaction."
        },
        {
            "title": "Limitations",
            "content": "Due to limitations in computational resources, we are unable to employ either larger-scale base model or more densely sampled frame acquisition approach to explore further results. Additionally, we only utilize lora fine-tuning method, rather than the full parameter fine-tuning approach. Thus, there is still room for improvement in the parameter size and fine-tuning method."
        },
        {
            "title": "Ethics Statements",
            "content": "Our model, fine-tuned on Role-playing-Video60k, may only have minimum safety alignment, so it will probably generate toxic and harmful content under induction. Therefore, the dataset and LLM are only for research purposes and should be carefully aligned in terms of safety in the future."
        },
        {
            "title": "Acknowledgements",
            "content": "In this paper, we propose the concept of dynamic role-playing for the first time by extending the RPAs with video modality. Moreover, we construct large-scale, high-quality video dataset covering various types, lengths, and roles for the development of RPAs. Furthermore, we design novel and comprehensive framework that integrates adaptive temporal sampling with dynamic and static role profile representation. Extensive experimental This work was supported by the National Natural Science Foundation of China (62293554, U2336212),\"Pioneer\" and \"Leading GooseR&D Program of Zhejiang (2024C01073), Ningbo Innovation \"Yongjiang 2035\" Key Research and Development Programme (2024Z292), and Young Elite Scientists Sponsorship Program by CAST (2023QNRC001)."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, and 1 others. 2024a. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495. Nuo Chen, Yan Wang, Yang Deng, and Jia Li. 2024b. The oscars of ai theater: survey on role-playing with language models. arXiv preprint arXiv:2407.11484. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, and 1 others. 2024c. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructblip: Towards general-purpose visionIn Adlanguage models with instruction tuning. vances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, and Zhiwu Lu. 2024. Mmrole: comprehensive framework for developing and evaluating multimodal role-playing agents. arXiv preprint arXiv:2408.04203. Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. 2024. Video recap: Recursive captioning of hourlong videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1819818208. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. 2024. Llava-next: Stronger llms supercharge multimodal capabilities in the wild. Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, and 1 others. 2023a. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. 2024. Llm-grounded video diffusion models. In ICLR. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2024. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems, 36. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Junru Lu, Jiazheng Li, Guodong Shen, Lin Gui, Siyu An, Yulan He, Di Yin, and Xing Sun. 2025. Rolemrc: fine-grained composite benchmark for role-playing and instruction-following. arXiv preprint arXiv:2502.11387. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 78287840. Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. 2024. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, and Deqing Yang. 2024. Capturing minds, not just words: Enhancing roleplaying language models with personality-indicative In Findings of the Association for Compudata. tational Linguistics: EMNLP 2024, pages 14566 14576. 10 Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1315313187. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, and 1 others. 2024. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434. Peipei Song, Dan Guo, Xun Yang, Shengeng Tang, and Meng Wang. 2024. Emotional video captioning with vision-based emotion interpretation network. IEEE Transactions on Image Processing, 33:11221135. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024. Emu: Generative pretraining in multimodality. In ICLR. Quan Tu, Shilong Fan, Zihang Tian, Tianhao Shen, Shuo Shang, Xin Gao, and Rui Yan. 2024. Charactereval: chinese benchmark for role-playing conversational agent evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1183611850. Noah Wang, Zy Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, and 1 others. 2024a. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1474314777. Xi Wang, Hongliang Dai, Shen Gao, and Piji Li. 2024b. Characteristic ai agents via large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 30163027. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. 2024c. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer. Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, and Dong Yu. 2025a. Opencharacter: Training customizable role-playing llms with large-scale synthetic personas. arXiv preprint arXiv:2501.15427. Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Wei Wang, and 1 others. 2025b. Coser: Coordinating llm-based persona simulation of established roles. arXiv preprint arXiv:2502.09082. Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, and 1 others. 2024d. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18401873. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, and 1 others. 2025c. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386. Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and Zhaopeng Tu. 2024e. Gpt4video: unified multimodal large language model for lnstruction-followed understanding and safety-aware generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 39073916. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. 2024. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer. Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, and Yanghua Xiao. 2024. Character is destiny: Can large language models simulate persona-driven arXiv e-prints, pages decisions in role-playing? arXiv2404. Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. 2024. Doraemongpt: toward understanding dynamic scenes with large language models (exemplified as video agent). In Proceedings of the 41st International Conference on Machine Learning, pages 5597655997. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Pei Ke, Guanqun Bi, Libiao Peng, and 1 others. 2024. Characterglm: Customizing social characters with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 14571476. Jinfeng Zhou, Yongkang Huang, Bosi Wen, Guanqun Bi, Yuxuan Chen, Pei Ke, Zhuang Chen, Xiyao Xiao, Libiao Peng, Kuntian Tang, and 1 others. 2025. Characterbench: Benchmarking character customization In Proceedings of the of large language models. AAAI Conference on Artificial Intelligence, 24, pages 2610126110."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Baseline Model URL List We provide list of URLs for the model APIs that are involved in this research, as shown in Figure 6. Figure 6: Model URL List A.2 Dataset Video Types Distribution. We conduct statistical analysis of the video type distribution based on their duration in our dataset, and the results are shown in the Figure 7. Figure 8: The SFT dialogue cases. we configure the caption generation process by setting the max_new_token parameter to 1024, 2048, and 4096 for live, vlog, and documentary video types, respectively. Moreover, to enhance the diversity of dialogues grounded in video captions, which will be used for fine-tuning our base model, we introduce multiple SOTA GPT-4.1, LLMs GPT-4o, Claude-3-7-Sonnet-Thinking, Gemini-2.5-Pro-Exp), tasked with guiding the dialogue generation process with the temperature parameter set to 1.0. Deepseek-R1, (Qwen-Max, each Figure 7: The video types distribution of our dataset. Video Caption. In order to clearly demonstrate the caption annotation effect on videos in our dataset, we present some specific video caption cases as shown in the Figure 12. Dialogues. To clearly demonstrate the quality of the dialogues generated from video captions, we present several specific cases in Figure 8. Generation Details. To effectively capture information from videos of varying lengths, 13 Bad Case. During our video annotation process, we encountered several challenges: 1) The large model occasionally generated repetitive or duplicate content when processing extensive datasets. 2) Videos with minimal scene changes, such as unboxing tutorials or fashion try-ons, presented difficulties in generating diverse global annotations. From visual perspective, consecutive frames in these videos often depict very similar actions or scenes, making it challenging to capture comprehensive and varied overall description. 3) Despite setting max_token (1024, 2048, or 4096) adjusted based on video length for annotation generation, for few videos with frequent scene changes, the substantial amount of information they contained means that Table 6: The Pearson, Spearman, and Kendall coefficients between human scores and LLM scores of Gemini2.5Pro-Exp. Gemini2.5-Pro-Exp Cons. Hall. Adh. Flu. Hum. Acc. Ton. Avg."
        },
        {
            "title": "Pearson\nSpearman\nKendall",
            "content": "0.5684 0.5018 0.2690 0.5015 0.6488 0.4534 0.5845 0.5473 0.4085 0.5903 0.5327 0.4327 0.4713 0.3480 0.2537 0.5893 0.5346 0. 0.5202 0.5203 0.3785 0.5465 0.5191 0.3750 Table 7: The Pearson, Spearman, and Kendall coefficients between human scores and LLM scores of our model."
        },
        {
            "title": "Ours",
            "content": "Cons. Hall. Adh. Flu. Hum. Acc. Ton. Avg. Pearson 0.6460 Spearman 0.5185 0."
        },
        {
            "title": "Kendall",
            "content": "0.5207 0.4816 0.3513 0.5878 0.5548 0.4989 0.6392 0.5907 0.4728 0.6655 0.6437 0.4928 0.5823 0.6078 0.4643 0.5293 0.5496 0. 0.5958 0.5638 0.4485 Figure 10: The dialogue filter prompt Figure 9: The dialogue generation prompt the generated descriptions still often surpass these token limits, leading to generation truncation and incomplete video captions. To address these issues, for the first two challenges, duplicate content and annotating scene static videos, we just rely on manual review and adjustment, as efficient automated solutions are still under investigation. For the third challenge, where descriptions are truncated due to token limits, we mitigate the problem by selectively increasing the max_token for the affected videos to facilitate more complete descriptions. Data Filter. Our conversation generation process aims to produce dialogues suitable for the SFT of base model. Operating under the guidance of Figure 11: User Study Results ICL, which utilizes high-quality dialogues from authentic social media comment sections as exemplars, the SOTA model takes video captions and generation prompts. Based on these inputs, the SOTA model generates initial dialogue candidates. We then employ regular expressions to extract relevant conversational segments from these responses. Recognizing that not all extracted content meets the required standards for scene relevance and di14 alogue quality, we implement further filtering mechanism involving prompt-based selection step where the SOTA model is guided to identify dialogues that best align with the specific conversational and video scene. Notably, the output from the SOTA model often presents significant formatting (e.g., **, 1, 2, 3). Therefore, final cleaning step is performed to remove these irrelevant and redundant characters, yielding the refined dialogues in the format required for SFT of the base model. The specific prompts of generation and filter are shown as Figure 9 and Figure 10. A.3 User Study To evaluate our model from human perspective, we conduct user study employing questionnaire. For each question in the questionnaire, participants are presented with three options: (1) response from our model, (2) response from the SOTA closed-source model, and (3) not sure. Participants are instructed to select the one they judged more closely aligned with real response from social media blogger. The results are presented in Figure A.3. Overall, 84 (57%) of participants found our models responses superior, while 52 (35%) preferred the responses from the closed-source model. The remaining 11 (8%) of participants selected not sure. Additionally, eight participants are instructed to follow the same evaluation criteria (0-100) used by the LLM judge and carefully assess each response across multiple dimensions. On average, each annotator spends approximately 52.43 minutes completing the process. For both Gemini2.5-pro and our model, we compute the Pearson, Spearman, and Kendall coefficients between human scores and LLM scores as presented in Table 6 and Table 7. These findings suggest that, from the human standpoint, our model demonstrates better performance compared to the closedsource model. A. Implementation Details We use the AdamW (Loshchilov and Hutter, 2019) optimizer with learning rate of 4e-5, weight decay of 5e-2, and warm-up ratio of 3e-2, training for one epoch. 15 Figure 12: Showcases of video caption. 16 Figure 13: Showcases of our framework. 17 Figure 14: Showcases of our framework. 18 Figure 15: Showcases of our framework. 19 Figure 16: Character consistency evaluation prompt. 20 Figure 17: Knowledge hallucination evaluation prompt. 21 Figure 18: Utterance fluency evaluation prompt. 22 Figure 19: Instructional adherence evaluation prompt. 23 Figure 20: Tone consistency evaluation prompt. 24 Figure 21: Response accuracy evaluation prompt. 25 Figure 22: Video-Text relevance evaluation prompt. 26 Figure 23: human likeness evaluation prompt."
        }
    ],
    "affiliations": [
        "Zhejiang University"
    ]
}