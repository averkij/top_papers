{
    "paper_title": "Auditing Prompt Caching in Language Model APIs",
    "authors": [
        "Chenchen Gu",
        "Xiang Lisa Li",
        "Rohith Kuditipudi",
        "Percy Liang",
        "Tatsunori Hashimoto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known."
        },
        {
            "title": "Start",
            "content": "Chenchen Gu 1 Xiang Lisa Li 1 Rohith Kuditipudi 1 Percy Liang 1 Tatsunori Hashimoto 1 5 2 0 2 1 1 ] . [ 1 6 7 7 7 0 . 2 0 5 2 : r Abstract Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than noncached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAIs embedding model is decoder-only Transformer, which was previously not publicly known. 1. Introduction Transformer large language models (LLMs) are computationally expensive and slow to run. To address this challenge, recent work has developed optimizations to make LLM inference and serving more efficient, such as prompt caching (Zheng et al., 2024a; Gim et al., 2024). In prompt caching, reuse of the attention key-value (KV) cache across requests results in cache hits and faster response times for prompts that share prefix with cached prompt. However, prompt caching results in data-dependent timing variationscached prompts will be processed faster than non-cached prompts, introducing the risk of side-channel timing attacks and information leakage. In particular, an attacker could identify prompts that yield fast API response 1Stanford University. <cygu@cs.stanford.edu>, Gu <thashim@stanford.edu>. Correspondence to: Chenchen Hashimoto Tatsunori 1We release code and data at https://github.com/ chenchenygu/auditing-prompt-caching. 1 Figure 1. An example illustrating prompt caching. (1) victim sends prompt to the API, which then becomes cached. (2) An attacker sends new prompt, resulting in cache miss and slow response time. (3) An attacker sends prompt that shares prefix with the victims prompt, resulting in cache hit. From the fast response time, the attacker can infer that cache hit occurred, which potentially reveals information about other users prompts. times; such prompts are likely cached. If the cache is shared across users, then prompt being cached implies that another user recently sent that prompt. Figure 1 illustrates an example of prompt caching and potential privacy leakage. In general, timing differences between cache hits and cache misses have been widely exploited in computer security, such as in the infamous Meltdown (Lipp et al., 2018) and Spectre attacks (Kocher et al., 2019). Because prompt caching may result in privacy leakage, it is important for users to know about the prompt caching policies of API providers. Some API providers have announced that they perform prompt caching, such as Anthropic (2024b) and OpenAI (2024b), but other API providers may be performing prompt caching without announcing it. Also, even if provider announces prompt caching, they may not state the level of cache sharing, i.e., per-user, per-organization, or global. Therefore, we develop and conduct an audit to determine if an API provider is caching prompts and the precise level of cache sharing. Our audit uses statistical hypothesis testing and outputs valid p-values with respect to the null hypothesis of no caching, enabling guarantees on the false positive rate. In our audit, we construct and sample response times from two procedures: one that attempts to produce cache hits, and one that produces cache misses. At high level, to Auditing Prompt Caching in Language Model APIs attempt to produce cache hit, we send prompt to the API to try to cache the prompt, then we send the prompt again to try to hit the cache. To produce cache miss, we simply send random prompt. Under the null hypothesis of no prompt caching, where only cache misses are possible, these procedures produce identical distributions of times. Accordingly, we detect caching if we find statistically significant difference between these distributions. We conducted audits on real-world LLM API providers in September and October 2024. We detected prompt caching in 8 out of 17 API providers. In 7 of these providers, we detected global cache sharing. On these APIs, an attacker could, in principle, detect cache hits from timing differences to infer that another user sent prompt that shares prefix with given prompt. Timing variations due to prompt caching can also result in leakage of information about models architecture. Cache hits between prompts that share prefix but have different suffixes are possible only in autoregressive decoder-only Transformers, where each token attends only to previous tokens. Therefore, detecting such prompt prefix caching indicates that the model has decoder-only architecture. Virtually all chat models are decoder-only, but embedding models can have either encoder or decoder architectures. As such, for proprietary embedding models, leakage of architecture information may represent leakage of intellectual property. By detecting prompt prefix caching, we find evidence that OpenAIs text-embedding-3-small has decoder-only architecture, which was previously not publicly known. Responsible disclosure. In October 2024, we disclosed our audit results with each API provider in which we detected prompt caching. We gave providers 60 days to address the vulnerabilities before publicly releasing our findings, and the actual time elapsed ended up being longer. To our knowledge, at least five providers made changes to mitigate vulnerabilities, e.g., disabling global cache sharing across organizations and updating documentation. 2. Preliminaries and Assumptions First, we briefly describe prompt caching, our assumptions on how users and attackers can interact with an API, and the levels of cache sharing and privacy leakage. 2.1. Prompt Caching Recent works have proposed prompt caching in Transformer (Vaswani et al., 2017) LLM serving by reusing the attention key-value (KV) cache across requests (Zheng et al., 2024a; Gim et al., 2024). In these methods, prompt is cached by storing the prompts attention KV cache. Then, if subsequent prompt has matching prefix with cached prompt, the KV cache for the matching prefix can be retrieved from the cache. As result, cache hits will tend to have faster time to first token (TTFT), which is the time taken to process the prompt and generate the first response token.2 In decoder-only Transformers, where each token attends only to previous tokens, reusing the KV cache for matching prefixes exactly preserves model behavior, even when the prompt suffixes differ. Figure 1 illustrates an example of prompt caching. Several API providers have recently announced prompt caching features, including Anthropic (2024b), DeepSeek (2024), Fireworks (2024), and OpenAI (2024b). These providers do not state technical details of prompt caching, but these providers state that cache hits occur for (and only for) exact prefix matches between prompts. For our purposes, the precise implementation of prompt caching is largely unimportant. The properties of prompt caching that we exploit are: 1. Cache hits occur on prefix matches between prompts. 2. Cache hits tend to have faster TTFT than cache misses (after accounting for prompt length). To describe these properties more formally, assume that model API takes in prompt and has TTFT (x). Note that (x) is random variable due to variance from network latency, server load, etc. Assume that the API has cache C, which is set of cached prompts. If has sufficiently long matching prefix with some cached prompt C, then cache hits occurs. For example, let = The quick brown fox jumps and = {c}. If x1 = The quick brown fox runs, then x1 and have matching prefix of The quick brown fox , so x1 could result in cache hit. On the other hand, if x2 = quick brown fox jumps, then x2 and do not share prefix, so x2 results in cache miss. Since x1 and x2 are similar lengths but x1 is cache hit and x2 is cache miss, we would expect that E[T (x1)] < E[T (x2)]. When prompt is sent to the API, we assume that is added to the cache and that will remain in for some finite period of time. The API may use multiple servers, each with their own separate caches. We do not make assumptions about how prompts are routed to servers. prompt may be randomly routed, or it may be intentionally routed to server where the prompt is already cached. 2In embedding models, we can view the embedding output as the first and only response token. Auditing Prompt Caching in Language Model APIs Figure 2. Organizations contain users, and the global level contains all users and organizations of an API. 2.2. API Assumptions We assume that it is possible to send arbitrary prompts to the API (possibly subject to some maximum length) and measure the TTFT. The TTFT can be measured by setting the maximum tokens parameter to 1, which restricts the LLM output to only contain 1 token. Then, the overall response time is equal to the TTFT. The max tokens parameter is supported by most, if not all, real-world LLM APIs. Either client-side or server-side timing suffices for our purposes. The client-side timing is obtained simply by measuring the time elapsed between sending the API request and receiving the API response. The server-side timing can be measured if it is contained somewhere in the API response.3 2.3. Levels of Cache Sharing and Privacy Leakage To facilitate our discussion of prompt cache sharing and privacy leakage in APIs, we define our terminology of users and organizations. user is one person that uses the API. Each user has unique email/username and login password. An organization contains many users, but shares billing system, centralized membership management, etc. Organizations can be used by companies, research groups, etc. Many, but not all, API providers support organizations, although sometimes under different terminology, such as teams or accounts. For consistency and simplicity, we refer to them all as organizations. Figure 2 shows the hierarchical structure of users and organizations. We consider three levels of cache sharing and their corresponding potential privacy leakages. 1. Per-user caching. Each user has their own cache, i.e., when user sends prompt, cache hit can occur only with cached prompt previously sent by user u. Therefore, there is no potential privacy leakage arising from prompt caching. 2. Per-organization caching. Each organization has its own cache, i.e., when user u, who belongs to organization o, sends prompt, cache hit can occur only with cached prompt previously sent by any user in organization o. There is slight risk of privacy leakage if certain users in the organization have access to privileged information that other users should not, e.g., the CEO knowing sensitive business data. However, this risk can be mitigated, as the organization owner has full control over which users are members. 3. Global caching. The cache is shared across all users of the API, e.g., when user sends prompt, cache hit can occur with any cached prompt, regardless of who sent it. This leads to the highest risk of privacy leakage, as an attacker could potentially learn information about any other users prompts, including users in other organizations. 3. An Audit to Detect Prompt Caching Next, we propose an audit to detect whether an API provider is caching prompts and determine the level of cache sharing. Our audit uses statistical hypothesis testing and outputs valid p-values with respect to the null hypothesis of no caching, allowing for guarantees on the false positive rate. 3.1. Audit Formulation: Statistical Hypothesis Testing To test for given level of cache sharing, let uvictim and uattacker be two users that are the farthest away within that level. For example, to test for per-organization caching, uvictim and uattacker should be different users in the same organization. We formulate our audit as statistical hypothesis test using the following null and alternative hypotheses: H0 : API does not cache prompts (at this level of sharing), H1 : API caches prompts (at this level of sharing). The caching in H0 does not refer only to prompt caching via the KV cache reuse described earlier. More verbosely, H0 can be written as when uvictim sends prompt to the API, the API does not store any information about that affects the TTFT (x) for any future prompt sent by uattacker. To test these hypotheses, we construct procedures that attempt to produce and measure the TTFT of cache hits and cache misses. Let be distribution of prompts. To produce cache miss, uattacker simply sends random prompt to the API and measures the TTFT (x). To attempt to produce cache hit, first, we sample prompt P, and uvictim sends to the API one or multiple times to try to cache x.4 Next, we sample such that and share prefix of certain length. To try to produce cache hit, uattacker sends and measures the TTFT (x). 3We can measure server-side timing in more than half of the APIs we test, often from undocumented fields in the HTTP headers of the API response. 4Multiple requests may be necessary in some scenarios, e.g., if API requests are randomly routed to one of several servers, and each server has separate cache. Auditing Prompt Caching in Language Model APIs Let Dhit and Dmiss be the distributions of TTFTs from these cache hit and cache miss procedures, respectively. Under the null hypothesis H0 of no caching, Dhit = Dmiss, as both procedures will produce only cache misses. In contrast, under the alternative hypothesis H1 of caching, we would expect the cache hit times to tend to be faster than the cache miss times, so Dhit = Dmiss. Now, we can reformulate our hypotheses as H0 : Dhit = Dmiss, H1 : Dhit = Dmiss. Given this reformulation, to perform our audit, we first sample TTFTs from the cache hit and cache miss procedures. Then, we run statistical test for whether our samples came from the same distribution, e.g., the two-sample Kolmogorov-Smirnov test, producing p-value with respect to the null hypothesis of no caching. 3.2. Audit Implementation Details Next, we describe the concrete implementation details of our audit. The procedure uses the following configuration parameters: PROMPTLENGTH, PREFIXFRACTION, NUMVICTIMREQUESTS, and NUMSAMPLES. The meanings of these parameters will become clear in the descriptions below. to produce cache hit, uattacker sends to the API and measures the TTFT (x). When PREFIXFRACTION = 1, we test for prompt caching when = x, i.e., exact prompt matches. When PREFIXFRACTION < 1, we test for prompt caching when and have the same prefix but different suffixes, e.g., c and c x. Statistical testing. Putting these pieces together, to perform the audit, we collect NUMSAMPLES timings each from the cache hit and cache miss procedures. We randomize the order in which we collect the timing samples. Then, we test for statistically significant difference between the distributions of times from the two procedures. We use the SciPy implementation (Virtanen et al., 2020) of the two-sample Kolmogorov-Smirnov (KS) test (Hodges Jr, 1958), which is nonparametric test for equality of distributions. The test statistic is the maximum difference between the empirical cumulative distribution functions at any point. More specifically, since we expect cache hits to be faster under the alternative, we perform one-sided test, so the test statistic is the maximum difference in the direction of cache hits being faster. The KS test outputs p-value, which we can use to reject or not reject the null hypothesis of no prompt caching at given significance level α. Prompt distribution. Our distribution of prompts is uniform distribution over all prompts consisting of PROMPTLENGTH English letters, lowercase and uppercase, each separated by space characters, e.g., N R. Because all commonly used byte pair encoding (BPE) tokenizers (Gage, 1994; Sennrich et al., 2016) split on whitespace during pre-tokenization, all prompts in will be exactly PROMPTLENGTH tokens long.5 4. Auditing Real-World APIs Next, we audit real-world LLM APIs to identify APIs that cache prompts and determine the level of cache sharing, i.e., per-user, per-organization, or global. Cache sharing results in potential privacy leakage, as an attacker could, in principle, identify cached prompts using timing data to learn information about other users prompts. Cache miss. uvictim sends random prompt to the API and measures the TTFT (x). Since the prompt consists of random letters, there is negligible probability that noticeable prefix has already been cached: the probability that two prompts sampled from share prefix of 15 tokens or longer is less than 1025. Therefore, this procedure accurately measures distribution of cache miss times. Cache hit. First, we sample random prompt P. Then, uvictim sends to the API NUMVICTIMREQUESTS times consecutively to try to cache x. Then, we sample such that and have shared prefix of exactly PREFIXFRACTION PROMPTLENGTH tokens. To attempt 5Many APIs add small number of tokens to the user prompt due to the default system prompt, special tokens for prompt and role formatting, etc. However, these additional tokens are unimportant for our procedure, as the number of additional tokens is small and remains constant across prompts to given model API. 4.1. Audit Setup and Configuration API providers and models. We audit 17 API providers: Anthropic, Amazon Bedrock, Microsoft Azure OpenAI, Cohere, Deep Infra, DeepSeek, Fireworks AI, Google, Groq, Hyperbolic, Lepton AI, Mistral, OctoAI, OpenAI, Perplexity, Replicate, and Together AI. The model APIs that we audit for each provider are included in Tables 1 and 2. For API providers that primarily serve open-weight models, we audit their Llama 3 or 3.1 8B Instruct API (Dubey et al., 2024). For providers that serve proprietary models, we audit the cheapest chat model in their most recent family of models. In addition, we audit APIs for proprietary embedding models, where available. We do not audit APIs for open-weight embedding models because we did not find any APIs that served open-weight decoder-only Transformer embedding models. Prefix caching is possible in decoder-only Transformers but not encoder-only Transformers, where each token attends to all other tokens in the prompt. 4 Auditing Prompt Caching in Language Model APIs Table 1. Audit results for APIs where we detected prompt caching. denotes caching was detected, denotes caching was not detected, and denotes that cache sharing within an organization was not tested, either because the API did not support organizations or because we did not have access to the organizations feature. We report the average precision for classifying times from the cache hit procedure, using the highest level of cache sharing detected in each API. We report the average precision for client-side timing and server-side timing separately, with denoting that the given timing method is unavailable for that API."
        },
        {
            "title": "Same prompt",
            "content": "Same prefix, different suffixes Avg. precision"
        },
        {
            "title": "Model",
            "content": "text-embedding-3-small Azure Deep Infra Llama 3.1 8B Instruct Llama 3.1 8B Instruct Fireworks Llama 3.1 8B Instruct Lepton text-embedding-3-small OpenAI Llama 3.1 8B Instruct Perplexity Llama 3 8B Instruct Replicate"
        },
        {
            "title": "Anthropic\nOpenAI",
            "content": "Claude 3 Haiku GPT-4o mini Per-user Per-user Per-org. Global"
        },
        {
            "title": "Server",
            "content": "0.80 0.84 0.77 0.71 0.78 0.90 0.84 0.79 0.79 0.70 0.79 1.00 0.86 Table 2. Audit results for APIs where we did not detect prompt caching. denotes that caching was not detected. Provider Model Claude 3 Haiku GPT-4o mini Command embed-english-v3.0 DeepSeek Chat Gemini 1.5 Flash text-embedding-004 Llama 3 8B Instruct Amazon Azure Cohere Cohere DeepSeek Google Google Groq Hyperbolic Llama 3.1 8B Instruct Mistral Mistral OctoAI Together Mistral Nemo Mistral Embed Llama 3.1 8B Instruct Llama 3.1 8B Instruct Same prompt Per-user Configuration and procedure. For our audits, we use PROMPTLENGTH = 5000 and NUMSAMPLES = 250. We run four levels of audits of increasing cache sharing and privacy leakage. At each level, we only continue to audit APIs if we detect caching during the previous level. We use significance level of α = 108. To narrow down our list of providers, the first level tests for the simplest level of prompt caching: 1. Same prompt, per-user caching. We test for prompt caching on exact prompt matches (x = x) by setting PREFIXFRACTION = 1. We set uvictim and uattacker to be the same user, and we set NUMVICTIMREQUESTS = 25. In the remaining three levels, we test for prompt caching 5 when and have the same prefix but different suffixes by setting PREFIXFRACTION = 0.95. We test for increasing levels of cache sharing by appropriately setting the victim and attacker users: 2. Per-user caching. uvictim and uattacker are the same user, as in the first level. 3. Per-organization caching. uvictim and uattacker are different users within the same organization. For APIs without organizations, we skip this level. 4. Global caching. uvictim and uattacker are different users in different organizations. For APIs without organizations, uvictim and uattacker are simply different users. In levels 24, to determine how many victim requests are needed to detect caching, we run tests using NUMVICTIMREQUESTS {1, 5, 25} in increasing order, stopping after the first significant p-value. To account for multiple testing, we perform Bonferroni correction by dividing the significance threshold for each test by three. In all levels, if only one timing method is available in an API (client-side or server-side timing), then we use that timing method. If both are available, we run tests using both timing methods and perform another Bonferroni correction, dividing by two this time. Cost per test. When NUMVICTIMREQUESTS = 25, one test uses roughly 34 million prompt tokens. The number of response tokens used is much smaller because we set the maximum response tokens parameter to 1. For the chat APIs we audit, the prices per million prompt tokens are 0.050.25 USD, resulting in cost per test of 1.698.44 USD. The tests are cheaper when NUMVICTIMREQUESTS is smaller. Auditing Prompt Caching in Language Model APIs Figure 3. Histograms of response times from the cache hit and cache miss procedures in APIs where we detected caching. The distributions of times are clearly distinguishable, with cache hits tending to be faster. Each histogram title states the API provider, model, level of cache sharing (per-org. or global), timing source (client-side or server-side timing), and the NUMVICTIMREQUESTS used, denoted V. 4.2. Audit Results We conducted our audits in September and early October 2024 using clients located in California. Table 1 shows audit results for APIs in which we detected prompt caching, and Table 2 shows APIs in which we did not detect prompt caching. We detected prompt caching in 8 out of 17 API providers, and we detected global cache sharing in 7 providers. This means that an attacker can potentially learn information about other users prompts by identifying cached prompts from timing data. To assess an attackers ability to distinguish between cache hits and cache misses, Figure 4 contains selected precision-recall curves for classifying times from the cache hit procedure.6 The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Figure 6 in the appendix shows precision-recall curves for the highest level of cache sharing we detected in each API. To numerically summarize these curves, we compute the average precision (Zhu, 2004), which is equal to the area under the precision-recall curve (the precision is averaged over the interval of recall scores from 0 to 1). Table 1 shows that the average precisions mostly lie around moderately high value of 0.8. Figure 3 displays histograms of times from the cache hit and cache miss procedures. The distributions of times are clearly distinguishable, with cache hits tending to be faster. Each histogram title states the minimum NUMVICTIMREQUESTS (denoted in the titles) that resulted in significant 6The cache hit procedure attempts to produce cache hits but cannot guarantee cache hits (e.g., due to server routing), so some times in the cache hit distribution may actually be cache misses. p-value. In most of the APIs where we detected caching, only NUMVICTIMREQUESTS = 1 was needed to detect caching. Only the OpenAI and Azure text-embedding3-small APIs required NUMVICTIMREQUESTS = 25 to achieve significant p-value. This may suggest that these APIs have multiple servers with separate caches and that requests are randomly routed to server, so multiple victim requests are needed to cache the prompt in enough servers for the attackers prompt to have sufficient probability of producing cache hit. In Appendix B, we report all the p-values from our audits. In many APIs, the p-values are many orders of magnitude smaller than our significance level of α = 108. In all APIs where we detected caching, all available timing methods resulted in significant p-values. In the Anthropic Claude 3 Haiku and OpenAI GPT-4o mini APIs, we detected per-organization cache sharing, but not global cache sharing. This exact level of cache sharing is stated in their prompt caching documentations, confirming the efficacy of our audit procedure. Since OpenAI (2024a) and Anthropic (2024a) document per-organization cache sharing, we do not consider it security vulnerability. Global cache sharing in the OpenAI text-embedding-3-small API was potential vulnerability, but has been patched after our responsible disclosure prior to the release of this paper. Although DeepSeek (2024) has prompt caching feature and returns the number of cache hit tokens in API responses, which we used to confirm that we produced cache hits, we were unable to detect caching from response times. There was no statistically significant difference between the distributions of cache hit and cache miss times, even in two-sided tests. DeepSeek states that the cache is isolated per-user, 6 Auditing Prompt Caching in Language Model APIs 4.4. Difficulty of Prompt Extraction Attacks Our results show that given specific prompt x, an attacker could potentially detect cache hits to learn whether another user sent prompt that shares prefix with x. natural question is whether an attacker could extract others users prompts token-by-token. One idea is to use breadth-first search: given partial candidate prompt, such an attack would try possible continuation tokens and determine which continuation token is cached. The cached token is appended to the candidate prompt and the process repeats. However, we were unable to execute practical prompt extraction attacks. successful attack requires extremely accurate detection of cached tokens, as there are many possible continuation tokens at each step. Just one incorrect token causes complete failure due to the exact prefix match required for cache hit. In preliminary experiments, we were unable to reliably detect the presence one additional cached token. It is also difficult to make repeated measurements to boost accuracy. To detect whether prompt is cached, the attacker must send the prompt to the API. Then, future measurements may produce cache hit not because another user sent the prompt, but because the attacker sent it. We do not claim that prompt extraction attacks are necessarily impossible. Such attacks face difficulties, but future work may yet develop successful, practical attacks. In addition, in more restricted sets of target prompts, e.g., known prompt templates with places for users to enter private personal information, it may be easier to overcome these difficulties. 5. Leakage of Architecture Information In addition to privacy implications, the detection of prompt caching can also reveal information about models architecture. This is because the conditions for cache hits to occur depend on model architecture. In decoder-only Transformer models, reuse of the attention KV cache enables cache hits between prompts with matching prefixes, even if the suffixes differ, since each token attends only to previous tokens. This prefix caching is not possible in encoder-only or encoder-decoder Transformer models, where each token in the prompt attends to all other tokens in the prompt. Therefore, detecting such prompt prefix caching indicates that model cannot have bidirectional encoder architecture. Virtually all chat models are decoder-only, but embedding models can have either encoder or decoder architectures, as seen in the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2023). As such, for proprietary embedding models, leakage of architecture information may represent leakage of intellectual property. In our audits  (Table 1)  , we detected prompt caching in Figure 4. Selected precision-recall curves for distinguishing between times from the cache hit and cache miss procedures. Cache hits are the positive class. The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Figure 6 in the appendix contains curves for other APIs. and we empirically verified that this is the case based on the number of cache hit tokens returned in the API responses. 4.3. Ablations to the run ablations determine PREFIXFRACTION, We of and model PROMPTLENGTH, size on the average precision, shown in Figure 5. We use the APIs in which we detected global caching with NUMVICTIMREQUESTS = 1, i.e., the Llama 3 or 3.1 8B Instruct APIs of Fireworks, Perplexity, and Replicate. effects Smaller PROMPTLENGTH decreases average precision. In Figures 5a and 5b, we vary the PROMPTLENGTH in the same prompt (PREFIXFRACTION = 1) and same prefix but different suffixes (PREFIXFRACTION = 0.95) settings, respectively. When the PROMPTLENGTH is moderately high ( 1000), the average precision is relatively high and stable. However, as the PROMPTLENGTH approaches zero, the average precision decreases to random chance. Decreasing PREFIXFRACTION decreases average precision. In Figure 5c, we vary the PREFIXFRACTION while setting PROMPTLENGTH = 1000. As the length of the matching prefix decreases, the average precision decreases to random chance. No clear relationship between model size and average precision. In Figure 5d, we vary the model size on the Fireworks API, which supports all models in the Llama 3.1 and 3.2 families. We detected caching in all model sizes, with no clear relationship between model size and average precision. Relationship to p-values. Figure 7 in Appendix shows the effects of the ablations on the audit p-values. We observe similar patterns as above, with decreases in average precision corresponding to increases in p-values. 7 Auditing Prompt Caching in Language Model APIs Figure 5. Ablations on the effects of PROMPTLENGTH, PREFIXFRACTION, and model size on the average precision. In (a)(c), as the prompt length or prefix match length decreases, the average precision decreases to random chance. In (d), we detect caching across all model sizes, with no clear relationship between model size and average precision. OpenAIs text-embedding-3-small API when prompts had the same prefix but different suffixes. We confirm that when the prompt suffix is changed, the returned embedding also changes, indicating that the caching mechanism does not simply return cached embedding outputs from similar prompts. Assuming that text-embedding-3-small is Transformer-based, this indicates that text-embedding-3small is decoder-only Transformer. This is new information, as OpenAI has not released any information about the architecture of their embedding models. Disclosure of cache sharing. We believe that providers should disclose their caching policies, particularly the level of cache sharing. It is important that users know how their data is handled and who could potentially learn information about their data. This way, users can make informed decisions about how they use an LLM API. For example, if company knows that an API uses per-organization cache sharing, the company can decide to create separate organizations for different groups of employees to prevent unauthorized information access. Floating-point precision of the cache. When we send the exact same prompt multiple times, when the response time is noticeably faster, indicating cache hit, the returned embedding differs slightly from the normal embedding in most of the responses with normal response times, which indicate cache misses. This behavior is consistent across different random prompts. These differences are small, on the order of 104 to 105 in each coordinate. We hypothesize that these differences may arise if the reused KV cache is stored in lower floating-point precision, resulting in slight discrepancies when the attention KV is computed from scratch in cache misses versus when it is retrieved from the cache in cache hits. Interestingly, in some responses, especially those that are noticeably slower, the embedding differs from both the normal and cache hit embeddings. This may be caused by some responses being processed by different GPU models, as floating point computations can differ slightly across different GPUs. Appendix contains examples of response times and embeddings showing this phenomenon. 6. Mitigations Per-user caching prevents privacy leakage. To completely prevent any privacy leakage from prompt caching, only per-user caching should be allowed. In per-user caching, an attacker will not be able to produce cache hits on prompts sent by other users. Since it is unlikely that different users will send prompts with long matching prefixes, per-user caching should retain many of the performance benefits from global cache sharing. Disabling caching prevents any information leakage. For information leakage that only requires per-user caching, such as leakage of architecture information, the strongest mitigation is to disable prompt caching. Since per-user caching does not result in privacy leakage, but may result in leakage of the API providers intellectual property, it is up to the provider to determine their level of risk tolerance. Another potential mitigation is to intentionally delay the response time for cache hits so that they look like cache misses. This eliminates the benefits of prompt caching for users, but API providers could still benefit, as cached prompts require less GPU processing time. 7. Related Work Prompt caching. Many recent works have developed optimizations for inference and serving of Transformer language models. Various methods involve reuse of the attention KV cache, improving latency and throughput for shared prompt prefixes (Kwon et al., 2023; Zheng et al., 2024a; Gim et al., 2024; Ye et al., 2024a;b; Qin et al., 2024; Juravsky et al., 2024). Recall that we do not assume any particular implementation of prompt caching in our attacks. Indeed, we do not know technical details about the caching mechanisms used by the APIs we audited. Other caching methods do not preserve exact model behavior, such as retrieving cached responses for semantically similar prompts (Bang, 2023) or reusing the KV cache even when the prefixes do not exactly match (Gim et al., 2024; Yao et al., 2024; Hu et al., 2024). We do not study such methods, but they are also likely sus8 Auditing Prompt Caching in Language Model APIs ceptible to similar cache timing attacks, and our audit can easily be adapted to detect other types of caching. Cache timing attacks. In computer security, many sidechannel timing attacks have extracted information by using timing differences to distinguish between cache hits and cache misses, e.g., in the CPU cache or web cache. For example, cache timing attacks have been used to extract AES keys (Bernstein, 2005; Osvik et al., 2006; Bonneau & Mironov, 2006; Tromer et al., 2010; Gullasch et al., 2011; Yarom et al., 2017), users private web information (Felten & Schneider, 2000; Bortz & Boneh, 2007; Van Goethem et al., 2015), and sensitive data from other processes on machine (Percival, 2005; Yarom & Falkner, 2014; Liu et al., 2015), as in the well-known Meltdown (Lipp et al., 2018) and Spectre attacks (Kocher et al., 2019). Attacks on language model APIs. Several recent works have attacked language model APIs. Carlini et al. (2024) and Finlayson et al. (2024) show that logits and logprobs leak information from an LLM API, including the models hidden dimension size and final layer weights. Weiss et al. (2024) partially extract encrypted and streamed LLM responses by inferring and analyzing token lengths from packet sizes. Carlini & Nasr (2024) and Wei et al. (2024) exploit speculative decoding (Leviathan et al., 2023; Chen et al., 2023) and similar methods to extract LLM responses with higher success by measuring delays between packets. Most related to our work are Song et al. (2024) and Zheng et al. (2024b), which also study timing attacks and privacy leakages arising from prompt caching, including both KV cache reuse and semantic caching, primarily in simulated, controlled environments. Our work differs in developing an audit that is practical and provides statistical guarantees, using these audits to precisely identify different levels of cache sharing, and extracting information about model architecture. Song et al. (2024) demonstrate prompt extraction attacks in simulated setting, but the attack is run locally without network latency, uses knowledge of the distribution of prompts, requires explicit clearing of the cache to make repeated measurements, and makes an average of over 200 measurements for each extracted token. Due to these limitations, we believe that these simulated attacks are currently unlikely to be real-world privacy threats. 8. Conclusion As LLMs and other machine learning systems become more widely deployed and used in the real world, it is increasingly important to consider security and privacy aspects of these systems. To this end, in this paper, we find that prompt caching in LLM APIs can leak private and proprietary information through timing differences. We develop and conduct rigorous statistical audits on real-world APIs, finding that multiple APIs were performing global cache sharing. We hope that future work will continue to evaluate and audit the security and privacy of machine learning systems, ensuring their robustness and trustworthiness."
        },
        {
            "title": "Impact Statement",
            "content": "Responsible disclosure. As discussed earlier, to mitigate real-world harms arising from our research, we performed responsible disclosure. In October 2024, we disclosed our audit results with each API provider in which we detected prompt caching. We gave providers 60 days to address the vulnerabilities before publicly releasing our findings, and the actual time elapsed ended up being longer. To our knowledge, at least five providers made changes to mitigate vulnerabilities, e.g., disabling global cache sharing across organizations and updating documentation. Broader impact. We believe that our audits for detecting prompt caching and the level of cache sharing in LLM APIs can improve transparency and trust. By increasing transparency around caching policies and how user data is handled, users can make better informed decisions about how they use an LLM API and have the appropriate level of trust that their data will be secure and private. More broadly, we believe that audits are promising method to ensure that machine learning systems are safe, secure, and trustworthy, especially as these systems become more widely deployed and have larger societal impact."
        },
        {
            "title": "Acknowledgments",
            "content": "CG was supported by the Stanford CURIS program. XL was supported by Two Sigma PhD Fellowship. PL was supported by NSF Award Grant no. 1805310 and an Open Philanthropy Project Award. TH was supported by gifts from Open Philanthropy, Amazon, Google, Meta, and grant under the NSF CAREER IIS-2338866."
        },
        {
            "title": "Conflicts of Interest",
            "content": "PL is co-founder of Together AI. However, this work was done in his Stanford capacity. The methods, providers audited, and results were not influenced by or shared with Together prior to the public release of this paper. All API providers were audited using the same procedure, including Together. None of the other authors have conflicts of interest with the providers audited in this paper."
        },
        {
            "title": "References",
            "content": "Anthropic. Prompt caching (beta). https: //web.archive.org/web/20241204121302/ Auditing Prompt Caching in Language Model APIs https://docs.anthropic.com/en/docs/ build-with-claude/prompt-caching, 2024a. //api-docs.deepseek.com/news/news0802, 2024. Anthropic. Prompt caching with claude. //web.archive.org/web/20240814170229/ https://www.anthropic.com/news/ prompt-caching, 2024b. https: Bang, F. GPTCache: An open-source semantic cache for LLM applications enabling faster answers and cost savings. In Tan, L., Milajevs, D., Chauhan, G., Gwinnup, J., and Rippeth, E. (eds.), Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pp. 212218, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlposs-1.24. URL https: //aclanthology.org/2023.nlposs-1.24. J. Bernstein, D. Cache-timing attacks on aes. 2005. URL https://cr.yp.to/antiforgery/ cachetiming-20050414.pdf. Bonneau, J. and Mironov, I. Cache-collision timing attacks against aes. In Cryptographic Hardware and Embedded Systems-CHES 2006: 8th International Workshop, Yokohama, Japan, October 10-13, 2006. Proceedings 8, pp. 201215. Springer, 2006. Bortz, A. and Boneh, D. Exposing private information by timing web applications. In Proceedings of the 16th international conference on World Wide Web, pp. 621 628, 2007. Carlini, N. and Nasr, M. Remote timing attacks on arXiv preprint efficient language model inference. arXiv:2410.17175, 2024. Carlini, N., Paleka, D., Dvijotham, K. D., Steinke, T., Hayase, J., Cooper, A. F., Lee, K., Jagielski, M., Nasr, M., Conmy, A., Wallace, E., Rolnick, D., and Tram`er, In F. Stealing part of production language model. Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 56805705. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/carlini24a.html. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. DeepSeek. Deepseek api introduces context caching on disk, cutting prices by an order of magnitude. https://web. archive.org/web/20241120201047/https: Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Felten, E. W. and Schneider, M. A. Timing attacks on web privacy. In Proceedings of the 7th ACM Conference on Computer and Communications Security, pp. 2532, 2000. Finlayson, M., Ren, X., and Swayamdipta, S. Logits of APIprotected LLMs leak proprietary information. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=oRcYFm8vyB. Fireworks. Prompt caching. https://web.archive. org/web/20250118003034/https://docs. fireworks.ai/guides/prompt-caching, 2024. Gage, P. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. Gim, I., Chen, G., Lee, S.-s., Sarda, N., Khandelwal, A., and Zhong, L. Prompt cache: Modular attention reuse for low-latency inference. Proceedings of Machine Learning and Systems, 6:325338, 2024. Gullasch, D., Bangerter, E., and Krenn, S. Cache games bringing access-based cache attacks on aes to practice. In 2011 IEEE Symposium on Security and Privacy, pp. 490505. IEEE, 2011. Hodges Jr, J. The significance probability of the smirnov two-sample test. Arkiv for matematik, 3(5):469486, 1958. Hu, J., Huang, W., Wang, H., Wang, W., Hu, T., Zhang, Q., Feng, H., Chen, X., Shan, Y., and Xie, T. Epic: Efficient position-independent context caching for serving large language models. arXiv preprint arXiv:2410.15332, 2024. Juravsky, J., Brown, B., Ehrlich, R., Fu, D. Y., Re, C., and Mirhoseini, A. Hydragen: High-throughput llm inference with shared prefixes. arXiv preprint arXiv:2402.05099, 2024. Kocher, P., Horn, J., Fogh, A., , Genkin, D., Gruss, D., Haas, W., Hamburg, M., Lipp, M., Mangard, S., Prescher, T., Schwarz, M., and Yarom, Y. Spectre attacks: Exploiting speculative execution. In 40th IEEE Symposium on Security and Privacy (S&P19), 2019. Auditing Prompt Caching in Language Model APIs Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. Lipp, M., Schwarz, M., Gruss, D., Prescher, T., Haas, W., Fogh, A., Horn, J., Mangard, S., Kocher, P., Genkin, D., Yarom, Y., and Hamburg, M. Meltdown: Reading kernel memory from user space. In 27th USENIX Security Symposium (USENIX Security 18), 2018. Liu, F., Yarom, Y., Ge, Q., Heiser, G., and Lee, R. B. Lastlevel cache side-channel attacks are practical. In 2015 IEEE symposium on security and privacy, pp. 605622. IEEE, 2015. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. MTEB: Massive text embedding benchmark. In Vlachos, A. and Augenstein, I. (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 20142037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main. 148. URL https://aclanthology.org/2023. eacl-main.148/. OpenAI. Prompt caching. https://platform. openai.com/docs/guides/prompt-caching, 2024a. OpenAI. in Prompt https://web.archive.org/web/ 20241003095307/https://openai.com/ index/api-prompt-caching/, 2024b. caching the api. Osvik, D. A., Shamir, A., and Tromer, E. Cache attacks and countermeasures: the case of aes. In Topics in Cryptology CT-RSA 2006: The Cryptographers Track at the RSA Conference 2006, San Jose, CA, USA, February 13-17, 2005. Proceedings, pp. 120. Springer, 2006. Percival, C. Cache missing for fun and profit. BSDCan Ottawa, 2005. URL https://www.daemonology. net/papers/htt.pdf. Qin, R., Li, Z., He, W., Zhang, M., Wu, Y., Zheng, W., and Xu, X. Mooncake: kvcache-centric disaggregated architecture for llm serving. arXiv preprint arXiv:2407.00079, 2024. Sennrich, R., Haddow, B., and Birch, A. Neural machine In Erk, translation of rare words with subword units. K. and Smith, N. A. (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162. Song, L., Pang, Z., Wang, W., Wang, Z., Wang, X., Chen, H., Song, W., Jin, Y., Meng, D., and Hou, R. The early bird catches the leak: Unveiling timing side channels in llm serving systems. arXiv preprint arXiv:2409.20002, 2024. Tromer, E., Osvik, D. A., and Shamir, A. Efficient cache attacks on aes, and countermeasures. Journal of Cryptology, 23:3771, 2010. Van Goethem, T., Joosen, W., and Nikiforakis, N. The clock is still ticking: Timing attacks in the modern web. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pp. 13821393, 2015. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2. Wei, J., Abdulrazzag, A., Zhang, T., Muursepp, A., and Saileshwar, G. Privacy risks of speculative decoding in large language models. arXiv preprint arXiv:2411.01076, 2024. Weiss, R., Ayzenshteyn, D., and Mirsky, Y. What was your prompt? remote keylogging attack on AI assistants. In 33rd USENIX Security Symposium (USENIX Auditing Prompt Caching in Language Model APIs Security 24), pp. 33673384, Philadelphia, PA, August 2024. USENIX Association. ISBN 978-1-939133-44-1. URL https://www.usenix.org/conference/ usenixsecurity24/presentation/weiss. Yao, J., Li, H., Liu, Y., Ray, S., Cheng, Y., Zhang, Q., Du, K., Lu, S., and Jiang, J. Cacheblend: Fast large language model serving with cached knowledge fusion. arXiv preprint arXiv:2405.16444, 2024. Yarom, Y. and Falkner, K. {FLUSH+ RELOAD}: high resolution, low noise, l3 cache {Side-Channel} attack. In 23rd USENIX security symposium (USENIX security 14), pp. 719732, 2014. Yarom, Y., Genkin, D., and Heninger, N. Cachebleed: timing attack on openssl constant-time rsa. Journal of Cryptographic Engineering, 7:99112, 2017. Ye, L., Tao, Z., Huang, Y., and Li, Y. ChunkAttention: Efficient self-attention with prefix-aware KV cache and two-phase partition. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1160811620, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.623. URL https: //aclanthology.org/2024.acl-long.623. Ye, Z., Lai, R., Lu, B.-R., Lin, C.-Y., Zheng, S., Chen, L., Chen, T., and Ceze, L. Cascade inference: Memory bandwidth efficient shared prefix batch decoding, February 2024b. URL https://flashinfer.ai/ 2024/02/02/cascade-inference.html. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/ forum?id=VqkAKQibpq. Zheng, X., Han, H., Shi, S., Fang, Q., Du, Z., Guo, Q., and Hu, X. Inputsnatch: Stealing input in llm services via timing side-channel attacks. arXiv preprint arXiv:2411.18191, 2024b. Zhu, M. Recall, precision and average precision. Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, 2(30):6, 2004. 12 A. Precision-Recall Curves Auditing Prompt Caching in Language Model APIs Figure 6 shows precision-recall curves for distinguishing between cache hit and cache miss times in APIs where we detected caching in our audits  (Table 1)  . B. P-values from Audits We report all the p-values from our audits on APIs. Table 3 contains p-values from level 1 of our audits: same prompt, per-user caching. Table 4 contains p-values from level 2 of our audits: prompts with the same prefix but different suffixes, per-user caching. Table 5 contains p-values from level 3 of our audits: prompts with the same prefix but different suffixes, per-organization caching. Table 6 contains p-values from level 4 of our audits: prompts with the same prefix but different suffixes, global caching. C. Ablation Effects on Audit p-values Figure 7 shows the effects of the ablations in 4.3 on the audit p-values. Each test is run using NUMSAMPLES = 250. We observe similar patterns as in 4.3, with decreases in average precision corresponding to increases in p-values. As the prompt length or prefix match length decreases, the p-values grow larger. We detect caching across all model sizes, with no clear relationship between model size and p-values. D. Embeddings and Response Times from the OpenAI text-embedding-3-small API Tables 7, 8, and 9 contain real examples of response times and embeddings illustrating the phenomenon discussed in 5. Each table contains the server-side response times and first five embedding coordinates when sending the same prompt 25 consecutive times to the OpenAI text-embedding-3-small API from the same user. More specifically, each table shows the victim requests for one prompt in the main audits, which used PROMPTLENGTH = 5000. The tables show that there is normal embedding (shown in blue) that is returned in most of the responses with normal response times, which indicate cache misses. When the response time is noticeably faster (shown in green), indicating cache hit, the embedding differs slightly from the normal embedding. Most, but not always all, fast responses have the same embedding. In some responses, especially those that are noticeably slower, the embedding (shown in red) differs from both the normal and cache hit embeddings. There are several different alternate embeddings among these responses. Auditing Prompt Caching in Language Model APIs Figure 6. Precision-recall curves for distinguishing between times produced by the cache hit and cache miss procedures in APIs where we detected caching in our audits  (Table 1)  . Cache hits are the positive class, and cache misses are the negative class. The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Note that our cache hit procedure attempts to produce cache hits but cannot guarantee cache hits (e.g., due to server routing), so some times in the cache hit distribution may actually be cache misses, which would hurt recall scores. 14 Auditing Prompt Caching in Language Model APIs Table 3. P-values from level 1 of our audits: same prompt, per-user caching. Each column shows one combination of NUMVICTIMREQUESTS and timing source (client-side or server-side timing). Green indicates significant p-value, after performing the appropriate Bonferroni corrections. Red indicates p-value that is not significant. indicates that the given timing source was not available for the API. APIs are grouped by whether caching was detected in this level and sorted alphabetically within the groups. NUMVICTIMREQUESTS"
        },
        {
            "title": "Anthropic\nAzure\nDeep Infra\nFireworks\nLepton\nOpenAI\nOpenAI\nPerplexity\nReplicate",
            "content": "Claude 3 Haiku text-embedding-3-small Llama 3.1 8B Instruct Llama 3.1 8B Instruct Llama 3.1 8B Instruct GPT-4o mini text-embedding-3-small Llama 3.1 8B Instruct Llama 3 8B Instruct Claude 3 Haiku GPT-4o mini Command embed-english-v3.0 DeepSeek Chat Gemini 1.5 Flash text-embedding-004 Llama 3 8B Instruct Amazon Azure Cohere Cohere DeepSeek Google Google Groq Hyperbolic Llama 3.1 8B Instruct Mistral Mistral OctoAI Together Mistral Nemo Mistral Embed Llama 3.1 8B Instruct Llama 3.1 8B Instruct Client 7.8 1021 1.7 1042 9.5 10116 2.0 1080 2.2 10138 2.4 1066 7.6 109 1.9 1090 0.27 0.95 0.62 0.41 0.75 0.17 0.20 0.41 0.72 0.56 0.67 0.32 0."
        },
        {
            "title": "Server",
            "content": "4.7 10109 2.2 10138 2.9 10105 2.3 1010 2.2 10140 0.51 0.72 0.56 0.20 0.24 0.51 0.96 0.91 0.27 0.96 Table 4. P-values from level 2 of our audits: prompts with the same prefix but different suffixes, per-user caching. Each column shows one combination of NUMVICTIMREQUESTS and timing source (client-side or server-side timing). Green indicates significant p-value, after performing the appropriate Bonferroni corrections. Red indicates p-value that is not significant. indicates that the given timing source was not available for the API. blank cell indicates that the given value of NUMVICTIMREQUESTS was not tested because caching was detected in the API using smaller value of NUMVICTIMREQUESTS. Caching was detected in all APIs audited in this level. APIs are sorted alphabetically. Provider Model Claude 3 Haiku text-embedding-3-small Anthropic Azure Deep Infra Llama 3.1 8B Instruct Llama 3.1 8B Instruct Fireworks Llama 3.1 8B Instruct Lepton GPT-4o mini OpenAI text-embedding-3-small OpenAI Llama 3.1 8B Instruct Perplexity Llama 3 8B Instruct Replicate Client 9.6 1037 0.20 0.03 4.3 1015 1.00 9.5 1027 0.03 5.4 1068 1 NUMVICTIMREQUESTS 5 25 Server Client Server Client Server 6.0 104 5.0 1022 7.7 1010 7.7 1010 6.9 1042 0.10 0. 2.6 1012 4.3 1015 5.0 1033 0.96 1.5 1039 0.03 8.610150 15 Auditing Prompt Caching in Language Model APIs Table 5. P-values from level 3 of our audits: prompts with the same prefix but different suffixes, per-organization caching. Each column shows one combination of NUMVICTIMREQUESTS and timing source (client-side or server-side timing). Green indicates significant p-value, after performing the appropriate Bonferroni corrections. Red indicates p-value that is not significant. indicates that the given timing source was not available for the API. blank cell indicates that the given value of NUMVICTIMREQUESTS was not tested because caching was detected in the API using smaller value of NUMVICTIMREQUESTS. Caching was detected in all APIs audited in this level. APIs are sorted alphabetically. Provider Model Anthropic Claude 3 Haiku Fireworks OpenAI OpenAI Llama 3.1 8B Instruct GPT-4o mini text-embedding-3-small 1 Client 1.7 1031 1.3 1021 1.1 1019 0. NUMVICTIMREQUESTS 5 25 Server Client Server Client Server 5.2 1032 4.6 1034 0.14 0.27 0.27 8.2 1014 8.2 10 Table 6. P-values from level 4 of our audits: prompts with the same prefix but different suffixes, global cache sharing. Each column shows one combination of NUMVICTIMREQUESTS and timing source (client-side or server-side timing). Green indicates significant p-value, after performing the appropriate Bonferroni corrections. Red indicates p-value that is not significant. indicates that the given timing source was not available for the API. blank cell indicates that the given value of NUMVICTIMREQUESTS was not tested because caching was detected in the API using smaller value of NUMVICTIMREQUESTS. APIs are grouped by whether caching was detected in this level and sorted alphabetically within the groups. Provider Model Client Server Client Server 1 NUMVICTIMREQUESTS 5 text-embedding-3-small Azure Deep Infra Llama 3.1 8B Instruct Llama 3.1 8B Instruct Fireworks Llama 3.1 8B Instruct Lepton text-embedding-3-small OpenAI Llama 3.1 8B Instruct Perplexity Llama 3 8B Instruct Replicate 0.46 6.5 105 9.0 1017 0.12 0.41 5.3 1074 5.2 1032 0.07 0.36 8.610150 25 Client 1.3 1021 Server 0.02 7.5 10 1.2 1010 0.20 1.4 109 0.08 1.1 1019 1.1 1019 Anthropic OpenAI Claude 3 Haiku GPT-4o mini 0.24 0.41 0.20 0.77 0.41 0.62 0.87 0. 0.94 16 Auditing Prompt Caching in Language Model APIs Table 7. Server-side response times and first five embedding coordinates when sending the same prompt 25 consecutive times to the OpenAI text-embedding-3-small API from the same user. Blue denotes the normal embedding returned in most of the responses with normal response times, which indicate cache misses. Green denotes fast response times, which indicate cache hits. Red denotes embeddings that differ from both the normal and cache hit embeddings. Time (s) Coordinate Coordinate 2 Coordinate 3 Coordinate 4 Coordinate"
        },
        {
            "title": "Embedding",
            "content": "0.100 0.096 0.119 0.088 0.216 0.100 0.096 0.088 0.077 0.036 0.076 0.124 0.280 0.032 0.089 0.034 0.092 0.089 0.103 0.127 0.094 0.039 0.035 0.080 0.034 0.00522740 0.00534875 0.00534875 0.00534875 0.00523302 0.00534875 0.00534875 0.00534875 0.00534875 0.00535751 0.00534875 0.00534875 0.00522179 0.00535751 0.00534875 0.00535751 0.00534875 0.00534875 0.00534875 0.00523272 0.00534875 0.00535751 0.00535751 0.00534875 0.00535751 0.02509154 0.02516800 0.02516800 0.02516800 0.02509207 0.02516800 0.02516800 0.02516800 0.02516800 0.02517040 0.02516800 0.02516800 0.02509099 0.02517040 0.02516800 0.02517040 0.02516800 0.02516800 0.02516800 0.02513466 0.02516800 0.02517040 0.02517040 0.02516800 0.02517040 0.01837845 0.01841400 0.01841400 0.01841400 0.01835683 0.01841400 0.01841400 0.01841400 0.01841400 0.01839376 0.01841400 0.01841400 0.01835604 0.01839376 0.01841400 0.01839376 0.01841400 0.01841400 0.01841400 0.01837780 0.01841400 0.01839376 0.01839376 0.01841400 0.01839376 0.02944954 0.02952400 0.02952400 0.02952400 0.02947217 0.02952400 0.02952400 0.02952400 0.02952400 0.02954882 0.02952400 0.02952400 0.02947091 0.02954882 0.02952400 0.02954882 0.02952400 0.02952400 0.02952400 0.02944849 0.02952400 0.02954882 0.02954882 0.02952400 0.02954882 0.04450446 0.04450600 0.04450600 0.04450600 0.04457144 0.04450600 0.04450600 0.04450600 0.04450600 0.04455426 0.04450600 0.04450600 0.04452551 0.04455426 0.04450600 0.04455426 0.04450600 0.04450600 0.04450600 0.04454690 0.04450600 0.04455426 0.04455426 0.04450600 0. 17 Auditing Prompt Caching in Language Model APIs Table 8. Server-side response times and first five embedding coordinates when sending the same prompt 25 consecutive times to the OpenAI text-embedding-3-small API from the same user. Blue denotes the normal embedding returned in most of the responses with normal response times, which indicate cache misses. Green denotes fast response times, which indicate cache hits. Red denotes embeddings that differ from both the normal and cache hit embeddings. Time (s) Coordinate 1 Coordinate Coordinate 3 Coordinate 4 Coordinate"
        },
        {
            "title": "Embedding",
            "content": "0.093 0.079 0.081 0.087 0.112 0.038 0.113 0.036 0.078 0.118 0.079 0.084 0.096 0.110 0.089 0.063 0.035 0.094 0.100 0.033 0.112 0.036 0.033 0.092 0.118 0.00455398 0.00455398 0.00455398 0.00455398 0.00455518 0.00453244 0.00455398 0.00453244 0.00455398 0.00455531 0.00455518 0.00455398 0.00455398 0.00455398 0.00455398 0.00453244 0.00453244 0.00455398 0.00455398 0.00453244 0.00455398 0.00453244 0.00453244 0.00455398 0.00454002 0.02148935 0.02148935 0.02148935 0.02148935 0.02146046 0.02149035 0.02148935 0.02149035 0.02148935 0.02148279 0.02146046 0.02148935 0.02148935 0.02148935 0.02148935 0.02149035 0.02149035 0.02148935 0.02148935 0.02149035 0.02148935 0.02149035 0.02149035 0.02148935 0.02149847 0.01970582 0.01970582 0.01970582 0.01970582 0.01972101 0.01968498 0.01970582 0.01968498 0.01970582 0.01974330 0.01972101 0.01970582 0.01970582 0.01970582 0.01970582 0.01968498 0.01968498 0.01970582 0.01970582 0.01968498 0.01970582 0.01968498 0.01968498 0.01970582 0.01983471 0.02810146 0.02810146 0.02810146 0.02810146 0.02807036 0.02810276 0.02810146 0.02810276 0.02810146 0.02809289 0.02807036 0.02810146 0.02810146 0.02810146 0.02810146 0.02810276 0.02810276 0.02810146 0.02810146 0.02810276 0.02810146 0.02810276 0.02810276 0.02810146 0.02807742 0.05159185 0.05159185 0.05159185 0.05159185 0.05157467 0.05159424 0.05159185 0.05159424 0.05159185 0.05153262 0.05157467 0.05159185 0.05159185 0.05159185 0.05159185 0.05159424 0.05159424 0.05159185 0.05159185 0.05159424 0.05159185 0.05159424 0.05159424 0.05159185 0. 18 Auditing Prompt Caching in Language Model APIs Table 9. Server-side response times and first five embedding coordinates when sending the same prompt 25 consecutive times to the OpenAI text-embedding-3-small API from the same user. Blue denotes the normal embedding returned in most of the responses with normal response times, which indicate cache misses. Green denotes fast response times, which indicate cache hits. Red denotes embeddings that differ from both the normal and cache hit embeddings. Time (s) Coordinate 1 Coordinate Coordinate 3 Coordinate 4 Coordinate"
        },
        {
            "title": "Embedding",
            "content": "0.113 0.033 0.087 0.097 0.163 0.142 0.033 0.119 0.090 0.100 0.261 0.426 0.040 0.036 0.075 0.087 0.093 0.116 0.140 0.034 0.033 0.033 0.032 0.089 0.032 0.00306934 0.00308367 0.00306934 0.00306934 0.00306934 0.00305834 0.00308367 0.00306308 0.00306934 0.00306934 0.00306656 0.00308757 0.00308367 0.00308367 0.00306934 0.00306934 0.00306934 0.00305834 0.00306308 0.00308367 0.00306686 0.00308367 0.00308367 0.00306934 0.00308367 0.02534029 0.02534279 0.02534029 0.02534029 0.02534029 0.02536461 0.02534279 0.02531247 0.02534029 0.02534029 0.02531808 0.02537424 0.02534279 0.02534279 0.02534029 0.02534029 0.02534029 0.02536461 0.02531247 0.02534279 0.02536540 0.02534279 0.02534279 0.02534029 0.02534279 0.02567696 0.02570194 0.02567696 0.02567696 0.02567696 0.02572376 0.02570194 0.02569395 0.02567696 0.02567696 0.02576698 0.02575598 0.02570194 0.02570194 0.02567696 0.02567696 0.02567696 0.02572376 0.02569395 0.02570194 0.02572455 0.02570194 0.02570194 0.02567696 0.02570194 0.02828057 0.02821602 0.02828057 0.02828057 0.02828057 0.02826022 0.02821602 0.02827457 0.02828057 0.02828057 0.02823594 0.02822604 0.02821602 0.02821602 0.02828057 0.02828057 0.02828057 0.02826022 0.02827457 0.02821602 0.02823864 0.02821602 0.02821602 0.02828057 0.02821602 0.05656114 0.05652182 0.05656114 0.05656114 0.05656114 0.05647554 0.05652182 0.05650425 0.05656114 0.05656114 0.05647188 0.05663172 0.05652182 0.05652182 0.05656114 0.05656114 0.05656114 0.05647554 0.05650425 0.05652182 0.05638750 0.05652182 0.05652182 0.05656114 0. 19 Auditing Prompt Caching in Language Model APIs Figure 7. Ablations on the effects of PROMPTLENGTH, PREFIXFRACTION, and model size on the audit p-values. Each test is run using NUMSAMPLES = 250. The top and bottom rows display the p-values on linear and logarithmic scales, respectively. In (a)(c), as the prompt length or prefix match length decreases, the p-values grow larger. In (d), we detect caching across all model sizes, with no clear relationship between model size and p-values."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}