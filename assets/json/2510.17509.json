{
    "paper_title": "Annotation-Efficient Universal Honesty Alignment",
    "authors": [
        "Shiyu Ni",
        "Keping Bi",
        "Jiafeng Guo",
        "Minghao Tang",
        "Jingtong Wu",
        "Zengxin Han",
        "Xueqi Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs."
        },
        {
            "title": "Start",
            "content": "ANNOTATION-EFFICIENT ALIGNMENT"
        },
        {
            "title": "HONESTY",
            "content": "Jiafeng Guo1,2 Minghao Tang1,2 Shiyu Ni1,2 Keping Bi1,2 Jingtong Wu Zengxin Han Xueqi Cheng1,2 1 State Key Laboratory of AI Safety, Institute of Computing Technology, CAS 2 University of Chinese Academy of Sciences {nishiyu23z, bikeping, guojiafeng, tangminghao25s, cxq}@ict.ac.cn wangzhenlingwu@163.com, zengxin.hanzx@gmail.com Code"
        },
        {
            "title": "Datasets and Models",
            "content": "5 2 0 2 0 2 ] . [ 1 9 0 5 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Honesty alignmentthe ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidenceis essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with small set of correctness annotations. To support large-scale study, we release HonestyBench, benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering scalable solution toward universal honesty alignment in LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Honesty alignmentthe ability of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Achiam et al., 2023), to accurately recognize their knowledge boundaries (i.e., knowing what they know and what they do not) and faithfully express their confidenceis critical for trustworthy AI deployment. Honesty is one of the HHH criteria in alignment: helpful, harmless, and honest (Askell et al., 2021). Ideally, such self-assessment should occur before generation. This enables models to give the answer when confidence is high and to abstain or seek external assistance (e.g., triggering retrieval-augmented generation) when uncertain. Existing research on honesty alignment falls into two categories: training-free and training-based methods. Training-free methods typically estimate confidence in three ways: 1) token-level generation probabilities (Guo et al., 2017; Jiang et al., 2021); 2) prompting models to verbally express confidence (Ni et al., 2024a; Yin et al., 2023); and 3) self-consistency, i.e., measuring semantic consistency across multiple responses (Manakul et al., 2023; Zhang et al., 2023). Among them, self-consistency achieves the strongest alignment with actual correctness (See Figure 4). By contrast, training-based methods leverage correctness annotations to calibrate model confidence (Lin et al., 2022; Zhang et al., 2024; Yang et al., 2023). While generally more effective, developing universal model that performs reliably across diverse tasks demands substantial ground-truth answers, which are expensive to obtain. This raises key question: Do LLMs truly require so many correctness annotations to achieve optimal honesty alignment? We posit that correctness annotations serve two roles: first, teaching models to express confidence, and second, calibrating this expressed confidence against correctness. If confidence can be elicited from models using inexpensive supervisione.g., self-consistency signalsthen only small amount of correctness-labeled data may be needed for calibration. This motivates our proposed annotation-efficient framework: Elicitation-Then-Calibration (EliCal)."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The models confidence in answering question is represented by the confidence of its most confident answer, computed via self-consistency as the proportion of generations agreeing with the greedy-search answer (Top). The models capability is reflected by the proportion of correct responses, measured as the fraction of generations matching the ground-truth answer (Bottom). These two signals show high correlation across questions. As illustrated in Figure 3, EliCal operates in two stages. In Stage 1: Confidence Elicitation, the model learns to express internal confidence from self-consistency-based supervision. This enables one-shot confidence expression without repeated sampling. Because self-consistency confidence aligns reasonably well with correctness and is inexpensive to collect at scale, this stage provides solid foundation. In Stage 2: Confidence Calibration, much smaller set of correctness annotations is sufficient to align confidence with actual accuracy. The two stages resemble pretrainingfinetuning paradigm, explaining why EliCal is more annotation-efficient than calibration-only (finetuning-only) approaches, hereafter abbreviated as Cal-Only. With less reliance on correctness annotations, EliCal also generalizes better to unseen tasks. To facilitate large-scale training and evaluation, we introduce HonestyBench, benchmark designed for universal honesty alignment across tasks. HonestyBench consolidates ten widely used free-form factual QA datasets, offering over 560k training samples, 38k in-domain evaluation samples, and 33k out-of-domain evaluation samples. For each modelquestion pair, HonestyBench includes twenty sampled responses and one greedy-search response of three representative LLMs, annotated with both correctness and self-consistency confidence. This benchmark facilitates large-scale pretraining and cross-task finetuning, advancing honesty alignment toward universal model and moving beyond the traditional in-domain evaluation paradigm (Yang et al., 2023; Ni et al., 2025). Extensive experiments on HonestyBench demonstrate three key findings: 1) Both EliCal and CalOnly achieve upper-bound alignment across ten QA tasks when trained with all 560k+ correctness annotations, outperforming the best training-free baseline by over 17%. 2) EliCal achieves approximately 98% of this upper bound using only 1k labeled samples (0.18%). 3) EliCal trained on HonestyBench consistently yields significantly better alignment performance on MMLU (Hendrycks et al., 2020) tasks compared to Cal-Only, confirming its superior generalization capability."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Research on model honesty alignment largely focuses on how to measure and calibrate confidence, which can be categorized into training-free and training-based approaches. 2.1 TRAINING-FREE CONFIDENCE INVESTIGATION Early works linked confidence to token probabilities (Guo et al., 2017; Desai & Durrett, 2020; Jiang et al., 2021), but these signals are often miscalibrated in free-form generation where probabilities can be dominated by semantically irrelevant tokens. To address this, self-consistency-based methods measure confidence from the semantic consistency of multiple generations (Manakul et al., 2023), achieving the most reliable results among training-free methods. Another line explores verbalized confidence, where LLMs explicitly their confidence in words (Lin et al., 2022; Yin et al., 2023; Tian et al., 2023), though these models often remain overconfident."
        },
        {
            "title": "2.2 TRAINING-BASED CONFIDENCE CALIBRATION",
            "content": "These studies leverage correctness annotations to calibrate model confidence, achieving better performance than training-free methods, and can be broadly divided into two categories. One line leverages LLMs internal states to predict confidence either after or even before generation (Azaria & Mitchell, 2023; Chen et al., 2024; Wang et al., 2024). Another line trains models to verbalize confidence reliably (Lin et al., 2022; Zhang et al., 2024). All these methods rely on correctness annotations, and achieving optimal performance requires high annotation costs. Although some works (Zhang et al., 2024; Tjandra et al., 2024) exploit LLMs internal uncertainty as supervision signal, it is only used to determine abstention rather than to teach models to express their own confidence. Apart from that, all the above methods are trained only on small-scale datasets. In contrast, this paper frames honesty alignment as two-stage learning problem and proposes an annotation-efficient method EliCal. EliCal first elicits the model to express its internal confidence estimated via self-consistency on large scale question set, and then calibrates the elicited confidence to true correctness using small amount of annotations. In addition, we introduce HonestyBench which establishes pathway toward achieving the upper bound of performance for universal models across diverse tasks. Due to space limitations, more related works can be found in A."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "In this section, we formalize the task of LLM honesty alignment and introduce confidence measurement through self-consistency. 3.1 TASK FORMULATION OF HONESTY ALIGNMENT We aim to enable the model to output its confidence for given question before response generation, which can accurately reflect the probability of correct response. For example, if model reports 80% confidence, its answer should have an 80% chance of being correct. Given question q, model with parameters θ, and decoding policy π, the model defines distribution pπ θ (r q) over outputs, with denoting the set of all possible responses. The models capability on can be represented by the expected accuracy over all its possible responses. Let G(q) denote the set of all correct responses for q, we define the correctness indicator of response as: Accuracyθ(q, r) I[ G(q) ] {0, 1} , (1) if G(q), it is deemed as correct; Otherwise, is wrong. The models actual capability can be reflected by the expected accuracy of all possible responses in R: Accuracyθ(q) Erpπ θ (q)[ Accuracyθ(q, r) ] = (cid:88) rR pπ θ (r q) Accuracyθ(q, r). (2) Honesty Alignment Objective. For question q, we aim to optimize an optimal target confidence score Confidence θ(q) which ranges from 0 to 1 (i.e., [0, 1]) that reflects its ability to provide correct answer, satisfying Confidence θ(q) = Accuracyθ(q). (3) Objective Approximation. Since obtaining all possible responses is impractical in real-world scenarios, Accuracyθ(q) is usually approximated based on ˆR, set of responses sampled under π. Accuracyθ(q) Erpπ θ (q)[ Accuracyθ(q, r) ] 1 (cid:88) ˆR Accuracyθ(q, r). (4) 3.2 CONFIDENCE ESTIMATION BASED ON SELF-CONSISTENCY models confidence in correctly answering question can be reflected by the generation probability of the models most confident response arg maxrR pπ θ (r q), which is defined as: Confidenceθ(q) = pπ θ (r q) (5)"
        },
        {
            "title": "Preprint",
            "content": "Recent studies (Manakul et al., 2023; Zhang et al., 2023) propose self-consistency as state-of-theart training-free method for confidence estimation. It evaluates models confidence in response by checking whether the model consistently generates responses with the same semantics as across multiple generations. We define s(r, r) to represent whether is semantically consistent with as: s(r, r) I[ Consistent(r, r)] {0, 1} , (6) where s(r, r) = 1 if the two responses are semantically consistent; Otherwise, s(r, r) = 0. pπ can be represented by Erpπ practice, pπ responses sampled under the decoding policy π. θ (r q) [ s(r, r) ]. Since it is infeasible to obtain all possible generations in θ (r q) is computed via self-consistency based on sampled set ˆR which consists of θ θ (r q) Erpπ pπ θ [ s(r, r) ] = (cid:88) rR pπ θ (r q) s(r, r) 1 (cid:88) ˆR s(r, r). (7) Self-consistency confidence vs. semantic uncertainty. Semantic uncertainty (Kuhn et al., 2023) measures models uncertainty about question by estimating the entropy of its response space. It samples multiple responses, clusters them into semantic groups (where responses with equivalent meanings are grouped together), and computes the entropy across these groups as the uncertainty metric. However, this method requires costly semantic clustering and thus brings substantial computational overhead. In contrast, self-consistency confidence provides more efficient alternative. It directly uses the models probability mass assigned to the largest semantic clusterthat is, the most consistent answeras its confidence. This avoids explicit clustering and significantly reduces computation while retaining similar ability to reflect semantic variability across model outputs."
        },
        {
            "title": "4 ELICAL:ELICITATION-THEN-CALIBRATION",
            "content": "In this section, we introduce EliCal (Elicitation-Then-Calibration), two-stage training framework for honesty alignment, which first activates the model to express its internal confidence on question, and then leverages small amount of correctness annotations for further calibration. An overview of EliCal is shown in Figure 3. 4.1 OVERVIEW Although consistency-based confidence estimation achieves strong alignment performance and is state-of-the-art (SOTA) among training-free approaches, it requires extensive sampling to reliably estimate confidence. To address this inefficiency, we propose one-shot alternative: eliciting the models internal confidence by training it with unsupervised, consistencybased confidence signals. Because this estimation and expression depend solely on the models internal representations, such confidence elicitation is inherently learnable. In Figure 2, we show comparison between self-consistency confidence and the models true capabilities. It can be seen that the model is generally overconfident, but self-consistency confidence is highly correlated with true capabilities. Figure 2: Self-consistency confidence vs. correctness on TQ (Qwen2.5-7B-Instruct). For enhanced honesty alignment, it is crucial to use correctness annotations to project and calibrate the models expressed confidence against its actual accuracy in answering questions. Unlike traditional calibration methods that attempt to adjust confidence from scratch, our proposed method, EliCal, first teaches the model to articulate its inherent confidence. This foundational step enables subsequent calibration to be more precise and annotation-efficient, requiring far fewer correctness labels than calibration-only approaches. 4.2 MODEL ARCHITECTURE To ensure that training the model for honesty does not compromise its original capabilities (e.g., QA performance), we freeze the model parameters θ and introduce Low-Rank Adaptation (LoRA) (Hu"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: EliCal reframes honesty alignment as two-stage learning problem: 1) Confidence Elicitation, which constructs training data from large set of questions with labels derived through self-consistency; 2) Confidence Calibration, which constructs correctness annotation using small set of QA pairs to bridge the gap between the models expressed confidence and its actual accuracy. et al., 2022) modules into all linear layers, enabling rich interaction with the internal states. An additional linear head is attached to the final layer to predict the confidence score. Consider an LLM with transformer layers and hidden dimension d. For an input question = (q1, . . . , qT ) containing tokens, let h(ℓ) Rd denote the internal state of token qt at layer ℓ {1, . . . , L}. The internal states are generated by the frozen backbone parameters θ together with the trainable LoRA parameters θLoRA. On top of the final layer, we attach linear head fϕ : Rd that maps the internal state of the last question token h(L) (θ, θLoRA) into confidence score: ˆc = fϕ(h(L) (θ, θLoRA)) = wh(L) (θ, θLoRA) + b, (8) where ϕ = {w, b} are the parameters of the linear head. During training, only θLoRA and ϕ are updated, while θ remains frozen. The supervision signal is given by confidence targets c, and the objective is mean squared error (MSE): L(ϕ, θLoRA) = 1 N (cid:88) i=1 (ˆci ci)2, (9) where is the number of training samples. Detailed application of LoRA can be found in D. 4.3 TWO STAGES OF ELICAL The two stages of EliCal construct the target confidence in different ways. Stage 1-Confidence Elicitation. The goal of this stage is to train the model to elicit its internal confidence. For model with frozen backbone parameters θ, given large question set annotated with self-consistency signals, we define the self-consistency target for each question as Confidenceθ(q) (See equation 7). The LoRA parameters and linear head are initialized as θ0 LoRA and ϕ0, and the internal state used is h(L) LoRA). (θ, θ0 These parameters are trained using the MSE objective: L(ϕ0, θ0 LoRA) = 1 (cid:88) qQ (cid:0)ˆc(q) Confidenceθ(q)(cid:1)2 , (10) where ˆc(q) = fϕ0 (h(L) in Q. After this stage, we obtain ϕ1 and θ1 (θ, θ0 LoRA. LoRA)) is the predicted confidence and means the count of samples"
        },
        {
            "title": "Preprint",
            "content": "Stage 2-Confidence Calibration. The goal of this stage is to calibrate the models confidence using small set of QA pairs Qsmall with correctness annotations. Starting from the parameters ϕ1 and θ1 LoRA obtained from Stage 1, we fine-tune the LoRA modules and the linear head to predict the correctness score Accuracyθ(q) (See equation 4) for each Qsmall. The internal state used is now: h(L) (θ, θ1 LoRA), and the MSE objective is 1 Qsmall L(ϕ1, θ1 LoRA) = (cid:88) qQsmall (cid:0)ˆc(q) Accuracyθ(q)(cid:1)2 , (11) where ˆc(q) = fϕ1(h(L) Qsmall. After this stage, the parameters are updated to ϕ2 and θ2 LoRA)) is the predicted score and Qsmall means the count of samples in LoRA. (θ, θ Dicussions. Elicitation-Then-Calibration can be viewed as pretrainingfinetuning paradigm specifically tailored for honesty alignment, with the elicitation stage providing solid foundation. Self-consistency confidence is inherently learnable, requires no human annotation, and could offer strong generalization by externalizing internal signals rather than fitting domain-specific labels. Following confidence elicitation, the model equipped with ϕ2 and θ2 LoRA can predict confidence prior to generation, avoiding the overhead of repeated sampling and consistency checking."
        },
        {
            "title": "5 HONESTYBENCH",
            "content": "To advance toward universal model with strong honesty alignment across tasks, we introduce HonestyBench (See Table 1), large-scale benchmark that consolidates 10 widely used public freeform factual question-answering datasets. HonestyBench comprises 560k training samples, along with 38k in-domain and 33k out-of-domain (OOD) evaluation samples. It establishes pathway toward achieving the upper bound of performance for universal models across diverse tasks, while also serving as robust and reliable testbed for comparing different approaches. Table 1: The number of training and evaluation samples is as follows. For ParaRel, we randomly sample 3,000 instances as the test set and use the rest for training. For the other datasets, we use the train set for training and, if available, the test set for evaluation; otherwise, we use the dev set. Training Data In-Domain Evaluation OOD Evaluation Datasets Set Count Datasets Set Count Datasets Set Count NQ TQ HQ 2Wiki ParaRel Total Train Train Train Train Split 87,925 NQ 87,622 TQ 90,447 HQ 167,454 134,199 2Wiki ParaRel Test Dev Dev Dev Split 3,610 Squad 11,313 WQ Dev Test Dev 12,576 MuSiQue Dev Dev PopQA 7,405 CWQ 3, 10,570 2,032 3,519 2,417 14,267 / 567,647 Total / 37,904 Total / 32,805 LLMs. We obtained the correctness annotations and self-consistency confidence of three representative open-source LLMs: Qwen2.5-7B-Instruct (Qwen et al., 2025), Qwen2.5-14B-Instruct (Qwen et al., 2025), and Llama3-8B-Instruct (Dubey et al., 2024). HonestyBench-Train. The training portion of HonestyBench integrates the training sets of five widely used QA datasetsNatural Questions (NQ) (Kwiatkowski et al., 2019), TrivialQA (TQ) (Joshi et al., 2017), 2WikiMultihopQA (2Wiki) (Ho et al., 2020), HotpotQA (HQ) (Yang et al., 2018), and ParaRel (Elazar et al., 2021). These datasets cover single-hop, multi-hop, and template-generated questions, amounting to over 560k QA pairs. For each question, the model generates one greedy response and (i.e., = 20) sampled responses (temperature=1). Sampled responses are annotated for semantic consistency with the greedy response, and all answers are annotated for correctness. HonestyBench-Eval. HonestyBench-Eval provides evaluation across both in-domain and OOD In-domain evaluation uses the test or development splits of the five datasets inscenarios. cluded in HonestyBench-Train. Out-of-domain (OOD) evaluation covers five additional factual QA"
        },
        {
            "title": "Preprint",
            "content": "datasetsSQuAD (Rajpurkar et al., 2016), WebQuestions (WQ) (Berant et al., 2013), ComplexWebQuestions (CWQ) (Talmor & Berant, 2018), MuSiQue (Trivedi et al., 2022), and PopQA (Mallen et al., 2022)spanning single-hop, multi-hop, and template-generated questions in diverse domains. The in-domain evaluation contains approximately 38k QA pairs, while the out-of-domain evaluation contains approximately 33k QA pairs. As in training, each question is annotated with both consistency and correctness scores. Details. For answer generation, we use the prompt shown in Figure 15. For correctness evaluation and semantic consistency checking, to ensure accuracy as much as possible, we employ the powerful LLM Qwen2.5-32B-Instruct (Qwen et al., 2025), with the specific prompts provided in Figure 16 and Figure 12, respectively."
        },
        {
            "title": "6 EXPERIMENTAL SETUP",
            "content": "In this section, we introduce the evaluation metrics, baselines, datasets, and implementation details. Metrics. For QA performance, we measure accuracy by verifying whether the models greedy search output matches any ground-truth answer using Qwen2.5-32B-Instruct (scored as 1 if correct, 0 if incorrect). To evaluate honesty alignment, we adopt the widely used AUROC (Hanley & McNeil, 1982) (Area Under the Receiver Operating Characteristic Curve) metric. AUROC measures models ability to distinguish correct from incorrect predictions: higher values indicate that the model assigns higher confidence to correct answers. It is computed as the area under the curve plotting the true positive rate against the false positive rate at varying confidence thresholds. value of 1 represents perfect discrimination, while 0.5 corresponds to random guessing. We also evaluate honesty alignment using ECE (Guo et al., 2017) in B. Baselines. We compare EliCal with six representative training-free baselines and two trainingbased baselines. The training-free methods include three types, each with two variants: 1) Probabilistic confidence (Prob): sequence-level generation probability, with length-normalized version (N-Prob); 2) Self-consistency (Consis) (Manakul et al., 2023; Ho et al., 2020): measured via lexical similarity (Consis-Lex) or an LLM for semantic similarity (Consis-Sem); 3) Verbalized confidence (Verbal) (Xiong et al., 2023): model expresses confidence in natural language, in zero-shot (Verbal-0) and few-shot (Verbal-10) settings. The training-based baselines are: 1) Elicitation-Only (Eli-Only): learning from Consis-Sem, and 2) Calibration-Only (Cal-Only) (Yang et al., 2023): learning from correctness from scratch. Implementation details are in E. Datasets. EliCal and Eli-Only perform elicitation using all questions in HonestyBench-Train with self-consistency confidence. We randomly sample correctness annotations of varying sizes (from 1k to over 560k) from HonestyBench to examine how the performance of EliCal and Cali-Only scales with the amount of annotated data. All methods are evaluated on HonestyBench-Eval. Details of the parameter settings and implementation details are provided in C."
        },
        {
            "title": "7 RESULTS AND ANALYSIS",
            "content": "We evaluate ARUOC scores of all training-free methods. The results are shown in Figure 4, indicating that: Consis-Sem provides the most accurate confidence estimation among training-free methods. As shown in Figure 4, Consis-Sem achieves the highest AUROC. That is why we use it for internal confidence estimation. In addition, Prob and N-Prob compute response generation probabilities at the token level, whereas Consis-Lex measures token-level similarity, which is negatively affected by semantically irrelevant tokens. The models ability to express confidence in words is limited, although fewshot prompting provides slight improvement. Figure 4: Average performance of training-free methods across all models in the in-domain setting. The AUROC scores of different methods for Qwen2.5-7BInstruct are reported in Table 2, while results for the other models are provided in Table 4 of B."
        },
        {
            "title": "Preprint",
            "content": "We vary the amount of annotated data from 1k to over 560k, with the results under in-domain setting presented in Figure 5. Results under the OOD setting can be found in Figure 8. The main conclusions are summarized as follows. Table 2: AUROC scores on Qwen2.5-7B-Instruct. The numbers in () indicate the amount of annotated data used. Bold denotes the best scores, and the second-best scores are underlined. Category Methods In-Domain Evaluation OOD Evaluation NQ TQ HQ 2Wiki Pararel Avg. Squad WQ CWQ MSQ PopQA Avg. Training-free Training-based Upper Bound Prob N-Prob Verbal-0 Verbal-10 Consis-Lex Consis-Sem Eli-Only Cal-Only (1k) EliCal (1k) Cal-Only (560k) EliCal (560k) 56.79 66.11 64.02 68.82 65.02 80.68 77.86 72.19 82.38 84.89 85.16 70.26 72.96 70.22 62.35 74.98 90.20 86.23 68.75 87. 88.96 89.09 54.29 61.96 66.49 70.53 68.98 80.12 77.27 74.34 84.48 85.64 86.09 41.73 59.33 65.02 73.24 67.82 55.40 54.36 76.17 82. 83.97 84.19 58.71 61.67 70.81 71.50 66.35 62.93 62.05 78.61 84.31 88.07 88.89 55.48 64.75 67.22 68.90 69.80 73.62 71.19 73.41 84. 86.20 86.49 56.63 60.72 65.76 72.54 62.12 66.16 60.66 71.59 78.48 81.19 81.04 61.30 66.06 70.41 68.20 65.43 76.26 76.61 71.48 80. 81.30 81.10 68.34 70.51 59.56 63.25 72.59 77.50 74.77 69.33 79.85 80.45 81.02 61.85 65.93 60.54 64.44 61.07 70.76 66.56 66.96 78. 79.58 80.68 71.30 74.73 70.64 73.40 77.07 70.44 74.60 86.13 91.74 92.11 92.11 64.94 68.58 67.12 71.05 69.87 70.20 69.66 77.32 84. 85.75 85.83 Figure 5: AUROC of EliCal and Cal-Only as the scale of annotated data varies. HonestyBench establishes pathway toward achieving the upper bound of performance for universal models across diverse task. As shown in Table 2 both EliCal and Cal-Only achieve very high AUROC scores after leveraging all annotated data in HonestyBench, significantly outperforming all training-free methods. Figures 5 and Figure 8 further show that for both in-domain and OOD settings, the performance of the two methods tends to saturate as the amount of annotated data increases. This is the first time that honesty alignment has been trained and validated on such large-scale dataset to explore its upper bound. EliCal is annotation-efficient, achieving about 98% of the performance of Cal-Only trained on over 560k annotations using only 1k annotated samples. Table 2 shows that with just 1k correctness annotations, EliCal significantly outperforms all baseline methods and achieves the highest AUROC on nearly all datasets. In comparison, Cal-Only (1k) fails to outperform the best trainingfree methods on many datasets, such as NQ and HQ. As shown in Figure 5, in the in-domain setting, EliCal generally outperforms Cal-Only, especially when annotated data is limited. This indicates that large-scale confidence elicitation provides strong foundation for subsequent calibration, reducing the reliance on correctness annotations. EliCal demonstrates strong generalization. As shown in Table 2, EliCal (1k) achieves strong performance in OOD settings. Figure 8 further shows that in standard OOD scenarios, where question formats resemble the training data, EliCal generally outperforms Cal-Only, with the two converging when sufficient annotations are available. In both in-domain and OOD settings, we observe very similar phenomena, which may be attributed to their shared question format (free-form questions) and the fact that most QA pairs are constructed from Wikipedia. To test more challenging cases, we evaluate on MMLU (Hendrycks et al., 2020), multi-choice benchmark that differs substantially from the free-form questions used in training. As shown in Figure 5, even with over 560k annotations, Cal-Only lags behind EliCal. These results indicate that leveraging the models internal signals at scale, rather than relying solely on task-specific labels, leads to better generalization."
        },
        {
            "title": "Preprint",
            "content": "LLMs can be taught to express their internal confidence. As shown in Table 2, Eli-Only performs on par with Consis-Sem, indicating that LLMs can be taught to express their internal confidence. Unlike Consis-Sem, Eli-Only does not require multiple generations and, without any annotated data, can reduce the cost of estimating model confidence during inference. Figure 6: Alignment of EliCal and Cal-Only as the scale of annotated data varies. The confidence output by EliCal can be binarized to determine whether the model answers correctly. In addition to AUROC, we use alignment (Ni et al., 2024a) to directly measure how reliably the models confidence reflects correctness. Alignment is defined as the proportion of predictions whose binarized confidence matches their true correctness. For each test set, 20% of samples (random selected) are used to select the threshold that maximizes alignment, and the remaining 80% for evaluation. The results are shown in Figure 6 and Figure 9. The alignment of EliCal significantly outperforms Cal-Only. In the in-domain setting, Cal-Only is comparable to EliCal when large amount of annotations is available, while in MMLU, EliCal consistently leads. This demonstrates that EliCal provides reliable confidence estimates for real-world scenarios requiring binarized decisions, such as determining whether to perform retrieval augmentation. 7.1 ABLATION Effects of training size for elicitation. To study the impact of training data size for elicitation, we apply confidence elicitation to Qwen2.5-7B-Instruct using varying amounts of training data. Average results across all indomain datasets are shown in Figure 7. It can be observed that as the training data increases, the elicitation performance improves, with the rate of improvement gradually slowing down, eventually approaching Consis-Sem. Training on linear head. Since more trainable parameters require more data for cold start, we conduct an ablation study using lighter network. We fix the model and train only linear head that maps the final-layer hidden state of the last question token to confidence score, with all other settings as in 6. Results, shown in Figure 10, indicate that honesty performance improves with more labeled data, and EliCal consistently outperforms Cal-Only, especially when data is limited. However, using only linear head limits interaction and expressiveness, leading to lower performance than in Figure 5. Figure 7: The impact of training size on elicitation performance of Qwen2.57B-Instruct in the in-domain setting."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this paper, we propose EliCal, an annotation-efficient two-stage training framework for honesty alignment, and introduce HonestyBench, large-scale benchmark enabling universal honesty training and comprehensive evaluation. Our results demonstrate that EliCal significantly improves model confidence expression with minimal labeled data, while HonestyBench supports the development of models that excel across diverse tasks. This work sets the stage for scalable, high-performance, and data-efficient honesty alignment in real-world AI applications."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "All models used in this paper are open-source, and all datasets are publicly available factual QA datasets that do not contain harmful information. Furthermore, this work is dedicated to improving model honesty and does not involve the generation of harmful content."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "First, the models we use are open-source, and our datasets are constructed from publicly available sources. In Section 5, we describe in detail the construction of HonestyBench and the prompts used. In Section E, we explain the implementation of each method, and in Section C, we provide the experimental parameter settings. We believe that the results in this paper are easy to reproduce. Moreover, since our training is based on LoRA rather than directly fine-tuning the full model, reproduction does not require extensive GPU resources. In addition, we will open-source all code, HonestyBench, and all trained models."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734, 2023. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 15331544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1160. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Inside: Llms internal states retain the power of hallucination detection. arXiv preprint Ye. arXiv:2402.03744, 2024. Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892, 2020. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. arXiv preprint arXiv:2402.10612, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:10121031, 2021."
        },
        {
            "title": "Preprint",
            "content": "Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frederic Blain, Francisco Guzman, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8: 539555, 2020. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 13211330. PMLR, 2017. James Hanley and Barbara McNeil. The meaning and use of the area under receiver operating characteristic (roc) curve. Radiology, 143(1):2936, 1982. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962977, 2021. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022. Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023. Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do llms need retrieval augmentation? mitigating llms overconfidence helps retrieval augmentation. arXiv preprint arXiv:2402.11457, 2024a. Shiyu Ni, Keping Bi, Lulu Yu, and Jiafeng Guo. Are large language models more honest in their probabilistic or verbalized confidence? arXiv preprint arXiv:2408.09773, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, and Xueqi Cheng. Towards fully exploiting llm internal states to enhance knowledge boundary perception. arXiv preprint arXiv:2502.11677, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022. Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Unsupervised real-time hallucination detection based on the internal states of large language models. arXiv preprint arXiv:2403.06448, 2024. Alon Talmor and Jonathan Berant. The web as knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641 651, 2018. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated arXiv preprint confidence scores from language models fine-tuned with human feedback. arXiv:2305.14975, 2023. Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, and Yarin Gal. Finetuning large language models to appropriately abstain with semantic entropy. arXiv preprint arXiv:2410.17234, 2024. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Yanling Wang, Haoyang Li, Hao Zou, Jing Zhang, Xinlei He, Qi Li, and Ke Xu. Hidden question representations tell non-factuality within and across large language models. arXiv e-prints, pp. arXiv2406, 2024. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty. arXiv preprint arXiv:2312.07000, 2023. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018."
        },
        {
            "title": "Preprint",
            "content": "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they dont know? arXiv preprint arXiv:2305.18153, 2023. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71067132, 2024. Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Malin, and Sricharan Kumar. Sac3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. arXiv preprint arXiv:2311.01740, 2023."
        },
        {
            "title": "A RELATED WORK",
            "content": "Honesty is evaluated by whether the models confidence aligns with its actual ability, where actual ability is typically measured by the correctness of its answers. Existing research focuses on how to measure and calibrate confidence, which can be broadly categorized into the two series. A.1 TRAINING-FREE CONFIDENCE INVESTIGATION 1) Probability-based Confidence. common approach links model confidence to the probabilities assigned during token generation (Guo et al., 2017; Desai & Durrett, 2020; Jiang et al., 2021; Kadavath et al., 2022; Si et al., 2022; Kuhn et al., 2023). Early work (Guo et al., 2017) revealed that modern neural networks such as ResNet (He et al., 2016) tend to produce overconfident predictions, and introduced temperature scaling as correction. Later, Desai & Durrett (2020) showed that pretrained language models such as BERT (Devlin, 2018) achieve more reliable calibration compared to models without pretraining. As generative models became prominent, Jiang et al. (2021) reported that T5 (Raffel et al., 2020) still exhibited miscalibration, often being more confident than warranted. Recent studies highlight that LLMs appear well calibrated in structured tasks (e.g., multiple-choice QA) under suitable prompting (Kadavath et al., 2022; Si et al., 2022), but their probabilities deviate substantially from correctness in free-form generation. 2) Consistency-based Confidence. Since raw token probabilities cannot always capture semantic reliability, and may not be accessible for black-box APIs, another line of work infers confidence from agreement across multiple responses (Fomicheva et al., 2020; Manakul et al., 2023; Kuhn et al., 2023; Zhang et al., 2023; Ding et al., 2024). The intuition is that confident models should yield stable answers across repeated generations. Early methods (Fomicheva et al., 2020) used surfacelevel similarity to assess agreement, while subsequent efforts employed semantic measures with NLI models or LLMs (Manakul et al., 2023; Kuhn et al., 2023). Recognizing that consistency alone does not guarantee correctness, Zhang et al. (2023) proposed cross-model agreement, leveraging the observation that different models often err differently. More recently, Ding et al. (2024) generalized this idea across multiple languages. 3) Verbalized Confidence. Another direction enables LLMs to explicitly articulate their confidence in natural language (Lin et al., 2022; Yin et al., 2023; Tian et al., 2023; Xiong et al., 2023; Zhang et al., 2024; Yang et al., 2023; Ni et al., 2024a). Yin et al. (2023) and Ni et al. (2024a) examined whether models can judge the answerability of questions, showing partial success but frequent overconfidence. Beyond coarse judgments, Tian et al. (2023) and Xiong et al. (2023) studied fine-grained verbalization: the former proposed generating multiple candidate answers at once to aid confidence expression, while the latter systematically evaluated black-box models. A.2 TRAINING-BASED CONFIDENCE CALIBRATION more recent stream of research investigates whether the internal representations of LLMs encode signals about factual correctness (Azaria & Mitchell, 2023; Su et al., 2024; Chen et al., 2024; Wang et al., 2024; Ni et al., 2025). Azaria & Mitchell (2023) showed that hidden states can reflect factuality judgments. Building on this, Su et al. (2024) and Chen et al. (2024) found that post-generation activations capture whether models own outputs are factual. More recently, Wang et al. (2024); Ni et al. (2025) demonstrated that pre-generation states already carry predictive cues, enabling estimation of correctness before the answer is fully produced. In parallel, some approaches explicitly train models to verbalize confidence reliably (Lin et al., 2022; Yang et al., 2023; Zhang et al., 2024), with Lin et al. (2022) being the first to introduce this idea. These methods typically evaluate models ability and then use answer correctness as supervision. Although some studies (Zhang et al., 2024; Tjandra et al., 2024) leverage the models internal uncertainty as supervision signal, they use it only to decide whether to abstain from answering, rather than to teach the model to express its own confidence. Moreover, these studies do not consider subsequent calibration and are limited to small-scale datasets. In contrast, this paper frames honesty alignment as two-stage learning problem: first, large-scale self-consistency confidence is used to activate the models ability to express internal confidence, and then small amount of supervised data is employed to calibrate this confidence."
        },
        {
            "title": "Preprint",
            "content": "Table 3: QA performance across all models and datasets. Models NQ TQ HQ 2Wiki Pararel Squad WQ CWQ MSQ PopQA Avg. Qwen-7B 41.33 Qwen-14B 51.91 51.88 Llama-8B 60.04 71.31 70.53 33.36 40.19 39. 31.53 34.06 29.71 49.93 60.43 61.47 32.17 39.00 33.91 58.02 64.67 66.04 34.47 38.28 36.89 12.74 16.55 16. 20.73 26.96 32.42 35.74 42.49 41."
        },
        {
            "title": "B FURTHER ANALYSIS USING ECE",
            "content": "Figure 8: AUROC of EliCal and Cal-Only with different amounts of annotated data. Figure 9: Alignment of EliCal and Cal-Only with different amounts of annotated data. In addition to evaluating whether the models confidence can distinguish between questions it can and cannot answer correctly, we also hope that the confidence values themselves are meaningful, i.e., that confidence reflects accuracy. This is also why we do not consider entropy-based methods, since the value of entropy itself does not represent confidence score between 0 and 1, and its value does not provide direct characterization of ability. We use ECE (Expected Calibration Error) to measure this, which can be formulated as: ECE = (cid:88) m= Bm acc(Bm) conf(Bm) , (12) where the predictions are partitioned into = 10 bins according to their confidence scores, Bm denotes the set of samples in the m-th bin, and is the total number of samples. For each bin, acc(Bm) is the empirical accuracy of the predictions and conf(Bm) is their average confidence. The absolute difference acc(Bm) conf(Bm) quantifies the miscalibration in that bin, and the overall ECE is obtained as the sample-sizeweighted average across bins. As shown in Figure 11, EliCal and Cal-Only achieve similarly low ECE in most cases, indicating that both learn calibrated"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Alignment of EliCal and Cal-Only with different amounts of annotated data. Both methods just train linear head. Figure 11: ECE of EliCal and Cal-Only with different amounts of annotated data. confidence overall. However, when labeled data is limited (Figure 5), Cal-Only shows worse AU-"
        },
        {
            "title": "Preprint",
            "content": "Table 4: AUROC performance of different methods across all models and datasets. Bold denotes the best scores across each model. The second-best value is underlined. Models Methods In-Domain Evaluation OOD Evaluation NQ TQ HQ 2Wiki Pararel Avg. Squad WQ CWQ MSQ PopQA Avg. Qwen-7B Qwen-14B Llama-8B Prob N-Prob Verbal-0 Verbal-10 Consis-Lex Consis-Sem Eli-Only Cal-Only (1k) EliCal (1k) 56.79 66.11 64.02 68.82 65.02 80. 77.86 72.19 82.38 70.26 72.96 70.22 62.35 74.98 90.20 86.23 68.75 87.51 54.29 61.96 66.49 70.53 68.98 80.12 77.27 74.34 84.48 Training-free Methods 41.73 59.33 65.02 73.24 67.82 55.40 58.71 61.67 70.81 71.50 66.35 62.93 55.48 64.75 67.22 68.90 69.80 73.62 56.63 60.72 65.76 72.54 62.12 66.16 Training-based Methods 54.36 76.17 82. 62.05 78.61 84.31 71.19 73.41 84.36 60.66 71.59 78.48 61.30 66.06 70.41 68.20 65.43 76.26 76.61 71.48 80.11 68.34 70.51 59.56 63.25 72.59 77. 74.77 69.33 79.85 61.85 65.93 60.54 64.44 61.07 70.76 66.56 66.96 78.09 71.30 74.73 70.64 73.40 77.07 70.44 74.60 86.13 91.74 64.94 68.58 67.12 71.05 69.87 70. 69.66 77.32 84.47 Cal-Only (560k) EliCal (560k) 84.89 85.16 88.96 89.09 85.64 86.09 83.97 84. Upper Bound 86.20 88.07 86.49 88.89 81.19 81.04 81.30 81.10 80.45 81.02 79.58 80.68 92.11 92. 85.75 85.83 Prob N-Prob Verbal-0 Verbal-10 Consis-Lex Consis-Sem Eli-Only Cal-Only (1k) EliCal (1k) 61.44 65.83 65.33 65.70 68.65 77.77 76.92 69.62 80.46 77.66 78.62 74.89 73.04 80.88 88. 84.95 72.45 85.85 67.33 70.55 70.87 70.00 75.43 81.12 76.61 75.35 83.48 Training-free Methods 46.07 58.89 71.44 75.61 66.91 57.02 63.02 67.63 72.11 71.35 69.38 66. 62.46 68.41 71.83 72.46 73.11 73.92 60.72 65.04 73.21 72.21 65.86 66.37 Training-based Methods 56.00 76.77 81.89 65.59 76.50 82.54 71.42 74.50 83. 60.67 70.51 78.96 64.07 65.78 68.91 72.18 65.83 73.17 73.33 66.93 76.07 73.47 74.62 61.61 63.52 75.56 74.60 72.38 69.99 78.49 66.21 68.41 63.55 64.74 67.90 73. 62.86 65.77 75.04 74.11 79.39 76.44 75.84 78.30 73.33 74.90 85.31 88.95 68.52 72.60 72.39 72.30 72.46 71.19 69.06 76.32 82.79 Cal-Only (560k) EliCal (560k) 83.95 84.57 88.30 88.66 85.66 85.71 83.57 83.97 Upper Bound 85.80 88.24 86.08 87.89 81.56 81. 80.07 79.86 80.90 81.46 79.63 80.68 90.32 90.35 85.06 85.33 Prob N-Prob Verbal-0 Verbal-10 Consis-Lex Consis-Sem Eli-Only Cal-Only (1k) EliCal (1k) 55.53 64.25 61.72 56.08 65.32 80.50 74.21 68.48 74.86 65.89 69.31 67.67 50.44 70.14 90.43 84.96 74.67 85.08 58.79 66.41 58.19 62.54 66.92 83. 78.07 75.88 81.65 Training-free Methods 45.87 56.76 58.66 58.79 57.88 61.10 59.46 64.86 64.12 62.13 67.19 77.84 56.36 63.75 61.98 57.03 64.75 77.43 54.30 61.80 65.74 63.26 64.40 74. Training-based Methods 55.66 77.12 74.80 74.16 71.32 78.90 72.01 74.86 79.54 70.74 72.22 75.59 59.84 62.05 64.50 63.01 62.37 74. 73.51 68.18 75.09 65.21 64.05 58.77 59.10 68.19 79.60 74.31 66.55 75.22 60.02 66.82 55.37 58.84 69.04 75.95 66.80 64.55 70.40 70.38 73.17 71.96 74.42 75.59 80. 79.69 81.57 85.67 63.23 67.37 66.86 67.33 69.89 77.52 74.90 74.86 79.52 Cal-Only (560k) EliCal (560k) 78.97 79.22 86.01 85. 83.62 83.26 81.80 81.94 Upper Bound 83.28 83.57 83.28 84.67 78.36 78.14 75.99 76.19 76.39 75. 75.04 74.16 86.89 86.70 81.47 81.12 ROC, suggesting that it captures only global trends (e.g., confidence close to average accuracy) but lacks fine-grained discriminative ability."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "We use Qwen2.5-32B-Instruct to measure consistency between two responses (See Figure 12). Yang et al. (2023) show that full-parameter fine-tuning for honesty alignment can negatively impact the models QA performance. To avoid affecting the models original capabilities, we train with LoRA (Hu et al., 2022) and linear head to output confidence score, where we set rank=8 and α=16. We use AdamW (Loshchilov & Hutter, 2017) as the optimizer, MSE (Mean Square Error) as the loss function and conduct training with the SFTTrainer from trl1, using batch size of 16 and accumulation steps of 8. For answer generation, we use vLLM2. For each question, we generate one greedy answer with temperature = 0, and sample 20 answers with temperature = 1, top-p = 0.95, and top-k = 50. Checkpoints for all training methods are selected using the in-domain test set. All other parameters are kept at their default settings. All the prompts can be seen in G. 1https://huggingface.co/docs/trl/sft trainer 2https://docs.vllm.ai/en/latest/"
        },
        {
            "title": "D DETAILS OF LORA",
            "content": "Consider an LLM with transformer layers and hidden dimension d. For an input question = (q1, . . . , qT ) where is the count of tokens in q, let h(ℓ) Rd denote the hidden state of token xt at layer ℓ {1, . . . , L}. Each layer contains multiple linear transformations, including the attention projections = WQh, = WKh, = WV h, = WOz, and the feed-forward projections: = Winh, = Woutσ(u), where WQ, WK, WV , WO Rdd and Win Rdffd, Wout Rddff. For any linear transformation = h, we apply low-rank trainable update : = + = + α AB, (13) (14) (15) where Rdinr, Rrdout, min(din, dout) is the LoRA rank, and α is scaling factor. Only and are trainable, while remains frozen. We denote all LoRA parameters across the layers as θLoRA. Figure 12: An example prompt for judging whether two responses are semantically consistent."
        },
        {
            "title": "E DETAILS OF BASELINES",
            "content": "In this section, we describe how each training-free baseline method is implemented. For the question q, suppose the greedy answer generated by the model is and the set of sampled answers is ˆR. ˆR contains 20 responses in our paper. Using the token generation probabilities of the model to represent confidence is common approach (Guo et al., 2017; Desai & Durrett, 2020; Jiang et al., 2021; Ni et al., 2024b); in this work, we implement two versions. Prob It computes the confidence Confidence(q) as the product of the generation probabilities of each token in the greedy answer: (cid:32) (cid:88) (cid:33) log pπ θ (rt q, r<t) , (16) Confidence(q) = exp where is the count of tokens in r. t="
        },
        {
            "title": "Preprint",
            "content": "Figure 13: An example prompt for asking the model to generate confidence in words. Figure 14: An example prompt for asking the model to generate confidence with 10 examples. N-Prob Since Prob decreases as the sequence length increases, N-Prob normalizes Prob by sequence length to eliminate the effect of output length: (cid:32) = exp 1 T (cid:88) t=1 (cid:33) log pπ θ (rt q, r<t) . (17) With the development of LLMs, models have been found capable of expressing their confidence in natural language. We implement both zero-shot and few-shot versions. Verbal-0 in natural language; the prompt is shown in Figure 13. asks the model to express its find-grained confidence in answering question correctly Verbal-10 Unlike Verbal-0, Verbal-10 includes 10 examples in the prompt. Since some datasets lack corresponding training sets, we randomly select 10 examples from the test set of each dataset to construct the prompt. The same 10 examples are used for all questions in given test set. As each dataset contains several thousand questions, selecting 10 has minimal impact on the results. The prompt can be seen in Figure 14. Consis-Lex The greedy answer is compared with 20 sampled responses in ˆR by computing the ROUGE score for each pair, and the average score is taken as the models confidence. ROUGE-L score is computed as: Given candidate answer with length and reference answer with length R, let LCS(C, R) denote the length of their longest common subsequence. The precision"
        },
        {
            "title": "Preprint",
            "content": "Figure 15: An example QA prompt. For this question, the correct answer is Wilhelm Conrad Rontgen. Figure 16: An example prompt for judging whether generated answer is correct. , recall R, and F1 score F1 of ROUGE-L are defined as: LCS(C, R) LCS(C, R) , = = , F1 = 2 + . (18) Consis-Sem Unlike Consis-Lex, where similarity between two responses is measured using ROUGE-L, here it is evaluated with Qwen2.5-32B-Instruct, which captures consistency more from semantic perspective. Using LLMs to measure semantic similarity is widely adopted and empirically validated approach (Achiam et al., 2023; Kuhn et al., 2023). The similarity between each pair of responses is binary (0 or 1), and the model score is obtained by averaging the similarities between the greedy answer and the 20 sampled answers."
        },
        {
            "title": "F THE USE OF LARGE LANGUAGE MODELS",
            "content": "We used LLMs for grammar correction, polishing sentences, and assisting with some repetitive plotting code. The content and experiments in the paper were entirely conducted by humans, and all model-polished text was manually reviewed."
        },
        {
            "title": "G PROMPTS",
            "content": "In this section, we show all the prompts used in this paper. They are shown in Figure ??, Figure 14, Figure 15, Figure 16, and Figure 12."
        }
    ],
    "affiliations": [
        "State Key Laboratory of AI Safety, Institute of Computing Technology, CAS",
        "University of Chinese Academy of Sciences"
    ]
}