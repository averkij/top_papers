{
    "paper_title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
    "authors": [
        "Eldar Kurtic",
        "Alexandre Marques",
        "Shubhra Pandit",
        "Mark Kurtz",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the \"best\" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous \"continuous batching\" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 5 3 2 0 . 1 1 4 2 : r GIVE ME BF16 OR GIVE ME DEATH? ACCURACY-PERFORMANCE TRADE-OFFS IN LLM QUANTIZATION Eldar Kurtic 1 2 Alexandre Marques 1 Shubhra Pandit 1 Mark Kurtz 1 Dan Alistarh 1 2 1Neural Magic 2Institute of Science and Technology Austria ABSTRACT Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the best format for given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous continuous batching deployment of midand large-size models on high-end GPUs. Our results provide set of practical guidelines for deploying quantized LLMs across scales and performance requirements."
        },
        {
            "title": "INTRODUCTION",
            "content": "The massive computational costs of serving large language models have led to significant amount of work on inference acceleration techniques, such as quantization (Frantar et al., 2022; Dettmers & Zettlemoyer, 2022; Lin et al., 2024a), speculative decoding (Chen et al., 2023; Leviathan et al., 2023), or model pruning (Xia et al., 2023; Muralidharan et al., 2024). Model quantization, i.e., reducing the bitwidth of weights, activations, or both, is arguably the standard approach to reducing memory and computational costs at deployment time. The critical trade-off in LLM quantization is between the speed/memory benefits and the accuracy of the resulting compressed model. Despite extensive research on quantization, few studies provide systematic benchmarks or practical guidelines on expected performance at different compression levels. This state of affairs is expressed in our fore-title, which uses direct quote from the wave of user feedback regarding the quantized Llama3.1-405B model (Dubey et al., 2024), initially suspected to drop significant accuracy (Zhang, 2024; Karpathy, 2024) and later shown to be near-lossless in LMSYS Arena user testing (Chiang et al., 2024). This uncertainty can be sigCorrespondence to: <firstname>@neuralmagic.com. nificant barrier to adoption of quantized formats. In this paper, we systematically consider the following question: What are the practical accuracy-performance trade-offs for popular quantization formats? Due to the broad range of existing quantization techniques, we focus our study on formats with efficient computational support across different batching levels and low accuracy loss. This reduces the range of formats to be examined to 8-bit weights and activations (W8A8), in integer (INT) precision for NVIDIA Ampere and older devices, and floatingpoint (FP) precision for NVIDIA Hopper and Ada Lovelace, as well as 4-bit integer weights with 16-bit activations (W4A16-INT). These formats are efficiently supported in the vLLM inference engine (Kwon et al., 2023), which we use for performance experiments through the GuideLLM framework (Neural Magic, 2024). To examine accuracy, we implemented broad, automated evaluation suite, including leading academic benchmarks, namely the Open LLM Leaderboard V1 (Beeching et al., 2023), and its recent, more challenging, V2 version (Fourrier et al., 2024). We complement this examination with set of real-world benchmarks, which focus Performance-Accuracy Trade-Offs in LLM Quantization on generative quality in an open-ended setup, specifically Arena-Hard-Auto-v0.1 (Li et al., 2024b), HumanEval (Chen et al., 2021), and HumanEval+ (Liu et al., 2023a). We further extend these benchmarks with an in-depth text similarity analysis between outputs from uncompressed and quantized models. Finally, we examine the performance implications of deploying quantized LLMs. For this, we benchmark across all quantization formats in vLLM (version 0.6.1.post2) across three GPU types (A6000, A100, H100) in six latency-sensitive use cases and two throughput use cases. Our main findings are as follows: 1. W8A8-FP quantization is essentially lossless: Across all benchmarks, it preserves the uncompressed models average accuracy or falls within the evaluations margin of error. Remarkably, we achieve this via simple and robust approach: dynamic per-token activation quantization and symmetric weight quantization via round-to-nearest assignment. 2. W8A8-INT quantization shows tolerable accuracy loss, in the 1-3% per-task average range. This is achieved using dynamic activation quantization or the more complex SmoothQuant technique (Xiao et al., 2022); for weights, we use GPTQ (Frantar et al., 2022) with symmetric quantization. Our findings stand in contrast with prior work (Li et al., 2024a; Lee et al., 2024b), which reported large accuracy drops (more than 10% on average) in the same setting. 3. W4A16-INT quantization maintains consistently low accuracy loss, being competitive in terms of accuracy to W8A8-INT. Our findings are obtained for simple variant of GPTQ, which we surprisingly find to be more accurate on real-world tasks than the more recent AWQ method (Lin et al., 2024a). 4. On the text analysis side, we show that, in autoregressive text generation, larger quantized models tend to closely follow the original word choices and sentence structures of the baseline. At the same time, on smaller models, quantization introduces moderate structure variability relative to full precision, but still preserves semantic meaning. 5. For quantized inference performance, we show that W4A16-INT offers favorable performance in synchronous deployments. For asynchronous deployments, W4A16 is preferable for small models on mid-tier GPUs, and W8A8 formats are more effective for mid-to-large-size models on high-end GPUs. The most cost-effective quantization scheme depends on model size, hardware, use case (e.g., code completion vs. multi-turn chat), and deployment scenario (synchronous vs. asynchronous). this work provides first Overall, in-depth study of accuracy-vs-performance-vs-cost trade-offs for quantized models across various formats, algorithms, use cases, and hardware types. We hope it can serve as guide for practitioners looking to deploy models efficiently, and as motivation to researchers interested in method improvements."
        },
        {
            "title": "2.1 A Primer on Quantization",
            "content": "In the following, we provide some background on posttraining quantization of LLM weights and activations. Early work on this topic has considered INT8 activation quantization and INT4/INT8 weight quantization (Dettmers et al., 2022; Yao et al., 2022; Park et al., 2022). standard approach for weight quantization has been direct round-tonearest (RTN) quantization over small groups: given group of consecutive weights, viewed as vector Rg, b-bit RTN is defined as: Q(x, b) = rnd (cid:32) min(x) max(x) min(x) (2b 1) (cid:33) = rnd((x z(x))/s(x)), (1) where rnd rounds to the nearest integer, = z(x) = min(x) is the zero point and = s(x) = (max(x) min(x))/(2b 1) is the scale, taken as min-max. This early work raised two key issues: RTN on weights loses significant accuracy for INT4 precision, and RTN quantization of activations even to higher INT8 precision can be lossy due to the large outlier features in LLMs (Dettmers et al., 2022). Weight quantization. Towards the first issue, GPTQ (Frantar et al., 2022) improved upon RTN for scalar, per-weight, quantization by allowing weights to be adjusted via secondorder updates computed over sample of calibration data. Follow-up scalar quantization methods such as AWQ (Lin et al., 2024a), SqueezeLLM (Kim et al., 2023), OWQ (Lee et al., 2024a), and SpQR (Dettmers et al., 2023) implemented variants of outlier-aware quantization, where small fraction of weights are effectively stored in higher precision. These methods can reach highly-accurate 4-bit quantization, but the latter three lack general kernel support. More recently, several high-compression methods have been proposed, such as QuIP (Chee et al., 2023), QuIP# (Tseng et al., 2024a), QTIP (Tseng et al., 2024b) AQLM (Egiazarian et al., 2024), and GPTVQ (van Baalen et al., 2024), targeting the 3-bit range or lower. These methods leverage much more complex representations, such as lattice/vector quantization, often paired with incoherence pre-processing of the weights. Despite high accuracy, these methods add overheads and lack efficient kernel support for batch sizes larger than 1, which is necessary for general deployments. Thus, our study does not currently consider these formats. Performance-Accuracy Trade-Offs in LLM Quantization Activation quantization. Quantizing both activations and weights to 8-bit or 4-bit precisions allows direct utilization of existing hardware support for lower-bit operations. However, activation matrices are difficult to quantize effectively (Dettmers et al., 2022) due to the presence of outlier features elements that can be up to 100 times larger than the average. Dettmers et al. (2022) extracted columns at runtime and quantized the rest to INT8, which is inefficient. SmoothQuant (Xiao et al., 2022) improves upon this by recognizing that outlier features are generally stable across the model and can be extracted utilizing calibration set and folded during compilation. Follow-ups to SmoothQuant investigate the possibility of W4A4 quantization (Ashkboos et al., 2023; 2024) or mixedprecision W4A8 (Lin et al., 2024b; Zhang et al., 2024), also considering quantizing the KV-cache. While promising, these approaches still lose significant accuracy and lack efficient support in high-performance inference frameworks. We, therefore, also leave their investigation for future work."
        },
        {
            "title": "2.2 Efficient Inference With Quantized LLMs",
            "content": "Several performant LLM inference frameworks exist, such as HuggingFace TGI (HuggingFace, 2024), NVIDIA TensorRT-LLM (NVIDIA, 2023), and vLLM (Kwon et al., 2023). We focus our benchmarking efforts on vLLM, popular open-source framework that addresses the specific challenges of serving large, autoregressive neural networks. The backend architecture of vLLM combines efficient execution management with high-performance computation graphs that utilize pre-compilation, optimized operator fusion, and performant scheduling. Additionally, vLLM incorporates computational acceleration through customized kernels leveraging NVIDIA CUTLASS and hand-written GPU kernels. At the same time, vLLM enables broad hardware compatibility, supporting NVIDIA and AMD GPUs, TPUs, and CPUs. Several optimization techniques, including FlashAttention (Dao et al., 2022), FlashInfer (FlashInfer, 2023), PagedAttention (Kwon et al., 2023), and Marlin (Frantar et al., 2024) are also supported. This support and its open-source nature have led to vLLMs adoption by industry and the open-source community. The framework integrates optimized CUTLASS kernels for W8A8 (FP and INT), and mixed-precision Marlin kernels for W4A16-INT."
        },
        {
            "title": "2.3 Accuracy Evaluations of Quantized LLMs",
            "content": "There is substantial amount of prior work on the evaluation of quantized LLMs, primarily focusing on accuracy tradeoffs under various quantization schemes (Yao et al., 2023; Liu et al., 2023b; Huang et al., 2024; Gong et al., 2024b; Li et al., 2024a; Gong et al., 2024a). However, much of this research has concentrated solely on academic benchmarks, which do not fully represent the real-world environments where LLMs are deployed. Furthermore, the fact that some of these works do not tune hyperparameters and algorithmic choices for the quantization algorithms can lead to incorrect conclusions regarding accuracy, as we demonstrate later in the experimental section. Specifically, we refer to the claim that 8-bit integer activation quantization causes significant accuracy drops (Li et al., 2024a; Lee et al., 2024b), which we contradict experimentally. The closest work to ours is by Lee et al. (2024b), which, like most prior studies, focuses solely on accuracy under quantization. However, we identify several points of divergence. First, although the authors claim to analyze models up to 405B in size, open-ended benchmarks are not included for this scale, and accuracy scores for the full-precision uncompressed model are missing even for academic benchmarks. Without baselines, it is difficult to draw meaningful conclusions on the impact of quantization at this scale. To address this, we enabled highly efficient multi-node evaluations for the 405B model, allowing comprehensive benchmarking in full precision across academic and real-world tasks. The second divergence point concerns their claims that AWQ outperforms GPTQ in 4-bit weight-only quantization setup. We attribute this finding to suboptimal hyperparameter choices in GPTQ quantization and provide detailed comparison of the methods in Table 1 and Appendix A.2. Our results show that both methods perform similarly on academic benchmarks, while GPTQ demonstrates notable improvements over AWQ on real-world benchmarks, especially in coding tasks. Third, we also challenge the prior conclusion that W8A8INT performs substantially worse than W8A8-FP and W4A16-INT. When tuned carefully, W8A8-INT proves competitive, with only minor accuracy losses. For instance, while Lee et al. (2024b) reports 10-point drop in accuracy for the W8A8-INT quantized 405B model on the Open LLM Leaderboard V2 benchmark relative to FP8 quantized one, our carefully optimized quantization parameters reduce this gap to just 0.7 points. Unlike previous work, we also explore how quantization affects text generation quality. This analysis provides deeper insights into the impacts of various quantization schemes on sentence structure, word choice, and semantic fidelity in text generated by quantized LLMs. Furthermore, we conclude our study by analyzing the performance impacts of deploying various quantization formats across range of scenarios that represent real-world applications. This examination provides insights into how each quantization approach performs under practical deployment conditions and hardware configurations. Performance-Accuracy Trade-Offs in LLM Quantization"
        },
        {
            "title": "3.1 Datasets and Benchmarks",
            "content": "Our accuracy evaluation suite is designed to encompass wide range of inference scenarios and use cases. It includes diverse set of tasks to ensure representative collection of topics relevant to practical deployments. Furthermore, the evaluation process is designed to be completely automated, as opposed to human-in-the-loop approaches, enabling streamlined experimentation. Overall, we group all of the benchmarks into three distinct categories. Academic benchmarks such as Open LLM Leaderboard V1 and V2 (Beeching et al., 2023; Fourrier et al., 2024), are essential for evaluating model improvements. They focus on structured tasks like question-answering and reasoning, providing consistent and reproducible scoring. However, they often lack alignment with real-world scenarios where semantics, variability, and context-awareness are important. The Open LLM Leaderboard V1 consists of diverse range of topics, including grade school math (GSM (Cobbe et al., 2021)), world knowledge and reasoning (MMLU (Hendrycks et al., 2020), ARC-Challenge (Clark et al., 2018)), language understanding (Winogrande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019)), and truthfulness (TruthfulQA (Lin et al., 2021)). The Open LLM Leaderboard V2 builds on this with topics such as expert knowledge and complex reasoning (MMLUPro (Wang et al., 2024), GPQA (Rein et al., 2023), Big Bench Hard (Suzgun et al., 2022)), multi-step reasoning (MuSR (Sprague et al., 2024)), advanced math problems (MATH Level 5 (Hendrycks et al., 2021)), and instruction following (IFEval (Zhou et al., 2023)). By evaluating with both Leaderboards, we cover broad spectrum of topics. We employ log-likelihood (multiplechoice) and text-generation evaluations, allowing for stress testing of quantized LLMs under diverse conditions. Real-world benchmarks, unlike academic benchmarks, test models in scenarios that mimic human usage, such as instruction following, chat, and code generation. These benchmarks include Arena-Hard-Auto-v0.1 (Li et al., 2024b; Chiang et al., 2024; Li et al., 2024c), HumanEval (Chen et al., 2021), and HumanEval+ (Liu et al., 2023a), which offer broader range of tasks with higher variation but better reflect real-world environments. The LMSYS Chatbot Arena (Chiang et al., 2024) has established itself as leading benchmark for LLMs, assessing how models align with human preferences. Arena-Hard-Auto-v0.1 is an automated extension of this benchmark, where an LLM judges responses to 500 complex prompts on various topics. It has demonstrated strong correlation with human evaluations, achieving state-of-the-art 89% agreement with human preference rankings (Li et al., 2024c). This enables reliable, fast, and automated evaluation of the models chat capabilities without requiring human. In addition to chat-based interactions, LLMs are widely deployed as coding assistants. To evaluate the performance of quantized models in code generation, we tested them on HumanEval and its more challenging variant, HumanEval+. These benchmarks measure models ability to generate correct and functional code based on programming problems, with HumanEval+ introducing more rigorous evaluation protocol. Text similarity analysis evaluates how closely the generated outputs of quantized models align with those of their unquantized counterparts. While real-world benchmarks reflect practical usage scenarios for LLMs, their inherently open-ended format introduces notable variance in results. To address this, we extend our evaluation suite to include an analysis of output similarity under identical prompt requests. This allows us to get insights into how quantized models text generation differs from that of full-precision models. For this investigation, we employ metrics such as ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and Semantic Textual Similarity (STS) to assess both semantic and structural consistency, ensuring that the intended meaning and quality of generated text are preserved under quantization. ROUGE-1 measures word-level (unigram) overlap between outputs from quantized and unquantized models, while ROUGE-L captures structural similarity by focusing on the longest common subsequence. BERTScore evaluates tokenlevel contextual similarity, computed using the RoBERTalarge models contextual embeddings. STS assesses overall semantic similarity at the sentence level, using the Sentence Transformers framework (Reimers & Gurevych, 2019) built on the 6-layer MiniLM architecture (Wang et al., 2020). With this extensive evaluation framework, we ensured that deployment scenarios ranging from structured, researchdriven tasks to open-ended, real-world applications were covered, providing holistic view of the performance and capabilities of quantized LLMs."
        },
        {
            "title": "3.2 Models, Formats, and Quantization Algorithms",
            "content": "We focus our evaluation on extensive testing of the recent Llama 3.1 series of models (Dubey et al., 2024), which have gained significant traction in research and deployment contexts. To obtain comprehensive understanding of tradeoffs, we conduct all of our experiments on the instructiontuned versions of all three available Llama 3.1 sizes (8B, 70B, and 405B), and for each of these, we focus on investigating three popular quantization formats with efficient inference support in vLLM: W8A8-FP, W8A8-INT, and W4A16-INT. Performance-Accuracy Trade-Offs in LLM Quantization Table 1. Comparison of GPTQ and AWQ 4-bit weight quantization algorithms (W4A16-INT). We observe small gap between methods on academic benchmarks (left) but more pronounced difference in favor of GPTQ on real-world (open-ended) benchmarks (right). Academic Benchmarks Real-World Benchmarks Model Average Score Leaderboard V1 Leaderboard V2 Average Score Arena-Hard HumanEval MBPP Llama-3.1-8B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) Llama-3.1-70B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) 50.84 49.82 50.05 62.93 62.18 62.53 74.06 73.11 72. 84.20 83.77 83.96 27.62 26.53 27.40 41.66 40.58 41.09 53.7 52.3 49.4 73.1 73.1 72.3 25.8 24.0 22. 57.0 57.0 56.7 67.3 67.1 63.0 79.7 80.5 79.4 68.1 65.8 62.8 82.5 81.9 80.8 W8A8-FP This scheme uses an 8-bit floating point format for weights and activations of all linear operators within transformer blocks. We compress models to this format using simple round-to-nearest algorithm. Weight quantization follows symmetric per-output-channel scheme, while activations are dynamically quantized per token. Due to the algorithms simplicity, this format requires no calibration data and is efficient even for massive models. W8A8-INT This scheme reduces the weights and activations of all linear operators within transformer blocks to 8bit integer values. We apply symmetric per-output-channel GPTQ quantization for weight compression, and for activations, we use dynamic per-token quantization. We found that this scheme performs consistently well across 8B and 405B models, though it results in noticeable accuracy drops for the 70B model. To address this, we employ SmoothQuant, which redistributes the complexity of activation quantization onto the generally easier-to-quantize weights. For calibration data, we found that random tokens are adequate for the 8B scale; however, for larger models, we found that using the high-quality OpenPlatypus dataset (Lee et al., 2023) is crucial for consistent accuracy recovery of competitive scores on academic benchmarks. W4A16-INT In this scheme, the weights of all linear operators within transformer blocks are quantized to 4-bit integers, while activations are retained at 16-bit precision. Weights are compressed using GPTQ quantization applied to groups of 128 consecutive elements with MSE-optimal clipping factors. Due to higher compression, using random tokens proved detrimental to accuracy; therefore, we employ calibration data sampled from the OpenPlatypus dataset. INT4 Quantization Algorithms. As shown by prior work (Dettmers et al., 2022; Lin et al., 2024a; Xiao et al., 2022), round-to-nearest performs poorly for integer weight quantization. Therefore, we focus on two very popular techniques with efficient inference support in vLLM: AWQ and GPTQ. We examine their performance on several benchmarks: Leaderboard V1 and V2, Arena-Hard, HumanEval, and MBPP. We report results in Table 1, and provide detailed per-task breakdown of scores in Appendix Tables 12, 13, and 8. We observe that, on average, on academic benchmarks, the methods are essentially tied. The advantage of AWQ, on scale of 0100, is only 0.23 and 0.35, respectively. However, we found that, on real-world benchmarks, GPTQ consistently outperforms AWQ by larger margins of 2.9 and 0.8, respectively. Consequently, we adopt GPTQ as our primary method for weight quantization. This conclusion is slightly at odds with prior work (Lin et al., 2024a; Huang et al., 2024) observing results that are either in favor of AWQ, or tied across sub-sample of academic benchmarks. This is explained as follows: (1) contrary to this prior work, we use the version of GPTQ with MSE-optimal clipping factors (as opposed to min-max optimal), which has no overhead and performs consistently better; (2) we use different, higher-quality calibration data; (3) we evaluate across real-world benchmarks."
        },
        {
            "title": "4 QUANTIZATION VERSUS ACCURACY",
            "content": "We begin our discussion by examining the accuracy of quantized models across Leaderboard V1  (Table 2)  , Leaderboard V2  (Table 3)  and real-world benchmarks  (Table 3)  . Given the density of the results, we examine them individually via average recoveries across higher-level benchmarks and discuss outlier observations."
        },
        {
            "title": "4.1 Academic Benchmarks",
            "content": "Academic benchmarks are the standard approach for evaluating LLM reasoning capabilities. They provide structured tasks with well-defined evaluation criteria. Our evaluations focus on the Open LLM Leaderboard V1 and V2 benchmarks. Testing on both allowed us to prevent overfitting to V1, where we optimized our quantization hyperparameters. The Open LLM Leaderboard V1 was evaluated by following Metas prompts for Llama-3.1 models to align with the expected baseline scores. This approach introduces two notable differences from the standard protocols: first, MMLU and ARC-Challenge are evaluated as text-generation tasks where models generate the correct answer as text, as opposed to the default log-likelihood-based protocol (Gao et al., 2021). Second, GSM8k is evaluated using chain-ofthought rather than the default few-shot format. Performance-Accuracy Trade-Offs in LLM Quantization Table 2. Detailed per-task breakdown of accuracy on subset of academic benchmarks (Open LLM Leaderboard V1) for quantized Llama-3.1-Instruct models across all three model sizes (8B, 70B, 405B). Higher score is better. Recovery % Average Score MMLU 5-shot MMLU CoT 0-shot ARC-C 0-shot GSM8k CoT 8-shot HellaSwag 10-shot Winogrande 5-shot TruthfulQA 0-shot 8B 70B 405B BF16 W8A8-FP W8A8-INT W4A16-INT BF16 W8A8-FP W8A8-INT W4A16-INT BF16 W8A8-FP W8A8-INT W4A16-INT 100.00 99.31 100.31 98.72 100.00 99.72 99.87 99.53 100.00 100.12 99.32 99.98 74.06 73.55 74.29 73.11 84.40 84.16 84.29 84. 86.79 86.89 86.20 86.78 68.3 68.0 67.8 66.9 83.8 83.8 83.7 83.6 87.4 87.5 87.1 87.2 72.8 71.6 72.2 71.1 86.0 85.5 85.8 85. 88.1 88.1 87.7 87.7 81.4 81.2 81.7 80.2 93.3 93.5 93.1 92.8 95.0 95.0 94.4 95.3 82.8 82.0 84.8 82.9 94.9 94.5 94.2 94. 96.0 95.8 95.5 96.3 80.5 80.0 80.3 79.9 86.8 86.6 86.7 86.3 88.5 88.5 88.2 88.3 78.1 77.7 78.5 78.0 85.3 84.6 85.1 85. 87.2 88.0 86.1 87.4 54.5 54.3 54.7 52.8 60.7 60.6 61.4 59.8 65.3 65.3 64.4 65.3 Table 3. Detailed per-task breakdown of accuracy on subset of academic (Open LLM Leaderboard V2) and on real-world (Arena-Hard, HumanEval) benchmarks for quantized Llama-3.1-Instruct models across all three model sizes (8B, 70B, 405B). higher score is better. Academic Benchmarks (Open LLM Leaderboard V2) Real-World Benchmarks Recovery %"
        },
        {
            "title": "Average\nScore",
            "content": "IFEval 0-shot BBH 3-shot Math lvl 5 4-shot GPQA 0-shot MuSR 0-shot MMLU-Pro 5-shot Arena-Hard Win-Rate HumanEval pass@1 HumanEval+ pass@1 8B 70B 405B BF16 W8A8-FP W8A8-INT W4A16-INT BF16 W8A8-FP W8A8-INT W4A16-INT BF16 W8A8-FP W8A8-INT W4A16-INT 100.00 101.23 101.52 96.05 100.00 100.00 97.29 97.41 100.00 99.86 98.36 98. 27.62 27.96 28.04 26.53 41.66 41.66 40.53 40.58 48.73 48.66 47.93 48.24 77.86 30.09 77.16 29.65 77.95 30.96 76.30 28.91 86.41 55.79 87.57 54.92 86.56 55.17 85.74 55.01 87.69 67.02 86.81 67.10 86.99 66.73 88.03 67. 15.68 16.53 15.51 14.80 26.07 28.03 23.94 24.38 38.96 38.84 35.82 37.61 3.68 5.74 5.38 4.04 7.61 7.52 7.61 6.33 15.40 18.16 14.58 17.16 13.61 16.82 13.85 17. 19.50 19.48 18.99 20.83 20.38 19.24 17.52 19.39 30.77 31.18 30.85 28.81 48.12 47.70 47.09 47.25 59.74 59.40 58.43 59.35 25.8 26.8 27.2 24.0 57.0 57.7 57.0 56. 67.4 66.9 64.6 66.5 67.3 67.3 67.1 67.1 79.7 80.0 78.7 80.5 86.8 87.0 86.9 85.1 60.7 61.3 60.0 59.1 74.8 75.0 74.0 74. 80.1 81.0 80.4 78.9 As shown in Figure 1 and Table 2, all quantization schemes, regardless of model size, recover around 99% of the average score achieved by the unquantized BF16 baseline. Appendix Table 10 provides per-task accuracy recoveries, indicating that the lowest recovery is 96.88% for the W4A16-INT 8B model on the TruthfulQA task. For larger models, the lowest recovery also occurs on TruthfulQA, reaching approximately 98.5%. We observe that the average accuracy recovery across 8-bit quantization schemes is approximately 99.75%, indicating negligible impact on model accuracy. Similarly, the W4A16-INT scheme achieves highly competitive average accuracy recovery of 99.36%. The Open LLM Leaderboard V2 was developed as scores on Leaderboard V1 reached plateau. This benchmark incorporates more challenging tasks designed to assess advanced reasoning capabilities. Unlike V1, Leaderboard V2 normalizes scores by subtracting the random accuracy baseFigure 1. Average score of quantized Llama-3.1-Instruct models on the Open LLM Leaderboard V1 benchmark. line and rescaling the adjusted score to 0-100 range. This normalization provides more robust method for calculating the aggregate average score by equally weighting each task, independent of its random prediction baseline. Performance-Accuracy Trade-Offs in LLM Quantization As illustrated in Figure 2 and Table 3, quantized models recover close to 99% of the baselines average score, with all models maintaining at least 96% recovery. However, the increased difficulty of these tasks, especially for smaller models, resulted in higher variance for benchmarks like GPQA and MuSR, where scores approached the random guessing threshold even for the full-precision baseline. This led to more variability and lack of clear signal for accuracy recovery, as seen in Appendix Table 11. Focusing on tasks where the full-precision model scores above 40%, representing cases where the model performs significantly better than random guessing, we observe that the lowest per-task accuracy recovery for 8-bit FP quantized models is 98.44% at the 70B scale on the Big Bench Hard (BBH) dataset. For 8-bit INT models, the lowest recovery is 97.8% at the 405B scale on the MMLU-Pro task. Notably, in this benchmarking setup, W4A16-INT models exhibit higher accuracy recovery than W8A8-INT models, with the lowest recovery being 98% for the 8B model on the IFEval task. Figure 3. Average score of quantized Llama-3.1-Instruct models on the Arena-Hard-Auto-v0.1. Figures 4 and 5 present the pass@1 scores obtained using the EvalPlus library (Liu et al., 2023a). As illustrated, quantized models demonstrate exceptional performance on both HumanEval and HumanEval+, with 8-bit models achieving 99.9% accuracy recovery and 4-bit models recovering 98.9%. These results highlight that quantized models maintain high performance in both simpler and more complex coding tasks, proving their reliability for real-world applications. In Appendix A.1, we report results for pass@10 metric, which show similar trends to the pass@1 metric. Figure 2. Average score of quantized Llama-3.1-Instruct models on the Open LLM Leaderboard V2 benchmark."
        },
        {
            "title": "4.2 Real-World Benchmarks",
            "content": "While academic benchmarks provide structured evaluations, real-world open-ended benchmarks better represent how models perform in dynamic environments. These benchmarks test models on varied prompts with longer generations and multiple potential solutions, focusing on responses correctness and semantic quality. Our evaluations targeted three key real-world benchmarks: Arena-Hard-Auto-v0.1, HumanEval, and HumanEval+, which measure performance in chat, instruction-following, and code generation. Table 3 summarizes results on these benchmarks. Figure 3 focuses on the Arena-Hard-Auto-v0.1 benchmark, averaging results from two evaluation runs per model and quantization scheme. The results illustrate that the response quality of quantized models remains highly competitive with their unquantized counterparts. As shown in the detailed results in Appendix Table 7, the 95% confidence intervals overlap for all model sizes and quantization schemes, highlighting the minimal impact of quantization on accuracy. Figure 4. HumanEval pass@1 scores for quantized Llama-3.1Instruct models. Figure 5. HumanEval+ pass@1 scores for quantized Llama-3.1Instruct models. Performance-Accuracy Trade-Offs in LLM Quantization Figure 6. Text similarity metrics comparing the outputs of quantized Llama-3.1-Instruct models to full-precision baselines. We refer to W8A8-FP as FP8, W8A8-INT as INT8, and W4A16-INT as INT4."
        },
        {
            "title": "4.3 Text Similarity Investigation",
            "content": "Following our investigations on various structured and openended tasks, we focus on examining the similarity of generated text between quantized models and their unquantized counterparts. As described before, we employ four metrics: ROUGE-1, ROUGE-L, BERTScore, and Semantic Textual Similarity (STS). All of these metrics were computed across responses generated to the Arena-Hard-Autov0.1 prompts with greedy sampling technique, allowing us to analyze how well-quantized models preserve the meaning and structure of outputs compared to full-precision models on challenging prompts while ensuring full reproducibility. For visualization purposes, we normalize STS score to 0-1 range. Results in Figure 6 show that large quantized models (70B and 405B) maintain high degree of similarity to their full-precision counterparts, with an average ROUGE-1 score of 0.7 and ROUGE-L score of 0.56, indicating strong preservation of word choice and structure. Additionally, an average BERTScore of 0.93 and STS score of 0.96 confirm that the overall meaning is consistent, despite minor token variations from quantization. While the 8B models exhibit slightly more variability in word choice, reflected in lower ROUGE-1 and ROUGE-L scores of 0.62 and 0.46, respectively, they still preserve core semantic meaning, as evidenced by the high BERTScore of 0.92 and STS score of 0.95. These findings demonstrate that quantized models produce high-quality outputs across all analyzed model sizes and schemes."
        },
        {
            "title": "PERFORMANCE",
            "content": "LLM inference occurs in two main stages: prefill, where all tokens in the input prompt are processed simultaneously, and decode, where new tokens are generated one at time. For most input prompts, prefill is compute-bound, while decode tends to be memory-bound. Weight quantization primarily enhances performance in the decode stage by reducing memory load times. Conversely, quantizing both weights and activations allows matrix multiplications to be executed at lower bitwidth, boosting computational speed and reducing bottlenecks during prefill. Therefore, the optimal quantization scheme for inference depends on the distribution of prefill versus decode tokens. In addition, quantization also offers end-to-end performance benefits by enabling more simultaneous queries, which improves efficiency, and by facilitating the use of lower-cost GPUs for tasks with tighter memory constraints. These factors interact during real-world LLM deployment, making the choice of optimal quantization format complex. In this study, we benchmarked three quantization schemes (W8A8-FP, W8A8-INT, and W4A16-INT) across three GPU types (A6000, A100, H100) in six latencysensitive use cases and two throughput use cases, as listed in Table 4. We considered both synchronous deployment and asynchronous deployment with latency constraints for latency-sensitive use cases, while throughput use cases were evaluated with asynchronous deployment only. We release the inference-benchmarking data as HuggingFace dataset. Furthermore, to examine the cost-efficiency of deployment in different hardware, we use Lambda Labs on-demand GPU pricing (Lambda Labs, 2024), which are representative of industry standards. The prices are reported in Table 9. Table 4. Use cases for benchmarking the performance of quantized models. For latency-sensitive use cases, we impose the following constraints: TTFT < 1s and ITL < 0.1s (TTFT = time to first token, ITL = inter-token latency). Latency-Sensitive Use Cases"
        },
        {
            "title": "Prefill Tokens Decode Tokens",
            "content": "Code Completion Docstring Generation Code Fixing RAG Instruction Following Multi-Turn Chat 256 768 1024 1024"
        },
        {
            "title": "Small Summarization\nLarge Summarization",
            "content": "1024 4096 1024 128 1024 128 128 256 128 512 Performance-Accuracy Trade-Offs in LLM Quantization Table 5. Synchronous inference performance comparison across model sizes and GPU configurations. Results show latency (in seconds) and cost-efficiency (Queries per USD) for various tasks. We refer to W8A8-FP as FP8, W8A8-INT as INT8, and W4A16-INT as INT4. Format CR"
        },
        {
            "title": "Instruction\nFollowing",
            "content": "Multi-Turn Chat Lat. Q/$ Lat. Q/$ Lat. Q/$ Lat. Q/$ Lat. Q/$ Lat. Q/$"
        },
        {
            "title": "Model GPU",
            "content": "8B A6000 A6000 70B A100 A100 405B # 1 1 1 4 2 2 2 1 2 1 1 BF16 INT8 INT4 BF16 INT8 INT4 BF16 INT8 INT4 BF16 FP8 INT"
        },
        {
            "title": "16 BF16\nINT8\n8\nINT4\n4",
            "content": "1.34 2.09 1.84 3.09 1.73 2.75 1.89 2.51 3.35 7.28 7.7 6.0 3. 8.7 14.0 8.2 7.9 11.5 6.3 4.9 5.2 4.5 22.8 14.0 13.1 584 746 1,201 129 160 128 175 319 115 212 241 6 18 38 3.2 2.2 1.6 8.7 8.3 5.2 6.3 6.6 4. 3.8 4.0 3.1 11.1 6.7 6.9 1,403 2,011 2,896 129 272 429 161 306 428 147 272 11 37 72 7.2 8.1 4.8 12.8 13.4 7.6 11.2 11.9 6.8 7.0 7.0 4.5 26.6 15.7 11. 629 554 933 88 167 296 90 170 297 81 156 244 5 16 44 3.3 2.3 1. 8.0 8.3 5.8 6.1 7.0 5.1 3.9 4.0 3.4 12.2 6.8 7.3 6.7 4.2 4.8 1,345 1,955 2, 141 270 390 164 290 396 144 272 325 10 37 69 11 36 61 3.3 1.9 1. 6.5 7.4 4.5 5.3 6.4 3.9 3.4 3.5 2.7 10.1 6.4 6.2 6.5 4.1 4.0 1,383 2,327 3, 172 303 502 190 312 510 165 317 411 12 39 81 12 37 72 5.7 4.3 2. 13.6 12.7 6.7 9.7 10.8 6.6 5.7 5.9 4.0 20.5 12.0 10.7 12.2 7.5 7.0 792 1,058 1, 83 177 334 104 186 306 99 185 271 6 21 47 6 20 42 H"
        },
        {
            "title": "16 BF16\n8\n4",
            "content": "3.45 6.75 CR: Cost Reduction factor compared to BF16 baseline. Higher is better. Lat.: Latency in seconds (lower is better). Q/$: Queries per USD (higher is better). 17.7 8.8 7.5 15.0 7.8 7.8 FP8 INT4 6.7 4.2 4.6 11 36 5 19 37 4"
        },
        {
            "title": "5.2 Asynchronous Deployment",
            "content": "Latency-sensitive use cases are sometimes deployed in synchronous mode, i.e., only one query is processed at time. Synchronous deployment leads to the lowest latency since different requests do not compete for computation, and it tends to be dominated by decode time. Table 5 compares the inference performance for synchronous deployment across different model sizes, GPU types, quantization schemes, and use cases. For each configuration, we highlight the number of GPUs that are most cost-effective. The results reveal that W4A16-INT quantization consistently delivers the highest performance gains for synchronous deployment across all model sizes, GPU types, and latency-sensitive use cases. For both the 8B and 70B model sizes, W4A16-INT achieves significant 2-3x cost reduction per query and 1.5-2x latency improvements compared to the baseline full-precision BF16 format. For the 405B model, W4A16-INT is exceptionally effective, offering 67x reductions in cost per query and enabling inference on smaller number of GPUs. Specifically, deploying the 405B model with W4A16-INT on 4x A100 or H100 GPUs meets performance thresholds that previously required 16 GPUs in BF16 format, reducing inter-GPU communication and lowering latency. As seen in the previous section, the accuracy differences are minor. Processing multiple queries simultaneously generally results in better computational efficiency than single query at time. vLLM automatically coordinates multiple asynchronous queries, balancing computation between prefill and decode stages of parallel requests. Asynchronous deployment usually results in increased latency compared to synchronous deployment, but the processing time is amortized across multiple queries, leading to significant gains in average queries per second. For latency-sensitive use cases, we report the maximum inference throughput (queries per second) under the constraints listed in Table 4, whereas for throughput use cases, we consider the absolute maximum inference throughput. Table 6 shows the inference performance for the asynchronous deployment of both latency-sensitive and throughput use cases. The hardware configurations were optimized to match the number of GPUs yielding the best performance for full-precision BF16 deployment, and we assume optimal orchestration for the quantized models. The data indicates that W8A8-INT and W8A8-FP are the most effective quantization schemes for the asynchronous deployment of midand large-size models on high-end GPUs. As expected, W8A8-FP performs best on H100 GPUs, while W8A8-INT is more suited for efficient inference on A100 GPUs. Performance-Accuracy Trade-Offs in LLM Quantization Table 6. Asynchronous inference performance evaluation across model sizes and hardware configurations. Results show throughput (queries per second) and cost-efficiency (queries per USD) for various use cases. We refer to W8A8-FP as FP8, W8A8-INT as INT8, and W4A16-INT as INT4. Mdl. HW Fmt. Speedup Code Compl. Doc. Gen. Code Fixing RAG Inst. Following Multi-Turn Chat Sum. (S) Sum. (L) QPS Q/$ QPS Q/$ QPS Q/$ QPS Q/$ QPS Q/$ QPS Q/$ QPS Q/$ QPS Q/$ 8B 1A6000 4A6000 70B 4A100 4H 16A100 405B BF16 INT8 INT4 BF16 INT8 INT4 BF16 INT8 INT4 BF16 FP8 INT BF16 INT8 INT4 1.16 1.56 3.10 3.17 1.75 1.33 1.67 1.10 8.09 7. 4.5 20.2k 4.5 20.3k 8.0 35.8k 5.5 24.8k 7.0 31.4k 9.5 42.8k 2.2 10.1k 1.9 8.7k 3.9 17.7k 4.2 18.9k 10.3 46.5k 5.2 23.3k 16.6 74.5k 7.4 33.4k 17.8 80.2k 5.3 23.7k 5.1 22.9k 8.7 39.2k 4.3 19.5k 5.8 26.3k 4.0 17.9k 0.7 2.4 3.1 5.5 7.6 7.7 11.7 19.4 13.5 0.4 3.7 3.7 794 2.7k 3.5k 2.8k 3.8k 3.9k 0.4 1.5 1.6 3.1 5.7 4.0 3.4k 7.6 5.6k 13.0 7.9 3.9k 48 461 464 0.4 2.1 2.2 491 1.6k 1.8k 1.6k 2.9k 2.0k 2.2k 3.8k 2.3k 47 269 280 0.4 1.2 0.8 2.3 4.1 2.7 5.4 8.9 5. 0.2 1.8 1.5 478 1.4k 947 1.2k 2.0k 1.4k 0.6 1.4 0.7 2.6 4.7 3.0 1.6k 6.4 2.6k 10.5 5.9 1.6k 25 228 190 0.3 2.1 1.6 631 1.5k 814 1.3 3.6 3.8 1.3k 6.2 2.4k 11.6 9.5 1.5k 1.9k 13.3 3.1k 22.9 1.7k 17. 38 270 202 0.8 6.0 5.1 1.4k 4.1k 4.3k 3.1k 5.8k 4.8k 0.5 1.8 2.2 3.6 6.7 5. 3.9k 7.9 6.7k 12.7 9.5 5.0k 96 755 639 0.3 3.1 2.5 537 2.0k 2.5k 1.8k 3.4k 2.6k 1.4 2.5 3. 4.2 6.9 7.7 2.3k 8.8 3.7k 19.4 2.8k 13.5 42 390 320 2.2 3.7 2.8 1.6k 2.8k 3.5k 2.1k 3.5k 3.9k 2.6k 5.6k 4.0k 276 470 353 16H100 BF16 FP8 INT4 302 0.8 464 4.9 344 3.1 QPS: Queries per second (higher is better). Q/$: Queries per USD (higher is better). Sum. (S/L): Small/Large Summarization. Numbers denoted with represent thousands (e.g., 20.3k = 20,300). 2.7 533 15.1 257 10. 204 1.1k 757 5.61 3.53 137 627 542 63 366 235 107 663 423 109 618 1.4 8.8 5.6 1.8 8.3 7.2 1.4 8.2 4.4 1.2 7.1 3.4 4.0 6.2 4.6 1.0 4.3k 1.1 5.1k 0.9 3.9k 0.3 312 0.4 504 0.3 363 1.0 514 1.7 844 0.9 443 2.2 630 4.3 1.3k 1.6 466 0.4 56 0.9 119 70 0.6 62 0.8 1.8 133 77 1. In contrast, W4A16-INT is the best option for cost efficiency on A6000 GPUs. It outperforms other quantization schemes, especially for smaller models and latency-sensitive tasks. It suggests that W4A16-INT is viable and cost-effective option for less memory-intensive applications on mid-tier hardware. When evaluating cost efficiency across different GPU classes for midand large-size models, W8A8-FP on H100 GPUs consistently stands out as the best option across all use cases. Overall, the results underscore no universally optimal quantization scheme for asynchronous deployment. The best choice depends on model size, GPU type, and whether the task prioritizes latency or throughput. Nonetheless, quantization leads to significant cost reduction per query regardless of the scenario, achieving 1.5-3x cost reduction for 8B and 70B and 5-8x for 405B models."
        },
        {
            "title": "6 CONCLUSION",
            "content": "a broad provided in-depth trade-offs study of We and accuracy-vs-performance-vs-cost for quantized LLMs across various deployment environments. Specifically, we covered all quantization formats with efficient computational support and range of quantization algorithms, deployment use cases, and GPUs. In Figure 7 we summarize our findings in terms of accuracy recovery per quantization format, using carefully-tuned state-of-the-art quantization techniques. Figure 7. Accuracy recovery trends across academic benchmarks highlight the challenges of integer activation quantization, particularly at larger model sizes. Broadly speaking, our findings show that, with judicious choice of algorithm and parametrization, these formats can offer higher accuracy than previously thought, significantly improve inference performance, and reduce costs. At the same time, we have also shown that the optimal choice of format can be task and algorithm specific, providing guidelines for this choice. Performance-Accuracy Trade-Offs in LLM Quantization"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We would like to express our sincere gratitude to the Neural Magic team, the vLLM community, and EleutherAI for developing the libraries that enabled this research. Specifically, we used the LLM Compressor to produce most of our quantized models, vLLM as an inference engine to accelerate evaluations, and the LM Evaluation Harness to run all of our academic benchmarks."
        },
        {
            "title": "REFERENCES",
            "content": "Ashkboos, S., Markov, I., Frantar, E., Zhong, T., Wang, X., Ren, J., Hoefler, T., and Alistarh, D. Towards end-toend 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. URL https://arxiv.org/abs/2404.00456. Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T. Open llm leaderboard (2023https://huggingface.co/spaces/ 2024). open-llm-leaderboard-old/open_llm_ leaderboard, 2023. Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit quantization of large language models with guarantees, 2023. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. 2021. Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E., and Stoica, I. Chatbot arena: An open platform for evaluating llms by human preference, 2024. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. FlashAttention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. SpQR: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E., Babenko, A., and Alistarh, D. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. FlashInfer, Z. Y. Kernel library for llm serving, 2023. and Wolf, T. Fourrier, C., Habib, N., Lozovskaya, A., Szafer, llm leaderboard Open https://huggingface.co/spaces/ K., v2. open-llm-leaderboard/open_llm_ leaderboard, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Frantar, E., Castro, R. L., Chen, J., Hoefler, T., and Alistarh, D. Marlin: Mixed-precision auto-regressive parallel inference on large language models. arXiv preprint arXiv:2408.11743, 2024. Performance-Accuracy Trade-Offs in LLM Quantization Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, September 2021. URL https: //doi.org/10.5281/zenodo.5371628. Gong, R., Yong, Y., Gu, S., Huang, Y., Lv, C., Zhang, Y., Liu, X., and Tao, D. Llmc: Benchmarking large language model quantization with versatile compression toolkit, 2024a. URL https://arxiv.org/abs/ 2405.06001. Gong, Z., Liu, J., Wang, J., Cai, X., Zhao, D., and Yan, R. What makes quantization for large language model hard? an empirical study from the lens of perturbation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1808218089, 2024b. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Huang, W., Ma, X., Qin, H., Zheng, X., Lv, C., Chen, H., Luo, J., Qi, X., Liu, X., and Magno, M. How good are low-bit quantized llama3 models? an empirical study, 2024. HuggingFace. Text generation inference (tgi), 2024. https://huggingface.co/docs/ URL text-generation-inference/en/index. Karpathy, A. Tweet about training neural networks, 2024. URL https://x.com/karpathy/status/ 1822839061574553945. [Tweet]. Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. arXiv Squeezellm: Dense-and-sparse quantization. preprint arXiv:2306.07629, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lambda Labs. Lambda labs gpu cloud, 2024. URL https: //lambdalabs.com/service/gpu-cloud. Accessed: 2024-10-28. Lee, A. N., Hunter, C. J., and Ruiz, N. Platypus: Quick, cheap, and powerful refinement of llms. 2023. Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Outlieraware weight quantization for efficient fine-tuning and inference of large language models, 2024a. Lee, J., Park, S., Kwon, J., Oh, J., and Kwon, Y. comprehensive evaluation of quantized instruction-tuned large language models: An experimental analysis up to 405b. arXiv preprint arXiv:2409.11055, 2024b. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang, H., and Wang, Y. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024a. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024b. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Zhu, B., Gonzalez, J. E., and Stoica, I. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024c. URL https://lmsys.org/blog/ 2024-04-19-arena-hard/. Lin, C.-Y. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024a. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Lin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C., and Han, S. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/ forum?id=1qvx610Cu7. Performance-Accuracy Trade-Offs in LLM Quantization Liu, P., Liu, Z., Gao, Z.-F., Gao, D., Zhao, W. X., Li, Y., Ding, B., and Wen, J.-R. Do emergent abilities exist in quantized large language models: An empirical study. arXiv preprint arXiv:2307.08072, 2023b. Muralidharan, S., Sreenivas, S. T., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J., and Molchanov, P. Compact language models via arXiv preprint pruning and knowledge distillation. arXiv:2407.14679, 2024. URL https://arxiv. org/abs/2407.14679. Neural Magic, I. Guidellm: Scalable inference and optimization for large language models. https://github. com/neuralmagic/guidellm, 2024. NVIDIA. TensorRT-LLM: TensorRT Large Language URL https://github.com/ Model, 2023. NVIDIA/TensorRT-LLM. Accessed: 2024-10-27. Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., and Durrett, G. Musr: Testing the limits of chain-of-thought with multistep soft reasoning, 2024. URL https://arxiv. org/abs/2310.16049. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. Tseng, A., Sun, Q., Hou, D., and De Sa, C. Qtip: Quantization with trellises and incoherence processing. arXiv preprint arXiv:2406.11235, 2024b. van Baalen, M., Kuzmin, A., Nagel, M., Couperus, P., Bastoul, C., Mahurin, E., Blankevoort, T., and Whatmough, P. Gptvq: The blessing of dimensionality for llm quantization. arXiv preprint arXiv:2402.15319, 2024. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33: 57765788, 2020. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multitask language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022. Yao, Z., Wu, X., Li, C., Youn, S., and He, Y. Zeroquantv2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. arXiv preprint arXiv:2303.08302, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, J. Y. Tweet about machine learning quantization URL https://x.com/ accuracy drops, 2024. zjasper666/status/1829259315599045063. [Tweet]. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C. D. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024a. URL https: //arxiv.org/abs/2402.04396. Zhang, Y., Zhang, P., Huang, M., Xiang, J., Wang, Y., Wang, C., Zhang, Y., Yu, L., Liu, C., and Lin, W. Qqq: Quality quattuor-bit quantization for large language models. arXiv preprint arXiv:2406.09904, 2024. Performance-Accuracy Trade-Offs in LLM Quantization Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models, 2023. URL https: //arxiv.org/abs/2311.07911."
        },
        {
            "title": "A ADDITIONAL RESULTS",
            "content": "A.1 Real-World Benchmarks In Tables 8 and 9 we report pass@10 scores for all models on HumanEval and HumanEval+ benchmarks. Figure 8. HumanEval pass@10 scores for quantized Llama-3.1Instruct models. Table 7. Scores and confidence intervals of two evaluation runs for Llama-3.1-Instruct models through Arena-Hard-Auto-v0.1. Llama-3.1 Instruct Score (1st run) Score (2nd run) Average Score BF16 405B W8A8-FP W8A8-INT W4A16-INT BF16 70B W8A8-FP W4A16-INT W8A8-INT BF16 8B W8A8-FP W8A8-INT W4A16-INT 67.3 66.3 64.3 66. 55.8 57.6 57.1 56.0 25.1 26.8 27.6 23.4 67.5 67.55 64.8 66.4 58.2 57.75 56.8 56.6 26.5 26.85 26.7 24.6 67.4 66.9 64.6 66. 57.0 57.7 57.0 56.3 25.8 26.8 27.2 24.0 95% CI (-2.6, 1.9) (-2.6, 2.3) (-2.4, 2.8) (-2.6, 2.3) (-2.6, 2.1) (-2.4, 3.1) (-2.8, 2.5) (-2.9, 2.4) (-2.1, 2.1) (-2.1, 2.6) (-2.0, 2.2) (-2.2, 2.0) Table 8. Comparison of GPTQ and AWQ quantization algorithms, both with group size of 128, across two runs of the Arena-HardAuto-v0.1 benchmark. Score (1st run) Score (2nd run)"
        },
        {
            "title": "Average\nScore",
            "content": "Llama-3.1-70B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) Llama-3.1-8B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) 55.8 57.1 56.3 25.1 23.4 22.4 58.2 56.8 57.0 26.5 24.6 22. 57.0 57.0 56.3 25.8 24.0 22.3 Lambda Labs only provides the 8x configuration. For scenarions with smaller number of A100 GPUs we assume price proportional to the number of GPUs. Table 9. On-demand hardware cost on Lambda Labs cloud."
        },
        {
            "title": "Hardware",
            "content": "On-demand cost (USD per hours) Figure 9. HumanEval+ pass@10 scores for quantized Llama-3.1Instruct models. In Table 7 we report scores of two Arena-Hard-Auto-v0.1 runs, aggregated average scores, and 95% confidence intervals (CI). A.2 Detailed Comparison of GPTQ and AWQ 1xA6000 2xA6000 4xA6000 8xA 1xH100 2xH100 4xH100 8xH100 0.80 1.60 3.20 14.32 3.29 6.38 12.36 23.92 To complement the results in Table 1, Tables 12, 13, 8 provide detailed per-task and per-run breakdown of scores. A.4 Academic Benchmarks A.3 GPU Pricing We use Lambda Labs on-demand GPU pricing (Lambda Labs, 2024), as displayed in Table 9. For A100 GPUs In Tables 10 and 11 we report accuracy recoveries per-task across academic benchmarks. Performance-Accuracy Trade-Offs in LLM Quantization Table 10. Accuracy recoveries in percentages (%) for each task in the Open LLM Leaderboard V1 benchmark. Llama-3.1-Instruct Baseline BF16 8B W8A8-FP W8A8-INT W4A16-INT Baseline BF16 70B W8A8-FP W8A8-INT W4A16-INT Baseline BF16 405B W8A8-FP W8A8-INT W4A16-INT MMLU 5-shot MMLU CoT 0-shot ARC-C 0-shot GSM8k CoT 8-shot HellaSwag 10-shot Winogrande 5-shot TruthfulQA 0-shot 100.00 99.59 99.27 97.95 100.00 100.00 99.88 99. 100.00 100.11 99.66 99.77 100.00 98.35 99.18 97.66 100.00 99.42 99.77 99.53 100.00 100.00 99.55 99.55 100.00 99.75 100.37 98.53 100.00 100.21 99.79 99. 100.00 100.00 99.37 100.32 100.00 99.03 102.42 100.12 100.00 99.58 99.26 99.47 100.00 99.79 99.48 100.31 100.00 99.38 99.75 99.25 100.00 99.77 99.88 99. 100.00 100.00 99.66 99.77 100.00 99.49 100.51 99.87 100.00 99.18 99.77 100.23 100.00 100.92 98.74 100.23 100.00 99.63 100.37 96.88 100.00 99.84 101.15 98. 100.00 100.00 98.62 100.00 Table 11. Accuracy recoveries in percentages (%) for each task in the Open LLM Leaderboard V2 benchmark. Llama-3.1-Instruct Baseline BF16 8B W8A8-FP W8A8-INT W4A16-INT Baseline BF16 70B W8A8-FP W8A8-INT W4A16-INT IFEval 0-shot 100.00 99.10 100.12 98.00 100.00 101.34 100.17 99.22 Baseline BF16 405B 100.00 99.00 W8A8-FP 99.20 W8A8-INT 100.39 W4A16-INT BBH acc norm 3-shot Math lvl 5 exact match 4-shot GPQA acc norm 0-shot MuSR acc norm 0-shot MMLU-Pro acc 5-shot 100.00 98.54 102.89 96.08 100.00 98.44 98.89 98.60 100.00 100.12 99.57 100.73 100.00 105.42 98.92 94. 100.00 107.52 91.83 93.52 100.00 99.69 91.94 96.53 100.00 155.98 146.20 109.78 100.00 94.68 88.38 89.94 100.00 97.38 104.51 89.85 100.00 98.82 100.00 83. 100.00 94.49 92.62 94.99 100.00 106.93 98.77 99.54 100.00 101.33 100.26 93.63 100.00 99.13 97.86 98.19 100.00 99.43 97.81 99.35 Table 12. Comparison of GPTQ and AWQ quantization algorithms, both with group size of 128, across Open LLM Leaderboard V1 benchmarks (Beeching et al., 2023) with Metas prompts (Dubey et al., 2024)."
        },
        {
            "title": "Average Score",
            "content": "MMLU 5-shot MMLU CoT 0-shot ARC-C 0-shot GSM8k CoT 8-shot HellaSwag 10-shot Winogrande 5-shot Llama-3.1-8B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) Llama-3.1-70B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) 74.06 73.11 72.69 84.20 83.77 83.96 68.30 66.90 66.37 82.37 82.03 82. 72.80 71.10 69.76 86.06 85.54 85.64 81.40 80.20 80.89 93.30 92.80 93.00 82.80 82.90 82.56 94.90 94.40 94. 80.50 79.90 79.61 86.80 86.30 86.44 78.10 78.00 76.80 85.30 85.50 85.79 TruthfulQA mc2 0-shot 54.50 52.80 52. 60.70 59.80 60.23 Performance-Accuracy Trade-Offs in LLM Quantization Table 13. Comparison of GPTQ and AWQ quantization algorithms, both with group size of 128, across Open LLM Leaderboard V2 benchmarks (Fourrier et al., 2024)."
        },
        {
            "title": "Average Score",
            "content": "IFEval 0-shot BBH acc norm 3-shot Math lvl 5 exact match 4-shot GPQA acc norm 0-shot MuSR acc norm 0-shot MMLU-Pro acc 5-shot Llama-3.1-8B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) Llama-3.1-70B-Instruct GPTQ (Frantar et al., 2022) AWQ (Lin et al., 2024a) 27.62 26.53 27.40 41.66 40.58 41.09 77.86 76.30 78.25 86.41 85.74 86. 30.09 28.91 27.20 55.79 55.01 55.24 15.68 14.80 13.87 26.07 24.38 25.14 3.68 4.04 5.21 15.40 13.85 13. 7.61 6.33 10.45 18.16 17.25 18.81 30.77 28.81 29.41 48.12 47.25 47."
        }
    ],
    "affiliations": [
        "Neural Magic",
        "Institute of Science and Technology Austria"
    ]
}