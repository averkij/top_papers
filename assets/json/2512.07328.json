{
    "paper_title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "authors": [
        "Ziyang Mai",
        "Yu-Wing Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 2 3 7 0 . 2 1 5 2 : r ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation"
        },
        {
            "title": "Ziyang Mai",
            "content": "Yu-Wing Tai"
        },
        {
            "title": "Dartmouth College",
            "content": "Figure 1. Overview of ContextAnyone. Given reference image and text prompt, our model generates character-consistent videos that preserve visual details across diverse scenes, while prior methods struggle to retain all elements from the reference. Pink boxes highlight key details such as the chef hat, collar shape, and pant. Green boxes indicate regions where these details are faithfully preserved, whereas red boxes mark inconsistencies, such as the collar mismatch in the lower left. Many other inconsistencies are omitted for simplicity."
        },
        {
            "title": "Abstract",
            "content": "fail identity but Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains major challenge. Existing personalization methods often focus on facial to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, context-aware diffusion framework that achieves character-consistent video generation from text and single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into DiT-based diffusion backbone through novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing referenceto-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone. 1. Introduction Recent advances in text-to-video (T2V) generation[5, 6, 15, 23, 3335, 41] have enabled realistic and temporally coherent videos from natural language descriptions. However, generating characters that remain visually consistent across scenes and motions remains challenging. In real-world storytelling and film production, character is defined not only by facial identity but also by visual context such as hairstyle, outfit, and body shape. Preserving this context is essential for narrative continuity and visual coherence. As shown in the second and third rows of Figure 1, failing to preserve context leads to inconsistent clothing, causing perceptual discontinuity and reduced realism. Although recent diffusion-based models have improved fidelity and motion coherence, many personalization methods [24, 42, 43, 45] focus on identity transfer by injecting face features from face encoders. These approaches often capture only partial identity cues while neglecting contextual appearance. Recent works [10, 11, 20, 25] move beyond traditional identity-injection frameworks by introducing pixel-level [10] or channel-level [11] fusion to enhance contextual and subject consistency. However, such fusion does not ensure full utilization of reference information, and the model may still suffer from identity drift with complex visual structures, as shown in Figure 1. Furthermore, conventional cross-attention in video diffusion models offers limited control over how reference information influences temporal tokens, often causing unstable identity transfer during denoising. We present ContextAnyone, context-aware diffusion framework designed to generate character-consistent videos from text and single reference image. Our approach simultaneously reconstructs the reference image and generates new video frames. Reconstructing the reference image enables the model to fully perceive and understand all visual cues of the reference identity. Meanwhile, the reconstructed result can also serve as an anchor that provides detailed appearance inforamtion for subsequent video generation. To efficiently inject this anchor information into newly generated frames, we propose an attention modulation strategy, including an Emphasize-Attention module that selectively reinforces reference-aware information during denoising. To avoid the temporal collapse caused by reconstructing the reference image and generating new video frames simultaneously, we introduce Gap-RoPE to explicitly separate the positional embeddings of reference and video tokens for stable temporal modeling. In addition, we introduce dual encoders that extract complementary semantic and fine-grained features. Together, these components enable our model to produce coherent, identity-consistent, and context-preserving character videos under complex motions and scene variations. Comprehensive experiments demonstrate that ContextAnyone achieves superior performance compared with state-of-the-art reference-to-video methods under the same parameter scale. As shown in Figure 1, our model generates realistic and temporally stable videos that preserve visual context and identity fidelity across diverse scenes. Quantitative results confirm higher identity consistency and visual quality, while ablation studies highlight the effectiveness of the various designs. Our main contributions are summarized as follows: We propose ContextAnyone, context-aware framework that jointly reconstructs the reference image and generates new frames, enabling comprehensive identity transferring. We introduce an attention modulation strategy with Emphasize-Attention to efficiently inject reference information and reinforce reference-aware cues. We design Gap-RoPE to explicitly separate positional embeddings of reference and video tokens for improved temporal stability, and employ dual encoders to capture complementary semantic and fine-grained visual features. Our method achieve SOTA performance among others under same parameter scale. 2. Related Works Diffusion Models for Video Generation Diffusion models have recently driven major advances in text-to-video (T2V) synthesis. Early methods [1, 12, 21, 31] extend pretrained image diffusion frameworks such as Stable Diffusion [30] by inserting lightweight temporal attention or motion adapter layers [16]. These extensions reuse strong spatial priors learned from large image datasets, enabling short video generation with minimal retraining. In contrast, training-based video diffusion models [2, 5, 6, 35] employ spatio-temporal U-Nets with 3D convolutions or dedicated temporal attention modules to directly capture motion dynamics from video data. While effective, such models are computationally expensive and difficult to scale. Recent research has shifted toward transformer-based diffusion architectures. Diffusion Transformers (DiT) [29] have become the dominant design for high-quality video generation [13, 15, 27, 41, 44]. Large-scale DiT models such as Wan [34], HunyuanVideo [23], and Mochi [33] demonstrate excellent scalability and visual fidelity compared with U-Net methods. Our work follows this trend by adopting DiT-based backbone for its strong generation capability and efficiency, but extends it with contextaware conditioning and attention modulation that explicitly preserve subject identity and contextual consistency across frames, which standard DiT models lack. Reference-Conditioned Video Generation Reference-tovideo (R2V) generation focuses on synthesizing personalized videos based on given reference image or video. Earlier methods [18, 3739] adapt pretrained diffusion models to each subject through test-time fine-tuning. For instance, VideoMage [18] trains LoRA [17] modules for each reference input before inference. Although such methods can produce identity alignment, they are computationally demanding and unsuitable for real-time or large-scale scenarios due to the need for per-instance optimization. To overcome these constraints, more recent approaches [3, 7, 9, 19, 20, 25, 42] integrate reference conditioning directly into the model architecture. For instance, Figure 2. Framework of ContextAnyone. (a) Input: The model takes text prompt and reference image Ir. The prompt is augmented by VLM and encoded by an LLM encoder, while the image is processed by two encoders: one for cross-attention guidance and one for latent concatenation. (b) Backbone: The DiT backbone contains stacked blocks with three attention modules. The input latent merges reference latents (green) and noisy video latents (blue), which are decoded separately by the VAE after the DiT blocks. (c) Emphasize Attention: Latents are split into reference and video parts; video latents serve as queries and reference latents as keys and values to reinforce identity. (d) Self-Attention: The attention map is masked so that reference latents do not query video latents, enforcing one-way information flow from reference to video tokens. ConsisID [42] employs frequency-domain decomposition to preserve facial identity, yet its focus remains limited to facial identity. SkyReels-A2 [11] extends video diffusion transformers with channel-wise composition for elementto-video generation, but relies on simple masking strategy that copies mask frames across time, weakening backbone modeling capacity [10] and causing instability and identity drift. Our method addresses these limitations with unified, context-aware framework that maintains both global identity and fine-grained appearance from single reference image, strengthening reference features during denoising and stabilizing temporal attention to prevent identity drift. 3. Method An overview of the framework is illustrated in Fig. 2, which highlights the dual-encoder design, diffusion transformer backbone, and attention modulation components that jointly enable consistent and context-aware video synthesis. 3.1. Preliminary Video generation models aim to learn denoising process that gradually transforms random noise into coherent video sequences. Given clean video sample x0, the forward diffusion process progressively adds Gaussian noise to produce latent variable zt at timestep t: zt = αt x0 + 1 αt ϵ, ϵ (0, I), (1) where αt denotes the cumulative noise schedule and ϵ represents the sampled noise. The denoising model ϵθ(zt, t) is trained to predict the noise component from the noised latent zt. During training, the objective is to minimize the difference between the true noise ϵ and the predicted noise ϵθ, expressed as mean squared error (MSE) loss: = Eϵ,zt,t (cid:2) ϵ ϵθ(zt, t) (cid:3) . (2) 2 By optimizing this objective, the model learns to reverse the diffusion process, progressively denoising the latent representation to reconstruct realistic video frames. 3.2. ContextAnyone Framework Given textual description and reference image Ir of the target person, our goal is to generate temporally coherent video = {ft}Tf t=1 that preserves both the identity and contextual appearance (e.g., hairstyle, outfit, and body shape) of the input person. Text Augmentation. Given an input caption , our framework uses vision-language model to expand it into two complementary components: First frame prompt and Later frame prompt. The First frame prompt describes the subjects appearance and global attributes as depicted in the reference image, while the Later frame prompt specifies the content to be generated in the subsequent video sequence, including background and motion. The Later frame prompt often uses the phrase the same person to maintain clear semantic link to the reference. As illustrated in Fig. 2(a), the First frame prompt may describe man wearing plaid shirt and having beard, while the Later frame prompt specifies that the same man is standing in front of laptop, gesturing with his hands, and surrounded by colorful posters. Feeding both prompts into the LLM Encoder allows the model to jointly encode identity-preserving and motion-descriptive semantics, which enhances temporal coherence and visual consistency in the generated videos. Reference Image Encoding. To preserve both semantic identity and visual detail in video generation, we adopt dual-encoder design that combines CLIP Image Encoder and Video VAE Encoder. This design is motivated by the need to bridge the gap between global appearance understanding and fine-grained visual reconstruction. While single encoder often struggles to capture both aspects simultaneously, the combination of these two complementary encoders provides more comprehensive representation of the reference image. The CLIP encoder extracts high-level semantic embeddings that capture global identity cues. These embeddings are injected into the diffusion backbone through crossattention layers, guiding the model to maintain consistent subject semantics throughout the video. In parallel, the Video VAE encoder converts the same reference image into dense latent representation that retains detailed visual characteristics such as texture, hairstyle, and clothing. These low-level features are concatenated with the noisy video latents and jointly processed within the diffusion transformer, ensuring that appearance details are faithfully reconstructed frame by frame. By integrating both semantic and visual pathways, the dual-encoder architecture enables the model to generate videos that are not only visually realistic but also identityconsistent across time, leading to higher overall quality and stability in dynamic scenes. Backbone and Guidance Signal. ContextAnyone uses DiT as backbone. As shown in Fig. 2(b), each DiT block integrates three attention modules: self-attention, crossattention, and emphasize-attention, to jointly model temporal dynamics, text semantics, and reference appearance cues. The input to the backbone is concatenated latent representation that merges the reference image latent with the noisy video latent. This fused representation provides unified multimodal context, allowing the model to learn both motion evolution and identity preservation within single denoising process. After passing through the DiT blocks, the model produces two latent outputs simultaneously: the reconstructed reference image and the generated video frames. The reconstructed reference frame is decoded and compared with the original reference image to deliver an explicit identitypreserving supervision signal. mean squared error loss ensures that the reconstructed frame closely matches the ground-truth appearance: Lref = ˆIr Ir2 2, (3) where ˆIr is the reconstructed reference frame and Ir is the original reference image. For the generated frames, we employ standard diffusion loss that measures the accuracy of the predicted noise: Lgen = Eϵ,zt,t (cid:2)ϵ ϵθ(zt, t)2 2 (cid:3) , (4) where ϵ is the sampled noise and ϵθ is the predicted noise at timestep t. The overall training objective combines these two guidance signals: Ltotal = Lgen + λ Lref, (5) where λ controls the strength of the identity reconstruction constraint. By reconstructing the reference image alongside generating new video frames, the model first forms detailed and explicit representation of the subjects appearance, which serves as an anchor for subsequent synthesis. This reconstructed anchor provides information for the generation of later frames, effectively reducing identity drift and improving consistency between the reference and generated frames, resulting in visually faithful generated videos. Attention Modulation. Beyond standard cross-attention in the DiT blocks, we introduce an additional Emphasize Attention module to strengthen the reference subjects identity representation. Inserted after the cross-attention layer, it provides secondary fusion stage that explicitly re-injects reference latents into the noisy video latents. As shown in Fig. 2(c), the latent is split into noisy video tokens (queries) and reference tokens (keys and values). This targeted interaction selectively integrates reference-aware features into the video representation, improving identity alignment and temporal consistency. The updated video latents are then merged back with the reference latents to maintain dimensional compatibility for subsequent diffusion steps. In conventional self-attention, all latents attend to each other bidirectionally. However, this unrestricted flow can degrade reconstruction of the reference frame, as reference latents may absorb irrelevant motion cues from noisy video Figure 3. Left: Architecture of self-attention with Gap-RoPE applied to Q, K, and V. Upper Right: Standard RoPE assigns continuous temporal indices across all tokens. Lower Right: Gap-RoPE introduces shift β from the generated video frame onward, creating positional gap between reference and video tokens. latents. To avoid this, we enforce unidirectional information flow from reference latents to video latents. As shown in Fig. 2(d), we mask the attention region where reference latents act as queries and video latents as keys, setting these entries to zero. This prevents reference tokens from being influenced by noisy video latents, while still allowing video latents to attend to them. The resulting directional constraint preserves the reference representation and stabilizes identity propagation across frames. 3.3. Gap-RoPE Rotary Position Embedding (RoPE) [32] is commonly used in self-attention to encode spatial and temporal order among latent tokens. In standard video diffusion models, RoPE enforces temporal continuity by assuming all tokens belong to consecutive frames in one coherent sequence. However, in our framework, the first frame is reconstructed reference image, while later frames are synthesized video content. Since these serve different purposes, applying single continuous positional embedding can cause temporal collapse, producing abrupt, unsmooth transitions between the two segments. Inspired by [36], we introduce Gap-RoPE, modified positional embedding that explicitly separates the positional spaces of reference and video tokens. As illustrated in Figure 3, Gap-RoPE introduces temporal shift β for each generated-frame token, effectively creating positional gap between the reference and video tokens. Formally, the positional encoding for token is defined as: Gap-RoPE(i) = RoPE(i + βgi), (6) where gi = 0 for reference tokens and gi = 1 for generated tokens. The spatial dimention is excluded for simplicity. By decoupling positional relationships in this way, Gap-RoPE prevents temporal confusion at the boundary between reference and video segments, stabilizing training and improving identity consistency across generated sequences. Figure 4. Dataset pipeline. Given ground-truth video, we extract its first frame (green box) and randomly sample action and environment prompt from pools. These prompts and first frame are fed into an image editing model to modify the persons action and the scenes illumination. vision-language model then assesses and filters out invalid edits, after which segmentation model isolates the person foreground to obtain the final reference image. 3.4. Dataset Pipeline High-quality, open-source R2V datasets remain scarce, particularly those featuring person-centric sequences with detailed outfit information. Prior work [11] typically extracts random frame from each video and segments the subject as the reference image. However, this strategy results in reference images that share nearly identical pixel regions with the target video frames, making it trivial for models to overfit by copying or directly pasting appearance information from the input. To address this issue, we design data generation pipeline that intentionally alters both the subjects action and the surrounding environment while preserving the persons identity. As illustrated in Figure 4, we build two prompt pools: an Action Prompt Pool containing 50 action descriptions (e.g., run forward, cross arms and smile), and an Environment Prompt Pool describing diverse scenes (e.g., cozy indoor cafe, shopping mall corridor). Given ground-truth video, we extract its first frame and independently sample one action prompt and one environment prompt. These prompts are then fed into an image editing model, which transforms the extracted frame into new image depicting the same person but with modified actions and illumination consistent with the target environment. Because image editing models may occasionally fail, resulting in missing limbs, distorted shapes, or incomplete renderings, we employ vision-language model to assess and filter out invalid edits. For all valid edits, we apply high-quality segmentation model to isolate the person Method Video Quality Video-Reference Consistency Inter-Video Consistency CLIP-I Temp. Con. ArcFace DINO-I ArcFace DINO-I VLM-App. VACE 1.3B 0.3012 Phantom 1.3B 0. Ours 0.3107 0.9903 0.9802 0.9831 0.5489 0.5636 0. 0.4468 0.4719 0.4824 0.5394 0.5412 0.4103 0.4419 0.5943 0. 0.8123 0.8490 0.9457 Table 1. Comparison of methods across video quality, intra-video consistency, and inter-video consistency. foreground, obtaining the final reference image. Although background are not retained in the segmented reference, the environment prompts remain essential to induce illumination variations and thereby increase the diversity and difficulty of the dataset. Through this pipeline, we construct challenging, detailrich, and identity-consistent person-centric R2V dataset, where each reference image exhibits altered actions and lighting conditions that prevent trivial pixel copying and promote robust model generalization. 4. Experiments 4.1. Experiment Settings Dataset. We constructed our dataset from 50,000 videos in OpenVid-HD [28] and processed them with our dataset pipeline shown in 4. We employ Flux.1 Kontext [? ] for image editing, Qwen3 [40] as the VLM, and SAM [22] for segmentation. This pipeline generate roughly 18,000 challenging image-video pairs. From these, we randomly sampled 5% (933 videos) to create our benchmark, and using VLM generate additional prompt for each video in the benchmark to evaluation cross-video performance. Implementation detail. We use Wan 2.1 T2V 1.3B [34] model as our base model. In our experiments, the value of λ is defined as λ = fr where fr is the frame count of referfv ence frame, which is always 1 in our case. And fv denotes the number of frames in each training video. This normalization ensures that each frame including reference frame contributes equally to the overall loss. The value of β is set to 4. In terms of parameter training, all DiT blocks are finetuned in our setting. Learning rate set to 1 104 with linearly warm up. We adopt the AdamW [26] optimizer with β1 set to 0.9 β2 set to 0.95. All experiments are conducted on 8 NVIDIA A6000 Ada GPUs. Baselines. To evaluate the performance of ContextAnyone, we compare it with SOTA R2V methods, including Phantom [25] and VACE [20]. For fair comparison, all models are given the same input: reference image and text prompt. Since these two models are not trained on augmented text as ours is, we use the original text prompt from OpenVid-HD [28]. In addition, we use the 1.3B-parameter versions of Phantom and VACE, which match the parameter scale of our model. All other hyperparameters and settings are kept at their default configurations. 4.2. Quantitative Evaluation We evaluate the proposed method using three categories of metrics that jointly assess video quality, video-reference consistency, and inter-video consistency. Video quality. We measure prompt responsiveness by CLIP-I score [14], which calculates the cosine similarity between the embedding of each frame and the embedding of text prompt. Our method achieves the highest CLIP-I score, indicating better semantic alignment between generated frames and the input prompt. Meanwhile, we also measure temporal consistency by calculating the cosine similarity between every two consecutive frames. Although VACE achieves the highest temporal consistency score, this is largely attributed to its tendency to generate nearly static videos, which artificially inflates frame-to-frame similarity. Video-reference consistency. In this module we assess the consistency between reference image and generated video. We evaluate the similarity between generated frames and their corresponding reference images using ArcFace [8] and DINO-I [4] features. Our approach obtains the highest identity similarity and the most stable appearance similarity, demonstrating its effectiveness in preserving fine-grained facial characteristics and overall person-specific attributes during generation. Inter-video consistency. To assess consistency across different videos, we use both ArcFace and DINO-I to compute the similarity between all frame pairs from two videos. Subtle visual details, such as clothing or hairstyle, occupy very limited pixel regions and can be easily affected by lighting or scene changes, making them difficult to capture using DINO-I alone. To address this, we introduce VLM-Appearance, vision-language modelbased metric, to analyze and score the perceived similarity of characters across videos. Our model surpasses all baselines in both ArcFace and DINO-I similarity, demonstrating superior robustness in maintaining consistent identity and appearance across varied prompts and scenes. Overall, the numerical results align with our qualitative findings: our method achieves stronger identity preservation, more coherent appearance modeling, and smoother temporal dynamics than existing approaches. Figure 5. Qualitative evaluation. Each group shows, from top to bottom, the reference image (Ref.), the results of our method (Ours), Phantom, and VACE. As illustrated, our method produces the most realistic and consistent results in terms of facial identity and overall appearance. In contrast, Phantom and VACE exhibit noticeable artifacts and inconsistencies in facial regions or outfit alignment (highlighted by red boxes). Our approach achieves superior appearance consistency and motion fidelity compared to existing methods. 4.3. Qualitative Evaluation Figure 5 provides qualitative comparison between our method and the baselines. For each group, we show the reference image (Ref.) followed by the results from different methods. Regions with inconsistencies or artifacts are highlighted with red boxes. Our method consistently preserves fine-grained facial features, including contours, skin tone, and expressions, while maintaining hairstyle structure and clothing textures across scenes. It demonstrates strong robustness to changes in background, lighting, and pose, achieving high visual coherence and accurate appearance transfer. In contrast, both baseline methods show clear limitations. Phantom often generates misaligned outfits or noticeable clothing discontinuities, particularly under complex body poses or uneven lighting, creating distracting artifacts that disrupt temporal coherence. VACE exhibits Method Temp. Con ArcFace DINO-I w/o Aug. Prompt w/o Recon. Loss w/o Attn. Mod. w/o Gap-RoPE Ours 0.9798 0.9721 0.9756 0.9410 0. 0.5681 0.5271 0.5592 0.5814 0.6003 0.4633 0.4203 0.4189 0.4622 0.4824 Table 2. Quantitative evaluation of ablation study. even stronger deviations from the reference, with inaccurate identity alignment and inconsistent facial regions, leading to identity drift, unnatural deformations, or local warping around key areas such as the eyes, mouth, and jawline. More qualitative comparisons are shown in Figure 6. 4.4. Ablation We conduct ablation experiments by removing the augmented prompt, reconstruction loss, attention modulation Figure 6. More results. Each group presents reference image alongside the augmented prompts in the first row, followed by the crossvideo generation results in the second row. The yellow-highlighted and blue-highlighted prompts correspond to the inputs of the left and right videos, respectively. Our method preserves strong consistency in the subjects identity and appearance across the generated videos. ArcFace and DINO-I scores similar to ours but introduces noise artifacts in early frames, significantly reducing temporal consistency to 0.9410. These results demonstrate that each component contributes to different aspect of performance, and the full model achieves the best scores across all metrics. 5. Conclusion We introduced ContextAnyone, context-aware diffusion framework for character-consistent text-to-video generation from text and single reference image. Unlike existing reference-to-video methods, which either require costly test-time adaptation or capture only limited identity cues with weak contextual preservation, ContextAnyone achieves robust and faithful identity retention by jointly reconstructing the reference image while generating the video sequence. The proposed Emphasize-Attention module reinforces reference-aware features throughout the diffusion process, and the Gap-RoPE positional embedding stabilizes temporal modeling by separating reference and generated tokens. Extensive experiments show that ContextAnyone consistently improves both identity fidelity and visual quality across diverse motions, poses, and environments. In future work, we aim to extend the framework to multi-reference and multi-character video generation, enabling richer scene composition and more complex narrative interactions while maintaining character consistency. Figure 7. Visualization of the ablation study. The upper-left panel shows the reference image and the text prompt. Among all variants, our full model yields the most faithful identity preservation and contextual consistency. module, and Gap-RoPE. As shown in Figure 7 and Table 2, removing the augmented prompt produces person who is less similar to the reference image, lowering both ArcFace and DINO-I similarity because the text prompt no longer provides strong semantic guidance. Excluding the reconstruction loss further reduces identity fidelity, leading to visibly distorted facial features and the lowest ArcFace score of 0.5271. Without attention modulation, the model still captures coarse identity cues but fails to maintain fine contextual details. For example, the mans purple pocket square is generated inconsistently, resulting in the lowest DINO-I score of 0.4189. Removing Gap-RoPE maintains"
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 2 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. 2 [3] Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang, Xiaokang Yang, Zhe Lin, and Alan Yuille. Omnivcus: Feedforward subject-driven video customization with multimodal control conditions. arXiv preprint arXiv:2506.23361, 2025. 2 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1, 2 [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 1, 2 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60996110, 2025. [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 6 [9] Yufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang, Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, and Chongyang Ma. Cinema: Coherent multisubject video generation via mllm-based guidance. arXiv preprint arXiv:2503.10391, 2025. 2 [10] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, and Chongyang Ma. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. 2, 3 [11] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, and Yahui Zhou. Skyreels-a2: Compose anyarXiv preprint thing in video diffusion transformers. arXiv:2504.02436, 2025. 2, 3, 5 [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. 2 [13] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 75147528, 2021. 6 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 2 [16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer In International conference on machine learning for nlp. learning, pages 27902799. PMLR, 2019. 2 [17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2 [18] Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, and Yu-Chiang Frank Wang. Videomage: Multi-subject and motion customization of text-toIn Proceedings of the Computer video diffusion models. Vision and Pattern Recognition Conference, pages 17603 17612, 2025. 2 [19] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [20] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 6 [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 2 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and In Proceedings of Ross Girshick. the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 6 Segment anything. [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2 [24] Hengjia Li, Haonan Qiu, Shiwei Zhang, Xiang Wang, Yujie Wei, Zekun Li, Yingya Zhang, Boxi Wu, and Deng Cai. Personalvideo: High id-fidelity video customization withIn Proceedings of out dynamic and semantic degradation. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1940619416, 2025. 2 [25] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via crossmodal alignment. arXiv preprint arXiv:2502.11079, 2025. 2, [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [27] Xin Ma, Yaohui Wang, Xinyuan Chen, Gengyun Jia, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. Transactions on Machine Learning Research, 2025. 2 [28] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 6 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [33] Genmo Team. Mochi 1. GitHub repository, 2024. 1, 2 [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 6 [35] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 1, 2 [36] Jiahao Wang, Hualian Sheng, Sijia Cai, Weizhan Zhang, Caixia Yan, Yachuang Feng, Bing Deng, and Jieping Ye. arXiv Echoshot: Multi-shot portrait video generation. preprint arXiv:2506.15838, 2025. [37] Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. Customvideo: Customizing textto-video generation with multiple subjects. CoRR, 2024. 2 [38] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. [39] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving moIn Proceedings of tion and concept composition abilities. the AAAI Conference on Artificial Intelligence, pages 8469 8477, 2025. 2 [40] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 6 [41] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, [42] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. 2, 3 [43] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magicmirror: Id-preserved video generation in video diffusion transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1446414474, 2025. 2 [44] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 2 [45] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Towards univerarXiv preprint identity-preserving video synthesis. and Chongxuan Li. sal arXiv:2503.14151, 2025. 2 Concat-id:"
        }
    ],
    "affiliations": []
}