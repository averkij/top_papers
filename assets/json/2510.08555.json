{
    "paper_title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning",
    "authors": [
        "Minghong Cai",
        "Qiulin Wang",
        "Zongli Ye",
        "Wenze Liu",
        "Quande Liu",
        "Weicai Ye",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 5 5 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "VIDEOCANVAS: UNIFIED VIDEO COMPLETION FROM ARBITRARY SPATIOTEMPORAL PATCHES VIA INCONTEXT CONDITIONING Minghong Cai1 Xintao Wang2, Pengfei Wan2, Kun Gai2, Xiangyu Yue1 (cid:66) 1MMLab, The Chinese University of Hong Kong, 2Kling Team, Kuaishou Technology , Qiulin Wang2 (cid:66), Zongli Ye1, Wenze Liu1, Quande Liu2, Weicai Ye2, Figure 1: VideoCanvas: Arbitrary Spatio-Temporal Video Completion. Given any conditions (frames or patches, outlined in red), the model fills in the remaining gray regions to generate coherent, high-quality videos. This unified formulation subsumes various tasks such as Any-TimestepPatch/Image-to-Video, In/Outpainting, Camera Control, and Cross-scene Video Transitions, all in zero-shot manner. More results are available on our Project Page. Best viewed zoomed in."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce the task of arbitrary spatio-temporal video completion, where video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on video canvas. This flexible formulation Work done at Kling Team, Kuaishou Technology. (cid:66)Corresponding Authors."
        },
        {
            "title": "Preprint",
            "content": "naturally unifies many existing controllable video generation tasksincluding first-frame image-to-video, inpainting, extension, and interpolationunder single, cohesive paradigm. Realizing this vision, however, faces fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, novel framework that adapts the InContext Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition continuous fractional position within the latent sequence. This resolves the VAEs temporal ambiguity and enables pixel-frame-aware control on frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing new state of the art in flexible and unified video generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video generation has made significant strides with the advent of Diffusion Transformers (DiTs) (Peebles & Xie, 2023; Chen et al., 2023; Wang et al., 2025a; Yang et al., 2024), marking turning point in the fields ability to synthesize high-quality videos. However, generating videos that truly align with user intent remains significant challenge. Existing controllable approaches are typically constrained by rigid, task-specific formatsfor example, conditioning only on first frame (Guo et al., 2023; Kong et al., 2024), using an initial clip with limited temporal horizon (Bar et al., 2025; Yang et al., 2025a), or performing structural inpainting and outpainting (Zhou et al., 2023; Wang et al., 2024; Yang et al., 2025b). These methods treat spatio-temporal control as set of isolated problems, lacking unified approach. We propose unified approach to bridge these fragmented tasks: treating video synthesis as painting on spatio-temporal canvas. In this framework, users can place arbitrary content patches at any location and timestamp, and the model will synthesize complete, temporally consistent video around them, as illustrated in Fig. 1. This finegrained control enables wide range of applications, from creative content generation to practical use cases, such as reconstructing videos from partially transmitted or corrupted data packets (Li et al., 2023; Du et al., 2020), or generating videos with specific spatial and temporal conditions for diverse domains. Realizing this vision presents fundamental challenges across both spatial and temporal dimensions, inherent to modern latent video models. Temporally, causal video VAEs compress multiple pixel frames into single latent slot, creating indexing ambiguity and making frame-accurate control core obstacle, as illustrated in Fig. 2(a). Spatially, conditions may take arbitrary formsfrom full frames to small, irregular patchesrequiring mechanism that can seamlessly unify inpainting and outpainting within one formulation. The core difficulty lies in designing conditioning paradigm that can resolve both temporal ambiguity and spatial irregularity simultaneously. Viewed through this lens, the limitations of existing paradigms become clear. Latent Replacement (HaCohen et al., 2024; Kong et al., 2024) was designed mainly for first-frame I2V but fails to generalize, as it overwrites entire latent slots and disrupts temporal consistency once applied to arbitrary timestamps. Channel Concatenation and Adapter-style injection methods (Yang et al., 2024; Wang et al., 2025a; Mou et al., 2024; Zhang et al., 2023) fuse conditional features either by concatenating at the input or injecting via lightweight encoders. Despite architectural differences, these approaches remain coarse-grained: pixel-frame-aware control ultimately requires feeding zero-padded frames to the VAE, but pretrained VAEs are not robust to such inputs. Making them work would require expensive VAE fine-tuning and re-training of the DiT backbone. More recent In-Context Conditioning (ICC) methods (Tan et al., 2024; Ju et al., 2025; He et al., 2025; Ye et al., 2025a; Guo et al., 2025) inherit the same difficulty when naively combined with zero-padding: they still demand VAE/DiT re-training to handle the distribution shift, and further double the sequence length by encoding padded frames, resulting in severe inefficiency during both training and inference."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Core challenge and solution for pixel-frame-aware conditioning. (a) Causal VAEs create temporal ambiguity by mapping frames to single latent. We propose hybrid solution combining Spatial Padding with Temporal RoPE Interpolation. (b) We show how competing paradigms are ill-suited for fine-grained control, while our ICC approach provides an effective solution. In this paper, we introduce VideoCanvas, the first framework to apply In-Context Conditioning to the challenging task of arbitrary spatio-temporal video completion. We also propose hybrid conditioning strategy that decouples space and time: spatial alignment is achieved by zero-padded VAE encoding of arbitrary patches, while temporal ambiguity is resolved by our novel RoPE Interpolation, which assigns continuous fractional indices to conditional frame tokens. This design removes the need for costly re-training of the VAE or architectural modifications of the DiT backbone, while allowing efficient fine-tuning to enable fine-grained pixel-frame-aware control within simple, parameter-free ICC architecture. To evaluate this new task and framework, we present VideoCanvasBench, comprehensive benchmark tailored for arbitrary spatio-temporal video completion. To the best of our knowledge, it is the first to systematically incorporate multi-frame, non-homologous image and patch conditions to test both intra-scene fidelity and inter-scene creativity. Our contributions are as follows: We introduce and formalize the task of arbitrary spatio-temporal video completion, unified framework that encompasses wide range of controllable video generation scenarios, including not only existing first-frame-to-video, video extension and painting tasks, but also new tasks such as any-timestamp patch-to-video and any-timestamp image-to-video, extending control to arbitrary timestamps in time and arbitrary regions in space. We propose VideoCanvas, the first framework to apply the In-Context Conditioning paradigm to the task of arbitrary spatio-temporal completion. We further introduce hybrid conditioning strategy: Spatial Zero-Padding and Temporal RoPE Interpolation. This approach enables efficient fine-tuning of the DiT model without the need for VAE retraining, achieving fine-grained spatiotemporal control. We design and release VideoCanvasBench, the first benchmark explicitly dedicated to arbitrary spatio-temporal completion, and demonstrate that VideoCanvas achieves state-of-the-art performance across diverse settings, outperforming existing conditioning paradigms."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 ARBITRARY SPATIO-TEMPORAL VIDEO COMPLETION Controllable video generation aims to synthesize content that adheres to user inputs beyond simple text prompt. Existing approaches are often constrained by rigid, task-specific formats, such as conditioning only on first frame (Guo et al., 2023; Kong et al., 2024; Wan et al., 2025; Shi et al., 2024; Gao et al., 2025), on short initial sequence (Bar et al., 2025; Yang et al., 2025a), or on structural inpainting and outpainting (Zhou et al., 2023; Wang et al., 2024; Bian et al., 2025; Yang et al., 2025b). Conceptually, these represent special cases of the broader challenge of video completion, yet prior work has treated them as separate sub-tasks, each requiring specialized solutions. Recent unified frameworks like VACE (Jiang et al., 2025) have made progress on consolidating diverse tasks, primarily focusing on inpainting, outpainting, and video extension. However, these models"
        },
        {
            "title": "Preprint",
            "content": "still remain constrained to specific forms of video completion and fail to address the general problem of arbitrary spatio-temporal control. In contrast, we introduce and formalize the task of arbitrary spatio-temporal video completion, unified and flexible paradigm that subsumes and extends prior settings. By allowing conditions of arbitrary shapes and at arbitrary spatio-temporal locations, our task formulation goes beyond task-specific or partially unified approaches. This enables genuinely unified spatio-temporal video generation. To facilitate systematic evaluation of this capability, we also introduce VideoCanvasBench, the first benchmark designed specifically for this setting, and demonstrate how our approach outperforms existing methods across range of video completion tasks."
        },
        {
            "title": "2.2 PARADIGMS FOR VIDEO CONDITIONING",
            "content": "Tackling the flexible task of arbitrary spatio-temporal completion requires robust and fine-grained conditioning mechanism. Existing paradigms (Shown in Fig. 2) for injecting condition into video foundation model can be broadly categorized as follows. Latent Replacement. This paradigm, used in LTX-Video and Hunyuan-Video (HaCohen et al., 2024; Kong et al., 2024), was primarily designed for the first-frame image-to-video setting. In this case, overwriting the initial latent with an encoded image remains relatively compatible with the training distribution. However, extending it to arbitrary frames introduces clear traininference mismatch: temporal VAEs compress multiple frames into one latent slot during training, while inference substitutes that slot with single-frame latent. This inconsistency often disrupts temporal dynamics, leading to static frames or motion collapse. Channel Concatenation and Adapter-based Injection. straightforward strategy is to fuse conditional features into the models data stream at fixed locations. Recent I2V models such as CogVideoX (Yang et al., 2024) and Wan (Wang et al., 2025a) adopt this variant by concatenating condition and noisy latents along the channel axis at the input layer. Extensions like T2IAdapter (Mou et al., 2024), VACE (Jiang et al., 2025) and ControlNet (Zhang et al., 2023) process conditions through lightweight encoders before injecting them into intermediate layers. Despite their differences, these approaches share fundamental limitation: pixel-frame-aware control requires feeding zero-padded frames to the VAE, but pre-trained VAEs are not robust to such out-ofdistribution inputs. Making them work would require costly VAE fine-tuning and retraining of the DiT backbone, which is prohibitively expensive for our task. Cross-Attention Injection. Another line of work adds cross-attention layers to incorporate conditioning features, typically for global controls such as text, audio, or style cues (Cui et al., 2025; Meng et al., 2025; Blattmann et al., 2023; Ye et al., 2025b). While effective for holistic guidance, this design requires substantial architectural modifications and introduces many new parameters, limiting scalability. In-Context Conditioning (ICC). ICC, paradigm pioneered in the image domain by OminiControl (Tan et al., 2024) and extended to video by FullDiT (Ju et al., 2025; He et al., 2025), UNIC (Ye et al., 2025a) and LCT (Guo et al., 2025), represents unified, parameter-free conditioning approach. It treats all inputs, including content and conditions, as tokens within single sequence, processed jointly by self-attention. This simple yet effective design allows for flexible conditioning, but ICC still struggles with the key challenge of pixel-frame ambiguity introduced by causal VAEs, which makes precise temporal alignment difficult. Building on ICC, we are the first to adapt this paradigm to the task of arbitrary spatio-temporal video completion. We introduce Hybrid Conditioning Strategy, which decouples spatial and temporal challenges. The novel alignment strategy, Temporal RoPE Interpolation, enables pixel-frame-aware conditioning on frozen causal VAEs, unlocking ICCs full potential for this setting."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 TASK DEFINITION AND PROBLEM SETUP We introduce the task of arbitrary spatio-temporal video completion, unified formulation that generalizes and subsumes wide range of controllable video generation scenarios. Formally, let"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: The pipeline of VideoCanvas, which fine-tunes base T2V model for arbitrary spatiotemporal control with zero new parameters. Our framework leverages the In-Context Conditioning (ICC) paradigm. After preparing conditional patches with zero-padding for spatial placement, we use independent VAE encoding for temporal decoupling. Our RoPE Interpolation then aligns each discrete token by mapping its source pixel-frame index to fractional position /N , where is the VAE temporal stride (here, = 4). As illustrated, this maps Frame 41 to position 10.25. This strategy enables fine-grained control without architectural changes. video be denoted as = {x0, x1, . . . , xT 1} with frames. user provides set of spatiotemporal conditions = {(pi, mi, ti)}M i=1, where pi is an image of the same size as xi, mi is spatial mask specifying its placement within frame, ti [0, 1] is the temporal index of the target frame, and denotes the total number of conditions provided by the user. The goal is to generate coherent video ˆX such that ˆX[ti] mi pi, {1, . . . , }, while simultaneously completing all unconditioned regions with plausible and consistent content. This task naturally unifies many prior settings as special cases: image-to-video (when contains the first full frame), video extension (when provides the first video clip), video inpainting/outpainting (when contains masked regions in video frames), and interpolation (when specifies keyframes at first and last timestamps). By allowing arbitrary spatial masks at arbitrary timestamps, our definition goes strictly beyond these rigid formats, enabling single framework to address them all. 3.2 PRELIMINARIES Video DiT with 3D RoPE. Our work builds upon latent video diffusion model that uses Diffusion Transformer (DiT) backbone (Peebles & Xie, 2023). Crucially, to handle the spatio-temporal nature of video data, the models self-attention mechanism is equipped with 3D Rotary Positional Embeddings (RoPE) (Su et al., 2024). The 3D RoPE integrates both temporal and spatial information by applying rotation to the query and key vectors in the attention mechanism. This rotation is driven by the position of each token in the 3D space (time, height, and width), allowing the model to capture both temporal continuity and spatial relationships. This mechanism is central to our ability to perform precise temporal alignment and interpolation in video generation tasks. Hybrid Causal Video VAE. Modern video foundation models employ Hybrid Causal Video VAE (Zhao et al., 2024; Yang et al., 2024; Wu et al., 2025) that supports both image and video modes. In video mode, the encoder performs causal temporal compression with fixed stride (e.g., = 4). At the beginning of sequence, frames are replicated so that the first latent uniquely corresponds to the first pixel frame. Subsequently, every consecutive frames collapse into single latent slot. For pixel-frame index ti, the latent index is calculated as . with replication ensuring that ti = 0 maps to latent index 0. This stride-based compression is computationally efficient (cid:108) ti (cid:109)"
        },
        {
            "title": "Preprint",
            "content": "but introduces pixel-frame ambiguity: multiple frames (e.g., Frame 1 and Frame 3) may collapse to the same latent, making precise frame-level conditioning non-trivial."
        },
        {
            "title": "3.3 VIDEOCANVAS PIPELINE",
            "content": "To address the challenge of arbitrary spatio-temporal completion, we propose VideoCanvas, unified framework built upon the In-Context Conditioning (ICC) paradigm. We are the first to leverage ICC for this task, introducing novel hybrid conditioning strategy that decouples spatial and temporal alignment, enabling fine-grained, pixel-frame-aware control on frozen VAE and fine-tuned DiT with zero new parameters. The entire pipeline is illustrated in Fig. 3. Conditional Token Preparation. Our hybrid strategy begins by preparing set of conditional latent tokens, illustrated on the left of Fig. 3. For each user-provided condition (pi, mi, ti) P, we first perform Spatial Conditioning. This is done by constructing full-frame canvas, placing the patch pi according to its mask mi, and filling the remaining pixels with zeros, resulting in prepared frame xprep,i = mi pi. Next, for Temporal Decoupling, each prepared frame is encoded independently by the frozen VAE to yield temporally-decoupled conditional latent token zcond,i = E(xprep,i). Unified Sequence Construction and Alignment. Following the ICC paradigm, we construct single, unified sequence. Let zsource be the source video latent (ground-truth), and zcond,i be the i-th conditional latent. We define the full sequence as the concatenation of the prepared conditional tokens and the source video latent: = Concat({zcond,i}M i=1, zsource). We assign standard integer RoPE indices along the frame part onto the source video latent zsource, while we mark the precise temporal location of each conditional token via our Temporal RoPE Interpolation strategy. This assigns each token zcond,i fractional temporal position based on its source pixel-frame index ti with VAE stride : post(zcond,i) = ti . As illustrated in Fig. 3, this maps condition from Frame 41 (where ti = 41 and the VAE stride = 4) to the precise fractional position 10.25. Training Objective. The DiT model, fθ, is fine-tuned with this unified sequence under the flow matching objective (Lipman et al., 2022; Liu et al., 2022). The noising process is applied to the source latent of the sequence. The model input at time is thus combination between the clean condition and the noisy latent: zt = (1 t)zsource + tϵ. The model is trained to predict the velocity field, and the loss only supervises the non-conditional regions: LFM(θ) = (cid:104)(cid:13) (cid:13)fθ(zt, t, ctext) (zsource + ϵ)(cid:1)(cid:13) (cid:13) 2(cid:105) . This objective trains the DiT to treat the conditional tokens as fixed context while generating coherent completion for the target video."
        },
        {
            "title": "4 VIDEOCANVASBENCH",
            "content": "Existing benchmarks focus on rigid tasks such as I2V or outpainting, and cannot assess the flexible spatio-temporal control central to our formulation. We therefore introduce VideoCanvasBench, the first benchmark systematically designed for arbitrary spatio-temporal video completion. The benchmark probes two complementary capabilities: high-fidelity completion within single scene (homologous) and creative synthesis across different sources (non-homologous). It consists of three categories: (1) AnyP2V, using partial patches at fixed anchor timestamps (Start, Middle, End). We construct all seven possible combinationssingle-frame (S, M, E), two-frame (S+M, S+E, M+E), and three-frame (S+M+E)to evaluate interpolation fidelity under varying temporal (2) AnyI2V, using full-frame conditions at the same timestamps, designed to test the sparsity."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Impact of Temporal RoPE Interpolation. Per-frame PSNR for single-frame I2V with targets 2/3/4. Our method (red, solid) peaks exactly at the target frame. w/o RoPE Interpolation (blue, dashed) misaligns, Latent-space Condition (orange, dot-dashed) collapses motion, and Pixel-space Padding (green, dotted) is precise but degraded. completion of full-frame content. (3) AnyV2V, covering video-level completion scenarios such as inpainting, outpainting, and transitions between non-homologous clips. In total, VideoCanvasBench comprises over 2,000 test cases. Further construction details are provided in Appendix C."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Our experiments are designed to answer two central questions: (1) Can our proposed Temporal RoPE Interpolation resolve the temporal ambiguity of causal VAEs, thereby enabling precise pixelframe alignment beyond the native VAE stride? (2) Even under the coarse granularity imposed by latent slots, is the In-Context Conditioning (ICC) paradigm intrinsically more effective than prior mechanisms such as Latent Replacement and Channel Concatenation? We address the first question through an ablation study of different pixel-frame alignment strategies (Sec. 5.3), and the second via paradigm-level comparison on our benchmark (Sec. 5.4). 5.1 SETUP Backbone and Training. We build our framework upon an internal latent video diffusion model, as no existing open-source model is designed for our new task of arbitrary spatio-temporal completion (see Appendix for details). The model is fine-tuned for 20k steps on 650k video clips (384 672 resolution, 5 seconds) using the Adam optimizer with learning rate of 5 105 and batch size of 32 on 32 GPUs. Inference uses 50 DDIM steps with CFG scale of 7.5. Baselines. As our task is new, no existing work provides direct solution. For fair comparison, we compare three representative conditioning paradigms (Fig. 2b) on the same backbone: (1) Latent Replacement, as used in LTX-Video and HunyuanVideo (HaCohen et al., 2024; Kong et al., 2024), (2) Channel Concatenation, widely adopted in CogVideoX and Wan (Yang et al., 2024; Wang et al., 2025a), and (3) our In-Context Conditioning (ICC). All paradigms are trained under identical settings and constrained to the same set of conditionable frames defined by the VAE stride, ensuring rigorous and controlled comparison. More details are shown in the Appendix B. Method PSNR Dynamic Degree Imaging Quality Ours (RoPE Interp.) w/o RoPE Interp. Pixel-space Pad. Latent-space Cond. 23.86 22.95 22.02 25.13 39.75 23.00 30.25 5.00 71.61 70.85 71.50 71. Figure 5: Padding vs. RoPE Interp. Table 1: Ablation on single-frame I2V."
        },
        {
            "title": "5.2 EVALUATION METRICS",
            "content": "Automated Metrics. Fidelity is measured by PSNR and FVD (Unterthiner et al., 2018), and perceptual quality by four metrics: Aesthetic Quality (LAION-AI, 2022), Imaging Quality (Ke et al., 2021), Temporal Coherence (Cai et al., 2025), and Dynamic Degree (Teed & Deng, 2020). User Study. To complement automated metrics, we conducted user study with 25 participants on 30 randomly sampled cases. In each case, participants viewed side-by-side outputs from three methods in three-way forced-choice setting, and rated them along three axes: Visual Quality (quality and dynamics), Semantic Quality (faithfulness to text and images), and Overall Preference (holistic choice). Results are reported as win rates (%) over competing methods."
        },
        {
            "title": "5.3 ABLATION STUDY: PIXEL-FRAME ALIGNMENT STRATEGIES",
            "content": "As discussed in Fig. 2 (a), causal video VAEs map several pixel frames into one latent, which creates ambiguity when conditioning on specific frame. One intuitive workaround is to keep the target frame and pad the rest with zeros before VAE encoding, which we denote as Pixel-space Padding. While this approach is temporally precise, it forces the frozen VAE to process highly outof-distribution inputs, often corrupting colors and textures. To disentangle this issue, we compare four alignment strategies: (i) Latent-space Conditioning: encode the entire video with the VAE (video mode) to obtain latent sequence; at designated timestamps, the corresponding latent slice is injected as the conditional input. (ii) Pixel-space Padding: construct pixel-space video in which non-target frames are zeroed; encode the entire padded video with the VAE (video mode) (iii) w/o RoPE Interpolation: encode each conditional frame independently with the VAE (image mode); assign each conditional token to the discrete temporal slot determined by the VAE compression window (no interpolation). (iv) Our full method with Temporal RoPE Interpolation. Qualitative evidence. Although pixel-space padding can in principle point to the correct frame, it introduces visible artifacts because the VAE never trained on zero-filled inputs. Fig. 5 illustrates this: the padded result shows clear color shifts and texture wash-out, whereas RoPE-based alignment preserves the conditional frame with high fidelity. Quantitative analysis. We further evaluate single-frame I2V at target indices (2, 3, 4). As shown in Fig. 4 and Tab. 1, Latent-space Conditioning yields nearly flat PSNR curve, indicating motion collapse. w/o RoPE Interpolation recovers dynamics but shifts the PSNR peaks due to slot misalignment. Pixel-space Padding peaks at the correct indices but with lower overall fidelity. In contrast, our method with RoPE Interpolation aligns exactly to the target frames and achieves the best fidelity. Together, these results make two points clear. First, padding-based solutions, despite being temporally precise, degrade quality due to VAE signal corruption. Second, latent-space conditioning and integer-only alignment cannot resolve frame-level ambiguity. In contrast, our ICC with Temporal RoPE Interpolation uniquely provides both fine-grained control and high-fidelity generation. 5.4 MAIN RESULTS: PARADIGM COMPARISON Having established that padding-based solutions are impractical due to quality degradation, we next compare the three representative conditioning paradigmsLatent Replacement, Channel Concatenation, and our In-Context Conditioningunder identical settings, where each latent corresponds to pixel frame. This ensures that performance differences arise solely from the conditioning mechanism itself (not zero-padding). Quantitative Comparison. Tab. 2 shows results on VideoCanvasBench across three task categories: AnyP2V, AnyI2V and AnyV2V. The data reveals consistent trend across all task categories. Latent Replacement achieves deceptively high scores in static similarity metrics like PSNR, but at the cost of synthesizing motion. Its extremely low Dynamic Degree scores indicate that it generates nearly frozen videos, which is reflected in its poor FVD, confirming large distributional gap with real videos. Channel Concatenation produces more dynamics but consistently lags behind our method in both reference fidelity (PSNR, FVD) and key perceptual metrics. In contrast, our ICC strikes the best balance, achieving competitive fidelity while attaining the highest Dynamic Degree. Most importantly, the user study confirms ICCs superiority, where it is overwhelmingly preferred by human evaluators across all three task levels."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance comparison on our VideoCanvasBench across three task categories. The best result is in bold and the second best is underscored. Higher scores are better for all metrics (except FVD). Abbreviations: AQ = Aesthetic Quality, IQ = Imaging Quality, TC = Temporal Coherence, DD = Dynamic Degree, VQ = Visual Quality, SQ = Semantic Quality, OP = Overall Preference. Method Reference Fidelity Perceptual Metrics (%) User Study (%) PSNR FVD AQ IQ TC DD VQ SQ OP AnyP2V (First Frame, First-Last Frames and Any Keyframes) Replace. (Kong et al., 2024) Channel. (Yang et al., 2024) ICC (Ours) 24.29 23.73 23.83 19.335 18.147 17.553 55.45 69.19 91.04 21.00 14.62 16.15 14.23 55.54 68.49 89.36 39.44 26.54 26.92 25.38 55.53 68.87 89.71 40.44 58.85 56.92 60.38 AnyI2V (First Frame, First-Last Frames and Any Keyframes) Replace. (Kong et al., 2024) Channel. (Yang et al., 2024) ICC (Ours) 26.72 25.83 26.06 12.534 10.947 10.805 55.64 69.32 90.37 24.22 7.31 55.37 68.74 88.88 41.22 23.46 27.31 24.23 55.40 69.25 89.02 44.78 68.08 65.38 68.46 7.31 8.46 AnyV2V (Video Transition, Video Inpainting and Outpainting) Replace. (Kong et al., 2024) Channel. (Yang et al., 2024) ICC (Ours) 23.90 23.54 23.68 15.958 11.371 10.252 53.28 67.23 89.37 47.39 5.00 54.16 68.33 88.88 53.04 26.92 25.77 25.38 53.16 69.43 89.40 53.20 68.08 70.00 69.62 4.23 5. Figure 6: Qualitative comparison. Our method preserves high quality and smooth transitions. Qualitative Comparison. Fig. 6 illustrates representative cases. In the two-frame I2V task (Fig. 6 a), Latent Replacement collapses into static repetition around the conditioning frame, while Channel Concatenation introduces unnatural distortions in the deers body. ICC instead generates smooth and plausible motion while maintaining identity. In the more challenging two-frame P2V setting (Fig. 6 b), the weaknesses of the baselines become even clearer. Latent Replacement produces abrupt, unnatural transitions, and Channel Concatenation suffers from severe identity drift, with kangaroo inexplicably mutating into dog mid-video. ICC alone preserves motion, identity, and structural consistency throughout the sequence, avoiding both freezing and semantic corruption. Overall, both quantitative and qualitative evidence converge on the same conclusion. Our ablation study (Sec. 5.3) demonstrates that Temporal RoPE Interpolation uniquely enables fine-grained pixelframe alignment without sacrificing fidelity, while the paradigm comparison (Sec. 5.4) shows that even at coarse latent-level granularity, ICC consistently outperforms Latent Replacement and Channel Concatenation. Taken together, these findings establish ICC as the most robust and effective conditioning mechanism for arbitrary spatio-temporal video generation. 5.5 APPLICATIONS AND EMERGING CAPABILITIES Beyond outperforming existing paradigms in controlled comparisons, the true strength of our VideoCanvas framework lies in the versatile and creative applications it unlocks. By treating video syn-"
        },
        {
            "title": "Preprint",
            "content": "thesis as unified completion problem, our model exhibits several powerful emerging capabilities, as showcased in our teaser  (Fig. 1)  and appendix (Sec. E). Flexible Temporal Control (AnyI2V). The first key capability enabled by our Temporal RoPE Interpolation is fine-grained control over arbitrary timestamps. As demonstrated in the AnyI2V examples, our model breaks free from the constraints of first-frame or first-last-frame settings. It can generate coherent video narrative from any number of full-frame conditions placed at any point along the timeline, successfully handling complex interpolation and extrapolation scenarios that are ill-defined for prior methods. Arbitrary Spatio-Temporal Control (AnyP2V). Building upon this temporal flexibility, our framework achieves true spatio-temporal control. The AnyP2V task showcases this core capability, where the model generates complete video from sparse set of disconnected patches, each placed at an arbitrary spatial location and an arbitrary timestamp. As shown in our results, the model can successfully synthesize plausible motion and context while maintaining the identity of the conditioned objects, even when the provided patches are very small. This demonstrates sophisticated ability to reason jointly about what, where, and when. Creative Video Transition. Leveraging its ability to handle non-homologous conditions, our model excels at creative synthesis. As shown in the Video Transition task, it can generate seamless and logical evolution between two entirely different scenes (e.g., morphing drone into butterfly). This demonstrates sophisticated ability to understand and interpolate high-level semantics, capability inspired by recent state-of-the-art generative models. Long-Duration Video Extension. Our framework enables long-duration synthesis by iteratively applying completion. short clip can be extended up to minute in length by autoregressively generating the next segment, conditioned on the end of the previous one. This process can be guided by interactive text prompts to evolve the narrative and can even be used to create perfectly seamless loops by generating transition from the videos end back to its beginning. Unified Video Painting and Camera Control. Furthermore, our spatio-temporal canvas formulation naturally subsumes variety of other tasks. By providing masked videos as conditions, the model performs both inpainting and outpainting. By progressively translating or scaling the conditional frames on the canvas, it can emulate dynamic camera effects such as zooms and pans, showcasing its potential for creative post-production."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced and formalized the task of arbitrary spatio-temporal video completion. To tackle the core challenge of temporal ambiguity in causal VAEs, we proposed VideoCanvas, framework based on In-Context Conditioning. We also propose hybrid conditioning strategy combining Spatial Zero-Padding and Temporal RoPE Interpolation, first to enable fine-grained, pixel-frame-aware control on frozen VAE via efficient DiT fine-tuning. Beyond strong quantitative and qualitative results on our new benchmark, VideoCanvasBench, our approach demonstrates remarkable flexibility across various applications, such as arbitrary spatiotemporal patches to video, any-timestamp images to video, long-duration extension, painting and camera control. We believe this work establishes robust and generalizable foundation for the next generation of controllable video synthesis. Discussion. Most leading video foundation models use causal VAEs not pre-trained on zero-padded temporal data, making them incompatible with naive zero-padding for arbitrary spatio-temporal control. Such inputs would cause distribution shifts, requiring costly re-training of both VAEs and DiT backbones. Our model-centric framework circumvents this by enabling fine-grained control without modifying the frozen VAE, while future foundation models may incorporate similar capabilities during pre-training with zero-padded data, making data-centric paradigms complementary path for further progress. Furthermore, our independent frame encoding, while highly effective for sparse conditions, presents computational trade-off for dense inputs. Future work could explore hybrid mechanisms that combine our fine-grained alignment with more efficient token pruning strategies for dense conditional sequences. Overall, we believe our work provides robust and generalizable foundation, inspiring further research into flexible and unified video synthesis."
        },
        {
            "title": "REFERENCES",
            "content": "Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1579115801, 2025. Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. arXiv preprint arXiv:2503.05639, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 77637772, June 2025. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2108621095, 2025. Kuntai Du, Ahsan Pervaiz, Xin Yuan, Aakanksha Chowdhery, Qizheng Zhang, Henry Hoffmann, and Junchen Jiang. Server-driven video streaming for deep learning inference. In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, pp. 557 570, 2020. Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, and Tianfan Xue. Lora-edit: Controllable first-frame-guided video editing via mask-aware lora fine-tuning. arXiv preprint arXiv:2506.10082, 2025. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Xuanhua He, Quande Liu, Zixuan Ye, Wecai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, and Kun Gai. Fulldit2: Efficient in-context conditioning for video diffusion transformers. arXiv preprint arXiv:2506.04213, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024."
        },
        {
            "title": "Preprint",
            "content": "Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: Allin-one video creation and editing. ArXiv, abs/2503.07598, 2025. URL https://api. semanticscholar.org/CorpusID:276928131. Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: multi-scale image quality transformer. CoRR, abs/2108.05997, 2021. URL https://arxiv.org/abs/2108. 05997. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. LAION-AI. aesthetic-predictor. aesthetic-predictor, 2022. https://github.com/LAION-AI/ Tianhong Li, Vibhaalakshmi Sivaraman, Pantea Karimi, Lijie Fan, Mohammad Alizadeh, and Dina Katabi. Reparo: Loss-resilient generative codec for video conferencing. arXiv preprint arXiv:2305.14135, 2023. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semi-body human animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 54895498, 2025. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 4296 4304, 2024. OpenAI. Video generation models as world simulators. https://openai.com/index/ video-generation-models-as-world-simulators/, 2023. Accessed: 2024-2. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41954205, October 2023. Pexels. Pexels license. https://www.pexels.com/license/, 2025. Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024."
        },
        {
            "title": "Preprint",
            "content": "Zachary Teed and Jia Deng. Raft: Recurrent all pairs field transforms for optical flow. In ECCV, 2020. Ultralytics. Yolov8. https://github.com/ultralytics/ultralytics, 2023. Unsplash. Unsplash license. https://unsplash.com/license, 2025. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. ArXiv, abs/1812.01717, 2018. URL https://api.semanticscholar.org/ CorpusID:54458806. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025a. Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Be-your-outpainter: Mastering video outpainting through input-specific adaptation. In European Conference on Computer Vision, pp. 153168. Springer, 2024. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 84288437, 2025b. Pingyu Wu, Kai Zhu, Yu Liu, Liming Zhao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Improved video vae for latent video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1812418133, 2025. Jiazhi Yang, Kashyap Chitta, Shenyuan Gao, Long Chen, Yuqian Shao, Xiaosong Jia, Hongyang Li, Andreas Geiger, Xiangyu Yue, and Li Chen. Resim: Reliable world simulation for autonomous driving. arXiv preprint arXiv:2506.09981, 2025a. Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, and Jian Zhang. Gencompositor: Generative video compositing with diffusion transformer. arXiv preprint arXiv:2509.02460, 2025b. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025a. Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 26302640, 2025b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023."
        },
        {
            "title": "Preprint",
            "content": "Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. Advances in Neural Information Processing Systems, 37:1284712871, 2024. Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. Propainter: Improving propagation and transformer for video inpainting. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1047710486, 2023."
        },
        {
            "title": "APPENDIX",
            "content": "A INTRODUCTION OF THE BASE TEXT-TO-VIDEO GENERATION MODEL We use an internal transformer-based latent diffusion model (Peebles & Xie, 2023) as the base T2V generation model, as illustrated in Fig. S7. We employ 3D-VAE to transform videos from the pixel space to latent space, upon which we construct transformer-based video diffusion model. Unlike previous models that rely on UNets or transformers, which typically incorporate an additional 1D temporal attention module for video generation, such spatially-temporally separated designs do not yield optimal results. We replace the 1D temporal attention with 3D self-attention, enabling the model to effectively perceive and process spatiotemporal tokens, thereby achieving high-quality and coherent video generation model. Specifically, before each attention or feed-forward network (FFN) module, we map the timestep to scale, thereby applying RMSNorm to the spatiotemporal tokens. Figure S7: Overview of the base text-to-video generation model."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Conditioning paradigms. Since no existing work is trained for our new task of arbitrary spatiotemporal completion, we re-implement three representative paradigms on the same base model for fair comparison (Fig. 2b), following the references used in the main text: Latent Replacement (HaCohen et al., 2024; Kong et al., 2024). For given conditional frame, the corresponding latent tokens are overwritten with VAE-encoded ground-truth latents. Training applies masked loss only to non-conditional regions, while conditional regions are assigned timestep 0. Channel Concatenation (Yang et al., 2024; Wang et al., 2025a). Condition frames are encoded into latents, assembled into zero-padded latent sequence, and concatenated with the noisy latent sequence along the channel dimension. learnable projection layer then restores the embedding dimension. In our implementation, concatenation is applied after patchification, as this setting empirically yields the best results; applying it before patchification leads to degraded visual quality. The tradeoff is that after-patchify concatenation substantially increases the channel dimensionality, resulting in projection layer with 16.6M trainable parameters. Thus, while this design enriches the conditioning signal and improves learning, it comes at the cost of significantly more parameters compared to the other paradigms. In-Context Conditioning (ICC) (Tan et al., 2024; Ju et al., 2025). Our method encodes condition frames into clean latent tokens and concatenates them with the noisy sequence along the token"
        },
        {
            "title": "Preprint",
            "content": "dimension. Temporal alignment is achieved with our RoPE Interpolation strategy (Sec. 3.3). The loss is applied only to noisy tokens, while conditional tokens are assigned timestep 0. This design requires no additional trainable parameters. All paradigms are trained under identical settings and restricted to the same set of conditionable frames defined by the VAE stride, ensuring rigorous and controlled comparison. Temporal granularity. Different conditioning paradigms impose different constraints on the indices where conditional frames can be applied. As discussed in Fig. 2 and Sec. 5.3, both Channel Concatenation and Latent Replacement require zero-padded inputs in order to achieve pixel-frameaware control, which significantly degrades quality. To ensure fair comparison across paradigms, we therefore restrict all methods to coarser temporal granularity. Specifically, with VAE temporal stride of 4, the conditionable frame indices are standardized to the discrete set {0, 4, 8, . . . , 76}. This guarantees that each method is trained on exactly the same set of indices and receives comparable supervision. Training strategy. At each iteration, three frames are randomly sampled from source video to serve as temporal anchors. From each anchor frame, we extract spatial region by cropping patch covering between 20%100% of the original frame size. This unified training strategy ensures that the model encounters diverse spectrum of conditioning scenarios, ranging from sparse local patches to nearly complete frames, and from early anchors to late anchors. Such exposure allows the model to learn arbitrary spatio-temporal conditioning in single framework. B.1 DETAILED EVALUATION METRICS This section provides additional details of the evaluation metrics used in our experiments. As described in the main paper, we evaluate video generation quality using PSNR, FVD, Aesthetic Quality, Imaging Quality, Temporal Coherence, and Dynamic Degree, together with complementary user study. Our protocol is designed to measure two aspects: (1) fidelity to visual conditions when ground-truth is available, and (2) perceptual quality and temporal consistency in general cases. Reference-based Visual Fidelity. We use two complementary metrics to evaluate reference-based visual fidelity: PSNR: We adopt PSNR to evaluate reconstruction accuracy in the conditional regions, measuring how faithfully the generated outputs reproduce the provided visual inputs. This metric focuses on pixel-level reconstruction accuracy and is widely used for assessing the quality of generated images and videos. Frechet Video Distance (FVD) (Unterthiner et al., 2018): We also employ FVD to measure the distance between the distributions of generated and real video sequences, capturing both temporal and spatial information. lower FVD indicates higher similarity between the generated video and real videos, reflecting better overall quality. This metric is particularly useful for comparing video generation models by assessing their ability to match the distribution of real-world videos. However, FVD is only applicable to the parts of our dataset with ground-truth videos (i.e., homologous videos). For non-homologous image-to-video and video transition tasks, where no ground truth exists, we do not compute FVD. Perceptual Quality and Consistency Metrics. We further employ the following metrics to comprehensively assess perceptual quality and temporal behavior: Aesthetic Quality (LAION-AI, 2022): evaluates the artistic and aesthetic value of each frame using the LAION aesthetic predictor. It reflects high-level properties such as composition, color harmony, photo-realism, and naturalness. Imaging Quality (Ke et al., 2021): measures the absence of low-level distortions using the MUSIQ predictor. It is sensitive to degradations such as over-exposure, noise, and blur, thereby reflecting frame-level fidelity from perceptual perspective."
        },
        {
            "title": "Preprint",
            "content": "Temporal Coherence (Cai et al., 2025): evaluates temporal stability by computing CLIP feature similarity (CSCV) between adjacent frames. This is modification of the Background Consistency metric from VBench (Huang et al., 2024), which compared all frames to the first frame. That design fails for videos with significant camera motion or large scene transition, where the first frame is not valid reference. This adjacent-frame-only formulation provides more robust measure of local temporal smoothness. Dynamic Degree (Teed & Deng, 2020): quantifies the level of motion by estimating optical flow magnitudes with RAFT. This prevents models from trivially achieving high consistency with static outputs, and explicitly rewards natural, dynamic motion. User Study. To complement automated metrics, we conducted user study with 25 participants on 30 randomly sampled cases. Each case contained outputs from three methods, presented in threeway forced-choice setting. Participants rated results along three axes: (1) Visual Quality (quality and dynamics), (2) Semantic Quality (faithfulness to text and images), and (3) Overall Preference (holistic choice). We report results as win rates (%) over competing methods."
        },
        {
            "title": "C VIDEOCANVASBENCH CONSTRUCTION DETAILS",
            "content": "This section provides comprehensive overview of the data curation and task generation pipeline for VideoCanvasBench, the first systematic evaluation suite for arbitrary spatio-temporal video completion. C.1 DATA CURATION We curate two complementary types of sources: (1) homologous videos for testing fidelity within single coherent scene, and (2) non-homologous images and videos for evaluating creativity across distinct content. Homologous Video Set (100 Videos). We began with an initial pool of 2,000 videos from Pexels (Pexels, 2025). multi-stage filtering pipeline was applied to ensure quality and diversity: Blur filtering: blurry videos were removed by calculating the CV2.Laplacian (Bradski, 2000) score for each frame and excluding those below threshold of 200. Motion filtering: static or nearly-static clips were excluded using RAFT-based motion magnitude thresholds exceeding 5 (Teed & Deng, 2020). Length filtering: only videos longer than 5 seconds were retained. From this pool, we selected 100 diverse, high-quality clips covering wide range of scenes (e.g., human activities, animals, landscapes). All were standardized to 77 frames at 15 FPS to provide consistent evaluation length. Each video is paired with captions generated by captioning model fine-tuned on Koala36M (Wang et al., 2025b) following the LLaVA-based (Liu et al., 2023) annotation pipeline. All captions are further verified by human annotators to ensure accuracy in both content and motion descriptions. Non-Homologous Image and Video Sets. To test the ability to synthesize across unrelated contexts, we manually curated visually distinct sources from Pexels (Pexels, 2025) and Unsplash (Unsplash, 2025), ensuring large appearance and semantic gaps. The set includes: 50 pairs of non-homologous images, selected to maximize dissimilarity (e.g., indoor vs. outdoor, object vs. scene). 50 triplets of non-homologous images, further increasing combinatorial diversity. 30 pairs of non-homologous video clips, curated for challenging video transitions, similar to the blending function of Sora (OpenAI, 2023). These non-homologous cases explicitly test the models capacity for creative interpolation and crossscene reasoning. Each non-homologous source is annotated with captions automatically generated by Gemini 2.5 Pro (Comanici et al., 2025) and manually corrected to ensure faithful descriptions of both appearance and motion."
        },
        {
            "title": "Preprint",
            "content": "C.2 BENCHMARK TASK DEFINITIONS Task 1: AnyI2V (Any-Timestamp Image-to-Video). This task uses full frames as conditions to test temporal reasoning and interpolation fidelity. We explicitly construct nine sub-tasks by combining conditions from fixed temporal anchors: start (frame 1), middle (frame 41), and end (frame 77). Homologous cases. From each source video we sample three anchor frames (start, middle, end), and construct: Single-frame I2V: start video, middle video, end video. Two-frame I2V: start+end video, start+middle video, middle+end video. Three-frame I2V: start+middle+end video. Non-homologous cases. For curated pairs of images, we construct the three two-frame tasks (start+end, start+middle, middle+end). For curated triplets of images, we construct the threeframe task (start+middle+end). Each non-homologous source is annotated with captions automatically generated by Gemini 2.5 Pro (Comanici et al., 2025) and manually checked for accuracy. Task 2: AnyP2V (Any-Timestamp Patch-to-Video). This variant follows the same nine sub-task definitions as AnyI2V setting, but replaces each full-frame condition with cropped patch. Patch extraction. For each conditional frame, patches are obtained via semi-automated process: 50% object-aware masks using SAM (Kirillov et al., 2023) or YOLO (Ultralytics, 2023), and 50% random crops. Temporal anchors. The same start, middle, and end frame positions are used to construct single-, two-, and three-frame variants, for both homologous and non-homologous cases. Difficulty. The subset explicitly includes challenging cases with very small subjects, requiring the model to extrapolate from minimal context. Task 3: AnyV2V (Transition, Inpainting and Outpainting). This task evaluates more general video-level completion scenarios beyond frameor patch-level control. It consists of three subcategories: Video Transition. For 30 curated pairs of non-homologous video clips, the first clip provides the start segment and the second the end segment, while the model synthesizes the intermediate transition. This setup parallels the blending function explored in Sora (OpenAI, 2023). Each case is annotated with captions generated by Gemini 2.5 Pro (Comanici et al., 2025) and manually corrected to ensure faithful descriptions of both content and motion. Inpainting. For homologous videos, interior rectangular masks are applied to each frame, covering 20%50% of the width/height. The model must fill the missing regions with temporally consistent content. Outpainting. Boundary masks are applied to crop the central region, masking out 60%90% of the width/height. The model is required to extrapolate plausible outer regions beyond the visible content. C.3 SCALE In total, VideoCanvasBench includes over 2,000 test cases: 900 for AnyP2V, 900 for AnyI2V, and 230 for AnyV2V. Each case is designed to probe specific aspect of fidelity, creativity, or temporal reasoning in the proposed unified task. C.4 LICENSING AND ANNOTATIONS. All videos in our benchmark are sourced from Pexels (Pexels, 2025), and images are sourced from both Pexels and Unsplash (Unsplash, 2025). Content on Pexels is provided under the Pexels License, which permits free use for commercial and non-commercial purposes without requiring attribution, with restrictions against reselling unaltered copies, use in trademarks, or misuse of identifiable people or brands. subset of Pexels content is explicitly marked as Creative Commons Zero (CC0),"
        },
        {
            "title": "Preprint",
            "content": "which places the work in the public domain. Unsplash photos are provided under the Unsplash License, which similarly allows free commercial and non-commercial use without attribution, while prohibiting resale of unaltered content, creation of competing stock services, or misleading association with brands or people. In both cases, all curated data is legally licensed for academic research use. Captions generated by Gemini 2.5 Pro (Comanici et al., 2025) were manually verified by the authors to ensure accuracy and consistency across all benchmark cases."
        },
        {
            "title": "D ADDITIONAL ANALYSIS",
            "content": "D.1 ANALYSIS OF ZERO-PADDED INPUTS In Section 3, we describe using zero-padding to indicate unconditioned regions when preparing conditional frames. This approach is crucial for our spatial conditioning strategy, as it allows us to precisely specify the location of condition patch within frame without modifying the pre-trained VAE backbone. However, critical question arises: can standard hybrid video VAE, trained on natural images and videos, effectively handle inputs that contain large areas of zero-valued pixels (i.e., spatial padding)? As illustrated in Figure S8 and Figure S9, this distinction between spatial and temporal padding is fundamental to understanding our method. To address this, we conducted an empirical study using two popular pre-trained VAE models: Hunyuan I2V and CogVideo. We evaluated their robustness to both spatial and temporal padding under realistic conditions. Setup. We collected 20 diverse full-resolution images and 20 short video clips from YouTube, representing wide range of content (e.g., landscapes, cityscapes, indoor scenes, moving vehicles). For each image, we applied random spatial zero-padding masks, covering approximately 40-60% of the pixels. For each video clip, we created three types of padded inputs: 1. video with conditional frames containing the original content, while all other frames are filled with zeros (pure temporal padding). 2. video where conditional frames contains cropped region of the original content, with all other frames being zero (temporal & spatial padding). Each input was then encoded and decoded using the two hybird VAE model. We measured the reconstruction fidelity using PSNR and qualitatively inspected the outputs. Results. The results provide clear evidence of the differential impact of padding modes: Spatial Padding Robustness: As shown in Figure S8, both VAE models demonstrate remarkable tolerance to spatial zero-padding. The average PSNR of reconstructed images with spatial padding is only marginally lower than that of the baseline (full image), with an average drop of 0.89 dB(Hunyuan I2V) and 1.13 dB(CogVideo). Temporal Padding Vulnerability: In stark contrast, Figure S9 reveals the limitations of traditional approaches. When applying temporal zero-padding (encoding single frame into sequence where most frames are zero), both VAE models exhibit dramatic degradation in reconstruction quality. The average PSNR drops by over 6.12 dB(Hunyuan I2V) and 7.01 dB(CogVideo) compared to the baseline. Conclusion. These findings confirm that the key to achieving pixel-frame-aware control lies in decoupling spatial and temporal handling. Our method leverages the inherent robustness of the VAE to spatial padding while bypassing the ineffectiveness of temporal padding through our proposed Temporal RoPE Interpolation. This separation allows us to harness the power of frozen, pretrained VAE for flexible and high-fidelity video completion, avoiding the need for costly retraining or architectural modifications. The experimental results thus strongly validate the necessity and effectiveness of our approach."
        },
        {
            "title": "Preprint",
            "content": "Figure S8: Robustness of Hybrid Video VAEs to Spatial Padding. This figure demonstrates that both the Hunyuan I2V and CogVideo VAE models can tolerate spatial zero-padding well. When reconstructing images with large zero-padded regions (middle row), the PSNR values are only slightly lower than those of the full, unpadded images (top row). Crucially, the original content within the non-zero regions is faithfully preserved, while the padded areas remain visually neutral. This empirical evidence confirms that our spatial conditioning strategy, which relies on zero-padding before VAE encoding, is stable and practical, enabling precise spatial control without degrading the quality of the conditioned content. D.2 ANALYSIS OF TEMPORAL ROPE INTERPOLATION Figure 4 in the main paper has shown that our Temporal RoPE Interpolation achieves precise oneto-one alignment between condition frames and their target temporal positions. Here we further demonstrate that such pixel-frame-level precision is not only feasible, but also crucial for improving video completion quality. To this end, we conduct an additional experiment on the homologous video set (100 videos) from VideoCanvasBench. Each video contains 77 frames. We compare two conditioning strategies:"
        },
        {
            "title": "Preprint",
            "content": "Figure S9: Vulnerability of Hybrid Video VAEs to Temporal Padding. This figure contrasts the robustness observed in spatial padding. When applying temporal zero-padding (where only specific frames contain content), both VAE models suffer relatively great drop in reconstruction quality. The PSNR values for the padded reconstructions (bottom rows) are much lower than those of the full video (top row), demonstrating degradation in fidelity. The reconstructed frames exhibit noticeable color shifts, and loss of detail, highlighting that the VAE cannot handle such distributionally mismatched inputs. This mode underscores why direct temporal zero-padding is ineffective and validates the necessity of our Temporal RoPE Interpolation strategy, which avoids this problem by operating at the latent token level with fractional positions. Sparse condition: only the 0th and 4th frames are provided, and the model interpolates the missing frames implicitly. Dense condition: the 0th4th frames are explicitly provided, ensuring frame-wise alignment at every step. Both settings are used to generate the full 77-frame video. We evaluate fidelity by computing PSNR between the generated outputs and the original ground-truth video, focusing on the first 5 frames, and report averages over all 100 videos. Table R3: Average PSNR (dB) across 100 videos under sparse vs. dense conditioning. Condition Type Conditioned Frames Sparse (two frames) Dense (five frames) 0, 4 0, 1, 2, 3, 4 PSNR () 24.789 25."
        },
        {
            "title": "Preprint",
            "content": "The results indicate that explicitly conditioning on consecutive frames yields consistently higher PSNR, demonstrating that RoPE Interpolation not only ensures precise alignment at arbitrary timestamps (as shown in Figure 4) but also effectively leverages dense temporal cues to improve reconstruction fidelity. This finding highlights the flexibility of VideoCanvas: it can operate effectively under sparse conditions for efficient zero-shot completion, while also benefiting from denser conditions that offer stronger temporal guidance, particularly in long-horizon generation where they mitigate motion collapse compared to first-frame-only baselines. D.3 TRAINING AND INFERENCE COST Tab. R4 summarizes the computational cost of different conditioning paradigms. Our ICC approach introduces no additional parameters, whereas channel concatenation requires projection layer with 16.6M parameters. Training ICC takes slightly longer (24.5h vs. 2122h) due to the additional conditioning tokens in the sequence. During inference, our method exhibits small overhead that grows with the number of conditional frames (168s 175s 184s), because longer context increases the sequence length processed by the transformer. While this makes inference marginally slower than the baselines with fixed cost, the trade-off is acceptable since ICC consistently achieves the best fidelity and alignment results (see Sec. 5.3, Tab. 1, Fig. 4 and Tab. 2). Table R4: Training and inference cost comparison across paradigms. Training time is measured over 20k steps. Inference time is per 77-frame video at 384 672 with different numbers of conditional frames. Method Params Train Inference Latent Replacement Channel Concat Ours 0 21.47h 16.6M 22.47h 24.54h 1 frame 2 frame 3 frame 159s 164s 168s 159s 164s 175s 159s 164s 184s"
        },
        {
            "title": "E APPLICATIONS AND QUALITATIVE RESULTS",
            "content": "The teaser figure  (Fig. 1)  has shown some cases, and in this section, we provide extensive qualitative results to demonstrate the versatility and effectiveness of our VideoCanvas framework across wide range of applications. The following figures showcase side-by-side comparisons with baseline paradigms, illustrating our methods superior performance in motion smoothness, detail sharpness, and temporal consistency. Any-Timestamp Patch-to-Video (AnyP2V). In Figure S10, we demonstrate our core capability of generating complete video from varying number of sparse patches. We showcase challenging scenarios using one, two, three, and even four conditional patches, placed at arbitrary timestamps to rigorously test the models spatio-temporal reasoning beyond simple first-frame conditioning. Any-Timestamp Image-to-Video (AnyI2V). Figure S11 illustrates the flexibility of our framework on full-frame conditions. The examples include standard cases like first-frame I2V and first-last-frame interpolation, as well as more challenging scenarios where conditions are placed at arbitrary middle timestamps, capability not well supported by prior methods. Video-Level Completion and Creation (AnyV2V). Our framework naturally unifies variety of video editing and creation tasks within single model. We provide examples of: Video Transition: Creative transitions between non-homologous clips are demonstrated in Figure S12. Video Painting: Inpainting and outpainting results are shown in Figure S13, where the red dashed contours indicate the generated regions. Video Extension and Looping: As demonstrated in Figure S14, we showcase long-duration synthesis by extending short clips to over minute in length while maintaining temporal consistency. This capability can be guided by interactive text prompts to evolve the narrative."
        },
        {
            "title": "Preprint",
            "content": "Furthermore, we can create perfectly seamless loops by generating smooth transition from the videos end back to its beginning. Our approach leverages motion context from the last segments frames to effectively avoid the stuttering artifacts that are common in naive firstlast frame-looping methods. Video Camera Control: As demonstrated in Figure S15, our framework can emulate camera cinematography by progressively translating or scaling content on the spatio-temporal canvas. This enables variety of standard camera effects, such as zooms and pans. We showcase this capability by applying dynamic camera movements to classic movie shots, demonstrating its potential for creative post-production. Finally, Figure S16 provides additional direct comparisons against baseline paradigms across diverse set of challenging cases, further highlighting the robustness and superiority of our approach."
        },
        {
            "title": "Preprint",
            "content": "Figure S10: Results on Any-timestamp Patches to Videos."
        },
        {
            "title": "Preprint",
            "content": "Figure S11: Results on Any-timestamp Images to Videos."
        },
        {
            "title": "Preprint",
            "content": "Figure S12: Results on Video Transition."
        },
        {
            "title": "Preprint",
            "content": "Figure S13: Results on Video Inpainting and Outpainting. The red dashed contours indicate the regions that are subject to inpainting or outpainting."
        },
        {
            "title": "Preprint",
            "content": "Figure S14: Results on Video Extension and Seamless Looping. The example showcases video extended to over 1,000 frames by first applying our video extension capability and then generating seamless transition back to the initial state. This highlights our models ability to maintain temporal consistency and visual quality over long generation horizon without suffering from quality degradation or motion collapse."
        },
        {
            "title": "Preprint",
            "content": "Figure S15: Results on Video Camera Control. The examples showcase emulated camera effects such as zoom and pan, achieved by progressively translating and scaling content on the spatiotemporal canvas."
        },
        {
            "title": "Preprint",
            "content": "Figure S16: Comparisons with baseline paradigms."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "MMLab, The Chinese University of Hong Kong"
    ]
}