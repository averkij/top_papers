{
    "paper_title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
    "authors": [
        "Soroush Mehraban",
        "Vida Adeli",
        "Jacob Rommann",
        "Babak Taati",
        "Kyryl Truskovskyi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 6 4 5 7 0 . 0 1 5 2 : r PICKSTYLE: VIDEO-TO-VIDEO STYLE TRANSFER WITH CONTEXT-STYLE ADAPTERS Soroush Mehraban1,2,3*, Vida Adeli1,2,3* Jacob Rommann1, Babak Taati2,3, Kyryl Truskovskyi1 1Pickford AI 2University of Toronto 3Vector Institute * Equal contribution Figure 1: PICKSTYLE addresses video-to-video style transfer by preserving motion and context while translating videos into diverse styles. Unlike prior methods that treat the task as artistic style transfer (colortexture statistics while ignoring geometric properties of the target style) and that often suffer from style degradation, visual inconsistency and temporal flicker, PICKSTYLE produces coherent translations across nine styles."
        },
        {
            "title": "ABSTRACT",
            "content": "We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in target style specified by text prompt. major challenge is the lack of paired video data for supervision. We propose PICKSTYLE, video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with sourcestyle correspondences for training. PICKSTYLE inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motionstyle transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we inProject page: https://pickstyle.pickford.ai/ 1 troduce ContextStyle Classifier-Free Guidance (CSCFG), novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CSCFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in video diffusion models enable the generation of realistic, temporally coherent videos (Wan et al., 2025; Kong et al., 2024; HaCohen et al., 2024). Following these advances, growing body of research explores ways to add controllability to text-to-video diffusion models, enabling finer-grained guidance over the generated content (He et al., 2025; Burgert et al., 2025; Jiang et al., 2025). While style transfer has advanced significantly for images, improvements in the video domain remain limited. This limitation is largely due to the scarcity of well-curated paired video datasets spanning diverse styles, in contrast to the abundance of such resources for images. To mitigate data limitations, several methods (Yang et al., 2024; 2023) leverage image priors to apply style transfer on key frames and subsequently integrate them into videos, yet achieving coherent motion and appearance remains persistent challenge. StyleMaster (Ye et al., 2025) synthesizes training data by leveraging the illusion property of VisualAnagrams (Geng et al., 2024), generating image pairs that share common style while differing in content. Building on the still-moving paradigm, it subsequently trains motion adapter on frozen video representations. Nevertheless, two key limitations remain. First, the synthetic pairs primarily capture artistic variations and are insufficient to model more complex styles, such as LEGO. Second, training motion adapter on frozen videos presupposes separation between spatial and temporal attention, whereas recent architectures (Wan et al., 2025; HaCohen et al., 2024; Kong et al., 2024) increasingly adopt spatiotemporal attention mechanisms, making such decoupling more challenging. To address these limitations, we exploit GPT-4os (Achiam et al., 2023) strong style transfer capability to convert Unity3D-rendered talk show into three distinct styles (anime, clay, and Pixar), thereby constructing curated image dataset. We then augment this dataset with subset of OmniConsistency (Song et al., 2025) to further increase stylistic diversity. To convert these image pairs into videos, we apply synthetic camera motions (e.g., zooming, sliding), creating sequences with simple movement and mitigating the risk of overfitting to static, motionless videos. Next, we keep the base model frozen and train LoRA module on an auxiliary branch that conditions on RGB videos. Motivated by advances in training-free diffusion guidance approaches (Rajabi et al., 2025; Hong, 2024; Ahn et al., 2024), we further strengthen the context condition by extending classifierfree guidance to contextstyle classifier-free guidance (CS-CFG), which jointly emphasizes the text prompt for style and the video for contextual information during denoising. Our empirical results demonstrate that this approach significantly outperforms existing models, effectively transferring style while maintaining consistency with the conditioning video."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Video style transfer with image prior. There are several models that leverage image-based diffusion models for video style transfer by extending them with temporal mechanisms. ControlVideo (Zhang et al., 2023b) adapts ControlNet from images to videos by adding full cross-frame self-attention and interleaved-frame smoothing, which allows strong structural fidelity under textand-condition guidance. However, it is heavily reliant on the quality of control signals (such as depth or edges), making it less robust when such guidance is noisy or unavailable. ReRender-AVideo (Yang et al., 2023) generates stylized key frames with hierarchical cross-frame constraints using an image diffusion model, and then propagates them to the full video through patch-based blending. This hybrid design balances efficiency and quality but can introduce blurred details or artifacts when large motion or scene changes occur. FRESCO (Yang et al., 2024) builds on image priors by enforcing spatial and temporal correspondences and introducing feature blending mechanism that aggregates spatially similar regions and propagates them along optical flow paths. While this reduces flicker and improves motion stability, it remains sensitive to flow errors and 2 Figure 2: Training and inference pipeline of PICKSTYLE. In training (left), both the style image and the context image are transformed into video tokens and context tokens with synthetic camera motion using motion augmentation; video tokens are noised and denoised conditioned on context tokens by the DiT-based PICKSTYLE model with context-style adapters. In inference (right), context video and style description are encoded and iteratively denoised under text, context, and null conditions, where the proposed CSCFG applies spatiotemporal permutation to the null context to generate the final styled video. adds computational complexity. Despite their progress, all these image-based approaches still find it challenging to fully preserve the natural motion of the input video without noticeable flicker. Video style transfer with video diffusion models. Models that build on video diffusion for style transfer include Control-A-Video (Chen et al., 2023), V-Stylist (Yue et al., 2025), and StyleMaster (Ye et al., 2025). Control-A-Video extends an image diffusion backbone with temporal layers and spatio-temporal attention, and incorporates motion-aware initialization and first-frame conditioning while also supporting per-frame controls such as edges, depth, or flow maps; this allows it to preserve structure and motion while applying styles described in the prompt, though its outputs are generally constrained to short clips and moderate resolutions. V-Stylist approaches the problem as multi-agent pipeline: it parses the input video into shots, interprets an open-ended style request with an LLM, and renders each shot with style-specific diffusion model and multiple ControlNets, guided by self-refinement loop that balances style and structure. This design makes it effective for long and complex videos while producing strong style fidelity. StyleMaster, in contrast, integrates both local and global style cues into video diffusion backbone, employs motion adapter to enhance temporal consistency, and uses tiled ControlNet for video-to-video translation; its styles are often more artistic, as they are grounded in curated training dataset created using VisualAnagrams, which emphasizes distinctive painterly and creative effects."
        },
        {
            "title": "3 PICKSTYLE",
            "content": "Our goal is to adapt text-to-video diffusion models for the task of video style transfer, where the content of an input video is preserved while its appearance is translated into target style specified by text prompt. key challenge is the lack of paired video datasets for style transfer. To address this, we construct training data from pairs of images with different artistic or visual styles, which provide supervision for learning consistent appearance transformations. 3.1 PRELIMINARIES Conditional Diffusion Models In conditional diffusion models, the forward process progressively corrupts clean sample x0 into noisy latent xt through q(xt xt1) = N(cid:0)xt; αt xt1, (1 αt)I(cid:1), (1) 3 until xT approximates Gaussian noise. The reverse process seeks to recover x0 by denoising in stepwise manner, modeled as pθ(xt1 xt, c), (2) where denotes the conditioning signal (e.g., class label, text, or image). This transition is parameterized by neural denoiser ϵθ(xt, t, c) that predicts the injected noise at each step. Training minimizes the conditional objective Ex0,t,ϵ ϵ ϵθ(xt, t, c)2(cid:105) (cid:104) , (3) ensuring that the learned reverse dynamics generate samples consistent with the condition c. Classifier Free Guidance. Classifier-free guidance (CFG) is widely used sampling technique that enhances the alignment of conditional diffusion models with given condition without requiring an external classifier. Instead of relying solely on ϵθ(xt, t, c), the denoiser is jointly trained with and without conditions, yielding an unconditional branch ϵθ(xt, t, ). During inference, the two predictions are interpolated as ˆϵθ(xt, t, c) = ϵθ(xt, t, ) + ω(cid:0)ϵθ(xt, t, c) ϵθ(xt, t, )(cid:1), where ω > 1 is the guidance scale. This formulation strengthens the influence of the condition by amplifying its contribution relative to the unconditional estimate, thereby producing samples that more faithfully follow while preserving sample diversity. (4) VACE Building on ACE (Han et al., 2024), VACE (Jiang et al., 2025) introduces multimodal input conditioning for text-to-video generation through the Video Condition Unit (VCU). Formally, VCU is defined as = (T , F, M), (5) where denotes the text prompt, = {u1, u2, . . . , um} RCT HW is the normalized video conditioning, and = {m1, m2, . . . , mn} {0, 1}T HW is binary mask, with 1 indicating tokens that can be modified and 0 indicating tokens that remain fixed. The model then computes reactive frames Fc = and inactive frames Fk = (1 M), which are concatenated as = [Fc; Fk] to form the final video conditioning input. To inject the condition, VACE uses signals such as optical flow, depth maps, grayscale videos, scribbles, human 2D poses, and bounding boxes as during training. Following ControlNet (Zhang et al., 2023a), it duplicates the pretrained text-to-video blocks into context blocks and trains them as separate branch. These context blocks are fewer than the main blocks and skip certain layers, which makes the model more lightweight and improves convergence. The output of each context block is then added back to the corresponding DiT block in the main branch. While VACE incorporates diverse conditioning signals during training, RGB frames are always treated as inactive frames. As result, the model can handle tasks such as inpainting and outpainting, but cannot encode RGB inputs as reactive frames, which limits its ability to perform tasks like style transfer."
        },
        {
            "title": "3.2 TRAINING WITH IMAGE PAIRS",
            "content": "To enable the model to generalize from static image pairs to dynamic video content, we simulate motion during training. Specifically, we apply conventional data augmentations such as zooming in/out and sliding the crop window, which act as synthetic camera motions. For each image pair (source, style), we generate two corresponding video clips of length frames, where both clips undergo identical augmentation trajectories. This ensures the paired clips exhibit aligned synthetic motion while differing in style, allowing the model to learn temporal consistency during style transfer. Fig. 2 shows our training and inference pipeline. We adapt pretrained VACE model built on DiT blocks from Wan2.1, and adds context blocks (M < ) to encode the additional condition. We finetune only the self-attention layers of the context blocks. Cross-attention layers, which handle text conditioning, are left untouched because the model already demonstrates strong language understanding. Restricting adaptation to self-attention layers avoids disrupting the pretrained textvideo alignment while still enabling the model to specialize in transferring motion and appearance across video domains. Formally, the standard QKV projections in self-attention layers are defined as: Qi = WQZi, Ki = WKZi, Vi = WV Zi, {n, c}, (6) 4 where Zn, Zc are input features for noise and context tokens, and WQ, WK, WV Rdd are shared projection matrices used across all branches. We introduce LoRA transformations exclusively on the context blocks: Qc = BQAQZc, Kc = BKAKZc, Vc = BV AV Zc, where AQ, AK, AV Rrd and BQ, BK, BV Rdr are low-rank matrices with d. The QKV for the context blocks is then updated as: = Qc + Qc, while the noise branch remains unchanged: = Kc + Kc, = Vc + Vc, = Qn, = Kn, = Vn. (7) (8) (9)"
        },
        {
            "title": "3.3 CONTEXT–STYLE CLASSIFIER-FREE GUIDANCE (CS–CFG)",
            "content": "Let xt denote the noised latent at diffusion step t, and let ϵθ(xt, t; , C) be the noise-prediction network conditioned on text prompt (style) and video-conditioning tensor (context). We construct null version of the context by independently permuting its temporal and spatial axes. Concretely, if Rthwc is the encoded context tensor in latent space, we draw independent uniform permutations πT ST , πH SH , πW SW , where ST (resp. SH , SW ) denotes the symmetric group of all permutations of {1, . . . , } (resp. {1, . . . , H}, {1, . . . , }). The null context tensor is then defined as with (πT C)t,h,w,c = CπT (t),h,w,c and analogously for πH and πW . We then evaluate three forward passes: Cnull = πW πH πT C, ϵcond = ϵθ(xt, t; , C), ϵnull text = ϵθ(xt, t; , C), ϵnull = ϵθ(xt, t; , Cnull), where denotes dropped text-conditioning (i.e., the classifier-free null token). CSCFG factorizes the guidance into style (text) direction and context (video) direction: text = ϵcond ϵnull text, context = ϵnull text ϵnull. Given user-selected scales tguide 0 (style) and cguide 0 (context), the guided prediction is (cid:98)ϵ = ϵnull text + tguide text + cguide context. 3.4 NOISE INITIALIZATION STRATEGY (10) (11) (12) (13) (14) (15) (16) To enhance temporal coherence and preserve the context structure of the input video, we depart from the standard diffusion process that initializes sampling from pure Gaussian noise. Instead, we propose to initialize sampling from partially noised version of the original video content C. Given total of denoising steps, we select hyperparameter [1, n], and construct xnk by applying the forward noising process to up to step k: xnk q(xnk x0 = C). (17) We then run the reverse process starting from xN down to x0 using the DPM++ (Lu et al., 2025) sampler: xt1 = DPM++(cid:0)xt, ϵθ(xt, t; , C)(cid:1), = k, . . . , 1, (18) where ϵθ(xt, t; , C) is the denoiser conditioned on the style prompt and video content C. By initializing from xnk rather than pure Gaussian noise, the model retains spatial and motion structure from the original video content C, while still allowing sufficient stochasticity to adapt the style specified by . The hyperparameter controls the trade-off between style strength (larger k) and content/motion fidelity (smaller k). 5 Table 1: Quantitative comparisons on Content and Style Alignment across baseline methods and our PICKSTYLE Models Content Alignment Style Alignment DreamSim UMT CLIP CSD Precision Control-A-Video Chen et al. (2023) Rerender Yang et al. (2023) FLATTEN Cong et al. (2024) FRESCO Yang et al. (2024) PICKSTYLE 0.52 0.41 0.34 0.45 0.34 1.33 2.47 2.80 1. 3.33 0.57 0.55 0.56 0.54 0.57 0.10 0.13 0.21 0.17 0.37 0.34 0.27 0.28 0. 0.75 0.54 0.39 0.43 0.22 0.85 0.65 0.54 0.53 0.32 0.91 Top@1 Top@2 Top@"
        },
        {
            "title": "4 EXPERIMENTS\nImplementation details. We use the multi-node training framework of (Modal) with RDMA sup-\nport to efficiently optimize the LoRA parameters. Our style adapter is trained on 32 H100 GPUs for\n3000 steps with a learning rate of 5.6 × 10−4 and rank r = 128 on the Wan2.1-VACE-14B variant.\nDuring inference, we apply n = 20 denoising steps with tguide = 5 and cguide = 4 in CS–CFG. To\nfurther improve results, we use TeaCache (Liu et al., 2025) to accelerate generation and APG (Sadat\net al., 2024) to mitigate oversaturation. Additional details are provided in the appendix.",
            "content": "Metrics. We evaluate our method based on Content Alignment, Style Alignment, and Video Quality. For content alignment, we compute frame-level similarity using the DreamSim (Fu et al., 2023) distance between corresponding frames in the original and generated videos, and report the final score by averaging across all frames. We further evaluate how well the generated video matches its high-level text description using UMTScore (Liu et al., 2023). For style alignment, we calculate the CLIP score (Hessel et al., 2021) between each generated frame and textual style prompt, then average over frames to obtain the final score. We also compute the CSD score (Somepalli et al., 2024) by first averaging the similarity between each generated frame and the target style exemplars, and then averaging across frames to produce the overall style alignment score. We further evaluate top-k Precision using Gemini (Team et al., 2023) by classifying the middle frame of each generated video against all candidate style prompts. For each frame, Gemini returns the top-k most likely styles in order, and we compute top-k precision for each frame, and averaging across frames to produce the final precision score. For Video quality, we use Motion smoothness, dynamic quality, and visual quality from VBench (Huang et al., 2024) benchmark. Motion smoothness leverages the motion priors in the AMT (Li et al., 2023) model to leverage the smoothness of generated videos. Dynamic quality uses RAFT (Teed & Deng, 2020) to estimate degree of dynamics, and Visual quality uses MUSIQ (Ke et al., 2021) on each frame to assess distortions such as over-exposure, noise, or blur. Dataset. Our training dataset consists of paired images across multiple styles. We begin by extracting 250 diverse frames from an animated 3D talk show rendered in Unity3D, which serve as our source images. Using GPT-4o, we transform each frame into three distinct styles: Anime, Pixar, and Claymation. To ensure consistency in content between the generated samples and the originals, we manually refine the prompts for each case. This process yields carefully curated dataset of 750 stylized samples, containing both the original reference frames and their three stylistic variants. To further enhance the diversity of training data, we incorporate six styles from OmniConsistencys dataset (Song et al., 2025): 3D Chibi, Vector, LEGO, Rick & Morty, Origami, and Macaron, and we further augment our Claymation style using their samples. 4.1 COMPARISONS WITH OTHER METHODS Quantitative comparison. Table 1 compares PICKSTYLE with prior approaches on both content and style alignment metrics. For content alignment, PICKSTYLE achieves the lowest DreamSim score (0.34) and the highest UMTScore (3.33), indicating stronger frame-level consistency and better alignment with high-level content descriptions than the baselines. On style alignment, PICKSTYLE reaches the highest CSD score (0.37). While CLIP score remains tied with Control-A-Video (0.57), PICKSTYLE achieves substantially higher Precision across all top-k levels, demonstrating more accurate alignment with the target styles. 6 Figure 3: Comparison on CSD Score and inference cost, per one second of generated video. Inference is evaluated on single H100 GPU. Table 2: Quantitative comparisons on Video Quality metrics across baselines and our PICKSTYLE Models Video Quality MotionSmooth DynamicQuality VisualQuality Control-A-Video (Chen et al., 2023) Rerender (Yang et al., 2023) FLATTEN (Cong et al., 2024) FRESCO (Yang et al., 2024) PICKSTYLE 0.976 0.990 0.977 0.993 0.982 0.602 0.667 0.780 0.632 0. 0.683 0.567 0.592 0.623 0.688 Overall 0.754 0.741 0.783 0.716 0.822 Fig. 3 further shows that our method achieves both faster inference and better CSD score for style alignment, whereas Rerender and FRESCO rely on Ebsynth blending (Jamriˇska et al., 2019), which introduces the main bottleneck during inference. Table 2 demonstrates that PICKSTYLE achieves clear margin over existing approaches in both dynamic quality and visual quality, the two metrics most reflective of temporal coherence and perceptual fidelity. MotionSmooth remains nearly perfect for all methods, since they are derived from video-to-video models that inherently preserve motion trajectories, and the small numerical differences are therefore negligible. When aggregated, PICKSTYLE obtains the highest overall score, highlighting its effectiveness in generating temporally consistent and perceptually compelling video outputs compared to prior work. Qualitative comparison. Fig. 4 presents qualitative comparison of PICKSTYLE with Rerender, Control-a-Video, FLATTEN, and FRESCO on LEGO and Anime styles. The competing methods, which rely on depth maps or HED edges (Xie & Tu, 2015) as inputs, lack access to color information, often producing mismatched hues and noticeable color artifacts in their generated videos. In addition, Rerender and FRESCO, being image-based models, exhibit poor temporal consistency and suffer from frame-to-frame flickering. Finally, while the geometry constraints in these baselines sometimes succeed in forming LEGO-like structures in local regions such as the head, they frequently fail to propagate these stylistic details across the entire body. In contrast, PICKSTYLE consistently delivers faithful color reproduction, stable temporal coherence, and coherent geometry throughout the video. Additional qualitative comparison results across styles are provided in the Appendix and supplemental video. Fig. 5 shows qualitative results on Unity3D animations that we collected and used to train Anime, Pixar, and Clay styles. Although this dataset differs from the photorealistic data used to train other styles, PICKSTYLE is still able to transfer styles such as LEGO, Rick & Morty, and Macaron from OmniConsistency, which were originally trained on photorealistic counterparts. This demonstrates that PICKSTYLE generalizes effectively across domains, handling both photorealistic and non-photorealistic inputs. Moreover, it highlights practical application for animated content: instead of depending on high-quality outputs from 3D engines, one can rely on simple Unity3D renderings and leverage style transfer to achieve visually compelling results. In Fig. 6, we further compare PICKSTYLE with VACE on Macaron style generation. Here, optical flows extracted using RAFT (Teed & Deng, 2020) serve as the input condition for VACE. Because these flows do not contain color information, VACE cannot preserve the lost appearance details in its outputs. In addition, since VACE was not originally designed for style transfer and is highly sensitive 7 Figure 4: Qualitative comparison of PICKSTYLE , Control-a-Video, Rerender, FRESCO, and FLATTEN in LEGO and anime styles. Figure 5: Qualitative evaluation of PICKSTYLE on non-photorealistic example rendered in Unity3D. to the input geometries, it struggles to capture the intended stylistic patterns and fails to achieve reliable style transfer. More extensive comparisons with alternative input modalities supported by VACE are provided in the Appendix. 4.2 ABLATION STUDIES Effect of motion augmentation. Fig. 7 shows the effect of motion augmentation on videos generated by PICKSTYLE in anime and Pixar styles. For the anime samples both the video description and the style prompt are provided, while for the Pixar samples only the style prompt is given. When the video description is included the generated results achieve both good motion quality and faithful style transfer. Without motion augmentation however small background motions such as people walking on treadmill are often missed, as the model pays less attention to fine motion details. The gap becomes larger when the video description is not provided. In the Pixar example the model without motion augmentation cannot fully preserve actions such as the jump at the end of the video 8 Figure 6: Comparison between PICKSTYLE and the VACE baseline in 3D Chibi style. VACE fails to capture the target style. Figure 7: Effect of motion augmentation of generated video of PICKSTYLE . Figure 8: Effect of CS-CFG on the style transferring, evaluated on Clay style. and focuses mostly on style transfer. With motion augmentation the model better captures both large scale and subtle motions even when detailed descriptions are not available. Effect of CSCFG. Fig. 8 highlights the effectiveness of CS-CFG in improving style transfer. With CFG, only the style guidance in text prompt influences the output, so while the video carries the intended clay style, it lacks fidelity to the original content. In this case, the model confuses the dog with swan due to its generative prior and produces hybrid appearance that diminishes contextual accuracy. An alternative design replaces the null video context in CS-CFG with zero pixels, which yields partial improvement over CFG but results in oversaturation and incomplete preservation of the clay style, as seen for instance in the persons hand where fine details are lost. In contrast, CS-CFG leverages spatiotemporal permutation to better capture contextual cues, leading to sharper details, faithful clay-style transfer, and stronger adherence to the intended content."
        },
        {
            "title": "5 LIMITATION",
            "content": "PICKSTYLE is built on Wan2.1 as the underlying generative backbone and therefore inherits artifacts and weaknesses present in that model. Typical issues include distortions in fine regions such as faces and hands, where the base model struggles to capture small details. As more advanced video backbones become available, the same pipeline can directly benefit from them, reducing such artifacts and further improving overall quality."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced PICKSTYLE , video-to-video style transfer framework built on VACE with contextstyle adapters and novel CSCFG mechanism. Despite being trained on relatively limited dataset, PICKSTYLE effectively preserves motion and context while rendering diverse target styles. By leveraging synthetic motion-augmented training pairs and noise initialization strategy, it achieves superior style fidelity, temporal stability, and perceptual quality compared to existing methods. Beyond quantitative improvements, PICKSTYLE consistently produces coherent color reproduction and faithful geometry across diverse styles while avoiding the temporal flicker and blending artifacts common in image-based approaches. These results highlight that even with constrained supervision, PICKSTYLE can deliver high-quality style transfer and establish strong baseline for future research in controllable video stylization. Acknowledgment. We gratefully acknowledge the support of Modal, whose computing credits facilitated the processing required for this work."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pp. 117. Springer, 2024. Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1323, 2025. Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. CoRR, 2023. Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. FLATTEN: optical flow-guided attention for consistent text-to-video editing. In ICLR, 2024. Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. DreamSim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2415424163, 2024. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. LTX-Video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. ACE: All-round creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. CameraCtrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 10 Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 37:6674366772, 2024. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Ondˇrej Jamriˇska, ˇSarka Sochorova, Ondˇrej Texler, Michal Lukaˇc, Jakub Fiˇser, Jingwan Lu, Eli Shechtman, and Daniel S`ykora. Stylizing video by example. ACM Transactions on Graphics (TOG), 38(4):111, 2019. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. VACE: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 51485157, 2021. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. HunyuanVideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. AMT: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 98019810, 2023. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7353 7363, 2025. Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. FETV: benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pp. 122, 2025. Modal. Modal: High-performance ai infrastructure. https://modal.com/. Accessed: 2025-09-23. Javad Rajabi, Soroush Mehraban, Seyedmorteza Sadat, and Babak Taati. Token perturbation guidance for diffusion models. arXiv preprint arXiv:2506.10036, 2025. Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. Yiren Song, Cheng Liu, and Mike Zheng Shou. OmniConsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pp. 402419. Springer, 2020. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Saining Xie and Zhuowen Tu. Holistically-nested edge detection."
        },
        {
            "title": "In Proceedings of the IEEE",
            "content": "international conference on computer vision, pp. 13951403, 2015. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pp. 111, 2023. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. FRESCO: Spatial-temporal correspondence for zero-shot video translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 87038712, 2024. Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. StyleMaster: Stylize your video with artistic generation and translation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 26302640, 2025. Zhengrong Yue, Shaobin Zhuang, Kunchang Li, Yanbo Ding, and Yali Wang. V-Stylist: Video stylization via collaboration and reflection of mllm agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 31953205, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023a. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023b."
        },
        {
            "title": "A APPENDIX",
            "content": "Supplemental Video. The supplemental video provides qualitative demonstrations that illustrate the effectiveness of our approach across various styles and scenarios. We strongly encourage readers to view the supplemental video for more comprehensive understanding of the results. A.1 MORE IMPLEMENTATION DETAILS Based on noise initilization strategy introduced in Sec. 3.3, we skip the first denoising steps that controls the trade-off between style strength and motion fidelity. By trial and error, we choose different values for each style presented in Table 3. For styles such as Vector that are more abstract, we use less value and for styles such as Pixar that more resembles the input RGB, we use higher value. For Precision, we employ Gemini-2.5-Flash as the style classifier. Table 3: Step skip values used for different styles. Style Step Skip Value Vector 3D Chibi Anime Pixar Clay LEGO Macaron Origami Rick & Morty 1 2 3 6 0 2 2 2 0 A.2 MORE COMPARISON WITH VACE Alternative conditions that VACE can use for style transfer include depth maps, shown in Fig. 9, and scribbles, shown in Fig. 10. However, because depth maps only provide relative geometry and scribbles capture edges, VACE is unable to perform effective style transfer in either case. Moreover, since these conditions are extracted from videos, they are prone to noise, which further degrades the quality of the generated output. Figure 9: Comparison between PICKSTYLE and the VACE baseline in Anime style when using Depth map as condition of VACE. Figure 10: Comparison between PICKSTYLE and the VACE baseline in LEGO style when using scribble as condition of VACE. 13 A.3 MORE QUALITATIVE COMPARISON Additional qualitative comparisons are shown in Fig. 11 and Fig. 12, covering Pixar, 3D Chibi, Origami, Vector, Clay, Macaron, and Rick & Morty styles. Across these diverse cases, competing approaches frequently suffer from color artifacts, style distortion, and unstable temporal consistency. For instance, methods like Rerender and FRESCO often introduce flickering due to their image-based design, while Control-A-Video and FLATTEN struggle to maintain coherent color reproduction and consistent geometry when translating styles across frames. In contrast, PICKSTYLE produces results that remain faithful to the source video while accurately reflecting the intended target style, demonstrating stronger robustness across both simple and complex stylizations. Figure 11: Qualitative comparison in Pixar, 3D Chibi, Origami, and Vector styles. A.4 THE USE OF LARGE LANGUAGE MODELS (LLMS) We use GPT-5 to refine the writing, paraphrase content, and improve readability. 14 Figure 12: Qualitative comparison in Clay, Macaron, and Rick & Morty styles."
        }
    ],
    "affiliations": [
        "Pickford AI",
        "University of Toronto",
        "Vector Institute"
    ]
}