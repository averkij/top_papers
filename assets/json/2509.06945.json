{
    "paper_title": "Interleaving Reasoning for Better Text-to-Image Generation",
    "authors": [
        "Wenxuan Huang",
        "Shuang Chen",
        "Zheyong Xie",
        "Shaosheng Cao",
        "Shixiang Tang",
        "Yufan Shen",
        "Qingyu Yin",
        "Wenbo Hu",
        "Xiaoman Wang",
        "Yuntian Tang",
        "Junbo Qiao",
        "Yue Guo",
        "Yao Hu",
        "Zhenfei Yin",
        "Philip Torr",
        "Yu Cheng",
        "Wanli Ouyang",
        "Shaohui Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation ."
        },
        {
            "title": "Start",
            "content": "Preprint INTERLEAVING REASONING FOR BETTER TEXT-TO-IMAGE GENERATION Wenxuan Huang1,2, Shuang Chen4, Zheyong Xie3, Shaosheng Cao3(cid:66) , Shixiang Tang2, Yufan Shen5, Qingyu Yin5, Wenbo Hu4, Xiaoman Wang1, Yuntian Tang1, Junbo Qiao1, Yue Guo3, Yao Hu4, Zhenfei Yin6(cid:66) , Philip Torr6, Yu Cheng2, Wanli Ouyang2, Shaohui Lin1(cid:66) 1East China Normal University 4University of California, Los Angeles osilly0616@gmail.com Github Repo: https://github.com/Osilly/Interleaving-Reasoning-Generation 2The Chinese University of Hong Kong 6University of Oxford 5Zhejiang University 3Xiaohongshu Inc. 5 2 0 2 8 ] . [ 1 5 4 9 6 0 . 9 0 5 2 : r Figure 1: As shown in (a), we illustrate an example of Interleaving Reasoning Generation (IRG). Given prompt, the model first produces text-based reasoning process and then generates an image conditioned on that reasoning. Next, building upon the initial image, the model reflects on how to improve its quality and produces refined image through this reflection process. IRG can substantially enhance image generation quality. For instance, in the top case of (a), IRG improves upon the previous generated image via multi-turn reasoning, enhancing rendering textures, shadow realism, and other visual properties. In the bottom case of (a), IRG significantly improves fine-grained details, such as the delicate structures of fingershighlighted within the red box (as detailed in (b)). As shown in (c), compared to current SoTA models, our proposed IRG achieves clearly superior performance across multiple mainstream T2I benchmarks. (cid:66)Corresponding authors. 1 Preprint Figure 2: Visualization results of IRG at 10241024 resolution. The examples are selected from WISE (Niu et al., 2025), TIIF (Wei et al., 2025), and GenAI-Bench (Li et al., 2024a). 2 Preprint"
        },
        {
            "title": "ABSTRACT",
            "content": "Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), framework that alternates between text-based thinking and image synthesis: the model first produces text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in subsequent image. We curate IRGL-300K, 300K-scale dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinkingimage trajectories. Starting from unified foundation model that natively emits interleaved textimage outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinkingimage trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 510 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. As an early exploration, our results demonstrate that interleaving reasoning is powerful paradigm for advancing T2I. The code, model weights and datasets will be released in: https: //github.com/Osilly/Interleaving-Reasoning-Generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Unified multimodal understanding and generation models consolidate image/text understanding and image synthesis capabilities within single foundation model, and have recently emerged as focal point of interest in the research community (Sun et al., 2024; Team, 2024; Tong et al., 2024; Wu et al., 2025a; Xie et al., 2024; Chen et al., 2025a; Xiao et al., 2025b;a; Liao et al., 2025; Xie et al., 2025b; Wu et al., 2025b; Deng et al., 2025). Representative efforts in this line of research, exemplified by GPT-4o (OpenAI, 2025c), seamlessly integrate comprehension and generation capabilities. This enables pronounced performance gap relative to existing unified models, particularly in instructionfollowing for image generation and in the preservation of visual details (Deng et al., 2025). Motivated by recent advances in text-based reasoning, notably test-time scaling techniques for (Multimodal) Large Language Models ((M)LLMs) (Jaech et al., 2024; Guo et al., 2025; Huang et al., 2025; Chen et al., 2025b), growing body of work has explored whether incorporating such text-based reasoning processes can yield improvements in the fidelity and overall quality of image generation (Fang et al., 2025; Xiao et al., 2025b; Deng et al., 2025; Jiang et al., 2025). These works underscore this perspective and seek to exploit large-scale interleaved textimage corpora to learn subtle cross-modal interaction patterns, thus enabling seamless knowledge transfer between the understanding and generation stages of the model. However, in Text-to-Image (T2I) tasks, they employ only single textual segment as auxiliary supervision in T2I generation, with the objective of producing outputs that more faithfully adhere to the original prompt. Recently, some works in (M)LLM field have focused on the interleaving reasoning, i.e., multi-turn interactions and exhibits sophisticated reasoning dynamics (Huang, 2025), while this reasoning modality has empirical demonstrated superior accuracy in addressing complex problems (OpenAI, 2025b;a). This observation motivates the exploration: Whether interleaving reasoning can further enhance T2I generation quality? As shown in Fig 1, in generating prompt-aligned image, the model is typically able to produce content that is broadly correct in terms of semantics, yet it remains challenging to attain superior visual quality and fine-grained fidelity (e.g., in rendering textures, shadow realism, and delicate 3 Preprint structures such as fingers). The idea is intuitive: if high-quality image synthesis is considered hard problem, adopting multi-step reasoning strategy to tackle it is both reasonable and necessary. To solve this, straightforward idea is to have the model first produce text-based thinking process and generate an image based on that reasoning. Then, building on the initial image, the model reflects on how to improve its quality, and produces an improved image through reflection. We denote this process as Interleaving Reasoning Generation (IRG). Thus, we argue two points, 1) an additional text-based reasoning process can serve as auxiliary supervision for image generation, thereby alleviating the difficulty of direct generation, and 2) producing one image that simultaneously attains high visual quality and precise instruction following is non-trivial, whereas multi-turn generation strategy can incrementally refine the output toward the desired goal. While these positions are aligned with prior reflection-based T2I generation approaches, the key distinction lies in their goals: prior methods (Zhuo et al., 2025; Wu et al., 2025b; Chern et al., 2025) generally employ reflection to rectify major semantic or structural errors in the generated content, with some adopting non-end-to-end frameworks, whereas our approach focuses on leveraging reflection to refine finegrained details and improve overall visual quality in an end-to-end manner, with the main subject matter established during the initial generation. Specifically, through IRG, we not only enhance the semantic correctness of the generated content, but also focus on improving the quality, fine-grained details, and aesthetic aspects of the generated images. Based on the insights, we firstly select unified multimodal understanding and generation model as the base model, given its capability to produce interleaved textimage outputs. To facilitate interleaving reasoning-based generation with reflection, we propose the Interleaving Reasoning Generation Learning (IRGL) paradigm and formulate the objective as two sub-goals. The first is to strengthen the models initial thinking and generation stage, which establishes the core content and base quality of the generated image. The second is to equip the model with the ability to produce detailed, high-quality text-based reflections, and to generate enhanced images that faithfully implement those reflections. In particular, we propose the IRGL-300K dataset, which refines the aforementioned two objectives into two complementary focuses: learning text-based thinking and mastering the image generation with thinking pipeline. It comprises six decomposed learning modes that jointly target comprehensive enhancement of model performance throughout the IRG process, while ensuring optimal exploitation of data resources. In the training pipeline, the model is first trained to generate accurate initial thinking given prompt, as well as to produce text-based reflections based on the initial reasoning step that improve quality. Furthermore, we incorporate full thinkingimage trajectories to mitigate potential degradation in the models core generative ability in this training stage. Then, leveraging the acquired thinking generation capability, we utilize the full thinkingimage trajectory data to efficiently train the entire IRG pipeline. Extensive experiments demonstrate that our proposed IRG achieves State-of-The-Art (SoTA) benchmark performance, delivering absolute improvements of 510 points across multiple benchmarks, including GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN. In addition, IRG significantly enhances visual quality and fine-grained fidelity. As an early effort to introduce Interleaving Reasoning into the T2I domain, we hope our work can inspire future research in this direction."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 PRELIMINARIES AND NOTATIONS Unified multimodal understanding and generation models based on an integrated transformer architecture, e.g., BAGEL (Deng et al., 2025), jointly perform image understanding and generation within single architecture, facilitated by large-scale training on interleaved textimage data. Such models can naturally handle both interleaved inputs and interleaved outputs. Let Tin and Iin represent the input text and image, respectively, while (1) out denote the initial output text and image, respectively. The standard image understanding and T2I generation with self-CoT reasoning process can be formulated as: out and (1) Tin + Iin (1) out, out (1) Tin (1) out. (1) Previous works that adopt self-CoT (Deng et al., 2025; Fang et al., 2025; Xiao et al., 2025b; Jiang et al., 2025) typically focus solely on text-based reasoning to improve image generation, while 4 Preprint Figure 3: Overview of our proposed IRG training and inference pipeline. IRG learns the text-based thinking process and the complete high-quality image generation pipeline under six decomposed learning modes. During inference, we introduce dedicated CFG condition design (Ho & Salimans, 2022) for IRGs improved image generation steps. overlooking the potential of leveraging the initially generated image to further enhance visual quality and perform multi-step information fusion for better results. Thus, to conduct additional T2I reasoning step conditioned on the previously generated text and image, we first encode the generated image into Vision Transformer (ViT) features and Variational Autoencoder (VAE) features, which are defined as If . Then, based on the full information cache, the multi-turn generation can be conducted and the pipeline denoted as follows: Tin (1) out (1) out enc (1) (2) out (2) out enc (2) (n) out (n) out, (2) where means the total turn number and enc represents the image feature encoding process, and note that for intermediate images generated at the k-th turn (e.g., (k) out ), only their encoded representations (k) are propagated to the subsequent computation stages. During the generation of the final image (n) out, the model exchanges and exploits multiple segments of interleaved textimage representations, process we term Interleaving Reasoning Generation (IRG). By formulating the synthesis of the final image as the ultimate goal, the model employs multi-stage progressive generation and cross-modal information fusion to maximize output quality. We note that an inherent strength of unified models lies in their capacity to process and generate interleaved inputs and outputs, inherently supporting multi-turn generation with self-CoT reasoning. This property enables us to investigate how interleaving reasoning within the generation pipeline can further extend the achievable limits of generative capability. 2.2 INTERLEAVING REASONING GENERATION 2.2.1 OVERVIEW As mentioned above, IRG can be defined as comprising two components: (1) an initial text-based reasoning process followed by image generation based on that reasoning, and (2) repetition of the first component to produce an improved image. In this work, we focus solely on single refinement iteration, i.e., limiting the second component to one turn (set in Eq 2 to 2), in order to validate 5 Preprint our hypothesis on whether Interleaving Reasoning can effectively enhance text-to-image generation quality. In the following sections, we detail the Interleaving Reasoning Generation Learning (IRGL) framework and explain how different forms of interleaving reasoning data can be effectively utilized to perform hierarchical learning with distinct emphases, as elaborated in Sec. 2.2.2. Furthermore, in Sec. 2.2.3, we introduce the data construction pipeline of IRGL-300k dataset, while in Sec. 2.2.4, we simply describe the inference strategy of IRG, such as the Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) condition designs. 2.2.2 INTERLEAVING REASONING GENERATION LEARNING When we set to 2, Eq. 2 will be reformulate as: Tin (1) out (1) out enc (1) (2) out (2) out. (3) It can be observed that when aiming to enhance the quality of the final image (2) out in IRG pipeline, we decompose the process into four progressive intermediate steps: (1) ensuring the correctness of the initial thinking process (1) out , (3) generating an accurate improving-thinking step (2) out based on the first image to guide the production of better image, and (4) synthesizing the final high-quality image (2) out by integrating all preceding decision steps. Evidently, as shown in Fig 3, various decomposed learning modes can be designed to improve the models intermediate reasoning capacity. We begin with the enhancement of the initial reasoning step, i.e., above steps (1) and (2), which can be instantiated as the following tasks: out , (2) improving the quality of the initial generated image (1) Initial Thinking Understanding Learning: In this task, we aim for the model to learn how to generate the correct initial thinking process given original prompt Tin and prior image features (1) . The design insight behind this task is as follows: when the model is provided with both prompt and an image consistent with that prompt, we construct an auxiliary question Tq in which the model learns, through image-understanding supervision, how to produce reasoning process aligned with the prompt and to recognize what kind of image such reasoning process would yield. Tin + (1) + Tq (1) out. (4) Initial Thinking Generation Learning: This task directly imitates the reasoning process for generating initial thinking given the original prompt, and can be considered more challenging task compared to Initial Thinking Understanding Learning. Tin (1) out. (5) Initial Full Learning: In this full initial reasoning learning setting, the model learns from both text-based reasoning sequences and high-quality image data to enhance the quality of its initial image generation, thereby providing stronger foundation for producing improved images in subsequent reasoning stages. Tin (1) out (1) out. (6) Furthermore, we also design three tasks to learning how to generate the improving thinking and improved image based on the initial reasoning step (the above steps (3) and (4)): Improving Thinking Understanding Learning: This task is closely related to Initial Thinking Understanding Learning, but focuses on enabling the model to, given prompt, learn to generate the improving thinking process for enhancing an initial image to an improved image. This is achieved by understanding the prompt Tin, comparing the differences between the features of the initial (1) and by answering carefully designed questions Tq. and improved images (2) Tin + (1) + (2) + Tq (2) out. (7) 6 Preprint Improving Thinking Generation Learning: This task builds on the initial reasoning stage, focusing on learning how to generate improving thinking. (2) out. Tin + (1) out + (1) (8) Improving Full Learning: This task represents complete IRG process, but we constrain the model, under the condition that the initial reasoning is already completed, to learn only the improving reasoning and the high-quality improved image components. As the most crucial stage of IRG, the model must learn to identify the differences in visual quality and fine-grained fidelity between the two images, and to leverage this understanding to generate the optimal image during the improving reasoning step. (2) out (2) out. Tin + (1) out + (1) (9) The aforementioned decomposed learning modes can be clearly divided into two training objectives: (1) learning the text-based thinking process (Initial Thinking Understanding Learning, Initial Thinking Generation Learning, Improving Thinking Understanding Learning, and Improving Thinking Generation Learning), and (2) learning the complete high-quality image generation pipeline under the auxiliary supervision of the reasoning process (Initial Full Learning and Improving Full Learning). This design likewise addresses the limited availability of high-quality, full IRG thinking-image trajectories data, while learning from text-based reasoning serves as partial remedy to this problem. Furthermore, we employ two-stage training pipeline. In stage 1, the model is optimized on all six tasks to generate accurate initial reasoning from given prompt and to produce text-based reflections derived from the initial reasoning step to enhance output quality. The main goal of this stage is to strengthen the text-based reasoning capability, while incorporating full thinkingimage trajectories to avoid degrading the core generative performance. Empirically, we find that this reasoning-focused training converges relatively rapidly. In the second stage, leveraging the thinking generation ability learned in Stage 1, we employ full thinkingimage trajectory data (i.e., data in Initial Full Learning and Improving Full Learning) to efficiently optimize the entire IRG pipeline. In this training stage involving image generation, convergence demands more iterations, since the model must spend additional training time learning the fine-grained fidelity transformations from the initial image to the improved image. Discussion: Unfortunately, constructing complete interleaving IRG data is non-trivial, even when considering only the two-turn case, due to the following two challenges. First, obtaining final high-quality images is inherently difficult, as the quality of existing open-source T2I datasets remains suboptimal. This limitation has motivated many recent works to distill images generated by powerful models such as GPT-4o (Chen et al., 2025a; Wu et al., 2025b). Second, although subset of high-quality data from GPT-4o is available, IRG requires paired samples linking an initial image to its improved counterpart. Designing the transformation process from the initial image to the improved image is itself challenge, which means that such pairs must be constructed from scratch and cannot be directly derived from GPT-4o-distilled T2I data. These two issues make it difficult to obtain complete IRG datasets at scale. To mitigate the scarcity of fully optimized training data, we design multiple intermediate training objectives. This is because we only learn the text-based thinking process during intermediate objective training, in order to avoid low-quality image data pollution. We expect that, when conditions permit, access to large quantity of complete IRG data would lead to even better performance. 2.2. INTERLEAVING REASONING DATA CONSTRUCTION This section introduces the IRGL-300k dataset construction pipeline for the aforementioned six decomposed learning modes. Data for learning the initial reasoning step. For the Initial Thinking Understanding Learning task, we construct training data from open-source T2I datasets containing promptimage pairs. First, we design an initial thinking template, and then instruct large language model (e.g., Qwen2.5-VL (Bai et al., 2025)) to generate reasoning process that is consistent with both the prompt and the corresponding image from the original T2I data. Finally, we organize the data according to Eq. 4: the prompt corresponds to Tin, the image is encoded to obtain features (1) , manually designed 7 Preprint understanding question (e.g., You have been given one prompt and one image ...) corresponds to Tq, and the MLLM-generated initial thinking corresponds to (1) out . For the Initial Thinking Generation Learning, the initial thinking acquisition pipeline is similar to the Initial Thinking Understanding Learning task. It uses the prompt and MLLM-generated initial thinking to obtain the train date (as Eq 5). For the Initial Full Learning data, to ensure learning from high-quality image information, we input the original prompt into high-quality image generation model (GPT-4o (OpenAI, 2025d)) to produce high-quality image, which serves as (1) in Eq. 6. The initial thinking is obtained in similar out manner to that described above, by providing the prompt and the GPT-4o-distilled high-quality image to an MLLM. Data for learning the improving reasoning step. For generating data for the improving reasoning step, we encounter key challenge: given access to high-quality image to serve as the improved image, we must determine the source of the initial reasoning step data. We choose to use data generated by the base model (i.e., BAGEL (Deng et al., 2025)) conditioned on the same prompt as the initial reasoning step data. This design decision is motivated by two considerations: (1) it provides simple and efficient way to obtain multi-turn IRG data at scale, and (2) it allows us to improve the models performance without compromising the original capabilities of the base model. For the Improving Thinking Understanding Learning task, we first input the prompt into the base model to generate the initial thinking and the corresponding initial image. We then design an instructional prompt to guide the MLLM in generating improved thinking based on the base models generated image and the image from the T2I dataset. The model is instructed to produce the improving thinking according to predefined template. We adopt stage-level template (Xu et al., 2024), which requires the model to first generate part of the analysis of previous generated image issues. Then, generate the stages in the format Detailed Explanation of Required Improvements: ..., Step-by-Step Modification Guidance: ..., and Final Comprehensive Prompt for the Improved Image: .... Finally, we organize the original prompt Tin and initial image generated by base model (1) , the image in the T2I dataset (2) , manually designed understanding question (e.g., You have been given one prompt and two images ...) Tq, and the improving thinking (2) For the Improving Thinking Generation Learning task, the data is same as Improving Thinking Understanding Learning, while it use the initial thinking generated by base model as (1) out by Eq 7. out in Eq 8 In the Improving Full Learning setting, GPT-4o is used to produce the improved image in the IRG trajectory. Given the prompt and the initial image from the base model, GPT-4o generates higher-quality, prompt-consistent image, which we adopt as the improved image (2) out in Eq. 9. An MLLM is then employed to produce stage-level improving thinking detailing the transformation from the initial image to the improved image. 2.2.4 INFERENCE STRATEGY As shown in Fig 3, the model produces textimagetextimage trajectory. This poses challenge: in conventional diffusion-based generation models, the CFG-conditioning design is typically straightforward, such as directly comparing the presence versus absence of the prompt. In contrast, for our proposed IRG under even only two-turn reasoning pipeline, particularly before generating the improved imagethere are four possible condition sources to compare (i.e., the prompt, the initial reasoning, the initial image, and the improving reasoning). Therefore, customized CFG-conditioning strategy is required. Based on this, we adopt the framework incorporates two complementary CFG condition schemes: (1) conditioning with versus without the image information from the initial generation, and (2) conditioning with versus without the reflection text. In practice, we set the the guidance scale hyper-parameters in CFG Image condition (versus without the image information) and CFG text condition (versus without the text information) to 2.0. This strategy helps maintain high visual quality and fidelity in images produced during the improving reasoning steps, with notable benefits for generation stability. 8 Preprint Table 1: Evaluation of text-to-image generation ability on GenEval benchmark. Gen. Only stands for an image generation model, and Unified denotes model that has both understanding and generation capabilities. refer to the methods using MLLM rewriter. * means we report the reproducing results using the official Github repository and checkpoint. The best Overall results are bolded. Type Model Single Obj. Two Obj. Counting Colors Position Color Attri. Overall O . d fi U PixArt-α (Chen et al., 2024) SDv2.1 (Rombach et al., 2022) DALL-E 2 (Ramesh et al., 2022) Emu3-Gen (Wang et al., 2024b) SDXL (Podell et al., 2024) DALL-E 3 (Betker et al., 2023) SD3-Medium (Esser et al., 2024) FLUX.1-dev (Labs, 2024) Chameleon (Team, 2024) LWM (Liu et al., 2024) SEED-X (Ge et al., 2024) TokenFlow-XL (Qu et al., 2024) ILLUME (Wang et al., 2024a) Janus (Wu et al., 2025a) Transfusion (Zhou et al., 2024) Emu3-Gen(Wang et al., 2024b) Show-o (Xie et al., 2024) Janus-Pro-7B (Chen et al., 2025c) MetaQuery-XL (Pan et al., 2025) BAGEL* (Deng et al., 2025) Show-o2 (Xie et al., 2025b) BAGEL w/ self-CoT* (Deng et al., 2025) IRG (Ours) GPT-4o (OpenAI, 2025c) 0.98 0.98 0.94 0.98 0.98 0.96 0.99 0.98 - 0.93 0.97 0.95 0.99 0.97 - 0.99 0.98 0.99 - 0.99 1. 0.99 0.98 0.99 0.50 0.51 0.66 0.71 0.74 0.87 0.94 0.93 - 0.41 0.58 0.60 0.86 0.68 - 0.81 0.80 0.89 - 0.95 0.87 0.92 0.94 0. 0.44 0.44 0.49 0.34 0.39 0.47 0.72 0.75 - 0.46 0.26 0.41 0.45 0.30 - 0.42 0.66 0.59 - 0.76 0.58 0.75 0.83 0.85 0.80 0.85 0.77 0.81 0.85 0.83 0.89 0.93 - 0.79 0.80 0.81 0.71 0.84 - 0.80 0.84 0.90 - 0.87 0. 0.89 0.86 0.92 0.08 0.07 0.10 0.17 0.15 0.43 0.33 0.68 - 0.09 0.19 0.16 0.39 0.46 - 0.49 0.31 0.79 - 0.50 0.52 0.54 0.74 0. 0.07 0.17 0.19 0.21 0.23 0.45 0.60 0.65 - 0.15 0.14 0.24 0.28 0.42 - 0.45 0.50 0.66 - 0.60 0.62 0.63 0.73 0.61 0.48 0.50 0.52 0.54 0.55 0.67 0.74 0.82 0.39 0.47 0.49 0.55 0.61 0.61 0.63 0.66 0.68 0.80 0.80 0.78 0. 0.79 0.85 0.84 Table 2: Comparison of world knowledge reasoning on WISE. WISE examines the complex Gen. Only stands for an semantic understanding and world knowledge for T2I generation. image generation model, and Unified denotes model that has both understanding and generation capabilities. * means we report the reproducing results using the official Github repository and checkpoint. The best results are bolded. Type Model Cultural Time Space Biology Physics Chemistry Overall n . d fi SDv1.5 (Rombach et al., 2022) SDXL (Podell et al., 2024) SD3.5-large (Esser et al., 2024) PixArt-Alpha (Chen et al., 2024) playground-v2.5 (Li et al., 2024b) FLUX.1-dev (Labs, 2024) Janus (Wu et al., 2025a) VILA-U (Wu et al., 2024c) Show-o-512 (Xie et al., 2024) Janus-Pro-7B (Chen et al., 2025c) Emu3 (Wang et al., 2024b) MetaQuery-XL (Pan et al., 2025) BAGEL (Deng et al., 2025) Show-o2* (Xie et al., 2025b) BAGEL w/ self-CoT (Deng et al., 2025) IRG (Ours) GPT-4o (OpenAI, 2025c) 0.34 0.43 0.44 0.45 0.49 0.48 0.16 0.26 0.28 0.30 0.34 0.56 0.44 0.64 0.76 0.78 0.81 0.35 0.48 0.50 0.50 0.58 0. 0.26 0.33 0.40 0.37 0.45 0.55 0.55 0.58 0.69 0.72 0.71 0.32 0.47 0.58 0.48 0.55 0.62 0.35 0.37 0.48 0.49 0.48 0.62 0.68 0.61 0.75 0. 0.89 0.28 0.44 0.44 0.49 0.43 0.42 0.28 0.35 0.30 0.36 0.41 0.49 0.44 0.58 0.65 0.81 0.83 0.29 0.45 0.52 0.56 0.48 0. 0.30 0.39 0.46 0.42 0.45 0.63 0.60 0.63 0.75 0.82 0.79 0.21 0.27 0.31 0.34 0.33 0.35 0.14 0.23 0.30 0.26 0.27 0.41 0.39 0.49 0.58 0. 0.74 0.32 0.43 0.46 0.47 0.49 0.50 0.23 0.31 0.35 0.35 0.39 0.55 0.52 0.61 0.70 0.77 0."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENT SETTINGS IRGL-300k dataset. For the Initial Thinking Understanding Learning, Initial Thinking Generation Learning, Improving Thinking Understanding Learning, and Improving Thinking Generation Learning tasks, we use the open-source T2I dataset OSP1024-286k (Lin et al., 2025). For each of the four tasks, we sample 50K instances from the dataset for data construction. We employ the GPT-4o-distilled T2I dataset BLIP3o-60k (Chen et al., 2025a) in Initial Full Learning. For Improving Full Learning, we construct the dataset by distilling GPT-4o with curated set of 9 Preprint Table 3: Quantitative evaluation results of instruct-following capability on TIIF testmini (QwenVL2.5-72B as the evaluation). * means we report the reproducing results using the official Github repository and checkpoint. The best Overall and Avg. results are bolded. Model Overall Basic Following Advanced Following Avg Attribute Relation Reasoning Avg Attribute +Relation Attribute +Reasoning Relation +Reasoning Style Text Designer Real World short long short long short long short long short long short long short long short long short long short long short long short long FLUX.1-dev (Labs, 2024) 66.24 66.72 74.41 76.67 72.50 75.50 78.20 79.78 72.52 74.73 60.72 60.95 66.76 65.50 61.76 60.74 56.60 57.49 63.33 60.00 44.49 54.75 74.63 72. FLUX.1-Pro (Labs, 2024) 63.75 63.53 71.39 73.57 70.00 68.50 68.51 79.97 75.66 72.23 64.63 61.42 70.69 72.99 62.34 57.27 64.65 57.11 63.00 63.00 34.39 36.65 69.94 66.78 DALL-E 3 (Betker et al., 2023) 74.47 72.94 77.35 78.40 77.62 75.00 80.22 79.67 74.22 80.54 70.11 68.45 76.65 75.05 68.39 68.07 63.64 59.92 79.31 80.00 74.07 75.51 76.12 62.69 SD3.5-large (Esser et al., 2024) 68.69 64.92 73.72 72.10 77.50 66.50 74.79 77.16 68.87 72.64 65.59 63.41 70.85 68.22 65.03 62.93 61.03 61.66 56.67 60.00 73.30 46.15 70.15 69. PixArt-Σ (Chen et al., 2024) 57.46 57.04 67.74 68.19 65.50 69.50 74.33 72.11 63.40 62.96 56.71 54.52 62.47 59.67 57.51 55.08 54.84 52.64 76.67 73.33 2.71 4.98 63.06 63.06 Lumina-Next (Zhuo et al., 2024) 46.83 51.81 59.62 62.48 49.50 61.50 63.30 65.51 66.04 60.44 43.72 47.20 47.52 51.35 42.65 42.06 44.90 50.87 53.33 66.67 2.71 6.33 51.49 61. Hunyuan-DiT (Li et al., 2024c) 49.14 52.67 65.39 67.79 59.00 63.00 79.89 76.82 57.27 63.56 51.61 52.25 62.49 59.93 49.14 45.71 49.38 54.74 53.33 73.33 0.45 2.26 31.34 34.70 Show-o (Xie et al., 2024) 57.34 61.33 69.99 75.30 66.50 80.00 76.47 71.88 67.00 74.04 58.25 58.19 67.21 64.33 54.26 58.86 61.38 56.19 46.67 66.67 4.98 11.31 71.64 68.66 LightGen (Wu et al., 2025c) 52.84 46.42 68.70 53.99 61.00 52.00 73.69 54.52 71.40 50.52 54.10 45.76 66.82 48.37 52.22 42.93 51.07 50.64 43.33 43.33 2.26 10.86 53.73 59.70 SANA 1.5 (Xie et al., 2025a) 62.57 63.48 73.92 72.31 71.50 73.00 82.21 78.39 68.04 65.52 60.36 60.36 65.65 67.33 56.41 56.13 62.20 60.18 66.67 76.67 28.51 23.53 61.94 70.52 Infinity (Han et al., 2025) 60.65 59.66 70.90 71.63 73.00 73.00 73.75 74.44 65.96 67.44 59.80 57.81 68.92 63.78 60.53 56.87 55.04 56.81 56.67 73.33 22.17 26.70 69.78 61.19 Janus-Pro-7B (Chen et al., 2025c) 65.38 61.10 74.99 73.19 74.50 78.00 73.69 70.51 76.77 71.04 61.77 56.03 65.71 66.48 62.01 55.62 61.16 49.34 43.33 70.00 38.46 42.08 79.48 73.51 T2I-R1 (Jiang et al., 2025) 67.61 68.34 81.14 79.45 80.50 78.50 83.09 79.49 79.81 80.37 67.38 65.90 69.92 65.27 70.10 71.62 68.69 64.68 50.00 63.33 32.13 37.56 74.25 74.25 BAGEL (Deng et al., 2025)* 70.97 71.79 78.16 78.12 78.00 79.50 80.24 79.08 76.25 75.77 68.23 68.19 73.37 77.49 64.36 66.15 68.92 61.48 80.00 80.00 40.72 52.40 76.87 74.63 MidJourney v7 (Midjourney, 2025) 65.92 62.43 73.96 74.63 75.00 82.00 78.74 78.51 68.12 68.55 63.44 62.59 70.60 74.03 64.43 59.58 58.84 61.34 66.67 33.33 31.67 34.39 79.22 75.32 Show-o2* (Xie et al., 2025b) 62.80 63.87 75.30 74.45 73.00 71.00 77.22 74.09 75.69 78.25 61.38 66.12 63.47 67.44 62.63 70.31 64.15 60.00 60.00 33.33 14.03 10.86 75.00 74.63 BAGEL w/ self-CoT (Deng et al., 2025)* 68.06 68.78 77.63 79.40 75.00 77.00 78.55 82.37 79.33 78.81 71.24 68.20 77.65 75.37 69.77 65.87 72.93 67.91 69.93 63.33 26.24 26.70 69.78 71.64 IRG (Ours) 76.00 73.77 83.17 81.28 81.00 76.00 82.96 81.86 85.54 85.98 75.25 74.66 75.82 77.25 78.16 77.76 73.84 72.93 90.00 70.00 43.89 47.51 72.76 74. GPT-4o (OpenAI, 2025c) 84.19 84.61 85.30 86.55 81.00 82.12 86.16 84.12 88.74 94.50 81.24 79.75 81.95 81.55 80.03 79.85 80.88 75.68 76.67 86.67 92.76 90.05 89.55 88.06 Table 4: GenAI-Bench Evaluation Results. * means we report the reproducing results using the official Github repository and checkpoint. The best results are bolded. Model Attribute Scene Basic Prompt Relation Spatial Action Part Advanced Prompt Avg Count Differ Compare Logical Negate Universal Overall Avg SD v2.1 (Rombach et al., 2022) SD-XL (Podell et al., 2024) FLUX.1-dev (Labs, 2024) LWM (Liu et al., 2024) Show-o (Xie et al., 2024) VILA-U (Wu et al., 2024c) Liquid (Wu et al., 2024a) UniTok (Ma et al., 2025) Mogao-7B (Liao et al., 2025) Janus-Pro-7B (Ma et al., 2024) BAGEL* (Deng et al., 2025) T2I-R1 (Jiang et al., 2025) Show-o2* (Xie et al., 2025b) BAGEL w/ self-CoT* (Deng et al., 2025) IRG (Ours) 0.80 0.84 0.87 0.63 0.72 0.78 0.85 0.89 0.87 0.85 0.86 0.90 0.79 0.84 0.88 0.62 0.72 0.78 0.87 0.90 0.89 0.88 0.88 0.92 0.76 0.82 0.87 0.65 0.70 0.77 0.85 0.89 0.89 0.86 0.87 0. 0.77 0.83 0.85 0.63 0.70 0.78 0.84 0.88 0.87 0.87 0.87 0.90 0.80 0.89 0.87 0.70 0.75 0.79 0.85 0.89 0.87 0.83 0.82 0.90 0.78 0.83 0.87 0.63 0.70 0.76 0.84 0.89 0.88 0.85 0.86 0. 0.68 0.71 0.75 0.59 0.70 0.70 0.76 0.76 0.77 0.73 0.77 0.81 0.74 0.81 0.84 0.70 0.73 0.78 0.58 0.62 0.71 0.73 0.76 0.74 0.73 0.77 0.82 0.74 0.78 0.78 0.68 0.69 0.74 0.54 0.71 0.74 0.74 0.79 0.77 0.71 0.79 0.78 0.76 0.81 0. 0.54 0.50 0.45 0.49 0.51 0.53 0.46 0.46 0.53 0.48 0.52 0.60 0.43 0.66 0.66 0.64 0.66 0.70 0.52 0.65 0.66 0.74 0.73 0.71 0.65 0.71 0.73 0.70 0.77 0.80 0.62 0.63 0.64 0.53 0.60 0.64 0.65 0.67 0.68 0.65 0.68 0.73 0.64 0.75 0. 0.70 0.73 0.76 0.58 0.65 0.70 0.75 0.79 0.81 0.75 0.81 0.84 prompts. Specifically, this prompt set is sourced from the training set of T2I-compbench (following Jiang et al. (2025)). In addition, for each prompt, we use Qwen3 (Yang et al., 2025) to generate 12 complex prompt variants. Furthermore, following Chen et al. (2025a), we construct prompts from common entities and then use Qwen3 to rewrite each into 23 complex variants (we do not use the original entity prompts directly to obtain the GPT-4o-distilled images). In total, this procedure produces approximately 30K samples. We use Qwen2.5VL (Bai et al., 2025) to generate the initial thinking and improving thinking processes, while GPT-4o generates high-quality images. Training settings. We adopt the unified multimodal understanding and generation model BAGEL Deng et al. (2025) as our base model. In the first training stage, we train the base model for 2K steps on the six decomposed learning modes using the cross-entropy (CE) loss and mean squared error (MSE) loss. We then continue training the model for 30K steps on the Initial Full Learning and Improving Full Learning tasks. 3.2 MAIN RESULTS To thoroughly assess the visual generation performance of our model, we conduct evaluation on series of representative benchmarks that cover complementary aspects of controllable and knowledgegrounded generation. Together, these benchmarks provide comprehensive view of our models 10 Preprint Table 5: Quantitative evaluation results on OneIG-EN. The overall score is the average of the five dimensions. The best results are bolded and the second best results are underlined. Model Alignment Text Reasoning Style Diversity Overall Janus-Pro (Chen et al., 2025c) BLIP3-o (Chen et al., 2025a) BAGEL (Deng et al., 2025) Show-o2-7B (Xie et al., 2025b) SDv1.5 (Rombach et al., 2022) SDXL (Podell et al., 2024) FLUX.1-dev (Labs, 2024) SANA-1.5 4.8B (PAG) (Xie et al., 2025a) Lumina-Image 2.0 (Qin et al., 2025) BAGEL w/ self-CoT (Deng et al., 2025) IRG (Ours) GPT-4o (OpenAI, 2025c) 0.553 0.711 0.769 0.817 0.565 0.688 0.786 0.765 0. 0.793 0.839 0.851 0.001 0.013 0.244 0.002 0.010 0.029 0.523 0.069 0.106 0.020 0.377 0.857 0.139 0.223 0.173 0.226 0.207 0.237 0.253 0.217 0. 0.206 0.239 0.345 0.276 0.361 0.367 0.317 0.383 0.332 0.368 0.401 0.354 0.390 0.427 0.462 0.365 0.229 0.251 0.177 0.429 0.296 0.238 0.216 0. 0.209 0.192 0.151 0.267 0.307 0.361 0.308 0.319 0.316 0.434 0.334 0.353 0.324 0.415 0.533 Table 6: Ablation study of IRG. The base method is BAGEL w/ self-CoT. High-quality Images Training means that we directly use the image data of Initial Full Learning and Improving Full Learning (see Sec. 2.2.2) to train the base model. Interleaving Reasoning Generation indicates that using the full IRG thinking-image trajectories during training. Decomposed Learning Modes means that we train the model in decomposed learning modes with two-stage training. Model WISE TIIF GenAI-Bench BAGEL w/ self-CoT (Deng et al., 2025) + High-quality Images Training + Interleaving Reasoning Generation + Decomposed Learning Modes (Ours) 0.70 0.73 0.76 0.77 68.06/68.78 70.69/69.85 73.90/71.37 76.00/73.77 0.81 0.80 0.83 0. strengths in alignment, reasoning, stylistic control, and text rendering. In the following, we present detailed comparisons with SoTA baselines and highlight the improvements achieved by our proposed IRG model. GenEval. Tab. 1 reports the quantitative results on the GenEval (Ghosh et al., 2023) benchmark, which evaluates compositional T2I generation across diverse object-centric attributes such as counting, color, and spatial position. We benchmark both generation-only models and unified understandinggeneration models. Among generation-only approaches, FLUX.1-dev achieves the best performance with an overall score of 0.82. Within the unified category, our proposed IRG model achieves the best overall score of 0.85, consistently outperforming all baselines across multiple sub-tasks, including challenging aspects such as counting (0.83) and position (0.74). These results establish IRG as new state of the art on GenEval, demonstrating strong controllability and precise compositional generation capabilities. WISE. Tab. 2 reports the quantitative results on the WISE (Niu et al., 2025) benchmark, which evaluates T2I models on complex semantic understanding and world knowledge reasoning across six domains: culture, time, space, biology, physics, and chemistry. Unlike compositional tests such as GenEval, WISE focuses on knowledge-grounded generation where models must accurately reflect real-world semantics in addition to compositional control. Among generation-only models, FLUX.1-dev achieves the best performance with an overall score of 0.50. Within the unified category, our proposed IRG model establishes new state of the art with an overall score of 0.77. It consistently outperforms prior unified models across all six domains, achieving 0.78 on cultural knowledge, 0.72 on temporal reasoning, 0.76 on spatial understanding, and above 0.80 in biology and physics. These results indicate that IRG not only improves general controllability but also integrates world knowledge more effectively than existing approaches, setting new benchmark for semantic alignment in T2I generation. 11 Preprint Table 7: Analysis of single-turn and multi-turn IRG pipeline. IRG reasoning step 1 means that the initial image generated by IRG as the evaluation images. Model Score WISE Rank score TIIF GenAI-Bench Score Score Qwen GPT-4o UnifiedReward Avg. IRG reasoning step 1 IRG (Ours) 0.79 0. 29% 71% 38% 62% 43% 57% 36.7% 63.3% 75.84/73.90 76.00/73.77 0.84 0. TIIF. Tab. 3 presents the quantitative results on TIIF testmini (Wei et al., 2025), benchmark specifically designed to evaluate T2I models ability to interpret and accurately follow complex natural language instructions. The benchmark covers both basic following (attributes, relations, and reasoning) and advanced following (multi-attribute reasoning, compositional control, stylistic adherence, and textual rendering), along with separate evaluation of designer-oriented prompts. When evaluated with QwenVL2.5-72B as the reference, our IRG model achieves the best overall performance with scores of 76.00 (short) and 73.77 (long). Notably, IRG demonstrates consistent improvements in advanced following tasks, achieving 75.25/74.66 on average, and excelling in compositional cases such as attribute+reasoning (78.16/77.76) and relation+reasoning (73.84/72.93). Overall, these results highlight the superior instruction-following ability of IRG, which consistently generalizes across different evaluation settings and significantly outperforms prior open-source systems, establishing it as new state of the art in controllable T2I generation. GenAI-Bench. Tab. 4 reports results on GenAI-Bench (Li et al., 2024a), which probes compositional text-to-visual generation across Basic (Attribute, Scene, and Relation: Spatial/Action/Part) and Advanced (Count, Differ, Compare, Logical: Negate/Universal) prompts. Our model IRG attains the best Overall score 0.84, exceeding strong baselines. IRG delivers the strongest overall performance on GenAI-Bench, combining robust basic grounding (attributes, scenes, relations) with improved compositional and logical generalization on advanced prompts. OneIG-EN. Tab. 5 summarizes the quantitative results on OneIG-EN (Chang et al., 2025), the English track of the OneIG-Bench benchmark that evaluates fine-grained T2I generation along five dimensions: alignment, text rendering, reasoning, style, and diversity. The final overall score is computed as the average across these dimensions. Our IRG model establishes new state of the art among open-source approaches with an overall score of (0.415), ranking second only to GPT-4o (0.533). IRG achieves the best alignment (0.839) and style (0.427) scores, and maintains balanced performance across reasoning (0.239) and text rendering (0.377). These results indicate that IRG not only excels in faithfully aligning with user prompts but also produces aesthetically consistent outputs, demonstrating its superior general-purpose generation capability. 3.3 ANALYSIS OF EXPERIMENTAL RESULTS Ablation study. Tab. 6 isolates the contributions of data and training objectives. Adding high-quality image training on top of self-CoT baseline brings moderate improvements (WISE 0.700.73, and TIIF 68.06/68.7870.69/69.85), while when introduce the IRG pipeline (i.e., using the data of Initial Full Learning and Improving Full Learning) achieve the significant improvement (0.03 benchmark score improvement in both WISE and GenAI-Bench). Furthermore, adopting IRG with six decomposed learning modes yields the largest jump (WISE 0.77, TIIF 76.00/73.77, GenAI-Bench 0.84). This supports our hypothesis that text-only thinking supervision (Sec. 2.2.2) is data-efficient proxy for scarce full IRG thinking-image trajectories, and that mixing text-based thinking learning, and full learning modes provides complementary signals. Analysis of single-turn and multi-turn IRG pipeline. As shown in Tab. 7, two-turn IRG and the initial generated images achieve similar benchmark scores, and on some benchmarks, initial generated image even attains slightly higher scores. However, this does not imply that multi-turn IRG offers no benefits. Our proposed IRG leverages image-conditioned reflection to enhance visual quality and fine-grained fidelity. 12 Preprint Figure 4: Visualization comparison results of BAGEL (Deng et al., 2025), BAGEL w/ self-CoT (Deng et al., 2025), IRG reasoning step 1 and our proposed IRG at 10241024 resolution. The examples are selected from WISE (Niu et al., 2025) and GenAI-Bench (Li et al., 2024a). Red boxes highlight the fine-grained details that have obvious flaws. 13 Preprint We evaluate the initial generated image and improved image generated by IRG on the WISE benchmark by prompting MLLM to directly compare the two images in terms of generation quality, fine-grained details, aesthetics, and other visual aspects. To eliminate positional bias, we randomly shuffle the order of the two images presented to the MLLM and repeat the evaluation three times, reporting the averaged results. Multiple MLLMs are employed as evaluators, including Qwen2.5-VL-72B (Bai et al., 2025), GPT-4o (OpenAI, 2025c), and UnifiedReward (Wang et al., 2025) (using its default pairwise comparison protocol, which incorporates the original task text prompt). This multiple MLLMs as judge method mitigates evaluator-specific biases and indicates better generalization of perceived quality. The ranking study in Tab. 7 shows that the full IRG pipeline improves agreement with multiple automatic raters compared to the first-step-only variant (average rank score: 63.3% vs. 36.7%), suggesting that two-turn IRG produces images whose improvements are consistently recognized by heterogeneous MLLM evaluators. Visualization comparison results. As illustrated in Fig. 4, compared to BAGEL and BAGEL w/ self-CoT, our proposed IRG achieves superior generation quality and visual fidelity. Moreover, relative to the first-turn generated images, the reflection step in IRG improves the visual quality and fine-grained fidelity of the initially generated images. For example, enhancing suboptimal textures and refining details that were previously poorly rendered. This demonstrates that IRG not only produces images that are semantically correct but also places strong emphasis on fine-detail quality in the generated content. Error analysis and failure modes. Visualization results (Fig. 1 and Fig 4) reveal remaining failure patterns: (1) Micro-structure saturation on repetitive textures (e.g., fabrics, foliage), where the improving step occasionally over-smooths high-frequency details; (2) Text rendering drift under dense constraints, where the refinement trades legibility for stylistic coherence; (3) Global-local tension in crowded scenes, where local edits improve parts while slightly perturbing global layout. We find these are most pronounced when (2) out introduces many simultaneous edits; conservative editing policy improves stability but may cap the attainable gains."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 UNIFIED MULTIMODAL UNDERSTANDING AND GENERATION MODELS Unified Multimodal Understanding and Generation Models has attracted much attention of the research community. The mainstream research works can be divide into three categories: (1) Autoregressive (Wu et al., 2025a; Chen et al., 2025c; Lu et al., 2024; Qu et al., 2024; Team, 2024; Wang et al., 2024b). These methods adopt the next token prediction paradigm to generate the text and image token in one unified model. (2) Additional Diffusion (Dong et al., 2024; Wu et al., 2024b; Pan et al., 2025; Tong et al., 2024). They usually combine pre-trained LLM backbone with an external diffusion module. The LLM is then used to obtain the semantic conditions that enable the diffusion module to generate an image. And (3) Unified Integrated Transformer (Deng et al., 2025; Liang et al., 2024; Ma et al., 2024; Shi et al., 2024; Zhou et al., 2024). In this category, works typically integrate the LLM and diffusion models in one transformer model. Our proposed IRG based on the unified integrated transformer model BAGEL (Deng et al., 2025) due to it was pre-trained on the large -scale interleaved textimage data. In principle, our proposed IRG framework can be effectively applied to all of the aforementioned types of unified models, as they naturally handle both interleaved inputs and interleaved outputs. 4.2 REASONING MODELS Text-based reasoning models have achieved significant progress in solving wide range of real-world tasks (Jaech et al., 2024; Guo et al., 2025; Huang et al., 2025; Chen et al., 2025b). Recently, several works have begun to adopt interleaving reasoning to address more complex problems (Huang, 2025; OpenAI, 2025b;a), i.e., incorporating non-text modalities into multi-turn reasoning processes. In the text-to-image (T2I) domain, the latest studies explore whether introducing text-based reasoning step can enhance image generation performance (Fang et al., 2025; Xiao et al., 2025b; Deng et al., 14 Preprint 2025; Jiang et al., 2025). Our IRG framework is inspired by these advances and seeks to integrate interleaving reasoning into the T2I generation process. While our approach shares conceptual similarities with reflection-based T2I methods (Zhuo et al., 2025; Wu et al., 2025b; Chern et al., 2025), the key distinction lies in IRGs dual focus: not only enhancing the semantic correctness of generated content but also improving visual quality, fine-grained fidelity, and aesthetic appeal."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose the Interleaving Reasoning Generation (IRG) framework, which generates high-quality images through textimagetextimage process. Specifically, given prompt, the model first produces text-based reasoning sequence and then generates an image conditioned on that reasoning. Next, building upon the initial image, the model reflects on how to improve its quality and produces refined image through this reflection process. We introduce the key designs for both the training and inference pipelines of IRG, and extensive experiments on mainstream benchmarks demonstrate significant improvements in generation performance. Furthermore, IRG places strong emphasis on enhancing visual quality and fine-grained details."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. OpenAI blog, 2023. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation, 2025. URL https://arxiv.org/abs/2506.07977. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025b. Xiaokang Chen, Chengyue Wu, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024. Preprint Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arxiv:2404.14396, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1573315744, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Wenxuan Huang. agi. for Awesome-Interleaving-Reasoning. Accessed 2025-08-19. GitHub repository, 2025. Interleaving reasoning: Next-generation systems URL https://github.com/Osilly/ reasoning Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. Genai-bench: Evaluating and improving compositional text-to-visual generation, 2024a. URL https://arxiv.org/abs/2406. 13743. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024b. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024c. 16 Preprint Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arxiv:2402.08268, 2024. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In CVPR, pp. 2643926455, 2024. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. Midjourney. midjourney v7, 2025. URL https://github.com/midjourney. Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. Introducing deep research. OpenAI Blog, 2025a. URL https://openai.com/ index/introducing-deep-research/. Accessed 2025-08-19. OpenAI. Introducing openai o3 and o4-mini. OpenAI Blog, 2025b. URL https://openai. com/index/introducing-o3-and-o4-mini/. Accessed 2025-08-19. OpenAI. Introducing 4o image generation, 2025c. URL https://openai.com/index/ introducing-4o-image-generation/. OpenAI. Introducing 4o image generation, March 2025d. URL https://openai.com/index/ introducing-4o-image-generation/. Accessed: 2025-05-09. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arxiv:2204.06125, 2022. 17 Preprint Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024a. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arxiv:2409.18869, 2024b. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions?, 2025. URL https://arxiv.org/abs/2506. 02161. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024a. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first ICML, 2024b. Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025c. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024c. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025a. 18 Preprint Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025b. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025a. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025b. Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arxiv:2408.11039, 2024. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems, 37:131278131315, 2024. Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080, 2025."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "The Chinese University of Hong Kong",
        "University of California, Los Angeles",
        "University of Oxford",
        "Xiaohongshu Inc.",
        "Zhejiang University"
    ]
}