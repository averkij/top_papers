{
    "paper_title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting",
    "authors": [
        "Nicole Cho",
        "William Watson",
        "Alec Koppel",
        "Sumitra Ganesh",
        "Manuela Veloso"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 9 6 6 1 . 8 0 5 2 : r QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting Nicole Cho, William Watson, Alec Koppel, Sumitra Ganesh, Manuela Veloso JP Morgan AI Research New York, NY nicole.cho@jpmorgan.com"
        },
        {
            "title": "Abstract",
            "content": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, bandit framework that designs rewrite strategies to maximize reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input queryand therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of query rewrite. Interestingly, certain static prompting strategies, which constitute considerable number of current query rewriting literature, have higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation."
        },
        {
            "title": "Introduction",
            "content": "As Large Language Models (LLMs) grow more powerful, their hallucinations increase in severity [76, 102]. Hallucinations refer to the generation of inaccurate outputs, according to the LLMs internal \"understanding\" of the query [41]. Mitigation efforts, however, remain largely confined to retrofitting the outputs instead of reshaping the query [41, 103], even though LLM outputs are highly variable to lexical perturbations of the incoming query [112, 16]. This style of analysis is in line with recent trends of mechanistic interpretability [3], where interpretable subgraphs of neural architectures are sought on the internal embeddings. However, these subgraphs may be unavailable due to enterprise restrictions [104]; moreover, there is no clear way to link particular subgraph template with hallucination. In this work, we design interventions to mitigate hallucinations based on semantic features of query. To formalize the relationship between query and hallucination, we associate with each query the outputs propensity to be hallucinatory, which is implicitly associated with reward model. We further hypothesize that this distribution is function of potential intervention which takes the form of query rewrite. To be more precise, for sllm [0, 1], binary score assigned by an LLM-based judge [66, 1], sfuzz [0, 1], fuzzy string similarity metric [8], and sbleu [0, 1], the BLEU-1 score Preprint. Under review. Figure 1: QueryBandits and Its Success in Mitigating Hallucination. The original query xt induces hallucinatory output: the LLM calculates 8 integers between 6 and 74/5. QueryBandits, by leveraging the feature vector, selects the EXPAND rewrite strategy. The rewritten query generates an accurate output of 9 integers. Noticeably, the feature vectors are different in the rewrite - subordination (more complex clauses) is now present while specialization (query requiring domainspecific knowledge for understanding) is absent - signifying effects of the EXPAND strategy. capturing unigram lexical overlap [79, 61, 13], we define hallucination in terms of reward model rt = α sllm + β sfuzz + γ sbleu where hallucinatory responses are those associated with low rewards. Through our ablations, we also discover the Pareto-optimal balance of weights (Fig. 2a), by assigning higher weight on LLM-as-a-judge, which is also evidenced in studies that highlight the efficacy of LLMs in Natural Language Generation (NLG) evaluation tasks [109, 121, 30]. As we are agnostic to the stationarity of the reward distribution or lack thereof due to the extreme dimensionality of the output space [86, 40], we propose to evaluate whether rewrite strategies exhibit advantages when seeking to optimize the average reward or its worst case. The usage of reinforcement learning (RL) [100] methods have been applied in Natural Language Processing (NLP) tasks such as optimizing document-level query search [75], fine-tuning LLMs [25, 78], and post-training [73]. Despite its prevalent usage, to our knowledge, there is no in-depth interactive rewriting research to mitigate hallucination. We focus on bandit based methods because: (i) modeling the long-term value of hallucination manifestation would require multiple queries from common sub-population; (ii) averaging hallucination propensity across distinct contexts may obscure per-query contextual idiosyncrasies; and (iii) the token concatenation that defines how vocabulary sampling occurs in output generation is deterministic, meaning it is unclear if an MDP transition model may even be defined. That is not to say bandit methods have no precedent in NLP. Proximal Policy Optimization [93] variants for LLMs such as GRPO (Group Relative Policy Optimization) [97] and ReMax [58] also remove the critic via grouped Monte Carlo or baseline-adjusted returns. We have meticulously selected five distinct rewriting arms/strategies; to our knowledge, current research tends to pursue \"one-size-for-all\" approach, leveraging one of these rewriting strategies for all query types, and does not pursue guided rewrites via bandits [68, 112]. We also have rigorously selected 17 different linguistic features that are known to hinder/enhance human or LLM understanding  (Table 1)  . We frame query rewriting as an online decision problem and by leveraging per-query features, our bandit framework, QueryBandits, allocates exploration where uncertainty is high and exploitation where features are meaningfulresulting in hallucination reduction. Contribution 1: We introduce an empirically validated reward function, combining an LLM-judge, fuzzy-match, and BLEU metrics (with α, β, γ = (0.6, 0.3, 0.1)) chosen inside the 1% Paretooptimal frontier on held-out human-labeled set, to drive context-aware bandit learning (Fig. 2a). Guided by this reward signal, our contextual QueryBandits learn to tailor each query rewrite to its linguistic/contextual fingerprint. Our best performing contextual bandit, QueryBandits-Thompson Sampling, drives 87.5% win-rate boost over the NO-REWRITE baselinea considerable margin that highlights the efficacy of rewriting in reducing hallucination. 2 Contribution 2: QueryBandits-Thompson Sampling delivers decisive 42% gain against the predominant static prompting strategy (PARAPHRASE), underscoring that feature-aware rewriting with bandits is effective for mitigating hallucination. In Figure 3, it is clear that the contextual QueryBandits hone into the optimal rewrite quickly, accruing an order-of-magnitude less cumulative regret than static prompting, vanilla (non-contextual) bandits, or no-rewriting. These gains confirm that feature-aware, online adaptation mechanism can consistently outpace one-shot heuristics in mitigating hallucinations. An interesting finding is that some static prompting methods have higher cumulative regret than NO-REWRITE, demonstrating that zero-shot prompting can cause more severe hallucinations. Contribution 3: We provide empirical evidence that there is no single rewrite strategy that maximizes the reward for all types of queries. Our analysis of the per-arm regression weights (Figure 5) reveals how each arms effectiveness hinges on the semantic features of query. For example, if query displays the feature (Domain) Specialization, meaning that the query can only be understood with domain-specific knowledge, the rewrite arm EXPAND is very effective in contrast to SIMPLIFY (Figure 1). Crucially, ablating the 17-dimensional feature input causes QueryBandits-Thompson Samplings performance to drop to just 81.7% win rate and 754.66 exploration-adjusted reward. This performance gap confirms that the linguistic features carry an associative signal about optimal rewrite strategy. Finally, we observe that across datasets, higher feature variance coincides with greater variance in arm selection (Figure 4), resulting in genuinely diverse arm choices (Figure 2b). Contribution 4: Optimizing queries post-training by embedding them directly into the stage of prompting with minimal computational or token overhead constitutes an efficient strategy for trustworthy interfacing with LLMs, particularly under resource-constrained or latency-sensitive conditions. We bypass the need for retraining or gradient-based adaptation through purely forward-pass mechanisms. Moreover, through QueryBandits, we provide mechanism to interpret the sensitivity of LLM performance to contextual rewrites. Interesting Findings: On many standard benchmark datasets, we discover that linear contextual bandits converge almost exclusively to the NO REWRITE arm (Figure 7), empirically exposing LLMs tendency for query memorization on benchmarks. Only when we introduce semantically invariant but lexically perturbed queries does the policy meaningfully diversify across rewrite strategies; meaningful insight for the research community that surface-form novelty is essential in training query rewriting algorithms. On the non-contextual bandits, we empirically discover that they converge to single rewrite strategy within dataset, in contrast to contextual bandits that tend to diversify its choices conditioned on the presence/absence of linguistic features."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Hallucinations are known to erode trustworthiness from societal perspective [24]. Recently, certain conceptual analyses frame it as new epistemic failure mode, requiring dedicated mitigation agendas [118, 78]. Moreover, the release of more advanced reasoning models are concerningly generating more hallucinations, as reported in OpenAIs technical report [76] on o3 and o4-mini. The Times [102] recently reported real-world case studies on how fabricated LLM outputs are prompting legal accountability. Especially, the advent of more LLM-Agent enabled systems [113, 111] will engender the compounding cost of hallucinations. Thus, mitigation is regarded as indispensable for faithful LLM interactions [41, 103, 38] and research is expanding from post-hoc detection [69], to preemptive grounding. Parallel to human-in-theloop RLHF, RL from AI Feedback (RLAIF) trains reward model on preferences generated by an LLMbypassing expensive human labelswhile exceeding RLHF quality on summarization and dialogue tasks [53, 78, 18]. Watson et al. [112] introduced pre-generation hallucination estimation via query perturbations. Ma et al. [68] has proposed the Rewrite-Retrieve-Read framework for Retrieval Augmented Generation pipelines while human-designed query rules has been heavily used for rewriting [64, 70, 15]. common theme is that either raw prompting or manual heuristics is used - not guided rewrites, through contextual signals of the original query. Blevins et al. [11] has conducted extensive research on the strong performance of Pretrained Language Models (PLMs) to retrieve linguistic features of query in few-shot manner. We employ this research and leverage an LLM to identify key linguistic features as outlined in Table 1. For the selection of 3 Table 1: Binary linguistic feature vector {0, 1}17 identified as challenging from linguistics and LLM perspective. Features are grouped by type and grounded in prior work. For more specific examples, see Appendix Table 5. Feature Description Structural Features Anaphora Subordination Contains anaphoric references (e.g., it, this) Contains multiple subordinate clauses Scenario-Based Features Mismatch Presupposition Assumptions within the query are implicitly regarded as truthful Pragmatics Query (e.g. open-ended) does not match task (e.g. retrieval) Queries with discourse-driven intent (i.e. \"can you pass me the salt\") Citation [94, 14] [39, 2, 11] [31, 46] [47, 55, 90] [98, 55, 89] Lexical Features Rarity Negation Superlative Polysemy Presence of rare words with poor representation Presence of negation (e.g., not, never) Usage of superlative forms (e.g., best, largest) with implicit semantics Presence of words that have multiple, related-meanings [91, 49] [37, 48, 105] [81, 29] [5, 34] Stylistic Complexity Answerability Excessive Subjectivity Ambiguity Query is not highly speculative, sarcastic or rhetorical Overloaded with large amount of details and information Query requires LLM to reflect creatively and engender personal opinion Presence of ambiguous phrasing that opens multiple interpretations [82, 9, 56] [57, 65] [28, 67] [12, 50, 63] Semantic Grounding Grounding Constraints Entities Specialization Presence of clear intention and goal Presence of temporal/spatial/task-specific constraints Presence of verifiable entities Query requires domain-specific knowledge for understanding [22, 114] [42, 56] [54, 6, 110] [113, 17, 123] Table 2: Overview of datasets, including domain, license, number of examples, associated scenarios, etc. These datasets span diverse range of question types, domains, and reasoning skills, supporting robust evaluation. = Extractive, = Multiple Choice, = Abstractive. Dataset Scenario Domain License Count Citation SQuADv2 TruthfulQA SciQ MMLU PIQA BoolQ OpenBookQA MathQA ARC-Easy ARC-Challenge WikiQA HotpotQA TriviaQA E, M, M, M M Wikipedia General Knowledge Science Various Physical Commonsense Yes/No Questions Science Reasoning Mathematics Science Science Wikipedia QA Multi-hop Reasoning Trivia CC BY-SA 4.0 Apache-2.0 CC BY-NC 3.0 MIT AFL-3.0 CC BY-SA 3.0 Apache-2.0 Apache-2.0 CC BY-SA 4.0 CC BY-SA 4.0 Other CC BY-SA 4.0 Apache-2.0 86K 807 13K 15K 17K 13K 6K 8K 5K 2.6K 1.5K 72K 88K [84, 85] [62] [43] [36] [10] [19, 108] [72] [4] [21] [21] [116] [117] [44] these features, we have thoroughly reviewed not only existing LLM literature but also traditional linguistics to identify features that are known to impact both human and LLM understanding."
        },
        {
            "title": "3 Methodology and Evaluation Metrics",
            "content": "In this section, we define the key ingredients to formulate sequential decision-making problem as multi-armed bandit [52]. Specifically, we define the action space, contextual attributes, and reward. Bandit Formulation. In the contextual bandit framework [52], learner is faced with, given context vector xt Rd at time t, selecting an arm at from an action set A. Upon that basis, Nature reveals reward rt(xt, at) which is function of the context and arm. To be more precise, 4 the reward is defined as : R. The goal of bandit algorithm is to select arms that are eventually good with respect to the average (or cumulative) reward. In the stochastic bandit setting, specifically, one is interested in selecting arms according to policy π : ρ(A) which performs as well as maxπΠ E[r(x, π(x))]. Here ρ(A) denotes the probability simplex over arms, and Π is some class of policies. Next we specify the concrete choices of action space, context variables, and rewards for the query rewrite setting. Action Space. Let = {a0, a1, a2, a3, a4} denote the set of rewriting strategies (arms), where each arm represents distinct style of query reformulation implemented via prompt-based instructions to an LLM. As outlined in 2, previous research tends to take \"one-size-for-all\" approach, a0: Paraphrasing - The incoming query is rewritten using prompt such as Paraphrase this question while preserving its meaning. This introduces lexical diversity while maintaining semantic similarity, testing whether alternative phrasings reduce hallucination. Many have explored how paraphrasing can improve factual consistency in LLMs [16, 27, 115] . a1: Simplification - The incoming query is rewritten to eliminate nested clauses or complex syntax. This targets hallucinations caused by long-range dependencies or overloaded details - and borrows ideas from educational psychology where simpler, granular, prompts enable child to learn new skill [59]. Recently, Van et al. [106], Zhou et al. [124] report on how simplified prompts decrease off-topic generations and enable complex reasoning tasks. a2: Disambiguation - The query is rewritten by disambiguating vague references (e.g., ambiguous pronouns or temporal expressions). Many have conducted extensive studies on LLMs inabilities to resolve ambiguous queries which leads to subpar performance [26, 95, 23]. a3: Expansion - The query is rewritten to explicitly expand on relevant named entities or attributes, elaborating on contextual cues through generation [120]. Since transformers-based LLMs optimize next-token likelihood over attention-mediated context windows [107], appending fine-grained query constraints effectively conditions the model on richer semantic prefix. a4: Clarification of Certain Terms - The query is rewritten to clarify the lexical and semantic meaning of jargons. Since LLMs are pre-trained on broad domain corpora using maximum likelihood next token prediction, rare domain-specific jargons [20] suffer from sparse exposure and less-calibrated embeddings [87, 80]. Contextual Attributes. For each incoming query, we extract 17-dimensional binary feature vector {0, 1}17 that captures key linguistic properties as outlined in Table 1, grounded by related works in NLP (2). We have rigorously selected features that are known to impact both human linguistic understanding and LLM performance. Reward Model. We model each rewritten querys reward rt [0, 1] as convex combination of three complementary correctness signals: rt = α sllm + β sfuzz + γ sbleu, α + β + γ = 1, α, β, γ 0 (1) sllm {0, 1} is binary consistency judgment by GPT-4o-based assessor, calibrated on factual correctness between system and reference answers [66, 1]. sfuzz [0, 1] is token-set similarity from RapidFuzz [8], capturing soft string overlap. sbleu [0, 1] is the BLEU-1 score (unigram precision) under unit-cap [79, 61, 13], ensuring lexical fidelity. This multi-faceted formulation mitigates individual failure modes inherent in any single metric (e.g. BLEUs paraphrase blindness or edit-distance oversensitivity) while remaining bounded for stable learning. Following Wang et al. [109], we leverage the strength of LLMs-as-judges, and, as demonstrated by Test-Time RL [125], even noisy, self-supervised signals (e.g. pseudo-labels from majority-voted LLM outputs) can effectively guide policy updates. However, to validate that our convex proxy aligns with human judgment, we assembled held-out, manually labeled set of 100 queryanswer pairs and measured ROCAUC of rt against binary human labels (Figure 2a). See Alg. 1 in Appendix A.2 for further detail. Reward-Weight Simplex Analysis. We swept (α, β, γ) over triangular grid (α + β + γ = 1) and computed ROCAUC on the human-labeled validation set. Figure 2a plots each grid points AUC and overlays the 1% Pareto frontier (dark region). Our manual weights (α, β, γ) = (0.6, 0.3, 0.1) lie well inside this frontier, demonstrating robustness. The Pareto frontier reveals the following: LLM-Judge Robustness (α): The ROCAUC surface is nearly invariant when α varies by 0.2: AUC shifts by <0.5%, indicating our formulation tolerates large LLM-judge weight swings. 5 (a) ROCAUC Pareto frontier on the (b) Mean-reward ranks (1 = best) of each rewrite arm per dataset under reward-weight simplex. our contextual bandit; color intensity indicates closeness to the top rank. Figure 2: (a) Our chosen (α, β, γ) lies deep in the 1% optimal frontier. (b) Breakdown of per-dataset arm performance: different datasets consistently favor different rewrite strategies Fuzzy-Match Sensitivity (β): Small increases in β rapidly exit the Pareto region, showing that the fuzzy-match term must be tuned carefully to avoid degrading overall accuracy. BLEU-Only Pitfall (γ): As γ increases, ROCAUC steadily declines, bottoming out at γ = 1 (pure-BLEU), where the model over-emphasizes surface overlap at the expense of true correctness. Pareto-Optimal Region: Our chosen (0.6, 0.3, 0.1) sits deep in the high-AUC plateau, confirming it is Pareto-optimal trade-off among semantic, fuzzy, and lexical signals. Together, these experiments on manually labeled data substantiate our reward design: the LLMjudge component provides forgiving anchor, fuzzy-match demands precise calibration, and BLEU contributes complementary lexical oversight. Remark 1 (On RL Methods vs. Bandits) Within LLMs, for each input query, the transformer attends over the fixed context window and computes softmax over the vocabulary to maximize token likelihood [83]. Consequently, we believe that hallucinations occur at the moment of generation for that single query, making hallucination per-query phenomenon. Indeed, recent PPO variants for LLMsGRPO [97] and ReMax [58]remove the critic via grouped Monte Carlo or baselineadjusted returns, highlighting critic-free policies that our bandit formulations naturally generalize. Therefore, full-episodic RL problem, which must solve Markov decision process with longhorizon credit assignment and nonstationary transition dynamics [100], can be practically suboptimal. Moreover, many of these methods rely on estimating fixed average reward or state-action value Q(s,a) which can obscure per-query idiosyncrasies; if the optimal rewrite arm varies sharply with linguistic context, mere empirical average will yield suboptimal policies. Remark 2 (Linkage between Algorithm Choices and RL Methods) Furthermore, it should be underscored that several algorithms whose usage we investigate here have analogues in RL: posterior sampling (PSRL) [77] as an analogue for Thompson sampling [101]; follow-the-regularized leader (FTRL) and its variants [96], originate from proximal gradient algorithms [88] whose usage in RL as proximal policy optimization (PPO) [92] is well-established. Other PPO-style advances like DAPO [119] improve exploration-exploitation via dynamic sampling and reward filtering, and VAPO [122] demonstrates stable Long-CoT training with an explicit value modelshowing the spectrum from model-based to model-free approaches that contextual bandits sit within. Choice of Algorithms. For linear contextual bandits we fit per-arm linear model θk and choose either UCB (LinUCB [51] / KL-UCB [32]), an FTRL regularized weight [71], or posterior draw (Thompson sampling [101]). For adversarial bandits we consider two parameter-free adversarial methodsEXP3 [7] and FTPL [45, 99]. For full scoring, update equations and regret bounds, see Appendix A.2 and Algorithm 1. Evaluation Metrics. We assess each rewrite policy using three complementary metrics that capture both its ability to explore promising rewrites and its final accuracy relative to no-rewrite baseline. All three metrics together give balanced view of (1) how well policy explores and exploits, (2) how quickly it converges to good answers, and (3) how often it beats the baseline in reward. Metric 1: Exploration-Adjusted Reward. Let rt [0, 1] be the reward at pull and let Ht [0, 1] be the normalized Shannon entropy of the policys strategy-selection history up to t. We define the 6 final exploration-adjusted reward as Radj = (cid:88) (cid:0)rt + λ Ht (cid:1), t=1 where λ = 0.1 weights the bonus for exploration and is the trajectory length. This metric rewards policies that both achieve high per-pull rewards and maintain sufficient exploration. Metric 2: Mean Cumulative Regret. At each pull we compute instantaneous regret as the gap between the oracle reward (the best achievable rewrite) and the observed reward. Summing these gives the cumulative regret per run, and we report its average over all runs. Where is the maximal reward at pull t: Regret ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) (cid:88) runs t=1 (cid:0)r rt (cid:1), Metric 3: Win Rate vs. Baseline. To measure final correctness head-to-head, we treat each test pull as an independent trial and compute the fraction of trials for which policys reward rpolicy strictly exceeds the no-rewrite baselines reward rbase . This directly quantifies how often each rewrite outperforms doing nothing. For = 100 test queries, the win rate is WinRate ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) t=1 1(cid:2)rpolicy > rbase (cid:3) 100%"
        },
        {
            "title": "4 Experiments and Results",
            "content": "Pipeline. For each decision round t: Extr. feat. ft{0,1}d ft xt Select at (rewrite strat.) = gat(xt) LLM yt Eval. rt[0,1] rt Update Bandit 1. Feature Extraction. For query xt, compute d-dimensional linguistic feature vector ft {0, 1}d. 2. Arm Selection. The bandit receives ft and selects rewrite arm at {1, . . . , K}. 3. Query Rewriting. Apply the selected arm to obtain the candidate query 4. LLM Inference. Issue to gpt-4o-2024-08-06, producing response yt. 5. Reward Evaluation. Compute scalar reward rt [0, 1] via our reward formulation. 6. Bandit Update. Update the internal state of the bandit based on (at, rt). = gat(xt) . Dataset and Query Construction. We evaluate on = 13 diverse QA benchmarks (see Table 2). For each dataset, we sample queries satisfying: (1) Original Answerability: the query in the dataset (q) is answered correctly by gpt-4o-2024-08-06, and (2) Perturbation Validity: of its five lexically perturbed but semantically invariant versions of the datasets query, measured by numerous metrics such as LLM-as-judge and n-gram based metrics [60, 79, 109, 30], between one and three perturbations yield incorrect answers. Then, we randomly choose xt in to train QueryBandits. The importance of this query construction process deserves emphasis. Through our investigations, we have discovered that the ubiquity of benchmark datasets in Table 2 within pre-training and fine-tuning regimes has engendered potentially pernicious form of prompt memorization. When we deploy our linear contextual bandits, the policy converges almost exclusively to no-rewriting, effectively demonstrating that the LLM has most likely memorized these exact phrasings rather than learning to exploit deeper linguistic structure (Figure 7). To evaluate genuine rewrite efficacyand to prevent our results from collapsing into trivial memorization baselinewe therefore choose perturbed version of the datasets query to construct the incoming query for our bandits. This experimental setup thus prioritizes query-rewriting strategies that are non-degenerate. Experimental Configuration. We compare three non-contextual and six linear contextual bandits against zero-shot prompting and no-rewrite baseline. All reported metrics are averaged over all dataset runs per algorithm. We compare bandit algorithms and prompting strategies over = 5 rewrite arms. Each algorithm runs for = QD rounds on each of the datasets  (Table 2)  . Thus, Total Pulls = QD = 253, 440, with QD 1050, = 15, and = 16. We bootstrap samples with replacement for TruthfulQA to obtain approximately 1050 queries. 7 Figure 3: Cumulative reward averaged across all datasets over the no-rewrite baseline for each algorithm (sorted by final performance), highlighting the superior gains achieved by contextual bandits compared to non-contextual learners and static prompt-based rewrites. Table 3: Rewrite-policy performance: final cumulative exploration-adjusted reward, mean cumulative regret, and win rate vs. no-rewrite baseline. Algorithm Contextual? Adj. Reward Cum. Regret Win Rate Thompson Sampling (Contextual) LinUCB with KL LinUCB Linear ϵ-FTRL EXP3 (Non-Contextual) Linear EXP3 Thompson Sampling (Non-Contextual) Linear FTPL FTPL (Non-Contextual) Prompting (Paraphrase) Prompting (Simplify) Prompting (Disambiguate) Prompting (Clarify Terms) Prompting (Expand) Baseline (No Rewrite) 819.04 818.79 818.60 799.57 797.47 781.05 754.66 738.07 716. 732.39 730.13 713.65 711.65 639.25 729.20 135.84 136.00 136.12 155.30 157.31 173.60 200.18 216.54 238.85 222.56 224.42 241.25 243.35 315.71 225.85 87.5% 87.0% 86.9% 85.0% 86.5% 83.8% 81.7% 76.3% 62.8% 44.9% 50.1% 42.4% 38.2% 27.2% Hyperparameters (learning rates, exploration coefficients, regularization constants) are tuned via grid search on held-out validation set. Hypothesis 1: Can QueryBandits reduce hallucination? Table 3 and Figure 3 compares our QueryBandit algorithms against the baseline and five static prompting scenarios on 13 QA benchmarks (1,050 queries per dataset). Our top contextual learnerThompson Sampling with the 17-dimensional feature vectorachieves an 87.5% win rate and 819.04 exploration-adjusted reward, compared to the BASELINE: NO REWRITE, signifying that contextual query rewriting can reduce hallucination. As side note: we aggregate results across datasets rather than report per-dataset Monte Carlo trials, as within-dataset permutation yielded trivial randomization. Hypothesis 2: Can QueryBandits outperform prompting? Our best performing bandit, Thompson Sampling, exceeds the performance of static prompting for PROMPTING (PARAPHRASE) (44.9% / 732.39) and PROMPTING (EXPAND) (27.2% / 639.25), as seen in Table 3 and Figure 3. These gains confirm that raw static prompting cannot approach the effectiveness of learner that adapts its rewrite choice to each querys linguistic fingerprint. By framing rewrite selection as an online decision problem and leveraging per-query context, QueryBandits allocate exploration where uncertainty is high and exploitation where features reliably predict hallucination riskresulting in up to double the hallucination reduction of any static strategy, with no additional model fine-tuning. Hypothesis 3: Do linear contextual bandits outperform those which are oblivious to context? Crucially, ablating the 17-dimensional feature input causes Thompson Samplings performance to drop to just 81.7% win rate and 754.66 exploration-adjusted reward (5.8 pts, 64.38 reward points), despite identical hyperparameters and run count. Since our reward directly measures output correctness, this performance gap confirms that the linguistic features carry associative signal about hallucination risk. Moreover, the majority of our linear QueryBandits outperform those which are oblivious to context  (Fig. 3)  , signifying the importance of taking context into account in rewrites. 8 Figure 4: Contextual Per-Feature Variance by Arm. For each arm, we compute the variance of each binary linguistic feature over all queries on which that arm was chosen. High variance means the bandit frequently switches the arm on that features presence. Figure 5: Contextual Feature Contribution Strength. These are the averaged θ weights (direct contributions) of each feature to the expected reward under each arm. Positive weights indicate features that boost that arms reward; negative weights indicate features that penalize it. Hypothesis 4: Is there an association between query features and reward? Each rewrite arm seems to exhibit different sensitivities toward different linguistic features. Figures 4 & 5 plot each arms average variance and learned regression weights θ across 17 binary linguistic features. Interestingly, the same linguistic feature can flip from strongly sensitive for one arm to insensitive for anotherfor example, the feature (Domain) Specialization is quite sensitive to the arm/strategy EXPAND but relatively much less to SIMPLIFY. plausible explanation might be that domain specific queries inherently require specialized context outside the LLMs broad-domain priors, so EXPANDby injecting explicit entity attributes or ontological qualifiersreinforces the models semantic grounding, whereas SIMPLIFY risks excising those critical signals. While our association matrix is interesting, each learned coefficient does not strictly prove causal mechanism. Hypothesis 5: Is there single rewrite strategy that maximizes reward for all types of queries? Our analysis of the per-arm regression weights (Figure 5) reveals that no single rewrite strategy dominates across all linguistic profiles. Instead, each arms effectiveness hinges on distinct feature footprint. For example, SIMPLIFY thrives when pragmatic cues (e.g. discourse markers, politeness markers) are presentthese guide safe syntactic pruningbut falters on superlative constructions, whose removal strips away essential comparative meaning. For our interpretation of these sharp inversions featurearm interactions, refer to Appendix Table 4. Therefore, our results demonstrate that the diversity of arm selected is correlated with feature varianceand that there is no single rewrite arm that fits all queries."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented novel bandit framework of query rewriting to mitigate hallucinations. By comparing QueryBandits against NO-REWRITE baseline and static prompting strategies across diverse benchmarks, our best-performing contextual bandit consistently outperforms by considerable margin in terms of win-rates and cumulative regret. We have also learnt that rewrite strategies are sensitive to linguistic features, and that we can exploit these features for no-regret rewriting. Through purely forward-pass mechanism, QueryBandits delivers meaningful results in terms of not only hallucination mitigation but also paves the way for LLM interpretability. Future work can dive deeper into casual inference techniques and capture pairwise polynomial feature interactions within the queryand thus, further enhance trustworthiness of LLM systems and their societal value (A.1)."
        },
        {
            "title": "Disclaimer",
            "content": "This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates (\"JPMorgan) and is not product of the Research Department of JPMorgan. JPMorgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."
        },
        {
            "title": "References",
            "content": "[1] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluating correctness and faithfulness of instruction-following models for question answering, 2024. URL https: //arxiv.org/abs/2307.16877. [2] Ahmed Alajrami and Nikolaos Aletras. How does the pre-training objective affect what large language models learn about linguistic properties?, 2022. URL https://arxiv.org/abs/2203.10415. [3] Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. Circuit tracing: Revealing computational graphs in language models. Transformer Circuits Thread, 2025. URL https://transformer-circuits.pub/2025/ attribution-graphs/methods.html. [4] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23572367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/ N19-1245. URL https://aclanthology.org/N19-1245. [5] Alan Ansell, Felipe Bravo-Marquez, and Bernhard Pfahringer. Polylm: Learning about polysemy through language modeling, 2021. URL https://arxiv.org/abs/2101.10448. [6] Dhananjay Ashok and Zachary C. Lipton. Promptner: Prompting for named entity recognition, 2023. URL https://arxiv.org/abs/2305.15444. [7] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):4877, 2002. doi: 10.1137/S0097539701398375. URL https://doi.org/10.1137/S0097539701398375. [8] Max Bachmann. rapidfuzz/rapidfuzz: Release 3.8.1, April 2024. URL https://doi.org/10.5281/ zenodo.10938887. [9] Anas Belfathi, Nicolas Hernandez, and Laura Monceaux. Harnessing gpt-3.5-turbo for rhetorical role prediction in legal cases, 2023. URL https://arxiv.org/abs/2310.17413. [10] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. [11] Terra Blevins, Hila Gonen, and Luke Zettlemoyer. Prompting language models for linguistic structure, 2023. URL https://arxiv.org/abs/2211.07830. [12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. 10 [13] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Diana McCarthy and Shuly Wintner, editors, 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 249256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032/. [14] Hong Chen, Zhenhua Fan, Hao Lu, Alan Yuille, and Shu Rong. PreCo: large-scale dataset in preschool vocabulary for coreference resolution. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 172181, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1016. URL https://aclanthology.org/D18-1016/. [15] Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, and Chuchu Fan. PRompt optimization in multi-step tasks (PROMST): Integrating human feedback and heuristic-based sampling. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 38593920, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.226. URL https: //aclanthology.org/2024.emnlp-main.226/. [16] Nicole Cho and William Watson. Multiq&a: An analysis in measuring robustness via automated crowdsourcing of question perturbations and answers, 2025. URL https://arxiv.org/abs/2502. 03711. [17] Nicole Cho, Nishan Srishankar, Lucas Cecchi, and William Watson. Fishnet: Financial intelligence from sub-querying, harmonizing, neural-conditioning, expert swarms, and task planning. In Proceedings of the 5th ACM International Conference on AI in Finance, ICAIF 24, page 591599. ACM, November 2024. doi: 10.1145/3677052.3698597. URL http://dx.doi.org/10.1145/3677052.3698597. [18] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/abs/1706.03741. [19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. [20] HERBERT H. CLARK and RICHARD J. GERRIG. Understanding old words with new meanings, 1983. URL https://web.stanford.edu/clark/1980s/Clark.Gerrig.oldwords.83.pdf. [21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. [22] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. Overview of the TREC 2009 web track. In Ellen M. Voorhees and Lori P. Buckland, editors, Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 17-20, 2009, volume 500-278 of NIST Special Publication. National Institute of Standards and Technology (NIST), 2009. URL http://trec.nist.gov/pubs/trec18/papers/WEB09.OVERVIEW.pdf. [23] Jeremy Cole, Michael Zhang, Daniel Gillick, Julian Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 530543, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.35. URL https://aclanthology.org/2023.emnlp-main.35/. [24] Dechert LLP. Ai ber ai-expert-challenged-for-relying-on-ai--hallucinations-.html. 05-12. \"hallucinations\", Decemchallenged https://www.dechert.com/knowledge/re-torts/2024/12/ 2025expert URL Accessed: relying 2024. for on ai [25] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng 11 Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [26] Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, and Tat-Seng Chua. Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Assonon-collaboration. ciation for Computational Linguistics: EMNLP 2023, pages 1060210621, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.711. URL https://aclanthology.org/2023.findings-emnlp.711/. [27] Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves, 2024. URL https://arxiv.org/abs/2311.04205. [28] Esin Durmus, Karina Nguyen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. Towards measuring the representation of subjective global opinions in language models, 2024. URL https: //arxiv.org/abs/2306.16388. [29] Donka Farkas and Katalin É Kiss. On the comparative and absolute readings of superlatives, 2000. [30] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023. URL https://arxiv.org/abs/2302.04166. [31] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. URL https://arxiv.org/abs/2312.10997. [32] Aurélien Garivier and Olivier Cappé. The kl-ucb algorithm for bounded stochastic bandits and beyond, 2013. URL https://arxiv.org/abs/1102.2490. [33] E. J. Gumbel. The return period of flood flows. URL doi:10.1214/aoms/1177731747. [34] Janosch Haber and Massimo Poesio. PolysemyEvidence from linguistics, behavioral science, and contextualized language models. Computational Linguistics, 50(1):351417, March 2024. doi: 10.1162/ coli_a_00500. URL https://aclanthology.org/2024.cl-1.10/. [35] James Hannan. Approximation to bayes risk in repeated play, 1957. [36] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [37] Md Mosharaf Hossain and Eduardo Blanco. Leveraging affirmative interpretations from negation improves natural language understanding, 2022. URL https://arxiv.org/abs/2210.14486. [38] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, January 2025. ISSN 1558-2868. doi: 10.1145/3703155. URL http://dx.doi.org/10. 1145/3703155. 12 [39] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. Adaptive-RAG: Learning to adapt retrieval-augmented large language models through question complexity. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 70367050, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.389. URL https://aclanthology.org/2024.naacl-long.389/. [40] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating LLM hallucination via self reflection. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 18271843, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.123. URL https://aclanthology.org/2023.findings-emnlp.123/. [41] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating hallucination in large language models via self-reflection, 2023. URL https://arxiv.org/abs/2310. 06271. [42] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: multi-level fine-grained constraints following benchmark for large language models, 2024. URL https://arxiv.org/abs/2310.20410. [43] Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. 2017. [44] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017. [45] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291307, 2005. ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2004. 10.016. URL https://www.sciencedirect.com/science/article/pii/S0022000004001394. Learning Theory 2003. [46] Gaurav Kamath, Sebastian Schuster, Sowmya Vajjala, and Siva Reddy. Scope ambiguities in large language models. Transactions of the Association for Computational Linguistics, 12:738754, 2024. ISSN 2307-387X. doi: 10.1162/tacl_a_00670. URL http://dx.doi.org/10.1162/tacl_a_00670. [47] Lauri Karttunen. Presupposition: What went wrong?, 2016. [48] Nora Kassner and Hinrich Schütze. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811 7818, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.698. URL https://aclanthology.org/2020.acl-main.698/. [49] Yerbolat Khassanov, Zhiping Zeng, Van Tung Pham, Haihua Xu, and Eng Siong Chng. Enriching rare word representations in neural language models by embedding matrix augmentation. In Interspeech 2019, interspeech2019, page 35053509. ISCA, September 2019. doi: 10.21437/interspeech.2019-1858. URL http://dx.doi.org/10.21437/Interspeech.2019-1858. [50] Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sang-goo Lee, and Taeuk Kim. Aligning language models to explicitly handle ambiguity. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 19892007, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.119. URL https: //aclanthology.org/2024.emnlp-main.119/. [51] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. [52] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020. [53] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback, 2024. URL https://arxiv.org/abs/2309.00267. [54] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation, 2023. URL https: //arxiv.org/abs/2206.04624. 13 [55] Stephen C. Levinson. in LinURL https://www.cambridge.org/highereducation/books/pragmatics/ pages 181184. Cambridge Textbooks Pragmatics. guistics, 1983. 6D0011901AE9E92CBC1F5F21D7C598C3#contents. [56] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https://arxiv.org/ abs/2005.11401. [57] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning, 2024. URL https://arxiv.org/abs/2404.02060. [58] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models, 2024. URL https://arxiv.org/abs/2310.10505. [59] Myrna Libby, Julie Weiss, Stacie Bancroft, and William Ahearn. comparison of most-to-least and least-to-most prompting on the acquisition of solitary play skills, 2008. URL https://pubmed. ncbi.nlm.nih.gov/22477678/. [60] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. [61] Chin-Yew Lin and Franz Josef Och. ORANGE: method for evaluating automatic evaluation metrics In COLING 2004: Proceedings of the 20th International Conference on for machine translation. Computational Linguistics, pages 501507, Geneva, Switzerland, aug 23aug 27 2004. COLING. URL https://www.aclweb.org/anthology/C04-1072. [62] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022. acl-long.229. [63] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah Smith, and Yejin Choi. Were afraid language models arent modeling ambiguity. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 790807, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.51. URL https://aclanthology.org/2023. emnlp-main.51/. [64] Jie Liu and Barzan Mozafari. Query rewriting via large language models, 2024. URL https://arxiv. org/abs/2403.09060. [65] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. URL https://arxiv.org/ abs/2307.03172. [66] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023. URL https://arxiv.org/abs/2303. 16634. [67] Fangrui Lv, Kaixiong Gong, Jian Liang, Xinyu Pang, and Changshui Zhang. Subjective topic meets LLMs: Unleashing comprehensive, reflective and creative thinking through the negation of negation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1231812341, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.686. URL https://aclanthology.org/2024.emnlp-main.686/. [68] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrievalaugmented large language models, 2023. URL https://arxiv.org/abs/2305.14283. [69] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. URL https://arxiv.org/abs/2303.17651. 14 [70] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. RaFe: Ranking feedback improves query rewriting for RAG. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 884901, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.49. URL https://aclanthology. org/2024.findings-emnlp.49/. [71] H. Brendan McMahan. survey of algorithms and analysis for adaptive online learning, 2015. URL https://arxiv.org/abs/1403.3465. [72] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? In Conference on Empirical Methods in Natural new dataset for open book question answering. Language Processing, 2018. [73] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models, 2024. URL https://arxiv.org/abs/2310.17022. [74] Gergely Neu and Julia Olkhovskaya. Efficient and robust algorithms for adversarial linear contextual bandits. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 30493068. PMLR, 0912 Jul 2020. URL https://proceedings.mlr.press/v125/neu20b.html. [75] Rodrigo Nogueira and Kyunghyun Cho. Task-oriented query reformulation with reinforcement learning. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 574583, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1061. URL https://aclanthology.org/D17-1061/. [76] OpenAI. Openai o3 and o4-mini system card, 2025. URL https://openai.com/index/ o3-o4-mini-system-card/. [77] Ian Osband, Daniel Russo, and Benjamin thompson. (more) efficient reinforcement learning via posterior sampling. Advances in Neural Information Processing Systems, 26, 2013. [78] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. [79] Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: method for automatic evaluation of machine translation. pages 311318, 2002. [80] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations, 2018. URL https://arxiv.org/abs/1802. 05365. [81] Valentina Pyatkin, Bonnie Webber, Ido Dagan, and Reut Tsarfaty. Superlatives in context: Modeling the implicit semantics of superlatives, 2024. URL https://arxiv.org/abs/2405.20967. [82] Yang Qiao, Liqiang Jing, Xuemeng Song, Xiaolin Chen, Lei Zhu, and Liqiang Nie. Mutual-enhanced incongruity learning network for multi-modal sarcasm detection. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI23/IAAI23/EAAI23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i8. 26138. URL https://doi.org/10.1609/aaai.v37i8.26138. [83] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [84] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text, 2016. [85] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad, 2018. 15 [86] Matthew Riemer, Sharath Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj, Maximilian Puelma Touzel, and Irina Rish. Continual learning in environments with polynomial mixing times. Advances in Neural Information Processing Systems, 35:2196121973, 2022. [87] Elijah Rippeth, Marine Carpuat, Kevin Duh, and Matt Post. Improving word sense disambiguation in neural machine translation with salient document context, 2023. URL https://arxiv.org/abs/2311. 15507. [88] Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877898, 1976. [89] Jerrold M. Sadock and Arnold M. Zwicky. Speech acts distinctions in syntax. In Timothy Shopen, editor, Language Typology and Syntactic Description, pages 155196. Cambridge University Press, Cambridge, 1985. [90] Rob A. Van Der Sandt. Presupposition projection as anaphora resolution. page 333377. Journal of Semantics, 1992. URL https://academic.oup.com/jos/article/9/4/333/1648227. [91] Timo Schick and Hinrich Schütze. Rare words: major problem for contextualized embeddings and how to fix it by attentive mimicking, 2019. URL https://arxiv.org/abs/1904.06707. [92] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [93] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [94] Ethel Schuster. Anaphoric reference to events and actions: representation and its advantages. In Coling Budapest 1988 Volume 2: International Conference on Computational Linguistics, 1988. URL https://aclanthology.org/C88-2126/. [95] Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Rasha Obeidat, and Prasad Tadepalli. Entity-aware elmo: Learning contextual entity representation for entity disambiguation, 2019. URL https://arxiv.org/ abs/1908.05762. [96] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107194, 2012. [97] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. [98] Settaluri Sravanthi, Meet Doshi, Pavan Tankala, Rudra Murthy, Raj Dabre, and Pushpak Bhattacharyya. PUB: pragmatics understanding benchmark for assessing LLMs pragmatics capaIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Associabilities. tion for Computational Linguistics: ACL 2024, pages 1207512097, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.719. URL https://aclanthology.org/2024.findings-acl.719/. [99] Arun Sai Suggala and Praneeth Netrapalli. Follow the perturbed leader: Optimism and fast parallel algorithms for smooth minimax games, 2020. URL https://arxiv.org/abs/2006.07541. [100] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, 2018. [101] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples, 1933. [102] New York Times. York Times, May 2025. ai-hallucinations-chatgpt-google.html. Accessed: 2025-05-12. Ai hallucinations: The New URL https://www.nytimes.com/2025/05/05/technology/ Chatgpt and googles challenges. [103] S. Towhidul Islam Tonmoy, Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. comprehensive survey of hallucination mitigation techniques in large language models, 2024. URL https://arxiv.org/abs/2401.01313. 16 [104] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. [105] Thinh Hung Truong, Timothy Baldwin, Karin Verspoor, and Trevor Cohn. Language models are not naysayers: An analysis of language models on negation benchmarks, 2023. URL https://arxiv.org/ abs/2306.08189. [106] Hoang Van, Zheng Tang, and Mihai Surdeanu. How may help you? using neural text simplification to improve downstream nlp tasks, 2021. URL https://arxiv.org/abs/2109.04604. [107] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706. 03762. [108] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537, 2019. [109] Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is ChatGPT good NLG evaluator? preliminary study. In Yue Dong, Wen Xiao, Lu Wang, Fei Liu, and Giuseppe Carenini, editors, Proceedings of the 4th New Frontiers in Summarization Workshop, pages 111, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.newsum-1.1. URL https://aclanthology.org/2023.newsum-1.1/. [110] Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. Gpt-ner: Named entity recognition via large language models, 2023. URL https://arxiv.org/ abs/2304.10428. [111] William Watson, Nicole Cho, Tucker Balch, and Manuela Veloso. HiddenTables and PyQTax: cooperative game and dataset for TableQA to ensure scale and data privacy across myriad of taxonomies. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 71447159, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.emnlp-main.442. URL https://aclanthology.org/2023.emnlp-main.442. [112] William Watson, Nicole Cho, and Nishan Srishankar. Is there no such thing as bad question? h4r: Hallucibot for ratiocination, rewriting, ranking, and routing. Proceedings of the AAAI Conference on Artificial Intelligence, 39(24):2547025478, Apr. 2025. doi: 10.1609/aaai.v39i24.34736. URL https://ojs.aaai.org/index.php/AAAI/article/view/34736. [113] William Watson, Nicole Cho, Nishan Srishankar, Zhen Zeng, Lucas Cecchi, Daniel Scott, Suchetha Siddagangappa, Rachneet Kaur, Tucker Balch, and Manuela Veloso. LAW: Legal agentic workflows for custody and fund services contracts. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, Steven Schockaert, Kareem Darwish, and Apoorv Agarwal, editors, Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 583594, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https: //aclanthology.org/2025.coling-industry.50/. [114] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [115] Sam Witteveen and Martin Andrews. Paraphrasing with large language models. In Proceedings of the 3rd Workshop on Neural Generation and Translation. Association for Computational Linguistics, 2019. doi: 10.18653/v1/d19-5623. URL http://dx.doi.org/10.18653/v1/D19-5623. [116] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 20132018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1237. URL https://aclanthology.org/D15-1237. 17 [117] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. [118] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Yu-Yang Liu, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples, 2024. URL https://arxiv.org/ abs/2310.01469. [119] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. [120] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators, 2023. URL https://arxiv.org/abs/2209.10063. [121] Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation, 2021. URL https://arxiv.org/abs/2106.11520. [122] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL https://arxiv.org/abs/2504.05118. [123] Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Balch, and Manuela Veloso. Flowmind: Automatic workflow generation with llms, 2024. URL https://arxiv.org/abs/ 2404.13050. [124] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023. URL https://arxiv.org/abs/2205.10625. [125] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning, 2025. URL https://arxiv.org/abs/ 2504.16084. Appendix / supplemental material A.1 Limitations Current limitations in our work are as follows: our current contextual bandit framework treats each of the 17 features as independent, but does not capture higher-order interactions. This can provide an exciting avenue of future research in terms of measuring whether the combination of features jointly exacerbates hallucination. Likewise, we would like to highlight that the feature-arm regression weights do not stipulate causal relationship - highly sophisticated causal relationships are difficult to formulate within LLMs due to the inherent difficulties of interpreting neural networks internal layers; thus, in this paper, we focus on providing empirical studies and the conclusions we can draw from them. Finally, even with our rigorous studies to find the ROC-AUC Pareto-frontier, our reward model leverages LLM-as-judge, which may reflect the LLMs bias. Overall, these limitations posit potential directions by which the research community can further pursue - and ultimately help expand our understanding of these powerful, albeit hallucinatory models. A.2 Summary of Bandits Non-Contextual Adversarial EXP3 [7] Maintains weights wk, samples at wk, updates wat wat exp(cid:0) γ rt pat (cid:1). 18 (a) Arm Diversity for Contextual Bandits, as Fraction of Trials. (b) Arm Diversity for Non-Contextual Bandits, as Fraction of Trials. Figure 6: For Non-Contextual bandits, almost every dataset is dominated by single arm with the highest global reward (typically 40%-60% of the trials). The remaining 40-60% is split among the other four arms as noise, the non-contextual policy has no way to \"know\" when within dataset different arm might do better. In contrast, Contextual bandits show more even mix: the top arm is only 25-30%, with two or three other arms contributing sizable shares (15-25% each). The contextual policy reads the features and diversifies its choices within each dataset. Algorithm 1 General Bandit + Rewrite Loop Require: arms A, context xt, algorithm algo {EXP3, FTPL, LinUCB, KL, FTRL, Thompson}, observe xt for each arm do sk Score(algo, k, xt) hyperparameters 1: for = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: end for end for select at = arg maxkA sk apply rewrite at to query and observe reward rt Update(algo, at, xt, rt) FTPL [45, 99] Adds Gumbel noise ξk Gumbel(0, 1/η) [33] to cumulative rewards, selects at = arg max(cum_rewardk + ξk), then increments the chosen arms reward. Contextual Stochastic LinUCB [51] Selects at = arg max (cid:0)x ˆθk + α (cid:113) A1 xt (cid:1), updates Ak Ak + xtx , bk bk + rtxt. bound. KL-UCB (LinUCB-KL) [32] Replaces the UCB term with KL-divergence-based confidence Thompson Sampling Maintains Gaussian posterior (µk, Σk); samples θk, picks at = arg max θk, updates the posterior. Contextual Adversarial FTRL [71] Selects arm maximizing ϵ-greedy FTRL ... LinearEXP3 [74] Contextual extension of EXP3, sampling arms based on exponentiated linear wk λwk1, with an ℓ1 regularizer. scores. LinearFTPL [35] Contextual adaptation of FTPL, applying Gumbel perturbations to linear reward estimates. A.3 LinUCB The estimated parameter is: ˆθa = A1 ba. 19 (2) (a) Soft Rank Heatmap for all Bandits, REWRITE. including arm NO (b) Arm Diversity when including NO REWRITE. Figure 7: Impact of the No-Rewrite Arm. Note that these experiments are conducted on the original query \"as-is\" in the benchmark dataset, with no perturbations. Upon enabling the NO REWRITE option, our contextual bandit rapidly converges to this arm, which then achieves the highest reward on several datasets. We attribute this behavior to the LLMs tendency to memorize benchmark questions. Given query feature vector x, the upper confidence bound (UCB) for arm is: UCBa(x) = ˆθa + α (cid:112) xA1 x, where α controls the explorationexploitation trade-off. The arm selected is: = arg max aA UCBa(x). Upon observing reward r, update: Aa Aa + xx, ba ba + x. (3) (4) (5) A.4 LinUCBKL Bandit Strategy: The algorithm is initialized with parameters: number of arms narms, dimension d, regularization parameter λ, exploration parameter α, noise variance σnoise, and KL-bound constant c. Each arm maintains matrix Aa and vector ba, initialized as λId and 0d, respectively. The select_arm method computes the score for each arm using the following formulation: ba θa = A1 µa = xθa vara = xA1 na = max(1, counts[a]) raw_bounda = log(t) + log(log(t + 1)) na bounda = max(raw_bounda, 0.0) (cid:112) bonusa = scorea = µa + bonusa 2 vara bounda where is the context vector, is the time step, and counts[a] is the number of times arm has been selected. The arm with the highest score is selected for exploration. The update method updates the matrix Aa and vector ba for the selected arm based on the received reward rt: Aa Aa + xx ba ba + rtx counts[a] counts[a] + 1 This strategy leverages the KL-bound to dynamically adjust exploration bonuses, enhancing the LinUCB algorithms ability to balance exploration and exploitation in contextual setting. 20 (a) Contextual Model Feature Variance. (b) Non-Contextual Model Feature Variance. Figure 8: Comparison of Feature Variance between (a) our contextual bandits and (b) its noncontextual counterparts. Polysemy, Constraints and Entities show the most variation. Presupposition, Excessive Details, and Grounding have the least. (a) Contextual Model KL Distance. (b) Non-Contextual Model KL Distance. Figure 9: Comparison of Inter-Arm Context Distances (Symmetric KL) between (a) our contextual bandits and (b) its non-contextual counterparts. Arm pairs such as EXPAND and PARAPHRASE in the non-contextual bandit setting exhibit high KL distances at 1.01. One interpretation is that the context-clouds barely overlap from dataset to dataset (Figure 6b). A.5 FTRL The algorithm is initialized with the following parameters: number of arms narms, dimension d, learning rate α, exploration parameter β, and regularization parameters l1 and l2. The cumulative gradient vectors for each arm are stored in za, initialized as zero vectors of dimension d. The weight vector wa for each arm is computed as: (cid:40) zisign(zi)l1 ni α +l2 β+ wi = 0 if zi > otherwise where zi is the cumulative gradient for the i-th feature of arm a, and ni is the cumulative squared gradient for the i-th feature. The arm with the highest score, calculated as the dot product of the weight vector and the context vector, is selected: at = arg max a{1,...,narms} (cid:33) wi xi (cid:32) (cid:88) i=1 21 (a) Contextual Model Raw Feature Strength. (b) Non-Contextual Model Raw Feature Strength. Figure 10: Comparison of Raw feature-level regression coefficients between (a) our contextual bandits and (b) its non-contextual counterparts. Each cell shows how enables raw view into how specific linguistic feature changes the expected reward under each rewrite strategy. Upon receiving reward rt for the selected arm at, the algorithm updates the cumulative gradient vector and the squared gradient sum for the selected arm: εerror = w, rt = εerror (cid:112)ni + g2 α σ = ni zi zi + gi σ wi ni ni + g2 This formulation allows the FTRL algorithm to adaptively adjust the exploration-exploitation trade-off by incorporating both the cumulative reward and the uncertainty in the form of regularization terms, which are scaled by the learning rate α and exploration parameter β. A.6 Linear EXP3 The algorithm is initialized with parameters: number of arms narms, dimension d, exploration parameter γ, and learning rate η. Each arm maintains parameter vector θa, initialized as 0d. We compute the probability distribution over arms using the following formulation: logitsa = θ logits = logits max(logits) exp_logitsa = exp(logitsa) base_probsa = exp_logitsa a=1 exp_logitsa (cid:80)narms probsa = (1 γ) base_probsa + γ narms where is the context vector. The arm is selected based on the probability distribution probs. 22 The update method updates the parameter vector θa for the selected arm using the estimated reward ˆrt: ˆrt = rt pa θa θa + η ˆrt where pa is the probability of selecting arm a, and rt is the received reward. This strategy leverages exponential weighting and exploration bonuses to balance exploration and exploitation in linear contextual setting. A.7 Linear FTPL The algorithm is initialized with parameters: number of arms narms, dimension d, and learning rate η. Each arm maintains parameter vector θa, initialized as 0d. The select_arm method computes the perturbed scores for each arm using the following formulation: linear_scorea = θ noisea Gumbel(0, 1 η ) scorea = linear_scorea + noisea where is the context vector. The arm with the highest perturbed score is selected: at = arg max a{1,...,narms} scorea θa θa + rt This strategy leverages random perturbations from Gumbel distribution to balance exploration and exploitation, allowing the algorithm to explore suboptimal arms while exploiting the accumulated knowledge of their performance in linear contextual setting. A.8 Thompson Sampling For given x, sample θa (µa, Σa) and select the arm maximizing: = arg max aA θa. (6) Standard Bayesian linear regression updates are then used to update µa and Σa based on the observed reward r. Σ1 Σ1 + (cid:16) µa Σa Σ1 1 σ2 xx, 1 µa + σ2 (cid:17) . (7) 23 (a) Contextual Model Relative Feature Strength. (b) Non-Contextual Model Relative Feature Strength. Figure 11: Comparison of Min-Max Normalized feature-level regression coefficients between (a) our contextual bandits and (b) its non-contextual counterparts. Each cell shows how enables relative view into how specific linguistic feature changes the expected reward under each rewrite strategy. Table 4 highlights contextual bandit trends. Table 4: Top Drivers (f + max) of Rewrite Strategies per Linguistic Features For each rewrite arm, we list the feature whose normalized coefficient was highest (100 %) and lowest (0 %), alongside brief rationale for its positive or negative impact on downstream reward. Arm max) and Reducers (f Interpretation Interpretation max + max DISAMBIGUATE Subordination (100 %) SIMPLIFY Pragmatics (100 %) Long or nested clauses benefit from targeted disambiguation, which isolates and clarifies the core semantic relation. Pragmatic cues (e.g. discourse markers, politeness) guide safe simplification without loss of meaning. Polysemy (0 %) Superlative (0 %) EXPAND Constraints (100 %) Queries already rich in constraints Ambiguity (0 %) PARAPHRASE Answerability (100 %) CLARIFY TERMS Rarity (100 %) (time, location, numeric bounds) gain precision when expanded with further qualifiers. Paraphrasing queries that are already answerable refreshes wording while preserving solvability, boosting LLM performance. Defining rare or domain-specific terms anchors the LLMs understanding of technical queries. Presupposition (0 %) Subordination (0 %) Highly polysemous terms lead disambiguation to pick the wrong sense, degrading downstream reward. Stripping superlative constructions removes essential comparative context, hurting reward. Underspecified queries offer no detail to expand, so further addition of terms only introduces noise. Altering queries with strong presuppositions can break implied assumptions, reducing effective reward. Clarifications in convoluted sentences can introduce further parsing difficulty, impeding reward. 24 (a) Contextual Model Feature Uplift. (b) Non-Contextual Model Feature Uplift. Figure 12: Reward Uplift by Contextual Feature and Strategy. Feature Uplift measures how much the presence of binary feature changes the expected reward for given rewrite arm, formally (fi, a) = E[rt arm = a, fi = 1] E[rt arm = a, fi = 0]. (a) Under the contextual bandit, the strongest positive uplifts come from Answerability ( +17 uniformly) and Grounding (+1518), while Ambiguity ( 15 to 18) and Subjectivity ( 10 to 14) impose the largest hits across all arms. Mid-range features like Presupposition and Constraints deliver modest boosts ( 5), and Excessive Details and Anaphora slightly hurt performance ( 5 to 7). (b) The non-contextual bandit amplifies these trends: Answerability and Grounding remain the top drivers ( +1820), but Ambiguity becomes even more detrimental ( 17 to 18), and Mismatch drops to nearly 16 under some arms. Notably, the non-contextual model shows stronger negative effect for Excessive Details (up to 12) and Entities ( 6) than the linear one, suggesting it more sharply penalizes noisy contexts. Together, these heatmaps reveal which linguistic signals each rewrite strategy leverages (or struggles with), and how context vs. context-blind policies weigh them differently. 25 Figure 13: Pairwise Normalized Coefficient Differences for Contextual Bandits. Each cell shows the minmaxnormalized difference in regression weight for given linguistic feature (rows) between two rewrite arms (columns), e.g. Paraphrase vs Disambiguate, Simplify vs Expand, etc. Cells labeled Win (blue) indicate the feature favors the first arm in the matchup, while Loss (red) indicates it favors the second. Values are expressed as percentage of the features full coefficient range. 26 u r e - a S i L t y Feature Definition Example Anaphora Presence of pronouns or references requiring external context. \"What about that one?\" (Unclear reference) Subordination Measures the presence of multiple subordinate clauses \"While was walking home, saw cat that looked just like my friends.\" Mismatch Mismatch between the querys intended output and its actual structure. Presupposition Unstated assumptions embedded in the query. \"Find me this paragraph in this document\" (When document isnt given, this query cannot be answered) \"Who is the musician that developed neural networks?\" (Assumes such musician exists) Pragmatics Captures context-dependent meanings beyond literal interpretation. \"Can you pass the salt?\" (A request, not literal ability) Rarity Use of rare or niche terminology. \"What are the ramifications of quantum decoherence?\" (Uses lowfrequency terms) Negation Presence of negation words (not, never). \"Is it not possible to do this?\" Superlatives Polysemy Detection of superlative expressions (biggest, fastest). Presence of ambiguous words with multiple related meanings. \"What is the fastest algorithm?\" \"Explain how bank operates.\" (Ambiguity: financial institution vs. riverbank) Answerability Assesses whether the query has verifiable answer. \"What is the exact number of galaxies?\" (Unanswerable) Excessive Subjectivity Ambiguity Grounding n S Constraints Entities Specialization Evaluates whether query is overloaded with information, potentially distracting the model. \"Can you explain how convolutional neural networks work, including all mathematical formulas?\" Query requires the degree of opinion or personal bias \"What is the best programming language?\" Highly ambiguous context, task, and wording \"Tell me about history.\" (Too broad) Evaluates how clearly the querys purpose is expressed. \"How does reinforcement learning optimize control in robotics?\" (Clear intent) Identifies explicit constraints (time, location, conditions) provided in the query. \"What was the inflation rate in the US in 2023?\" Checks for the inclusion of verifiable named entities. Determines whether the query belongs to specialized domain (e.g., finance, law). \"Who founded OpenAI?\" \"What are the legal implications of the GDPR ruling?\" Table 5: Detailed Summary and Examples of Feature Categories, Definitions, and Examples (See Table 1)"
        }
    ],
    "affiliations": [
        "JP Morgan AI Research, New York, NY"
    ]
}