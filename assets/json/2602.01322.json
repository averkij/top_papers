{
    "paper_title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
    "authors": [
        "Panagiotis Koromilas",
        "Andreas D. Demou",
        "James Oldfield",
        "Yannis Panagakis",
        "Mihalis Nicolaou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics."
        },
        {
            "title": "Start",
            "content": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Panagiotis Koromilas 1 2 Andreas D. Demou 1 James Oldfield 3 Yannis Panagakis 2 4 Mihalis A. Nicolaou"
        },
        {
            "title": "Abstract",
            "content": "(a) Additive Interactions (b) Multiplicative Interactions Brand Brand 6 2 0 2 ] . [ 1 2 2 3 1 0 . 2 0 6 2 : r Sparse autoencoders (SAEs) have emerged as promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether Starbucks arises from the composition of star and coffee features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 210 larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics. 1The Cyprus Institute 2University of Athens 3University of Oxford 4Archimedes AI/Athena Research Center 5University of Cyprus. Correspondence to: Panagiotis Koromilas <pakoromilas@di.uoa.gr>. Preprint. February 3, 2026. 1 Starbucks Coffee (zi) Star (zj ) Famous Star Coffee Constrained to the linear span of features Beverage Famous zi zj Beverage Interaction lifts into new semantic dimensions Figure 1. Semantic Dimension Expansion via Feature Interaction. Consider two semantic directionsFamous and Beverageand their associated learned features Star and Coffee. (a) Additive interactions yield co-occurrence semantics that remain in the original feature span. (b) Multiplicative interactions enable representations to escape this subspace via zi zj, lifting into orthogonal dimensions (Brand) to capture emergent concepts like Starbucks.Starbucks example from Table 3. 1. Introduction As AI systems are increasingly deployed in real-world domains, ensuring their safety and reliability has become critical challenge (Amodei et al., 2016; Hendrycks et al., 2021; Bengio et al., 2025). Developing interpretable models offers promising path towards aligning AI with human values: understanding why model produces given output enables us to (i) monitor its reasoning (Lindsey et al., 2025), (ii) debug failure modes (Wong et al., 2021), and (iii) steer away from unwanted behavior (Rimsky et al., 2024). Mechanistic interpretability pursues this agenda at the level of neural network internals (Bereska & Gavves, 2024), aiming to uncover interpretable features and circuits within model and thereby provide principled insights into its behavior. Sparse Autoencoders (SAEs), grounded in the principles of sparse dictionary learning, have emerged as leading tool for mechanistic interpretability. SAEs decompose neural network activations to recover human-interpretable features that models typically represent in superpositionencoded in overlapping directions due to limited representational capacity (Elhage et al., 2022). This framework has been shown to uncover safety-relevant concepts such as deception, bias, and harmful content, enabling targeted interventions that predictably steer model behavior (Templeton et al., 2024). PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Figure 2. An overview of PolySAE: (1) sparse latent features are first extracted with standard SAE encoder. (2) Activations in the residual stream are then reconstructed by modeling 2ndand 3rd-order interactions in addition to the standard linear component. The example Investing.com Philippines stocks were higher after comes from Table 4. However, recent work has highlighted fundamental limitations of the SAE paradigm due to their reliance on the strong linear representation hypothesis (Engels et al., 2025; Csordás et al., 2024). Standard SAEs reconstruct activations as weighted sums of independent features, expressing each activation as linear combination where features contribute additively. This linearity assumption raises fundamental question: what level of abstraction do learned features naturally capture? The answer has direct implications for mechanistic interpretability. If features truly combine linearly, we would expect individual dictionary atoms to represent atomic components, such as morphemes, simple concepts, or basic semantic primitives, that combine through superposition to form complex expressions. Such atomic features would enable transparent circuit analysis and precise interventions on elemental building blocks of meaning. Yet linguistic theory demonstrates that composition operates non-linearly across multiple levels of language structure. Morphologically, administrators is not simply the sum of stem and suffix; the combination produces distinct lexical item with specific syntactic and semantic properties (Haspelmath & Sims, 2013). Semantically, phrasal meanings such as kick the bucket or proper names like Starbucks (Figure 1) exhibit emergent properties irreducible to their parts (Partee, 1995). Vanilla SAEs demonstrably succeed at many interpretability tasks, yet their linear reconstruction mechanism cannot, in principle, represent nonlinear composition. Without explicit interaction mechanisms, SAEs cannot simultaneously represent atomic features and their non-linear compositions. When Starbucks appears in context, linear model must either (i) allocate dedicated feature for this compositional entity, sacrificing atomicity, or (ii) represent it through separate star and coffee features that cannot distinguish this specific composition from mere co-occurrence. Ideally, SAEs with sufficient capacity can learn features at multiple levels of abstraction simultaneously (such as morphemes, words, phrases, and compositional expressions) coexisting as independent atoms in an overcomplete dictionary. While this leads to good reconstruction and intervention, it fundamentally limits our understanding: we cannot decompose Starbucks into its constituents, cannot trace how administrators emerge from stem and suffix binding, and cannot distinguish compositional phrases from accidental co-occurrence. The conflation of atomic and compositional features obscures the mechanisms by which networks build complex representations from simpler parts. This problem connects to longstanding debate in cognitive science about systematic compositionality in neural representations (Fodor & Pylyshyn, 1988). Smolensky (1990) proposed tensor product variable binding as solution: features bind through multilinear interactions rather than linear superposition, allowing networks to maintain atomic constituents while representing their combinations. In this framework, administrators would be represented not as single indivisible feature, but as an explicit composition of stem and suffix, where the tensor product captures the binding operation. For interpretability of modern LLMs, this principle is critical: to understand how networks compose meaning, our tools must themselves model compositional structure faithfully. However, explicit tensor products are computationally prohibitive for overcomplete sparse codes with tens of thousands of features, requiring methods that capture multilinear interactions while remaining tractable. In this work, we introduce the Polynomial Sparse Autoencoder (PolySAE) (Figure 2), sparse autoencoder that extends vanilla SAEs with explicit feature interaction terms. PolySAE preserves linear encoder for interpretability while extending the decoder with quadratic and cubic terms that model pairwise and triple feature interactions. Through low-rank tensor factorization on shared projection subspace, PolySAE captures compositional structure in 2 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding tractable manner, adding small parameter overhead (3% for GPT2 small). Critically, PolySAE is strict generalization of standard SAEs that enables capturing multiplicative (non-additive) concept interactions. Setting interaction coefficients to zero recovers vanilla SAE behavior, allowing PolySAE to be readily applied to existing SAE variants, including TopK (Gao et al., 2025), BatchTopK (Bussmann et al., 2024), and Matryoshka (Bussmann et al., 2025b). We summarize below our four main contributions: C1. We introduce PolySAE, sparse autoencoder with polynomial decoder that explicitly models quadratic and cubic feature interactions while preserving linear encoder for interpretability. Through low-rank tensor factorization, PolySAE adds small parameter overhead (3% for GPT2 small) and can be readily applied to existing SAE variants (TopK, BatchTopK, Matryoshka). C2. Across four language models of different scales (GPT2 Small, Pythia-410M/1.4B, Gemma-2-2B) and three sparsification strategies, PolySAE achieves an average 8% F1 improvement, while maintaining comparable reconstruction error. C3. PolySAE produces 210 larger Wasserstein distances between class-conditional feature distributions, indicating more separated semantic structure in the learned representations. C4. We show that learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. = 0.82 for SAE feature covariance), and provide qualitative examples demonstrating that polynomial terms capture compositional structure such as morphological binding, phrasal composition, and contextual disambiguation. 2. Related Work Sparse dictionary learning. In sparse dictionary learning, signals are represented as sparse linear combinations of overcomplete basis elements (Mallat & Zhang, 1993), an approach also integrated into neural network architectures (Hinton & Salakhutdinov, 2006; Lee et al., 2007; Konda et al., 2014). Sparse Autoencoders (SAEs) recently emerged as leading paradigm for feature discovery in large language models (Huben et al., 2024; Bricken et al., 2023), scaling to millions of features (Gao et al., 2025). Subsequent work has produced architectural variants including BatchTopK (Bussmann et al., 2024), Matryoshka (Bussmann et al., 2025a), Gated (Rajamanoharan et al., 2024a), and JumpReLU (Rajamanoharan et al., 2024b) SAEs, with standardized benchmarks enabling systematic comparison (Karvonen et al., 2025). However, all these methods assume features combine additively through linear reconstruction. Modeling feature interactions. Multiplicative interactions between features have rich history in deep learning (Jayakumar et al., 2020), from early bilinear models for visual data (Tenenbaum & Freeman, 1996; Freeman & Tenenbaum, 1997) to modern gating mechanisms (Shazeer, 2020). Feature interactions through the Hadamard product (Chrysos et al., 2025) serve as powerful conditioning mechanism (Perez et al., 2018; Dumoulin et al., 2017), while multiplicative structure also enables parameter-efficient mixture-of-experts (Oldfield et al., 2025a). Recent work has explored multiplicative interactions for interpretability: Bilinear MLPs (Pearce et al., 2025) model pairwise feature interactions enabling weight-based interpretability, while Gauderis & Dooms (2025) propose fully interpretable architectures based on tensor networks. We extend this line of work by modeling feature interactions in the SAE setting. Polynomials. One natural way to model higher-order interactions is through polynomials (Shin & Ghosh, 1991). In deep learning, polynomials have been used for variety of applications, such as image generation (Chrysos et al., 2020; 2021), classification (Babiloni et al., 2021; Chrysos et al., 2022a), privacy preservation (Zhang et al., 2019), interpretability (Dubey et al., 2022), and dynamic safety guardrails (Oldfield et al., 2025b). The work most closely related to ours is the Bilinear Autoencoder (BAE) (Dooms & Gauderis, 2025), which similarly introduces interaction terms for interpretability. The key difference lies in the level at which interactions are modeled: BAE captures pairwise interactions between input neurons, whereas PolySAE models interactions directly between learned sparse features, including higher-order terms. As result, PolySAE preserves the interpretability of linear SAE latents while explicitly allocating capacity to non-additive feature composition. 3. Sparse Polynomial Decoding 3.1. Preliminaries Notation. Bold lowercase letters denote vectors and bold uppercase letters denote matrices. The i-th column of is mi, and M:,1:r denotes its first columns. We use for the Hadamard product, for the Kronecker product, and for the KhatriRao product. Rd and Rdsae denote the activation and sparse-code spaces, with dsae d. S() denotes sparsification operator, such as Top-K or BatchTop-K. Sparse Autoencoders. Sparse autoencoders (SAEs) build on overcomplete dictionary learning (Olshausen & Field, 1997) to decompose neural activations into sparse set of latent features. Given activations Rd from an intermediate layer of pretrained network, an SAE learns sparse code Rdsae with dsae and reconstructs via ˆx = bdec + Dz, = S(cid:0)ReLU(Ex + benc)(cid:1) where is linear encoder, is the decoder (dictionary), and enforces sparsity. The overcomplete latent space allows PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding multiple features to align with similar activation directions, supporting disentangled and interpretable representations. Motivated by the superposition hypothesis (Elhage et al., 2022), SAEs assume that features combine additively in the decoder, so reconstruction is linear in z. This corresponds to strong form of the linear representation hypothesis applied to decoding, which has recently been questioned (Engels et al., 2025). When multiple features co-activate, their joint effect may not be well captured by linear sumfor example, coffee feature and star feature may require reconstruction direction distinct from either individual atom to capture the Starbucks concept. This motivates extending the decoder to explicitly model feature interactions, while preserving linear and interpretable encoder. 3.2. Design Principles for Feature Interactions We extend sparse autoencoders to capture higher-order feature interactions by establishing design principles grounded in prior work. Each architectural choice in PolySAE follows directly from these principles. P1. Linear Encoding (Interpretability). Each sparse code coefficient zi is derived by linear projection of the input activation x. The linear representation hypothesis in mechanistic interpretability posits that learned features should correspond to directions in activation space (Elhage et al., 2022; Bricken et al., 2023), view supported by the success of linear probes for extracting semantic content (Belinkov, 2022; Alain & Bengio, 2016). P2. Polynomial Reconstruction (Expressivity). The decoder may capture compositional structure by using polynomial terms in z. Modeling compositional structure, i.e. how features interact, polynomials have strong precedent in the literature: Volterra series (Volterra, 1959) represent nonlinear systems as sums of multilinear kernels, second-order pooling (Carreira et al., 2012; Gao et al., 2016) captures feature co-occurrences via outer products, and polynomial networks (Chrysos et al., 2022b) parameterize functions as products of linear projections. P3. Factorized Interaction Structure (Coherence & Efficiency). Higher-order terms should operate in lowdimensional subspace aligned with the linear feature space. Using shared projection ensures that interactions are compositions of the same underlying features. This alignment principle underlies factorized interaction models (Rendle, 2010; Blondel et al., 2016) and compact bilinear pooling (Gao et al., 2016; Kim et al., 2017). Constraining interactions to low-rank subspaces imposes strong inductive bias, favoring coherent, reusable interaction modes over arbitrary pairwise composition. P4. Structural Constraints (Parsimony & Identifiability). Lower-order terms should have higher representational capacity than higher-order terms, following polynomial approximation theory (Mason & Handscomb, 2002). The latent interaction subspace should have orthonormal columns to ensure geometrically distinct directions. Orthogonality constraints are standard in dictionary learning and independent component analysis to prevent degenerate solutions (Arora et al., 2015; Bao et al., 2016; Hyvärinen & Oja, 2000). Orthonormality removes rotational ambiguity and ensures the model does not allocate redundant capacity to correlated interaction directions. 3.3. PolySAE: Polynomial Sparse Autoencoder To satisfy P1 PolySAE adopts the standard SAE encoder (Huben et al., 2024; Bricken et al., 2023) to first performs linear map followed by sparsification: = S(cid:0)ReLU(Ex + benc)(cid:1), Rdsae , (1) where feature activates when aligns with direction ei, enabling visualization, clustering, and causal intervention via activation patching (Meng et al., 2022). Following P2, we extend the decoder to include quadratic and cubic terms: ˆx = bdec + y1 + λ2 y2 + λ3 y3, (2) where y1 = z, y2 = (z z), y3 = Γ (z z), and λ2, λ3 are learnable scalar coefficients that control the contribution of each polynomial order. Setting λ2 = λ3 = 0 recovers standard linear sparse autoencoder, making PolySAE strict generalization of existing SAE architectures. This can be viewed as third-order Volterra expansion (Volterra, 1959) or Π-net polynomial parameterization (Chrysos et al., 2022b), adapted to sparse codes. However, explicitly modeling all pairwise or higher-order feature combinations would require O(d2 sae) parameters, leading to unstructured interaction effects and high risk of overfitting. Following P3 and P4, we constrain interactions to low-rank subspace: sae) or O(d3 y1 = (z ) (1), y2 = (cid:0)(z U:,1:R2) (z U:,1:R2 )(cid:1) C(2), y3 = (cid:0)(z U:,1:R3) (z U:,1:R3 ) (z U:,1:R3)(cid:1) C(3), (3) where denotes element-wise product and C(k) RdRk are output projection matrices. This parameterization restricts the interaction dictionaries to rank at most Rk, enforcing strong inductive bias on how features may combine. Notice that this parameterization satisfies P3 by applying single projection to the sparse code and forming interactions via polynomial operations: zU , (zU ) (zU ), and 4 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding (zU ) (zU ) (zU ). Using the same projected representation at every order ensures that interaction effects remain aligned with the linear feature basis and interpretable as compositions of the same underlying features. Furthermore, PolySAE satisfies the parsimony aspect of P4 by following nested low-rank approximation (Grasedyck et al., 2013) and utilizing ranks (R1, R2, R3) with R1 R2 R3. This nested structure means span(U:,1:R3) span(U:,1:R2 ) span(U ). In practice, R2 = R3 R1 (e.g., R2 = R3 = 64) suffices to capture most interaction structure, confirming our hypothesis that higher-order contributions are low-dimensional (Section 4). Finally, to satisfy the identifiability aspect of P4, we enforce orthonormality of the interaction subspace. Following Stiefel optimization (Absil et al., 2008; Bonnabel, 2013), we impose = via QR retraction after each gradient step. We use positive QR retraction (Edelman et al., 1998), which corrects column signs to ensure continuity and avoids discontinuous representation changes during training. 3.4. Discussion Context-Dependent Dictionary Structure. In standard SAEs, each feature is associated with fixed dictionary atom di: regardless of context, activating feature contributes zidi to the reconstruction. PolySAE fundamentally alters this picture. Because reconstruction includes higherorder terms, the effective contribution of feature becomes context-dependent, varying with which other features are simultaneously active. This can be seen by expanding Equation (2). The linear term defines dictionary over individual features, while the quadratic and cubic terms define dictionaries over feature pairs and triples, respectively. Under our low-rank factorization, these dictionaries are implicitly given by = C(1) = (2)(U:,1:R2 U:,1:R2 ) Rddsae, Rdd2 sae, Γ = C(3)(U:,1:R3 U:,1:R3 U:,1:R3) Rdd3 sae, (4) where is the linear dictionary, the pairwise interaction dictionary, and Γ the triple interaction dictionary. Column (i, j) of specifies how the co-activation zizj modifies the reconstruction, while column (i, j, k) of Γ specifies the contribution arising from the joint activation zizjzk. The computational form in Equation (3) is algebraically equivalent to Equation (4) but avoids explicitly materializing the d2 sae-dimensional dictionaries. saeand d3 Compositional Capacity. Using the same dsae base features as standard SAE, PolySAE can support interactiondriven structure across (cid:0)dsae (cid:1)R3 feature pairs (cid:1)R2 + (cid:0)dsae 2 3 and triples, enabling substantially larger space of distinct semantic compositions without increasing the number of learned features. This capacity is mediated through shared low-rank interaction space: rather than allocating independent parameters to each feature combination, interactions are expressed via R2 and R3 shared modes. As result, large number of potential feature combinations are realized through small number of reusable interaction directions, reflecting the empirically observed low-dimensional structure of feature interactions. Parameter Efficiency. PolySAE modifies only the decoder; the encoder is unchanged. standard SAE has 2d dsae + + dsae parameters. When the linear term is full rank (R1 = d), PolySAE adds = d2 + d(R2 + R3) + 2 parameters. With the empirically optimal choice R2 = R3 and R2 [0.06R1, 0.11R1], this yields = (1.121.22) d2 (up to constants). For GPT-2 small (d = 768, dsae = 16,384), this corresponds to an increase of 2.53% of the full SAE. 4. Empirical Evaluation 4.1. Experimental Setup Our training pipeline is built by extending SAELens (Bloom et al., 2024) to include PolySAE. We train and evaluate our methods against the standard SAE with three sparsification strategies, TopK (Gao et al., 2025), BatchTopK (Bussmann et al., 2024), and Matryoshka (Bussmann et al., 2025b). Throughout all experiments, we use sparsity level of = 64 with 16,384 latents trained on residual-stream activations from four pretrained language models of different scales: Gemma-2-2B (Gemma Team, 2024) (layer 19), Pythia-410M and Pythia-1.4B (Biderman et al., 2023) (layers 15 and 12, respectively), and GPT-2 Small (Radford et al., 2019) (layer 8). Training uses 500M tokens (300M for GPT-2 Small) with context length 128. For Gemma-2-2B and GPT-2 Small, we use OpenWebText (Gokaslan et al., 2019); for Pythia models, we use an uncopyrighted variant of the deduplicated Pile (Gao et al., 2021). We evaluate learned features using SAEBench (Karvonen et al., 2025), which reports reconstruction metrics on held-out data from the training distribution and sparse probing performance on six classification tasks: Bias in Bios (De-Arteaga et al., 2019), AG News (Zhang et al., 2015), EuroParl (Koehn, 2005), GitHub programming languages (CodeParrot, 2022), Amazon Sentiment, and Amazon-15 (Hou et al., 2024). For more implementation details see Section B. 4.2. Reconstruction and Semantic Modeling We evaluate models along two axes: (Q1) reconstruction fidelity and (Q2) semantic modeling of the learned representations. Reconstruction quality is measured using mean squared error between the decoder output and the unnormalized network activations. To assess semantic structure, we 5 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 1. F1 Scores (%) across datasets at K=1. Format: F1 / Wasserstein (103). Mean Probing column shows mean F1 across datasets. LLM SAE variant MSE Mean F1 Europarl Bios GitHub AG News Amazon 15 GPT-2 Small Pythia-410m Pythia-1.4b Gemma2-2b Topk Topk + PolySAE BTopk BTopk + PolySAE Matryoshka Matr. + PolySAE Topk Topk + PolySAE BTopk BTopk + PolySAE Matryoshka Matr. + PolySAE Topk Topk + PolySAE BTopk BTopk + PolySAE Matryoshka Matr. + PolySAE Topk Topk + PolySAE BTopk BTopk + PolySAE Matryoshka Matr. + PolySAE 0.52 0.55 0.53 0.54 0.60 0. 0.03 0.04 0.03 0.04 0.04 0.04 0.23 0.23 0.22 0.23 0.24 0.23 1.59 1.65 1.58 1.68 1.69 1.64 67.1 77.9 65.7 78.0 65.7 77.7 71.2 77.0 65.0 77.3 64.2 74.6 75.9 81.9 64.6 76.4 64.4 72. 67.7 68.4 64.8 69.4 60.9 65.6 67.7 / 19.0 86.1 / 35.2 67.4 / 19.0 92.0 / 39.9 65.8 / 12.5 95.0 / 30.0 96.1 / 2.0 96.7 / 6.8 90.9 / 0.8 97.8 / 8.2 79.1 / 0.6 99.2 / 2.8 97.8 / 1.6 96.8 / 7.9 74.0 / 0.6 93.7 / 4.5 70.2 / 0.5 91.1 / 2.9 78.6 / 5.3 86.8 / 12.0 68.3 / 1.9 92.8 / 13.2 60.8 / 0.7 77.6 / 2.1 use two complementary metrics. Probing. We evaluate the linear separability of semantic concepts in the learned sparse representations by training logistic regression classifiers on SAE activations to predict ground-truth labels across multiple datasets. For each task, classification is performed using the feature with the largest mean activation difference between positive and negative classes, isolating semantic signal at the feature level. Distributional separation. Probing relies on post-hoc decision boundaries and may not fully reflect the intrinsic geometry of the representation. We therefore additionally compute the 1-Wasserstein distance between class-conditional activation distributions. Unlike probing, which evaluates separability at specific threshold, the Wasserstein distance captures global distributional separation, with larger values indicating more distinct semantic separation across space. Table 1 demonstrates that across four language models and three sparsification strategies, PolySAE achieves comparable MSE to standard SAE across all configurations, confirming that polynomial decoding does not sacrifice reconstruction fidelity. For probing, PolySAE consistently outperforms SAE by large margins with mean gains of more than 10% on GPT-2, and 8% on average across models (Pythia-410M, Pythia-1.4B, and Gemma2-2B) and sparsifiers. Crucially, PolySAE also achieves consistently substantially higher Wasserstein distances, with improvements of approximately 210 across all other models. Amazon Sentiment 76.0 / 4.3 83.1 / 9.7 68.8 / 4.4 84.2 / 8.5 76.2 / 3.2 81.4 / 8.1 61.5 / 0.7 75.9 / 2.3 63.9 / 0.4 74.6 / 2.1 64.4 / 0.4 66.9 / 1. 69.5 / 0.8 88.1 / 3.7 57.2 / 0.6 67.1 / 3.1 63.1 / 0.6 58.0 / 2.1 61.0 / 7.7 75.5 / 16.8 59.6 / 7.3 70.9 / 17.3 61.2 / 4.0 72.9 / 14.3 67.4 / 1.1 70.8 / 3.8 60.5 / 0.3 74.0 / 4.1 63.6 / 0.3 71.0 / 1.2 72.5 / 1.4 77.2 / 6.4 65.0 / 0.5 73.0 / 3.4 62.8 / 0.5 72.4 / 2.0 63.4 / 8.7 73.0 / 20.6 68.1 / 8.8 74.4 / 18.5 60.9 / 7.9 71.5 / 18.6 71.4 / 8.5 81.0 / 18.9 65.3 / 8.2 83.2 / 20.0 68.1 / 4.3 77.4 / 16. 64.6 / 1.5 74.0 / 5.3 59.7 / 1.1 75.2 / 4.8 62.3 / 1.1 81.8 / 3.7 69.3 / 1.9 74.7 / 9.2 63.3 / 2.3 73.8 / 8.2 65.6 / 1.9 68.2 / 6.7 71.8 / 1.2 73.3 / 4.0 58.7 / 0.3 78.6 / 4.4 58.7 / 0.3 64.8 / 1.2 77.3 / 1.4 83.4 / 6.3 65.2 / 0.4 77.6 / 3.4 64.1 / 0.4 73.6 / 2.1 69.6 / 7.1 64.7 / 16.8 67.6 / 2.6 78.3 / 18.3 64.0 / 0.8 67.5 / 3.1 71.8 / 4.4 64.5 / 10.5 71.1 / 2.8 56.4 / 10.2 57.3 / 1.5 61.7 / 4. 60.7 / 6.1 64.1 / 16.1 64.4 / 4.5 65.0 / 16.1 61.5 / 2.5 63.7 / 8.8 60.7 / 7.2 61.9 / 16.9 59.9 / 2.7 64.0 / 18.8 61.0 / 0.8 60.9 / 3.5 63.3 / 2.8 69.0 / 6.7 65.1 / 2.9 63.2 / 6.0 62.1 / 2.4 68.0 / 5.6 65.9 / 0.4 71.5 / 1.4 56.6 / 0.3 63.5 / 1.3 57.3 / 0.3 63.8 / 0.9 69.0 / 0.5 71.1 / 2.4 63.2 / 0.4 73.2 / 2.1 60.8 / 0.4 69.4 / 1.5 64.8 / 2.7 68.5 / 6.3 57.6 / 1.9 60.0 / 6.4 60.5 / 1.0 62.2 / 3. 0.8 0.7 0.6 1 i P M SAE PolySAE"
        },
        {
            "title": "64\nSparsity k (active features)",
            "content": "32 128 Figure 3. Probing Mean F1 vs. sparsity k. Shaded regions show range across widths (2k16k). PolySAE consistently outperforms SAE with significant separation at higher k. This indicates that the gains observed in probing accuracy reflect genuinely better-separated class-conditional representations, rather than improvements driven solely by favorable decision boundaries. 4.3. Ablations PolySAE Enables Competitive Performance with Sparser Codes. We ask (Q3) whether PolySAEs capacity to model feature interactions enables the use of sparser representations. Figure 3 shows probing F1 as function of active features k, with shaded regions indicating variance across dictionary widths (2k16k). PolySAE consistently 6 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding ) 3 ( R u 128 256 512 768 Reconstruction MSE 0. 0.56 0.6 0.64 0.61 0.6 0. 0.6 0.63 0.62 0.7 0.79 0. 0.63 0.63 0.62 0.64 0.6 0. 0.62 0.62 0.61 0.59 0.59 0. 64 128 512 256 Quadratic Rank (R2) 768 0.8 0.75 0. 0.65 0.6 0.55 Figure 4. Reconstruction MSE for different R2 and R3 values, with R1 = 768, using activations from GPT-2 Small. outperforms standard SAEs across all sparsity levels, with the gap widening at higher k. PolySAE also exhibits lower variance across widths, enabling competitive performance with fewer active features in smaller dictionaries where standard SAEs would require substantially more. Semantic Concentration Across Features. We next ask (Q4) whether PolySAE concentrates semantic signal into fewer features. Table 2 reports the F1 gain 15 when expanding from K=1 to K=5 active features, averaged across all probing datasets and sparsifiers. PolySAE exhibits smaller gains than standard SAEs in 3 out of 4 models, with the largest difference on GPT-2 Small (-7.6%). One interpretation is that PolySAE concentrates semantic information into fewer features, reducing the marginal benefit of additional features. This indicates that higher-order interactions absorb contextual variability while PolySAEs linear features remain more semantically focused. Ablating different ranks. Figure 4 examines the effect of interaction ranks on reconstruction, for fixed R1 = 768 on GPT-2 Small. PolySAE achieves competitive reconstruction with modest interaction ranks (R2 = R3 = 64). Increasing ranks beyond this does not improve reconstruction, suggesting that the additional capacity is unnecessary for capturing interaction structure in this setting. 5. Qualitative Analysis: Making Sense of"
        },
        {
            "title": "Learned Interactions",
            "content": "To better understand the learnt interactions, we firstly ask (Q5) whether PolySAEs higher-order terms encode genuine Table 2. Mean F1 Gain from K=1 to K=5 after averaging the results from all 6 probing datasets and all 3 sparsifiers. Model SAE PolySAE +14.3 GPT-2 Small Pythia-410m +11.0 +10.3 Pythia-1.4b +13.6 Gemma2-2b +6.7 +10.5 +10.5 +10.6 Overall +12.3 +9.6 PolySAE effect 7.6 0.5 +0.2 3. 2.7 (cid:13)(ui uj)C(2)(cid:13) (cid:13) compositional structure or merely reflect surface-level co-occurrence. To study this, we analyze SAE and PolySAE activations trained with Top-K sparsification on GPT-2 small, using 1M OpenWebText texts. For each feature pair (i, j), we first define the learned quadratic (cid:13)2 , which interaction strength Bij = λ2 depends only on the trained decoder parameters and measures how much representational capacity the model allocates to the (i, j) interaction. Second, we compute the empirical co-occurrence frequency Nij by counting token positions in which both features appear in the top-K active set across the same corpus. If polynomial interactions merely replicated bigram statistics, Bij and Nij would correlate strongly. As baseline, we consider the empirical covariance of SAE activations, which captures the full pairwise structure accessible to linear model. As expected, this covariance correlates strongly with cooccurrence frequency (r = 0.82). In contrast, PolySAEs learned interactions exhibit negligible correlation with cooccurrence (r = 0.06), indicating that interaction capacity is allocated based on criteria largely orthogonal to frequency. Since higher-order dictionaries do not simply encode cooccurrence, we finally ask (Q6) whether the learned interactions are interpretable. To analyze this structure, we construct dictionary mapping each feature to its most activating tokens, then examine feature pairs and triples with high interaction strength by extracting representative contexts in which the corresponding features co-activate. Selected examples in Table 3 and Table 4 illustrate the qualitative structure captured by PolySAEs higher-order terms. Second-order interactions often correspond to coherent phrase-level compositions that are not recoverable from either feature in isolation, such as coffee star yielding contexts referring to Starbucks, highly non-linear semantic mapping. In contrast, SAEs typically activate broad or weakly related features in these contexts, failing to recover the composed meaning. Third-order interactions further refine such compositions by conditioning on additional context. For example, PolySAE distinguishes financial investing from unrelated -ing usages by integrating morphological cues with market-related features, and disambiguates 7 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 3. Second-Order Interaction Examples Captured by PolySAE. Quadratic interactions bind two features to capture context-dependent semantic structure beyond co-occurrence. SAE often recovers individual components but fails to represent the composed meaning. Poly F1 Poly F2 Context SAE Observed Pattern [star, stars] [coffee, tea] [surgery, repair] [Trans, LGBT] Weve all certainly heard of beers brewed with espresso, but how about one with an espresso shot poured over the top? Starbucks [Apple, Google] The interaction binds features to represent specific named entity creating new semantic category. Some in the transgender community are worried suspicious fire at Montreal clinic will add delays to an already lengthy process to get gender reassignment surgery [birth, baby] Specialization: general concept (surgery) gets specialized by domain context (Trans,LGBT) narrowing the semantic scope. [DNA, genetic] [mod, mods] Activists are opening up new front in their campaign against genetic modification. The latest target is genetically-modified trees [modified, edit] Multiple modifiers stack to create specific compound meanings. Interaction binds genetic with the action modification. [secret, hidden] [Snowden, WikiLeaks] On May 24th PBS aired Frontline documentary about alleged Wikileaker Bradley Manning called WikiSecrets [secret, secrets] Feature interaction binds topical concepts to create coined term that could not be modeled via co-occurrence alone. Table 4. Third-Order Interaction Examples Captured by PolySAE. Cubic interactions condition pairwise compositions on additional context, disambiguating meaning through three-way binding. Vanilla SAE typically activates broader or less specific features. Poly Poly F2 Poly F3 Context SAE Observed pattern [proved, proven] [star, stars, superstar] [reputation, fame] David Bowie proved some stars are big enough not to make themselves available [star, stars, superstar] [nuclear, reactor] [test, testing] [radiation, magnetic] US tests nuclear-capable missile with the range to strike North Korea [nuclear, atomic] [black, racial] [Americans, Canadians] [people, women] In push to get more Black Americans involved in the world of tech [ing, ting] [stock, market] [invest, investing] Investing.com Philippines stocks were higher after [Americans, Muslims, Jews] [ing, training, running] [historic, historical] [UFC, MMA] [strong, impressive] Jon Jones historic UFC title reign came to an end [the, .] Three-way relational binding, all arguments must be present; reputation disambiguates which aspect of stars is relevant to the proving action. Specifying concept; Event type (testing) domain (nuclear) capability (radiation) (Parsons, 1990) Multi-attribute category intersection, binding demographic attributes. Three-way interaction between morphological marker (ing) and domain (stock, market) (Asher, 2011). The standard for historic is calibrated by specific domain and assessed quality; Degree (historic) domain (UFC) evaluation (strong, impressive) generic entities such as nuclear or Americans based on surrounding semantic attributes. Across examples, higherorder terms absorb contextual variation that would otherwise fragment linear features, allowing PolySAE to express compositional meaning through structured interactions rather than proliferating context-specific atoms. Further examples in Section confirm these patterns. 6. Conclusion We introduced PolySAE, sparse autoencoder that extends the decoder with higher order terms to model feature interactions while preserving linear encoder for interpretability. Through low-rank tensor factorization on shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead. Across four language models and three SAE variants, PolySAE consistently improves probing F1 by 8% on average while maintaining comparable reconstruction error. PolySAE also achieves 210 larger Wasserstein distances between classconditional feature distributions, indicating that polynomial decoding produces representations with more separated semantic structure. Crucially, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06), suggesting that the model allocates interaction capacity based on compositional structure rather than sur8 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding face statistics. Limitations: We study models up to 2B parameters and, despite general applicability, restrict experiments to forced-sparsity SAE variants. layers with 3rd order polynomials. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1051810528, 2021."
        },
        {
            "title": "Impact Statement",
            "content": "This work contributes to the field of interpretable machine learning by introducing PolySAE, method for modeling non-additive feature interactions in sparse autoencoders. By enabling explicit representation of compositional structure while preserving linear, human-interpretable features, this approach advances tools for mechanistic analysis of large language models. Improved interpretability has the potential to support downstream efforts in model auditing, debugging, and safety research by making it easier to identify, analyze, and intervene on meaningful internal representations. The primary anticipated benefits of this work are methodological and scientific. PolySAE is intended as an analysis tool rather than deployment-facing component, and it does not directly increase the capabilities of language models. As with other interpretability methods, there is possibility that insights into internal representations could be misused to more effectively manipulate model behavior, but we do not identify novel or unique risks introduced by this work beyond those already present in the interpretability literature. Overall, we believe this work has net positive societal impact by strengthening the technical foundations of interpretability and contributing to the long-term goal of building more transparent, controllable, and trustworthy machine learning systems."
        },
        {
            "title": "References",
            "content": "Absil, P.-A., Mahony, R., and Sepulchre, R. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2008. Alain, G. and Bengio, Y. Understanding intermediate arXiv preprint layers using linear classifier probes. arXiv:1610.01644, 2016. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016. Arora, S., Ge, R., Ma, T., and Moitra, A. Simple, efficient, and neural algorithms for sparse coding. In Conference on Learning Theory (COLT), pp. 113149. PMLR, 2015. Asher, N. Lexical Meaning in Context: Web of Words. Cambridge University Press, 2011. Babiloni, F., Marras, I., Kokkinos, F., Deng, J., Chrysos, G., and Zafeiriou, S. Poly-nl: Linear complexity non-local Bao, C., Ji, H., Quan, Y., and Shen, Z. Dictionary learning for sparse coding: Algorithms and analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38 (7):13561369, 2016. Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219, 2022. Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Fox, P., Garfinkel, B., Goldfarb, D., et al. International ai safety report. arXiv preprint arXiv:2501.17805, 2025. Bereska, L. and Gavves, S. Mechanistic interpretability for AI safety - review. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https:// openreview.net/forum?id=ePUVetPKu6. Survey Certification, Expert Certification. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., Skowron, A., Sutawika, L., and Van Der Wal, O. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 23972430. PMLR, 2023. URL https://proceedings.mlr.press/ v202/biderman23a.html. Blondel, M., Fujino, A., Ueda, N., and Ishihata, M. HigherIn Advances in Neural order factorization machines. Information Processing Systems, volume 29, pp. 3351 3359, 2016. Bloom, J., Tigges, C., Duong, A., and Chanin, D. SAELens. https://github.com/jbloomAus/ SAELens, 2024. GitHub repository. Bonnabel, S. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Automatic Control, 58 (9):22172229, 2013. doi: 10.1109/TAC.2013.2254619. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Towards monosemanticity: DeAskell, A., et al. composing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html. Bussmann, B., Leask, P., and Nanda, N. BatchTopK In NeurIPS Workshop on Scisparse autoencoders. entific Methods for Understanding Deep Learning, 9 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding 2024. URL https://openreview.net/forum? id=d4dpOCqybL. Bussmann, B., Nabeshima, N., Karvonen, A., and Nanda, N. Learning multi-level features with matryoshka sparse autoencoders. arXiv preprint arXiv:2503.17547, 2025a. Bussmann, B., Nabeshima, N., Karvonen, A., and Nanda, N. Learning multi-level features with Matryoshka sparse autoencoders. In Proceedings of the 42nd International Conference on Machine Learning (ICML), volume 267 of Proceedings of Machine Learning Research. PMLR, 2025b. Carreira, J., Caseiro, R., Batista, J., and Sminchisescu, C. Semantic segmentation with second-order pooling. In European Conference on Computer Vision (ECCV), pp. 430443. Springer, 2012. Chrysos, G., Georgopoulos, M., and Panagakis, Y. Conditional generation using polynomial expansions. Advances in Neural Information Processing Systems, 34:28390 28404, 2021. Chrysos, G. G., Moschoglou, S., Bouritsas, G., Panagakis, Y., Deng, J., and Zafeiriou, S. P-nets: Deep polynomial neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73257335, 2020. Chrysos, G. G., Georgopoulos, M., Deng, J., Kossaifi, J., Panagakis, Y., and Anandkumar, A. Augmenting deep classifiers with polynomial neural networks. In European Conference on Computer Vision, pp. 692716. Springer, 2022a. Chrysos, G. G., Moschoglou, S., Bouritsas, G., Deng, J., Panagakis, Y., and Zafeiriou, S. Deep polynomial neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):40214034, 2022b. doi: 10. 1109/TPAMI.2021.3058891. Published online 2021. Chrysos, G. G., Wu, Y., Pascanu, R., Torr, P., and Cevher, V. Hadamard product in deep learning: Introduction, advances and challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. CodeParrot. Github code dataset. //huggingface.co/datasets/codeparrot/ github-code, 2022. https: Csordás, R., Potts, C., Manning, C. D., and Geiger, A. Recurrent neural networks learn to store and generate sequences using non-linear representations. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 248262, 2024. 10 De-Arteaga, M., Romanov, A., Wallach, H., Chayes, J., Borgs, C., Chouldechova, A., Geyik, S. C., Kenthapadi, K., and Kalai, A. T. Bias in bios: case study of semantic representation bias in high-stakes setting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* 19), pp. 120128. ACM, 2019. doi: 10.1145/3287560.3287572. Dooms, T. and Gauderis, W. Finding manifolds with In Mechanistic Interpretabilbilinear autoencoders. ity Workshop at NeurIPS 2025, 2025. URL https: //openreview.net/forum?id=ybJXIh4vcF. Dubey, A., Radenovic, F., and Mahajan, D. Scalable interpretability via polynomials. Advances in neural information processing systems, 35:3674836761, 2022. Dumoulin, V., Shlens, J., and Kudlur, M. learned repIn International Conferresentation for artistic style. ence on Learning Representations, 2017. URL https: //openreview.net/forum?id=BJO-BuT1g. Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find interpretable LLM feature circuits. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=J6zHcScAo0. Edelman, A., Arias, T. A., and Smith, S. T. The geometry of algorithms with orthogonality constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303353, 1998. Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et al. Toy models of superposition. Transformer Circuits Thread, 2022. https://transformercircuits.pub/2022/toy_model/index.html. Engels, J., Michaud, E. J., Liao, I., Gurnee, W., and Tegmark, M. Not all language model features In The Thirteenth Inare one-dimensionally linear. ternational Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=d63a4AM4hb. Fodor, J. A. and Pylyshyn, Z. W. Connectionism and cognitive architecture: critical analysis. Cognition, 28(1-2): 371, 1988. Freeman, W. T. and Tenenbaum, J. B. Learning bilinear models for two-factor problems in vision. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 554560. IEEE, 1997. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800GB dataset PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2021. Gao, L., Dupre la Tour, T., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/ forum?id=tcsZt9ZNKD. Gao, Y., Beijbom, O., Zhang, N., and Darrell, T. Compact bilinear pooling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 317326, 2016. Gauderis, W. and Dooms, T. Compositionality unlocks deep interpretable models. In Connecting Low-Rank Representations in AI: At the 39th Annual AAAI Conference on Artificial Intelligence, 2025. Gemma Team. Gemma 2: Improving open language models at practical size. Technical report, Google URL https://storage. DeepMind, googleapis.com/deepmind-media/gemma/ gemma-2-report.pdf. 2024. Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S. OpenWebText corpus. Zenodo, 2019. URL https:// zenodo.org/records/3834942. Grasedyck, L., Kressner, D., and Tobler, C. literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):5378, 2013. Haspelmath, M. and Sims, A. Understanding morphology. Routledge, 2013. Hendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J. Unsolved problems in ML safety. arXiv preprint arXiv:2109.13916, 2021. Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. science, 313 (5786):504507, 2006. Hou, Y., Li, J., He, Z., Yan, A., Chen, X., and McAuley, J. Bridging language and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952, 2024. Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interIn The Twelfth pretable features in language models. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=F76bwRSLeK. Hyvärinen, A. and Oja, E. Independent component analysis: Algorithms and applications. Neural Networks, 13(4-5): 411430, 2000. 11 Jayakumar, S. M., Czarnecki, W. M., Menick, J., Schwarz, J., Rae, J., Osindero, S., Teh, Y. W., Harley, T., and Pascanu, R. Multiplicative interactions and where to find them. In International conference on learning representations, 2020. Karvonen, A., Rager, C., Lin, J., Tigges, C., Bloom, J. I., Chanin, D., Lau, Y.-T., Farrell, E., McDougall, C. S., Ayonrinde, K., Till, D., Wearden, M., Conmy, A., Marks, S., and Nanda, N. SAEBench: comprehensive benchmark for sparse autoencoders in language model interpretability. In Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pp. 2922329264. PMLR, 2025. URL https://proceedings.mlr.press/ v267/karvonen25a.html. Kim, J.-H., On, K.-W., Lim, W., Kim, J., Ha, J.-W., and Zhang, B.-T. Hadamard product for low-rank bilinear pooling. In International Conference on Learning Representations, 2017. Koehn, P. Europarl: parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pp. 7986, Phuket, Thailand, September 13-15 2005. Konda, K., Memisevic, R., and Krueger, D. Zero-bias autoencoders and the benefits of co-adapting features. arXiv preprint arXiv:1402.3337, 2014. Lee, H., Ekanadham, C., and Ng, A. Sparse deep belief net model for visual area v2. Advances in neural information processing systems, 20, 2007. Lindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce, A., Turner, N. L., Citro, C., Abrahams, D., Carter, S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T., McDougall, C., Cunningham, H., Henighan, T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thompson, T. B., Zimmerman, S., Rivoire, K., Conerly, T., Olah, C., and Batson, J. On the biology of large language model. Transformer Circuits Thread, 2025. URL https://transformer-circuits.pub/ 2025/attribution-graphs/biology.html. Mallat, S. G. and Zhang, Z. Matching pursuits with timeIEEE Transactions on signal frequency dictionaries. processing, 41(12):33973415, 1993. Mason, J. C. and Handscomb, D. C. Chebyshev Polynomials. CRC Press, 2002. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 1735917372, 2022. PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Oldfield, J., Im, S., Li, S., Nicolaou, M., Patras, I., and Chrysos, G. Towards interpretability without sacrifice: Faithful dense layer decomposition with mixture of decoders. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. URL https: //openreview.net/forum?id=jcvX8XFNqX. Oldfield, J., Torr, P., Patras, I., Bibi, A., and Barez, F. Beyond linear probes: Dynamic safety monitoring for language models. arXiv preprint arXiv:2509.26238, 2025b. Olshausen, B. A. and Field, D. J. Sparse coding with an overcomplete basis set: strategy employed by V1? Vision Research, 37(23):33113325, 1997. Parsons, T. Events in the semantics of english, 1990. Partee, B. H. Lexical semantics and compositionality. In Gleitman, L. R. and Liberman, M. (eds.), An Invitation to Cognitive Science: Language, volume 1, pp. 311360. MIT Press, Cambridge, MA, 1995. Pearce, M. T., Dooms, T., Rigg, A., Oramas, J., and Sharkey, L. Bilinear MLPs enable weight-based mechanistic interpretability. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=gI0kPklUKS. Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. I. Radford, A., Wu, J., Child, R., Luan, D., Amodei, Language models are unreport, Technical URL https://cdn.openai. D., and Sutskever, supervised multitask learners. OpenAI, 2019. com/better-language-models/language_ models_are_unsupervised_multitask_ learners.pdf. Rajamanoharan, S., Conmy, A., Smith, L., Lieberum, T., Varma, V., Kramár, J., Shah, R., and Nanda, N. Improving dictionary learning with gated sparse autoencoders. arXiv preprint arXiv:2404.16014, 2024a. Rajamanoharan, S., Lieberum, T., Sonnerat, N., Conmy, A., Varma, V., Kramár, J., and Nanda, N. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024b. Rendle, S. Factorization machines. In 2010 IEEE International Conference on Data Mining, pp. 9951000. IEEE, 2010. Rimsky, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger, E., and Turner, A. Steering llama 2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of 12 the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1550415522, 2024. Shazeer, N. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Shin, Y. and Ghosh, J. The pi-sigma network: An efficient higher-order neural network for pattern classification and function approximation. In IJCNN-91-Seattle international joint conference on neural networks, volume 1, pp. 1318. IEEE, 1991. Smolensky, P. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159216, 1990. Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https: //transformer-circuits.pub/2024/ scaling-monosemanticity/index.html. Tenenbaum, J. and Freeman, W. Separating style and content. Advances in neural information processing systems, 9, 1996. Volterra, V. Theory of Functionals and of Integral and Integro-Differential Equations. Dover Publications, 1959. Originally published 1930. Wong, E., Santurkar, S., and Madry, A. Leveraging sparse In Interlinear layers for debuggable deep networks. national Conference on Machine Learning, pp. 11205 11216. PMLR, 2021. Zhang, S.-X., Gong, Y., and Yu, D. Encrypted speech recognition using deep polynomial networks. In ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 56915695. IEEE, 2019. Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volURL https://proceedings. ume 28, 2015. neurips.cc/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper. pdf. PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Algorithm 1 PolySAE Training (Shared-U) Input: activations {x}, ranks (R1, R2, R3), sparsity K, learning rate η Initialize qr+(Urand); for each minibatch do λ2 0.5; λ3 0.5 // Encode (P1: linear encoder with norm rescaling) Ex + benc Compute decoder norms Rdsae: di = PolyDec(ei)2 TopK(ReLU(h d), K) // Decode (P2P4: polynomial, shared, hierarchical) y1 (z ) (1) A2 U:,1:R2 y2 (A2 A2) C(2) A3 U:,1:R3 y3 (A3 A3 A3) C(3) bdec + y1 + λ2 y2 + λ3 y3 // Update with manifold retraction (P4) x2 Update all parameters via (Q, R) qr(U ) diag(sgn(diag(R))) QS 2 + architecture specific regularizations (e.g. Matryoshka) end for A. PolySAE Algorithm We provide the full training algorithm for PolySAE in Algorithm 1, detailing the encoding, polynomial decoding, and optimization steps used throughout all experiments. B. Implementation Details Architecture and Sparsification. We train standard sparse autoencoders (SAEs) and PolySAEs with latent width of 16,384 and sparsity level = 64. Encoders use one of three sparsification strategies: Top-K (Gao et al., 2025), BatchTopK (Bussmann et al., 2024), or Matryoshka (Bussmann et al., 2025b). For Top-K and BatchTopK, the largest activations per token (or batch) are retained and the remainder zeroed. All models are trained on residual-stream activations extracted from pretrained language models. LLMs. We evaluate SAEs and PolySAEs on standard set of pretrained language models spanning range of scales: GPT-2 Small (Radford et al., 2019), Pythia-410M and Pythia-1.4B (Biderman et al., 2023), and Gemma-2-2B (Gemma Team, 2024). For each model, we extract residual-stream activations from single transformer layer chosen near the center of the network, following the methodology of Dunefsky et al. (2024). PolySAE Decoder Ranks. The rank configurations used in our experiments are: GPT-2 Small (Radford et al., 2019): (768, 64, 64) Pythia-410M (Biderman et al., 2023): (1024, 128, 128) Pythia-1.4B (Biderman et al., 2023): (2048, 128, 128) Gemma-2-2B (Gemma Team, 2024): (2304, 128, 128) Training Setup. All models are trained using the Adam optimizer with β1 = 0.9 and β2 = 0.999, constant learning rate of 3 104 with no warmup or decay schedules. We use batch size of 4096 tokens and context length of 128. We apply gradient clipping with maximum norm of 1.0 to stabilize training. No weight decay or L1 regularization is applied to the 13 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding encoder or decoder weights. Training runs for 5 108 tokens for Gemma-2-2B and Pythia models, and 3 108 tokens for GPT-2 Small, following the protocol used in the main experiments. Datasets. For Gemma-2-2B and GPT-2 Small, training data is drawn from OpenWebText (Gokaslan et al., 2019). For Pythia-410M and Pythia-1.4B, we use an uncopyrighted variant of the deduplicated Pile (Gao et al., 2021). Reconstruction is evaluated on held-out data from the same distribution as training. Evaluation. We evaluate learned representations using SAEBench (Karvonen et al., 2025). Reported metrics include reconstruction error on held-out data and sparse probing performance on six classification tasks: Bias in Bios (De-Arteaga et al., 2019), AG News (Zhang et al., 2015), EuroParl (Koehn, 2005), GitHub programming languages (CodeParrot, 2022), Amazon Sentiment, and Amazon-15 (Hou et al., 2024). Implementation. Our training pipeline extends SAELens (Bloom et al., 2024) to support PolySAE while preserving the standard SAE training interface. PolySAE differs from standard SAEs only in the decoder; all other components, including the encoder, sparsification strategy, optimizer, and evaluation pipeline, are shared across models. C. Extended Qualitative Analysis We present an extended qualitative analysis of the interaction structure learned by PolySAE. The analysis proceeds hierarchically, first examining second-order (pairwise) interactions and then extending to third-order (triplet) compositions. Throughout, we compare PolySAE to vanilla Top-K SAE trained under identical conditions. C.1. Second-Order Analysis We begin by analyzing pairwise interactions to assess whether PolySAE captures compositional structure beyond surfacelevel co-occurrence. Setup. Both models are applied to 1M documents from OpenWebText. Features are ranked by total activation mass, and the top 10,000 are retained, yielding approximately 5 107 candidate feature pairs. Interaction Strength. For PolySAE, we quantify the strength of feature pair (i, j) using the learned quadratic decoder weights: Bij = λ2 (cid:13) (cid:13) (cid:13)(ui uj)C(2)(cid:13) (cid:13) (cid:13) , (5) which reflects how much decoder capacity is assigned to that interaction. For the vanilla SAE, which lacks explicit interaction parameters, we use empirical feature covariance, Covij = E[zizj] E[zi]E[zj], (6) as proxy for pairwise structure. Relation to Co-occurrence. We independently estimate empirical co-occurrence by counting positions where both features appear in the Top-K active set. For the vanilla SAE, covariance is strongly correlated with co-occurrence (r = 0.82), indicating that pairwise structure largely mirrors frequency. In contrast, PolySAEs interaction strengths show negligible correlation with co-occurrence (r = 0.06), suggesting that learned interactions reflect structure beyond surface statistics. Qualitative Regimes. The weak coupling between interaction strength and frequency allows us to identify qualitatively distinct regimes. Of particular interest are latent interactions: feature pairs with strong learned interactions despite low empirical co-occurrence. These pairs often correspond to meaningful compositional patterns that are not recoverable from frequency alone. Examples. For interactions above the 80th percentile in Bij, we extract representative contexts in which both features co-activate. We mark the target token in each sentence and label features by their top-activating tokens. Comparing these contexts with vanilla SAE activations highlights cases where PolySAE captures relationships that the linear model does not. 14 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 5. Compositional Interactions Captured by PolySAE. PolySAE features (A and B) bind in context to represent specific compositional concepts. Vanilla SAE features (with high-frequency features filtered) fail to capture these compositions. Poly Feature Poly Feature Context [star, stars] [coffee, tea] Weve all certainly heard of beers brewed with espresso, but how about one with an espresso shot poured over the top? Starbucks Vanilla SAE [Apple, Google] [officially, ically] [traditional, conventional] Its hard to say whats most impressive about Eduardo Garcia. The classically-trained chef spent years cooking aboard yachts [newly, ly] [surgery, repair] [Trans, LGBT] [DNA, genetic] [mod, mods] Some in the transgender community are worried suspicious fire at Montreal clinic will add delays to an already lengthy process to get gender reassignment surgery [birth, baby] Activists are opening up new front in their campaign against genetic modification. The latest target is genetically-modified trees, which scientists believe could bring huge sustainability [modified, edit] [secret, hidden] [business, businesses] [Snowden, WikiLeaks] On May 24th PBS aired Frontline documentary about alleged Wikileaker Bradley Manning called WikiSecrets [secret, secrets] [man, woman] By Joseph George. The businessman dad of the boy who drove Ferrari and was arrested by police in Kerala, India [man, President] C.2. Third-Order Analysis We next examine whether third-order interactions refine or disambiguate pairwise compositions. Candidate Selection. We focus on latent second-order pairsthose with high interaction strength but low cooccurrenceand identify corpus positions where both features are simultaneously active. Triplet Scoring. Within these contexts, we evaluate all co-active third features using the learned cubic decoder: Gamma(f1, f2, k) = λ3 (cid:12) (cid:12)(uf1 uf2 ) (cid:12) C(3)(cid:12) (cid:12) (cid:12) . (7) For each pair, we retain the third feature with the highest score, after filtering stopword-like features. Interpretation. The resulting triplets are consistently interpretable, with the third feature modulating the meaning of the pair rather than introducing unrelated content. Common patterns include entityattributedomain and subjectobject context structures. Representative examples are shown in Table 10, illustrating how higher-order interactions sharpen and contextualize pairwise compositions. C.3. Additional Second-Order Interaction Examples Tables 59 show additional second-order interaction examples. Each row highlights token where two PolySAE features are simultaneously active. Across these tables, the interacting features are typically more specific than the corresponding vanilla SAE features in the same context. The vanilla SAE often activates on single high-level, morphological, or broadly related feature, while PolySAE activations reflect more refined decomposition at the highlighted token. C.4. Additional Third-Order Interaction Examples Tables 10 and 11 present further third-order examples. Each row shows contexts in which three PolySAE features co-activate at the same token. In these cases, the activated features vary with context and appear more specific than the corresponding vanilla SAE activations, which often capture only one component or default to generic features. 15 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 6. PolySAE Interactions Brand & Proper Noun Decomposition. PolySAE decomposes compound names into their semantic constituents. Vanilla SAE (with high-frequency features filtered) often fires on unrelated entities or only captures the surface form. Poly Feature Poly Feature Context [economic, economy] [Times, magazine] [field, fields] [University, school] [Star, Chronicle] [staff, crew] [Dragon, Iron] [steel, Pittsburgh] The RBI on Wednesday did not allow Stanley Pignal, the South Asian business and finance correspondent for the Economist magazine, to attend the central banks John Doe is Jesuit with ADHD. He was an outstanding student and compassionate senior at Fairfield University who played sports and volunteered often at literacy Man Arrested after Stolen Mower Runs Out of Gas. By West Kentucky Star Staff. PADUCAH, KY The Iron Horde is on the march, and the Warlords of Draenor are primed to invade Azeroth on November 13! Steel yourself for the onslaught by watching Vanilla SAE [economics, economist] [York, Washington] [staff, faculty] [assault, steel] [Italian, Italy] [gang, mob] Details obtained by the Guardian reveal extent to which Sicilian mafia clans are migrating north after running into financial problems in Italy. [State, ISIS] Table 7. PolySAE Interactions Morphological Composition. PolySAE binds suffix/prefix features with semantic content to form derived words. Vanilla SAE (with high-frequency features filtered) captures only generic morphological patterns without semantic binding. Poly Feature Poly Feature Context [ers, Workers] [administration, administrative] Piedmont High School. school reveals it has Fantasy Slut League Administrators try to do the right thing, but fall woefully short of Vanilla SAE [members, ers] [ing, ings] [arrested, arrest] [protest, protests] [making, ing] [ers, Workers] [photos, pictures] [ized, ization] [treatment, drugs] [bound, ice] [gun, guns] Earlier this year, The Heritage Foundations Meese Center released Arresting Your Property, comprehensive report on civil asset forfeiture-the much mal [ing, training] Major League Baseball can no longer claim to be free of any anthem-protesting players. On Saturday night, As catcher Bruce Maxwell took [ing, training] In-Sight Film. The film in-sight was produced in conjunction with the Format Photography Festival to mark 10 years of the Street Photographers group [members, ers] The flu shot is quack science medical hoax. While some vaccines do confer immunization effectiveness, the flu shot isnt one of them [development, ation] new Texas law gives gun owners new right to store weapon (any lawfully owned firearm, not just those owned under Concealed Handgun License [gun, weapons] 16 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 8. PolySAE Interactions Domain-Specific Collocations. PolySAE captures specialized terminology through the interaction of domain features. Vanilla SAE (with high-frequency features filtered) often misses the domain-specific meaning. Poly Feature Poly Feature Context [football, NFL] [conference, conferences] Statement from the Southeastern Conference Office Regarding the Florida-LSU football game: The LSU-Florida football game scheduled for Saturday in Gainesville Vanilla SAE [League, Conference] [earnings, financial] [number, numbers] T-Mobile US, Inc. TMUS is scheduled to report fourth-quarter 2015 financial numbers, before the opening bell on Feb 17. Last [numbers, figures] [technology, tech] [development, developers] [Canada, Canadian] [oil, pipeline] You cant look at internet news lately without seeing the latest and greatest in nanotechnology developments. Everything these days is being manufactured smaller, faster [it, said] Eddy Radillo holds Texas flag and sign opposing the Transcanada Keystone Pipeline in February 2012 outside the Lamar County Courthouse in Paris [oil, gas] [diet, fitness] [train, rail] Heres what you need to know... Your gains will stagnate if you only weight train within the same rep ranges and loading patterns. [training, train] Table 9. PolySAE Interactions Compound Words & Phrases. PolySAE captures compound words and multi-word phrases through feature interactions. Vanilla SAE (with high-frequency features filtered) often misses the compositional meaning entirely. Poly Feature Poly Feature Context [director, founder] [lead, managing] FRIEND OF MINE recently made the following observation about Ezra Koenig, the founder and lead singer of Vampire Weekend. Did you realize, Vanilla SAE [led, lead] [written, designed] [alleged, allegations] [research, researcher] [level, levels] [music, musical] [official, officer] [involved, involvement] [support, help] [shooting, shot] [focus, focused] The following story was written and researched by Rone Tempest for The Utah Investigative Journalism Project in partnership with The Salt Lake Tribune. Dustin Porter said [created, made] Back to previous page. Accusations against generals cast dark shadow over Army. By Ernesto Londoño. The accusations leveled against [place, made] ROCHESTER, N.Y. Members of Rochesters music community continue to pull together to remember and help the family fellow musician who [man, President] STEAL THIS SHOWs Patreon campaign helps keep us free and independent. If you enjoy the show, get involved. Our patrons get access to [started, ready] Berenice Abbott was an American photographer best known for her black-and-white photography of New York City. She heavily focused her shooting [ing, training] [document, documents] [content, contents] Use these links to rapidly review the document. TABLE OF CONTENTS. INDEX TO CONSOLIDATED FINANCIAL STATEMENTS [Introduction, History] 17 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 10. Third-Order Compositional Interactions Captured by PolySAE. Three PolySAE features (Fi, Fj, Fk) bind in context to represent compositional concepts. Vanilla SAE often captures individual components but misses the compositional structure. Poly Fi Poly Fj Poly Fk Context Vanilla SAE [nuclear, reactor, atomic] [nuclear, Fukushima, reactor] [black, white, racial] [test, testing, tested] [Americans, Canadians, Australians] [radiation, laser, magnetic] [people, women, men] US tests nuclear-capable missile with the range to strike North Korea. The US has test-fired nuclear-capable intercontinental ballistic missile In push to get more Black Americans involved in the world of tech, slew of organizations have teamed up with South by Southwest [Americans, Muslims, Jews] [ing, ings, ting] [stock, trading, market] [investment, invest, investing] Philippines stocks higher at close of trade; PSEi Composite up 0.57%. Investing.com Philippines stocks were higher after [ing, training, running] [line, lines, lining] [supply, supplies, shortage] [road, route, pipeline] The same is true of supply lines into landlocked Afghanistan. Within months of the 2001 invasion, Mr. Musharraf signed deal [the, ., , of] [the, , of, a] [proved, proven, prove] [star, stars, superstar] [reputation, popularity, fame] Arguably the biggest surprise would have been if he had turned up, but David Bowie proved some stars are big enough not to have make themselves available [star, stars, superstar] [historic, historical, historically] [treated, treat, treating] [UFC, fight, MMA] [strong, impressive, solid] After 1,501 days as UFC light-heavyweight champion, Jon Jones historic title reign came to an end late Tuesday when he was stripped [the, ., , of] [consumers, consumer, consumption] [customers, customer, clients] Jeremy Corbyn today warned the banking industry it must not treat consumers and entrepreneurs as cash cow and attacked the links between senior politicians [the, ., , of] 18 PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding Table 11. Additional Third-Order PolySAE Interactions. Further examples of three-way feature compositions. Vanilla SAE sometimes captures individual components but misses the compositional structure. Poly Fi Poly Fj [Army, Force, Navy] [Israel, Israeli, Jewish] Poly Fk [IDF] Context Earlier this week, the Friends of the Israel Defense Forces, an organization dedicated to supporting the men and women serving in the IDF, held its annual dinner Vanilla SAE [Israel, Israeli, Jewish] [the, ., , of] [annual, monthly, annually] [get, make, getting] [well, ill, poorly] [accept, accepted, accepting] [percent, %, points] [regular, regularly, frequent] According to the latest research from our Wireless Smartphone Strategies (WSS) service, global smartphone shipments grew 6 percent annually to reach 360 million units [annual, monthly, annually] [the, ., , of] [film, movie, films] [documented, depicted, depicts] Three tips on how to film anywhere; slums, red light districts, museums, exhibitions, churches, and not get your video camera gear stolen [film, movie, films] [the, ., , of] [widely, commonly, widespread] [final, ultimate, preliminary] [best, better, good] April 6, 2014. CR Sunday Interview: Zack Soto. ***** is widely well-liked cartoonist, publisher and [well, poorly, badly] [the, ., , of] [great, considerable, significant] NEW YORK Dedicated Hillary Clinton supporters accepted final defeat Wednesday morning even as they struggled to accept that their candidate lost [final, finals, ultimate] [the, ., , of] [percent, %, percentage] [currency, dollar, euro] [cents] The Canadian dollar dipped below 75 cents (U.S.) in Tuesdays trading as equity markets worldwide remained extremely volatile [the, ., , of] [., $, on, to] [identified, identify, diagnosed] [virus, Ebola, HIV] [label, labels, labeled] the governor of New York State announced that the first case of Ebola had been diagnosed at Bellevue [the, ., , of] [base, bases, baseline] [fans, fan, supporters] [demand, turnout, attendance] Its shared problem among fan bases across the National Hockey League: They watch their own players so closely that, after while [the, ., , of] [the, , of, a] [unique, distinct, distinctive] [two, different, three] [separate, separated, distinction] For their collaborative project Jus Now, U.K. producer Sam Interface and Trinidad producer LAZAbeam find singularity in mashing up two distinct [people, men, officers] [the, ., , of] [line, lines, lining] [supply, supplies, shortage] [road, route, pipeline] The same is true of supply lines into landlocked Afghanistan. Within months of the 2001 invasion, Mr. Musharraf signed deal [the, ., , of] [the, , of, a] [largest, most, biggest] [able, ible, ability] [stable, stability, flexible] Groundwater, the globes most dependable water insurance system, is not as renewable as researchers once thought [the, ., , of] [the, , of, a]"
        }
    ],
    "affiliations": [
        "Archimedes AI/Athena Research Center",
        "The Cyprus Institute",
        "University of Athens",
        "University of Cyprus",
        "University of Oxford"
    ]
}