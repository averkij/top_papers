{
    "paper_title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
    "authors": [
        "Junfeng Wu",
        "Dongliang Luo",
        "Weizhi Zhao",
        "Zhihao Xie",
        "Yuanhao Wang",
        "Junyi Li",
        "Xudong Xie",
        "Yuliang Liu",
        "Xiang Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we reveal the limitations of visual tokenizers and VAEs in preserving fine-grained features, and propose a benchmark to evaluate reconstruction performance for two challenging visual contents: text and face. Visual tokenizers and VAEs have significantly advanced visual generation and multimodal modeling by providing more efficient compressed or quantized image representations. However, while helping production models reduce computational burdens, the information loss from image compression fundamentally limits the upper bound of visual generation quality. To evaluate this upper bound, we focus on assessing reconstructed text and facial features since they typically: 1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to collapse, and 4) are highly sensitive to human vision. We first collect and curate a diverse set of clear text and face images from existing datasets. Unlike approaches using VLM models, we employ established OCR and face recognition models for evaluation, ensuring accuracy while maintaining an exceptionally lightweight assessment process <span style=\"font-weight: bold; color: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to complete. Using our benchmark, we analyze text and face reconstruction quality across various scales for different image tokenizers and VAEs. Our results show modern visual tokenizers still struggle to preserve fine-grained features, especially at smaller scales. We further extend this evaluation framework to video, conducting comprehensive analysis of video tokenizers. Additionally, we demonstrate that traditional metrics fail to accurately reflect reconstruction performance for faces and text, while our proposed metrics serve as an effective complement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 2 4 1 8 1 . 5 0 5 2 : r TokBench: Evaluating Your Visual Tokenizer before Visual Generation Junfeng Wu, Dongliang Luo, Weizhi Zhao, Zhihao Xie, Yuanhao Wang, Junyi Li, Xudong Xie, Yuliang Liu, Xiang Bai Huazhong University of Science and Technology wjf5203@gmail.com, ldl@hust.edu.cn, zhaoweizhi@hust.edu.cn, zhxie17@hust.edu.cn, yhwang7@hust.edu.cn, ljy1308598378@gmail.com, xdxie@hust.edu.cn, ylliu@hust.edu.cn, xbai@hust.edu.cn HomePage: https://wjf5203.github.io/TokBench Dataset: https://huggingface.co/datasets/Junfeng5/TokBench"
        },
        {
            "title": "Abstract",
            "content": "In this work, we reveal the limitations of visual tokenizers and VAEs in preserving fine-grained features, and propose benchmark to evaluate reconstruction performance for two challenging visual contents: text and face. Visual tokenizers and VAEs have significantly advanced visual generation and multimodal modeling by providing more efficient compressed or quantized image representations. However, while helping production models reduce computational burdens, the information loss from image compression fundamentally limits the upper bound of visual generation quality. To evaluate this upper bound, we focus on assessing reconstructed text and facial features since they typically: 1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to collapse, and 4) are highly sensitive to human vision. We first collect and curate diverse set of clear text and face images from existing datasets. Unlike approaches using VLM models, we employ established OCR and face recognition models for evaluation, ensuring accuracy while maintaining an exceptionally lightweight assessment process requiring just 2GB memory and 4 minutes to complete. Using our benchmark, we analyze text and face reconstruction quality across various scales for different image tokenizers and VAEs. Our results show modern visual tokenizers still struggle to preserve fine-grained features, especially at smaller scales. We further extend this evaluation framework to video, conducting comprehensive analysis of video tokenizers. Additionally, we demonstrate that traditional metrics fail to accurately reflect reconstruction performance for faces and text, while our proposed metrics serve as an effective complement."
        },
        {
            "title": "Introduction",
            "content": "In recent years, we have witnessed rapid advancements in visual generation and its tremendous application potential. Diffusion models [43, 40, 6, 39, 22] have elevated the quality of visual generation to amazing levels while enabling versatile conditional control. Meanwhile, autoregressive approaches [42, 49, 50, 58] have gradually demonstrated comparable performance and the potential for seamless integration with large language models (LLMs), offering unified framework for multimodal generation. Early diffusion models [12, 48] operated directly in pixel space, but their high computational cost motivated subsequent works [43, 40, 39] to shift the diffusion process into the latent space of Equal contribution. Technical report. Figure 1: Comparison of Different Metrics with Human Judgments. In each case, previous metrics (PSNR, SSIM, LPIPS) demonstrate discrepancies with human assessments, whereas our proposed face similarity and text accuracy effectively reflect the reconstruction quality. The reference image represents the original, while Patch 0 and Patch 1 show reconstruction results from different visual tokenizers. The same regions are cropped from the complete images for visualization. pretrained variational autoencoders (VAEs) [19, 43]. This approach achieves near-optimal trade-off between computational efficiency and detail preservation. In contrast to diffusion-based methods, which decompose image generation into iterative denoising steps, autoregressive models [7, 42] generate visual content sequentially while achieving comparable or even superior [49, 51] visual quality. Their inherent compatibility with LLMs further positions them as promising candidates for unified multimodal generation frameworks [25, 50, 58]. For autoregressive visual generation, VQVAE [52] first introduced discrete latent representations of images, modeling their distribution autoregressively. VQGAN [7] significantly improved reconstruction quality, enabling efficient highresolution image synthesis via transformers or LLMs. Both image generation approaches have been successfully extended to the video generation domain [13, 63, 21, 30]. However, encoding images or videos into latent space typically incurs information loss, particularly due to vector quantization (VQ) from continuous features to discrete tokens. This loss fundamentally constrains the upper bound of generation fidelity. There have been several classical methods for evaluating the quality of reconstructed images. Traditional pixel-level metrics, such as PSNR, measure pixel-wise intensity differences, emphasizing global fidelity but disregarding perceptual relevance. SSIM [56] and FSIM [68] further incorporate luminance, contrast, structural, and edge-texture information, but they are more sensitive to noise. These pixel-level metrics typically focus on only few aspects of image quality and fail to measure similarity in way that aligns with human judgment. To address these limitations, feature-based metrics like FID [11], IS [45], and LPIPS [69] have emerged to assess semantic and distributional consistency of reconstructed images using features from pretrained networks. While these feature-based metrics better approximate human perception compared to pixel-level ones, their reliance on pretrained models makes evaluation unreliable when reconstructed images deviate from the pretraining distribution, as illustrated in Fig 1. Since human judgments of similarity depend on high-order, context-dependent image structures that may not conform to feature distance metrics, we naturally consider certain high-dimensional image features - particularly faces and texts - are more reliant on human assessment than generic natural image characteristics. Compared to other visual contents, the detection and evaluation of faces and text have been extensively studied, resulting in mature toolchains [35, 16]. Moreover, unlike subtle pixel-level variations, text readability and identity preservation are far more perceptually critical to human observers. Pixel-level metrics fail to penalize semantically critical errors (e.g., misaligned strokes in text), while feature-based metrics lack the granularity to assess domain-specific attributes (e.g., facial symmetry or character recognition accuracy). This gap highlights the need for tailored benchmark that integrates task-aware evaluation to complement existing metrics. To address this gap, we propose Visual Tokenizer Benchmark (TokBench). Specifically, we curated 12,398 images and 403 video clips (51,590 frames) rich in faces and text from publicly available datasets, encompassing both natural scenes and document contexts, with balanced scale distributions for both facial and text content. To assess text reconstruction quality, we employ an OCR model to 2 determine whether the reconstructed text remains accurately recognizable, subsequently computing the T-ACC (Text Recognition Accuracy) and T-NED (Text Normalized Edit Distance) metrics. For facial content, we leverage face recognition model to extract facial features and compute the F-Sim (Facial Similarity) metric, quantifying identity preservation. For reconstructed videos, we perform frame-by-frame evaluation and report the average results. These metrics offer intuitive quantification of visual tokenizers ability to retain the most visually challenging content typesareas where current evaluation methods frequently underperform. Leveraging this benchmark, we conducted comprehensive evaluation of existing visual tokenizers and VAEs, demonstrating that the proposed metrics serve as meaningful complement to conventional reconstruction quality standards. In summary, the main contributions of this paper can be categorized into the following points: We reveal that conventional metrics exhibit inconsistencies with human evaluation when assessing the reconstruction quality of human-sensitive content like text and face. We propose TokBench, comprising diverse image dataset rich in faces and text, along with lightweight evaluation pipeline, requiring only 2GB VRAM within 4 minutes. We conduct comprehensive evaluations of existing image tokenizers and VAEs on face and text reconstruction, and further extend this assessment to video tokenizers to explore the upper bounds of visual generation models."
        },
        {
            "title": "2.1 Visual Tokenizers and VAEs",
            "content": "Image Since Latent Diffusion Models [43] achieved promising results by learning visual generation in VAEs latent space, the study of continuous or discrete visual latent spaces has played critical role in visual generation, with increasing exploration focused on tokenizer design. The conventional VAE [4, 19] demonstrated both theoretical and empirical evidence for the advantages of learning data representation encoded to images with learned generator. [52] introduced the Vector Quantised Variational Autoencoder (VQVAE), which learns discrete representations of images and models their distribution autoregressively. VQGAN [7] further enhances the visual reconstruction capability of VQVAE by incorporating GAN loss and demonstrates the potential of autoregressive models in generating high-resolution images. Visual AutoRegressive modeling (VAR) [51] redefined autoregressive learning on images as coarse-to-fine next-scale prediction. UniTok [29] explores the introduction of semantic informations training for discrete visual tokens, enriching semantic information to further improve the understanding and generation capabilities of unified models [50, 58]. Meanwhile, VAVAE [64] and REPA [67] address the high-dimensional challenges of continuous VAE spaces by leveraging semantic space supervision, while TokenBridge [55] and Layton [62] explore the communication and fusion between continuous and discrete tokens. In different vein, MAGVIT-v2 [65], FSQ [34], BSQViT [71] propose lookup-free quantization, presenting an alternative approach that bypasses traditional lookup mechanisms. TiTok [66] performs 2D-to-1D distillation, compressing the number of tokens used to represent the same image. Video Videos contain both spatial and temporal information, making their data volume substantially larger than images. Early video models typically employed image VAEs or VQVAEs [13] directly for generation, but spatial-only modeling often produces jittery outputs. Some approaches [24, 73] attempted 3D VAEs for temporal compression, yet limited latent channels still yielded blurry and unstable results. Recent methods [30, 21, 63] utilizing 3D Causal VAEs have demonstrated superior video encoding performance."
        },
        {
            "title": "2.2 Evaluation of Image Reconstruction",
            "content": "Pixel-level Evaluation Traditional low-level metrics assess reconstruction quality through pixelwise comparisons. Mean Squared Error (MSE) quantifies average squared intensity differences, while Peak Signal-to-Noise Ratio (PSNR) extends this concept logarithmically using the ratio between the maximum possible power of signal and the power of corrupting noise that affects the fidelity of its representation. The structural similarity index measure (SSIM) [56] models human perception through luminance, contrast, and structural comparison, carrying important information about the structure of the objects in the visual scene. Feature Similarity Index (FSIM) [68] measures the 3 similarity between two images based on their low-level features. HDR-VDP [31] specializes in varying luminance conditions, predicting both quality degradation and change visibility. Feature-level Evaluation Previous pixel-level metrics are simple, shallow functions, and fail to account for many nuances of human perception. Advanced feature-level metrics leverage deep learning for semantic evaluation. Learned Perceptual Image Patch Similarity (LPIPS) [69] compares deep features from pretrained networks to better align with human judgment. Fréchet Inception Distance (FID) [11] measures distributional similarity between generated and real images using Inception-v3 features, while Inception Score (IS) [45] evaluates both diversity and recognizability through classifier predictions. These high-level metrics address limitations of pixel-based methods but require careful interpretation when evaluating out-of-distribution samples. Furthermore, these features typically represent high-dimensional global characteristics, small-scale objects such as text and faces have relatively minor influence on these features. As illustrated in Figure 1, previous metrics fail to reflect the reconstruction quality of small-scale objects, which is critical aspect that modern high-quality visual generation models particularly focus on."
        },
        {
            "title": "2.3 Text and Face Datasets",
            "content": "Text Data Texts are representative texture elements in images and unsatisfactory generation quality would seriously affect their readability. Previous datasets for text recognition are focused on cropped text regions, restricting the diversity of text scales and image scenarios. Therefore, we consider collecting data from text spotting datasets [18, 17, 3, 47, 27], which are annotated with the locations and transcriptions of texts. Additionally, some datasets for key information extraction [15, 38] and document-oriented VQA [33, 32] also provide the above annotations. In this work, we collect text data from 8 different text image datasets that vary in fonts, styles, scales and backgrounds, enriching the comprehensiveness of our benchmark. In addition, text spotting in videos has been receiving growing attention recently, and the related datasets [17, 60] are released. They support us to further extend our assessment to video tokenizers. We unify the text representations for consistent evaluation. Face Data For evaluating face generation quality, we considered datasets originally curated for two primary face-related tasks: facial landmark detection and face recognition. Key datasets for facial landmark detection include WFLW [59], 300W [44], and AFLW [20]. For face recognition, frequently utilized datasets include LFW [14], CALFW [72], and CFPW [46], among others. However, most of these datasets were deemed unsuitable for our benchmark since they consist predominantly of single-face portrait images, which do not accurately represent the distribution of faces in in-the-wild scenarios. Consequently, we selected the WFLW dataset, which composed of images captured in naturalistic, unconstrained environments, which often contain multiple faces. For video data, we observe that many video understanding datasets contain abundant scenes and faces. For instance, VideoMME [10], MVBench [23], and MMBench-Video [9] are popular benchmarks for evaluating multimodal video understanding in VLLMs, which include numerous facial segments that can serve as our data pool."
        },
        {
            "title": "3 TokBench",
            "content": "Our goal is to provide novel benchmark specifically designed to evaluate the reconstruction quality of two critical visual elements: texts and human faces in images. To establish this benchmark, we first curate diverse collection of images rich in textual and facial content, systematically categorized by their spatial scales within the images. Then we incorporate specialized evaluation metrics that assess: (1) the legibility of reconstructed text and (2) identity preservation in reconstructed faces. As result, TokBench provides targeted evaluation of discrete or continuous tokenizers capability in reconstructing faces and text, thereby ensuring the upper bound of high-quality visual generation. Furthermore, we curate videos containing rich texts and faces to extend TokBench to assess video tokenizers and VAEs. 4 Figure 2: Statistics and Sample Diversity of TokBench-Image. TokBench features balanced instance-scale distribution with particular emphasis on small-scale face and text instances, presenting significant challenges for existing visual reconstruction approaches. 3."
        },
        {
            "title": "3.1.1 Text Data Curation",
            "content": "Data Collection We first collect text images from eight existing open-source datasets for diversity. Specifically, they include scene text datasets, i.e., ICDAR 2013 [18], IC15 [17], Total-Text [3] and TextOCR [47], and document datasets, i.e., CORD [38], SROIE [15], InfographicVQA [32] and DocVQA [33]. We use their validation or accessible test set to build our benchmark. For datasets that are not divided into training and test sets, we sample from them. These datasets provide word-level annotations that contain both the position and transcription for each text instance, allowing us to perform consistent evaluations. Next, we uniformly use the horizontal bounding box {xt i, ht i} to represent the the i-th text regions. , wt i, yt Difficulty Rating We consider the relative scale of texts as the major factor distinguishing the reconstruction difficulty of the evaluated data. Due to the large variation of scales and character lengths of texts, we focus on the character-level text scale for measurement, which can be approximately derived from annotations. Given text image RHW 3. We assume that characters are 5 Figure 3: Overview of the evaluation process of TokBench. Figure 4: Comparison between reconstructed images (right) and original images (left) under different T-ACC and F-Sim metrics. Higher metric values indicate reconstructed images that more closely resemble the original. (Zoom in for better comparison.) uniformly distributed in the bounding box for most texts. Thus, we approximate the relative scale of the i-th text by normalizing the scale of one character by the maximum length of the image: rt = max(ht i, wt i) max(Hi, Wi) i , (1) where is the number of characters of the i-th text instance. Data Cleaning The feasibility of reconstructing tiny regions should be considered. Meanwhile, the assessment of the reconstruction quality of text images is based on pretrained text recognition model Mt, requiring the predictions of Mt completely accurate on the original images. To ensure the validity of the evaluation, we remove extremely tiny cases and unrecognized instances that would cause ambiguity with the following steps: 1) We assume the minimum pixels to clearly represent character is 5 5. Hence, we remove instances with min(ht, wt) < 5 or rt < 0.005. 2) We filter out the instances containing characters out of the vocabulary of the recognizer and regions that contain only one special symbol, avoiding ambiguous and invalid recognition results. 3) We only keep text instances that can be correctly recognized by Mt from the remaining, guaranteeing the performance degradation in the benchmark is mainly caused by poor reconstructions. Afterward, we keep the images that contain at least one valid text instance. As result, the text set in TokBench consists of 6,000 images and 76,126 valid text instances as shown in Fig. 2. Multiple sources enrich the diversity of text fonts, styles, scales and backgrounds. i, rt Each instances is annotated using {xt i, ˆsi}, where ˆsi is the ground truth transcription. Using rt i, we empirically set 3 different difficulty levels (Small, Medium, and Large). The lowest limit scale in evaluation for the resolution during reconstruction is no less than 5/L, so that the text regions are valid as illustrated in data cleaning. The scale range for each level is in the Appendix. , wt i, ht i, yt"
        },
        {
            "title": "3.2 Evaluation Protocols\nThe overall evaluation pipeline is illustrated in Fig. 3. Text and face images are first reconstructed by\nthe given visual tokenizer T . For the reconstructed text images, each valid text region is cropped\naccording to the ground truth (GT). The cropped regions are fed into a pretrained text recognition\nmodel Mt, obtaining the transcription predictions, which are further evaluated by the corresponding\nGT using T-ACC and T-NED metrics. Similarly, for the face images, each face area is cropped by\nGT. The corresponding areas between the original image and the reconstructed image are encoded\nby a pretrained face recognition model Mf . The encoded feature vectors are measured by F-Sim to\nevaluate the quality of the generated face.",
            "content": "Text We choose the recent PARSeq [2] as the pretrained recognizer for its good balance between accuracy and efficiency. We use the implementation by docTR 2 [35], an OCR toolbox which can be easily installed. Following the metrics in text recognition tasks, the results are evaluated by the text recognition accuracy (T-ACC) and Normalized Edit Distance (T-NED) [70] between the recognition result si and the ground truth ˆsi. Since our goal is to assess the reconstruction quality, we distinguish between uppercase and lowercase letters because their appearances are different, which should be maintained after decent reconstruction. It is regarded as true positive only when the predicted word is exactly the same as GT in our T-ACC metric. Secondly, T-NED gives more fine-grained analysis considering the accuracy of characters, which is formulated as: T-NED = 1 (cid:88) D(si, ˆsi) max(li, ˆli) , (2) where li and ˆli are the numbers of characters of the predicted text and the corresponding GT. is the number of text instances. indicates the Levenshtein distance. Face Just as one cannot paint the Mona Lisa without having seen her, visual tokenizer that fails to accurately reconstruct faces will prevent generative models trained on its latent space from correctly generating corresponding identities. In fact, distorted identities may even mislead the learning process of generative models. To evaluate the fidelity of face reconstruction, we employ the insightface [16] recognition model Mf to measure the similarity between reconstructed and original faces. Specifically, we input the same facial keypoints from annotations with both original and reconstructed images into the recognition model to extract corresponding facial features, then compute the cosine distance between these feature vectors as our face similarity metric (F-Sim). As shown in Figure 4, higher similarity scores indicate better face reconstruction quality, with Table 1 in Supp. demonstrating that high-resolution resizing achieves the highest F-Sim of 1."
        },
        {
            "title": "3.3 Video Data Curation",
            "content": "Text We collect real-world videos from the ICDAR 201315 Text-in-Videos Challenge [17] and the test set of DSTextV2 [60]. Word-level annotations for texts in each frame are given. Similar to the processing procedures illustrated in Sec. 3.1.1, we get rid of invalid text instances while preserving the original video clips. Since the resizing strategy for video tokenizers is based on the short side, we remove instances with min(ht, wt) < 5 or rt < 5min(H,W ) 480max(H,W ) , where 480 is the upper bound of resized short side in our evaluation. Thus, we obtained 15,921 frames that contain 347,468 valid text 2https://github.com/mindee/doctr 7 Type Method Factor Text(%) Face rFID LPIPS PSNR SSIM Discrete Resize TiTok [66] FlexTok [1] VQGAN [7] Chameleon [50] LlamaGen [49] VAR [51] MaskBit [57] TokenFlow [41] O-MAGVIT2 [28] O-MAGVIT2(pretrain) [28] UniTok [29] OmniTokenizer [54] LlamaGen(F8) [49] O-MAGVIT2(F8) [28] Continuous DC-AE [61] VA-VAE [64] SD-XL [40] SD-3.5 [6] FLUX.1-dev [22] T-ACCs T-ACCm T-NEDs T-NEDm F-Sims F-Simm Resolution: 256 256 1 1D 1D 16 16 16 16 16 16 16 16 16 8 8 8 32 16 8 8 8 86.05 0.05 0.55 0.05 0.11 0.16 1.24 0.16 0.28 0.34 0.80 13.53 2.14 4.39 9. 1.42 6.92 6.94 36.26 50.69 93.02 0.09 6.95 1.10 2.87 4.28 15.74 2.54 6.73 7.52 10.58 44.59 20.63 29.41 40.24 16.35 37.04 34.21 67.04 75.91 92.98 3.04 7.80 4.34 4.67 5.41 10.89 4.45 6.41 6.46 9.59 38.73 13.24 19.69 30.82 10.95 25.14 25.03 59.04 70.70 96.53 4.23 21.09 8.22 12.08 14.77 34.19 10.85 20.46 20.99 27.59 65.84 39.14 49.00 59. 33.82 56.32 53.68 80.58 86.42 0.85 0.03 0.06 0.05 0.08 0.07 0.10 0.06 0.07 0.08 0.08 0.15 0.15 0.17 0.23 0.10 0.22 0.18 0.43 0.52 0.93 0.04 0.15 0.10 0.18 0.15 0.23 0.11 0.15 0.19 0.20 0.35 0.37 0.40 0.48 0.26 0.49 0.42 0.70 0.76 5.39 16.25 8.87 12.63 17.32 11.17 8.91 12.53 9.09 8.51 8.39 7.82 9.26 8.65 7. 12.88 6.68 7.60 7.11 6.42 0.06 0.52 0.35 0.36 0.36 0.30 0.24 0.38 0.28 0.27 0.27 0.20 0.30 0.19 0.17 0.23 0.16 0.19 0.13 0.11 27.71 13.54 17.37 17.29 17.81 18.22 19.98 18.07 18.74 19.05 19.33 21.15 15.15 21.50 22.53 20.88 22.94 22.52 24.89 25.50 0.84 0.47 0.57 0.55 0.56 0.58 0.63 0.57 0.59 0.60 0.61 0.66 0.59 0.67 0. 0.65 0.70 0.69 0.75 0.77 Table 1: Performance of discrete and continuous tokenizer on TokBench. and mdenote the average metrics for small-scale instances and all scales, respectively. In this table, we compute traditional metrics such as rFID across both the text set and face set. The Factor denotes the downsampling ratio in latent space, while 1D indicates that images are encoded into one-dimension. instances. The evaluation is conducted per frame, whose pipeline and metrics are consistent with Fig. 3. We only need to recognize text in the cropped regions while ignoring frames containing no valid text, improving the efficiency. Face We first downloaded all videos from the VideoMME [10], MVBench [23], and MMBenchVideo [9] datasets. Each video was sampled at 1 FPS and processed using insightface [16] for face detection, retaining only videos containing faces with the longer edge exceeding 512 pixels. The retained videos then underwent frame-by-frame analysis to select clips meeting two criteria: continuous face presence for at least 3 seconds and detection of more than 3 faces. After filtering out videos where most frames contained only single face, we manually curated the remaining clips based on video quality and content richness, resulting in 328 selected 3-second video segments (25,980 frames total). Within these frames, we performed additional insightface detection to identify faces with confidence scores above 0.5 and scale factors exceeding 0.03, yielding 81,556 valid target faces for frame-by-frame similarity evaluation between reconstructed and original faces."
        },
        {
            "title": "4.1 Evaluation Setting",
            "content": "In this section, we conduct comprehensive comparisons of existing classical continuous or discrete visual tokenizers on the proposed TokBench. We evaluate image reconstruction quality at three resolutions: 256, 512, and 1024. For each resolution, we first center-pad the original image into square and then resize it to the target resolution. After reconstruction within the target resolution, we resize the image back to its original padding size and crop out the padded regions to obtain reconstructed result matching the original resolution. We additionally provide baseline results for each resolution by applying the same padding and resizing process without reconstruction, representing the theoretical upper limit at that resolution. For video reconstruction, we conduct experiments under resolutions at 256 and 480. Notably, we resize the shorter edge of videos to these target lengths while padding both the longer edge and frame count to meet the required dimensions for tokenizers. After reconstruction, we crop out the padded regions and resize the videos back to their original resolutions. The reconstructed videos are then evaluated frame-by-frame using the same protocols as images. Our evaluation framework demonstrates efficiency and lightweight characteristics. After the reconstruction of all images in TokBench, the complete calculation of T-ACC and F-Sim metrics for images requires only 2GB of GPU memory and can be completed within 4 minutes on single RTX 4090 GPU. For evaluating all reconstructed videos, the process requires 2GB of GPU memory and approximately 30 minutes to complete, which can be reduced to 6 minutes through multi-GPU parallel processing. 8 Type Method Factor T-ACC(%) T-NED(%) Small Medium Large Mean Small Medium Large Mean rFID LPIPS PSNR SSIM Discrete Resize TiTok FlexTok VQGAN Chameleon LlamaGen VAR MaskBit TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT Continuous DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev Discrete Resize VQGAN Chameleon LlamaGen VAR TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT2 Continuous DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev Resize VQGAN Chameleon LlamaGen VAR TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT2 Discrete DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev Continuous 1 86.05 1D 0.05 1D 0.55 16 0.05 16 0.11 16 0.16 16 1.24 16 0.16 16 0.28 16 0.34 16 0.80 16 13.53 8 2.14 8 4.39 8 9. 32 16 8 8 8 1.42 6.92 6.94 36.26 50.69 1 92.51 16 0.15 16 0.60 16 0.67 16 3.71 16 1.06 16 1.40 16 3.02 16 17.25 8 6.21 8 12.13 8 20.66 32 5.31 16 12.72 8 16.53 8 56.55 8 70.29 1 95.15 16 0.76 16 3.00 16 3.30 16 9.64 16 4.46 16 5.76 16 9.08 16 26.90 8 14.27 8 25.42 8 35.29 32 15.32 16 25.14 8 31.41 8 74.88 8 83. Resolution: 256 256 98.37 0.17 18.06 3.14 8.19 12.25 39.26 7.26 18.29 20.73 27.76 77.35 51.28 65.97 77.24 42.45 75.96 70.85 92.68 94.89 93.02 0.09 6.95 1.10 2.87 4.28 15.74 2.54 6.73 7.52 10.58 44.59 20.63 29.41 40.24 16.35 37.04 34.21 67.04 75.91 92.98 3.04 7.80 4.34 4.67 5.41 10.89 4.45 6.41 6.46 9.59 38.73 13.24 19.69 30. 10.95 25.14 25.03 59.04 70.70 Resolution: 512 512 98.86 96.52 6.12 17.45 11.55 31.39 15.01 40.43 29.31 63.62 17.40 44.88 21.65 54.04 27.33 62.72 50.10 81.20 42.48 82.91 52.56 88.89 60.56 90.66 79.33 88.43 91.20 97.33 98.02 38.25 53.30 56.86 81.84 87.64 96.25 5.20 7.63 7.76 18.01 10.00 10.79 16.87 44.75 23.44 34.11 46. 20.91 34.80 40.43 75.56 84.67 Resolution: 1024 1024 99.30 41.53 59.33 67.63 75.35 68.57 77.71 79.77 74.29 91.49 94.61 94.84 92.72 93.84 96.29 98.50 98.72 97.61 15.00 23.52 27.19 38.36 29.30 34.74 39.40 49.70 53.48 63.89 69.68 52.14 62.84 67.84 89.71 93. 97.97 7.90 14.46 14.13 29.07 18.62 19.42 28.65 54.36 36.63 50.33 60.97 35.47 48.94 56.60 87.57 92.68 94.65 0.06 2.24 0.12 0.31 0.44 6.72 0.19 1.62 1.49 3.17 42.87 8.46 17.86 34.16 5.16 28.25 24.83 72.18 82.14 98.18 0.76 2.67 3.93 20.59 6.27 9.51 16.25 51.86 38.33 56.66 70.36 30.10 58.73 62.87 91.64 94. 98.39 2.69 8.22 10.62 30.08 14.86 20.74 29.35 47.91 54.67 71.63 78.91 48.36 69.54 75.83 95.76 96.83 97.22 4.07 14.26 5.33 6.65 7.50 26.26 5.72 12.34 12.41 19.15 68.51 30.50 44.56 59.89 24.06 55.30 50.96 85.64 90.67 99.24 8.99 17.82 20.17 49.56 28.39 33.15 44.50 76.28 65.99 77.45 85.41 57.78 78.86 80.83 96.44 97. 99.33 15.03 29.14 33.02 59.16 41.21 47.04 57.43 72.48 76.92 86.45 90.36 71.69 85.17 88.34 98.15 98.69 99.38 5.58 41.21 15.00 24.91 31.40 65.42 22.37 42.64 44.10 54.02 90.27 73.67 82.76 89.19 66.45 88.52 85.03 97.06 97.90 96.53 4.23 21.09 8.22 12.08 14.77 34.19 10.85 20.46 20.99 27.59 65.84 39.14 49.00 59.97 33.82 56.32 53.68 80.58 86. 99.64 98.38 17.32 37.77 26.80 54.95 30.44 63.39 50.00 82.44 35.49 68.07 39.63 74.94 47.28 80.48 71.08 92.20 60.70 92.66 68.99 95.39 76.00 96.00 89.98 95.30 96.40 98.91 99.26 99.77 63.47 77.56 83.57 89.51 84.43 89.63 90.78 87.93 96.53 97.95 98.03 96.89 97.52 98.52 99.44 99.52 56.22 69.65 72.55 90.30 93.86 99.02 28.80 40.39 43.58 59.25 48.09 52.03 58.95 71.59 70.02 78.24 83. 68.02 77.21 81.15 95.05 96.96 5.66 18.41 11.01 15.66 17.60 14.23 10.30 17.05 11.04 10.18 9.83 9.21 12.70 10.51 8.99 14.61 8.22 8.93 8.40 7.19 0.26 6.87 5.61 5.28 3.78 5.14 3.65 3.51 3.27 5.67 2.60 2.34 2.33 2.23 2.00 1.33 0.73 0.18 4.03 2.98 3.35 4.85 3.34 2.47 2.32 4.02 4.13 1.74 2. 1.11 1.59 1.01 0.54 0.41 0.07 0.50 0.31 0.33 0.33 0.29 0.22 0.37 0.26 0.25 0.24 0.19 0.30 0.18 0.16 0.22 0.16 0.18 0.13 0.12 0.01 0.19 0.17 0.15 0.12 0.15 0.13 0.12 0.09 0.20 0.07 0.07 0.09 0.07 0.06 0.03 0.03 0.01 0.11 0.09 0.09 0.10 0.09 0.07 0.07 0.07 0.16 0.04 0. 0.04 0.04 0.03 0.02 0.01 25.40 13.80 17.58 17.17 17.66 18.04 19.74 17.90 18.61 18.85 19.15 20.58 14.73 20.85 21.71 20.42 21.94 21.69 23.46 23.93 29.80 19.24 19.81 20.21 21.27 20.46 21.11 21.54 22.54 15.00 23.46 24.39 23.24 24.07 24.67 26.57 27.25 inf 21.67 22.33 22.74 22.40 23.26 23.86 24.46 24.22 15.30 26.57 27. 27.01 27.31 28.60 29.80 30.55 0.81 0.50 0.61 0.58 0.59 0.61 0.66 0.60 0.62 0.63 0.64 0.68 0.62 0.68 0.71 0.67 0.71 0.70 0.75 0.76 0.91 0.65 0.66 0.68 0.73 0.68 0.71 0.72 0.76 0.67 0.78 0.80 0.76 0.79 0.80 0.85 0.86 0.96 0.74 0.75 0.77 0.79 0.78 0.80 0.81 0.83 0.74 0.86 0. 0.85 0.87 0.88 0.92 0.94 Table 2: Performance of discrete and continuous tokenizer on TokBench text-set."
        },
        {
            "title": "4.2 Main Results",
            "content": "We primarily evaluate performance at 256 resolution since most tokenizers are trained at this scale, with results presented in Table 1. Most discrete tokenizers employ 16 downsampled spatial quantization (F16), while we additionally evaluate 8 downsampled (F8) variants of LlamaGen [49] and Open-MAGVIT2 [28] tokenizers for comparison. At 256 resolution, discrete tokenizers demonstrate notably poor performance in reconstructing small-scale text and faces. UniToks [29] multi-codebook design preserves finer details, achieving significantly superior text reconstruction compared to other tokenizers - even outperforming continuous-space VAEs from VA-VAE [64] and SDXL [40]. For face reconstruction, UniTok also surpasses other F16 tokenizers. The higher-compression 1D tokenizer TiTok [66] yields the weakest results for both text and face reconstruction. Notably, F8 tokenizers consistently outperform their F16 counterparts with identical architectures, while continuous VAEs from SD3.5 [6] and FLUX [22] achieve the highest scores. Compared to conventional metrics (FID [11], LPIPS [69], PSNR, SSIM [56]), improved text reconstruction typically correlates with better scores. However, comparisons between UniTok vs. 9 Figure 5: T-ACC and F-Sim metrics across reconstruction resolutions versus target scales. Smaller scales present greater challenges, and even the best-performing VAE show gap for improvement when compared to the resize upper bound. VA-VAE/SDXL and VAR [51] vs. Open-MAGVIT2 (pretrain) reveal contradictory trends. Moreover, FID and PSNR exhibit limited discriminative power for text/face reconstruction quality, even with substantial T-ACC and F-Sim variations, their metric gaps remain marginal in FID. This evidences existing metrics inadequacy in comprehensively evaluating these specific reconstruction tasks."
        },
        {
            "title": "4.3 Detail Evaluation for Text and Face",
            "content": "Table 2 further presents the evaluation results of various tokenizers on text data across multiple resolutions. First, we observe that most tokenizers achieve progressively better performance with increasing resolution, even without being trained at 1024 resolution. Additionally, more discrepancies emerge between traditional metrics and T-ACC, as evidenced by cases like LlamaGen vs. TokenFlow at 512 resolution, UniTok vs. Open-MAGVIT2 at 1024 resolution, and LlamaGen(F8) vs. OpenMAGVIT2(F8) at 1024 resolution. These findings further validate the complementary value of our proposed metric to existing evaluation methods. Notably, the performance gap between continuous and discrete tokenizers widens significantly with increasing resolution. At 1024 resolution, FLUXs VAE even achieves T-NED comparable to simple resizing. Its worth noting that since many original text images exceed 1024 pixels in size, even resizing cannot achieve 100% T-ACC and T-NED. We further visualize the relationship between T-ACC/F-Sim metrics and instance scales across different resolutions in Figure 5. For small-scale objects, the performance gap between continuous and discrete tokenizers becomes more pronounced at higher resolutions. Detailed evaluations on face data and the difficulty rating are provided in the supplementary materials."
        },
        {
            "title": "4.4 Video Tokenizers and VAEs",
            "content": "We evaluated video reconstruction quality at two standard resolutions (256 and 480) using series of VAEs [37] with identical architectures but varying compression ratios, along with three top-performing 3D causal VAEs from Step-Video [30], Hunyuan-Video [21], and CogVideoX [63], as shown in Table 3. Discrete video tokenizers remain understudied and demonstrate inferior performance. The Cosmos-VAE framework enables clear observation of the performance gap between discrete and continuous tokenizers under same architectural designs, while also revealing the impact of different compression factors. While all 4 8 8 VAEs demonstrate effective video compression and reconstruction capabilities, their performance on small-scale text reconstruction still shows significant gaps compared to the theoretical upper bound (Resize). In contrast, face reconstruction achieves closer results to the theoretical upper bound, likely due to these VAEs extensive facial data exposure"
        },
        {
            "title": "Factor",
            "content": "T-ACC(%) T-NED(%) F-Sim"
        },
        {
            "title": "Continuous",
            "content": "Resize Cosmos-VAE [37] Cosmos-VAE [37] 1 1 1 4 8 8 8 16 16 76.09 1.49 0.02 Cosmos-VAE [37] Hunyuan-Video [21] CogVideoX [63] Cosmos-VAE [37] Step-Video [30] 4 8 8 5.80 4 8 8 26.85 4 8 8 24.80 8 16 16 0.45 8 16 16 17."
        },
        {
            "title": "Continuous",
            "content": "Resize Cosmos-VAE [37] Cosmos-VAE [37] 1 1 1 4 8 8 8 16 16 64.44 0.90 0.02 Cosmos-VAE [37] Hunyuan-Video [21] CogVideoX [63] Cosmos-VAE [37] Step-Video [30] 4 8 8 5.30 4 8 8 28.65 4 8 8 28.02 8 16 16 0.36 8 16 16 20.27 Resolution: 256 96.18 66.12 2.79 78.34 87.47 86.34 48.99 82.41 88.14 30.14 1.06 45.41 61.15 61.21 18.56 53.73 Resolution: 480 96.92 73.71 13. 86.82 91.83 91.71 61.81 87.14 84.04 31.64 4.91 46.31 61.66 61.71 23.86 53.86 85.77 7.76 0.84 15.80 45.55 43.06 3.25 33.16 77.71 6.74 0. 14.99 44.43 43.47 3.20 35.43 92.14 22.82 0.38 52.09 69.12 72.47 6.23 61.40 90.74 20.32 0.90 46.80 64.49 65.41 9.40 54.18 95.79 44.61 3. 68.06 80.54 82.29 24.62 75.67 95.72 41.81 4.20 64.63 77.83 78.24 22.70 71.39 98.32 77.15 12.95 85.63 93.12 92.41 64.08 89.76 98.57 83.53 27. 92.20 95.83 95.60 73.76 93.04 93.29 43.18 5.64 56.50 73.07 72.59 30.65 66.19 90.67 44.03 10.69 57.27 72.70 72.43 33.22 66.62 0.81 0.29 0. 0.47 0.60 0.58 0.21 0.48 0.82 0.44 0.19 0.60 0.69 0.67 0.34 0.60 0.91 0.52 0.13 0.72 0.80 0.78 0.39 0.69 0.89 0.60 0. 0.77 0.82 0.80 0.47 0.73 0.97 0.76 0.25 0.89 0.92 0.91 0.65 0.86 0.95 0.80 0.31 0.90 0.92 0.91 0.71 0.86 0.90 0.52 0. 0.69 0.77 0.76 0.42 0.67 0.89 0.61 0.23 0.76 0.81 0.79 0.51 0.73 Table 3: Performance of video tokenizer on TokBench-Video. The resolution refers specifically to the shorter edge of the videos, while maintaining the original aspect ratio throughout. The categorization into small, medium, and large scales is dynamically adjusted based on resolution. during training. comparison between the 8 16 8 Cosmos-VAE and Step-Video reveals that at identical compression ratios, Step-VAE demonstrates much more superior capabilities. Although its performance remains below that of Hunyuan-Video and CogVideoXs VAEs, it achieves an 8 compression ratio while maintaining highly efficient compression and reconstruction capabilities."
        },
        {
            "title": "4.5 Ablation of Training Data",
            "content": "F8 F8 F16 F16 2.99 3.42 2.96 3.93 0.02 0.09 4.84 5. 12.11 14."
        },
        {
            "title": "Method Data",
            "content": "ImageNet ImageNet+Text ImageNet ImageNet+Text T-ACCs T-ACCm T-NEDs T-NEDm Since different tokenizers typically release weights trained on distinct datasets, we conduct ablation studies on training data to investigate its impact on text and face reconstruction performance. Following LlamaGens [49] training protocol, we augment the ImageNet [5] dataset with an additional 230k text-rich images. We train both F16 and F8 VQGAN models for 400k steps on either the mixed dataset or the original ImageNet alone, then evaluate them on TokBench text set as shown in Table 4. The results demonstrate that incorporating more text data indeed improves T-ACC and T-NED scores, though these improvements prove relatively marginal compared to architectural enhancements. This suggests that while training data influences text and face reconstruction quality, the tokenizer structural design remains the more critical factor. The detailed training data components are provided in the supplementary materials. Table 4: Ablations on Training Data. While augmenting ImageNet with text-rich data yields performance improvements, the gains remain limited, indicating that model architecture design exerts more substantial influence than training data composition. 45.09 47. 16.25 18.05 25.99 27."
        },
        {
            "title": "6 Conclusion\nIn this work, we propose TokBench for evaluating the image and video compression quality of visual\ngenerative models, with targeted assessments of two challenging yet visually sensitive targets, text\nand human faces, which exhibit wide-scale distributions. Unlike conventional metrics focusing on",
            "content": "11 pixel-level or global high-dimensional semantic information, we directly evaluate text readability and identity preservation, which are more perceptually critical to human observers. Leveraging mature toolchains, we achieve efficient and accurate assessment of reconstructed faces and text. Our experiments demonstrate that directly evaluating these elements serves as an effective complement to existing metrics, mitigating potential confusion or misleading results from previous approaches, thereby helping to ensure the upper bound of visual generation quality."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. FlexTok: Resampling images into 1d token sequences of flexible length. arXiv 2025, 2025. [2] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In European conference on computer vision, pages 178196, 2022. [3] Chee-Kheng Chng, Chee Seng Chan, and Cheng-Lin Liu. Total-text: toward orientation robustness in scene text detection. Int. J. Document Anal. Recognit., 23(1):3152, 2020. [4] Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789, 2019. [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255, 2009. [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [8] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [9] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. [10] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in neural information processing systems, volume 33, pages 68406851, 2020. [13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale In International Conference on pretraining for text-to-video generation via transformers. Learning Representations, 2023. [14] Gary Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: database forstudying face recognition in unconstrained environments. In Workshop on faces inReal-LifeImages: detection, alignment, and recognition, 2008. [15] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In International Conference on Document Analysis and Recognition, pages 15161520, 2019. [16] insightface team. insightface. https://github.com/deepinsight/insightface, 2024. [17] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In International Conference on Document Analysis and Recognition, pages 11561160, 2015. 13 [18] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In International Conference on Document Analysis and Recognition, pages 14841493, 2013. [19] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. [20] Martin Koestinger, Paul Wohlhart, Peter Roth, and Horst Bischof. Annotated facial landmarks In IEEE in the wild: large-scale, real-world database for facial landmark localization. international conference on computer vision workshops (ICCV workshops), pages 21442151, 2011. [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [22] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [24] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [25] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. [26] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: In proceedings of the Real-time scene text spotting with adaptive bezier-curve network. IEEE/CVF conference on computer vision and pattern recognition, pages 98099818, 2020. [27] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recognition, 90:337345, 2019. [28] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Openmagvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [29] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [30] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [31] Rafat Mantiuk, Kil Joong Kim, Allan Rempel, and Wolfgang Heidrich. Hdr-vdp-2: calibrated visual metric for visibility and quality predictions in all luminance conditions. ACM Transactions on graphics (TOG), 30(4):114, 2011. [32] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [33] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 22002209, 2021. 14 [34] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. [35] Mindee. doctr: Document text recognition. https://github.com/mindee/doctr, 2021. [36] Nibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan Feng, Dimosthenis Karatzas, Zhenbo Luo, Umapada Pal, Christophe Rigaud, Joseph Chazalon, et al. Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt. In International Conference on Document Analysis and Recognition, volume 1, pages 14541459, 2017. [37] NVIDIA. Cosmos-tokenizer. https://research.nvidia.com/labs/dir/ cosmos-tokenizer/, 2024. [38] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. Cord: consolidated receipt dataset for post-ocr parsing. In Workshop on Document Intelligence at NeurIPS 2019, 2019. [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [41] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. [42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 88218831, 2021. [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [44] Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results. Image and vision computing, 47:318, 2016. [45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [46] Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M. Patel, Rama Chellappa, and David W. Jacobs. Frontal to profile face verification in the wild. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 19, 2016. [47] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88028812, 2021. [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [50] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 15 [51] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [53] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. [54] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. In Advances in Neural Information Processing Systems, volume 37, pages 2828128295, 2024. [55] Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging continuous and discrete tokens for autoregressive visual generation. arXiv preprint arXiv:2503.16430, 2025. [56] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600612, 2004. [57] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and LiangChieh Chen. Maskbit: Embedding-free image generation via bit tokens. Transactions on Machine Learning Research, 2024. [58] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. [59] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou. Look at boundary: boundary-aware face alignment algorithm. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21292138, 2018. [60] Weijia Wu, Yiming Zhang, Yefei He, Luoming Zhang, Zhenyu Lou, Hong Zhou, and Xiang Bai. Dstext v2: comprehensive video text spotting dataset for dense and small text. Pattern Recognition, 149:110177, 2024. [61] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [62] Qingsong Xie, Zhao Zhang, Zhe Huang, Yanhao Zhang, Haonan Lu, and Zhenyu Yang. Layton: Latent consistency tokenizer for 1024-pixel image reconstruction and generation by 256 tokens. arXiv preprint arXiv:2503.08377, 2025. [63] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [64] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [65] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [66] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940128966, 2024. [67] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In International Conference on Learning Representations, 2025. 16 [68] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: feature similarity index for image quality assessment. IEEE Trans. Image Process., 20(8):23782386, 2011. [69] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [70] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In International Conference on Document Analysis and Recognition, pages 15771581, 2019. [71] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. [72] Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age LFW: database for studying cross-age face recognition in unconstrained environments. CoRR, abs/1708.08197, 2017. [73] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "A Evaluation Setting",
            "content": "A.1 Tokenizer Selection In this section, we detail the tokenizers used in our evaluation. For continuous-space compression VAEs, we employed VA-VAE [64] along with VAEs from SDXL [40], SD3.5 [6], and FLUX [22] obtained from their HuggingFace models. For DC-AE, we employ the 32 downsampling version with 32 latent dimensions as used in SANA [61]. For discrete VQVAEs and other discrete modeling approaches, we adopted the ImageNet-trained VQGAN [7] model with downsampling factor of f=16 and codebook dimensionality of 16,384 as our baseline. For LlamaGen [49], we utilized both F16 and F8 model variants. For VAR [51], we selected the largest VAR-d36 model with 2.3B parameters. The TiTok [66] implementation used the TiTok-L-32 tokenizer, representing each image with 32 tokens. For Open-MAGVIT2 [28], we evaluated both F16 and F8 models trained on ImageNet, along with an F16 model pretrained on 100M data featuring codebook size of 262,144. For MaskBit [57], we utilize the 12-bit variant. OmniTokenizer [54] is implemented using the recommended imagenet_k600 version, while FlexTok [1] adopts the version trained on the DFN dataset [8]. A.2 Difficulty Rating"
        },
        {
            "title": "Medium",
            "content": "Type Cat. Res. 0.04 1.00 0.03 1.00 0.02 1.00 0.03 0.04 0.02 0.03 0.01 0.02 256 0.02 0.03 512 0.01 0.02 1024 0.005 0.01 As mentioned in Section 3, we classified different target instances into three difficulty levels based on their scales. For text reconstruction tasks, we theoretically assume that at least 5 5 pixels are required to represent single character. Based on this lower bound, we filtered out targets that are theoretically unrepresentable at each reconstruction resolution. For instance, at 256 resolution, the minimum character scale equals 5 256 0.02, so text instances with scales smaller than 0.02 are excluded from evaluation in this setting. As shown in the Table 6, we determined the scale lower bound for each resolution following this rule, and categorized all targets into small, medium, and large scales according to the distribution curve in Figure 5. For face evaluation, through visualization and performance analysis of Resize upper bound, we set 25 pixels as the minimum representation for recognizable faces. Based on this lower bound, we define minimum evaluable face scales for different resolutions, for instance, at 256 resolution, the 256 0.10 0.20 512 0.05 0.10 1024 0.02 0. 256 0.01 0.02 480 0.005 0.01 0.20 0.30 0.10 0.20 0.05 0.10 0.30 1.00 0.20 1.00 0.10 1.00 256 0.05 0.10 480 0.02 0.05 Table 6: Difficulty Rating 0.02 0.03 0.01 0. 0.10 0.20 0.05 0.10 0.20 1.00 0.10 1.00 0.03 1.00 0.02 1."
        },
        {
            "title": "Text",
            "content": ""
        },
        {
            "title": "Factor",
            "content": "Similarity"
        },
        {
            "title": "Small Medium Large Mean",
            "content": "Resolution: 256 256 rFID LPIPS PSNR SSIM"
        },
        {
            "title": "Discrete",
            "content": "Resize TiTok FlexTok VQGAN Chameleon LlamaGen VAR MaskBit TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT"
        },
        {
            "title": "Continuous",
            "content": "DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev"
        },
        {
            "title": "Discrete",
            "content": "Resize VQGAN Chameleon LlamaGen VAR TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT"
        },
        {
            "title": "Continuous",
            "content": "DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev"
        },
        {
            "title": "Discrete",
            "content": "Resize VQGAN Chameleon LlamaGen VAR TokenFlow O-MAGVIT2 O-MAGVIT2(pretrain) UniTok OmniTokenizer LlamaGen O-MAGVIT"
        },
        {
            "title": "Continuous",
            "content": "DC-AE VA-VAE SD-XL SD-3.5 FLUX.1-dev 1 1D 1D 16 16 16 16 16 16 16 16 16 8 8 8 32 16 8 8 8 1 16 16 16 16 16 16 16 16 8 8 8 32 16 8 8 8 1 16 16 16 16 16 16 16 16 8 8 8 32 16 8 8 8 0.85 0.03 0.06 0.05 0.08 0.07 0.10 0.06 0.07 0.08 0.08 0.15 0.15 0.17 0.23 0.10 0.22 0.18 0.43 0.52 0.97 0.03 0.12 0.08 0.15 0.11 0.20 0.09 0.13 0.15 0.16 0.32 0.34 0.38 0.48 0.21 0.48 0.40 0.76 0.83 0.98 0.05 0.25 0.17 0.30 0.26 0.41 0.19 0.26 0.34 0.35 0.58 0.61 0.66 0. 0.45 0.76 0.69 0.92 0.95 Resolution: 512 512 0.95 0.08 0.13 0.11 0.14 0.11 0.13 0.13 0.22 0.24 0.28 0.35 0.16 0.31 0.29 0.61 0.71 0.99 0.11 0.21 0.17 0.24 0.16 0.22 0.22 0.36 0.45 0.49 0.58 0.29 0.54 0.51 0.84 0. 1.00 0.37 0.50 0.48 0.57 0.45 0.58 0.57 0.74 0.80 0.83 0.88 0.71 0.87 0.87 0.98 0.98 Resolution: 1024 1024 1.0 0.10 0.19 0.15 0.23 0.14 0.20 0.21 0.33 0.40 0.49 0.57 0.27 0.49 0.50 0.86 0.92 1.0 0.19 0.30 0.27 0.34 0.26 0.34 0.34 0.50 0.60 0.66 0. 0.45 0.68 0.69 0.94 0.97 1.0 0.47 0.58 0.57 0.62 0.55 0.66 0.65 0.76 0.82 0.87 0.91 0.78 0.89 0.91 0.99 0.99 0.93 0.04 0.15 0.10 0.18 0.15 0.23 0.11 0.15 0.19 0.20 0.35 0.37 0.40 0.48 0.26 0.49 0.42 0.70 0.76 0.98 0.19 0.28 0.25 0.32 0.24 0.31 0.31 0.44 0.50 0.53 0. 0.39 0.57 0.55 0.81 0.86 1.0 0.25 0.36 0.33 0.40 0.31 0.40 0.40 0.53 0.61 0.67 0.74 0.50 0.69 0.70 0.93 0.96 7.83 23.11 13.54 18.08 25.87 15.30 13.11 15.92 13.43 12.91 12.92 11.25 12.06 12.01 11.47 17.58 9.26 11.19 9.91 9.32 0.08 7.33 6.62 5.28 4.59 6.48 4.67 4.55 3.95 5.11 2.73 2. 2.81 2.41 2.46 1.20 0.71 0.01 4.27 3.63 3.45 6.21 4.63 3.62 3.48 3.78 4.63 2.11 2.11 1.44 2.38 1.25 0.42 0.24 0.05 0.53 0.38 0.38 0.39 0.32 0.25 0.39 0.30 0.29 0.29 0.21 0.31 0.20 0.18 0.25 0.16 0.20 0.13 0.11 0.00 0.23 0.22 0.18 0.15 0.19 0.16 0.16 0.11 0.20 0.08 0. 0.11 0.07 0.07 0.03 0.02 0.00 0.13 0.11 0.10 0.12 0.10 0.09 0.09 0.07 0.15 0.04 0.05 0.05 0.04 0.03 0.01 0.01 29.83 13.31 17.18 17.39 17.94 18.38 20.20 18.23 18.85 19.24 19.49 21.66 15.53 22.09 23.27 21.30 23.85 23.29 26.20 26.94 37.34 20.42 20.98 21.41 22.16 21.39 22.40 22.66 24.34 15.93 25.49 26. 25.08 26.84 27.14 30.06 31.06 inf 23.98 24.54 24.77 23.67 25.00 26.01 26.12 26.54 16.00 28.89 29.92 29.48 30.69 31.39 33.07 33.61 0.87 0.43 0.54 0.52 0.53 0.55 0.61 0.55 0.56 0.58 0.59 0.65 0.56 0.66 0.69 0.62 0.70 0.68 0.75 0.78 0.97 0.61 0.61 0.65 0.69 0.64 0.67 0.68 0.74 0.63 0.77 0. 0.73 0.79 0.79 0.87 0.90 1.00 0.72 0.73 0.76 0.77 0.76 0.79 0.79 0.84 0.71 0.87 0.89 0.84 0.88 0.89 0.96 0.97 Table 5: Performance of discrete and continuous tokenizer on TokBench face-set. minimum valid face scale is approximately 25 256 0.1. For video evaluation, given that most videos follow 16:9 aspect ratio and we resize the shorter edge to specified dimensions according to common evaluation standards, resulting in longer edges around 500 pixels, we adopted more lenient rating strategy compared to image-level evaluation to accommodate these pre-processing differences."
        },
        {
            "title": "B Detailed Comparison on Face Set",
            "content": "Table 5 presents comprehensive evaluation of various tokenizers on the face set across multiple resolutions. First, most tokenizers achieve better performance as resolution increases. Since most face images do not exceed 1024 resolution, resizing to 1024 preserves nearly identical facial details, resulting in the highest possible similarity score of 1. At this resolution, both SD3.5 and FLUX VAEs achieve near-perfect performance (close to 1), while discrete VQVAEs only reach maximum similarity of 0.5 for small-scale faces. This indicates significant performance gap between discrete 18 and continuous compression methods for small-scale objects, even at higher resolutions. Furthermore, results degrade substantially at lower resolutions, demonstrating that facial features require higher resolutions to maintain quality."
        },
        {
            "title": "C More Visualization",
            "content": "Tables 6 and 7 present qualitative comparisons of reconstruction results from different methods at 256 and 1024 resolutions respectively. At 256 resolution, most discrete tokenizers fail to accurately reconstruct text and faces, while the high-compression DC-AE also performs poorly. In contrast, SD3.5 and FLUX VAEs demonstrate significantly better visual quality. At 1024 resolution, both VAEs and low-compression (F8) discrete tokenizers achieve satisfactory results, though F8 Open-MAGVIT2 exhibits noticeable color distortion, and F16 discrete tokenizers still struggle with small-scale objects."
        },
        {
            "title": "D Ablation Setting",
            "content": "In our ablation study examining the impact of text-rich training data augmentation. Following LlamaGen [49], we train VQGANs of F16 and F8 across two datasets. Our baseline implementation uses the ImageNet [5] training set, for the ablation we supplement with 230,000 text-rich images sourced from the training sets of Synth150K [26], ICDAR 2017 MLT [36], Total-Text [3], TextOCR [47], CTW1500 [27] and COCO-Text [53]. The additional text images deviate from the evaluated data. Here, we only need the image rich in texts for training and no annotation is required. To ensure fair comparison, both training are executed for 400,000 iterations under identical conditions. 19 Figure 6: Visualization results of text and face reconstruction performance for different methods at 256 resolution. (Zoom in for better comparison.) 20 Figure 7: Visualization results of text and face reconstruction performance for different methods at 1024 resolution. (Zoom in for better comparison.)"
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology"
    ]
}