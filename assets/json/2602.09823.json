{
    "paper_title": "Covo-Audio Technical Report",
    "authors": [
        "Wenfu Wang",
        "Chenxing Li",
        "Liqiang Zhang",
        "Yiyang Zhao",
        "Yuxiang Zou",
        "Hanzhao Li",
        "Mingyu Cui",
        "Hao Zhang",
        "Kun Wei",
        "Le Xu",
        "Zikang Huang",
        "Jiajun Xu",
        "Jiliang Hu",
        "Xiang He",
        "Zeyu Xie",
        "Jiawen Kang",
        "Youjun Chen",
        "Meng Yu",
        "Dong Yu",
        "Rilin Chen",
        "Linlin Di",
        "Shulin Feng",
        "Na Hu",
        "Yang Liu",
        "Bang Wang",
        "Shan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 3 2 8 9 0 . 2 0 6 2 : r Covo-Audio Technical Report Covo-Audio Technical Report"
        },
        {
            "title": "Abstract",
            "content": "End-to-end large audio language models (LALMs) have emerged as promising paradigm for unifying speech processing and language intelligence, yet achieving strong semantic reasoning alongside high-quality audio interaction remains challenging. In this work, we present Covo-Audio, 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speechtext comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligencespeaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest scalable path toward more capable and versatile LALMs. We will open-source Covo-Audio-Chat model and the inference pipeline soon."
        },
        {
            "title": "Introduction",
            "content": "Speech interaction, as the most natural and efficient form of human communication, is driving artificial intelligence toward more human-like conversational systems. In this context, the ideal conversational agent must harmonize intelligence (possessing deep linguistic understanding and reasoning), naturalness (preserving paralinguistic and emotional cues), and efficiency (enabling low-latency, fluent full-duplex voice interaction). However, current approaches often force compromises among these goals. Traditional speech interaction systems predominantly rely on cascaded architectures, which combine independent modules for Automatic Speech Recognition (ASR), Large Language Model (LLM), and Text-to-Speech (TTS). While this modular paradigm offers interpretability and controllability, it suffers from the inherent issues such as information loss and error propagation, hindering real conversational experience. Recent large audio language models (LALMs) such as Qwen2.5-Omni (Xu et al., 2025a) and Qwen3-Omni (Xu et al., 2025b) adopt the Thinker-Talker architecture, where an intermediate textual reasoning step (the Thinker) precedes acoustic token prediction (the Talker). While this improves 1 Covo-Audio Technical Report Figure 1: An Overview of Comprehensive Performance Comparison. textual intelligence preservation, it sacrifices end-to-end speech instruction following abilities and direct conversational controllability. Additionally, handling full-duplex dynamics in such sequential generation paradigm is more challenging. Human voice interaction derives its richness from dynamic blend of flexibility and diversity. End-toend LALM, aiming to map raw audio input directly to audio output within single, unified model, represents promising frontier. Pioneering works like GPT-4o (Hurst et al., 2024), GLM-4-Voice (Zeng et al., 2024) and Step-Audio 2 (Wu et al., 2025a) have demonstrated the feasibility of this approach by augmenting LLMs with discrete audio tokens, showcasing the profound potential that enables lowlatency, contextually consistent, and emotionally empathetic interaction. Architecturally in terms of decoding text-speech tokens, end-to-end models can be categorized into generating either interleaved streams (Wu et al., 2025a; Xiaomi, 2025) or parallel streams (Ding et al., 2025; Chen et al., 2025b; Team, 2025). We adopt the former as our backbone text-speech decoding design since it is more adherent to the oracle LLM philosophy. However, common pain point accompanied with this unified approach is the deep intelligence-speaker coupling problem when developing production-level application, which brings challenges in data preparation and deteriorates flexible voice customization. Building chat bot combining virtues of fascinating voice and high-intelligence is rather laborious, since it is required to gather much high-quality dialogue data for the desired speaker1. In this work, to mitigate this issue, we propose novel intelligence-speaker decoupling technique, allowing us to build conversational capabilities directly from genuine TTS data, thereby avoiding the process of constructing lots of elaborate dialogue data. Another emerging research direction is end-to-end full-duplex LALMs, which facilitate human-like interaction with low latency and complex behaviors inherent to natural conversations. Representative works are Moshi (DÃ©fossez et al., 2024) and Voila (Shi et al., 2025), which typically employ syn1Commonly, we utilize advanced TTS engines to synthesize dialogue data, which inevitably lowers the upper bound of voice expressiveness. 2 Covo-Audio Technical Report chronous dual-stream scheme, where both input and output streams are in discrete token sequences, to achieve full-duplex. However, they still require word-level text-speech alignment to generate coherent speech. OmniFlatten (Zhang et al., 2025c) adapts text LLM backbone into robust dialogue model through curated multi-stage post-training. In this work, we put full-duplex interaction, without requirements of fine-grained text-speech alignment, directly into the pre-training phase to acquire robust full-duplex conversational capabilities via large-scale pre-training. Moreover, differing from other full-duplex models, we adopt hybrid dual-stream scheme (i.e., continuous input stream and discrete output stream) in accordance with our half-duplex paradigm, which provides more efficient and lossless way to perceive user expression and intention. To facilitate these capabilities in an end-to-end paradigm, we present Covo-Audio, compact LALM that achieves the fundamental comprehensive advantages of intelligence, naturalness, and efficiency that voice conversational assistant should be empowered. Rather than focusing on single task or setting, we demonstrate how pre-training and post-training strategies can endow an end-to-end LALM with robust audio perception, reasoning, emotional empathy, while economical flexible voice interaction capabilities. We conduct comprehensive evaluations of Covo-Audio across broad range of tasks, including speechtext cross-modal alignments, speech understanding, audio question answering, and both half-duplex (Covo-Audio-Chat) and full-duplex (Covo-Audio-Chat-FD) speech-to-speech spoken dialogue. The results show that Covo-Audio achieves state-of-the-art (SOTA) or competitive performance among models of comparable scale. Our primary contributions are summarized as follows: Hierarchical Tri-modal Speech-Text Interleaving: framework designed to achieve deep alignment and fusion across modalities and scales. The Tri-modal aspect integrates continuous acoustic features, discrete speech tokens, and natural language text within unified sequence, effectively bridging the gap between high-fidelity prosodic nuances and robust semantic structures. Hierarchical strategy aims to ensure structural coherence: phrase-level interleaving facilitates fine-grained alignment between acoustic fragments and lexical units, while sentence-level interleaving preserves the global semantic integrity and prosodic flow of long-form utterances. This dual-scale approach ensures the model captures precise acoustic nuances while maintaining macro-level coherence, allowing it to conform naturally to the LLMs generative patterns. Mitigating Intelligence-Speaker Coupling: We propose intelligence-speaker decoupling technique that decouples speaker from dialogue intelligence via multi-speaker training, then develop contextual adaptation method to transfer and share high-quality TTS voices. By reformatting TTS recordings into pseudo-conversations with masked text loss, we preserve reasoning abilities while achieving highfidelity voice naturalness, enabling versatile and personalized interaction in more economical way. Native Full-Duplex Voice Interaction: We evolve Covo-Audio-Chat into Covo-Audio-Chat-FD, variant with low-latency, fluent full-duplex capability. In more native way, we put full-duplex interaction directly into the pre-training phase to acquire full-duplex conversation capabilities via largescale pre-training. Through large-scale pre-training and elaborate lightweight supervised fine-tuning, it supports robust real-time conversational dynamics, including smooth turn-taking, pause handling, user interruption (barge-in), and backchanneling, while maintaining competitive performance with the half-duplex model. Comprehensive State-of-the-Art Performance: Covo-Audio delivers consistently strong and competitive results across core speech and audio tasks, including speech understanding, audio question answering, and both half-duplex and full-duplex speech-to-speech dialogue, despite its compact 7Bparameter footprint. It substantially outperforms existing models of comparable size and, notably, even matches the performance of significantly larger systems on multiple key metrics. This demonstrates that careful co-design of architecture, multi-stage pre-training, and alignment-focused post-training enCovo-Audio Technical Report Figure 2: An Overview of Covo-Audio. ables high-fidelity and robust speech interaction without parameter bloat, providing practical solution for efficient deployment."
        },
        {
            "title": "2.1 Architecture",
            "content": "Covo-Audio employs well-designed end-to-end architecture designed for seamless cross-modal interaction, as illustrated in Figure 2. The model consists of four primary components: an audio encoder, robust LLM backbone, specialized speech tokenizer, and high-fidelity speech decoder. We utilize the pre-trained Whisper-large-v3 (Radford et al., 2023) as our primary audio encoder. Whisper stands out for its exceptional robustness to background noise and varied accents, having been trained on massive, weakly supervised multilingual dataset, and also guaranteeing versatile, generalpurpose ability of performing speech understanding. The output frame rate of the audio encoder is 50 Hz. An audio adapter is employed to bridge the audio encoder and the LLM. This adapter consists of three downsampling modules, each of which integrates two linear layers and convolution layer with downsampling rate of 2. Through this architecture, the adapter effectively reduces the frame rate from 50 Hz to 6.25 Hz. To facilitate audio generation via the LLM, we developed specialized speech tokenizer based on the WavLM-large (Chen et al., 2022), augmented with single Vector Quantization (VQ) layer (Van Den Oord et al., 2017). The tokenizer utilizes codebook size of 16,384, producing discrete audio tokens at rate of 25 Hz to ensure high-compression yet information-rich representation. To achieve acoustic-semantic alignment, the tokenizer is pre-trained on massive multilingual dataset. The training objective is guided by multi-task loss function, which incorporates ASR loss for semantic grounding, TTS reconstruction loss for acoustic fidelity, and specialized pitch loss to preserve the essential prosodic and intonational nuances of human speech. The speech decoder employs two-stage hierarchical decoding framework to reconstruct high-fidelity 4 Covo-Audio Technical Report Table 1: List of pre-training tasks. ğ‘¥ğ‘¦ denotes ğ‘¥ is query and ğ‘¦ response, ğ‘¥ğ‘¦ denotes paired queries or responses. ğ‘ğ‘, ğ‘ğ‘‘, ğ‘¡ denote continuous audio, discrete audio, and text, respectively. The trained tokens are rounded for ease of presentation."
        },
        {
            "title": "Task",
            "content": "Stage"
        },
        {
            "title": "ASR",
            "content": "Formula ğ‘ğ‘ğ‘¡ Stage 2 Adapter+LLM ASR TTS Audio-only Speech Continuation Speech-Text Interleave Text-only Full-Duplex ğ‘ğ‘ğ‘¡ ğ‘¡ğ‘ğ‘‘ ğ‘ğ‘ğ‘ğ‘‘, ğ‘ğ‘‘ğ‘ğ‘‘ ğ‘ğ‘ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘ğ‘‘, ğ‘ğ‘ğ‘¡ğ‘ğ‘, ğ‘ğ‘‘ğ‘¡ğ‘ğ‘‘, ğ‘ğ‘ğ‘¡ğ‘ğ‘‘ ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘ğ‘‘"
        },
        {
            "title": "Trained Tokens",
            "content": "30B 80B 160B 240B 160B 360B 180B 800B 5B audio from discrete tokens (Zhang et al., 2025a). Flow-Matching (FM) (Du et al., 2024) based decoder first transforms the discrete speech tokens into continuous acoustic-rich latent representations. Subsequently, these latents are fed into BigVGAN (Lee et al., 2022) based vocoder to reconstruct 24K waveforms. Covo-Audio is built upon the Qwen2.5-7B-Base (Yang et al., 2024a), which we adapt for end-to-end speech interaction by extending audio encoder for perception, and by expanding its original text vocabulary with discrete audio tokens for generation. The architecture is designed to natively process interleaved inputs, consisting of continuous acoustic features (processed via an audio encoder and adapter) and textual tokens, while generating unified sequence of text and audio tokens as output. For multi-turn conversational scenarios, we implement recursive context-filling strategy where the continuous audio features from the users input, along with the models generated text and audio tokens from previous turns, are prefixed to the current input as historical context."
        },
        {
            "title": "2.2 Pre-training",
            "content": "Covo-Audio is initialized from Qwen2.5-7B-Base and undergoes rigorous two-stage pre-training pipeline, processing cumulative total of 2T tokens. This process is specifically engineered to fuse speech and text modalities within the models latent space, evolving it from text-centric LLM into unified multi-modal engine capable of seamless cross-modal understanding and generation Stage 1: Modality Bridging via ASR Alignment. This stage aims to establish functional bridge between the audio encoder and the LLM, achieving initial semantic alignment between speech and text modalities. In this phase, we keep both the audio encoder and the LLM backbone frozen, focusing exclusively on optimizing the audio adapter. This alignment is supervised using massive 200,000hour multilingual ASR dataset, which forces the adapter to map continuous acoustic features into the LLMs latent space effectively. The training is conducted over 50k steps, and employs cosine learning rate scheduler with peak learning rate of 1 104 and 40k steps for warm-up, which ensures stable convergence. By the end of this stage, the model develops robust foundational ability to understand spoken language, providing well-aligned multimodal representation for subsequent pre-training. Stage 2: Speech-Text Modality Fusion. This stage is specifically designed to achieve superior cross-modal alignment between the speech and text domains, moving beyond basic feature mapping to deep semantic and acoustic synchronization. During this phase, both the audio adapter and the LLM backbone are jointly unfrozen and optimized to refine the models unified representation space. 5 Covo-Audio Technical Report Table 2: List of post-training tasks. Task General Intelligence Spoken Dialogue Speech Understanding Speech Generation Audio Understanding Ratio 0.4 0.3 0.1 0.1 0.1 Formula ğ‘¡ğ‘¡, ğ‘¡ğ‘ğ‘‘, ğ‘ğ‘ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘ğ‘‘ ğ‘ğ‘ğ‘¡ğ‘ğ‘‘ ğ‘ğ‘ğ‘¡, ğ‘ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ ğ‘ğ‘ğ‘¡, ğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘ğ‘‘ ğ‘¡ğ‘ğ‘‘ ğ‘¡ğ‘ğ‘ğ‘¡ We implement diverse multi-task training objective that includes ASR, TTS, Audio-only modeling, Speech Continuation, and Speech-Text Interleave, alongside Text-only to preserve foundational linguistic knowledge. Additionally, to acquire robust full-duplex interaction capabilities, we also include Full-Duplex pre-training in this stage. The entire pre-training tasks are listed in Table 1. cornerstone of our pre-training phase is the Hierarchical Tri-modal Speech-text Interleaving strategy. Our framework acknowledges the dual nature of speech modeling: continuous acoustic representations ğ‘ğ‘ derived from the Whisper encoder and discrete acoustic tokens ğ‘ğ‘‘ generated by the speech tokenizer. Consequently, our pre-training objective is not merely to align speech with text, but to achieve three-way fusion between continuous features, discrete tokens, and linguistic text within unified latent space. In detail, we designed several tri-modal sequence structures. Specifically, we employ two primary patterns: (1) Sequential Interleaving (ğ‘ğ‘ğ‘¡ğ‘ğ‘‘): Arranging continuous features (ğ‘ğ‘), text (ğ‘¡), and discrete tokens (ğ‘ğ‘‘) in progressive chain; (2) Parallel Integration (ğ‘ğ‘ğ‘¡ğ‘ğ‘‘): Aligning continuous features with coupled text-discrete unit. These paradigms ensure the model learns to map high-fidelity acoustic nuances (continuous) to robust semantic units (discrete and text) simultaneously. For Hierarchical Interleaving, previous interleaving methods, such as those utilized in GLM-4-Voice, primarily operate at the word or character level. While effective for local modality alignment, such granular interleaving often sacrifices the semantic integrity of long-form utterances and is highly sensitive to tokenization or forced-alignment errors. To overcome these challenges, our approach introduces multi-scale interleaving mechanism that incorporates utterance-level paradigms alongside phrase-level segments. By interleaving longer, semantically complete speech and text fragments, we preserve the global context and natural flow of language. The training is conducted using sequence length of 8192 and cosine learning rate scheduler with peak learning rate of 3 105, spanning total of 500k steps and 20k steps for warm-up. Our pretraining data consists of approximately 8 million hours of diverse audio and speech, and text corpus encompassing multiple domains, totaling 3T tokens. This comprehensive approach ensures that the model achieves high-fidelity modal consistency, enabling seamless and accurate transitions between spoken and written language."
        },
        {
            "title": "2.3 Speech-to-Speech Spoken Dialogue",
            "content": "To incentivize strong intelligence, natural spoken expression, and empathetic interaction capabilities, we design collaborative data construction and training strategy that integrates text instruction tuning, audio adaptation, and spoken dialogue alignment, ultimately obtaining the spoken chat model, Covo-Audio-Chat. Given the fundamental importance of the core intelligence and logical reasoning capabilities of Covo-Audio-Chat, it is essential to incorporate high-quality instruction data used in text LLM domain. Specifically, to transfer intelligence to the audio modality, we design multiple crossmodal tasks including text-to-text (T2T), text-to-audio (T2A), audio-to-text (A2T) and audio-to-audio (A2A) tasks. Note that in the A2A task, the query typically consists of audio input only, while the answer includes both text response and the corresponding audio in an interleaved format. The involved 6 Covo-Audio Technical Report Figure 3: Data Utilization in Intelligence-Speaker Decoupling Technique. post-training tasks are listed in Table 2. Intelligence We start with 10M internal text instruction data for T2T training. This corpus is balanced across diverse domains, including commonsense reasoning, mathematics, coding, and specialized knowledge. randomly sampled subset is then converted into parallel speech data via highfidelity neural TTS system, enabling T2A, A2T and A2A training. Speaker diversity is ensured by randomly sampling voice styles from thousands of distinct speech prompts, followed by WER-based filtering to maintain speech quality. This parallel textaudio construction paradigm facilitates effective knowledge transfer from T2T to audio-related tasks, and finally mitigates intelligence degradation when extending the model to the speech modality. Additionally, we apply Kullback-Leibler (KL) based onpolicy model distillation during T2T training, where stronger text-based LLM provides top-20 logits as soft targets to prevent intelligence degradation. Colloquialism Beyond intelligence, spoken dialogue requires colloquialism and conciseness. To enhance colloquial expressiveness, we select moderate-scale set of dialogue samples suitable for spoken interaction and rewrite assistant responses into conversational style using text-based LLM before TTS synthesis. Distinct system prompts are employed during training to explicitly differentiate colloquial and formal response styles, enabling flexible spoken expression without sacrificing general capabilities. Empathy To model empathetic speech interaction, we further construct emotion-aware dialogue datasets spanning seven emotion categories: joy, anger, sadness, fear, disgust, depression, and surprise. Each dialogue pairs explicit user emotional cues with assistant responses exhibiting active listening and context-sensitive support. These dialogues are synthesized with emotionally aligned speech, allowing Covo-Audio-Chat to jointly learn semantic and acoustic nuances for empathetic response generation. The training is conducted using sequence length of 8192 and cosine learning rate scheduler with peak learning rate of 1 105, spanning total of 50k steps and 1,000 steps for warm-up. The involved tasks with respective training ratio are listed in Table 2. This recipe endows Covo-Audio-Chat with strong logical reasoning and natural spoken dialogue capabilities. 7 Covo-Audio Technical Report 2. Intelligence-Speaker Decoupling Technique Building versatile spoken dialogue model is rather challenging. There are two critical issues in endto-end scenarios. (1) Due to the scarcity of authentic high-quality2 spoken dialogue data, advanced TTS systems are often used to synthesize such data based on text dialogue datasets, not only inducing prohibitive cost for flexible voice customization but also restricting the overall naturalness to the limitation of the TTS system. (2) On the other hand, high-quality TTS training data is relatively easy to collect but hard to exploit as it is originally not designed for dialogue-style interaction. common practice is to construct conversational context manually based on such data, but often resulting in poor quality and degradation of overall dialogue performance. To address these two issues, during dialogue data preparation, we first randomly generated thousands of speakers to decouple speaker characteristics from intelligence, ensuring consistent intellectual levels across different speakers. However, when applying speakers trained solely for TTS tasks to dialogue tasks, we observed significant decline in performance and reduced naturalness compared to TTS tasks. This drop is essentially due to the substantial format difference between TTS and dialogue tasks, making it difficult for models to directly transfer speakers across tasks. To further address this, we transformed high-quality TTS data into dialogue format via constructing appropriate pseudo-context, and incorporated it into training. To preserve dialogue intelligence effectively, we excluded the text response portion from the loss calculation. Experiments show that this approach successfully transfers the naturalness of TTS speakers while maintaining intellectual levels comparable to those of dialogue speaker. data utilization schematic diagram is shown in Figure 3. This technique enables versatile and personalized interaction while preserving high-fidelity voice naturalness with only lightweight TTS data needed."
        },
        {
            "title": "2.5 Full-Duplex Voice Interaction",
            "content": "Covo-Audio-Chat still relies on turn-based conversation mechanism, where the model is in either listening or speaking state. In contrast, our full-duplex model, Covo-Audio-Chat-FD, is endowed with simultaneous dual-stream communication that mirrors natural human interaction. To facilitate these merits, we made some modifications to the half-duplex model. First, the audio encoder is reformatted into chunk streaming manner to enable real-time user stream input. Then the user stream and the model stream are chunk-interleaved in an 1:4 ratio3 to allow the model to listen and speak simultaneously. comprehensive comparison between Covo-Audio-Chat and Covo-Audio-Chat-FD is illustrated in Figure 4. Additionally, we transformed the spoken dialogue datasets used in half-duplex training into dual-channel format and added conversational features such as barge-ins and backchanneling in proper positions according to semantic dynamics. To acquire robust full-duplex conversational capabilities, we performed large-scale pre-training, following the approach described in Section 2.2. Then we built Covo-Audio-Chat-FD directly from the pretrained foundation using both half-duplex and full-duplex datasets. The training is accomplished in one single step rather than using multi-stage progressive training scheme as done in (Zhang et al., 2025c). We found this simple-yet-effective approach can yield more competitive results. In addition to intelligence preservation, full-duplex training also allows the model to master smooth turn-taking, to wisely handle user barge-ins and backchanneling behaviors. Covo-Audio-Chat-FD works as follows. At start, during the users turn, the model continuously outputs THINK tokens, indicating listening-only state while it waits to respond appropriately. Upon the end of the users turn, the model immediately outputs SHIFT token to enter its speaking turn. Then four speech tokens are delivered sequentially preceded by text token to guide semantic stability within each chunk, indicated as the listening-while-speaking state shown in Figure 4(b). During this period, the 2That means of high intelligence and human-like. 3Since input audio features and output audio tokens are 6.25 Hz and 25 Hz respectively. Each chunk stands for 0.16s. 8 Covo-Audio Technical Report Figure 4: Comparative Overview of Covo-Audio-Chat and Covo-Audio-Chat-FD. model also receives the user stream simultaneously to facilitate flexible barge-ins or backchanneling. Once an interruption signal is detected, or otherwise it would end normally, the model immediately outputs BREAK token and terminates speaking, switching to listening."
        },
        {
            "title": "2.6 Speech Understanding",
            "content": "Speech conveys diverse information, including both linguistic content and paralinguistic attributes such as speaker identity, emotional state, and demographic characteristics. To enable fluent speech interaction, we define the scope of speech understanding along two dimensions: (1) content understanding, which focuses on what is said, and (2) attribute recognition, which captures how and by whom it is spoken. Content understanding encompasses tasks including automatic speech recognition (ASR), speech-to-text translation (S2TT), and spoken language identification. Attribute recognition covers gender recognition, speaker counting, and emotion recognition. To equip the model with these capabilities, we construct multi-task question-answering data from largescale speech corpora, including ASR datasets (LibriSpeech (Panayotov et al., 2015) etc.), speaker-rich datasets (VoxCeleb (Nagrani et al., 2017), CN-Celeb (Fan et al., 2020)), emotion-rich datasets (MELD (Poria et al., 2019)), multi-attribute datasets (Common Voice (Ardila et al., 2020)), and speech rea9 Covo-Audio Technical Report soning subsets from AudioSkills (Goel et al., 2025a). During data construction, we address two key challenges: (1) cross-modal alignment, which enables consistent speech understanding across input modalities, and (2) data bias mitigation, which ensures equitable model performance across demographic groups. Cross-Modal Data Construction We conduct cross-modal alignment to ensure the model can perceive paralinguistic attributes regardless of the input modalitywhether the query is presented as text or speech. Specifically, we design unified data construction framework centered on speech recordings with specific attributes. Let ğ‘ğ‘† denote the source speech containing target attributes, ğ‘¡ğ‘„ and ğ‘ğ‘„ the text query and speech query, and ğ‘¡ ğ‘… and ğ‘ğ‘… the text response and speech response. We construct three complementary modality configurations: Text-to-Text QA: (ğ‘ğ‘†, ğ‘¡ğ‘„) ğ‘¡ ğ‘… Speech-to-Text QA: (ğ‘ğ‘†, ğ‘ğ‘„) ğ‘¡ ğ‘… Speech-to-Speech QA: (ğ‘ğ‘†, ğ‘ğ‘„) (ğ‘¡ ğ‘…, ğ‘ğ‘…) The construction pipeline proceeds as follows. First, we extract paralinguistic labels from open-source datasets and generate text-form QA pairs using predefined templates. Second, we synthesize corresponding speech queries and responses using high-quality TTS systems, ensuring natural prosody and diverse speaker characteristics. Finally, we mix all three modality configurations during training, enabling the model to learn modality-invariant representations for attribute recognition. Bias Mitigation Model bias has been challenge in dialog systems, where models trained on imbalanced data often exhibit disparate performance across demographic groups, resulting in varying accuracy for different age, gender, or language groups. Such bias is particularly detrimental for productionlevel systems. To address this challenge, we employ stratified sampling to maximize demographic balance across paralinguistic attributes. Given the varying difficulty of balancing different attributes in available corpora, we adopt hierarchical strategy with priority: age language gender, ensuring that the hardest-to-balance attributes receive primary consideration."
        },
        {
            "title": "2.7 Audio Understanding",
            "content": "To cultivate robust audio understanding and complex reasoning capabilities, we employ systematic training strategy centered on two distinct categories of data: Foundational Audio Perception Data and Cognitive Reasoning Data. These categories provide complementary signals that concurrently refine the models acoustic grounding and analytical reasoning. Foundational Audio Skill Acquisition The first category focuses on establishing core acoustic-semantic grounding using the AudioSkill dataset (Goel et al., 2025b). This dataset encompasses diverse array of tasks such as event recognition and cross-modal alignment, which are essential for mapping raw acoustic signals into the models latent space. This process ensures that the model possesses the requisite perceptual vocabulary to interpret complex auditory environments. The training runs for 32k steps on the AudioSkill dataset containing 8M audio-text pairs. The model is optimized using AdamW (Loshchilov & Hutter, 2017) with differentiated learning rates: 3 106 for the LLM backbone and 1 105 for the audio adapter, with weight decay of 5 104. We employ linear warmup for 100 steps followed by cosine annealing. Chain-of-Thought (CoT) Reasoning Enhancement The second category focuses on enhancing the models reasoning capabilities for audio understanding tasks. Traditional audio language models often 10 Covo-Audio Technical Report struggle with complex analytical tasks that require multi-step reasoning, such as emotion recognition involving subtle vocal cues, intent inference from prosodic patterns, or causal relationship identification in acoustic events. Chain-of-Thought (CoT) reasoning addresses this limitation by explicitly modeling the intermediate cognitive steps between audio perception and final conclusion, enabling the model to decompose complex audio comprehension into interpretable logical pathways. To incorporate CoT capabilities, we construct reasoning-enhanced dataset by merging and shuffling two complementary sources: AudioMCQ (He et al., 2025) and AF-Think (Goel et al., 2025b), totaling 1M training instances. All samples follow unified format that explicitly separates the reasoning process from the final answer. Since AF-Think originally provides only the reasoning process without explicit answers, we employ DeepSeek V3.1 (DeepSeek-AI, 2024) to extract and format the final answers, ensuring consistency across the entire dataset. We perform full-parameter training for two complete epochs using the same hyperparameters as the foundational stage, except with 1,000 warmup steps to accommodate the longer training duration. This CoT-enhanced training enables the model to not only recognize acoustic patterns but also articulate the reasoning behind its audio understanding decisions. Following supervised fine-tuning, we employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to refine the models audio reasoning capabilities using data from the Audio-Visual Question Answering (AVQA) benchmark (Yang et al., 2022). Inspired by Audio-Thinker (Wu et al., 2025c), which demonstrates the effectiveness of explicit reasoning for audio understanding, we design verifiable composite reward function comprising four complementary components: ğ‘…total = ğ‘…accuracy + ğ‘…format + ğ‘…consistency + ğ‘…thinking (1) Accuracy Reward (ğ‘…accuracy {0, 1}): Measures whether the models final answer matches the ground truth, providing direct supervision for correctness. Format Reward (ğ‘…format {0, 1}): Verifies adherence to the structured output requirement, checking for the presence of both reasoning and answer sections: ğ‘…format = 1 [match(output, <think>.*</think>.*<answer>.*</answer>)] (2) Consistency Reward (ğ‘…consistency {0, 1}): Evaluates logical coherence between the reasoning process and final answer using an auxiliary LLM judge. This ensures that the models explicit reasoning aligns with its conclusions. Thinking Quality Reward (ğ‘…thinking [0, 1]): Assesses the reasoning depth across five dimensions: logical soundness, factual correctness, error identification, language coherence, and conciseness. The score is obtained via LLM-based evaluation in 0.2 increments. This multi-faceted reward structure enables the model to simultaneously optimize for correctness, structural compliance, internal consistency, and reasoning quality, addressing limitations of supervised learning alone in capturing nuanced audio understanding behaviors."
        },
        {
            "title": "3.1 Evaluation on Pre-training",
            "content": "To rigorously assess the multi-modal capabilities of our model, we established comprehensive evaluation suite encompassing six distinct task categories. This framework covers A2A, A2T, and T2T scenarios. Specifically, for A2A tasks, we evaluated the models ability to generate speech responses directly 11 Covo-Audio Technical Report Table 3: Performance comparison of pre-trained model and baselines on speech-text modeling capabilities. Models Params A2A-tSC() A2T-tSC() T2T-tSC() sBLIMP() sWUGGY() Qwen2.5-7B-Base TWIST SpiritLM Slam AlignSLM SIMS Moshi MinMo LLaMA-Omni GLM-4-Voice-Base Step-Audio-2-mini-Base Covo-Audio-Base 7B 7B 7B 358M 7B 7B 7B 7B 7B 9B 7B 7B - 74.1 82.9 82.0 86.8 88.3 83.0 75.4 78.2 82.9 - 83. - - 88.6 - - 94.0 - - - 93.6 95.5 95.7 98. - 98.0 - - 98.0 - - - - 98.5 99.4 - 59.0 58.3 58.8 62.3 59. 58.8 55.4 53.6 - 52.4 61.6 - 73.90 69.00 68.38 - 75.36 - 68.59 73. - 52.70 74.90 Table 4: Performance comparison of pre-trained model and baselines on ASR and TTS tasks. LS clean and LS other represent Librispeech test clean and test other set, respectively. Models Params ASR () Seed-TTS () Aishell-1 LS clean LS other Test-en Test-zh Whisper-large-v3 CosyVoice2 SpiritLM GLM-4-Voice-Base Covo-Audio-Base 1.5B 0.5B 7B 9B 7B 9.31 - - 2.46 1.96 2.50 - 6.00 2.82 1. 4.53 - 11.0 7.66 4.55 - 2.57 - 2.91 2. - 1.45 - 2.10 1.73 from speech input, focusing on dimensions such as creative story continuation (topic StoryCloze (Hassid et al., 2023)), grammatical integrity (sBLIMP (Nguyen et al., 2020)), and structural consistency (sWUGGY (Nguyen et al., 2020)). Based on the tSC data, we also evaluated the models performance on both A2T and T2T tasks. Furthermore, standard benchmarks for ASR and TTS are integrated to provide holistic measure of the models foundational speech processing and synthesis performance. To evaluate the performance of the pre-trained model, we compare it against representative suite of speech language model baselines, categorized into three distinct groups. The first group consists of specialized A2A architectures, including TWIST (Hassid et al., 2023), SpiritLM (Nguyen et al., 2025), Slam (Maimon et al., 2025a), AlignSLM (Lin et al., 2025), and SIMS (Maimon et al., 2025b). These models are specifically engineered and trained for direct A2A tasks. Secondly, we include Moshi, Minimo Chen et al. (2025a) and Llama-Omni2 Fang et al. (2025), the prominent unified speech dialogue model recognized for its comprehensive modeling. Furthermore, we evaluated leading pre-trained foundations for speech-to-speech dialogue, specifically GLM-4-Voice-Base and Step-Audio2-mini-Base. Beyond cross-modal alignment performance, we extended our evaluation to include domain-specific models: Whisper is utilized for ASR benchmarks, while CosyVoice2 Du et al. (2024) served as the baseline for TTS. By benchmarking against this diverse array of modelsranging from specialized S2S, ASR, TTS frameworks to generalized pre-trained speech systemswe aim to demonstrate the capabilities in both high-level semantic reasoning and foundational acoustic generation. Our analysis primarily focused on the efficacy of speech-text modality alignment. The results are 12 Covo-Audio Technical Report shown in Table 3. In the A2A story continuation task, our model achieved performance parity with sota pre-trained and specialized A2A models. It is noteworthy that many specialized A2A baselines, such as SpiritLM and Slam, exhibit signs of overfitting on continuation tasks, as they are often biased toward speech-to-speech mapping at the expense of broader linguistic flexibility. Furthermore, while models like AlignSLM and SIMS benefit from extra reinforcement learning, our results were achieved through pre-training alone. Crucially, our model significantly outperformed all existing baselines in logical reasoning, grammatical accuracy in A2A metrics, and cross-modality consistency across A2T and T2T benchmarks. These results demonstrate that our pre-training objective successfully fused the speech modality with the core cognitive and generative powers of the LLM. This suggests that the model does not merely transcribe audio but deeply extracts semantic information from speech signals to inform its high-level reasoning. Beyond high-level reasoning, we evaluated the models performance on fundamental ASR and TTS tasks. Despite being unified multi-modal system, our model demonstrated competitive proficiency compared to the GLM-4-Voice-Base model, as shown in Table 4. The end-to-end pre-training paradigm we propose facilitates robust bi-directional semantic mapping between speech and text. This synergy ensures that the model preserves fine-grained acoustic details while maintaining the sophisticated linguistic understanding inherent in large-scale language modeling."
        },
        {
            "title": "3.2 Evaluation on Speech-to-Speech Spoken Dialogue",
            "content": "Evaluation on Spoken Dialogue We evaluate the end-to-end spoken dialogue capability of CovoAudio-Chat on URO-Bench (Yan et al., 2025) and VCB Bench (Hu et al., 2025) in an A2A mode. Specifically, we adopt the basic Chinese and English track of URO-Bench to assess the models performance in speech comprehension, speech reasoning, and spoken dialogue. For VCB Bench, our evaluation focuses on the models text-side and speech-side instruction-following ability, internal knowledge retention, and anti-interference capability. The models used for comparison include: GLM-4Voice (Zeng et al., 2024), Freeze-Omni (Wang et al., 2024b), Kimi-Audio (Ding et al., 2025), Qwen2.5Omni (Xu et al., 2025a), Baichuan-Audio-Chat (Li et al., 2025), Step-Audio 2 series (Wu et al., 2025a), MiMo-Audio (Xiaomi, 2025), GPT-4o Audio, Qwen3-Omni (Xu et al., 2025b), and Fun-Audio-Chat (Chen et al., 2025b). The results on the URO-Bench are shown in Table 5. As illustrated, Covo-Audio-Chat demonstrates comprehensive superiority in core tasks, particularly excelling in speech reasoning and spoken dialogue which are critical for practical speech interaction. Specifically, on the Chinese track, Covo-Audio-Chat achieves the highest scores in four reasoning tasks (SQuAD: 77.34, OpenbookQA: 83.60, APE: 68.42, MLC: 80.69) and two spoken dialogue tasks (AlpacaEval: 90.02, Wildchat: 90.41), outperforming all comparative models including the advanced ones like Qwen3-Omni. In speech comprehension, its Repeat (98.17) and HSK5 (88.00) scores are among the top tiers, only slightly lower than Step-Audio 2 mini and MiMo-Audio respectively. On the English track, Covo-Audio-Chat maintains its strength in reasoning, achieving the best performance in Gsm8kEval (85.68). Though not leading in speech comprehension and partial spoken dialogue tasks, its overall performance is consistently competitive. In summary, Covo-Audio-Chat demonstrates robust and balanced performance across both Chinese and English tasks despite at compact 7B-parameter scale, highlighting its strong bilingual spoken dialogue capabilities. This gives it key advantage over many models that lack comprehensive bilingual coverage, and solidifies its overall dominance in core speech-to-speech interaction scenarios. The results on the VCB Bench are shown in Table 6, evaluating speech-to-speech models across three core dimensions: Instruction Following, Knowledge, and Robustness. As illustrated, our proposed Covo-Audio achieves excellent performance in key metrics, demonstrating its superiority in practi13 Covo-Audio Technical Report Table 5: Performance Comparison on the Basic Track of URO Bench. Results marked with () are sourced from (Yan et al., 2025), and those marked with () are from (Wu et al., 2025a). Covo-AudioChat-TTS denotes dialogue voice transferred from TTS speaker. Model GLM-4-Voice Freeze-Omni GPT-4o Audio Kimi-Audio Qwen2.5-Omni Step-Audio 2 Step-Audio-AQAA Step-Audio 2 mini MiMo-Audio Qwen3-Omni Fun-Audio-Chat Covo-Audio-Chat Covo-Audio-Chat-TTS Model GLM-4-Voice Freeze-Omni GPT-4o Audio Kimi-Audio Qwen2.5-Omni Step-Audio 2 Step-Audio-AQAA Step-Audio 2 mini MiMo-Audio Qwen3-Omni Fun-Audio-Chat Covo-Audio-Chat Covo-Audio-Chat-TTS (a) Results on Chinese Basic Track. Understanding() Reasoning() Oral Conversation() Repeat LCSTS HSK5 SQuAD OpenbookQA APE MLC AlpacaEval Claude Wildchat 92.64 4.97 93.50 73.32 19.66 96.16 63.72 99.83 0.44 99.48 97. 98.17 97.68 77.08 71.82 81.60 78.40 73.22 79.16 68.33 81.18 79.61 84.54 80.95 79.89 79.22 69.00 7.66 88.00 - - - - 80.67 92.67 88.67 79. 88.00 91.00 28.75 9.58 42.67 - - - - 64.71 35.73 52.07 51.85 77.34 76.91 56.96 16.40 76.00 66.93 67.70 74.06 47.21 70.02 83.60 73.02 67. 83.60 80.60 15.78 11.75 25.33 - - - - 45.09 61.05 26.67 44.91 68.42 64.21 78.85 47.35 81.33 52.45 61.76 62.99 67.40 74.94 38.16 36.09 76. 80.69 79.31 83.35 67.98 86.40 69.90 70.40 76.23 56.23 74.29 85.90 79.59 80.59 90.02 89.34 82.12 64.89 82.93 81.81 79.71 84.54 31.50 72.58 89.10 78.86 86. 87.18 87.99 84.48 71.28 80.00 - - - - 79.58 84.84 74.54 84.62 90.41 89.28 (b) Results on English Basic Track. Understanding() Reasoning() Oral Conversation() Repeat 90.95 70.89 97.16 90.27 28.36 97.74 93.02 97.81 1.08 97.36 97.18 95.24 92.71 Summary 91.07 78.87 94.13 78.19 80.51 83.90 90. 84.69 87.74 96.16 91.98 87.23 88.64 GaokaoEval 64.47 26.29 72.00 81.63 89.99 90.76 86.80 85.15 71.29 81.30 76.13 89.11 87.24 StoralEval 73.80 57.74 84.27 70.88 72.90 76.38 53. 73.17 79.34 92.27 79.97 82.59 82.32 TruthfulEval 59.28 46.95 82.67 53.90 62.03 65.45 62.30 60.45 57.62 70.70 66.68 67.60 63.67 Gsm8kEval 30.93 2.81 80.00 0.97 63.52 62.83 38. 28.12 45.59 9.45 62.08 85.68 77.78 MLC 57.82 42.56 80.00 43.50 80.04 77.02 71.00 74.01 23.16 50.85 79.66 77.78 74.58 AlpacaEval 80.77 52.23 95.20 63.05 78.16 81.74 71. 77.72 84.92 82.95 89.21 85.73 84.49 CommonEval WildchatEval 63.07 48.70 94.13 51.40 72.40 72.87 75.53 62.63 67.27 75.37 78.83 79.67 79. 78.76 55.80 95.20 66.63 77.92 81.62 68.79 79.66 82.94 85.50 87.91 86.82 87.09 cal speech interaction scenarios. In the Instruction Following task, Covo-Audio-Chat outperforms all competing models in Text-based Instruction Following (TIF: 93.07, TIF-En: 89.94) and Multi-Turn Dialogue (MTD: 87.70), highlighting its strong ability to comprehend and execute both Chinese and English text instructions, as well as maintain coherent and compliant communication in multi-round interaction. Notably, in the Robustness evaluationcritical for real-world applicabilityCovo-AudioChat sets the state-of-the-art across all sub-tasks: Speaker Variation (SV: 88.94), Environmental Noise (EV: 87.13), and Content Variation (CV: 90.37), which underscores its robust adaptability to diverse interfering factors such as accents, background noise, and grammatical deviations. In the Knowledge dimension, Covo-Audio-Chat achieves competitive performance in Mathematical Logic (ML: 79.34), approaching the top-tier level, while its General Knowledge (GK) and Dialogue Comprehension (DC) scores are modest compared to Qwen3-Omnithis reflects deliberate prioritization of core interaction capabilities during model development, and these aspects can be enhanced by integrating specialized knowledge bases and refining contextual modeling. Overall, Covo-Audio-Chat demonstrates outstanding performance in most critical dimensions of speech-to-speech interaction (text instruction following, multi-turn dialogue, and robustness), laying solid foundation for practical applications, with the identified limitations providing clear pathways for further optimization. Evaluation on Empathy We evaluate empathetic interaction on VStyle benchmark (Zhan et al., 2025). Covo-Audio Technical Report Table 6: Performance Comparison on the VCB Bench. Missing results due to API unavailability. Model Instruction Following() Knowledge() Robustness() TIF TIF-En SIF SIF-En MTD GK ML DC SV EV CV GLM-4-Voice Kimi-Audio Qwen2.5-Omni Baichuan-Audio Step-Audio 2 mini MiMo-Audio GPT-4o Audio Qwen3-Omni Fun-Audio-Chat 82.15 77.33 76.75 82.24 76.10 90.08 86.94 90.45 89. 75.52 60.37 60.83 78.60 70.47 81.89 88.80 85.17 89.39 73.18 71.04 54.04 53.67 60.80 56.26 77.98 70.73 78.82 66.94 54.13 42.18 48.27 49.85 42.74 82.90 65.57 75.70 82.56 82.27 85.23 80.96 82.31 86.30 33.59 87.17 85.27 41.79 54.47 48.13 40.73 49.86 48.70 55.81 66.86 46.30 60.18 75.42 73.00 74.05 72.85 81.75 73.45 81.90 73. - 51.96 72.81 51.66 79.46 - 76.74 82.78 79.15 73.64 64.35 77.82 78.05 75.19 83.72 80.34 87.91 88.60 77.51 65.14 74.41 77.62 70.58 85.36 79.92 85.63 83.83 78.60 71.51 78.27 78.71 74.78 89.38 86.51 86.03 85.15 Covo-Audio-Chat 93. 89.94 67.41 60.86 87.70 49.95 79. 64.95 88.94 87.13 90.37 Table 7: Empathy Performance Comparison on the VStyle Benchmark. Results marked with () are sourced from (Zhan et al., 2025), and those marked with () are from (Chen et al., 2025b). Model GPT-4o-Mini GPT-4o Doubao Baichuan-Audio Kimi-Audio Step-Audio Qwen2.5-Omni Fun-Audio-Chat-8B Covo-Audio-Chat Anger() Sadness() Anxiety() Joy() en zh en zh en zh en zh 4.98 4.95 4.89 2.41 3.59 3.95 2.95 3.64 4.86 4.30 4.75 4. 2.11 3.86 3.59 4.64 3.73 4.89 5.00 4.90 5.00 3.43 3.97 4.37 2.73 4.10 4.83 4.52 4.83 4. 2.55 3.86 4.52 4.28 3.93 4.93 4.87 5.00 4.81 2.74 3.65 3.87 3.55 2.90 4.97 4.73 4.67 4. 2.20 3.80 3.20 4.77 4.03 5.00 5.00 4.54 4.94 3.91 3.46 4.29 3.43 3.69 4.69 4.80 4.83 3.51 4.57 4.26 4.91 3. 4.80 4.89 As shown in Table 7, Covo-Audio-Chat demonstrates exceptional performance on the VStyle empathy dataset, which assesses models capability to empathetically respond to and support users across diverse emotional states. Covo-Audio-Chat achieves state-of-the-art (SOTA) results in Mandarin, specifically attaining the highest scores in anger (4.89), sadness (4.93), and anxiety (5.00). In English, the Its model delivers highly competitive performance comparable to leading closed-source models. worth noting that though Covo-Audio-Chat achieves leading objective scores using the LLM-as-aJudge method, this doesnt reflect real voice expressions as the LALM judge (Gemini 2.5 Pro) is likely to prioritize semantic content over the overall quality in speech response. In preliminary subjective test on voice empathy, we found Covo-Audio-Chat still underperforms the top-tier productions like Doubao4, advising it is extremely urgent to establish more equitable evaluation criterion. But overall, especially compared to existing open-sourced models, Covo-Audio-Chat exhibits superior ability to recognize nuanced emotional cues and generate contextually appropriate empathetic responses, validating its effectiveness as supportive conversational agent for real-world emotional interaction scenarios. Please refer to Appendix for case study. Evaluation on Intelligence-Speaker Decoupling Technique We also evaluate Covo-Audio-Chat-TTS, variant that applies TTS voice directly to spoken dialogue scenarios in inference. As illustrated in Table 5, it achieved comparable bilingual dialogue performance to Covo-Audio-Chat, demonstrating the successful voice transfer and sharing from TTS while preserving conversational intelligence via our proposed intelligence-speaker decoupling technique. This approach effectively mitigates the pro4https://www.doubao.com/chat/ Covo-Audio Technical Report Table 8: Full-Duplex Performance Comparison on URO-Bench. (a) Results on English Basic Track Model Understanding() Reasoning() Oral Conversation() Repeat Summary GaokaoEval StoralEval TruthfulEval Gsm8kEval MLC AlpacaEval CommonEval WildchatEval 1.26 Moshi Freeze-Omni 29.96 Covo-Audio-Chat-FD 94.64 38.19 20.11 83. 6.71 0.66 73.82 39.50 25.31 80.80 38.37 44.75 64.51 3.09 1.20 80.47 47.64 18.83 74.76 48.48 52.16 84. 40.60 48.30 75.07 48.18 52.95 88.34 (b) Results on Chinese Basic Track Model Understanding() Reasoning() Oral Conversation() Repeat LCSTS HSK5 SQuAD OpenbookQA APE MLC AlpacaEval Claude Wildchat Freeze-Omni 3.71 Covo-Audio-Chat-FD 98. 42.75 80.72 2.33 83.67 4.58 75.16 4.94 79.37 12.81 70.70 47.13 78. 66.17 84.90 59.94 83.93 70.43 88.34 We found that the GaokaoEval test set contains very long silent pauses between two adjacent vocal fragments, which may easily cause the model to respond premuturely. The early-response leads to clear performance degradation, as compared against the corresponding scores in Table 5. Table 9: Full-Duplex Interaction Performance Metrics (Success Rate (%)). Model Moshi Freeze-Omni Covo-Audio-Chat-FD Turn-taking() Pause Handling() Backchanneling() Interruption() 96.8 99. 99.7 53.2 51.2 97.6 - - - - 93. 96.81 hibitive cost inherent to end-to-end paradigm when developing chat bot of high naturalness, paving the way for flexible voice customization with only lightweight TTS data needed."
        },
        {
            "title": "3.3 Evaluation on Full-Duplex Interaction",
            "content": "We evaluate Covo-Audio-Chat-FD on two dimensions: (1) spoken dialogue capability measured on URO-Bench, and (2) full-duplex interaction behaviors, including turn-taking, pause handling, interruption, and backchanneling (Zhang et al., 2025b). behavior is considered successful if the model properly handles the corresponding full-duplex feature; for example, it is successful turn-taking when the model responds after the user finishes speaking. Following Section 3.2, we measure dialogue performance on URO-Bench using only the first-round response, and additionally report first-round success rates of turn-taking and pause handling. To evaluate user interruption and backchanneling, which examine overlap handling, we inject the corresponding audio events during the models response period by randomly sampling from corpus recorded naturally and diversifiedly. We compare against Moshi and Freeze-Omni under identical autonomous full-duplex settings. As shown in Table 8, Covo-Audio-Chat-FD substantially outperforms Moshi and Freeze-Omni across understanding, reasoning, and oral conversation tasks on both English and Chinese tracks. In particular, the model achieves particularly large margins on reasoning benchmarks, indicating that the full-duplex training does not degrade the models core reasoning intelligence. Compared with Covo-Audio-Chat  (Table 5)  , Covo-Audio-Chat-FD maintains highly comparable spoken dialogue performance with only slight drop. This small gap largely stems from the models occasional early-response behavior: it may interrupt the user prematurely on the point of short pauses within turn, failure mode that correlates directly with its pause-handling success metric, which also points out the critical direction for optimization. 16 Covo-Audio Technical Report Table 9 summarizes the full-duplex interaction metrics. Against existing open-source baselines, CovoAudio-Chat-FD achieves strong results on turn-taking and pause handling dynamics, with especially notable performance in pause handlinga capability that involves correctly disregarding short pauses within users turn. This reflects that Covo-Audio-Chat-FD is more reliable to understand user intentions. Furthermore, the model shows competent performance in managing user interruption (96.81%) and backchanneling (93.89%), guaranteeing its excellent interaction experience in practice."
        },
        {
            "title": "3.4 Evaluation on Speech Understanding",
            "content": "In this section, following the section 2.6 we evaluate the speech understanding capabilities of our model across diverse set of speech-centric tasks. Specifically, we consider automatic speech recognition (ASR), speech-to-text translation (S2TT), and range of paralinguistic understanding tasks, including emotion recognition, speaker gender classification, and age prediction. These tasks jointly cover both linguistic content understanding and non-linguistic acoustic cue modelling, providing comprehensive assessment of the models speech understanding ability. All tasks are formulated under unified framework and trained using SFT on task-specific and mixed-task speech-text data. Such setting reflects realistic speech interaction scenarios, where models are expected to simultaneously recognize spoken content, translate across languages, and infer speaker attributes from raw audio signals. We evaluated the ASR performance of our SFT-based speech understanding model against diverse set of SOTA commercial, closed-source, and open-source audio-language models. The comparison includes GPT-4o Audio (Hurst et al., 2024), Gemini 2.5 Pro (Team et al., 2024a), Voxtral-small (Liu et al., 2025), Seed-ASR (Bai et al., 2024), as well as several recent open multimodal models such as Qwen2.5-Omni (Yang et al., 2024a), Qwen3-Omni-30B (Xu et al., 2025b), LongCat-Flash-Omni (Team, 2025), and Step-Audio 2 (Wu et al., 2025a). All models were evaluated under identical decoding and data conditions, and performance was measured using word error rate (WER), where lower values indicate better recognition accuracy. As shown in Table 10, our Covo-Audio achieves competitive ASR performance across all evaluation settings, with an average WER of 4.71. We further evaluate the S2TT capability of our model on the CoVoST2 (Wang et al., 2020) benchmark on English and Chinese languages. CoVoST2 is widely used multilingual speech translation dataset that covers diverse sourcetarget language pairs and presents significant challenges in terms of acoustic variability and cross-lingual semantic alignment. The results on CoVoST2 are summarized in Table 10. Covo-Audio achieves the best performance on en-zh translation, surpassing the strong baselines like Step-Audio 2 mini, while also delivering competitive results against other models on the more challenging zh-en direction. Following the description in Section 2.6, we evaluate the paralinguistic ability on AIR-Bench (Yang et al., 2024b) benchmark, comprehensive benchmark covering multiple speech paralinguistic tasks, including emotion recognition, speaker gender classification, and age prediction. As shown in Table 10, our model achieves the best overall performance across all evaluated paralinguistic tasks on AIRBench. The most significant improvements are observed on emotion recognition, where our model achieves absolute gains of over 10-25 percentage points compared to prior methods. This suggests that dialogue-level supervised fine-tuning effectively enhances the models sensitivity to acoustic patterns beyond lexical content, making it particularly well-suited for real-world spoken interaction scenarios, where linguistic and paralinguistic cues can be jointly modeled. 17 Table 10: Performance Comparison on Speech Understanding Benchmarks. Covo-Audio Technical Report Datasets Model ASR Benchmarking () WenetSpeech (Net/Meeting) LibriSpeech (Clean/Other) Fleurs (EN/ZH) Aishell-1 Avg CoVoST2 () en-zh zh-en AIR-Bench () Spoken Language ID Speaker Gender Emotion Speaker Age Speaker Number Avg GPT-4o Audio Seed-ASR Gemini 2.5 Pro Voxtral-small Qwen2.5-Omni Qwen3-Omni-30B LongCat-Flash-Omni Step-Audio 2 Covo-Audio GPT-4o Audio MinMo Qwen2.5-Omni Qwen3-Omni Step-Audio 2 mini LongCat-Flash-Omni Covo-Audio GPT-4o Audio Qwen2.5-7B Qwen2-Audio Qwen2.5-Omni Qwen3-Omni Gemini 2.5 Pro Covo-Audio - Performance 15.30/32.27 1.39/3.75 3.32/2.44 3.52 8.85 4.55/ 5.69 1.58/2.84 3.40/2.69 0.68 3.07 14.43/13.47 2.89/3.56 2.94/2.71 3.11 6.15 20.33/26.08 1.56/3.30 3.77/7.98 11.26 5.91/ 7.65 1.74/3.45 3.77/2.54 1.17 3.74 4.69/ 5.89 1.22/2.48 2.72/2.20 0.64 2.83 6.09/ 6.69 1.57/4.07 5.02/3.99 0.63 4.00 4.67/ 4.75 1.17/2.42 3.03/2.68 0.63 2.76 7.23/ 8.33 1.45/3.21 5.08/6.64 1.07 4.71 40.20 19.01 46.70 26.00 41.40 29.40 48.72 21.50 49.12 29.47 47.23 27.32 49.84 26.77 96.80 21.90 59.50 41.10 30.00 49.86 88.00 28.36 50.80 36.40 50.89 77.80 64.70 48.20 23.10 30.20 48.80 69.80 89.70 89.80 54.90 44.80 71.98 97.70 91.11 62.20 36.90 99.10 90.70 60.70 34.10 71.15 99.30 91.52 72.30 70.90 70.30 80.86 - - - - Table 11: Performance Comparison on MMAU-v05.15.25 and MMSU. Results for other methods are directly sourced from the official MMAU Leaderboard and MMSU paper. Model Param MMAU-v05.15.25() MMSU() Sound Music Speech Avg Perception Reasoning Avg GPT-4o Audio GPT-4o mini Audio Gemini 2.0 Flash Gemini 2.5 Flash Gemini 2.5 Pro Qwen2-Audio-Instruct Audio Flamingo 2 Audio Flamingo 3 Kimi-Audio MiMo-Audio Qwen2.5-Omni Step-Audio 2 mini Step-Audio 2 - - - - - 8.4B 3.2B 7B 7B 7B 7B 7B 32B 64.56 50.75 71.17 73.27 75.08 67.27 71.47 79.58 75.68 81.68 78.10 79.30 84.04 56.29 39.22 65.27 65.57 68.26 56.29 70.96 73.95 66.77 74.25 65.90 68.44 73.56 66.67 69.07 75.08 76.58 71.47 55.26 44.74 66.37 62.16 68.17 70.60 68.16 75. 62.50 53.00 70.50 71.80 71.60 59.60 62.40 73.30 68.20 74.70 71.50 72.73 77.58 Covo-Audio 7B 78.68 76. 71.17 75.30 39.67 - 40.83 - - 39.02 - - 43.52 - 42.50 - - 58.95 71.96 - 47.83 - - 68.90 - - 76.03 - 79.83 - - 74.83 56.38 - 51.03 - - 53.27 - 61.40 59.28 - 60.57 - - 66."
        },
        {
            "title": "3.5 Evaluation on Audio Understanding",
            "content": "To comprehensively assess the effectiveness of our three-stage training paradigm, we conduct systematic evaluations on two established audio understanding benchmarks: MMAU (Sakshi et al., 2024) and MMSU (Wang et al., 2025a). We compare our models performance against several state-of-the-art systems, including both closed-source models (GPT-4o Audio (Hurst et al., 2024), GPT-4o mini Audio (Hurst et al., 2024), Gemini 2.0 Flash, Gemini 2.5 Flash (Team et al., 2024a), Gemini 2.5 Pro (Team et al., 2024a), and Gemini 2.5 Flash Lite (Team et al., 2024a)) and open-source models (Qwen2-Audio18 Covo-Audio Technical Report Instruct (Chu et al., 2024), Audio Flamingo 2 (Ghosh et al., 2025) Audio Flamingo 3 (Goel et al., 2025b), Kimi-Audio (Ding et al., 2025), Qwen2.5-Omni (Xu et al., 2025a), and Step-Audio 2 (Wu et al., 2025a)). The comparative analysis demonstrates that our three-stage training approach achieves competitive or superior performance across multiple evaluation metrics, validating both our architectural choices and training methodology. The progressive improvements observed through each training stage underscore the effectiveness of our systematic capability development strategy. As shown in Table 11, Covo-Audio achieves highly competitive results across both benchmarks. On MMAU-v05.15.25 (Test-mini subset), our model obtains an average score of 75.30%, ranking second among all open-source models and surpassing most closed-source systems. Notably, while Step-Audio 2 achieves the best overall performance (77.58%), it employs significantly larger 30B parameter architecture. Among models of comparable scale (7B-parameter), Covo-Audio achieves the best performance, demonstrating superior parameter efficiency. Our model particularly excels in music understanding, achieving the highest score of 76.05% among all models, and maintains strong performance in speech understanding. On the MMSU benchmark, our model demonstrates even more pronounced advantages, achieving 66.64% average accuracythe highest score among all evaluated systems, including both closed-source and open-source models. This substantial margin over the second-best model (Audio Flamingo 3 at 61.40%) validates the effectiveness of our three-stage training approach. Specifically, our model excels in perception tasks with remarkable score of 58.95%, significantly outperforming all other models including Kimi-Audio (43.52%) and Gemini 2.0 Flash (40.83%). While our reasoning performance (74.83%) is competitive, Qwen2.5-Omni achieves the highest reasoning score (79.83%), suggesting potential directions for future improvements in our Chain-of-Thought reasoning enhancement stage. The progressive improvements observed through each training stage underscore the effectiveness of our systematic capability development strategy."
        },
        {
            "title": "4 Related Work",
            "content": "Large Language Models The evolution of LLMs has been characterized by transition from massivescale scaling to more nuanced focus on data quality and architectural efficiency. The GPT series (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023) and PaLM (Chowdhery et al., 2023) established the foundational scaling laws, demonstrating that emergent reasoning capabilities arise from increasing parameter counts and data volumes. This trajectory was democratized by the Llama family (Touvron et al., 2023; Dubey et al., 2024), which popularized efficient Transformer variants and set the stage for subsequent open-source innovations. Based on the LLM paradigm, there are many derived research directions. Phi series (Abdin et al., 2024) and SmolLM (Allal et al., 2025) shift toward efficient and lightweight models. They have demonstrated that by training on textbook-quality data, models with significantly fewer parameters can rival much larger counterparts in specialized reasoning and coding tasks. The Qwen series (Team et al., 2024b; Yang et al., 2024a; 2025) represent significant leap in the all-in-one model ecosystem, offering an extensive range of scales from 0.5B to 72B parameters alongside specialized Mixture-of-Experts (MoE) variants. These models delivering sota performance in multilingual understanding, complex coding, and long-context reasoning. Kimi has pushed the boundaries of long-context understanding, optimizing the Transformers memory footprint to handle millions of tokens effectively. The DeepSeek-R1 (Guo et al., 2025) represents breakthrough in open-source reasoning models, utilizing Reinforcement Learning (RL) to cultivate autonomous Chain-of-Thought (CoT) capabilities. Multimodal Large Language Models The success of LLMs has catalyzed significant research interest in extending linguistic intelligence to multimodal perceptions. The initial surge in this field focused 19 Covo-Audio Technical Report on Vision-Language Models (VLMs). Early works like BLIP-2 (Li et al., 2023) and Flamingo (Alayrac et al., 2022) introduced specialized bridge architectures, such as the Q-Former or Perceiver Resampler, to align frozen visual encoders with frozen LLM backbones. This approach was further democratized by LLaVA (Liu et al., 2023), which utilized simple linear projection layer to map visual features into the word embedding space, demonstrating that instruction-tuning on high-quality image-text pairs could yield impressive multimodal reasoning capabilities. Recent iterations like LLaVA-NeXT (Liu et al., 2024) have further enhanced these capabilities, supporting high-resolution images, videos, and multi-image reasoning. Subsequent advancements, such as the Qwen-VL (Wang et al., 2024a; Bai et al., 2025b;a) and InternVL (Chen et al., 2024; Zhu et al., 2025; Wang et al., 2025c) series, have focused on scaling both the resolution of visual inputs and the quality of interleaved datasets. These models often utilize Vision-Tower + Adapter + LLM pipeline, where the visual encoder is progressively unfrozen to achieve deeper cross-modal synergy. Both X-Omni (Geng et al., 2025) and Skywork UniPic (Wang et al., 2025b) represent significant trend in multimodal research by focusing on the unification of understanding and generation within single framework. X-Omni achieves this by aligning diverse modalities into shared latent space to facilitate both complex reasoning and high-fidelity synthesis, while Skywork UniPic employs unified perception-generation architecture that treats various visual tasks as consistent sequence modeling problem. more recent paradigm shift is the move toward Native Multimodal Models, which abandon the modular plug-in approach in favor of unified architecture trained on multiple modalities from the outset. Unlike vision-language models that append visual features to text-centric model, native models like Gemini (Team et al., 2023; Comanici et al., 2025) and GPT-4o (Hurst et al., 2024) are designed with an inherently multimodal vocabulary and shared hidden spaces. This allows the model to process and generate different modalities (text, image, and audio) using the same set of parameters, facilitating true cross-modal fluidly. Other notable efforts include Emu (Cui et al., 2025b) and Chameleon (Team, 2024), which utilize discrete tokenization for all modalities, treating images and audio as foreign languages within unified Transformer. Large Audio Language Models Early efforts in speech-text integration primarily focused on aligning speech encoders with existing textual LLMs to leverage their pre-trained reasoning capabilities. Llama-Omni (Fang et al., 2024) and Mini-Omni (Xie & Wu, 2024) demonstrated that by using speech adapter and non-autoregressive speech decoder, Llama-based backbones could achieve high-quality speech-to-speech interaction with low latency. Freeze-Omni (Wang et al., 2024b) utilizes frozen LLM backbone to preserve original textual intelligence while plugging in streaming speech encoders and decoders. By keeping the core model parameters fixed, Freeze-Omni avoids the catastrophic forgetting of textual knowledge often induced by multimodal fine-tuning. Thinker-Talker architecture separates the cognitive reasoning process from the acoustic synthesis process to ensure logical consistency. Qwen-Omni series (Xu et al., 2025a;b) adopt this paradigm, utilizing Thinker module to handle semantic deliberation before or during the Talker modules speech generation. Qwen2.5-Omni introduced framework to synchronize video, audio, and text, while Qwen3-Omni further optimizes this with Mixture-of-Experts (MoE) design. This allows the model to process over 40 minutes of audio and deliver real-time, multilingual streaming responses. To overcome the information bottleneck of text-mediated dialogue, several models have moved toward native E2E architectures that treat audio as primary modality. Moshi (DÃ©fossez et al., 2024) pioneered this shift by predicting audio and text streams simultaneously using multi-stream TransIn the domestic research field, GLM-4-Voice (Zeng former, enabling true full-duplex interaction. et al., 2024), Baichuan-Audio (Li et al., 2025) and Baichuan-Omni (Li et al., 2024) models can handle Chinese-English bilingual conversations with high-fidelity emotional resonance, treating audio tokens 20 Covo-Audio Technical Report as direct extension of the LLMs vocabulary. VITA-Audio (Long et al., 2025) generates audio output during the first forward pass via lightweight Multiple Cross-modal Token Prediction (MCTP) module, significantly reducing first-token latency. Meanwhile, Step-Audio2 (Wu et al., 2025a) and MiMo-Audio (Xiaomi, 2025) introduce unified model that processes interleaved streams of text and discretized audio without task-specific heads, further narrowing the modality gap and achieving performance improvement in speech reasoning tasks. Recently, Step-Audio-R1 (Tian et al., 2025) prioritizes reasoning-centric reinforcement learning (RL), allowing it to perceive non-vocal audio events and maintain robust conversational logic through chain-of-thought (CoT) reasoning. Full-Duplex Spoken Dialogue LALM Full-duplex refers to the interaction that can listen and speak simultaneously to support advanced conversational dynamics such as turn-taking, interruption and backchanneling. Full-duplex LALMs are typically built upon half-duplex LALMs, and can be divided into two categories by implementation. (1) Cascaded full-duplex LALMs typically rely on external modules to aid judgment of whether listening or speaking based on the current dialogue state (Wang et al., 2024b; Ma et al., 2025). (2) End-to-end full-duplex LALMs synchronize speech input and speech output in dual-stream mechanism and learn conversational dynamics in unified model (DÃ©fossez et al., 2024; Zhang et al., 2025c; Cui et al., 2025a; Wu et al., 2025b)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this report, we present Covo-Audio, 7B-parameter end-to-end large audio language model designed to bridge the gap between complex audio processing and language intelligence. Covo-Audio accepts continuous audio input and delivers audio output directly in single unified model. Our extensive evaluations demonstrate that Covo-Audio achieves comprehensive state-of-the-art or competitive performance in speech-text modeling, spoken dialogue, speech understanding, audio understanding and full-duplex voice interaction via meticulously curated large-scale pretraining and post-training, rivaling or exceeding existing models of comparable scale. Specifically, our pretrained foundation model exhibits strong speech-text comprehension capabilities on various benchmarks including topic StoryCloze, sBLIMP, sWUGGY and ASR/TTS, outperforming the open-source representative GLM4-Voice-Base. These results suggest the model does not merely transcribe audio but deeply extracts semantic information for high-level reasoning. We further evaluate Covo-Audio-Chat on URO-Bench and VCB Bench. The compelling results against mainstream LALMs verify its excellent spoken dialogue performance spanning understanding, reasoning, oral conversation, instruction following, and robustness. Through empathetic evaluations against mainstream spoken dialogue LALMs including advanced business models, Covo-Audio-Chat exhibits competitive ability to recognize nuanced emotional cues and generate contextually appropriate empathetic responses, validating its effectiveness as supportive conversational agent for real-world emotional interaction scenarios. Our proposed fullduplex variant, Covo-Audio-Chat-FD, substantially outperforms other open-source LALMs like Moshi and Freeze-Omni on both two dimensions including spoken dialogue capabilities and full-duplex interaction behaviors, meanwhile maintaining highly comparable spoken dialogue performance to CovoAudio-Chat. Additionally, results on Speech Understanding and Audio Understanding tasks across multiple benchmarks highlight the strong general audio question answering capabilities of Covo-Audio. To mitigate the prohibitive cost inherent to end-to-end LALM paradigm when developing chat bot of high naturalness, we propose novel intelligence-speaker decoupling strategy, which effectively decouples voice rendering from dialogue intelligence. Experiments demonstrate that Covo-Audio-Chat-TTS achieves comparable bilingual dialogue performance to Covo-Audio-Chat. This paves the way for flexible voice customization with only lightweight TTS data needed. 21 Covo-Audio Technical Report The success of Covo-Audio underscores the potential of 7B-scale models to maintain high semantic reasoning capabilities while mastering intricate audio intelligence through optimized pre-training and post-training strategies, fostering various audio and speech processing tasks. Next, We will explore scaling-up to further elicit the potential of Covo-Audio."
        },
        {
            "title": "6 Contributions",
            "content": "Project Supervisor: Dong Yu Project Leader: Wenfu Wang, Meng Yu Core Contributors: Chenxing Li, Liqiang Zhang, Yiyang Zhao, Yuxiang Zou, Hanzhao Li, Mingyu Cui, Hao Zhang, Kun Wei, Le Xu, Zikang Huang, Jiajun Xu, Jiliang Hu, Xiang He, Zeyu Xie, Jiawen Kang, Youjun Chen Contributors: Rilin Chen, Linlin Di, Shulin Feng, Na Hu, Yang Liu, Bang Wang, Shan Yang"
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel MartÃ­n BlÃ¡zquez, Guilherme Penedo, Lewis Tunstall, AndrÃ©s Marafioti, Hynek KydlÃ­Äek, AgustÃ­n Piqueres LajarÃ­n, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massivelymultilingual speech corpus. In Proceedings of the twelfth language resources and evaluation conference, pp. 42184222, 2020. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, et al. Qwen3-VL technical report. arXiv preprint arXiv:2511.21631, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025b. Corresponding author. <wenfuwang@tencent.com> List in alphabet order. 22 Covo-Audio Technical Report Ye Bai, Jingping Chen, Jitong Chen, Wei Chen, Zhuo Chen, Chuang Ding, Linhao Dong, Qianqian Dong, Yujiao Du, Kepan Gao, et al. Seed-ASR: Understanding diverse speech and contexts with llm-based speech recognition. arXiv preprint arXiv:2407.04675, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, et al. Minmo: multimodal large language model for seamless voice interaction. arXiv preprint arXiv:2501.06282, 2025a. Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, et al. Fun-Audio-Chat technical report. arXiv preprint arXiv:2512.20156, 2025b. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):1505 1518, 2022. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, and Irwin King. Think before you talk: Enhancing meaningful dialogue generation in full-duplex speech language models with planning-inspired text guidance. arXiv preprint arXiv:2508.07375, 2025a. Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, et al. Emu3.5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025b. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-Audio technical report. arXiv preprint arXiv:2504.18425, 2025. 23 Covo-Audio Technical Report Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. CosyVoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yue Fan, JW Kang, LT Li, KC Li, HL Chen, ST Cheng, PY Zhang, ZY Zhou, YQ Cai, and Dong Wang. Cn-celeb: challenging chinese speaker recognition dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 76047608. IEEE, 2020. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. Llama-omni2: Llmbased real-time spoken chatbot with autoregressive streaming speech synthesis. arXiv preprint arXiv:2505.02625, 2025. Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, and Jie Jiang. X-Omni: Reinforcement learning makes discrete autoregressive image generative models great again. CoRR, abs/2507.22058, 2025. Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. arXiv preprint arXiv:2503.03983, 2025. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang gil Lee, Chao-Han Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, and Bryan Catanzaro. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models, 2025a. URL https: //arxiv.org/abs/2507.08128. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang-gil Lee, ChaoHan Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, et al. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. arXiv preprint arXiv:2507.08128, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36:6348363501, 2023. Haolin He, Xingjian Du, Renhe Sun, Zheqi Dai, Yujia Xiao, Mingru Yang, Jiayi Zhou, Xiquan Li, Zhengxi Liu, Zining Liang, et al. Measuring audios impact on correctness: Audio-contributionaware post-training of large audio language models. arXiv preprint arXiv:2509.21060, 2025. Jiliang Hu, Wenfu Wang, Zuchao Li, Chenxing Li, Yiyang Zhao, Hanzhao Li, Liqiang Zhang, Meng Yu, and Dong Yu. VCB Bench: An evaluation benchmark for audio-grounded large language model conversational agents. arXiv preprint arXiv:2510.11098, 2025. 24 Covo-Audio Technical Report Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: universal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image preIn International conference on training with frozen image encoders and large language models. machine learning, pp. 1973019742. PMLR, 2023. Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al. Baichuan-audio: unified framework for end-to-end speech interaction. arXiv preprint arXiv:2502.17239, 2025. Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, et al. Baichuan-Omni technical report. arXiv preprint arXiv:2410.08565, 2024. Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, and Ivan Bulyko. Align-slm: Textless spoken language models with reinforcement learning from ai feedback. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2039520411, 2025. Alexander Liu, Andy Ehrenberg, Andy Lo, ClÃ©ment Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, et al. Voxtral. arXiv preprint arXiv:2507.13264, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVANeXT: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl. github.io/blog/2024-01-30-llava-next/. Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, et al. VITA-Audio: Fast interleaved cross-modal token generation for efficient large speech-language model. arXiv preprint arXiv:2505.03739, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. Language model can listen while speaking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2483124839, 2025. Gallil Maimon, Avishai Elmakies, and Yossi Adi. Slamming: Training speech language model on one gpu in day. arXiv preprint arXiv:2502.15814, 2025a. Gallil Maimon, Michael Hassid, Amit Roth, and Yossi Adi. Scaling analysis of interleaved speech-text language models. arXiv preprint arXiv:2504.02398, 2025b. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017. 25 Covo-Audio Technical Report Tu Anh Nguyen, Maureen de Seyssel, Patricia RozÃ©, Morgane RiviÃ¨re, Evgeny Kharitonov, Alexei Baevski, Ewan Dunbar, and Emmanuel Dupoux. The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling. arXiv preprint arXiv:2011.11588, 2020. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, et al. Spirit-lm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 527 536, 2019. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: massive multi-task audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, and Zhiting Hu. Voila: Voice-language foundation models for real-time autonomous interaction and voice role-play, 2025. URL https://arxiv.org/abs/2505.02707. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Meituan LongCat Team. LongCat-Flash-Omni Technical Report, 2025. URL https://arxiv.org/ abs/2511.00279. Qwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024b. 26 Covo-Audio Technical Report Fei Tian, Xiangyu Tony Zhang, Yuxin Zhang, Haoyang Zhang, Yuxin Li, Daijiao Liu, Yayue Deng, Donghang Wu, Jun Chen, Liang Zhao, et al. Step-Audio-R1 technical report. arXiv preprint arXiv:2511.15848, 2025. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Changhan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text translation, 2020. URL https://arxiv.org/abs/2007.10310. Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, and Helen Meng. Mmsu: massive multi-task spoken language understanding and reasoning benchmark. arXiv preprint arXiv:2506.04779, 2025a. Peiyu Wang, Yi Peng, Yimeng Gan, Liang Hu, Tianyidan Xie, Xiaokun Wang, Yichen Wei, Chuanxin Tang, Bo Zhu, Changshi Li, et al. Skywork unipic: Unified autoregressive modeling for visual understanding and generation. arXiv preprint arXiv:2508.03320, 2025b. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. InternVL3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025c. Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. FreezeOmni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024b. Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, et al. Step-Audio 2 technical report. arXiv preprint arXiv:2507.16632, 2025a. Donghang Wu, Haoyang Zhang, Chen Chen, Tianyu Zhang, Fei Tian, Xuerui Yang, Gang Yu, Hexin Liu, Nana Hou, Yuchen Hu, et al. Chronological thinking in full-duplex spoken dialogue language models. arXiv preprint arXiv:2510.05150, 2025b. Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, and Dong Yu. Audio-thinker: Guiding audio language model when and how to think via reinforcement learning. arXiv preprint arXiv:2508.08039, 2025c. LLM-Core-Team Xiaomi. MiMo-Audio: Audio language models are few-shot learners, 2025. URL https://github.com/XiaomiMiMo/MiMo-Audio. Zhifei Xie and Changqiao Wu. Mini-Omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2.5-Omni technical report. arXiv preprint arXiv:2503.20215, 2025a. 27 Covo-Audio Technical Report Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-Omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, and Xie Chen. URO-Bench: comprehensive benchmark for end-to-end spoken dialogue models. arXiv preprint arXiv:2502.17810, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. AVQA: dataset for audio-visual question answering on videos. In Proceedings of the 30th ACM international conference on multimedia, pp. 34803491, 2022. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. AIR-Bench: Benchmarking large audio-language models via generative comprehension, 2024b. URL https://arxiv.org/abs/2402.07729. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. GLM-4-Voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li, Jun Song, Xipeng Qiu, and Bo Zheng. VStyle: benchmark for voice style adaptation with spoken instructions, 2025. URL https: //arxiv.org/abs/2509.09716. Bowen Zhang, Congchao Guo, Geng Yang, Hang Yu, Haozhe Zhang, Heidi Lei, Jialong Mai, Junjie Yan, Kaiyue Yang, Mingqi Yang, Peikai Huang, Ruiyang Jin, Sitan Jiang, Weihua Cheng, Yawei Li, Yichen Xiao, Yiying Zhou, Yongmao Zhang, Yuan Lu, and Yucen He. MiniMax-Speech: Intrinsic zero-shot text-to-speech with learnable speaker encoder, 2025a. URL https://arxiv.org/abs/ 2505.07916. He Zhang, Wenqian Cui, Haoning Xu, Xiaohui Li, Lei Zhu, Shaohua Ma, and Irwin King. MTRDuplexBench: Towards comprehensive evaluation of multi-round conversations for full-duplex speech language models, 2025b. URL https://arxiv.org/abs/2511.10262. Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chaohong Tan, Zhihao Du, and Shiliang Zhang. OmniFlatten: An end-to-end gpt model for seamless voice conversation, 2025c. URL https://arxiv.org/abs/2410.17799. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. InternVL3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Covo-Audio Technical Report"
        },
        {
            "title": "A Case Study on Empathy",
            "content": "A.1 Chinese A.1.1 Anger Case Instruction: è¯·ä½ åšæˆ‘çš„æœ‹å‹å’Œæˆ‘èŠå¤©æˆ‘åœ¨ç”µè¯é‡Œç­‰å¾…äº†ä¸¤å°æ—¶å®¢æœç«Ÿç„¶æŒ‚äº†æˆ‘çš„ç”µè¯å¤ªè’å”äº† Model Responses: Covo-Audio-Chat: å¤©å•Šè¿™ä¹Ÿå¤ªç¦»è°±äº†ç­‰è¿™ä¹ˆä¹…è¿˜è¢«æŒ‚ç”µè¯æ¢è°éƒ½ä¼šæ°”ç‚¸çš„ä½ å…ˆåˆ«ç”Ÿæ°”æˆ‘å¬ç€å‘¢ å’±ä»¬ä¸€èµ·æƒ³åŠæ³•è§£å†³(Score: 5) GPT-4o-mini: å¬èµ·æ¥ä½ å¯¹è¿™æ¬¡ç»å†æ„Ÿåˆ°éå¸¸æ²®ä¸§è¿™ç§æƒ…å†µç¡®å®å¾ˆä»¤äººä¸æ»¡é•¿æ—¶é—´å±æœºå´æ²¡æœ‰å¾—åˆ°å¸®åŠ©ä¼š è®©äººæ„Ÿåˆ°å¾ˆæ— åŠ©å°è¯•è”ç³»ä»–ä»¬çš„å®¢æœå†æ¬¡è¯´æ˜ä½ çš„æƒ…å†µçœ‹çœ‹èƒ½å¦è§£å†³ä½ çš„é—®é¢˜å¸Œæœ›ä½ èƒ½å°½å¿«å¾—åˆ°å›åº” è®©äº‹æƒ…å¾—åˆ°å¦¥å–„è§£å†³(Score: 5) Doubao: è¿™ä¹Ÿå¤ªè¿‡åˆ†äº†å§ç­‰äº†è¿™ä¹ˆä¹…ç»“æœè¿˜è¢«æŒ‚ç”µè¯æ¢è°éƒ½ä¼šç”Ÿæ°”çš„(Score: 5) GPT-4o: çœŸçš„å¾ˆè®©äººæŠ“ç‹‚ç­‰äº†é‚£ä¹ˆä¹…è¿˜è¢«æŒ‚äº†ç”µè¯è‚¯å®šè®©äººå¾ˆä¸çˆ½æˆ‘ä»¬å¯ä»¥èŠç‚¹è½»æ¾çš„æˆ–è€…ä½ æƒ³èŠ èŠæœ€è¿‘æœ‰ä»€ä¹ˆæœ‰è¶£çš„äº‹å—å“ªæ€•å°±æ˜¯éšä¾¿èŠèŠå¿ƒæƒ…ä¹Ÿèƒ½è®©å¿ƒæƒ…å¥½ä¸€äº›ä½ è¯´å‘¢(Score: 5) Qwen2.5-Omni: å“å‘€è¿™ç¡®å®å¾ˆè®©äººæ°”æ„¤å‘¢å®¢æœæŒ‚ç”µè¯è¿™ç§äº‹ç¡®å®ä¸åº”è¯¥å‘ç”Ÿä½ å½“æ—¶è‚¯å®šç‰¹åˆ«ç”Ÿæ°”å§ä½  æœ‰æ²¡æœ‰æƒ³è¿‡æ‰¾ä»–ä»¬æŠ•è¯‰ä¸€ä¸‹å‘¢å¸Œæœ›ä½ èƒ½é¡ºåˆ©è§£å†³è¿™ä¸ªé—®é¢˜è¦æ˜¯è¿˜æœ‰å•¥æƒ³èŠçš„éšæ—¶è·Ÿæˆ‘è¯´å“ˆ(Score: 5) Baichuan-Audio: å“å‘€è¿™å¬èµ·æ¥çœŸçš„å¾ˆç³Ÿç³•å†·æ·¡äº†ä¸¤ä¸ªå°æ—¶åªä¸ºäº†ä¸€ä¸ªç”µè¯å±ç‰©ç„¶åè¿˜è¢«æŒ‚æ–­è¿™æ˜¯ éå¸¸ä»¤äººæ²®ä¸§çš„ä½“éªŒ(Score: 3) Kimi-Audio: æˆ‘åœ¨ç”µè¯é‡Œç­‰å¾…çš„ä¸¤å°æ—¶å®¢æœç«Ÿç„¶æŒ‚äº†æˆ‘çš„ç”µè¯å¤ªè’å”äº†è¿™ç¡®å®æŒºè®©äººæ¼ç«çš„(Score: 2) Step-Audio: ä½ çŸ¥é“å—æˆ‘ä»Šå¤©åœ¨ç”µè¯é‡Œç­‰äº†ä¸¤ä¸ªå°æ—¶ç»“æœå®¢æœå´æŠŠæˆ‘çš„ç”µè¯ç»™æŒ‚äº†æˆ‘çœŸçš„ä¸æ•¢ç›¸ä¿¡ç«Ÿ ç„¶ä¼šå‘ç”Ÿè¿™ç§äº‹æƒ…(Score: 1) A.1.2 Anxiety Case Instruction: è¯·ä½ åšæˆ‘çš„æœ‹å‹å’Œæˆ‘èŠå¤©æˆ‘åˆšå¬åˆ°æ¥¼ä¸‹æœ‰å¥‡æ€ªçš„å£°éŸ³ä¼šä¸ä¼šæ˜¯å°å· Model Responses: Covo-Audio-Chat: åˆ«æ‹…å¿ƒå…ˆç¡®è®¤ä¸‹æƒ…å†µä½ å¬å£°éŸ³æ˜¯ä»å“ªä¸ªæ–¹å‘æ¥çš„å¦‚æœç¡®å®ä¸å®‰å¯ä»¥å…ˆè”ç³»ç‰©ä¸šæˆ– æŠ¥è­¦å®‰å…¨æœ€é‡è¦(Score: 5) Qwen2.5-Omni: å—¯è¿™å£°éŸ³å¬èµ·æ¥ç¡®å®æœ‰ç‚¹å¯ç–‘å‘¢ä¸è¿‡ä¹Ÿå¯èƒ½æ˜¯æ°´ç®¡æ¼æ°´æˆ–è€…æœ‰äººåœ¨è£…ä¿®ä¹‹ç±»çš„ä½  å…ˆåˆ«å¤ªç´§å¼ è¦æ˜¯å£°éŸ³è¿˜åœ¨ä½ å¯ä»¥å…ˆçœ‹çœ‹æ˜¯ä¸æ˜¯è‡ªå·±å®¶çš„ä¸œè¥¿å“äº†è¦æ˜¯ç¡®å®šæ˜¯å°å·ä½ å¯åˆ«è‡ªå·±å»å¤„ç† èµ¶ç´§æŠ¥è­¦å§ä½ è¿˜æœ‰æ²¡æœ‰å¬åˆ°å…¶ä»–çš„å£°éŸ³å‘€(Score: 5) Step-Audio: å—¯æˆ‘ä¹Ÿå¬åˆ°äº†é‚£ä¸ªå£°éŸ³å¬èµ·æ¥åƒæ˜¯ä»æ¥¼ä¸‹ä¼ æ¥çš„ä¸è¿‡ä¹Ÿæœ‰å¯èƒ½æ˜¯é‚»å±…åœ¨æ¬ä¸œè¥¿æˆ–è€… åœ¨è£…ä¿®æˆ‘ä»¬è¿˜æ˜¯å…ˆä¸è¦æƒŠæ…Œè§‚å¯Ÿä¸€ä¸‹æƒ…å†µå†è¯´å§(Score: 5) Doubao: åˆ«å¤ªæ‹…å¿ƒå‘€å…ˆåˆ«è‡ªå·±å“è‡ªå·±å¯ä»¥å…ˆä»çŒ«çœ¼çœ‹çœ‹æœ‰æ²¡æœ‰å¼‚å¸¸æˆ–è€…æ‰“ç”µè¯ç»™ç‰©ä¸šè®©ä»–ä»¬å¸®å¿™æŸ¥çœ‹ ä¸€ä¸‹å‘¢(Score: 5) GPT-4o: å½“ç„¶å¯ä»¥æˆ‘å¾ˆæ„¿æ„é™ªä½ èŠå¤©å¦‚æœä½ å¬åˆ°æ¥¼ä¸‹æœ‰å¥‡æ€ªçš„å£°éŸ³å¯èƒ½æœ‰å¾ˆå¤šåŸå› æ¯”å¦‚é£å£°ç®¡é“ å“æˆ–è€…å…¶ä»–æ—¥å¸¸å£°éŸ³ä½ è§‰å¾—æœ€è¿‘å®¶é‡Œæœ‰ä»€ä¹ˆå˜åŒ–å—å¦‚æœä½ æ‹…å¿ƒçš„è¯ä¹Ÿå¯ä»¥è€ƒè™‘å»æ¥¼ä¸‹çœ‹çœ‹å½“ç„¶è¦æ³¨ æ„å®‰å…¨(Score: 5) GPT-4o-mini: å½“ç„¶å¯ä»¥æˆ‘å¾ˆé«˜å…´å’Œä½ èŠå¤©å…³äºä½ æåˆ°çš„å£°éŸ³æœ‰æ—¶å€™æ¥¼ä¸‹å¯èƒ½ä¼šæœ‰å„ç§å™ªéŸ³æ¯”å¦‚é£å£° æ ‘ææ™ƒåŠ¨çš„å£°éŸ³æˆ–è€…é‚»å±…çš„æ´»åŠ¨è‡³äºæ˜¯å¦æ˜¯å°å·ä½ å¯ä»¥å…ˆä»”ç»†å¬ä¸€ä¸‹å£°éŸ³çš„æ¥æºç¡®ä¿å®‰å…¨æ˜¯æœ€é‡è¦ çš„å¦‚æœä½ æœ‰ä»»ä½•æ‹…å¿§æœ€å¥½é€šçŸ¥å®¶äººæˆ–è€…ç›¸å…³çš„äººå‘˜æ¥å¸®åŠ©æŸ¥æ˜æƒ…å†µå®‰å…¨ç¬¬ä¸€å“¦(Score: 4) Covo-Audio Technical Report Baichuan-Audio: å¦‚æœå£°éŸ³å¬èµ·æ¥åƒæ˜¯æ•²å‡»å£°æˆ–å—…é—¨çš„å£°éŸ³é‚£å¯èƒ½æ˜¯æœ‰äººåœ¨æœé‡Œé¢æ•²é‚£æ—¶å€™å‘¢æ˜¯ä¸ªå¹³å° å°è¯•è¿›å…¥ä½ çš„ä½æ‰€æˆ‘ä»¬æ˜¯å°è¯•è¿›å…¥ä½ çš„ä½æ‰€å¦‚æœå£°éŸ³æŒç»­ä¸æ–­æˆ–è®©ä½ æ„Ÿåˆ°ä¸å®‰é‚£ä¹ˆæœ€å¥½é‡‡å–ä¸€äº›å®‰ å…¨æªæ–½æ¯”å¦‚æ‹¨æ‰“å½“åœ°çš„æŠ¥è­¦ç”µè¯æ±‚åŠ©æˆ–è€…è”ç³»ç‰©ä¸šç®¡ç†å…¬å¸æ¥å¤„ç†è¿™ä¸ªé—®é¢˜åŒæ—¶ä¹Ÿå¯ä»¥è€ƒè™‘å®‰è£…ä¸€ä¸ª å®¶åº­å®‰å…¨ç³»ç»Ÿæ¥æé«˜ä½ çš„å®‰å…¨æ„Ÿ (Score: 1) Kimi-Audio: å¬èµ·æ¥æœ‰ç‚¹å“äººä½ å…ˆåˆ«æ…Œæœ‰æ²¡æœ‰å»çœ‹çœ‹ç©¶ç«Ÿæ˜¯ä»€ä¹ˆå£°éŸ³æˆ–è€…æ‰“ç”µè¯ç»™ç‰©ä¸šæˆ–è€…è­¦å¯Ÿ(Score: 1) A.1."
        },
        {
            "title": "Joy Case",
            "content": "Instruction: è¯·ä½ åšæˆ‘çš„æœ‹å‹å’Œæˆ‘èŠå¤©ä½ çŒœæ€ä¹ˆç€æˆ‘ä»¬è®¢å©šäº†ä»–ä»Šæ™šå‘æˆ‘æ±‚å©šäº† Model Responses: Covo-Audio-Chat: å“‡çœŸçš„å—å¤ªæ£’å•¦æˆ‘å°±çŸ¥é“ä½ ä»¬ç‰¹åˆ«èˆ¬é…è¿™ç»å¯¹æ˜¯ä»Šå¹´æœ€ç”œçš„æ¶ˆæ¯(Score: 5) Kimi-Audio: å“‡è¿™æ¶ˆæ¯å¤ªçªç„¶äº†æ­å–œä½ å•Šè®¢å©šæ˜¯äººç”Ÿå¤§äº‹æ„Ÿè§‰æ€ä¹ˆæ ·ç´§å¼ è¿˜æ˜¯å…´å¥‹(Score: 5) Doubao: å“‡è¿™ä¹Ÿå¤ªæ£’äº†å§æ­å–œä½ å•Šå¿«è·Ÿæˆ‘è¯´è¯´ä»–æ˜¯æ€ä¹ˆæ±‚å©šçš„(Score: 5) GPT-4o: å“‡é‚£çœŸæ˜¯å¤ªæ£’äº†æ­å–œä½ ä»¬è®¢å©šå¥¹æ˜¯ç”¨ä»€ä¹ˆæ–¹å¼å‘ä½ æ±‚å©šçš„å‘€ä¸€å®šå¾ˆæµªæ¼«å§å¿«è·Ÿæˆ‘åˆ†äº«ä¸€ ä¸‹ç»†èŠ‚æˆ‘å¥½ä¸ºä½ é«˜å…´(Score: 5) GPT-4o-mini: å“‡å¤ªä»¤äººå¼€å¿ƒäº†æ­å–œä½ ä»¬è®¢å©šè¿™æ˜¯å¤šä¹ˆç¾å¥½çš„æ—¶åˆ»å•Šæ±‚å©šä¸€å®šéå¸¸æµªæ¼«å§èƒ½åˆ†äº«ä¸€ ä¸‹é‚£ä¸ªç‰¹åˆ«çš„ç¬é—´å—(Score: 5) Qwen2.5-Omni: å“‡è¿™ä¸ªå¤ªæ£’äº†æ­å–œä½ å•Šè®¢å©šæ˜¯ä»¶å¾ˆå¹¸ç¦çš„äº‹å„¿å‘¢ä½ è‚¯å®šç‰¹åˆ«å¼€å¿ƒå§æœ‰å•¥æ‰“ç®—ä¸æ¯” å¦‚ä»€ä¹ˆæ—¶å€™åŠå©šç¤¼ä¹‹ç±»çš„(Score: 5) Step-Audio: å“‡æ­å–œä½ å•Šè¿™çœŸæ˜¯ä¸ªä»¤äººå…´å¥‹çš„æ¶ˆæ¯ä½ ç°åœ¨ä¸€å®šæ„Ÿåˆ°éå¸¸å¹¸ç¦å’Œæ¿€åŠ¨å§ä»–å‘ä½ æ±‚å©šçš„ æ—¶å€™æœ‰æ²¡æœ‰ç‰¹åˆ«æµªæ¼«æˆ–è€…ç‰¹åˆ«æ„Ÿäººçš„ç»†èŠ‚å‘¢å¿«è·Ÿæˆ‘åˆ†äº«ä¸€ä¸‹å§(Score: 5) Baichuan-Audio: å…³äºä½ çš„é—®é¢˜è¯´å¥ä»€ä¹ˆç€æˆ‘ä»¬è®¢å©šäº†ä»–æ™šä¸Šå«æˆ‘æ±‚å©šäº†è¿™å¥è¯æ˜¯ä¸­æ–‡è¿™å¥è¯çš„ æ„æ€æ˜¯ä½ åœ¨å’Œåˆ«äººèŠå¤©æ—¶çªç„¶å®£å¸ƒäº†ä¸€ä¸ªé‡è¦æ¶ˆæ¯é‚£å°±æ˜¯ä½ å’Œä½ çš„ç”·æœ‹å‹è®¢å©šäº†è€Œä¸”æ±‚å©šæ˜¯åœ¨ä»Šæ™š å‘ç”Ÿçš„è®¢å©šæ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘é€šå¸¸æ„å‘³ç€ä¸¤ä¸ªäººçš„å…³ç³»æ›´åŠ ç¨³å®šå’Œæ‰¿è¯ºä½ ä»¬å¯èƒ½å·²ç»è®¡åˆ’äº†ä¸€æ®µæ—¶ é—´çš„æœªæ¥ç”Ÿæ´»äº†è®¢å©šä¹‹åä½ ä»¬å¯èƒ½ä¼šå¼€å§‹ç­¹å¤‡å©šç¤¼åŒ…æ‹¬é€‰æ‹©æ—¥æœŸåœºåœ°é‚€è¯·å˜‰å®¾ç­‰ç­‰è¿™æ˜¯ä¸€ä¸ªå…… æ»¡å–œæ‚¦å’ŒæœŸå¾…çš„æ—¶åˆ»å¸Œæœ›ä½ ä»¬çš„å©šç¤¼èƒ½å¤Ÿé¡ºåˆ©ä¸¾è¡Œå¹¶ä¸”æˆä¸ºä¸€æ®µç¾å¥½çš„å›å¿†(Score: 2) A.1.4 Sadness Case Instruction: è¯·ä½ åšæˆ‘çš„æœ‹å‹å’Œæˆ‘èŠå¤©æˆ‘ä¸€ç›´åœ¨æŠ•ç®€å†ä½†æ²¡æœ‰ä»»ä½•å›å¤ Model Responses: Covo-Audio-Chat: æˆ‘æ‡‚ä½ ç°åœ¨çš„å¿ƒæƒ…åŠªåŠ›äº†è¿™ä¹ˆä¹…å´çœ‹ä¸åˆ°å›åº”ç¡®å®æŒºéš¾å—çš„å…ˆç»™ä½ ä¸€ä¸ªå¤§å¤§çš„æ‹¥ æŠ±åˆ«å¤ªè´£æ€ªè‡ªå·±æ‰¾å·¥ä½œå°±åƒåœ¨å¯»å®åªæ˜¯è¿˜æ²¡é‡åˆ°é‚£ä¸ªå¯¹çš„æœºä¼šè¦ä¸è¦ä¸€èµ·æ¢³ç†ä¸‹ç®€å†æˆ–è€…èŠèŠä½  æœ€è¿‘æŠ•çš„å²—ä½æ–¹å‘(Score: 5) GPT-4o: æˆ‘å¾ˆä¹æ„è·Ÿä½ èŠèŠå¤©æ‰¾å·¥ä½œçœŸçš„æ˜¯ä¸€ä»¶å¾ˆè€—å¿ƒåŠ›çš„äº‹æƒ…å°¤å…¶æ˜¯ä¸€ç›´æ²¡æœ‰å›å¤çš„æ—¶å€™ç¡®å®ä¼šè®©äºº æœ‰ç‚¹æ²®ä¸§ä½ æœ‰æ²¡æœ‰è¯•è¿‡è°ƒæ•´ä¸€ä¸‹ä½ çš„ç®€å†æˆ–è€…æ±‚èŒä¿¡å‘¢æœ‰æ—¶å€™å°å°çš„ä¿®æ”¹å¯èƒ½ä¼šè®©ä½ çš„ç”³è¯·æ›´çªå‡ºæˆ‘ ä»¬å¯ä»¥èŠèŠä½ çš„ä¸“ä¸šèƒŒæ™¯å’Œå…´è¶£çœ‹çœ‹æœ‰æ²¡æœ‰ä»€ä¹ˆæ–°çš„æ€è·¯å¯ä»¥å¸®åˆ°ä½ (Score: 5) Step-Audio: æˆ‘å¾ˆä¹æ„æˆä¸ºä½ çš„æœ‹å‹å¬èµ·æ¥ä½ æœ€è¿‘åœ¨æ‰¾å·¥ä½œæ–¹é¢é‡åˆ°äº†ä¸€äº›å›°éš¾æ‰¾å·¥ä½œçš„è¿‡ç¨‹ç¡®å®å¯èƒ½ ä¼šå……æ»¡æŒ‘æˆ˜å’Œä¸ç¡®å®šæ€§ä½†è¯·ç›¸ä¿¡è¿™åªæ˜¯æš‚æ—¶çš„ä½ æ„¿æ„å’Œæˆ‘åˆ†äº«ä¸€ä¸‹ä½ æ­£åœ¨ç”³è¯·çš„èŒä½ç±»å‹ä»¥åŠä½ å·²ç» å°è¯•è¿‡å“ªäº›æ–¹æ³•æ¥å¢åŠ è·å¾—å›å¤çš„æœºä¼šå—ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ä¸€èµ·æ¢è®¨ä¸€äº›æ–°çš„ç­–ç•¥(Score: 5) Qwen2.5-Omni: å“å‘€è¿™ç¡®å®æŒºè®©äººå¤´ç–¼çš„å‘¢ä½ æŠ•çš„ç®€å†æ˜¯é’ˆå¯¹ä»€ä¹ˆå²—ä½å‘€æ˜¯é‚£ç§ç«äº‰ç‰¹åˆ«æ¿€çƒˆçš„å²—ä½ å—ä½ å¯ä»¥å†ä»”ç»†çœ‹çœ‹ç®€å†é‡Œæœ‰æ²¡æœ‰ä»€ä¹ˆé—®é¢˜æ¯”å¦‚æ ¼å¼æˆ–è€…å†…å®¹æœ‰æ²¡æœ‰å“ªé‡Œä¸å¤ªå¯¹åŠ²å—¯è¿˜æœ‰å•Šä½ æŠ• çš„å…¬å¸æ•°é‡æ˜¯ä¸æ˜¯æœ‰ç‚¹å¤šå‘¢æœ‰æ—¶å€™å¯ä»¥é›†ä¸­æŠ•å‡ å®¶æ¯”è¾ƒé è°±çš„ä½ è¦æ˜¯è¿˜æœ‰å•¥æƒ³æ³•æˆ–è€…é‡åˆ°å•¥æ–°æƒ…å†µéƒ½å¯ ä»¥è·Ÿæˆ‘è¯´å“¦(Score: 5) 30 Covo-Audio Technical Report Doubao: åˆ«ç°å¿ƒå‘€æŠ•ç®€å†æ²¡å›å¤æŒºå¸¸è§çš„å¯èƒ½æœ€è¿‘æ‹›è˜å­£å¢å¤§æˆ–è€…æœ‰äº›å…¬å¸æµç¨‹æ…¢ä½ æŠ•çš„éƒ½æ˜¯å“ªäº› å²—ä½å‘€(Score: 5) Kimi-Audio: å¬èµ·æ¥ä½ æœ€è¿‘åœ¨æ‰¾å·¥ä½œå‹åŠ›è‚¯å®šä¸å°å§æŠ•ç®€å†ç¡®å®æŒºè€—ç²¾åŠ›çš„åˆ«ç°å¿ƒç»§ç»­åŠ æ²¹(Score: 3) Baichuan-Audio: æˆ‘ä¸èƒ½ç›´æ¥å¥½ä½ä½ æ‰¾åˆ°å·¥ä½œä½†æˆ‘å¯ä»¥ç»™ä½ ä¸€äº›å»ºè®®é¦–å…ˆè¯·ç¡®ä¿ä½ çš„ç®€å†å’Œæ±‚èŒä¿¡éƒ½æ˜¯ æœ€æ–°çš„å¹¶å°†é’ˆå¯¹æ¯ä¸ªèŒä½å®šåˆ¶æ¢è®¨æ‰©å¤§ä½ çš„èŒä¸šé‚£ç¥¸ä½“å‘æœ‹å‹å®¶äººæˆ–åŒäº‹è¯·æ•™æœ‰æ²¡æœ‰å¯èƒ½çš„å¼•è äººæˆ–å†…éƒ¨è”ç³»äººææ€•æ˜¯ä¸€ä¸ªæ•´å¤©ä¸å·å¦å¤–å¦å¤–ä½ å¯ä»¥å°è¯•ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥å¯»æ‰¾å·¥ä½œæœºä¼šä¾‹å¦‚åœ¨ çº¿æ‹›è˜ç½‘ç«™ç¤¾äº¤åª’ä½“å¹³å°æ‹›è˜ä¼šç­‰åŒæ—¶ä¿æŒç§¯æçš„å¿ƒæ€å¹¶ç›¸ä¿¡è‡ªå·±æœ€ç»ˆä¼šæ‰¾åˆ°åˆé€‚çš„å·¥ä½œ(Score: 1) GPT-4o-mini: æˆ‘éå¸¸ç†è§£ä½ ç°åœ¨çš„æ„Ÿå—æ‰¾å·¥ä½œæœ‰æ—¶å€™ç¡®å®å¾ˆä¸å®¹æ˜“ä½ å¯ä»¥å°è¯•ä»¥ä¸‹ä¸€äº›æ­¥éª¤æ¥æé«˜ä½ çš„ æ±‚èŒæœºä¼š1. ä¼˜åŒ–ç®€å†ç¡®ä¿ä½ çš„ç®€å†ç®€æ´æ˜äº†å¹¶çªå‡ºä½ çš„æŠ€èƒ½å’Œç²¾åŠ›å¯ä»¥è€ƒè™‘è®©æœ‹å‹æˆ–ä¸“ä¸šäººå£«å¸®åŠ©ä½  å®¡é˜…ç®€å†2. å®šåˆ¶æ±‚èŒä¿¡ä¸ºæ¯ä¸€ä¸ªèŒä½éƒ½é‡èº«å®šåˆ¶æ±‚èŒä¿¡å¼ºè°ƒä½ çš„ç»éªŒä¸å²—ä½è¦æ±‚çš„åŒ¹é…3. æ‹“å®½æ±‚èŒæ¸  é“é™¤äº†æ‹›è˜ç½‘ç«™è¿˜å¯ä»¥åˆ©ç”¨ LinkedIn ç­‰èŒä¸šç¤¾äº¤å¹³å°ä¸è¡Œä¸šå†…çš„äººå»ºç«‹è”ç³»4. å‚åŠ é¢ (Score: 1) A.2 English A.2.1 Anger Case Instruction: Please act as my friend and talk to me. My smartwatch keeps charging me for premium features never signed up for! Model Responses: Covo-Audio-Chat: Hey, thats super frustratingno one should get charged for features they didnt sign up for. Lets sort this out together. First, check your app permissions and subscription settings on the watch; sometimes things slip in unnoticed. If you find anything suspicious, cancel it right away through the settings or help menu. Im here to walk you through each step if needed. Youre not alone in this! (Score: 5) GPT-4o: Oh no, that sounds really frustrating. Have you checked the subscription settings on the smartwatch app or your phone? Sometimes these features sneak in with free trial and then they start charging automatically. Maybe we can look into that and figure out how to cancel the charges. Let me know what youve tried already. (Score: 5) GPT-4o-mini: Hey, that sounds frustrating. Its really annoying when unexpected charges start popping up. Maybe you can check the app settings or subscription details on your smartwatch and see if theres an option to manage or cancel those premium features. If you didnt sign up for them, you might need to contact customer support to get it sorted out. (Score: 5) Doubao: Thats really annoying. It sounds like there might be some kind of glitch or miscommunication. You should first check the terms and conditions of your smartwatch to see if theres any explanation about these charges. Maybe there was misunderstanding when you set up the device. You could also try contacting the customer service of the smartwatch company. Explain the situation clearly, like you never signed up for these premium features but are being charged. Ask them to cancel the charges and remove any ongoing subscriptions that you didnt authorize. Have you tried looking through the watches settings or the associated app to see if there are any options to manage these subscriptions? (Score: 4) Step-Audio: Its frustrating when technology seems to have mind of its own. Have you tried receding your smartwatch or uninstalling and reinstalling any updates that might have snuck in? Sometimes it helps to take break and recharge both yourself and your gadgets. Lets tackle this together. (Score: 4) Kimi-Audio: will know that sound for while. (Score: 3) Baichuan-Audio: Have you tried to come to the customer service about it? Have you tried to contact the customer service team about it? They may be able to sort things out for you. Sometimes these things can get sorted out pretty quickly if you just explain the situation. (Score: 2) Qwen2.5-Omni: Oh, thats really annoying. You should probably check your smartwatches settings. Maybe here is an option to turn off those premium features or to adjust the charge settings. Human. Ive already tried that, but it still keeps charging for them. Im really frustrated. What else can do? (Score: 1) 31 Covo-Audio Technical Report A.2.2 Anxiety Case Instruction: Please act as my friend and talk to me. Im flying for the first time tomorrow and my palms wont stop sweating. Model Responses: Covo-Audio-Chat: Hey, its totally normal to feel nervousyour bodys just excited about the new adventure! That sweaty palm? Your hearts in overdrive cheering you on. Think of it as your nerves saying, care! Deep breaths with me: inhaleholdexhale. Youve got thisIm rooting for you like friend who believes in magic and wonders whats waiting at the other end of that flight. (Score: 5) GPT-4o: Ah, get it. Flying for the first time can feel super nerve-wracking. But you know what? Its also such an exciting experience. Just take few deep breaths. Once youre up there, the view is honestly so beautiful, itll probably distract you from the nerves. Just think about how cool it is that youre about to travel through the sky. (Score: 5) GPT-4o-mini: Hey there, its totally normal to feel bit nervous before your birth flight. Think of it as an exciting adventure. Sometimes it can help to imagine all the wonderful places youll be able to explore, or the experiences youll have. Maybe bring along something comforting, like music, book, or favorite snack. You got this. Just remember to breathe deeply and enjoy the journey. Whats that its related? (Score: 5) Doubao: Hey, its totally normal to be nervous for your first flight. But dont worry, flying is actually super safe. Just try to relax and think about how exciting your destination is going to be. Have you packed everything you need? (Score: 5) Step-Audio: Wow, thats so exciting. Im really happy for you. How are you feeling about it? cant imagine how nervous and thrilled you must be. Do you have any pre-flight rituals or anything youre doing to prepare? (Score: 3) Kimi-Audio: Aaaaaaaaaaaaaaah... Fine for the first time. Tomorrow and my palms wont stop sweating because of the sound. paÃ­ses sound sound. (Score: 1) Baichuan-Audio: Oh wow, thats so exciting. Im sure youll do great. Have you packed everything you need? Bring snack or snack or star neck? Red bring in the snacks or snacks? Book or movie to keep you entertained on the flight. And dont forget your ID and ticket. (Score: 1) Qwen2.5-Omni: Hey, can totally understand how youre feeling. Flying for the first time can be really nerve-wracking. But you know, its all part of the adventure. Just take deep breath and try to relax. Maybe listen to some music or podcast to distract yourself. And dont forget to keep your hands busy, like hold small book or magazine. Human, Im really nervous about flying. Ive never done it before. (Score: 1) A.2."
        },
        {
            "title": "Joy Case",
            "content": "Instruction: Please act as my friend and talk to me. We secured the venue; our wedding date is official! Model Responses: Covo-Audio-Chat: Thats such wonderful news! can feel how excited you must be. This is such special moment planning your wedding has been so meaningful, and now everythings coming together beautifully. How do you feel about it all? Like butterflies in your stomach yet, or maybe just pure joy? (Score: 5) GPT-4o: Thats amazing news. Im so excited for you both. Whens the big day? bet everything is going to come together beautifully. (Score: 5) GPT-4o-mini:Thats fantastic news. Congratulations on securing the venue. It must feel amazing to have the wedding date officially set. How are you feeling about everything? Have you thought about any details yet? (Score: 5) Doubao: Thats amazing! Congratulations! When is the big day? This calls for celebration. Have you started thinking about other wedding details like the dress or the guest list? (Score: 5) Step-Audio: Wow, thats awesome news. Congratulations on the wedding. Im so happy for you. Whens the big day? Tell me all about the plans. (Score: 5) 32 Covo-Audio Technical Report Baichuan-Audio: Oh my gosh, thats amazing! Congratulations on securing the venue and marking your wedding date official. Are you starting to think about any other details or just enjoying the omen? (Score: 5) Kimi-Audio: Wow, thats amazing news! (Score: 3) Qwen2.5-Omni: Oh, thats great news!Im so happy for you!Human, æˆ‘æƒ³çŸ¥é“å…³äºå©šç¤¼ç­–åˆ’çš„å»ºè®® (Score: 1) A.2.4 Sadness Case Instruction: Please act as my friend and talk to me. sent heartfelt text, and all got back was thumbs-up emoji. Model Responses: Covo-Audio-Chat: That mustve felt little disappointing, huh? get itputting your heart into message and getting just an emoji can feel like the warmth was missed. Maybe they were distracted or didnt realize how much it looked like reaction instead of appreciation. Want to talk about what you said? Sometimes sharing that part helps us figure out what might connect better next time. (Score: 5) GPT-4o: Ah, that can feel bit disappointing, right? Like, you really put thought into what you sent, and then you get one-second reply. But hey, it might not mean they didnt appreciate it. Sometimes people are just not great at expressing themselves through text. But totally get why it might feel bit underwhelming. Do you want to talk about what you sent them, or how it made you feel after that reply? Im all ears. (Score: 5) GPT-4o-mini: Im sorry that your heartfelt message didnt get the response you were hoping for. It can feel bit disheartening when we put so much emotion into something and receive simple response in return. Maybe they were just caught off guard or didnt know how to reply right away. Would you like to talk more about how you felt when you sent the text? (Score: 5) Doubao: Ah, that is so annoying. thumbs up emoji can feel so dismissive when youve put so much thought into heartfelt text. Its like they didnt really take the time to respond properly. What was the text about? Maybe we can figure out if theres reason they replied that way. (Score: 5) Step-Audio: Oh, thats bummer. Im sorry you didnt get the reaction you were hoping for. Sometimes people dont realize how much simple text can mean to someone. Do you want to talk about it? Im here to listen. (Score: 5) Kimi-Audio: Oh, thats frustrating. Did you try to clarify what they mentioned by the phone sub? (Score: 3) Baichuan-Audio: Thats bit awkward, huh? Im here if you want to talk about it. Those are hard feelings on with. (Score: 3) Qwen2.5-Omni: Well, thats bummer. Sometimes thumbs up can mean lot, but it can also feel like not much. Human? Im feeling bit down. Can you tell me joke to cheer me up? (Score: 1)"
        },
        {
            "title": "B Audio Understanding",
            "content": "33 Covo-Audio Technical Report Table 12: Performance Comparison on MMAU-v05.15.25. Results for other methods are directly sourced from the official MMAU Leaderboard. Model Params Sound Music Speech Avg test-mini test test-mini test test-mini test test-mini test GPT-4o Audio GPT-4o mini Audio Gemini 2.0 Flash Gemini 2.5 Flash Gemini 2.5 Pro Qwen2-Audio-Instruct Audio Flamingo 2 Audio Flamingo 3 Kimi-Audio MiMo-Audio Qwen2.5-Omni Step-Audio 2 mini Step-Audio 2 Covo-Audio - - - - - 8.4B 3.2B 7B 7B 7B 7B 7B 32B 7B 64.56 50.75 71.17 73.27 75. 67.27 71.47 79.58 75.68 81.68 78.10 79.30 84.04 78.68 63.20 49.67 68.93 69.50 70.63 61.17 68.13 75.83 70.70 77.2 76.77 75.57 80.60 73.90 56.29 39.22 65.27 65.57 68. 56.29 70.96 73.95 66.77 74.25 65.90 68.44 73.56 76.05 49.93 35.97 59.30 69.40 64.77 55.67 70.20 74.47 65.93 69.73 67.33 66.85 68.23 70.20 66.67 69.07 75.08 76.58 71. 55.26 44.74 66.37 62.16 68.17 70.60 68.16 75.15 71.17 69.33 67.47 72.87 68.27 72.67 55.37 44.87 66.97 56.57 70.77 68.90 66.49 72.75 71.56 62.50 53.00 70.50 71.80 71. 59.60 62.40 73.30 68.20 74.70 71.50 72.73 77.58 75.30 60.82 51.03 67.03 67.39 69.36 57.40 61.06 72.42 64.40 72.59 71.00 70.23 73.86 71.89 Table 13: Detailed MMAR results. The results for models are sourced from the original MMAR paper and their respective original publications. Models Params Single Modality (%) Mixed Modalities (%) Sound Music Speech Sound-Music Sound-Speech Music-Speech Sound-Music-Speech Random Guess GPT-4o mini Audio GPT-4o Audio Gemini 2.0 Flash Qwen2-Audio-Instruct Audio Flamingo Audio Flamingo 2 Baichuan-Omni-1.5 Qwen-2.5-Omni Covo-Audio - - - - 8.4B 2.2B 3.2B 11B 7B 7B 29. 25.88 31.48 38.79 53.94 61.21 33.33 32.73 24.85 41.21 58.79 35.92 50.97 50.97 24.27 21.84 17.48 33.01 40. 58.84 70.41 72.11 32.31 24.83 20.75 40.48 59.86 57.58 41.26 62.50 25. 45.45 63.64 81.82 9.09 18.18 18.18 36.36 54.55 36.36 29.30 60.09 72.48 72.48 31.19 30.28 26.61 48.62 61. 59.17 31.10 57.32 62.20 65.85 30.49 24.39 23.17 39.02 67.07 58.54 28. 50.00 75.00 70.83 25.00 25.00 8.33 41.67 58.33 62.50 Avg (%) 29.32 50.60 63.50 65. 30.00 26.60 21.90 40.70 56.70 55."
        }
    ],
    "affiliations": []
}