{
    "paper_title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
    "authors": [
        "Yongsheng Yu",
        "Wei Xiong",
        "Weili Nie",
        "Yichen Sheng",
        "Shiqiu Liu",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 5 4 6 0 2 . 1 1 5 2 : r PixelDiT: Pixel Diffusion Transformers for Image Generation Yongsheng Yu1,2 * Wei Xiong1 Weili Nie1 Yichen Sheng1 Shiqiu Liu1 Jiebo Luo2 1NVIDIA 2University of Rochester Project Lead and Main Advising Abstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts fully transformer-based architecture shaped by dual-level design: patch-level DiT that captures global semantics and pixel-level DiT that refines texture details, enabling efficient training of pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256256, surpassing existing pixel generative models by large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 10242 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models. (a) High-resolution text-to-image samples at the megapixel scale (approximately 10241024) generated by PixelDiT-T2I, which is directly trained on pixel space. (b) Train-free image editing with FlowEdit [1] based on different diffusion models. The imperfect reconstruction of VAEs used by FLUX [2] and Stable Diffusion 3 [3] produce severe and non-invertible distortions on the small details such as the scene texts on the wall, therefore the subsequent editing of the latent diffusion models fails. In contrast, by denoising directly in pixel space without VAE, our PixelDiT avoids VAE reconstruction artifacts and helps maintain background consistency during local manipulations. For fair comparison, all models are evaluated with nmin= 0 in FlowEdit to preserve more structures. Figure 1 Visual results of PixelDiT on text-to-image generation and training-free image editing. Please zoom in for the details. Additional examples are provided in the Appendix. * Work was done while Yongsheng was an intern at NVIDIA. Contact: {yongshengy, wxiong}@nvidia.com 2025 NVIDIA. All rights reserved. PixelDiT: Pixel Diffusion Transformers for Image Generation 1. Introduction Latent diffusion models (LDMs) [4] perform denoising in compressed representation space and have become the standard paradigm for Diffusion Transformers [5, 6]. This choice yields substantial savings in compute and memory. However, it also inherits two structural limitations. First, LDMs couple diffusion to separately pretrained autoencoder whose reconstruction objective is only partially aligned with the downstream generative objective [7, 8]. Second, the autoencoder introduces lossy reconstruction that can remove high-frequency details and cap sample fidelity for image generation and editing tasks even when the diffusion model is strong [3, 9, 10, 11], as shown in Figure 1b. These limitations motivate us to revisit pixelspace diffusion, where both learning and sampling operate directly in the original pixels without autoencoders. The core challenge in pixel-space diffusion can be framed as pixel modeling. By pixel modeling, we refer to the mechanism of capturing dense, per-pixel interactions and high-frequency details, which is distinct from the semantic structural generation typically handled by coarse patch tokens. Effectively modeling these per-pixel tokens is crucial for texture fidelity but computationally expensive. Prior attempts expose fundamental trade-off in how pixel interactions are organized. One line of work adopts aggressive patchification [12, 13, 14, 15, 16] to keep attention affordable, but this significantly weakens per-pixel token modeling and hinders the generation of finer visual contents. Another line pushes toward near-pixel granularity (e.g., small patch sizes or U-ViT-like designs) to better preserve details [17, 18, 19], but the global attention must process very long token sequences with quadratic complexity, or rely on heavy decoder-style stacks, resulting in high training and sampling costs. Cascaded pipelines [20, 21] mitigate some cost, but may introduce additional stages and accumulated errors. Taken together, these observations suggest that the obstacle to practical pixel-space diffusion is the lack of an efficient pixel modeling mechanism that can model both global semantics and per-pixel updates. To address these challenges, we propose PixelDiT, singlestage, fully transformer-based diffusion model that performs end-to-end training and sampling in pixel space while explicitly structuring pixel modeling. PixelDiT decouples image semantics from per-pixel learning with dual-level architecture design: patch-level DiT with an aggressive patch size that performs long-range attention on short patch token sequences to capture global layout and content, and pixel-level DiT that performs dense, per-pixel token modeling to refine local texture details. We propose two key techniques to make this design effective: (i) pixel-wise AdaLN modulation that conditions each pixel token using semantic tokens, aligning per-pixel updates with global context; and (ii) pixel token compaction mechanism that compresses each pixel token before full attention and decompresses them back afterward, enabling per-pixel token modeling while keeping global attention efficient. The dual-level pathways, together with efficient pixel modeling via pixel-wise AdaLN and token compaction, yield high training efficiency and faster convergence while preserving fine details. Extensive experiments show that PixelDiT generates highquality images when trained end-to-end on pixel space without any autoencoders. On ImageNet 256 256, PixelDiT achieves an FID of 1.61, outperforming recent pixelspace models by large margin. We further demonstrate the scalability of our architecture by extending PixelDiT to text-to-image generation. Using multi-modal DiT blocks in the patch-level pathway, we directly train PixelDiT at 10242 resolution in pixel space. To our knowledge, this represents significant breakthrough, as prior pixel-space models have struggled to scale effectively to such resolutions. PixelDiT can generate high-fidelity text-aligned 1K resolution images as shown in Figure 1a and achieves competitive scores on standard benchmarks compared to state-of-the-art latent diffusion models. Moreover, operating directly in pixel space enables PixelDiT to bypass VAE reconstruction artifacts, leading to significantly better content preservation for image editing tasks, as shown in Figure 1b. We highlight our main contributions as follows: single-stage, We propose PixelDiT, fully transformer-based pixel-space diffusion model that is trained end-to-end without separate autoencoder. We demonstrate that efficient pixel modeling is the key factor to practical pixel-space diffusion and propose dual-level DiT architecture that disentangles the learning of global semantics from pixel-level texture details. We introduce pixel-wise AdaLN modulation mechanism and pixel token compaction mechanism that jointly enable dense per-pixel token modeling. PixelDiT achieves high image quality in both classconditioned image generation and text-to-image generation, significantly outperforming existing pixelspace generative models and approaching the stateof-the-art latent diffusion models. 2. Related Works 2.1. Latent Diffusion Models with Autoencoders Latent diffusion models (LDMs) perform denoising in latent space produced by an autoencoder, delivering substantial compute and memory savings and enabling training at higher resolutions with larger backbones on fixed budget [4]. In practice, most LDMs adopt varia2 PixelDiT: Pixel Diffusion Transformers for Image Generation Figure 2 Overview of PixelDiT: dual-level, fully transformer-based diffusion architecture that operates directly in pixel space. The left figure shows the overall framework of PixelDiT, while the right figure illustrates the detailed structure of the PiT blocks. tional autoencoder (VAE) that trades off reconstruction fidelity and compression rate; rich line of work improves the autoencoder via better architecture or learning objective, including stronger compression schemes, tokenizers [22, 23, 24, 25] and analysis of the reconstructiongeneration optimization dilemma [7]. End-to-end approaches have also been explored: REPA-E jointly tunes the VAE and diffusion transformer to align the latent representation with the generative objective [8]. Recently, several works [11, 26] replace the variational bottleneck with representation autoencoders and report competitive latents without explicit variational modeling. Why revisit pixel space? Despite their efficiency, LDMs inherit reconstruction bottleneck: sample fidelity is bounded by the autoencoder, and aggressive compression tends to remove high-frequency details and fine structures [4, 22]. Pretraining or co-training large autoencoder adds additional data and compute overhead compared to pixel-space training. Moreover, misalignment between the autoencoders reconstruction objective and the downstream generative objective introduces distribution shift in latent space (e.g., texture smoothing or color shifts) that the diffusion model must compensate for [7, 8]. Finally, decoding latency at sampling time incurs additional cost compared to pixel-space models. memory-efficient convolutional networks with skip connections [18, 19]; Fractal Generative Models introduce fractal designs for long-range structure [21]; PixelFlow develops hierarchical flow-based pixel-space models [17]; and PixNerd employs lightweight neural field layers for efficient pixel-space diffusion [12]. In contrast to these efforts, we present purely transformer-based pixel-space diffusion architecture trained directly at 10242 resolution. Concurrent to our work, EPG [15] adopts two-stage framework bridging self-supervised pre-training and generative fine-tuning, while FARMER [14] integrates normalizing flows with autoregressive modeling to handle high-dimensional pixels. Distinctively, JiT [16] demonstrates that plain Transformers can efficiently model high-dimensional data by predicting clean images (ğ‘¥0prediction). 3. Method In this section, we present PixelDiT, transformer-based diffusion model that directly performs denoising in pixel space. Our objective is to make pixel token modeling computationally efficient while preserving the convergence behavior and sample quality that have motivated latent space approaches. 2.2. Pixel-Space Diffusion Models 3.1. Dual-level DiT Architecture Pixel-space diffusion models preceded latent-space methods and remain an active area of research. Early work establishes high image quality via direct denoising in pixel space [27], while cascaded diffusion improves highresolution synthesis through multi-scale pipelines [20]. However, the quadratic compute and memory cost in image resolution renders end-to-end training at megapixel scales prohibitively expensive. Recent efforts revisit pixel space with improved architectures and training: JetFormer formulates autoregressive generation over raw pixels and text [13]; Simple Diffusion proposes simplified, As illustrated in Figure 2, we adopt dual-level transformer organization that concentrates semantic learning on coarse patch-level pathway and leverages dedicated Pixel Transformer (PiT) blocks in the pixel-level pathway for detail refinement. This organization allows most semantic reasoning to occur on the low-resolution grid, which reduces the burden on the pixel-level pathway and accelerates learning, consistent with observations found in [11, 28, 29]. Patch-level architecture: Let the input image be ğ‘¥ PixelDiT: Pixel Diffusion Transformers for Image Generation Rğµğ¶ğ»ğ‘Š . We form non-overlapping ğ‘ ğ‘ patch tokens ğ‘¥patch Rğµğ¿(ğ‘2ğ¶), where ğ¿=( ğ» ğ‘ ) is the number of tokens, and project them to hidden size ğ·: ğ‘ )( ğ‘Š ğ‘ 0 = ğ‘Špatch ğ‘¥patch, ğ‘ = SiLU(ğ‘Šğ‘¡ğ‘¡ + ğ‘Šğ‘¦ğ‘¦ + ğ‘) Rğµ1ğ·. (1) (2) Following [7], we augment the DiT block by replacing LayerNorm with RMSNorm and applying 2D RoPE in all attention layers. The patch-level pathway consists of ğ‘ augmented DiT blocks; for block ğ‘–, we write ğ‘ ğ‘– = RMSNorm(ğ‘ ğ‘–), (3) ğ‘ ğ‘– = ğ‘ ğ‘– + ğ›¼1(ğ‘) Attn(ğ›¾1(ğ‘) ğ‘ ğ‘– + ğ›½1(ğ‘); RoPE), (4) ğ‘ ğ‘–+1 = ğ‘ ğ‘– + ğ›¼2(ğ‘) MLP( ğ›¾2(ğ‘) RMSNorm(ğ‘ ğ‘–) + ğ›½2(ğ‘)), (5) where AdaLN modulation parameters are produced from the global conditioning vector ğ‘ and then broadcast across the ğ¿ patch tokens. This global-to-patch broadcasting applies identical per-feature AdaLN parameters to all patch tokens (i.e., token-independent at the patch level), in contrast to the pixel-wise AdaLN used later in the pixel-level pathway. After ğ‘ blocks, we obtain semantic tokens ğ‘ ğ‘ Rğµğ¿ğ·. In the spirit of designs [11, 28], we define the conditioning signal for the pixel-level pathway as ğ‘ cond := ğ‘ ğ‘ + ğ‘¡, where ğ‘¡ is the timestep embedding. These tokens provide semantic context to the PiT blocks via pixel-wise AdaLN. Pixel-level architecture: The pixel-level DiT is composed of ğ‘€ layers of PiT Blocks. It takes the pixel tokens and the output of the patch-level DiT ğ‘ cond as inputs to perform the pixel token modeling and generate the final result. The details of each PiT block are described below. Design notes. The patch-level pathway exclusively processes patch tokens to capture global semantics. By delegating detail refinement to the pixel-level pathway, we can employ larger patch sizes ğ‘, which shortens the sequence length and accelerates inference while preserving per-pixel fidelity. Furthermore, the pixel-level pathway operates with reduced hidden dimension ğ·pix ğ· (e.g., ğ·pix=16), ensuring that dense per-pixel computations remain highly efficient. 3.2. Pixel Transformer Block Each PiT block has two core components. First, pixelwise AdaLN enables dense conditioning at the level of individual pixels, aligning per-pixel updates with global context. Second, pixel token compaction mechanism reduces redundancy among pixel tokens so that global attention operates on manageable sequence length. Figure 3 AdaLN modulation strategies. (A) naive AdaLN broadcasts global conditioning vector to all pixels. (B) Patch-wise AdaLN expands semantic tokens to the ğ‘2 pixels within each patch. (C) Pixel-wise AdaLN applies an MLP to each semantic token to produce perpixel scale, shift, and gating parameters, enabling fully context-aligned updates at every pixel. Pixel-wise AdaLN Modulation. In the pixel-level pathway, each image is embedded into one token per pixel with linear layer: ğ‘‹ Rğµğ¶ğ»ğ‘Š reshape + linear Rğµğ»ğ‘Š ğ·pix. (6) To align with patch-level semantic tokens, we reshape into ğµ ğ¿ sequences of ğ‘2 pixel tokens, i.e., ğ‘‹ R(ğµğ¿)ğ‘2ğ·pix . For each patch, we form semantic conditioning token ğ‘ cond R(ğµğ¿)ğ· that summarizes global context. straightforward patch-wise modulation would repeat the same parameters for all ğ‘2 pixels within patch, as illustrated in Figure 3(B). However, this cannot capture dense per-pixel variation. Instead, we assign independent modulation to each pixel by expanding ğ‘ cond into ğ‘2 sets of AdaLN parameters via linear projection Î¦ : Rğ· Rğ‘26ğ·pix: Î˜ = Î¦(ğ‘ cond) R(ğµğ¿)ğ‘26ğ·pix, (7) and we partition the last dimension of Î˜ into six groups of size ğ·pix, yielding (ğ›½1, ğ›¾1, ğ›¼1, ğ›½2, ğ›¾2, ğ›¼2) (R(ğµğ¿)ğ‘2ğ·pix)6 . These modulation parameters are learned and are distinct at every pixel, as illustrated in Figure 3(C). They are applied to ğ‘‹ via pixel-wise AdaLN, enabling pixel-specific updates; in contrast, patch-wise AdaLN broadcasts single set of parameters to all pixels within patch and thus cannot capture such spatial variation. Pixel Token Compaction. In the pixel-level pathway, direct attention over all ğ» ğ‘Š pixel tokens is computa4 PixelDiT: Pixel Diffusion Transformers for Image Generation Method Training Epochs #params gFID sFID IS Precision Recall Generation@256 Latent Generative Models LDM-4-G [4] DiT-XL [5] SiT-XL [6] MaskDiT [30] REPA [31] LightningDiT [7] SVG-XL [26] DDT-XL [28] RAE-XL [11] StyleGAN-XL [32] ADM-U [27] CDM [20] RIN [33] VDM++ [34] JetFormer [13] Simple Diffusion [18] FractalMAR-H [21] FARMER [14] EPG [15] PixelFlow-XL [17] PixNerd-XL [12] JiT-G [16] PixelDiT-XL PixelDiT-XL 170 1400 1400 1600 800 800 1400 400 800 / 400 2160 480 / / / 600 320 800 320 320 600 80 320 400M 675M 675M 675M 675M 675M 675M 675M 839M / 554M / 410M / 2.8B 2.0B 844M 1.9B 583M 677M 700M 2B 797M 797M 3.60 2.27 2.06 2.28 1.42 1.35 1.92 1.26 1.13 Pixel Generative Models 2.30 4.59 4.88 3.42 2.12 6.64 2.44 6.15 3.60 2.04 1.98 1.93 1.82 2.36 1.61 - 4.60 4.50 5.67 4.70 4.15 - - - 4.02 5.25 - - - - - - - - 5.83 - - 5.11 4.68 247.6 278.2 270.3 276.5 305.7 295.3 264.9 310.6 262.6 265.1 186.7 158.7 182.0 267.7 - 256.3 348.9 269.2 283.2 282.1 298.0 292.6 282.3 292.7 0.87 0.83 0.82 0.80 0.80 0.79 - 0.79 0.78 0.78 0.82 - - - 0.69 - 0.81 0.81 0.80 0.81 0.80 0. 0.80 0.78 Table 1 Quantitative results on ImageNet 256256 for class-conditioned generation. 0.48 0.57 0.59 0.61 0.65 0.65 - 0.65 0.67 0.53 0.52 - - - 0.56 - 0.46 0.51 0.56 0.60 0.60 0.62 0.57 0.64 tionally prohibitive. We therefore compress the ğ‘2 pixel tokens inside each patch into compact patch token before global attention, and later expand the attended representation back to pixels. This reduces the attention sequence length from ğ» ğ‘Š to ğ¿=( ğ» ğ‘ ), ğ‘2-fold reduction; with ğ‘=16, this yields 256 shrinkage while preserving per-pixel updates through pixel-wise AdaLN and the learned expansion. ğ‘ )( ğ‘Š We instantiate the compaction operators with learned flattening: linear map ğ’ : Rğ‘2ğ·pix Rğ· that jointly mixes spatial and channel dimensions, paired with an expansion â„° : Rğ· Rğ‘2ğ·pix. This compressattendexpand pipeline keeps global attention efficient. Unlike the lossy bottleneck in VAEs, this mechanism only compresses the representation momentarily for the attention operation. Crucially, this compaction operates purely to reduce the computational overhead of self-attention; it does not compromise fine-grained details, because high-frequency information is preserved through residual connections and learned expansion layers that effectively bypass the pixeltoken bottleneck. 3.3. PixelDiT for Text-to-Image Generation MM-DiT block, image and text tokens form two streams with separate QKV projections. Text embeddings ğ‘¦ Rğµğ¿txtğ·txt are produced by frozen Gemma-2 encoder [35]. Following [36], we prepend concise system prompt to the user prompt before feeding the sequence to the text encoder. The resulting token embeddings are projected to the model width and used as the text stream in MM-DiT. Empirically, we find the semantic tokens from the patchlevel pathway are sufficient to convey textual intent to the pixel updates. The pixel-level pathway is therefore architecturally identical to the class-conditioned model: it operates on pixel tokens and is conditioned only through the semantic tokens together with the timestep. No text tokens are routed directly to the pixel stream. 3.4. Training Objectives We adopt the Rectified Flow formulation [37] in pixel space and train the model with its velocity-matching loss: â„’diff = Eğ‘¡,ğ‘¥,ğœ€ [ğ‘“ğœƒ(ğ‘¥ğ‘¡, ğ‘¡, ğ‘¦) ğ‘£ğ‘¡2 2 ]. (8) We extend the patch-level pathway with multi-modal DiT (MM-DiT) blocks [3] that fuse text and image semantics while leaving the pixel-level pathway unchanged. In each Following [31], we include an alignment objective that encourages mid-level patch-pathway tokens to agree with features from frozen DINOv2 encoder [38]. The overall 5 PixelDiT: Pixel Diffusion Transformers for Image Generation Figure 4 Qualitative results on ImageNet 256 256 using PixelDiT-XL. We use classifier-free guidance scale ğ›¼cfg = 4.0. objective is â„’ = â„’diff + ğœ†repa â„’repa. We use the same formulation for classand text-conditional models. 1 105 for the remainder. We apply gradient clipping at 1.0 initially and 0.5 thereafter for stability. 4. Experiments We evaluate the effectiveness of PixelDiT through extensive experiments. To demonstrate the scalability of our approach, we instantiate PixelDiT with three model sizes: Base (B), Large (L), and Extra Large (XL) for experiments on ImageNet. The detailed configurations are summarized in Table 2. Unless otherwise specified, we use PixelDiTXL as the default model for all experiments on ImageNet."
        },
        {
            "title": "Config",
            "content": "ğ‘ ğ‘€ ğ· ğ·pix Heads Params (M)"
        },
        {
            "title": "12\nPixelDiT-B\nPixelDiT-L\n22\nPixelDiT-XL 26",
            "content": "2 4 4 768 1024 1152 16 16 16 12 16 16 184 569 797 Table 2 Model configurations for PixelDiT variants for experiments on ImageNet 256256. ğ‘ and ğ‘€ denote the depth of the patch-level and pixel-level pathways, respectively. ğ· and ğ·pix represent the hidden dimension of the patch-level and pixel-level pathways. 4.1. Implementation Details Class-conditioned Image Generation. We follow the training setup of [5] and train our model on ImageNet1K [39] at 256256 resolution (denote as ImageNet 256256) for class-conditioned generation tasks. For the representation alignment objective, we set ğœ†repa=0.5 and apply alignment at the eighth block of the patch-level pathway. Throughout diffusion training, we use logit-normal sampling [3], EMA with decay 0.9999, AdamW with betas (0.9, 0.999), batch size of 256, and bfloat16 mixed precision. We train the model with constant learning rate of 1 104 for the first 160 epochs, then step down to Text-to-Image Generation. We use Gemma-2 [35] as the text encoder and incorporate the MM-DiT conditioning design [3] on the patch-level pathway. Our PixelDiT-T2I uses hidden size 1536, patch-level pathway depth ğ‘ =14, and pixel-level pathway depth ğ‘€ =2. We collect approximately 26M imagetext pairs at 10242 resolution with various aspect ratios to train our model. We first pre-train the model from scratch at 512 512 resolution for 400K iterations with AdamW (learning rate 1 104; betas (0.9, 0.999)), batch size 1,024, gradient clipping 0.5, and the shifting strategy [3] with shift value ğ›¼=3.0. We then finetune the model at 10242 resolution for another 100K iterations with AdamW (learning rate 2 105), batch size 768, higher shift value ğ›¼=4.0, and gradient clipping 0.1. Evaluation Settings on ImageNet. Following ADM [27], we report FID (gFID), sFID, Inception Score, and PrecisionRecall on 50K samples. On ImageNet 256256, we use guidance scale 3.25 with an interval [40] [0.1, 1.0] for the 80-epoch checkpoint. All other evaluations use guidance 2.75 with an interval [0.1, 0.9], unless otherwise stated. Evaluation Settings for Text-to-Image. We evaluate PixelDiT-T2I at 512 512 and 10242 resolutions on GenEval [41] and DPG-Bench [42] using 533 and 1,065 prompts, respectively. Unless noted, we use fixed guidance scale 4.5 and the shift value aligns with the training setting at corresponding resolutions. Sampling at Inference. For both class-conditioned and text-conditional image generation tasks, we use FlowDPMSolver [36], modified DPMSolver++ [43] in the Rectified Flow formulation, with bfloat16 precision. By default, we perform sampling with 100 steps on ImageNet and 25 steps for text-to-image generation. 6 PixelDiT: Pixel Diffusion Transformers for Image Generation (a) Patch-size ablations for B/L/XL models on ImageNet 256256. (b) Comparison at fixed patch size. Figure 5 Convergence analysis of PixelDiT on ImageNet 256256. (a) gFID vs. training iterations for B, L, and XL models with varying patch sizes. (b) Comparison of B/L/XL models at fixed patch size ğ‘=16. Methods 512 512 resolution PixArt-ğ›¼ [44] PixArt-Î£ [45] PixelFlow [17] PixNerd [12] PixelDiT-T2I 1024 1024 resolution PixArt-Î£ [45] LUMINA-Next [46] SDXL [47] Playground v2.5 [48] Hunyuan-DiT [49] DALLE 3 [10] FLUX-dev [2] PixelDiT-T2I Params (B) GenEval DPG Throughput (samples/s) 0.6 0.6 0.9 1.2 1.3 0.6 2.0 2.6 2.6 1.5 - 12. 1.3 0.48 0.52 0.60 0.73 0.78 0.54 0.46 0.55 0.56 0.63 0.67 0.67 0.74 71.6 79.5 77.9 80. 83.7 80.5 74.6 74.7 75.5 78.9 83.5 84.0 83.5 1.5 1.5 0.05 1.04 1.07 0.4 0.12 0.15 0.21 0.05 - 0. 0.33 Table 3 Comprehensive comparison of our method with text-to-image approaches. We highlight the best and second-best entries. indicates pixel-space diffusion models. 4.2. Class-conditioned Image Generation on ImageNet As shown in Table 1, at 320 epochs, our PixelDiT-XL obtains gFID of 1.61, surpassing recent pixel-space models including PixelFlow-XL (gFID 1.98), PixNerdXL (gFID 1.93), and EPG (gFID 2.04) by large margin, showing very significant improvement in image quality. Besides fidelity, PixelDiT-XL exhibits stronger diversityfaithfulness trade-off, achieving recall of 0.64 (vs. 0.60 for PixelFlow-XL), with competitive precision (0.78). Notably, PixelDiT-XL converges quickly: after only 80 epochs it already reaches gFID 2.36 and IS 282.3, outperforming classical pixel diffusion (ADM-U, gFID 4.59 at 400 epochs) and autoregressive baselines (JetFormer, gFID 6.64), indicating that structuring pixel modeling accelerates convergence. Note that SiD2 [19] reports gFID of 1.38 at 256 256, but its model size, training epochs, IS, and Precision/Recall are unknown. For fairness, we include its gFID for reference but exclude it from Table 1. While latent-space models such as REPA and RAE achieve lower gFID, PixelDiT-XL narrows the gap without any pretrained autoencoder, demonstrating the potential of pixel-space diffusion models in generating high-quality images. This is further supported by qualitative results in Figure 4. PixelDiT-XL produces sharp textures, coherent object boundaries, and reasonable global structures, showing superiority of our dual-level architecture design. 4.3. Text-to-Image Generation Table 3 presents the quantitative results of PixelDiT-T2I on text-to-image generation task. At 512 512 resolution, PixelDiT-T2I attains GenEval score of 0.78 and DPG of 83.7, outperforming recent pixel-space models such as PixNerd and PixelFlow. At 10242 resolution, PixelDiTT2I achieves GenEval score of 0.74 and DPG of 83.5, surpassing many latent diffusion models on GenEval and remaining competitive on DPG. These results indicate that end-to-end pixel-space diffusion with dual-level design scales to text-to-image generation with strong text grounding and robust compositionality. Qualitatively, Figure 1(a) shows representative samples produced by PixelDiT-T2I, illustrating high-resolution synthesis at the megapixel scale and consistent multi-resolution generation. More visualizations are provided in the appendix. Efficiency. Throughput in Table 3 is measured with fp16 precision on single NVIDIA A100 GPU. PixelDiT-T2I reaches 1.07 samples per second at 512 512 resolution, exceeding pixel-space baselines PixelFlow and PixNerd. At 10242 resolution, our PixelDiT-T2I reaches 0.33 samples per second, comparable to latent diffusion models, despite denoising directly in pixel space. 4.4. Detail Preservation in Image Editing In Figure 1(b), we perform image editing with FlowEdit [1] and compare the results using latent diffusion models (Stable Diffusion 3, FLUX) and PixelDiT. While both latent models generate motorcycle to replace the bicycle, the small scene text on the wall is heavily distorted. To investigate the cause of the distortion, FigPixelDiT: Pixel Diffusion Transformers for Image Generation Model Components A: Vanilla DiT/16 A: + RoPE, RMSNorm B: + Dual-level, patch-wise AdaLN (no compaction) B: + Pixel Token Compaction C: + Pixel-wise AdaLN Epoch gFID 80 80 80 80 80 9.84 8.53 OOM 3.50 2.36 1.61 Table 4 Ablations of PixelDiT-XL on ImageNet 256256. Results start from Vanilla DiT and incrementally add architectural improvements and inference strategies. OOM indicates the dual-level variant without token compaction exceeds memory limits. Labels AC match the design schematic in Figure 3. Configuration GFLOPs PixelDiT-XL No Pixel Token Compaction No Pixel-Pathway Attention 311 82247 279 Epoch 80 IS FID Epoch 160 IS FID 2. 282.3 1.97 299.4 OOM 2.56 256. 2.22 281.7 Table 5 Computation and convergence analyses of pixel token compaction. We report GFLOPs alongside ImageNet 256256 metrics at 80 and 160 epochs. No Pixel Token Compaction removes the compressexpand pathway, resulting in out-of-memory (OOM) at our training scale. No Pixel-Pathway Attention ablates selfattention in all PiT blocks. ure 1(b) presents the direct reconstruction results by the latent models VAEs, which show that the scene text has already been distorted by the VAE encoder due to its lossy compression nature, and the error further accumulates in the latent diffusion process. In contrast, since PixelDiT is end-to-end trained in pixel space without relying on VAEs, it does not suffer from the lossy compression distortion. Therefore, it not only makes correct modifications, but also preserves the small scene text well, showing significant advantages on preserving fine details for image editing. 4.5. Ablation Study 4.5.1. Contribution of Core Components Table 4 quantifies the contribution of the proposed pixelmodeling components by comparing different model variants. Note that labels AC in Table 4 correspond to the schematic variants in Figure 3. Specifically, we use 30-layer, 16 16 patchified DiT that directly performs denoising in pixel space as the baseline model (Vanilla DiT/16). This baseline operates solely on patch tokens without dedicated pixel-level pathway, treating each 16 16 patch as high-dimensional vector. It obtains 9.84 gFID at 80 epochs. Introducing dual-level architecture without pixel token compaction causes global attention to scale quadratically with the number of pixels and leads to an out-of-memory condition (OOM). Adding pixel token compaction resolves this bottleneck by shortening the global-attention sequence from ğ»ğ‘Š pixels to ğ¿=( ğ» ğ‘ ) patches, yielding significant quality improvement to 3.50 gFID at the same 80-epoch budget. Incorporating pixel-wise AdaLN further aligns per-pixel updates with the semantic context produced by the patchlevel pathway, improving gFID to 2.36 at 80 epochs and to 1.61 at 320 epochs. The comparison between model variants A, B, and demonstrates the importance of each proposed component. More importantly, the comparison between our full PixelDiT model and the vanilla DiT/16 reveals that pixel-level token modeling plays key role in pixel genğ‘ )( ğ‘Š erative models. Without pixel modeling, i.e., the visual content is only learned at the patch level, it will be challenging for the model to learn the fine details, and the visual quality will degrade significantly. 4.5.2. Analysis on Pixel Token Compaction Token compaction is essential for making pixel-space training feasible. global attention over ğ‘ =ğ» ğ‘Š pixel tokens incurs ğ‘‚(ğ‘ 2) memory and ğ‘‚(ğ‘ 2ğ·) FLOPs, producing billions of attention entries even at 256 256 resolution, as reflected by the 82,247 GFLOPs reported for this variant in Table 5. Grouping pixels into ğ‘ ğ‘ patches using pixel token compaction reduces the sequence length to ğ¿=ğ‘/ğ‘2, yielding ğ‘4-fold reduction in attention cost. To analyze the role of attention in the pixel-level pathway, we include No Pixel-Pathway Attention ablation that removes the attention and only keeps pixel-wise AdaLN and an MLP at the pixel level. As shown in Table 5, while this variant reduces GFLOPs, it is consistently inferior to our full PixelDiT model across different training iterations (e.g., from 80 to 160 epochs), with visible degradation in gFID and IS. This indicates that compact global attention is necessary to align local updates with global context. 4.5.3. Impact of Model Size and Patch Size We investigate the impact of the patch size ğ‘ on the performance of models at different scales: PixelDiT-B, PixelDiTL, and PixelDiT-XL. For all evaluations, we use the same CFG guidance scale 3.25 with interval [0.10, 1.00]. We evaluate patch sizes of 4, 8, 16, and 32 on ImageNet 256256; Figure 5(a) visualizes the resulting convergence behavior. For the base model, moving from ğ‘=32 to ğ‘=16 and ğ‘=4 substantially accelerates convergence: at 200K iterations gFID drops from 48.5 (B/32) to 15.1 (B/16) and 6.7 (B/4), and B/4 ultimately reaches 3.4 gFID at 500K iterations. Larger models follow similar trend, but the benefit of very small patches diminishes with scale. For PixelDiT-L, using ğ‘=8 rather than ğ‘=16 improves gFID only modestly (from 2.72 to 2.15 at 300K iterations), and for PixelDiT-XL the gap between ğ‘=8 and 8 PixelDiT: Pixel Diffusion Transformers for Image Generation ğ‘=16 essentially vanishesboth configurations converge to gFID near 2.0. These results highlight clear trade-off: smaller patches yield better image quality or faster convergence but incur quadratic cost in sequence length, and the relative gain shrinks as the model capacity increases. In practice, we therefore use ğ‘=16 as the default patch size for PixelDiT-XL, which offers near-optimal quality at substantially lower compute. To analyze the standalone effect of model size, Figure 5(b) directly compares B, L, and XL variants at fixed patch size ğ‘=16. Scaling the model yields consistent gains across the entire training trajectory: at 200K iterations, gFID improves from 15.1 (B/16) to 4.95 (L/16) and 2.95 (XL/16), and at 1M iterations XL/16 reaches 1.94 gFID compared to roughly 2.1 for L/16. Thus, increasing capacity improves both image quality and the speed at which given quality level is reached, demonstrating the scalability of PixelDiT. We leave more ablation studies to the Appendix. 5. Conclusion In this work, we revisited diffusion modeling in pixel space and showed that, with appropriate architectural design, pixel-space Diffusion Transformers can achieve high fidelity and efficiency without relying on pretrained autoencoder. PixelDiT factors pixel modeling into duallevel transformer design and introduces pixel-wise AdaLN and pixel token compaction to decouple global semantics from per-pixel token learning while keeping attention affordable. Experiments on class-conditioned image generation and text-to-image generation tasks demonstrate that this design closes much of the gap between latent-space and pixel-space methods and yields strong performance at high resolutions. While pixel-space diffusion incurs higher computational costs than latent approaches due to the raw data dimensionality, our work narrows this efficiency gap. Overall, PixelDiT highlights that the main barrier to practical pixelspace diffusion is not the representation space, but the absence of efficient pixel modeling architectures. We hope this perspective will inspire more fundamental research on pixel-space generative modeling."
        },
        {
            "title": "References",
            "content": "[1] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. In ICCV, pages 1972119730, 2025. [2] Black Forest Labs. Flux. https://github.com/blackforest-labs/flux, 2024. [3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [5] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [6] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [7] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. [8] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. In ICCV, 2025. [9] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [10] OpenAI. Dalle-3, 2023. [11] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. [12] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. [13] Michael Tschannen, AndrÃ© Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. In ICLR, 2025. [14] Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, and Rui Zhu. Farmer: Flow autoregressive transformer over pixels. arXiv preprint arXiv:2510.23588, 2025. [15] Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, and Xiangxiang Chu. Advancing endto-end pixel space generative modeling via self-supervised pre-training. arXiv preprint arXiv:2510.12586, 2025. [16] Kaiming He Tianhong Li. Back to basics: Let dearXiv preprint noising generative models denoise. arXiv:2511.13720, 2025. [17] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 9 PixelDiT: Pixel Diffusion Transformers for Image Generation [18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. [19] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. In CVPR, 2025. [20] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [21] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. [22] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In ICLR, 2025. [23] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space, 2025. [24] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In ICML, 2025. [25] Yongsheng Yu, Haitian Zheng, Zhifei Zhang, Jianming Zhang, Yuqian Zhou, Connelly Barnes, Yuchen Liu, Wei Xiong, Zhe Lin, and Jiebo Luo. Zipir: Latent pyramid diffusion transformer for high-resolution image restoration. arXiv preprint arXiv:2504.08591, 2025. [26] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. [27] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. [28] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer, 2025. [29] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher Pal, and Marc Aubreville. WÃ¼rstchen: An efficient architecture for large-scale text-to-image diffusion models. In ICLR, 2024. [30] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. TMLR, 2023. [31] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [32] Axel Sauer, Katja Schwarz, and Andreas Geiger. StyleganIn SIGxl: Scaling stylegan to large diverse datasets. GRAPH, 2022. [33] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In ICML, 2023. [34] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. NeurIPS, 36, 2024. [35] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Improving open Alexandre RamÃ©, et al. Gemma 2: arXiv preprint language models at practical size. arXiv:2408.00118, 2024. [36] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In ICLR, 2025. [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [38] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In TMLR, 2023. [39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. [40] Tuomas KynkÃ¤Ã¤nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. [41] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [42] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [44] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-ğ›¼: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [45] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-ğœ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. 10 PixelDiT: Pixel Diffusion Transformers for Image Generation [46] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making luminaarXiv preprint t2x stronger and faster with next-dit. arXiv:2406.18583, 2024. [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [48] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [49] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [50] Loshchilov. Decoupled weight decay regularization. In ICLR, 2019. 11 PixelDiT: Pixel Diffusion Transformers for Image Generation A. Architecture and System Details A.1. Summary of Model Size To study the impact of model size, we evaluate the base (B), large (L), and extra-large (XL) variants of PixelDiT on ImageNet 256256. Tables 6 and 7 summarize the detailed architectural specifications and training settings for B, L, XL and T2I variants. Note that the default configuration of all experiments in the main paper is PixelDiT-XL. If not otherwise specified, we use the XL configuration for all ImageNet 256256 experiments in this appendix. A.2. Text-to-Image Architecture with MM-DiT Figure 6 illustrates the T2I variant of PixelDiT, where the patch-level pathway is extended with MM-DiT blocks [3] to fuse text embeddings, while the pixel-level pathway remains unchanged. The figure emphasizes stream separation, conditioning flow, and the pixel-wise modulation interface used by the pixel-level pathway. B. Solvers and Guidance Scales B.1. Ablation of Solvers We compare three diffusion samplers for denoising on ImageNet 256256: FlowDPMSolver [36, 43], Euler, and Heun, all run for 100 steps without classifier-free guidance. Figure 7 plots gFID, sFID, Inception Score (IS), precision, and recall as training progresses from 100K to 1,200K iterations. Across most of the training trajectory, FlowDPMSolver achieves lower or comparable gFID and sFID than Euler and Heun, with the gap particularly pronounced in the lowand mid-epoch regimes (up to roughly 12 gFID points around 400K800K iterations). FlowDPMSolver maintains the best overall trade-off: it matches or exceeds the competing solvers on sFID and IS while keeping precision and recall high. These results motivate our choice of FlowDPMSolver as the default sampler for all main ImageNet and text-to-image evaluations. B.2. Inference Steps We further analyze the impact of the number of inference steps when using FlowDPMSolver. Figure 8 shows gFID for PixelDiT-XL at three training stages (100K, 400K, and 1.6M iterations) as we vary the sampling budget from 25 to 100 steps. At 100K iterations the model is undertrained and additional steps give only modest improvements, with gFID remaining in the 67 range. Once the model has learned reasonable global structure (400K iterations), increasing the budget from 25 to 50 steps reduces gFID from about 3.19 to 2.51, and 100 steps further improves it to 2.36. For the fully converged checkpoint at 1.6M iteraArchitecture Input dim. Patch-level depth ğ‘ Pixel-level depth ğ‘€ Hidden size ğ· Heads Pixel hidden size ğ·pix Patch size ğ‘ #Params (M) PixelDiT-B PixelDiT-L PixelDiT-XL 12 2 768 12 16 16 184 256 256 3 22 4 1024 16 16 16 26 4 1152 16 16 16 797 Representation Alignment [31] Alignment depth (patch-level) Loss weight ğœ†repa Alignment encoder 8-th layer 0.5 Frozen DINOv2 [38] Optimization Training iteration Batch size Timestep reweighting Optimizer EMA decay Class drop prob. Gradient clipping Learning rate Weight decay Inference Sampler Sampling steps CFG scale (80-ep) CFG interval (80-ep) CFG scale (320-ep) CFG interval (320-ep) Guidance for Figs. 5, 810 1.6M 800K 1M 256 Logit-normal [3] AdamW [50], ğ›½1=0.9, ğ›½2=0.999 0.9999 0.1 1.0 1e-4 0 1.0 1e-4 0 1.0 0.5 1e-4 1e-5 0 3.25 FlowDPMSolver [36, 43] 100 3.25 [0.10, 1.00] 3.25, [0.10, 1.00] for all checkpoints 2.75 [0.10, 0.90] 3.25 Table 6 Detailed architecture and training configurations for PixelDiT B/L/XL models on ImageNet-256."
        },
        {
            "title": "Hyperparameter",
            "content": "Architecture Input dim. Patch-level depth ğ‘ Pixel-level depth ğ‘€ Hidden size ğ· Heads Pixel hidden size ğ·pix Patch size ğ‘ #Params (M) PixelDiT-T2I 5122 3 10242 3 14 2 1536 24 16 16 1311 Representation Alignment [31] Alignment depth (patch-level) Loss weight ğœ†repa Alignment encoder 6-th layer 0.5 Frozen DINOv2 [38] 0.0 (disabled) Optimization Training iteration Batch size Learning rate Gradient clipping Text drop prob."
        },
        {
            "title": "Inference\nSampler\nSampling steps\nCFG scale",
            "content": "400K 1024 1e-4 0.5 100K 768 2e-5 0.1 0.1 FlowDPMSolver [36, 43] 25 4.5 Table 7 Implementation details for PixelDiT-T2I model. PixelDiT: Pixel Diffusion Transformers for Image Generation Figure 6 T2I architecture of PixelDiT with MM-DiT blocks on the patch-level pathway. The pixel-level pathway performs dense per-pixel modeling conditioned on semantic tokens. Figure 7 Comparison of FlowDPMSolver, Euler, and Heun samplers on ImageNet 256256 with 100 inference steps and no classifier-free guidance. FlowDPMSolver achieves the best combined trade-off between fidelity in gFID and sFID and diversity in IS, precision, and recall, which motivates its use as our default sampler. tions, 25 steps already achieve gFID 2.50, but 5075 steps lower it to around 1.761.74, and 100 steps obtain the best score of approximately 1.61 gFID. Overall, more inference steps consistently benefit well-trained models, though the marginal gain beyond 50 steps becomes small. In practice we therefore use 100 steps for class-conditioned ImageNet experiments to match the strongest quality, and 25 steps for text-to-image generation where sampling latency is more critical. guidance: scale of 2.75 active on the interval [0.10, 0.90] achieves the best gFID of 1.61 together with strong recall of 0.64, whereas both larger and smaller scales yield at most marginal IS gains at the cost of higher gFID. In the main paper we therefore adopt guidance scale of 3.25 with interval [0.10, 1.00] for the 80-epoch ImageNet 256256 results, and scale of 2.75 with interval [0.10, 0.90] for the 320-epoch checkpoint that underpins our best reported scores. B.3. Guidance Scale and Interval C. Model Architecture Design We report the classifier-free guidance (CFG) settings used for PixelDiT-XL at 80 and 320 epochs on ImageNet 256256. Table 8 lists the CFG scale, active time interval, and the resulting gFID, sFID, IS, precision, and recall. For the 80-epoch checkpoint, the best gFID is 2.36, obtained with relatively strong guidance scale of 3.25 applied over the entire denoising trajectory from ğ‘¡=0.10 to ğ‘¡=1.00. Increasing the scale to 3.50 or decreasing it to 3.00 slightly worsens gFID while mainly trading off IS and recall, and restricting the active interval to [0.10, 0.95] or [0.10, 0.90] does not lead to better performance. For the 320-epoch checkpoint, the optimum shifts toward milder C.1. Ablation on Depth and We analyze how to allocate depth between the patch-level pathway (ğ‘ layers) and the pixel-level pathway (ğ‘€ layers) under fixed total budget of roughly ğ‘ +ğ‘€ 30 layers. Figure 9 shows convergence curves for several (ğ‘, ğ‘€ ) configurations evaluated on ImageNet 256256. All evaluations use the same CFG guidance scale 3.25 with interval [0.10, 1.00]. Introducing even shallow pixel pathway (e.g., ğ‘ =28, ğ‘€ =2) dramatically improves convergence and reduces final gFID to around 2.1. Our default configuration (ğ‘ =26, ğ‘€ =4) provides the best overall behavior: 13 PixelDiT: Pixel Diffusion Transformers for Image Generation Model Epochs Training Steps CFG Interval gFID sFID IS Prec. Rec. PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL PixelDiT-XL 80 80 80 80 80 80 320 320 320 320 320 320 400K 400K 400K 400K 400K 400K 1600K 1600K 1600K 1600K 1600K 1600K 3.25 3.50 3.00 2.75 3.25 3.25 2.75 2.75 2.75 2.50 2.50 2.50 [0.10, 1.00] [0.10, 1.00] [0.10, 1.00] [0.10, 1.00] [0.10, 0.95] [0.10, 0.90] [0.10, 0.90] [0.10, 0.95] [0.10, 1.00] [0.10, 0.95] [0.10, 0.90] [0.10, 1.00] 2.36 2.60 2.60 2.76 2.73 2. 1.61 1.65 1.66 1.69 1.71 1.71 5.11 5.07 5.06 5.17 5.24 5.32 4.68 4.64 4.60 4.68 4.60 4.62 282.3 305.2 277.6 259.2 285.8 285.4 292.7 293.8 294.2 276.9 275.2 277.9 0.80 0.82 0.80 0.79 0.80 0. 0.78 0.77 0.78 0.77 0.77 0.77 0.57 0.57 0.58 0.59 0.58 0.58 0.64 0.64 0.64 0.65 0.65 0.65 Table 8 CFG settings and results for PixelDiT-XL on ImageNet 256256. Model Params (B) Overall Objects Single Two Counting Colors Position Color Attribution 512 512 resolution PixArt-ğ›¼ PixArt-Î£ PixelFlow [17] PixNerd [12] PixelDiT-T2I 1024 1024 resolution LUMINA-Next [46] SDXL [47] PlayGroundv2.5 [48] Hunyuan-DiT [49] DALLE3 [10] FLUX-dev [2] PixelDiT-T2I 0.6 0.6 0.9 1.2 1.3 2.0 2.6 2.6 1.5 - 12.0 1. 0.48 0.52 0.60 0.73 0.78 0.46 0.55 0.56 0.63 0.67 0.67 0.74 0.98 0.98 - 0.97 1. 0.92 0.98 0.98 0.97 0.96 0.99 1.00 0.50 0.59 - 0.86 0.94 0.46 0.74 0.77 0.77 0.87 0.81 0. 0.44 0.50 - 0.44 0.70 0.48 0.39 0.52 0.71 0.47 0.79 0.55 0.80 0.80 - 0.83 0. 0.70 0.85 0.84 0.88 0.83 0.74 0.88 0.08 0.10 - 0.71 0.53 0.09 0.15 0.11 0.13 0.43 0.20 0. 0.07 0.15 - 0.53 0.65 0.13 0.23 0.17 0.30 0.45 0.47 0.68 Table 9 GenEval category-wise results at 512 512 and 1024 1024 for text-to-image generation. Overall is the unweighted mean over Single Object, Two Objects, Counting, Colors, Position, and Color Attribution. it reaches gFID 2.34 by 300K iterations and continues to improve to 1.94 at 1M iterations, outperforming both the shallower pixel pathway (ğ‘ =28, ğ‘€ =2) and the deeper one (ğ‘ =22, ğ‘€ =8). The latter attains similar final gFID but converges more slowly in early epochs. These trends indicate that dedicating moderate but not excessive number of layers to the pixel-level pathway is crucial for efficient pixel modeling and underpins the strong ImageNet results reported in the main paper. C.2. Study on Pixel Token Compaction (PTC) Rate We investigate the effectiveness of Pixel Token Compaction (PTC) by varying the compaction rate. Recall that our default patch size is ğ‘. Without compaction, the pixel-level pathway would process sequence of length ğ» ğ‘Š . With standard compaction (denoted as Seq Len ğ¿ (1) or Base), the ğ‘ ğ‘ pixels in patch are compressed into single token, reducing the sequence length to ğ¿ = (ğ»/ğ‘) (ğ‘Š/ğ‘). We explore relaxing this compression by allowing the compacted sequence length to be multiples of the base length ğ¿. Specifically: Seq Len ğ¿ (1): The default setting. Compresses ğ‘2 pixels to 1 token. Compression rate: ğ‘2. Seq Len 2ğ¿ (2): Compresses ğ‘2 pixels to 2 tokens. Compression rate: ğ‘2/2. Seq Len 4ğ¿ (4): Compresses ğ‘2 pixels to 4 tokens. Compression rate: ğ‘2/4. Figure 10 presents the ablation results. Across training, all three settings converge to strong gFID values around 2.0, while the model with the most aggressive compression (Seq Len 1) obtains slightly better results. For example, at 300K iterations the three configurations obtain gFID of roughly 2.34 (1), 2.38 (2), and 2.43 (4), respectively. At 1M iterations the 1 variant is further improved to 1.94 while the longer sequences plateau slightly higher. The result suggests that, for the pixel-level pathway, compact representation is sufficient to capture the residual information needed for texture refinement. This could be due to the redundant nature of the pixel-space tokens. Inter14 PixelDiT: Pixel Diffusion Transformers for Image Generation Model Params (B) Overall Global Entity Attribute Relation Other 512 512 resolution PixArt-ğ›¼ [44] PixArt-Î£ [45] PixelFlow [17] PixNerd [12] PixelDiT-T2I 1024 1024 resolution LUMINA-Next [46] SDXL [47] PlayGroundv2.5 [48] Hunyuan-DiT [49] PixArt-Î£ [45] DALLE3 [10] FLUX-dev [2] PixelDiT-T2I 0.6 0.6 0.9 1.2 1.3 2.0 2.6 2.6 1.5 0.6 - 12.0 1. 71.6 79.5 77.9 80.9 83.7 74.6 74.7 75.5 78.9 80.5 83.5 84.0 83.5 81.7 87.5 - 80.5 88. 82.8 83.3 83.1 84.6 86.9 91.0 82.1 83.0 80.1 87.1 - 87.9 90.9 88.7 82.4 82.6 80.6 82.9 89.6 89.5 88. 80.4 86.5 - 87.2 87.6 86.4 80.9 81.2 88.0 88.9 88.4 88.7 87.8 81.7 84.0 - 91.3 89. 80.5 86.8 84.1 74.4 86.6 90.6 91.1 91.2 76.5 86.1 - 72.8 88.5 81.8 80.4 83.5 86.4 87.7 89.8 89.4 89. Table 10 DPG-Bench category-wise results at 512 512 and 1024 1024 resolutions for text-to-image generation. Figure 8 Effect of the number of FlowDPMSolver inference steps on gFID for PixelDiT-XL at different training stages on ImageNet 256256. Increasing the number of steps is most beneficial once the model is moderately or fully trained (400K and 1.6M iterations), with diminishing returns beyond 50 steps; we adopt 100 steps as the default for ImageNet experiments. estingly, the results indicate that lower compression rates do not necessarily lead to better image quality. We suspect that longer, redundant token sequence and larger attention space can be more challenging to optimize and slower to converge under the same training setting. Using more tokens in the pixel-level pathway may require carefully adjusted training settings to unlock its full potential. Since the attention cost grows almost quadratically with the compressed token length, the model with 1 compaction performs similarly to other configurations, thus we adopt it as the default setting throughout the paper for efficiency. Figure 9 Ablation of depth allocation between patch-level (ğ‘ ) and pixel-level (ğ‘€ ) pathways on ImageNet 256256. Our chosen configuration (ğ‘ =26, ğ‘€ =4), highlighted in brown, offers the best trade-off between early convergence and final image quality. D. Benchmark Details D.1. DPG-Bench and GenEval Category Breakdown Tables 9 and 10 report category-wise results for GenEval and DPG-Bench at 5122 and 10242 resolutions. On GenEval at 5122, PixelDiT-T2I outperforms prior pixelspace models. At 10242 resolution, PixelDiT-T2I matches or surpasses several widely used latent diffusion systems despite using fewer parameters, and maintains competitive performance across all individual categories. On DPGBench, PixelDiT-T2I ranks among the top-performing models while maintaining balanced scores across categories. These detailed breakdowns corroborate that PixelDiT delivers strong textimage alignment and compositional reasoning, closing much of the gap to heavily engineered latent diffusion models. 15 PixelDiT: Pixel Diffusion Transformers for Image Generation Methods Params Latent Generative Models DiT-XL/2 [5] SiT-XL/2 [6] REPA, SiT-XL/2 [31] LightningDiT-XL/2 [7] DDT-XL/2 [28] RAE, DiTDH-XL/2 [11] 675+49M 675+49M 675+49M 675+49M 675+49M 839+415M Pixel Generative Models ADM-G [27] RIN [33] SiD, UViT/2 [18] VDM++, UViT/2 [34] SiD2, UViT/2 [19] SiD2, UViT/1 [19] PixelFlow-XL/4 [17] PixNerd-XL/16 [12] JiT-G/16 [16] PixelDiT-XL (ours) 559M 320M 2B 2B N/A N/A 677M 700M 2B 797M GFLOPs (multi-add = 2 FLOPs) FID 238 238 238 238 238 292 2240 668 1110 1110 274 1306 5818 268 766 2.27 2.06 1.42 1.35 1.26 1.13 7.72 3.95 2.44 2.12 1.73 1.38 1.98 2.15 1.82 1.61 Table 11 Compute comparison on ImageNet 256256. We report model parameters, GFLOPs per forward pass, and FID under our convention that one multiply-add equals two FLOPs. such as human hands and intricate architectural scenes. In future work, we will address these challenging cases by scaling up the model capacity and collecting more highquality data. Figure 10 Ablation of Pixel Token Compaction rates on ImageNet 256256. The curves compare three postcompaction sequence lengths, denoted as Seq Len ğ¿ (1), Seq Len 2ğ¿ (2), and Seq Len 4ğ¿ (4), where ğ¿ is the number of patch tokens after compaction. Seq Len ğ¿ (1) corresponds to our default configuration in which each ğ‘2 pixel block is compacted into single token. E. FLOPs Estimation and Comparison We estimate GFLOPs for single forward pass at 2562 input resolution. For the GFLOPs of prior work, we reuse the numbers reported in their papers [16, 19] and convert them to unified convention where one multiply-add counts as two FLOPs. Table 11 compares the compute cost and FID of PixelDiT-XL with representative latentspace and pixel-space generative models on ImageNet 256256. Latent models achieve very strong FIDs with around 240290 GFLOPs, whereas many pixel models require several hundred to several thousand GFLOPs to close this quality gap. In contrast, PixelDiT-XL obtains 1.61 FID with only 311 GFLOPs, offering superior image quality over the state-of-the-art pixel generators and closing much of the gap between the best latent models, while using compute that is close to latent models and substantially less than most prior pixel-space models. F. Qualitative Examples We include additional qualitative results for ImageNet 256256 single-class-conditioned image generations, as shown in Figures 1522, together with high-resolution (approximately 10242) text-to-image generation results in Figures 1114, illustrating the visual quality, diversity, and prompt alignment achieved by PixelDiT. G. Limitations Due to the limited model capacity and insufficient highquality training data, our PixelDiT-T2I text-to-image model (1.3B parameters) sometimes struggles to generate objects that are both geometrically and texturally complex, PixelDiT: Pixel Diffusion Transformers for Image Generation group of figures are gathered at table near trellised terrace, overlooking river with rowers and boat. The scene, rendered in an Impressionistic style, employs loose brushstrokes and soft, diffused light. The man in the foreground, dressed in blue and white striped shirt, gestures casually, holding what appears to be cigarette. The table is set with wine bottles and glasses, indicating leisurely gathering. Through the trellis, glimpse of the river reveals rowers in action, with lone figure in boat further out. The greenery of the trellis and the river landscape blend, contributing to the paintings overall sense of depth and atmospheric perspective. woman stands on rooftop at dusk, overlooking cityscape illuminated by twinkling lights. Her curly hair frames serious expression, and she leans casually against the rooftop railing. The soft lighting of the sunset blends with the artificial glow of the city, creating warm yet muted atmosphere. The out-of-focus background emphasizes the vastness of the urban landscape, dotted with skyscrapers and distant roads. Her dark jacket adds layer of contrast, focusing the viewers attention on her face. The overall style evokes sense of urban solitude and reflection against backdrop of vibrant cityscape. golden-hued lioness rests serenely in sunlit grassy field. The lioness, bathed in the warm glow of the setting or rising sun, is positioned in the foreground, lying comfortably on small mound of earth covered with dry grass. Her paws are outstretched, relaxed and slightly crossed. The background is soft, blurred mix of green and golden foliage, hinting at savanna-like landscape. The golden light emphasizes the texture of her fur and highlights the contours of her face, adding sense of calm and natural grandeur to the scene. Figure 11 Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt used to generate it. 17 PixelDiT: Pixel Diffusion Transformers for Image Generation Photo of person moving with motion blur, shot with Leica M6 and VISION3 500T Color Negative Film, reminiscent of Wong Kar Tai film set. young man wearing 18th century noble clothing in blues and pinks and standing in front of the green grass with white flowers. Portrait shot of pretty woman, latex suit fashion, contrasting background, fashion magazine cover, 35mm kodachrome. portrait of human growing colorful flowers from her hair. Hyperrealistic oilpainting. knitted cat-whale plush toy on rug in warm sunlit living room, cozy decor. Behold the Joymonger, photorealistic, 1990s, hyper realism, extremely detailed. Figure 12 Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt used to generate it. 18 PixelDiT: Pixel Diffusion Transformers for Image Generation young woman with striking eyes gazes directly at the viewer, set against soft, blurred background. Her long, auburn hair falls loosely around her shoulders, partially obscuring one side of her face, while vertically striped, collared shirt completes her casual yet elegant look. The lighting is warm and natural, emphasizing the subtle contours of her face and the slight flush in her cheeks. The photograph captures blend of relaxed confidence and introspective beauty, rendered with soft focus that lends dreamlike quality to the image. The surrounding environment is intentionally muted, ensuring that the woman remains the primary focal point. The color palette leans towards earthy tones, enhancing the overall warmth and approachability of the portrait. Close-up portrait of beautiful Baltic model wearing white flower-shaped earrings, emphasis on the earrings, everything is in full focus, pores and skin imperfections are visible, neutral lighting from large studio softbox, natural beauty, professional studio shooting. funny Candid photo, cat sleeping across amans eyes as he sleeps on his back,blocking his face, man is 25 years old,neck length frizzy black-brown hair,very unruly hair, stubble, wearing blackdress pants, long sleeve white button upshirt, socks, laying on post bed. Ultra-realistic photo of an anthropomorphic chili pepper with glossy red surface, smiling with human teeth and wearing black sunglasses. Surrounded by realistic flames, the pepper is sharply in focus, while the fiery background is slightly blurred. Tiny fluffy lamb standing on fingertip, ultra-detailed wool texture, soft natural light. Colorful woolen parrot plush wearing ornate psychicstyle vest, detailed fabric textures. young woman in warm sunset light, soft shadows, long blonde hair, pink pajamas, calm expression. Figure 13 Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt used to generate it. 19 PixelDiT: Pixel Diffusion Transformers for Image Generation An image of very tired old man, fisherman, long beard, black background settings. The facial expression reflects wisdoms and test of time. snow-covered mountains rising above calm turquoise lake, their peaks perfectly mirrored in the water, framed by dense autumn-tinged pines. leopard hiding in the jungle, photorealistic portrait, captured with Canon EOS R5 camera and macro lens for detailed wildlife portraits. serene moment unfolds as dog leisurely navigates tranquil lavender field at sunset. The golden light enhances the beauty of the swaying purple blooms. Close-up shot of an African American man wearing blue beanie, against beige background, with vintage aesthetic, in the style of Kodak film photography. Two conjoined strawberry. The strawberrys are resting on undulating red-pink holoIntense graphic icy slush. Vaporwave. color. Ice textures. Solar aesthetic. Figure 14 Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt used to generate it. 20 PixelDiT: Pixel Diffusion Transformers for Image Generation Figure 15 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 291. CFG scale = 4.0. Figure 17 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 24. CFG scale = 4.0. Figure 16 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 259. CFG scale = 4.0. Figure 18 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 970. CFG scale = 4.0. 21 PixelDiT: Pixel Diffusion Transformers for Image Generation Figure 19 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 39. CFG scale = 4.0. Figure 21 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 373. CFG scale = 4.0. Figure 20 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 263. CFG scale = 4.0. Figure 22 Uncurated ImageNet 256256 PixelDiT-XL samples. Class 386. CFG scale = 4.0."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Rochester"
    ]
}