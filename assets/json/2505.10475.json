{
    "paper_title": "Parallel Scaling Law for Language Models",
    "authors": [
        "Mouxiang Chen",
        "Binyuan Hui",
        "Zeyu Cui",
        "Jiaxi Yang",
        "Dayiheng Liu",
        "Jianling Sun",
        "Junyang Lin",
        "Zhongxin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning."
        },
        {
            "title": "Start",
            "content": "2025-05-"
        },
        {
            "title": "Parallel Scaling Law for Language Models",
            "content": "Mouxiang Chen1,2, Binyuan Hui2,, Zeyu Cui2, Jiaxi Yang2, Dayiheng Liu2, Jianling Sun1, Junyang Lin2, Zhongxin Liu1, 1Zhejiang University, 2Qwen Team, Alibaba Group {chenmx,liu zx}@zju.edu.cn, binyuan.hby@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "It is commonly believed that scaling language models should commit significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inferencetime scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the models parallel computation during both training and inference time. We apply diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the outputs. This method, namely parallel scaling (PARSCALE), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose new scaling law and validate it through large-scale pre-training, which shows that model with parallel streams is similar to scaling the parameters by O(log P) while showing superior inference efficiency. For example, PARSCALE can use up to 22 less memory increase and 6 less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-theshelf pre-trained model into parallelly scaled one by post-training on small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning. 5 2 0 2 5 1 ] . [ 1 5 7 4 0 1 . 5 0 5 2 : r Figure 1: (1) Illustrations of our proposed parallel scaling (PARSCALE). (2) Parallel scaling laws for pre-training models on 42B tokens from Stack-V2 (Python subset). (3) Loss scaling curve with inference cost. Results are averaged from batch size {1, 2, 4, 8} and input + output tokens {128, 256, 512, 1024}. Work is partially done during internship in Qwen Team, Alibaba Group. Corresponding authors."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the rapid scaling of large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Llama Team, 2024; Qwen Team, 2025b) to narrow the gap towards Artificial General Intelligence (AGI). Mainstream efforts focus on parameter scaling (Kaplan et al., 2020), practice that requires substantial space overhead. For example, DeepSeek-V3 (Liu et al., 2024a) scales the model size up to 672B parameters, which imposes prohibitive memory requirements for edge deployment. More recently, researchers have explored inference-time scaling (OpenAI, 2024) to enhance the reasoning capability by scaling the number of generated reasoning tokens. However, inference-time scaling is limited to certain scenarios and necessitates specialized training data (DeepSeek-AI, 2025; Qin et al., 2024), and typically imposes significant time costs. For example, Chen et al. (2024b) find that the most powerful models can generate up to 900 reasoning tokens for trivial problems like 2+3=?. This motivates the question: Is there universal and efficient scaling approach that avoids excessive space and time costs? We draw inspiration from classifier-free guidance (CFG) (Ho & Salimans, 2022), widely used trick during the inference phase of diffusion models (Ho et al., 2020), with similar concepts also developed in the NLP community (Sanchez et al., 2024; Li et al., 2023). Unlike traditional methods that use single forward pass, CFG utilizes two forward passes during inference: it first performs normal forward pass to obtain the first stream of output, then perturbs the input (e.g., by discarding conditions in the input) to get second stream of output. The two streams are aggregated based on predetermined contrastive rules, yielding superior performance over single-pass outputs. Despite its widespread use, the theoretical guarantee of CFG remains an open question. In this paper, we hypothesize that the effectiveness of CFG lies in its double computation. We further propose the following hypothesis: Hypothesis 1. Scaling parallel computation (while maintaining the nearly constant parameters) enhances the models capability, with similar effects as scaling parameters. We propose proof-of-concept scaling approach called parallel scaling (PARSCALE) to validate this hypothesis on language models. The core idea is to increase the number of parallel streams while making the input transformation and output aggregation learnable. We propose appending different learnable prefixes to the input and feeding them in parallel into the model. These outputs are then aggregated into single output using dynamic weighted sum, as shown in Figure 1(1). This method efficiently scales parallel computation during both training and inference time by recycling existing parameters, which applies to various training algorithms, data, and tasks. Our preliminary theoretical analysis suggests that the loss of PARSCALE may follow power law similar to the Chinchilla scaling law (Hoffmann et al., 2022). We then carry out large-scale pre-training experiments on the Stack-V2 (Lozhkov et al., 2024) and Pile (Gao et al., 2021) corpus, by ranging from 1 to 8 and model parameters from 500M to 4.4B. We use the results to fit new parallel scaling law that generalizes the Chinchilla scaling law, as depicted in Figure 1(2). It shows that parallelizing into streams equates to scaling the model parameters by O(log P). Results on comprehensive tasks corroborate this conclusion. Unlike parameter scaling, PARSCALE introduces negligible parameters and increases only little space overhead. It also leverages GPU-friendly parallel computation, shifting the memory bottleneck in LLM decoding to computational bottleneck and, therefore, does not notably increase latency. For example, for 1.6B model, when scaling to = 8 using PARSCALE, it uses 22 less memory increase and 6 less latency increase compared to parameter scaling that achieves the same model capacity (batch size = 1, detailed in Section 3.3). Figure 1(3) illustrates that PARSCALE offers superior inference efficiency. Furthermore, we show that the high training cost of PARSCALE can be reduced by two-stage approach: the first stage employs traditional training with most of the training data, and PARSCALE is applied only in the second stage with small number of tokens. Based on this, we train 1.8B models with various and scale the training data to 1T tokens. The results of 21 downstream benchmarks indicate the efficacy of this strategy. For example, when scaling to = 8, it yields 34% relative improvement for GSM8K and 23% relative improvement for MMLU using exactly the same training data. We also implement PARSCALE on an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024), and demonstrate that PARSCALE is effective in both full and parameter-efficient fine-tuning settings. This also shows the viability of dynamic parallel scaling, which allows flexible adjustment of during deployment while freezing the backbone weights, to fit different application scenerios. Table 1 compares PARSCALE with other mainstream scaling strategies. Beyond introducing an efficient scaling approach for language models, our research also tries to address more fundamental question in machine learning: Is models capacity determined by the parameters or by the computation, and what is their individual contribution? Traditional machine learning models typically scale both parameters and computation simultaneously, making it difficult to determine their contribution ratio. The PARSCALE and the fitted parallel scaling law may offer novel and quantitative perspective on this problem. 2 Table 1: Comparisons of mainstream LLM scaling strategies. We subdivide parameter scaling into traditional Dense Scaling and Mixture-of-Expert (MoE) Scaling (Fedus et al., 2022) for comparison. Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference. Method Inference Time Inference Space Training Cost Specialized Strategy Dense Scaling MoE Scaling Inference-Time Scaling Parallel Scaling Moderate Low High Moderate High High Moderate Moderate Pre-training only Pre-training only Post-training Preor Post-training No Load balancing RL / reward data No We posit that large computing can foster the emergence of large intelligence. We hope our work can inspire more ways to scaling computing towards AGI and provide insights for other areas of machine learning. Our key findings in this paper can be summarized as follows: Scaling times of parallel computation is similar to scaling parameters by ratio of O(log P), and larger models reap greater benefits from PARSCALE. Reasoning-intensive tasks (e.g., coding or math) benefit more from PARSCALE, which suggests that scaling computation can effectively push the boundary of reasoning. PARSCALE offers superior inference efficiency compared to parameter scaling due to the effective use of memory, particularly suitable for low-resource edge deployment. The training cost of PARSCALE can be significantly alleviated through two-stage training strategy. PARSCALE remains effective with frozen main parameters for different P. This illustrates the potential of dynamic parallel scaling: switching to dynamically adapt model capabilities during inference. Our code and 67 trained model checkpoints are publicly available at https://github.com/QwenLM/ ParScale and https://huggingface.co/ParScale."
        },
        {
            "title": "2 Background and Methodology",
            "content": "Classifier-Free Guidance (CFG) CFG (Ho & Salimans, 2022) has become de facto inference-time trick in diffusion models (Ho et al., 2020), with similar concepts also developed in NLP (Sanchez et al., 2024; Li et al., 2023). At high level, these lines of work can be summarized as follows: given an input Rdi and trained model fθ : Rdi Rdo , where θ is the parameter and di, do are dimensions, we transform into bad version based on some heuristic rules (e.g., removing conditions), obtaining two parallel outputs fθ(x) and fθ(x). The final output gθ(x) is aggregated based on the following rule: gθ(x) = fθ(x) + (cid:0) fθ(x) fθ(x)(cid:1) . (1) Here, > 0 is pre-set hyperparameter. Intuitively, Equation (1) can be seen as starting from good prediction and moving steps in the direction away from bad prediction. Existing research shows that gθ(x) can perform better than the vanilla fθ(x) in practice (Saharia et al., 2022). Motivation In Equation (1), is simply degraded version of x, suggesting that gθ(x) does not gain more useful information than fθ(x). This raises the question: why is fθ(x) unable to learn the capability of gθ(x) during training, despite both having the same parameters? We hypothesize that the fundamental reason lies in gθ(x) having twice the computation as fθ(x). This inspires us to further expand Equation (1) into the following form: gθ(x) = w1 fθ(x1) + w2 fθ(x2) + + wP fθ(xP), where denotes the number of parallel streams. x1, , xP are distinct transformations of x, and w1, , wP are aggregation weights. We term Equation (2) as parallel scaling (PARSCALE) of the model fθ with streams. This scaling strategy does not require changing the structure of fθ and training data. In this paper, we focus on Transformer language models (Vaswani et al., 2017; Brown et al., 2020), and regard the stacked Transformer layers as fθ(). (2) Implementation Details and Pivot Experiments We apply Equation (2) in both training and inference time, and perform series of pivot experiments to determine the best input transformation and output aggregation strategies (refer to Appendix A). The findings revealed that variations in these strategies 3 minimally affect model performance; the significant factor is the number of computations (i.e., P). Finally, for input transformation, we employ prefix tuning (Li & Liang, 2021) as the input transformation, which is equivalent to using different KV-caches to distinguish different streams. For output aggregation, we employ dynamic weighted average approach, utilizing an MLP to convert outputs from multiple streams into aggregation weights. This increases about 0.2% additional parameters for each stream."
        },
        {
            "title": "3 Parallel Scaling Law",
            "content": "This section focuses on the in-depth comparison of scaling parallel computation with scaling parameters. In Section 3.1, we theoretically demonstrate that parallel scaling is equivalent to increasing parameters by certain amount. In Section 3.2, we validate this with practical scaling law through large-scale experiments. Finally, in Section 3.3, we analyze latency and memory usage during inference to show that parallel scaling is more efficient. 3.1 Theoretical Analysis: Can PARSCALE Achieve Similar Effects as Parameter Scaling? From another perspective, PARSCALE can be seen as an ensemble of multiple different next token predictions, despite the ensemble components sharing most of the parameters. Existing theory in literature finds that the ensembling performance depends on the diversity of different components (Breiman, 2001; Lobacheva et al., 2020b). In this section, we further validate this finding by theoretically proposing new scaling law that generalizes existing language model scaling laws, and demonstrate that PARSCALE can achieve similar effects as parameter scaling. We consider special case that w1 = w2 = = 1/P to simplify our analysis. This is degraded version of PARSCALE, therefore, we can expect that the full version of PARSCALE is at least not worse than the theoretical results we can obtain (See Appendix for further numeric comparison). Let ˆpi( x) = fθ(xi) denote the next token distribution for the input sequence predicted by the i-th stream. Based on Equation (2), the final prediction ˆp( x) = gθ(x) is the average across ˆpi, i.e., ˆp( x) = 1/P ˆpi( x). Chinchilla (Hoffmann et al., 2022) proposes that the loss of language model with parameters is function of after convergence. We assume the prediction of each stream adheres to the Chinchilla scaling law, as follows: Lemma 3.1 (Chinchilla Scaling Law (Hoffmann et al., 2022)). The language model cross-entropy loss Li for the i-th stream prediction (with parameters) when convergence is: Li = (cid:19)α (cid:18) + E, 1 P, (3) where {A, E, α} are some positive constants. is the entropy of natural text, and is the number of parameters.1 Based on Lemma 3.1, we theoretically derive that after aggregating streams, the prediction follows new type of scaling law, as follows: Proposition 1 (Theoretical Formula for Parallel Scaling Law). The loss for PARSCALE (with streams and parameters) is (cid:18) = P1/α DIVERSITY (cid:19)α + E. (4) We define DIVERSITY as: DIVERSITY = [(P 1)ρ + 1]1/α , where ρ is the correlation coefficient between random variables pi and pj (i = j), and pi is the relative residuals for the i-th stream prediction, i.e., pi = [ ˆpi(yx)p(yx)]/p(yx). p(y x) is the real next token probability. Proof for Proposition 1 is elaborated in Appendix B. From it, we can observe two key insights: 1. When ρ = 1, predictions across different streams are identical, at which point we can validate that Equation (4) degenerates into Equation (3). Random initialization on small number of parameters introduced (i.e., prefix embeddings) is sufficient to avoid this situation in our experiments, likely due to the impact being magnified by the extensive computation of LLMs. 2. When ρ = 1, is inversely correlated to P. Notably, when ρ = 0, residuals are independent between streams and the training loss exhibits power-law relationship with (i.e., P1). This aligns with 1Chinchilla also considers the limited data and training steps. In this paper, we focus on the impact of computation and parameters on model capacity and assumes that the model has already been trained to convergence. 4 Figure 2: Loss of LLMs scaled on parameters and number of parallel streams trained on 42B tokens. Each point depicts the loss from training run. The fitted scaling law curve from Equation (5) is displayed, with annotated fitted parameters (E, A, k, α) and the goodness of fit R2. findings in Lobacheva et al. (2020b). When ρ is negative, the loss can further decrease and approach zero. This somewhat demystifies the effectiveness of CFG: by widening the gap between good input and bad input x, we force the model to think from two distinct perspectives, which can increase the diversity between the two outputs. Despite the difficulty in further modeling ρ, Proposition 1 suggests that scaling times of parallel computation is equivalent to scaling the model parameter count, by factor of (P1/αDIVERSITY). This motivates us to go further, by empirically fitting practical parallel scaling law to validate Hypothesis 1. 3.2 Practical Parallel Scaling Laws Experiment Setup To fit parallel scaling law in practice, we pre-train Transformer language models with the Qwen-2.5 dense architecture and tokenizer (Qwen Team, 2024) from scratch on the open-source corpus. Models have up to 4.7 billion parameters (with 4.4B non-embedding parameters) and 8 parallel streams. We primarily focus on the relationship between parallel scaling and parameter scaling. Therefore, we fix the training data size at 42 billion tokens without data repeat2. We introduce the results for more training tokens in the next section, and leave the impact of data scale on our scaling laws for future work. We use batch size of 1024 and sequence length of 2048, resulting in 20K training steps. For models with > 1, we incorporate prefix embeddings and aggregation weight, as introduced in Appendix A. No additional parameters are included for = 1 models to maintain alignment with existing architectures. We report the last step training loss using exponential moving average, with smoothing weight of 0.95. Other hyperparameters follow existing works (Muennighoff et al., 2023) and are detailed in Appendix C. Our pre-training is conducted on two widely utilized datasets: Stack-V2 (Python subset) (Lozhkov et al., 2024) and Pile (Gao et al., 2021). Pile serves as general corpus aimed at enhancing common sense and memorization skills, while Stack-V2 focuses on code comprehension and reasoning skills. Analyzing PARSCALE across these contexts can assess how parameters and computations contribute to different skills. Parametric Fitting We plot the results in Figure 2, where each point represents the loss of training run, detailed in Appendix F. We observe that increasing yields benefits following logarithmic trend. Similar gains are seen when raising from 1 to 2, 2 to 4, and 4 to 8. Thus, we preliminarily try the following form: (cid:18) = (k log + 1) (cid:19)α + E, (5) where we assume that P1/α DIVERSITY = log + 1 in Equation (4) based on the finding of the logarithmic trend. (A, k, α, E) are parameters to fit, and we use the natural logarithm (base e). We follow the fitting procedure from Hoffmann et al. (2022); Muennighoff et al. (2023), detailed in Appendix E. Figure 2 illustrates the parallel scaling law fitted for two training datasets. It shows high goodness of fit (R2 up to 0.998), validating the effectiveness of Equation (5). Notably, we can observe that the value for Stack-V2 (0.39) is higher than for Pile (0.33). Recall that reflects the benefits of increased 2In Appendix D, we test PARSCALE with repeated data on small-scale corpus, OpenWebText (Gokaslan et al., 2019), and found that PARSCALE helps mitigate the overfitting when data is limited. We leave further exploration on the data-constrained parallel scaling law for future work. Figure 3: Predicted loss contours for PARSCALE. Each contour line indicates combination of (parameter, P) with similar performance. Table 2: Average performance (%) on two code generation tasks, HumanEval(+) and MBPP(+), after pre-training on the Stack-V2-Python dataset. Table 3: Average performance (%) on six general lm-evaluation-harness tasks after pre-training on the Pile dataset. = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 26.7 30.3 30.1 32. 28.4 32.4 32.5 34.0 31.6 33.6 34.1 37.2 33.9 37.4 37.6 39.1 36.9 39.4 40.7 42.1 39.2 42.6 42.6 45.4 = 1 = 2 = 4 = 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 49.1 49.9 50.6 50.7 50.6 51.0 51.8 51.8 52.1 52.4 53.3 54.2 53.1 54.4 55.0 55.7 55.2 57.0 57.8 58. 57.2 58.5 59.1 59.6 parallel computation. Since Stack-V2 emphasizes coding and reasoning abilities while Pile emphasizes memorization capacity, we propose an intuitive conjecture that model parameters mainly impact the memorization skills, while computation mainly impacts the reasoning skills. This aligns with recent findings on inference-time scaling (Geiping et al., 2025). Unlike those studies, we further quantitatively assess the ratio of contribution to model performance between parameters and computation through our proposed scaling laws. Recall that Equation (5) implies scaling equates to increasing parameters by O(N log P). It suggests that models with more parameters benefit more from PARSCALE. Figure 3 more intuitively displays the influence of computation and parameters on model capacity. As model parameters increase, the loss contours flatten, showing greater benefits from increasing computation. Downstream Performance Tables 2 and 3 illustrate the average performance on downstream tasks (coding tasks for Stack-V2-Python and general tasks for Pile) after pre-training, with comprehensive results in Appendix G. It shows that increasing the number of parallel streams consistently boosts performance, which confirms that PARSCALE is able to enhance the model capabilities and similar to scale the parameters. Notably, PARSCALE offers more substantial improvements for coding tasks compared to general tasks. For example, as shown in Table 2, the coding ability of the 1.6B model for = 8 aligns with the 4.4B model, while Table 3 indicates that such setting performs comparably to the 2.8B model on general tasks that focusing on common-sense memorization. These findings affirm our previous hypothesis that the number of parameters impacts the memorization capacity, while computation impacts the reasoning capacity. 3.3 Inference Cost Analysis We further compare the inference efficiency between parallel scaling and parameter scaling at equivalent performance levels. Although some work uses FLOPS to measure the inference cost (Hoffmann et al., 2022; Sardana et al., 2024), we argue that this is not an ideal metric. Most Transformer operations are bottlenecked by memory access rather than computation during the decoding stage (Ivanov et al., 2021). Some work (such as flash attention (Dao et al., 2022)) incurs more FLOPS but achieves lower latency by reducing memory access. Therefore, we use memory and latency to measure the inference cost, based on the llm-analysis framework (Li, 2023). Memory determines the minimum hardware requirements, while latency measures the time overhead from input to output. We analyze the inference cost across various inference batch sizes. It is worth mentioning that all models in our experiments feature the same number of layers, differing only in parameter width and parallel streams (detailed in Appendix C). This enables more fair comparison of the efficiencies. 6 (a) batch size = 1 (b) batch size = 2 (c) batch size = (d) batch size = 8 (e) batch size = 1 (f) batch size = 2 (g) batch size = 4 (h) batch size = 8 Figure 4: Model capacity (indicated by loss) scales on the inference space-time cost, with three parameters (1.6B, 2.8B, and 4.4B) and batch sizes {1, 2, 4, 8}. Results are averaged from input / output tokens {64, 128, 256, 512}. Blue arrows indicate parameter scaling; gray arrows represent parallel scaling. Space Cost Figures 4(a) to 4(d) compare the inference memory usage of two scaling strategies, where we utilize loss on Stack-V2-Python as an indicator for model capacity. It shows that PARSCALE only marginally increases memory usage, even with larger batch sizes. This is because PARSCALE introduces negligible amounts of additional parameters (i.e., prefix tokens and aggregation weights, about 0.2% parameters per stream) and increases KV cache size (expanded by times with streams), which generally occupies far less GPU memory than model parameters. As the input batch size increases, the KV cache size also grows; however, PARSCALE continues to demonstrate significantly better memory efficiency compared to parameter scaling. This suggests that PARSCALE maximizes the utility of memory through parameter reusing, while parameter scaling employs limited computation per parameter and cannot fully exploit computation resources. Time Cost Figures 4(e) to 4(h) compare the inference time of two scaling strategies. It shows that PARSCALE adds minimal latency at smaller batch sizes since the memory bottlenect is converted tothe computation bottleneck. Given that parallel computation introduced by PARSCALE is friendly to GPUs, it will not significantly raise latency. As batch sizes increase, decoding shifts from memory to computation bottleneck, resulting in higher costs for PARSCALE, but it remains more efficient than parameter scaling up to batch size of 8. The above analysis indicates that PARSCALE is ideal for low-resource edge devices like smartphones, smart cars, and robots, where queries are typically few and batch sizes are small. Given limited memory resources in these environments, PARSCALE effectively utilizes memory and latency advantages at small batch sizes. When batch size is 1, for 1.6B model and scaling to = 8 using PARSCALE, it uses 22 less memory increase and 6 less latency increase compared to parameter scaling that achieves the same performance. We anticipate that the future LLMs will gradually shift from centralized server deployments to edge deployments with the popularization of artificial intelligence. This suggests the promising potential of PARSCALE in the future."
        },
        {
            "title": "4 Scaling Training Data",
            "content": "Due to our limited budget, our previous experiments on scaling laws focus on pre-training with 42 billion tokens. In this section, we will train 1.8B model (with 1.6B non-embedding parameters) and scale the training data to 1T tokens, to investigate whether PARSCALE is effective for production-level training. We also apply PARSCALE to an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024) (which is pre-trained on 18T tokens), under two settings: continual pre-training and parameter-efficient fine-tuning (PEFT). 7 Table 4: Performance comparison of the 1.8B models after training on 1T tokens from scratch using the two-stage strategy. We incorporate recent strong baselines (less than 2B parameters) as comparison to validate that our = 1 baseline is well-trained. The best performance and its comparable performance (within 0.5%) is bolded. Appendix elaborates the evaluation details. @1: Pass@1. @10: Pass@10. Tokens Data General Math Code MMLU WinoGrande Hellaswag OBQA PiQA ARC Average General gemma-3-1B Llama-3.2-1B Qwen2.5-1.5B SmolLM-1.7B SmolLM2-1.7B Baseline (P = 1) PARSCALE (P = 2) PARSCALE (P = 4) PARSCALE (P = 8) 2T 15T 18T 1T 12T 1T 1T 1T 1T Private Private Private Public Public Public Public Public Public 53.4 54.8 63.6 57.0 63.3 56.0 56.2 57.2 58.6 14.9 30.1 55.8 37.9 41.6 45.6 47.4 48.6 49.9 1.9 4.7 52.3 6.0 24.3 25.5 27.1 30.0 32. Math 26.4 30.8 61.0 29.7 50.1 28.5 29.0 30.0 35.1 61.4 62.1 65.6 61.8 68.2 61.9 62.4 63.4 64.9 37.8 39.2 42.6 42.8 42. 40.6 42.0 42.0 42.6 75.6 75.9 76.6 77.3 78.3 75.2 74.9 75.6 76.1 56.2 55.3 67.9 63.3 67.3 64.8 64.3 66.3 66.0 63.0 65.7 68.0 67.3 73. 65.0 64.7 65.9 67.0 Code Tokens Data GSM8K GSM8K Minerva HumanEval HumanEval+ MBPP @10 +CoT Math MBPP+ @1 @10 @1 @10 @10 @1 @1 gemma-3-1B Llama-3.2-1B Qwen2.5-1.5B SmolLM-1.7B SmolLM2-1.7B Baseline (P = 1) PARSCALE (P = 2) PARSCALE (P = 4) PARSCALE (P = 8) 2T 15T 18T 1T 12T 1T 1T 1T 1T Private Private Private Public Public Public Public Public Public 1.8 5.1 61.7 6.4 30.4 28.7 32.6 34.7 38.4 2.3 7.2 67.2 8.3 30. 35.9 35.6 40.8 43.7 1.5 1.8 28.1 3.2 11.8 12.0 13.0 14.5 16.4 6.7 16.5 36.0 20.1 23.8 26.8 26.2 27.4 28.7 15.9 27.4 62.8 35.4 44. 44.5 50.0 47.6 50.6 6.1 14.0 31.1 15.9 18.9 20.7 20.1 23.8 24.4 14.6 25.0 55.5 32.3 37.8 38.4 42.1 43.9 44.5 13.0 29.1 33.1 54.5 61.9 79.6 40.7 66.4 45.2 68. 51.6 75.9 52.9 77.0 55.3 77.0 56.3 79.1 10.8 23.0 27.0 43.4 50.8 68.5 34.7 57.4 36.0 57.9 43.9 62.7 45.0 65.6 47.1 66.7 48.1 67.2 4.1 Two-Stage Pretraining While PARSCALE is efficient for the inference stage (as we discuss in Section 3.3), it still introduces about times of floating-point operations and significantly increases overhead in the computation-intensive training processes. To address this limitation, we propose two-stage strategy: in the first stage, we use traditional pre-training methods with 1 trillion tokens; in the second stage, we conduct PARSCALE training with 20 billion tokens. Since the second stage accounts for only 2% of the first stage, this strategy can greatly reduce training costs. This two-stage strategy is similar to long-context fine-tuning (Ding et al., 2024), which also positions the more resource-intensive phase at the end. In this section, we will examine the effectiveness of this strategy. Setup We follow Allal et al. (2025) and use the Warmup Stable Decay (WSD) learning rate schedule (Hu et al., 2024; Zhai et al., 2021). In the first stage, employing 2K step warm-up followed by fixed learning rate of 3e-4. In the second stage, the learning rate is annealed from 3e-4 to 1e-5. The rest of the hyperparameters remain consistent with the previous experiments (see Appendix C). In the first phase, we do not employ the PARSCALE technique. We refer to the recipe proposed by Allal et al. (2025) to construct our training data, which consists of 370B general data, 80B mathematics data, and 50B code data. We train the model for two epochs to consume 1T tokens. Among the general text, there are 345B from FineWeb-Edu (Penedo et al., 2024) and 28B from Cosmopedia 2 (Ben Allal et al., 2024); the mathematics data includes 80B from FineMath (Allal et al., 2025); and the code data comprises 47B from Stack-V2-Python and 4B from Stack-Python-Edu. In the second phase, we use the trained model from the first phase as the backbone and introduce additional parameters from PARSCALE, which are randomly initialized using standard deviation of 0.02 (based on the initialization of Qwen-2.5). Following (Allal et al., 2025), in this phase, we increase the proportion of mathematics and code data, finally including total of 7B general text data, 7B mathematics data, and 7B Stack-Python-Edu data. Training Loss Figure 5 intuitively demonstrates the loss curve during our two-stage training. We find that at the beginning of the second phase, the loss for > 1 initially exceed those for = 1 due to the introduction of randomly initialized parameters. However, after processing small amount of data (0.0002T tokens), the model quickly adapts to these newly introduced parameters and remains stable thereafter. This proves that PARSCALE can take effect rapidly with just little data. We can also find that in the later stages, PARSCALE yields similar logarithmic gains. This aligns with previous scaling law 8 Table 5: Comparison of the performance of different Instruct models, where the few-shot examples are treated as multi-turn conversation. IFEval MMLU GSM8K 5-shot 0-shot 4-shot SmolLM-1.7B-Inst Baseline-Inst (P = 1) PARSCALE-Inst (P = 2) PARSCALE-Inst (P = 4) PARSCALE-Inst (P = 8) 16.3 54.1 55.8 58.4 59.5 28.4 34.2 35.1 38.2 41. 2.0 50.3 55.3 54.8 56.1 Figure 5: Loss for two-stage training, smoothing using an exponential moving average with weight of 0.95. findings, suggesting that our earlier conclusions for from-scratch pre-training parallelism with streams equates to O(N log(P)) increase in parameters also applies to continued pretraining. Additionally, larger (such as = 8) can gradually widen the gap compared to smaller values (such as = 4). This demonstrates that parallel scaling can also benefit from data scaling. Downstream Performance In Table 4, we report the downstream performance of the model after finishing two-stage training, across 7 general tasks, 3 math tasks, and 8 coding tasks. It can be observed that with the increase of P, the performance presents an upward trend for most of the benchmarks, which validates the effectiveness of PARSCALE trained on the large dataset. Specifically, when increases from 1 to 8, PARSCALE improves by 2.6% on general tasks, and by 7.3% and 4.3% on math and code tasks, respectively. It achieves 10% improvement (34% relative improvement) on GSM8K. This reaffirms that PARSCALE can more effectively address reasoning-intensive tasks. Moreover, in combination with CoT, it achieves about an 8% improvement on GSM8K, suggesting that parallel scaling can be used together with inference-time scaling to achieve better results. Compared to the SmolLM-1.7B model, which was also trained on 1T tokens, our = 1 baseline achieves comparable results on general tasks and significantly outperforms on math and code tasks. One explanation is that SmolLM training set focuses more on commonsense and world knowledge corpus, while lacking high-quality math and code data. This validates the effectiveness of our dataset construction and training strategy, and further enhances the credibility of our experimental results. Instruction Tuning We follow standard practice to post-train the base models, to explore whether PARSCALE can enhance performance during the post-training stage. We conducted instruction tuning on four checkpoints (P {1, 2, 4, 8}) from the previous pre-training step, increasing the sequence length from 2048 to 8192 and the RoPE base from 10,000 to 100,000, while keeping other hyperparameters constant. We used 1 million SmolTalk (Allal et al., 2025) as the instruction tuning data and trained for 2 epochs. We refer to these instruction models as PARSCALE-Inst. The experimental results in Table 5 show that when increases from 1 to 8, our method achieves 5% improvement on the instruction-following benchmark IFEval, along with substantial gains in the general task MMLU and the reasoning task GSM8K. This demonstrates that the proposed PARSCALE performs effectively during the post-training phase. 4.2 Applying to the Off-the-Shelf Pre-Trained Model We further investigate applying PARSCALE to off-the-shelf models under two settings: continual pretraining and parameter-efficient fine-tuning (PEFT). Specifically, we use Pile and Stack-V2 (Python) to continue pre-training the Qwen-2.5 (3B) model. The training settings remain consistent with Appendix C, with the only difference being that we initialize with Qwen2.5-3B weights and adjust the RoPE base to the preset 1,000,000. Continual Pre-Training Figures 6(a) and 6(b) illustrates the training loss after continuous pre-training on Stack-V2 (Python) and Pile. Notably, Qwen2.5 has already been pre-trained on 18T of data, which possibly have significant overlap with both Pile and Stack-V2. This demonstrates that improvements can still be achieved even with thoroughly trained foundation model and commonly used training datasets. Parameter-Efficient Fine-Tuning We further utilize PEFT to fine-tune the introduced parameters while freezing the backbone weights. Figure 6(c) shows that this strategy can still significantly improve (a) Continual Pre-training (b) Continual Pre-training (c) Freezing the Model Figure 6: (a)(b) Loss for continual pre-training the Qwen-2.5-3B model on the two datasets. (c) Code generation performance after fine-tuning on Stack-V2 (Python), averaged from HumanEval(+) and MBPP(+). We only fine-tune the introduced parameters (prefix tokens and aggregation weights), with different sharing exactly the same Qwen2.5-3B pre-trained weights. downstream code generation performance. Moreover, this demonstrates the promising prospects of dynamic parallel scaling: we can deploy the same backbone and flexibly switch between different numbers of parallel streams in various scenarios (e.g., high throughput and low throughput), which enables quick transitions between different levels of model capacities."
        },
        {
            "title": "5 Related Work",
            "content": "Beyond language modeling, our work can be connected to various machine learning domains. First, scaling computation while maintaining parameters is also the core idea of inference-time scaling. Second, as previously mentioned, PARSCALE can be viewed as dynamic and scalable classifier-free guidance. Third, our method can be seen as special case of model ensemble. Lastly, the parallel scaling law we explore is generalization of the existing language model scaling laws. Inference-Time Scaling The notable successes of reasoning models, such as GPT-o1 (OpenAI, 2024), DeepSeek-R1 (DeepSeek-AI, 2025), QwQ (Qwen Team, 2025a), and Kimi k1.5 (Kimi Team, 2025) have heightened interest in inference-time scaling. These lines of work (Wei et al., 2022; Madaan et al., 2023; Zhou et al., 2023a) focus on scaling serial computation to increase the length of the chain-of-thought (Wei et al., 2022). Despite impressiveness, it results in inefficient inference and sometimes exhibits overthinking problems (Chen et al., 2024b; Sui et al., 2025). Other lines of approaches focus on scaling parallel computation. Early methods such as beam search (Wiseman & Rush, 2016), self-consistency (Wang et al., 2023b), and majority voting (Chen et al., 2024a) require no additional training. We also provide an experimental comparison with Beam Search in Appendix H, which shows the importance of scaling parallel computing during the training stage. Recently, the proposal-verifier paradigm has gained attention, by employing trained verifier to select from multiple parallel outputs (Brown et al., 2024; Wu et al., 2025; Zhang et al., 2024b). However, these methods are limited to certain application scenarios (i.e., generation tasks) and specialized training data (i.e., reward signals). More recently, Geiping et al. (2025) introduce training LLMs to reason within the latent space and scale sequential computation, applicable to any application scenarios without needing specialized datasets. However, this method demands significant serial computation scaling (e.g., 64 times the looping) and invasive model modifications, necessitating training from scratch and complicating integration with existing trained LLMs. Classifier-Free Guidance Classifier-Free Guidance (CFG) stems from Classifier Guided Diffusion (Dhariwal & Nichol, 2021), which uses an additional classifier to guide image generation using diffusion models (Ho et al., 2020). By using the generation model itself as classifier, CFG (Ho & Salimans, 2022) further eliminates dependency on the classifier and leverage two forward passes. Similar concepts have emerged in NLP, such as Coherence Boosting (Malkin et al., 2022), PREADD (Pei et al., 2023), ContextAware Decoding (Shi et al., 2024), and Contrastive Decoding (Li et al., 2023). Recently, Sanchez et al. (2024) proposed transferring CFG to language models. However, due to constraints of human-designed heuristic rules, these techniques cannot leverage the power of training-time scaling (Kaplan et al., 2020) and the performance is limited. Model Ensemble Model ensemble is classic research field in machine learning and is also employed in the context of LLMs (Chen et al., 2025). In traditional model ensembles, most ensemble components do not share parameters. Some recent work consider setups with partially shared parameters. For example, Monte Carlo dropout (Gal & Ghahramani, 2016) employs multiple different random dropouts during the inference phase, while BatchEnsemble (Wen et al., 2020; Tran et al., 2022) and LoRA ensemble (Wang et al., 2023a) use distinct low-rank matrix factorizations for model weights to differentiate different streams (we also experimented with this technique as input transformation in Appendix A). Weight sharing (Yang et al., 2021; Lan et al., 2019) is another line of work, where some weights of model are shared across different components and participate in multiple computations. However, these works have not explored the scaling law of parallel computation from the perspective of model capacity. As we discuss in Appendix A, we find that the specific differentiation technique had minimal impact, and the key factor is the scaling in parallel computation. Scaling Laws for Language Models Many researchers explore the predictable relationships between LLM training performance and various factors under different settings, such as the number of parameters and data (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022; DeepSeek-AI, 2024; Frantar et al., 2024), data repetition cycles (Muennighoff et al., 2023; Hernandez et al., 2022), data mixing (Ye et al., 2025; Que et al., 2024), and fine-tuning (Zhang et al., 2024a). By extending the predictive empirical scaling laws developed from smaller models to larger models, we can significantly reduce exploration costs. Recently, some studies have investigated the scaling effects during inference (Sardana et al., 2024), noting log-linear relationship between sampling number and performance (Brown et al., 2024; Snell et al., 2025). But they are limited to certain application scenarios. Our work extends the Chinchilla scaling law (Hoffmann et al., 2022) by introducing the intrinsic quantitative relationship between parallel scaling and parameter scaling. Existing literature has also identified power-law relationship between the number of ensembles and loss in model ensemble scaling laws (Lobacheva et al., 2020a), which can be considered special case of Proposition 1 when ρ = 0."
        },
        {
            "title": "6 Discussion and Future Work",
            "content": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024; Allal et al., 2025; Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide computation-optimal strategy during the inference phase (Wu et al., 2025; Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is promising direction. Further Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there growth rate that exceeds O(log P), and whether there is performance upper bound for 8, remain open questions. Optimal Division Point of Two-Stage Strategy Considering that PARSCALE is inefficient in the training phase, we introduced two-stage strategy and found that LLMs can still learn to leverage parallel computation for better capacity with relatively few tokens. We currently employ 1T vs. 20B tokens as the division point. Whether there is more optimal division strategy and its trade-off with performance is also an interesting research direction. Application to MoE Architecture Similar to the method proposed by Geiping et al. (2025), PARSCALE is computation-heavy (but more efficient) strategy. This is somewhat complementary to sparse MoE (Fedus et al., 2022; Shazeer et al., 2017), which is parameter-heavy. Considering that MoE is latencyfriendly while PARSCALE is memory-friendly, exploring whether their combination can yield more efficient and high-performing models is worth investigating. Application to Other Machine Learning Domains Although we focus on language models, PARSCALE is more general method that can be applied to any model architecture, training algorithm, and training data. Exploring PARSCALE in other areas and even proposing new scaling laws is also promising direction for the future."
        },
        {
            "title": "7 Conclusions",
            "content": "In this paper, we propose new type of scaling strategy, PARSCALE, for training LLMs by reusing existing parameters for multiple times to scale the parallel computation. Our theoretical analysis and extensive experiments propose parallel scaling law, showing that model with parameters and parallel computation can be comparable to model with O(N log P) parameters. We scale the training data to 1T tokens to validate PARSCALE in the real-world practice based on the proposed two-stage strategy, and show that PARSCALE remains effective with frozen main parameters for different P. We also demonstrate that parallel scaling is more efficient than parameter scaling during the inference time, especially in low-resource edge scenarios. As artificial intelligence becomes increasingly widespread, we believe that future LLMs will progressively transition from centralized server deployments to edge deployments, and PARSCALE could emerge as promising technique for these scenarios."
        },
        {
            "title": "Acknowledgement",
            "content": "This research is partially supported by Zhejiang Provincial Natural Science Foundation of China (No. LZ25F020003) and the National Natural Science Foundation of China (No. 62202420). The authors would like to thank Jiquan Wang, Han Fu, Zhiling Luo, and Yusu Hong for their early idea discussions and inspirations, as well as Lingzhi Zhou for the discussions on efficiency analysis."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. Smollm - blazingly fast and remarkably powerful. https://huggingface.co/blog/smollm, 2024. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlıˇcek, Agustın Piqueres Lajarın, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. URL https://arxiv.org/abs/2502.02737. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, February 2024. URL https://huggingface.co/datasets/HuggingFaceTB/cosmopedia. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 19, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1. URL https://aclanthology.org/2022.acl-short.1/. Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, Francois Yvon, and Andy Zou. Lessons from the trenches on reproducible evaluation of language models. arXiv preprint arXiv:2405.14782, 2024. URL https://arxiv.org/abs/2405.14782. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 74327439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239. 12 Leo Breiman. Random forests. Mach. Learn., 45(1):532, October 2001. ISSN 0885-6125. doi: 10.1023/A: 1010933404324. URL https://doi.org/10.1023/A:1010933404324. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. URL https://arxiv.org/abs/2407.21787. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. URL https: //arxiv.org/abs/2005.14165. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. Are more LLM calls all you need? towards the scaling properties of compound AI systems. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https: //openreview.net/forum?id=m5106RRLgx. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024b. URL https://arxiv.org/abs/2412.21187. Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, and Philip S. Yu. Harnessing multiple large language models: survey on llm ensemble. arXiv preprint arXiv:2502.18036, 2025. URL https://arxiv.org/abs/2502.18036. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https: //arxiv.org/abs/2110.14168. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. URL https://arxiv. org/abs/2205.14135. DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. URL https://arxiv.org/abs/2401.02954. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=AAWuCvzaVt. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. URL https://arxiv.org/abs/2402.13753. 13 William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23(1), January 2022. ISSN 1532-4435. Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparselyconnected foundation models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=i9K2ZWkYIP. Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceeduncertainty in deep learning. ings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 10501059, New York, New York, USA, 2022 Jun 2016. PMLR. URL https: //proceedings.mlr.press/v48/gal16.html. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2021. URL https: //arxiv.org/abs/2101.00027. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. URL https://arxiv.org/abs/2502. 05171. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. //Skylion007.github.io/OpenWebTextCorpus, 2019. http: Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=lIsCS8b6zj. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022. URL https://arxiv.org/abs/2205.10487. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. URL https://arxiv.org/abs/1712.00409. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. URL https://arxiv.org/abs/2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203.15556. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=3X2L2TFr0f. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: case study on optimizing transformers. In A. Smola, A. Dimakis, and I. Stoica (eds.), Proceedings of Machine Learning and Systems, volume 3, pp. 711732, 2021. URL https://proceedings. mlsys.org/paper files/paper/2021/file/bc86e95606a6392f51f95a8de106728d-Paper.pdf. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. URL https://arxiv.org/abs/2501.12599. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/ D17-1082. URL https://aclanthology.org/D17-1082/. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. URL https://arxiv.org/abs/1909.11942. Minh Le, Chau Nguyen, Huy Nguyen, Quyen Tran, Trung Le, and Nhat Ho. Revisiting prefix-tuning: Statistical benefits of reparameterization among prompts. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=QjTSaFXg25. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243/. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. URL https://arxiv.org/abs/2206.14858. Cheng Li. Llm-analysis: Latency and memory analysis of transformer models for training and inference. https://github.com/cli99/llm-analysis, 2023. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long. 353/. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL https://aclanthology.org/2023.acl-long.687/. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Math. Program., 45(13):503528, August 1989. ISSN 0025-5610. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= 1qvx610Cu7. 15 Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024b. URL https://arxiv.org/abs/2402.14905. Llama Team. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv. org/abs/2407.21783. Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, and Dmitry Vetrov. On power laws in deep ensembles. arXiv preprint arXiv:2007.08483, 2020a. URL https://arxiv.org/abs/2007.08483. Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, and Dmitry Vetrov. On power laws in deep ensembles. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 23752385. Curran Inc., 2020b. URL https://proceedings.neurips.cc/paper files/paper/2020/file/ Associates, 191595dc11b4d6e54f01504e3aa92f96-Paper.pdf. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu noz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. URL https://arxiv.org/abs/2402.19173. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB. Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. Coherence boosting: When your pretrained language model is not paying enough attention. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 82148236, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.565. URL https://aclanthology.org/2022.acl-long.565/. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. URL https://arxiv.org/abs/1609.07843. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=j5BuTrEj35. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/abs/ 2303.08774. OpenAI. New reasoning models: Openai o1-preview and o1-mini. https://openai.com/research/ o1-preview-and-o1-mini, 2024. Jonathan Pei, Kevin Yang, and Dan Klein. PREADD: Prefix-adaptive decoding for controlled text generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1001810037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.636. URL https://aclanthology.org/ 2023.findings-acl.636/. 16 Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. URL https://arxiv.org/abs/2406.17557. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: strategic progress report part 1. arXiv preprint arXiv:2410.18982, 2024. URL https://arxiv.org/abs/2410.18982. Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, and Bo Zheng. D-cpt law: Domain-specific continual pre-training scaling law for large language models. arXiv preprint arXiv:2406.01375, 2024. URL https://arxiv.org/abs/2406.01375. Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/ abs/2412.15115. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/, 2025a. Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025b. URL https://arxiv.org/ abs/2505.09388. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. URL https://arxiv.org/abs/2205. 11487. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, August 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381. Guillaume Sanchez, Alexander Spangher, Honglu Fan, Elad Levi, and Stella Biderman. Stay on topic with classifier-free guidance. In Proceedings of the 41st International Conference on Machine Learning, pp. 4319743234, 2024. Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: accounting for inference in language model scaling laws. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. URL https://arxiv.org/abs/1701.06538. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 783791, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-short. 69. URL https://aclanthology.org/2024.naacl-short.69/. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. URL https://arxiv.org/abs/1909.08053. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=4FWAwZtd2n. Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. Inference scaling flaws: The limits of llm resampling with imperfect verifiers. arXiv preprint arXiv:2411.17501, 2024. URL https://arxiv.org/ abs/2411.17501. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. URL https: //arxiv.org/abs/2104.09864. 17 Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. URL https://arxiv.org/abs/2503. 16419. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision . In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 28182826, Los Alamitos, CA, USA, June 2016. IEEE Computer Society. doi: 10.1109/CVPR.2016.308. URL https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.308. Dustin Tran, Jeremiah Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan Singhal, Zachary Nado, Joost van Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, and Balaji Lakshminarayanan. Plex: Towards reliability using pretrained large model extensions. arXiv preprint arXiv:2207.07411, 2022. URL https://arxiv.org/abs/2207.07411. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. URL https://arxiv.org/abs/2211.04325. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant ˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2. Xi Wang, Laurence Aitchison, and Maja Rudolph. Lora ensembles for large language model fine-tuning. arXiv preprint arXiv:2310.00035, 2023a. URL https://arxiv.org/abs/2310.00035. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Self-consistency improves chain of thought reasoning in lanIn The Eleventh International Conference on Learning Representations, 2023b. URL Chowdhery, and Denny Zhou. guage models. https://openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. URL https://arxiv.org/abs/2201.11903. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin (eds.), Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413/. Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: An alternative approach to efficient ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020. URL https://arxiv.org/abs/2002.06715. Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 12961306, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1137. URL https://aclanthology.org/D16-1137. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=VNckp7JEHn. Shuo Yang, Le Hou, Xiaodan Song, Qiang Liu, and Denny Zhou. Speeding up deep model training by sharing weights and then unsharing. arXiv preprint arXiv:2110.03848, 2021. URL https://arxiv.org/ abs/2110.03848. 18 Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jjCB27TMK3. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https: //aclanthology.org/P19-1472/. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv preprint arXiv:2106.04560, 2021. URL https://arxiv.org/abs/2106.04560. Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets LLM finetuning: The effect of data, model and finetuning method. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=5HCnKDeTws. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024b. URL https://openreview.net/forum?id=CxHRoTLmPX. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=WZH7099tgfM. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023b. URL https://arxiv.org/abs/2311.07911."
        },
        {
            "title": "A Implementation Details and Pivot Experiments",
            "content": "Input Transformation We expect that the transformations applied to the input embedding can significantly influence the output, which avoids excessively similar outputs across different parallel streams. This can be achieved through the Soft Prompting technique (Lester et al., 2021). Specifically, Lester et al. (2021) introduced trainable continuous embeddings, known as soft prompts, which are appended to the original sequence of input word embeddings. Freezing the main network while only fine-tuning these soft prompts can be comparable to full fine-tuning. Building on this concept, prefixtuning (Li & Liang, 2021) incorporates soft prompts into every attention layer of the Transformer model and appends them to the key and value vectors, showing superiority performance to soft prompts. We utilize prefix-tuning to implement input transformation. To be specific, we first duplicate the input into parallel copies, distinguishing them with different prefixes in each attention layer. This can be implemented as using different KV caches for different streams. We found that randomly initializing the prefixes is sufficient to ensure diverse outputs across different streams. We also leverage the prefix reparameterization trick (Li & Liang, 2021; Han et al., 2024), which is theoretically proved effectiveness by Le et al. (2025). As comparison, we also compared using other parameter-efficient fine-tuning strategy for discriminating the input, including LoRA (Hu et al., 2022) and BitFit (Ben Zaken et al., 2022). Notably, LoRA has been also applied to the model ensemble scenario in the literature (Wang et al., 2023a), but only used in the fine-tuning setting while freezing the main parameters in their experiments. Output Aggregation We found that using dynamic aggregation weights performs better than static ones. Specifically, we concatenate each output together and use an MLP : RdoP RP to convert it into vector of length as aggregation weights. The process can be formalized as: w1, , wP Softmax (h (Concat[ fθ(x1); ; fθ(xP)])) , (6) where Softmax ensures aggregation weights are normalized. It can be seen as dynamically weighting different parallel streams during forward process for each token. We observed that, in the early stages of training, the model may assign nearly all weight to few streams, leaving others with near-zero weights. It prevents the prefix parameters of these unlucky streams from receiving gradients and updates. This is similar to the load imbalance phenomenon in sparse MoE architectures (Fedus et al., 2022; Shazeer et al., 2017), where most tokens are sometimes assigned to few experts. To address this, we apply label smoothing (Szegedy et al., 2016) to set non-zero minimum for each weight, formulated as: wi wi (1 ϵ) + ϵ , (7) where ϵ denotes the smoothing parameter and we set it to 0.1 in our experiments. As comparison, we also compare using Linear layer to aggregate different outputs and directly average the outputs. Results We trained 0.5B model on Stack-V2-Python. Table 6 compares the impact of different strategies on final performance. For output aggregation, dynamic weighted sum with label smoothing proved most effective. The differences between methods for input transformation were minor (around 0.1%), much less than the benefits obtained from changing P. Therefore, we opt for the simplest strategy, prefix tuning. Unlike LoRA and BitFit, it requires minimal changes to the model, only necessitating adjustments to the KV-cache. Proof for Proposition 1 Proof. We first decompose the individual loss Li. Based on the definition of language model loss, we have: Li = Ex yV = Ex yV = Ex yV [p(y x) log ˆpi(y x)] p(y x) log {p(y x) (1 + pi(y x))} [p(y x) log p(y x)] + Ex yV p(y x) log (1 + pi(y x)) , (cid:124) (cid:123)(cid:122) entropy of natural text (cid:125) (cid:124) (cid:123)(cid:122) approximation error for the language model (cid:125) 20 Table 6: Comparisons of different strategies for input transformations and output aggregation. Input Transformation Output Aggregation Loss Rel. Improvement 1 2 2 2 2 2 2 2 2 2 2 8 - Prefix (48 tokens) Prefix (48 tokens) Prefix (48 tokens) Prefix (48 tokens) - Dynamic Weighted Sum (ϵ = 0.1) Dynamic Weighted Sum (ϵ = 0.0) Average Linear Layer Dynamic Weighted Sum (ϵ = 0.1) Prefix (48 tokens) Dynamic Weighted Sum (ϵ = 0.1) Prefix (96 tokens) Dynamic Weighted Sum (ϵ = 0.1) LoRA (r = 2) Prefix (48 tokens) + LoRA (r = 2) Dynamic Weighted Sum (ϵ = 0.1) Prefix (48 tokens) + LoRA (r = 2) + BitFit Dynamic Weighted Sum (ϵ = 0.1) Dynamic Weighted Sum (ϵ = 0.1) Prefix (48 tokens) + LoRA (r = 4) Dynamic Weighted Sum (ϵ = 0.1) Dynamic Weighted Sum (ϵ = 0.1) Prefix (48 tokens) Prefix (48 tokens) 1.1518 1.1276 1.1284 1.1288 1.1323 1.1276 1.1278 1.1281 1.1263 1.1263 1.1260 1.1145 1.1019 0.00% 2.10% 2.03% 2.00% 1.69% 2.10% 2.08% 2.06% 2.21% 2.21% 2.24% 3.24% 4.33% where is the vocabulary. In Chichilla scaling law, the entropy of natural text is and the approximation error is (A/N)α. Therefore, we have: Ex yV p(y x) log (1 + pi(y x)) = (cid:19)α . (cid:18) (8) Based on the Taylor series expansion, log(1 + x) = x2 we have: (cid:18) pi(y x)2 2 pi(y x) p(y x) (cid:19)α (cid:20) (cid:16) pi(y x)3(cid:17)(cid:21) + 2 + O(x3). Apply this expansion to Equation (8), = Ex yV (cid:34) = Ex (cid:34) = Ex (cid:124) Ex,y ( ˆpi(y x) p(y x)) (cid:35) (cid:34) + Ex yV p(y x) (cid:35) pi(y x)2 2 + (cid:16) pi(y x)3(cid:17) yV +ExE yx pi(y x)2 + (cid:16) pi(y x)3(cid:17) (cid:35) (cid:125) yV ˆpi(y x) + yV p(y x) (cid:123)(cid:122) =0 (cid:21) (cid:20) pi(y x)2 2 , (9) where the higher-order terms (cid:0)pi(y x)3(cid:1) are omitted in the last line. The results suggest that minimizing the approximation loss of language model is equal to minimizing the mean square error (MSE) of relative residuals. After fitting the data, an MSE estimator is usually assumed to be unbiased, meaning that Ex,ypi(y x) = 0. Here we follow this unbiased assumption to simplify the following derivation. We next consider the aggregated loss L. Let p(y x) denote the new relative residual after the aggregation: p(y x) = = = = ˆp(y x) p(y x) p(y x) i=1 ˆpi(y x) p(y x) 1 p(y x) ˆpi(y x) p(y x) p(y x) pi(y x). 1 1 P i=1 i=1 Let ρ denote the correlation coefficient between any two relative residuals pi(y x) and pj(y x) for 21 = j. Repeating the above process to decomposite the aggregated loss L, we have: = Ex yV [p(y x) log p(y x)] + Ex yV p(y x) log (1 + p(y x)) (cid:124) (cid:123)(cid:122) entropy of natural text, equal to (cid:20) p(y x)2 2 (cid:21) = + Ex,y (cid:125) (cid:124) (cid:123)(cid:122) approximation error (cid:125) = + (cid:32) Ex,y 1 2 1 i= pi(y x) (cid:33)2 = + 1 2P2 Ex,y (cid:34) i= p2 (y x) + 2 i<j pi(y x)pj(y x) (cid:35) = + 1 P2 (cid:34) i=1 Ex,y (cid:35) (cid:34) p2 (y x) + 2 i<j Ex,y (cid:20) pi(y x)pj(y x) 2 (cid:21)(cid:35) . Based on the Corollary of Chinchilla Scaling Law (Equation (9)), for the first term: Ex,y (cid:20) pi(y x)2 2 (cid:21) = (cid:19)α , (cid:18) for the cross terms: Ex,y (cid:20) pi(y x)pj(y x) 2 (cid:21) (cid:118) (cid:117) (cid:117) (cid:116)Ex,y = ρ (cid:34) p2 (y x) 2 (cid:35)(cid:118) (cid:117) (cid:117) (cid:116)Ex,y (cid:34) p2 (y x) (cid:35) = ρ (cid:19)α . (cid:18) Combining the results, we obtain the desired result: = + = + = + 1 P2 (cid:18) (cid:18) (cid:20) (cid:19)α (cid:18) + P(P 1) (cid:19)α (cid:21) ρ (cid:18) (cid:19)α (cid:20) 1 + (P 1)ρ (cid:21) (cid:19)α (cid:32) (cid:19)α (cid:18) 1 P1/α (cid:33)α 1 [(P 1)ρ + 1]1/α (cid:32) = + NP1/α [(P 1)ρ + 1]1/α (cid:33)α ."
        },
        {
            "title": "C Training Details",
            "content": "Training Hyperparameters Our training is based on Megatron-LM (Shoeybi et al., 2019). Specifically, the learning rate undergoes linear warm-up over 2,000 steps, reaching peak of 3 104 before decreasing to minimum of 1 105 according to cosine decay schedule. The models are trained using batch size of 1,024 and sequence length of 2,048, alongside RoPE base of 10,000 (Su et al., 2021). We utilize bfloat16 precision and the Adam optimizer (Kingma & Ba, 2015), setting the epsilon to 1e-8, β1 to 0.9, and β2 to 0.95. All parameters, including backbones and additional ones weve introduced, are initialized with Gaussian distribution having standard deviation of 0.02. Furthermore, we maintain dropout rate of 0, enforce weight decay rate of 0.1, and clip gradients at 1.0. The hyperparameter choices are mostly adopted from existing research (Muennighoff et al., 2023; Hoffmann et al., 2022; Qwen Team, 2024). Model Architectures The model architectures are mostly based on the dense model of Qwen-2.5 (Qwen Team, 2024). Recent work has indicated that the number of layers still significantly impacts the final performance of smaller models (Allal et al., 2024; Liu et al., 2024b). To eliminate the influence of the number of layers and derive cleaner scaling law, we utilize the architecture of Qwen-2.5-3B (comprising 36 layers, 16 attention heads, and 2 KV groups) and vary the hidden size / intermediate size within this framework. By keeping the number of layers constant and increasing the parameter width, we can more fairly compare the latency of parallel scaling and parameter. The final model structure is presented in the Table 7. Table 7: Model architectures. Parameters (Non-Embedding) Hidden Size Intermediate Size 1 2 4 1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8 896 896 896 1,024 1,024 1,024 1,024 1,280 1,280 1,280 1,280 1,536 1,536 1,536 1,536 2,048 2,048 2,048 2,048 2,560 2,560 2,560 2,560 4,864 4,864 4,864 4, 5,504 5,504 5,504 5,504 6,912 6,912 6,912 6,912 8,320 8,320 8,320 8,320 11,008 11,008 11,008 11,008 13,824 13,824 13,824 13,824 535,813,376 538,195,842 540,577,412 545,340, 693,753,856 696,738,818 699,722,756 705,690,632 1,088,376,320 1,092,762,882 1,097,148,164 1,105,918,728 1,571,472,384 1,577,522,690 1,583,571,460 1,595,669,000 2,774,773,760 2,784,937,986 2,795,100,164 2,815,424,520 4,353,203,200 4,368,529,922 4,383,854,084 4,414,502,"
        },
        {
            "title": "D Training Loss for OpenWebText with Repeating Data",
            "content": "(a) Training Loss (b) Validation Loss Figure 7: Loss for training on OpenWebText for repeating several epochs. On the fifth epoch, the validation loss suddenly increases, while the model with more computations (N = 3B, = 2) shows stronger resistance to overfitting compared to the model with more parameters (N = 5B, = 1). Both Stack-V2-Python and Pile datasets contain more tokens than the total number used in our experiment (42 billion), and therefore previous scaling law experiments did not involve data reuse. Muennighoff et al. (2023) noted that the performance of scaling laws tends to change when training data is repeated. In this section, we explore how PARSCALE performs on smaller dataset, OpenWebText (Gokaslan et al., 2019), with repeating data. Figure 7 shows comparison of training loss and validation loss, with OpenWebText repeated over four cycles. At the transition from the end of the fourth epoch to the beginning of the fifth epoch, we observe significant decrease in training loss and notable increase in validation loss, indicating overfitting. This aligns with the optimal number of epochs being four for training language models, as reported by Muennighoff et al. (2023). Comparing parallel scaling and parameter scaling, we observed an intriguing phenomenon: parallel scaling results in smaller decline in performance when overfitting occurs, while parameter scaling leads to larger decline. Specifically, by the time overfitting occurs, the validation loss of 3B parameter model with two-way parallel scaling matched that of 5 billion parameter model. This suggests that PARSCALE may alleviate the risk of overfitting, possibly due to having fewer parameters. As we increasingly face the depletion of pre-training data (Villalobos et al., 2022), further research into the scaling laws of computation in data-constrained scenarios presents compelling future direction."
        },
        {
            "title": "E Parametric Fitting for the Parallel Scaling Law",
            "content": "To obtain the practical parallel scaling law in Equation (5), based on the 24 runs (i.e., four six N) we obtain for each dataset, we follow the strategy from Hoffmann et al. (2022) and Muennighoff et al. (2023) to use the LBFGS algorithm and Huber loss for curve fitting. This process can be fomulated as: min A,k,E,α run (cid:16) HUBERδ log Li pred, log Li true (cid:17) , true is the i-th observed final loss obtained from experiments and Li where Li pred is the corresponding prediction based on the corresponding observations {N, P} and parameters {A, k, E, α}. We use δ = 0.001 for the Huber loss to avoid overfitting. Following Muennighoff et al. (2023); Hoffmann et al. (2022), we utilize the LBFGS algorithm (Liu & Nocedal, 1989) via SciPy (Virtanen et al., 2020) to locate local minima of the objective. The initialization grid is defined by: {e1, e0.5, e0}, {e4, e2, e0, e2, e4} 109, α {0, 0.5, 1, 1.5, 2}, {0.2, 0.4, 0.6, 0.9}. All parameters are constrained to be positive. After fitting, the optimal initialization is found to be away from the boundaries of our sweep. 24 Table 8: Fitting results of the logarithmic scaling law (Equation (5)) for Stack-V2 (Python). Table 9: Fitting results of the logarithmic scaling law (Equation (5)) for Pile. 1.130616 107 0.393463 0.691237 α 0.189371 3.677 105 0.9978 Fitting Huber Loss Fitting R2 1.973520 108 0.334456 1.288766 α 0.196333 1.814 105 0.9987 Fitting Huber Loss Fitting R2 Table 10: Prediction of the logarithmic scaling law (Equation (5)) for Stack-V2 (Python). Table 11: Prediction of the logarithmic scaling law (Equation (5)) for Pile. Parameters 1 1 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 4 8 8 8 8 8 8 535,813,376 693,753,856 1,088,376,320 1,571,472,384 2,774,773,760 4,353,203,200 538,195,842 696,738,818 1,092,762,882 1,577,522,690 2,784,937,986 4,368,529,922 540,577,412 699,722,756 1,097,148,164 1,583,571,460 2,795,100,164 4,383,854,084 545,340,552 705,690,632 1,105,918,728 1,595,669,000 2,815,424,520 4,414,502,408 Lpred 1.1728 1.1498 1.1123 1.0840 1.0439 1.0151 1.1509 1.1290 1.0932 1.0662 1.0280 1.0005 1.1340 1.1129 1.0784 1.0524 1.0156 0.9891 1.1198 1.0994 1.0661 1.0410 1.0053 0."
        },
        {
            "title": "Ltrue",
            "content": "1.1722 1.1496 1.1131 1.0817 1.0451 1.0213 1.1507 1.1262 1.0940 1.0623 1.0244 1.0025 1.1354 1.1124 1.0808 1.0490 1.0126 0.9906 1.1231 1.0997 1.0688 1.0383 1.0016 0.9794 Error 0.0006 0.0002 -0.0008 0.0023 -0.0012 -0.0062 0.0002 0.0028 -0.0008 0.0039 0.0036 -0.0020 -0.0014 0.0005 -0.0024 0.0034 0.0030 -0.0015 -0.0033 -0.0003 -0.0027 0.0027 0.0037 0.0003 Parameters 1 1 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 4 8 8 8 8 8 535,813,376 693,753,856 1,088,376,320 1,571,472,384 2,774,773,760 4,353,203,200 538,195,842 696,738,818 1,092,762,882 1,577,522,690 2,784,937,986 4,368,529,922 540,577,412 699,722,756 1,097,148,164 1,583,571,460 2,795,100,164 4,383,854,084 545,340,552 705,690,632 1,105,918,728 1,595,669,000 2,815,424,520 4,414,502,408 Lpred 2.1107 2.0701 2.0039 1.9542 1.8839 1.8335 2.0770 2.0381 1.9747 1.9270 1.8596 1.8113 2.0501 2.0125 1.9514 1.9053 1.8402 1.7936 2.0272 1.9908 1.9315 1.8869 1.8238 1."
        },
        {
            "title": "Ltrue",
            "content": "2.1113 2.0671 2.0027 1.9539 1.8876 1.8451 2.0772 2.0363 1.9730 1.9266 1.8610 1.8137 2.0544 2.0128 1.9509 1.9040 1.8394 1.7938 2.0364 1.9933 1.9318 1.8856 1.8218 1.7772 Error -0.0006 0.0030 0.0012 0.0003 -0.0037 -0.0116 -0.0002 0.0018 0.0017 0.0004 -0.0014 -0.0024 -0.0043 -0.0003 0.0005 0.0013 0.0008 -0.0002 -0.0092 -0.0025 -0.0003 0.0013 0.0020 0.0013 Tables 8 and 9 present the fitted parameters and evaluation metrics. Tables 10 and 11 show the prediction results based on the fitted parameters. 25 Table 12: Fitting results of the theoretical scaling law (Equation (4)) for Stack-V2 (Python). Table 13: Fitting results of the theoretical scaling law (Equation (4)) for Pile. 1.187646 107 ρ 0.891914 0.660016 α 0.175036 5.259 105 0.9959 Fitting Huber Loss Fitting R2 2.150890 108 ρ 0.899475 1.272178 α 0.190100 4.545 105 0.9968 Fitting Huber Loss Fitting R2 Table 14: Prediction of the theoretical scaling law (Equation (4)) for Stack-V2 (Python). Table 15: Prediction of the theoretical scaling law (Equation (4)) for Pile. Parameters 1 1 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 4 8 8 8 8 8 8 535,813,376 693,753,856 1,088,376,320 1,571,472,384 2,774,773,760 4,353,203,200 538,195,842 696,738,818 1,092,762,882 1,577,522,690 2,784,937,986 4,368,529,922 540,577,412 699,722,756 1,097,148,164 1,583,571,460 2,795,100,164 4,383,854,084 545,340,552 705,690,632 1,105,918,728 1,595,669,000 2,815,424,520 4,414,502,408 Lpred 1.1734 1.1507 1.1135 1.0853 1.0450 1.0158 1.1453 1.1238 1.0887 1.0620 1.0239 0.9964 1.1310 1.1102 1.0762 1.0503 1.0133 0.9866 1.1234 1.1030 1.0695 1.0440 1.0077 0."
        },
        {
            "title": "Ltrue",
            "content": "1.1722 1.1496 1.1131 1.0817 1.0451 1.0213 1.1507 1.1262 1.0940 1.0623 1.0244 1.0025 1.1354 1.1124 1.0808 1.0490 1.0126 0.9906 1.1231 1.0997 1.0688 1.0383 1.0016 0.9794 Error 0.0012 0.0011 0.0004 0.0036 -0.0001 -0.0055 -0.0054 -0.0024 -0.0053 -0.0003 -0.0005 -0.0061 -0.0044 -0.0022 -0.0046 0.0013 0.0007 -0.0040 0.0003 0.0033 0.0007 0.0057 0.0061 0.0020 Parameters 1 1 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 4 8 8 8 8 8 535,813,376 693,753,856 1,088,376,320 1,571,472,384 2,774,773,760 4,353,203,200 538,195,842 696,738,818 1,092,762,882 1,577,522,690 2,784,937,986 4,368,529,922 540,577,412 699,722,756 1,097,148,164 1,583,571,460 2,795,100,164 4,383,854,084 545,340,552 705,690,632 1,105,918,728 1,595,669,000 2,815,424,520 4,414,502,408 Lpred 2.1129 2.0726 2.0069 1.9574 1.8872 1.8367 2.0700 2.0317 1.9695 1.9225 1.8559 1.8080 2.0482 2.0110 1.9505 1.9048 1.8400 1.7935 2.0364 1.9998 1.9403 1.8953 1.8315 1."
        },
        {
            "title": "Ltrue",
            "content": "2.1113 2.0671 2.0027 1.9539 1.8876 1.8451 2.0772 2.0363 1.9730 1.9266 1.8610 1.8137 2.0544 2.0128 1.9509 1.9040 1.8394 1.7938 2.0364 1.9933 1.9318 1.8856 1.8218 1.7772 Error 0.0016 0.0055 0.0042 0.0035 -0.0004 -0.0084 -0.0072 -0.0046 -0.0035 -0.0041 -0.0051 -0.0057 -0.0062 -0.0018 -0.0004 0.0008 0.0006 -0.0003 -0.0000 0.0065 0.0085 0.0097 0.0097 0.0085 We also test our derived theoretical parallel scaling law (Equation (4)), in which the correlation coefficient ρ is treated as constant to fit. Tables 12 and 13 present the fitted parameters and evaluation metrics, while Tables 14 and 15 show the fitted results. It is evident that, whether for Stack or Pile, treating ρ as constant yields fitting accuracy that is inferior to the previously proposed logarithmic parallel scaling law. 26 Training Loss for Pile and Stack-V2-Python Figure 8 illustrates the curve of loss versus data size in our scaling law experiments. It clearly shows that scaling yields benefits, regardless of the data scale. (a) (b) (c) (d) (e) (g) (h) (j) (k) (f) (i) (l) Figure 8: Training loss for the Stack-V2-Python and the Pile, smoothing with 0.98 exponential moving average."
        },
        {
            "title": "G Downstream Datasets",
            "content": "Table 16: HumanEval Pass@1 (%) Table 17: HumanEval+ Pass@1 (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 12.8 15.9 15.2 18.9 15.9 20.7 17.1 18. 17.7 20.1 18.3 21.3 18.3 19.5 18.3 18.3 18.3 18.9 22.0 21.3 19.5 24.4 20.7 25.0 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 11.0 13.4 14.0 15.9 13.4 17.7 14.6 15.2 15.9 17.7 15.9 18.3 15.9 16.5 15.9 16.5 15.9 16.5 18.9 19.5 16.5 21.3 18.3 20. Table 18: MBPP Pass@1 (%) Table 19: MBPP+ Pass@1 (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 27.8 34.4 35.2 33.3 30.2 33.6 33.9 36. 31.5 36.8 39.4 39.9 36.0 42.6 40.7 45.5 40.5 47.4 45.5 47.4 45.8 47.1 50.3 48.4 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 23.5 27.0 28.0 29.1 26.2 27.8 27.2 29.1 25.7 30.7 32.3 33.6 31.2 35.4 32.5 38.9 34.7 39.4 37.3 40.5 38.4 38.9 40.5 42. Table 20: HumanEval Pass@10 (%) Table 21: HumanEval+ Pass@10 (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 25.0 25.0 26.2 29.9 21.3 27.4 30.5 32. 29.9 28.0 28.0 31.7 29.9 35.4 36.0 37.2 32.9 34.8 38.4 38.4 37.2 40.2 40.2 47.0 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 23.8 22.0 24.4 27.4 21.3 25.6 27.4 29.9 26.8 24.4 25.6 29.9 26.8 31.7 33.5 32.9 29.9 32.3 33.5 36.0 33.5 36.0 34.8 42. Table 22: MBPP Pass@10 (%) Table 23: MBPP+ Pass@10 (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 49.2 57.9 54.0 57.7 54.0 57.7 59.8 60. 57.7 60.3 61.1 66.1 61.4 64.0 66.9 67.5 68.3 67.7 70.9 72.8 66.4 72.0 73.5 75.1 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 40.2 46.8 43.7 46.0 44.7 48.4 49.5 50.8 47.9 50.5 52.1 56.6 51.6 54.2 56.6 56.1 54.5 58.2 59.0 60.6 56.3 60.6 62.7 62. Table 24: WinoGrande Performance (%) Table 25: Hellaswag Performance (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 51.9 51.0 52.4 51.7 51.2 51.4 53.0 53. 51.6 53.0 53.5 55.0 52.2 53.3 54.5 53.4 53.5 56.0 56.7 55.6 54.7 57.4 56.4 56.9 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 35.7 36.8 37.4 38.6 37.4 38.4 39.4 40.6 40.1 41.3 42.9 44.1 42.6 44.5 45.7 46.8 46.7 48.4 50.0 51.0 49.3 51.9 53.8 54. Table 26: OpenBookQA Performance (%) Table 27: PiQA Performance (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 26.0 26.8 26.6 26.8 28.8 26.6 29.0 27. 28.2 27.8 29.8 29.4 28.0 29.8 29.4 31.0 29.0 30.6 31.0 31.6 32.4 29.8 32.0 30.6 = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 65.0 65.7 65.8 66.5 65.8 66.8 66.7 66.9 66.9 67.4 68.0 67.8 67.5 68.5 68.8 69.5 68.8 70.5 70.6 70.9 69.5 71.3 72.1 71. Setup for 42B token experiments For HumanEval(+) (Chen et al., 2021) and MBPP(+) (Austin et al., 2021), we use the EvalPlus framework (Liu et al., 2023) for evaluation, where Pass@1 employs greedy decoding and Pass@10 employs temperature of 0.8. Considering that the pretrained base model cannot follow instructions, we use the direct completion format. For general tasks, we employ lm-eval harness (Biderman et al., 2024) and report normalized accuracy when provided. The number of few-shot mostly follows existing research configurations. Benchmarks include: WinoGrande (5-shot, Sakaguchi et al. (2021)), Hellaswag (10-shot, Zellers et al. (2019)), OpenBookQA (5-shot, Mihaylov et al. (2018)), PiQA (5-shot, Bisk et al. (2020)), ARC-Easy and ARC-Challenge (25-shot, Clark et al. (2018); we reporting the average of both), and SciQ (3-shot, Welbl et al. (2017)). 28 Table 28: ARC Performance (%) Table 29: SciQ Performance (%) = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 36.9 38.6 39.1 39.4 38.5 39.9 39.9 39.8 40.5 40.9 41.1 42.0 42.1 43.2 44.2 44.8 44.7 45.9 47.4 48.2 46.4 49.1 48.8 49. = 1 = 2 = 4 = 8 0.5B 0.7B 1.1B 1.6B 2.8B 4.4B 78.9 80.8 82.3 81.3 81.7 83.2 82.7 83.0 85.2 84.0 84.4 86.9 86.1 87.3 87.2 88. 88.5 90.5 91.2 91.2 91.0 91.4 91.7 94.2 Setup for 1T token experiments In the 1T token experiments, we use more challenging datasets for comprehensive testing, including MMLU (5-shot, Hendrycks et al. (2021a)) and RACE (4-shot, Lai et al. (2017)). In addition, we introduce mathematics reasoning datasets, including GSM8K (4-shot, Cobbe et al. (2021)), GSM8K-CoT (8-shot), and MATH (4-shot, Hendrycks et al. (2021b); using the Minerva evaluation rules (Lewkowycz et al., 2022)). When evaluating the Instruct model, we also use IFEval (0-shot, Zhou et al. (2023b)) and report the average among four metrics provided by lm-eval harness. Tables 16 to 29 show the downstream task performance in the 42B token experiments. We report the average of these performances in the main text (Tables 2 and 3)."
        },
        {
            "title": "H Compared with Beam Search",
            "content": "Some training-free inference-time scaling methods also expand parallel computation during the inference phase, with Beam Search being representative method. We applied Beam Search to the trained Baseline1.8B and compared it with PARSCALE to emphasize the importance of expanding parallel computation during the training phase. It is worth mentioning that Beam Search cannot be applied to the general tasks in Table 4 because these tasks primarily evaluate next-token prediction, whereas Beam Search can only be used for generation tasks. Therefore, we compared Beam Search on mathematics reasoning benchmarks. Table 30: Comparison with Beam-Search. Number of Parallels Method GSM8K GSM8K-CoT Minerva MATH 2 2 4 4 8 8 - PARSCALE (P = 2) Beam-Search (S = 2) PARSCALE (P = 4) Beam-Search (S = 4) PARSCALE (P = 8) Beam-Search (S = 8) 28.7 32.6 29. 34.7 27.7 38.4 22.5 35.9 35.6 37.8 40.8 37.8 43.7 30. 12.0 13.0 13.6 14.5 12.5 16.4 10.0 The results are shown in Table 30. It can be observed that Beam Search is only effective when beams = 2. As beams increases, the performance of Beam Search actually decreases. This indicates that, without strong verifier, it is difficult for LLM alone to select the correct result from multiple sampling results. This aligns with the finding in Stroebl et al. (2024). This experiment further emphasizes the importance of expanding parallel computation in both the training stage and inference stage."
        },
        {
            "title": "I Visualization for Different Parallel Streams",
            "content": "In the output aggregation, we assign different dynamic weights to different streams. These weights indicate the contribution ratio of different parallel streams to the next token prediction. We visualize the stream that contributes the most to each token (i.e., the stream with the highest aggregated weight), based on the 4.4B model pre-trained on the Pile. Tokens of the same color indicating that these tokens are primarily contributed by the same stream. Tables 31 to 33 visualize the results. An interesting observation is the locality of the colors: tokens in close proximity are often primarily contributed by the same stream, especially when is relatively small (e.g., = 2). 29 Table 31: Parallel stream visualization, where paragraphs are sampled from wikitext (Merity et al., 2016). Tokens with the same color indicate they are primarily contributed by the same stream. = 2 = 4 = Somerset progressed to the second round of the competition after Trinidad and Tobago beat Deccan in the final group match , but lost Trescothick , who flew home after recurrence of his illness . Wes Durston , who replaced Trescothick in the side , top - scored for Somerset in their next match , making 57 . Only two other players reached double - figures for the county , and the Diamond Eagles chased down the total with eight balls to spare . Somerset went into their final match , against the New South Wales Blues with slim mathematical chance of progressing , but strong bowling display from Brett Lee and Stuart Clark restricted Somerset to 111 , which the Australian side reached with ease . Somerset progressed to the second round of the competition after Trinidad and Tobago beat Deccan in the final group match , but lost Trescothick , who flew home after recurrence of his illness . Wes Durston , who replaced Trescothick in the side , top - scored for Somerset in their next match , making 57 . Only two other players reached double - figures for the county , and the Diamond Eagles chased down the total with eight balls to spare . Somerset went into their final match , against the New South Wales Blues with slim mathematical chance of progressing , but strong bowling display from Brett Lee and Stuart Clark restricted Somerset to 111 , which the Australian side reached with ease . Somerset progressed to the second round of the competition after Trinidad and Tobago beat Deccan in the final group match , but lost Trescothick , who flew home after recurrence of his illness . Wes Durston , who replaced Trescothick in the side , top - scored for Somerset in their next match , making 57 . Only two other players reached double - figures for the county , and the Diamond Eagles chased down the total with eight balls to spare . Somerset went into their final match , against the New South Wales Blues with slim mathematical chance of progressing , but strong bowling display from Brett Lee and Stuart Clark restricted Somerset to 111 , which the Australian side reached with ease . Table 32: Parallel stream visualization, where paragraphs are sampled from GSM8K (Cobbe et al., 2021). Tokens with the same color indicate they are primarily contributed by the same stream. = 2 = = 8 Question: baker is making bread according to recipe that requires him to use 3 eggs for every 2 cups of flour. If the baker wants to use up the 6 cups of flour he has remaining in his pantry, how many eggs will he need to use?. Answer: If he uses 6 cups of flour, the baker will be making 6/2 = <<6/2=3>>3 times the normal amount that the recipe describes. Thus, he must use 3*3 = <<3*3=9>>9 eggs. #### 9 Question: baker is making bread according to recipe that requires him to use 3 eggs for every 2 cups of flour. If the baker wants to use up the 6 cups of flour he has remaining in his pantry, how many eggs will he need to use?. Answer: If he uses 6 cups of flour, the baker will be making 6/2 = <<6/2=3>>3 times the normal amount that the recipe describes. Thus, he must use 3*3 = <<3*3=9>>9 eggs. #### 9 Question: baker is making bread according to recipe that requires him to use 3 eggs for every 2 cups of flour. If the baker wants to use up the 6 cups of flour he has remaining in his pantry, how many eggs will he need to use?. Answer: If he uses 6 cups of flour, the baker will be making 6/2 = <<6/2=3>>3 times the normal amount that the recipe describes. Thus, he must use 3*3 = <<3*3=9>>9 eggs. #### 9 30 Table 33: Parallel stream visualization, where paragraphs are sampled from RACE (Lai et al., 2017). Tokens with the same color indicate they are primarily contributed by the same stream. = 2 = 4 = 8 The Mysterious Universe By Ellen Jackson and Nic Bishop How did the universe begin? How big is it? What is dark matter? Cosmologist and expert supernova hunter Alex Filippenko hopes that supernovas can help us answer some of these questions. But first weve got to find them! Join Alex and his team as they go on the hunt with huge telescopes and banks of computers. The Time and Space of Uncle Albert By Russell Stannard What would you say if your uncle asked you whether you would like to go into space? Youd say, When do leave?, just like the girl in this story. Gedanken is speeding across the universe trying to help her uncle answer some questions, such as How big is space? and Where does gravity come from? Along the way she also discovers how to get heavier without getting fat, how to live forever without knowing it, and the strange things that can happen when you go really fast. Georges Secret Key to the Universe By Lucy Hawking and Stephen Hawking When George chases his pet pig through hole in the fence, little does he expect that he will soon be riding comet around Saturn . But just as he discovers the joys of space exploration with the computer Cosmos, which can open doors anywhere in the universe, everything starts to go wrong. The Mysterious Universe By Ellen Jackson and Nic Bishop How did the universe begin? How big is it? What is dark matter? Cosmologist and expert supernova hunter Alex Filippenko hopes that supernovas can help us answer some of these questions. But first weve got to find them! Join Alex and his team as they go on the hunt with huge telescopes and banks of computers. The Time and Space of Uncle Albert By Russell Stannard What would you say if your uncle asked you whether you would like to go into space? Youd say, When do leave?, just like the girl in this story. Gedanken is speeding across the universe trying to help her uncle answer some questions, such as How big is space? and Where does gravity come from? Along the way she also discovers how to get heavier without getting fat, how to live forever without knowing it, and the strange things that can happen when you go really fast. Georges Secret Key to the Universe By Lucy Hawking and Stephen Hawking When George chases his pet pig through hole in the fence, little does he expect that he will soon be riding comet around Saturn . But just as he discovers the joys of space exploration with the computer Cosmos, which can open doors anywhere in the universe, everything starts to go wrong. The Mysterious Universe By Ellen Jackson and Nic Bishop How did the universe begin? How big is it? What is dark matter? Cosmologist and expert supernova hunter Alex Filippenko hopes that supernovas can help us answer some of these questions. But first weve got to find them! Join Alex and his team as they go on the hunt with huge telescopes and banks of computers. The Time and Space of Uncle Albert By Russell Stannard What would you say if your uncle asked you whether you would like to go into space? Youd say, When do leave?, just like the girl in this story. Gedanken is speeding across the universe trying to help her uncle answer some questions, such as How big is space? and Where does gravity come from? Along the way she also discovers how to get heavier without getting fat, how to live forever without knowing it, and the strange things that can happen when you go really fast. Georges Secret Key to the Universe By Lucy Hawking and Stephen Hawking When George chases his pet pig through hole in the fence, little does he expect that he will soon be riding comet around Saturn . But just as he discovers the joys of space exploration with the computer Cosmos, which can open doors anywhere in the universe, everything starts to go wrong."
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Group",
        "Zhejiang University"
    ]
}