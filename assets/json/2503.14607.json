{
    "paper_title": "Can Large Vision Language Models Read Maps Like a Human?",
    "authors": [
        "Shuo Xing",
        "Zezhou Sun",
        "Shuangyu Xie",
        "Kaiyuan Chen",
        "Yanjia Huang",
        "Yuping Wang",
        "Jiachen Li",
        "Dezhen Song",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 7 0 6 4 1 . 3 0 5 2 : r CAN LARGE VISION LANGUAGE MODELS READ MAPS LIKE HUMAN? Shuo Xing1*, Shuangyu Xie2*, Zezhou Sun3*, Kaiyuan Chen2, Yanjia Huang1, Yuping Wang4, Jiachen Li5, Dezhen Song3, Zhengzhong Tu1 1Texas A&M University 2UC Berkeley 3MBZUAI 4University of Michigan 5UC Riverside {shuoxing, tzz}@tamu.edu Figure 1: MapBench is dataset of over 1600 map space path-finding problems from 100 diverse map images. MapBench evaluates language-based navigation instructions generated by Large Vision-Language Models (LVLMs) with map images with cluttered and potentially occluded visual symbols."
        },
        {
            "title": "ABSTRACT",
            "content": "In this paper, we introduce MapBenchthe first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given map image and query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLMgenerated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source Equal Contribution. Corresponding author. Can Large Vision Language Models Read Maps like Human? PREPRINT and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench."
        },
        {
            "title": "Introduction",
            "content": "Not all those who wander are lost. J. R. R. Tolkien Maps have guided travellers since ancient times. From traditional nautical charts to modern digital navigation systems, our capacity to transform two-dimensional pixel representations into actionable navigation instructions remains distinctly human skill. The recent emergence of Large Vision-Language Models (LVLMs) [19] reshapes the boundaries of human and machines in visual perception [1012] and language understanding [13, 14] across various domains, including biomedical imaging [1517], autonomous vehicles [1822], robotics [2326], and sciences [] In this paper, we explore whether LVLMs can replicate the human cognitive ability of map-space path finding: given humanreadable map image with landmarks and pathways, and query with beginning and end landmarks, LVLM generates language-based navigation instructions. Effective map-space path finding by LVLMs necessitates three fundamental capabilities: (a) perception that recognizes visual symbols such as colors, texts, areas, and icons in artistic or stylized forms; (b) spatial understanding that contextualizes the symbols to physical environment, addressing orientations, viewpoint, occlusion handling, and scaling; (c) planning that routes between endpoints through landmarks and intersections. While research has advanced each individual component independently [2731], the integration of these capabilities must simultaneously interpret visual symbolic representations and extract spatial relationships to enable coherent path finding. Existing LVLMs exhibit limitations in cross-modal misalignment [19, 3234] and understanding complex geometric relationships within visual inputs [35]. As result, it is critical to investigate LVLM capabilities specifically as unified context of map-space path finding, effectively bridging visual perception, spatial reasoning and route planning. We study the map-space path finding task for VLM with Map Space Scene Graph (MSSG), structured representation that captures visual symbolic-spatial-topological relationship of human-readable maps. Based on MSSG, we introduce MapBench, the first comprehensive dataset specifically designed to evaluate LVLMs on human-readable maps. MapBench encompasses over 1,600 visual path planning queries derived from 100 distinct maps, categorized across 4 visual style elements (landmarks, traversable areas, projection methods, and annotation formats) and 9 scenarios (urban, university, theme park, etc.). Each map includes manually annotated MSSG along with associated queries of beginning and end landmarks. We evaluate multiple state-of-the-art LVLMs on MapBench, including Llama-3.211B-Vision-Instruct [5], Qwen-2-VL-7B [7], GPT-4o [36], and GPT-4o mini [37] under both zero-shot prompting and chain-of-thought reasoning protocols. To further investigate the performance in map-space path finding, we propose Chain-of-Thought (CoT) augmented reasoning framework that aligns with the proposed MSSG by decomposing map navigation into sequential cognitive processes, including (1) localization of start and destination landmarks; (2) description of surrounding context; (3) path connection through identifying intermediate landmarks to enable LVLMs to perform explicit spatial reasoning. Our results indicate that MapBench poses significant challenges for state-of-the-art LVLMs. Additionally, closed-ended LVLMs outperform their open-sourced LVLMs. The proposed CoT reasoning framework generally delivers better performance compared to zero-shot prompting, though it occasionally introduces redundant information. In summary, our contribution is as follows: Map Space Scene Graph (MSSG), visual symbolic-spatial-topological representation of maps, MSSG-based LVLMs evaluation metrics and conversion algorithms between MSSG and natural language, MapBench, human readable map benchmark with 1600 human-annotated map queries CoT-augmented reasoning framework that decomposes map navigation into sequential prompting, Evaluation results of LVLMs on MapBench with both zero-shot and Chain-of-Thought augmented prompting."
        },
        {
            "title": "2 Related Works",
            "content": "Vision Language Model Reasoning and Planning Chain-of-Thought (CoT), whereby the simple inclusion of accurate deduction steps for few-shot examples within the original prompt empowers LLMs to achieve substantial performance improvements in reasoning tasks [38], pioneers the revolution reasoning and planning tasks capabilities of large language models and vision language models by prompting based strategies [3945]. Moreover, integrating ex2 Can Large Vision Language Models Read Maps like Human? PREPRINT ternal tools, knowledge bases, and iterative self-reflection enhances the robustness and adaptability of VLMs, driving them toward more sophisticated, human-like visual intelligence in complex real-world tasks [22, 23, 25, 4648]. Visual Language Navigation Visual Language Navigation (VLN) tasks require agents to interpret natural language instructions to navigate through photorealistic or real-world environments. Some previous work has mad significant progress in field. For example, leveraging large language models (LLMs) like NavGPT enhances instruction understanding and reasoning capabilities, enabling zero-shot sequential action predictions by decomposing instructions into sub-goals and incorporating commonsense knowledge [49] [50] [51]. Additionally, frameworks such as DAVIS utilize visual consistency to anticipate discrepancies in unseen environments by combining semi-supervised learning with reinforcement learning and integrating online visual-language mapping techniques, allowing agents to parse natural language instructions into executable plans aligned with real-time visual inputs, thereby facilitating better adaptation during performance [52] [53] [54] [55]. Despite these advancements, even with the use of frontier-based exploration methods in language-driven zero-shot object navigation [56] [57], existing approaches still heavily rely on detailed visual inputs, including depth and semantic information, and struggle to generalize to unseen environments due to discrepancies between training and testing conditions. Moreover, they seldom utilize high-level, human-readable maps that humans commonly rely on for navigation. VQA reasoning and planning benchmarks Visual Question Answering (VQA) has been widely studied task in the field of computer vision and natural language processing, with the aim of enabling models to answer questions about images. Prior VQA benchmark extends from CV dataset focusing on evaluating simple visual-language correlation task, such as image classification (VQA [58], MSCOCO [59]). The general VQA benchmark covers domain specific tasks such as (MM-Vet [60], MMBench [61], MMMU [62], MMMU-Pro [63]). Reasoning about spatial relationships using images with simple context is exemplified in works such as MathVista [64], ChartQA [65], and ScienceQA [66]. We focus on visually complicated map reading task that has multimodality unstructured information. Dataset Perception ScienceQA [66] TextVQA [67] VizWiz [68] MM-Vet [60] MMMU [62] MMMU-Pro [63] MapBench Text Recognition Spatial Reasoning Long Horizon Table 1: The comparison between MapBench and existing LVLM VQA benchmarks."
        },
        {
            "title": "3 Map Space Scene Graph",
            "content": "We present map space scene graph (MSSG) as indexing data structure for the human readable map. We provide functions that enabling conversion from language instruction to visual map and visual map to language. 3.1 MSSG Definition For each map image Ii Dm, we manually construct an outdoor scene graph Gi = (V, E) that captures semanticspatial-topology relationships between different locations. In the graph Gi, vertices are the locations and key intersections/road crossings = Vl Vc and edges reflect the semantics of connectivity. Specifically, we define the landmark node vl = (x, y, r, s) Vl that contains an image pixel coordinate (x, y) Im reflecting the center of the landmark, radius with center coordinate that defines covering circle for the landmark, and semantic label referring to the name of the location, e.g. = \"castle\". The intersection node vc = (x, y, s) Vc that contains an image pixel coordinate (x, y) Im reflecting the center of the intersection region and semantic label = \"intersection\". We define fully connected graph and for every edges, = (vi, vj, c) 3 Can Large Vision Language Models Read Maps like Human? PREPRINT Figure 2: Map Space Scene Graph for human-readable map. E, we define as connectivity attribute: = \"connect\", \"adjacent\", \"observable\", \"unrelated\", if vi, vj Vc 1road(vi, vj), if (vi Vl vj Vl) 1adj(vi, vj), if (vi Vl vj Vl) 1obs(vi, vj), Otherwise. where 1road(vi, vj) is the indicator function of road existence between vi and vj, 1adj(vi, vj) is the indicator function of adjacency between vi and vj. Note that the semantic label is for convenience of interpreting the scene graph into language description [69]. An example of scene graph is shown in Fig. 2: Definition 1. MSSG Connectivity Every landmark node is at least adjacent to one intersection node. deg(vl) Every intersection node is at least connected to one intersection node. deg(vc) 1 Except for end intersection node, every other intersection node should connect to two other intersection node. The connectivity conditions ensure there is at least one path that exists in the graph for any given start node and end node pair. 3.2 MSSG-Language Conversion 3.2.1 MSSG to Language Given path on MSSG, we convert it to language description. The algorithm (Alg. 1) iteratively converts the edges to language directives from the start node to the end node. In the conversion, there are three different types of language template functions that handle the road walking description (describeWalk()), the move description from landmark to road intersection (describeMove()), and surrounding description function (describeSurrounding()). After the conversion, the language description for each edge is concatenated together as navigation narrative. 3.2.2 Language to MSSG Path To convert natural language navigation instruction into structured MSSG path, we employ systematic parsing approach. Given an annotated map image Ii and its corresponding MSSG Gi , we use Alg. 2 to extract meaningful spatial relationships and transform them into structured path representation. The process begins with getLocations(), which identifies the locations mentioned in the textual instruction and maps them to corresponding nodes in the MSSG. Next, split2edges() establishes connections between these nodes by analyzing spatial relationships described in the instruction, effectively segmenting the parsed locations into sequence of connected edges. Finally, getPath() constructs 4 Can Large Vision Language Models Read Maps like Human? PREPRINT Algorithm 1: MSSG Path Parser Input: Gm, vs, ve, path Output: language-path 1 language-path := /* 2 = findStartEdge(vs, path) 3 while True do language conversion /* parsing each edge direction = getDirection(e.vi, e.vj) if e.c == connected then /* Walk along the road walk = describeRoad(direction,vi, vj) else /* Move between landmark and node walk = describeMove(direction,vi, vj) surrounding = describeSurrounding(Gm,e) language-path += (walk + surrounding) if e.vj == ve then language-path += arrival-note = findNextEdge(e, path) 5 6 7 8 9 11 12 13 */ */ */ */ navigable MSSG path by linking these edges into coherent route that aligns with the original language input. This conversion enables LVLMs to reason about navigation in structured, graph-based manner, improving their ability to process and interpret complex map-based pathfinding tasks. By structuring the navigation process into nodes (landmarks and intersections) and edges (connections between locations), this algorithm facilitates machine-readable, scalable, and spatially accurate representation of human-readable navigation instructions. Algorithm 2: Language Path Parser Input: language-path, Gi Output: path 1 loc := getLocations(language-path, Gi) ; 2 roads := split2edges(loc, language-path, Gi) ; 3 path := getPath(loc, roads)"
        },
        {
            "title": "4 MAPBENCH",
            "content": "// Nodes // Edges In this section, we present MAPBENCH, diverse and comprehensive benchmark designed to evaluate the performance of LVLMs in map-space path finding tasks across wide range of artistic illustrated human-readable maps = {I}. Each query consists of an image of map, specified starting point, and destination. The goal is to assess the models ability to perceive and interpret visual input, understand map structures, reason about spatial relationships, and generate an accurate and efficient path between the given points. 4.1 Data Collection and Curation As first-of-its-kind benchmark, MAPBENCH comprises curated collection of 100 high-quality map images spanning nine distinct types based on usage scenarios: Zoo, Museum, National Park, Campus, Google Maps, Theme Park, Trail, Urban, and Mall. For each map image, we utilize LabelMe [70] to manually construct the ground truth MSSG introduced in Section 3.1. This process enables the extraction of key spatial and semantic elements, including landmarks, pathways, intersections, and region boundaries, providing structured representation of the maps information. To ensure rigorous and reliable evaluation, we use GPT-4o as reference standard, generating 20 randomly selected start-destination pairs per map, each repeated three times. We then filter out queries that do not receive valid responses and use the remaining ones to construct the final benchmark queries for each image. In total, we compile final dataset of 1649 instances, with detailed statistics presented in Appendix C. To further characterize the dataset, we categorize maps according to three fundamental attributes: landmark representation, traversable area type, and map projection, as illustrated in Figure 3. Can Large Vision Language Models Read Maps like Human? PREPRINT Landmark Representation: Landmarks are categorized as points, contours, or images, each influencing how models segment and interpret the map. Traversable Areas: These are classified into roads and areasroads feature well-defined intersections, while areas do not, which may impact the models ability to discern connectivity between landmarks. Map Projection: Maps are categorized as orthographic or oblique projections, as projection type may affect the models perception of spatial relationships and distances. 4.2 Metrics In this section, we introduce set of metrics designed to characterize the unique instructions and features of humanreadable maps, providing comprehensive framework for their evaluation. First, to capture image complexity and query difficulty, we define several key indices, including the Elements Index, Meshedness Index, Average Shortest Path Length Index, and Query Difficulty Index, as detailed in Section 4.2.1. These metrics quantify the structural properties of maps and the challenges they pose for LVLM-based pathfinding (details statistics can be found in Table 5). Additionally, we establish performance evaluation metrics in Section 4.2.2, which assess the accuracy and effectiveness of model predictions. These include Landmark/Roadname Accuracy, which measures the precision of textual map references, Path Quality Score, which evaluates the optimality of generated routes."
        },
        {
            "title": "4.2.1 Task Complexity",
            "content": "Task complexity is determined by both graph complexity and query difficulty. For graph complexity, we evaluate it across three dimensions: the number of graph elements, the degree of connectivity, and the overall graph scale. We define Elements Index to quantify the total number of nodes, representing landmarks and intersections, along with the total number of edges in given graph. Definition 2. Elements Index. For graph = (V, E) : EI = + (1) As for connectivity, we define the Meshedness Index to evaluate the number of cycles in graph relative to the maximum possible number of cycles. higher Meshedness Index indicates more interconnected network. Definition 3. Meshedness Index [71]. For graph = (V, E) : = V + 1 2 5 (2) The Meshedness Index ranges from 0 to 1, providing measure of connectivity that is independent of the number of nodes in the graph. Being solely based on the number of nodes and edges, EI and MI indices remain limited in revealing structural differences between networks of equal size. To address this, we define the Average Shortest Path Length Index as more comprehensive measure of the internal structural differences within network. Scenes EI MI ASPLI Difficulty Google Map Mall Museum National Park Theme Park Trail Campus Urban Zoo 71.5 93.5 59.5 38.8 59.8 59.0 45.5 71.8 102.5 0.121 0.023 0.041 0.071 0.152 0.055 0.150 0.095 0.143 Easy Easy 4.678 Medium 5.839 Medium 4.124 5.185 4.870 Medium 5.277 Medium 4.180 Medium 6.230 Hard 5.663 Hard Table 2: The EI, MI and ASPLI index for each map type. Definition 4. Average Shortest Path Length Index [72]. For graph = (V, E) : ASP LI = 1 (V 1) (cid:88) i=j d(vi, vj) (3) 6 Can Large Vision Language Models Read Maps like Human? PREPRINT Here, d(vi, vj) denotes the shortest distance between vi and vj, where vi, vj Similarly, we define the Query Difficulty Index to considers the difficulty of solving or navigating the query case. Definition 5. Query Difficulty Index. For query in the graph described by (G, s, t) : QDI(G, s, t) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ℓ(Li) (4) Here, Li denotes simple path from source to target t. is the total number of all simple paths from to t, where simple path [73] is path with no repeated nodes. ℓ(L) denotes the path length. Scenes Mean Min Max Variance Difficulty Google Map Mall Museum National Park Theme Park Trail Campus Urban Zoo 11.45 10.47 6.93 12.43 9.62 9.60 13.45 10.07 18.71 4.00 5.50 1.71 2.37 1.44 2.77 2.13 2.76 2.04 21.01 20.17 12.77 33.11 18.73 22.21 23.00 19.99 41.43 29.82 47.00 8. Easy Easy Easy Easy Easy 96.15 Medium 50.29 55.69 46.86 Medium 52.91 Easy 145.04 Hard Table 3: Query Difficulty statistics for each map type. 4.2.2 Performance Metrics Given language path, we convert it to MSSG path and compare it with the MSSG graph path. We can evaluate the path feasibility and quality metrics including connectivity descriptions and format accuracy. Language Description Accuracy We evaluate whether the model correctly identifies and describes language paths. The assessment consists of three levels: path existence check, linguistic coherence check, and format compliance check: Missing Paths refer to cases where the model either fails to output route or generates response indicating that no path is found. Linguistic Incoherence occurs when the generated route descriptions are discontinuous, such as inconsistencies between the start and end points or interruptions in the middle of route. Format Non-Compliance refers to cases where the route descriptions do not adhere to predefined formatting requirements, such as failing to use landmarks for path descriptions or relying on informal phrasing instead. We calculate the proportion of errors for each of these three categories, providing insights into the systems robustness in generating accurate and comprehensible navigation instructions. Path Quality Score We define the Path Quality Score based on the ratio of the length of the MSSG path to the length of the shortest path. Definition 6. Path Quality Score For query in the graph described by (G, s, t). QS(G, s, t) = ℓ(LM SSG) ℓ(Lsp) (5) Here, Lsp denote the shortest path from source to target t, and LM SSG denote the MSSG path. 4.3 Comparison with Existing Benchmarks MapBench is the first comprehensive and specialized benchmark focusing on the map-space pathfinding, featuring high-quality human annotations. Unlike the previous VQA benchmarks, which primarily focus on static object recognition and short-form question-answering, MapBench integrates spatial reasoning and long horizon planning, making 7 Can Large Vision Language Models Read Maps like Human? PREPRINT Figure 3: Sampled MapBench examples from each scenario. Segmenting the map and navigating based on the query require expert-level spatial reasoning and understanding. it uniquely suited for assessing LVLMs capabilities in real-world navigation tasks. Furthermore, MapBench pioneers the first structurally-grounded human annotation framework for pathfinding tasks MSSG. It not only provides precise and effective evaluation of model performance but also serves as valuable tool for future research and development in map-based reasoning and navigation, bridging the gap between topological understanding and multimodal AI. The detailed comparsion can be found in Tab. 1. Figure 4: Illustration of CoT extension to VLMs."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct comprehensive empirical analysis of state-of-the-art vision-language models (VLMs) on our proposed MapBench, assessing both open-source and proprietary LVLMs (detailed information can be found in the appendix). Setup For all evaluation experiments, we take the input maps as the visual input of the LVLMs, along with the textual instructions. The evaluated models spans both open-sourced and proprietary LVLMs: Llama-3.2-11B-VisionInstruct [5], Qwen-2-VL-7B [7], GPT-4o [36], and GPT-4o mini [37]). To ensure comprehensive analysis, we assess model performance under two distinct settings: 8 Can Large Vision Language Models Read Maps like Human? PREPRINT Method Model Google Map Mall Museum National Park Theme Park Trail Campus Urban Zoo Zero-shot CoT Llama-3.2 Qwen2-VL GPT-4o mini GPT-4o Llama-3.2 Qwen2-VL GPT-4o mini GPT-4o 1.868 3.289 2.755 1.670 2.095 2.532 2.277 1.820 3.962 2.363 3.086 2.372 3.366 2.087 4.182 1. 2.652 2.817 3.526 1.706 3.196 1.879 3.215 2.142 3.859 2.154 2.367 2.372 2.859 2.308 2.222 2.766 3.401 2.027 1.327 1.436 2.995 1.577 1.520 1. 3.121 2.188 2.866 2.766 2.121 2.088 1.213 1.884 2.195 2.459 4.032 2.133 3.328 2.203 4.037 1.865 2.194 3.840 2.084 2.755 3.153 2.951 1.855 2. 1.989 1.961 2.084 1.886 2.365 2.213 2.067 2.040 Table 4: The averaged score of the SOTA LVLMs on MapBench under zero-shot prompting and CoT reasoning. Algorithm 3: Overview of CoT reasoning Input: I, Ls, Le, P, VLM Output: readable-navigation 1 readable-navigation := /* language conversion 2 location-information = Localization(VLM, I, Ls, Le, P) 3 concat(P, location-information) 4 surrounding = describeSurrounding(VLM, I, Ls, P) 5 concat(P, surrounding) 6 readable-navigation := 7 while True do 8 navigation = connectPath(VLM, I, Ls, Le, P) concat(readable-navigation, navigation) if reach Le then break 9 10 */ /* Connecting each intermediate landmark and major intersection into feasible path */ 12 concat(P, detailed-navigation) 13 readable-navigation = Summarize(VLM, I, P) Zero-Shot Prompting: The models are prompted to generate accurate and relevant responses based solely on the provided visual inputs (maps) and textual instructions (presented as the following), without any prior specific training on similar tasks. CoT Reasoning: To further investigate model performance on MapBench, we introduce CoT reasoning framework that aligns seamlessly with our proposed structural data, aiming to simulate more deliberative cognitive process. The details of our curated CoT reasoning framework for generating navigation instructions from map inputs are presented in Figure 4 and Algorithm 3. Generally, the framework is built upon four fundamental, interdependent components as follows: Localization: Prompt the LVLMs to identify key landmarks and their spatial relationships within the map, associating them with corresponding coordinates. This process allows LVLMs to generate simplified MSSG, using landmark locations to understand connections and structures within the map. DescribeSurrounding The backbone LVLM is then prompted to generate detailed information about the surroundings of the starting landmark, including their spatial relationships and connectivity, ensuring comprehensive understanding of the environment. ConnectPath After obtaining the topology and structural information from the constructed simplified MSSG in natural language, the model is tasked with extracting key clues and spatial relationships essential for generating an optimal navigation path (shortest path) between the given start and destination points. This process is completed by iteratively verifying whether the destination has been reached or if further adjustments are required. Summarize The complete navigation route is generated as human-readable output, which provides concise yet comprehensive navigation summary. This summary distills the step-by-step route details, ensuring clarity and interpretability for users. 9 Can Large Vision Language Models Read Maps like Human? PREPRINT Throughout the execution of Algorithm 3, the textual instruction is progressively augmented in alignment with the sequential order of these four components, forming coherent reasoning chain tailored for the navigation planning task. Results The detailed performance of LVLMs on MapBench is shown in Table 1, and our key findings are summarized as follows: Challenging Evaluation. As shown in Table 4, the results indicate the relative quality of the navigation routes generated by the model compared to the ground truth shortest path. Specifically, the evaluation highlights how closely the model-generated routes align with optimal paths in terms of efficiency, providing insights into the models navigation performance. Notably, the results indicate that the performance of LVLMs falls significantly short of the theoretically optimal navigation, exposing key limitations in multimodal information understanding, spatial reasoning, and decision-making under complex long-horizon planning. The instances of output responses can be found in Appendix E. Open-source Models versus Closed-source Models. The open-sourced LVLMs generally outperform the closedsource LVLMs, demonstrating superior adaptability and robustness in map-space pathfinding tasks. Among all evaluated models, GPT-4o achieves the highest performance as expected. Furthermore, huge proportion of queries fail to produce valid response on navigation when using open-sourced LVLMs, highlighting the challenges these models face in interpreting complex map structures, reasoning over spatial relationships, and providing coherent navigation instructions. Performance across Different Prompting Strategy. Generally, LVLMs exhibit superior performance on MapBench when using Chain-of-Thought (CoT) prompting compared to zero-shot prompting. CoT prompting enhances spatial reasoning and step-by-step path planning, allowing models to break down complex navigation tasks into interpretable intermediate steps. However, in pathfinding tasks, LVLMs with CoT reasoning often generate redundant information about landmarks and intersections, even when these locations are far from the intended route. Instead of focusing on the optimal and necessary path elements, CoT prompting sometimes overemphasizes peripheral locations, which may distract from efficient route planning. This tendency to generate excessive contextual details is primary reason why the CoT results in Table 4 show inconsistencies or suboptimal performance, despite the overall improvements in logical reasoning and interoperability (instance can be found in Appendix E)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce MapBench, the first-of-its-kind benchmark designed to evaluate LVLMs in human-readable map-space pathfinding tasks. MapBench comprises 100 high-quality map images spanning nine distinct types based on real-world usage scenarios, including Zoos, Museums, National Parks, Campuses, Google Maps, Theme Parks, Trails, Urban Areas, and Malls, along with 1,649 diverse queries. It establishes new standard for assessing LVLMs capabilities in perception, text recognition, spatial reasoning, and long-horizon planning. Additionally, we present the MSSG, structured indexing data representation that encodes landmarks, paths, and spatial relationships within human-readable maps. To support robust evaluation, we propose suite of task complexity and performance metrics, designed to assess both the difficulty of maps and queries and the quality of model-generated responses."
        },
        {
            "title": "7 Limitations",
            "content": "As the first benchmark designed to evaluate LVLMs in map-space pathfinding tasks, MapBench has certain limitations that highlight potential areas for future improvement. First, we integrate only 100 high-quality maps with human annotations of Map Space Scene Graph (MSSG). While this dataset covers diverse range of real-world scenarios, its scale remains limited, and expanding it to include more maps could further enhance its robustness. Additionally, due to the expert-level challenges in MapBench requiring advanced perception, text recognition, spatial reasoning, and long-horizon planning capabilities that are still evolving in current LVLMs, only small subset of state-of-theart LVLMs could produce valid responses suitable for evaluation, underscoring the gap between current LVLMs and human-level navigation abilities."
        },
        {
            "title": "References",
            "content": "[1] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 2 10 Can Large Vision Language Models Read Maps like Human? PREPRINT [2] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [4] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [5] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. 2024. URL https: //ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. 2, 8 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [7] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 8 [8] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhutowards real-world vision-language understanding. arXiv preprint oshu Li, Hao Yang, et al. Deepseek-vl: arXiv:2403.05525, 2024. [9] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 2 [10] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36:7209672109, 2023. 2 [11] Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan Bovik. Ugc-vqa: Benchmarking blind video quality assessment for user generated content. IEEE Transactions on Image Processing, 30:44494464, 2021. [12] Qi Zheng, Yibo Fan, Leilei Huang, Tianyu Zhu, Jiaming Liu, Zhijian Hao, Shuo Xing, Chia-Ju Chen, Xiongkuo Min, Alan Bovik, et al. Video quality assessment: comprehensive survey. arXiv preprint arXiv:2412.04508, 2024. 2 [13] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2 [14] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019. 2 [15] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. 2 [16] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. [17] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, et al. Biomedgpt: unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv e-prints, pages arXiv2305, 2023. 2 [18] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. [19] Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, et al. Autotrust: Benchmarking trustworthiness in large vision language models for autonomous driving. arXiv preprint arXiv:2412.15206, 2024. 2 11 Can Large Vision Language Models Read Maps like Human? PREPRINT [20] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. [21] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. arXiv preprint arXiv:2312.14150, 2023. [22] Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, and Zhengzhong Tu. Openemma: Open-source multimodal model for end-to-end autonomous driving. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 10011009, 2025. 2, 3 [23] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. arXiv preprint arXiv:2307.06135, 2023. 2, 3 [24] Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, and Ken Goldberg. Blox-net: Generative design-for-robot-assembly using vlm supervision, physics simulation, and robot with reset. IEEE International Conference on Robotics and Automation (ICRA), 2025. [25] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 3 [26] Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Siwei Han, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao. Grape: Generalizing robot policy via preference alignment. arXiv preprint arXiv:2411.19309, 2024. 2 [27] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [28] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [29] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37:7539275421, 2024. [30] Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, and Renjing Xu. Mapnav: novel memory representation via annotated semantic maps for vlm-based vision-and-language navigation. arXiv preprint arXiv:2502.13451, 2025. [31] Dylan Goetting, Himanshu Gaurav Singh, and Antonio Loquercio. End-to-end navigation with vision language models: Transforming spatial reasoning into question-answering. arXiv preprint arXiv:2411.05755, 2024. 2 [32] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. [33] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. [34] Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, and Zhengzhong Tu. Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization. arXiv, February 2025. doi: 10.48550/arXiv.2502.13146. 2 [35] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-llava: Solving geometric problem with multi-modal large language model, 2023. 2 [36] OpenAI. Hello gpt-4o. 2024. URL https://openai.com/index/hello-gpt-4o/. 2, 8 [37] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. 2024. URL https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. 2, [38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv, January 2022. doi: 10.48550/arXiv.2201.11903. 2 12 Can Large Vision Language Models Read Maps like Human? PREPRINT [39] Rui Pan, Shuo Xing, Shizhe Diao, Wenhe Sun, Xiang Liu, Kashun Shum, Renjie Pi, Jipeng Zhang, and Tong Zhang. Plum: Prompt learning using metaheuristic. ACL Findings, 2024. [40] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with gradient descent and beam search. arXiv preprint arXiv:2305.03495, 2023. [41] Shunyu Yao, Dian Yu, Google Deepmind, Jeffrey Zhao, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. In Advances in Neural InforTree of thoughts: Deliberate problem solving with large language models. mation Processing Systems, volume 36, pages 1180911822, 12 2023. URL https://github.com/ princeton-nlp/tree-of-thought-llm. [42] Weijia Xu, Andrzej Banburski, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt inference through Gibbs sampling. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5485254865. PMLR, 2127 Jul 2024. [43] Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models. 2 2024. URL https://arxiv.org/abs/2402.07754v2. [44] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal Chain-ofThought Reasoning in Language Models. arXiv, February 2023. doi: 10.48550/arXiv.2302.00923. [45] Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, and William Yang Wang. Visual chain of thought: bridging logical gaps with multimodal infillings. arXiv preprint arXiv:2305.02317, 2023. 2 [46] Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, et al. Emma: End-to-end multimodal model for autonomous driving. arXiv preprint arXiv:2410.23262, 2024. 3 [47] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European Conference on Computer Vision, pages 256274. Springer, 2024. [48] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. 3 [49] Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. URL https://arxiv.org/abs/2305.16986. 3 [50] Dylan Goetting, Himanshu Gaurav Singh, and Antonio Loquercio. End-to-end navigation with vision language models: Transforming spatial reasoning into question-answering, 2024. URL https://arxiv.org/abs/ 2411.05755. 3 [51] Davide Buoso, Luke Robinson, Giuseppe Averta, Philip Torr, Tim Franzmeyer, and Daniele De Martini. Select2plan: Training-free icl-based planning through vqa and memory retrieval, 2024. URL https://arxiv. org/abs/2411.04006. 3 [52] Yujie Lu, Huiliang Zhang, Ping Nie, Weixi Feng, Wenda Xu, Xin Eric Wang, and William Yang Wang. Anticipating the unseen discrepancy for vision and language navigation, 2022. URL https://arxiv.org/abs/ 2209.04725. 3 [53] Chengguang Xu, Hieu T. Nguyen, Christopher Amato, and Lawson L. S. Wong. Vision and language navigation in the real world via online visual language mapping, 2023. URL https://arxiv.org/abs/2310. 10822. 3 [54] Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, and Bin Zhao. Aerial vision-and-language navigation via semantic-topo-metric representation guided llm reasoning, 2024. URL https://arxiv.org/ abs/2410.08500. [55] Haodong Hong, Sen Wang, Zi Huang, Qi Wu, and Jiajun Liu. Navigating beyond instructions: Vision-andlanguage navigation in obstructed environments, 2024. URL https://arxiv.org/abs/2407.21452. 3 [56] Congcong Wen, Yisiyuan Huang, Hao Huang, Yanjia Huang, Shuaihang Yuan, Yu Hao, Hui Lin, Yu-Shen Liu, and Yi Fang. Zero-shot object navigation with vision-language models reasoning, 2024. URL https:// arxiv.org/abs/2410.18570. 3 13 Can Large Vision Language Models Read Maps like Human? PREPRINT [57] Tianrui Guan, Yurou Yang, Harry Cheng, Muyuan Lin, Richard Kim, Rajasimman Madhivanan, Arnie Sen, and Dinesh Manocha. Loc-zson: Language-driven object-centric zero-shot object retrieval and navigation, 2024. URL https://arxiv.org/abs/2405.05363. [58] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi In Proceedings of the IEEE international conference on computer Parikh. Vqa: Visual question answering. vision, pages 24252433, 2015. 3 [59] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 3 [60] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and arXiv preprint Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv:2308.02490, 2023. 3 [61] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. URL https://arxiv.org/abs/2307.06281. [62] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 3 [63] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 3 [64] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 3 [65] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 3 [66] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [67] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 3 [68] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. 3 [69] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2016. URL https://arxiv.org/abs/ 1602.07332. 4 [70] Bryan Russell, Antonio Torralba, Kevin Murphy, and William Freeman. Labelme: database and webbased tool for image annotation. International journal of computer vision, 77:157173, 2008. 5 [71] Jerˆome Buhl, Jacques Gautrais, Ricard Sole, Pascale Kuntz, Sergi Valverde, Jean-Louis Deneubourg, and Guy Theraulaz. Efficiency and robustness in ant networks of galleries. The European Physical Journal B-Condensed Matter and Complex Systems, 42:123129, 2004. [72] Jean-Paul Rodrigue. The geography of transport systems. Routledge, 2020. 6 [73] R. Sedgewick. Algorithms in C, Part 5: Graph Algorithms. Addison Wesley Professional, 3rd edition, 2001. 7 14 Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "A Map Visual Styles",
            "content": "Each human readable map can be viewed as combination of content and visual style. For maps presenting identical content, variations in the visual styles of landmarks and traversable areas may hinder robots ability to interpret the map accurately. From the perspective of the map creation process, we divide it into four phases: Sketching: This phase focuses on identifying the outline and location of the landmark. Contours contribute to size and geometric agency, while location allows the reader to clarify spatial relationships with other landmarks. Coloring: Color is applied to both landmarks and the ground. For landmarks, color serves as distinguishing feature between same type of landmarks. For the ground, different colors represent different areas, helping readers identify the areas characteristics (e.g., river, green space, dining area) and whether it is traversable. Figure 5: Illustrating the four phases of map creation. Projection: Projection determines how the three-dimensional world is represented on two-dimensional map. Different projection methods emphasize various aspects of spatial relationships. Common approaches include orthographic projection, which preserves the shape and proportional relationships of objects, and oblique projection, which provides certain degree of perspective effect, enhancing depth perception and spatial awareness, making the map more intuitive and readable. Annotation: Symbols and text provide additional information about landmarks, including their names and numbering, and help differentiate same type of landmarks and roads. In summary, we categorized the visual styles of human-readable maps based on how they differentiate landmarks, traversable areas, projection methods, and annotation details. These visual styles influence how planned routes are described in natural language. They are independent of each other and can be combined in various ways within readable map. However, to ensure consistency, we assume uniform visual style throughout the map. 15 Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "B Difficulty Classification",
            "content": "In this section, we explain how we classify graph difficulty and query difficulty into three levels: Easy, Medium, and Hard. B.1 Graph Difficulty Classification As defined in Section 4.2.1, there are three indexes for evaluating graph difficulty: EI, MI and ASPLI. First, we normalize the three indices by: Xnorm = Xmin Xmax Xmin (6) Here, stands for the set of EI, MI or ASPLI Index. Xmin and Xmax are the minimum and maximum values of the indexes, respectively. We assign the same weight to these three indexes, so the graph difficulty is calculate as: = EInorm + MInorm + ASPLInorm 3 (7) For the average graph difficulty davg in each scenario , we consider it to be easy for scenarios with davg < 0.33, moderate for scenarios with 0.33 davg 0.66, and difficult for scenarios with davg > 0.66. B.2 Query Difficulty Classification We similarly start by normalizing the query difficulty in each scenario. However, when calculating the query difficulty for each scenario, we consider both the mean and variance of the query difficulty: = Meannorm + Variancenorm 2 (8) For the query difficulty in each scenario , we similarly consider it to be easy for scenarios with < 0.33, moderate for scenarios with 0.33 0.66, and difficult for scenarios with > 0.66."
        },
        {
            "title": "C Map Query Statistics",
            "content": "We provides detailed breakdown of the MapBench dataset, summarizing the distribution of maps and queries across various real-world navigation scenarios. The dataset consists of 100 map images, with 93% using orthographic projection and 7% using oblique projection, ensuring diverse spatial representations. It includes total of 1,649 queries, categorized into nine different map types: Zoo (30.44%), Museum (13.22%), National Park (12.92%), Google Maps (11.16%), Campus (10.37%), Theme Park (10.13%), Trail (5.94%), Urban (5.94%), and Mall (2.85%). The dataset also classifies queries by difficulty levels, with 46.27% labeled as easy, 23.29% as medium, and 30.44% as hard, reflecting the varying complexities of pathfinding tasks. Furthermore, the difficulty of the graph is classified as easy (42%), medium (24%), and hard (34%), based on the structural complexity of the maps. These statistics highlight the diversity and challenge of MapBench, making it robust benchmark to evaluate the capabilities of LVLMs in human-readable map-based pathfinding."
        },
        {
            "title": "D Additional Experiment Results",
            "content": "Table 6 presents the failure case analysis of LVLMs evaluated on our proposed MapBench, highlighting the types and frequencies of errors encountered during the evaluation, categorizing errors into three primary types: Missing Paths, Linguistic Incoherence, and Format Non-Compliance. It can be observed that: Zero-shot prompting ensures high format compliance but introduces minor linguistic errors. CoT prompting generally improves linguistic coherence but may cause severe format inconsistencies in some models. GPT-4o mini struggles with format adherence under CoT, whereas GPT-4o sees major linguistic drop in the same setting. Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "Total Map images",
            "content": "* Orthographic Projection * Oblique Projection"
        },
        {
            "title": "Total Queries",
            "content": "* Zoo * Museum * National Park * Google Maps * Campus * Theme Park * Trail * Urban * Mall Query Difficulty: * Easy * Medium * Hard Graph Difficulty: * Easy * Medium * Hard 100 93 1649 502 218 213 184 171 167 98 98 47 763 384 502 42 24 34 93% 7% 30.44 % 13.22 % 12.92 % 11.16 % 10.37 % 10.13 % 5.94 % 5.94 % 2.85 % 46.27 % 23.29 % 30.44 % 42 % 24 % 34 % Table 5: Dataset statistics. Method Model Missing Paths Linguistic Incoherence Format Non-Compliance Zero-shot CoT Llama-3.2 Qwen2-VL GPT-4o mini GPT-4o Llama-3.2 Qwen2-VL GPT-4o mini GPT-4o < 1% 0% 0% 1.6% 9.0% 21.5% 48.4% 0% 1.3% 3.2% 3.2% 2.8% 0% < 1% 0% 24.1% < 1% < 1% 0% 0% 0% < 1% < 1% 0% Table 6: The failure rate of the SOTA LVLMs on MapBench under zero-shot prompting and CoT reasoning. 17 Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "E Response Examples",
            "content": "In this section, we present variety of navigation samples generated by LVLMs under two distinct prompting settings: zero-shot prompting and Chain-of-Thought (CoT) reasoning. Instance of Responses under Zero-Shot Prompting on Zoo Map Instuction: Please provide me navigation from Carousel to Safari Camp Classroom LLama-3.2: Carousel American Exhibit (to the Right, moving along the road American Exhibit Primate Exhibit (to the Left, moving along the road) Primate Exhibit Childrens Zoo (to the Right, moving along the road) Childrens Zoo Safari Camp Classroom (to the Left, moving along the road) Qwen2-VL: Carousel Childrens Zoo (from North, moving along Childrens Zoo Road) Childrens Zoo Primate Exhibit (from East, moving along Primate Exhibit Road) Primate Exhibit Safari Camp Classroom (from South, moving along Safari Camp Classroom Road) GPT-4o mini: Carousel Education Center (from South, moving along Education Center) Education Center Safari Camp Classroom (from North, moving along Safari Camp Center) GPT-4o: Carousel Primate Exhibit (head west along the main path) Primate Exhibit Safari Camp Center (continue west along the main path) Safari Camp Center Safari Camp Classroom (head north along the path) 18 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Zoo Map Instuction: Please provide me navigation from Carousel to Safari Camp Classroom LLama-3.2: No path found. Qwen2-VL: No path found. GPT-4o mini: No path found. GPT-4o: Carousel Childrens Zoo (from Right and Down, moving along Childrens Zoo Path from Carousel) Childrens Zoo Safari Camp Classroom (from Left and Down, moving along Nevada Safari Path from Childrens Zoo) 19 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Museum Map Instuction: Please provide me navigation from Science Stage to Fire Stairs 1 LLama-3.2: Science Stage Exit (from East, moving along Road) Exit Fire Stairs 1 (from North, moving along Road) Qwen2-VL: Science Stage Main Entrance (from Right, moving along Road) Main Entrance XPLOR Store (from Right, moving along Road) XPLOR Store Main Entrance (from Right, moving along Road) Main Entrance Down to Ground Floor (from Right, moving along Road) Down to Ground Floor Fire Stairs 1 (from Right, moving along Road) GPT-4o mini: Science Stage Main Entrance (from East, moving along the wall) Main Entrance XPLOR Store (from Left, moving along Main Lobby) XPLOR Store Scaife Gallery (from Left, moving along the wall) Scaife Gallery Fire Stairs 1 (from Up, moving along the wall) GPT-4o: Science Stage SpacePlace (from East, moving along Exit) SpacePlace H2Oh! Field Station (from South, passing Enter) H2Oh! Field Station Fire Stairs 1 (from West, moving along H2Oh! Why Our Rivers Matter) 20 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Museum Map Instuction: Please provide me navigation from Science Stage to Fire Stairs LLama-3.2: Science Stage Main Entrance (from Up, moving along Fire Stairs from Science Stage) Main Entrance XPLOR Store (from Up, moving along Fire Stairs from Main Entrance) XPLOR Store Fire Stairs 1 (from Up, moving along Fire Stairs from XPLOR Store) Qwen2-VL: No path found. GPT-4o mini: Science Stage Grable Atrium (from Left and Up, moving along Main Entrance from Science Stage) Grable Atrium Fire Stairs 1 (from Left and Down, moving along Main Entrance from Grable Atrium) GPT-4o: Science Stage All-User Restroom (from Left and Down, moving along Enter from Science Stage) All-User Restroom SpacePlace (from Left and Down, moving along Enter from All-User Restroom) 21 Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "Instance of Responses under CoT Reasoning on Theme Park Map",
            "content": "Instuction: Please provide me navigation from Lake Hamilton to Hot Springs National Park LLama-3.2: Lake Hamilton Lake Hamilton Bridge (across, moving along Lake Hamilton Bridge Lake Hamilton Bridge Hwy 7 (south, moving along Hwy 7 Hwy 7 Oaklawn Park (west, moving along Hwy 7 Oaklawn Park Hot Springs National Park (south, moving along Hwy 7 Qwen2-VL: Lake Hamilton Oaklawn Park (from South, moving along Road) Oaklawn Park Hot Springs National Park (from North, moving along Road) GPT-4o mini: Lake Hamilton Hot Springs National Park (from North, moving along Lake Hamilton GPT-4o: Lake Hamilton Oaklawn Park (head north along Lakeshore Drive Oaklawn Park Hot Springs National Park (continue north along Central Avenue Can Large Vision Language Models Read Maps like Human? PREPRINT"
        },
        {
            "title": "Instance of Responses under CoT Reasoning on Theme Park Map",
            "content": "Instuction: Please provide me navigation from Lake Hamilton to Hot Springs National Park LLama-3.2: Lake Hamilton Oaklawn Park (from Right, moving along N. Reynolds Rd from Lake Hamilton Oaklawn Park Hurricane Lake (from Right, moving along N. Reynolds Rd from Oaklawn Park Hurricane Lake Hot Springs National Park (from Right, moving along N. Reynolds Rd from Hurricane Lake Qwen2-VL: No path found. GPT-4o mini: Im unable to assist with that. GPT-4o: Lake Hamilton Oaklawn Park (from Right and Down, moving along Lake Park Road from Lake Hamilton) Oaklawn Park Hot Springs National Park (from Left and Down, moving along Central Avenue from Oaklawn Park) Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Google Map Instuction: Please provide me navigation from Auntie Annes 1 to Cold Stone Creamery LLama-3.2: Auntie Annes 1 Universal Studios Store (from North, moving along CityWalk) Universal Studios Store Cinnabon (from North, moving along CityWalk) Cinnabon Cold Stone Creamery (from East, moving along CityWalk) Qwen2-VL: Auntie Annes 1 Auntie Annes (from North, moving along Road) Auntie Annes Coca-Cola Refresh (from East, moving along Road) XPLOR Store Main Entrance (from Right, moving along Road) Coca-Cola Refresh Red Oven Pizza Bakery (from South, moving along Road) Red Oven Pizza Bakery Cold Stone Creamery (from West, moving along Road) GPT-4o mini: Auntie Annes 1 Burger King (from South, moving along the walkway) Burger King Cold Stone Creamery (from East, moving along the pathway) GPT-4o: Auntie Annes 1 Voodoo Doughnut (from South, moving along the main path near the water) Voodoo Doughnut Gold Stone Creamery (from South, continuing along the same path) 24 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Google Map Instuction: Please provide me navigation from Auntie Annes 1 to Cold Stone Creamery LLama-3.2: Auntie Annes 1 Universal Studios Store Universal Studios Store Burger King Burger King Cold Stone Creamery Qwen2-VL: Auntie Annes 1 Coca-Cola Refresh (from Right and Up, moving along Main Street from Auntie Annes 1) Coca-Cola Refresh Cold Stone Creamery (from Right and Up, moving along Park Avenue from Coca-Cola Refresh) GPT-4o mini: No path found. GPT-4o: Voodoo Doughnut Universal Studios Store (from Right and Down, moving along Central Path from Voodoo Doughnut) Universal Studios Store Cold Stone Creamery (from Right and Down, moving along Final Pathway from Universal Studios Store) 25 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Campus Map Instuction: Please provide me navigation from Print Shop to McCandless Hall LLama-3.2: Print Shop North Beaty Street (west, moving along North Beaty Street) North Beaty Street Pryor Street (south, moving along North Beaty Street) Pryor Street Hobbs Street (west, moving along Pryor Street) Hobbs Street Clinton Street (south, moving along Hobbs Street) Clinton Street McCandless Hall (east, moving along Clinton Street) Qwen2-VL: Print Shop Freehauf House (from North, moving along Hargrove Street) Freehauf House Smith House (from West, moving along Pryor Street) Smith House McCandless Hall (from South, moving along Pryor Street) GPT-4o mini: Print Shop Waters Hall (from North, moving along Hobbs Street) Waters Hall McCandless Hall (from East, moving along Northern Avenue) GPT-4o: Print Shop Security Office (from South, moving along Hobbs Street) Security Office Founders Hall (from West, moving along North Beaty Street) Founders Hall Brown Hall (from South, adjacent pathway) Brown Hall McCandless Hall (from East, adjacent pathway) 26 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Campus Map Instuction: Please provide me navigation from Print Shop to McCandless Hall LLama-3.2: No path found. Qwen2-VL: No path found. GPT-4o mini: No path found. GPT-4o: Print Shop Kares Library (from Left and Down, moving along Hobbs Street from Print Shop) Kares Library McCandless Hall (from Left and Down, moving along Pryor Street from Kares Library) 27 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on National Park Map Instuction: Please provide me navigation from Front Lake to Woods 2 LLama-3.2: Start at Front Lake Amphitheater (from East, moving along Road Name) Amphitheater Main Parking Lot (from South, moving along Road Name) Main Parking Lot Woods (from West, moving along Road Name) Woods Woods 2 (from South, moving along Road Name) Qwen2-VL: No path found. GPT-4o mini: Front Lake Amphitheater (from East, moving along the shoreline of Front Lake) Amphitheater Woods 2 (from North, moving along the wooded trail) GPT-4o: Front Lake Dam (northeast, moving along Trail around Front Lake) Dam Amphitheater (east, moving along Main Trail to Sandburg Home) Amphitheater Woods 2 (northwest, moving along Trail to Woods) 28 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on National Park Map Instuction: Please provide me navigation from Front Lake to Woods LLama-3.2: No path found. Qwen2-VL: No path found. GPT-4o mini: No path found. GPT-4o: Front Lake Dam (from Right and Down, moving along Trail around Front Lake from Front Lake) Dam Amphitheater (from Right and Down, moving along Main Trail from Dam) Amphitheater Restrooms (from Right and Down, moving along Main Trail from Amphitheater) 29 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Trail Map Instuction: Please provide me navigation from Equestrain Camping Loop to Parking Area1 Picnic Area1 Fishing Pond LLama-3.2: Equestrain Camping Loop South Road (from South, moving along South Road) South Road Sunset Road (from South, moving along Sunset Road Sunset Road Parking Area1 Picnic Area1 Fishing Pond (from East, moving along Parking Area1 Picnic Area1 Fishing Pond) Qwen2-VL: Equestrian Camping Loop Forest Road (east) Forest Road Sunset Road (south) Sunset Road Hidden Springs Road (west) Hidden Springs Road Bee Oak Road (south) Bee Oak Road Parking Area1 Picnic Area1 Fishing Pond (north) GPT-4o mini: Equestrian Camping Loop Sunset Road (From South, moving along the path) Sunset Road Parking Area 1 (From West, moving along Sunset Road) Parking Area 1 Picnic Area 1 (From East, moving along the path) Picnic Area 1 Fishing Pond (From South, moving along the path) GPT-4o: Equestrian Camping Loop Intersection with Main Road (move south along Youth Group Loop) Intersection with Main Road Parking Area 1 (head southeast on Bee Oak Road) Parking Area 1 Picnic Area 1 (continue south along Bee Oak Road) Picnic Area 1 Fishing Pond (go south on Bee Oak Road, then east on Candy Loop to reach the pond) 30 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Trail Map Instuction: Please provide me navigation from Equestrain Camping Loop to Parking Area1 Picnic Area1 Fishing Pond LLama-3.2: No path found. Qwen2-VL: No path found. GPT-4o mini: No path found. GPT-4o: Equestrian Camping Loop Intersection with Forest Road (from Left and Down, moving along Bee Oak Road from Equestrian Camping Loop) 31 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Urban Map Instuction: Please provide me navigation from Pinner Park to Harrow on the Hill LLama-3.2: Pinner Park Pinner Road (from North, moving along Pinner Road) Pinner Road Station Road (from East, moving along Station Road) Station Road Harrow Weald (from South, moving along Station Road) Harrow Weald Harrow School (from East, moving along Station Road) Harrow School Harrow on the Hill (from North, moving along Station Road) Qwen2-VL: Ruislip High Street (from North, moving along High Street) High Street Pinner Rd (from East, moving along Pinner Rd) Pinner Rd Pinner Park (from South, moving along Pinner Rd) Pinner Park St. Georges Shopping Centre (from West, moving along Pinner Rd) St. Georges Shopping Centre Harrow School (from North, moving along Station Road) Harrow School High Street (from East, moving along High Street) High Street Bentley Priory (from South, moving along High Street) GPT-4o mini: Pinner Park High Street (From South, moving along Pinner Rd) High Street Harrow on the Hill (From North, moving along High Street) GPT-4o: Pinner Park Harrow Weald (from South, moving along Pinner Rd) Harrow Weald Harrow on the Hill (from South, moving along High Street) 32 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Urban Map Instuction: Please provide me navigation from Pinner Park to Harrow on the Hill LLama-3.2: No path found. Qwen2-VL: Pinner Park High Street (from Up, moving along High Street from Pinner Park) High Street Pinner Rd (from Up, moving along Pinner Rd from High Street) Pinner Rd Harrow on the Hill (from Right, moving along Pinner Rd from Pinner Rd) Harrow on the Hill Northwick Park (from Down, moving along Northwick Park from Harrow on the Hill) GPT-4o mini: Pinner Park Harrow Weald Museum (from Right and Down, moving along Pinner Road from Pinner Park) Harrow Weald Museum St Georges Shopping Centre (from Right and Down, moving along Station Road from Harrow Weald Museum) St Georges Shopping Centre Harrow on the Hill (from Right and Down, moving along High Street from St Georges Shopping Centre) GPT-4o: Pinner Park Museum (from Right and Up, moving along Pinner Rd from Pinner Park) Museum Harrow School (from Left and Up, moving along High Street from Museum) Harrow School Harrow on the Hill (from Right and Up, moving along High Street from Harrow School) 33 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Mall Map Instuction: Please provide me navigation from Coca Cola Machine2 to Macys LLama-3.2: Coca Cola Machine2 Entrance (from Left, moving along Food Court road) Entrance Food Court (from Right, moving along Food Court road) Food Court H&M (from Right, moving along Food Court road) H&M Macys (from Right, moving along Food Court road) Qwen2-VL: Coca Cola Machine2 Management Offices (from North, moving along Road Name) Management Offices Dillards Mens/Home (from East, moving along Road Name) Dillards Mens/Home Macys (from East, moving along Road Name) GPT-4o mini: Coca Cola Machine2 Dillards Mens/Home (from Right, moving along the path) Dillards Mens/Home Food Court (from Up, moving along the path) Food Court Macys (from Right, moving along the path) GPT-4o: Coca Cola Machine2 4000 (from South, moving along Main Corridor) 4000 5002 (from West, moving along Main Corridor) 5002 6001 (from East, passing H&M) 6001 Macys (from North, moving along Main Corridor) 34 Can Large Vision Language Models Read Maps like Human? PREPRINT Instance of Responses under Zero-Shot Prompting on Mall Map Instuction: Please provide me navigation from Coca Cola Machine2 to Macys LLama-3.2: No path found. Qwen2-VL: No path found. GPT-4o mini: No path found. GPT-4o: Food Court Macys (from Right and Down, moving along Main Corridor from Food Court)"
        }
    ],
    "affiliations": [
        "MBZUAI",
        "Texas A&M University",
        "UC Berkeley",
        "UC Riverside",
        "University of Michigan"
    ]
}