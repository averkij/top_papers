{
    "paper_title": "VideoScore2: Think before You Score in Generative Video Evaluation",
    "authors": [
        "Xuan He",
        "Dongfu Jiang",
        "Ping Nie",
        "Minghao Liu",
        "Zhengxuan Jiang",
        "Mingyi Su",
        "Wentao Ma",
        "Junru Lin",
        "Chun Ye",
        "Yi Lu",
        "Keming Wu",
        "Benjamin Schneider",
        "Quy Duc Do",
        "Zhuofeng Li",
        "Yiming Jia",
        "Yuxuan Zhang",
        "Guo Cheng",
        "Haozhe Wang",
        "Wangchunshu Zhou",
        "Qunshu Lin",
        "Yuanxing Zhang",
        "Ge Zhang",
        "Wenhao Huang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 9 7 2 2 . 9 0 5 2 : r VIDEOSCORE2: THINK BEFORE YOU SCORE IN GENERATIVE VIDEO EVALUATION 2Dongfu Jiang 3Ping Nie 4Minghao Liu 7Zhengxuan Jiang 1Xuan He 2Mingyi Su 6Wentao Ma 6Junru Lin 2Chun Ye 6Yi Lu 2Keming Wu 2Benjamin Schneider 2Quy Duc Do 2Zhuofeng Li 6Yiming Jia 2Yuxuan Zhang 9Guo Cheng 2Haozhe Wang 5Wangchunshu Zhou 8Qunshu Lin 5Yuanxing Zhang 2,5Ge Zhang 5Wenhao Huang 2Wenhu Chen 1University of Illinois Urbana-Champaign, 2University of Waterloo, 3Independent, 42077AI, 5M-A-P, 6University of Toronto, 7Zhejiang University, 8Abaka AI, 9Netmind.AI xuanhe4@illinois.edu {dongfu.jiang, wenhuchen}@uwaterloo.ca"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VIDEOSCORE2, multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on large-scale dataset VIDEOFEEDBACK2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VIDEOSCORE2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VIDEOSCORE-BENCHV2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://TIGER-AI-Lab.github.io/VideoScore2/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent progress in text-to-video (T2V) generation (OpenAI, 2024; Kuaishou, 2025; Wan et al., 2025) has enabled models to produce increasingly realistic and coherent videos, expanding their potential across domains such as entertainment, education, and simulation. Yet, evaluating the quality of such videos remains core bottleneck. Unlike text or image evaluation, video assessment must jointly consider visual fidelity, semantic adherence to prompts, and physical plausibilitydimensions. These abilities requires the backbone vision language model (VLM) to not only possess superior visual understanding ability but also comprehensive physical common senses. Many powerful video evaluators/reward models have been developed these days and demonstrate strong performance in various video preference and point-score benchmarks. Examples works include VideoScore (He et al., 2024), VideoPhy2 (Bansal et al., 2025), VideoReward (Liu et al., 2025), etc. However, while these works present superior ability in scoring accuracy, they all collapse into single opaque score, where no rationale is given for accountability. Whats more, these models Equal contribution. Xuan He is the project leader. 1 Figure 1: Overview of VIDEOSCORE2. We curate data from five different prompt sources and 22 T2V models with human annotated scores and rationale, which is further used for 2-stage training (cold-start SFT then RL) to elicit the models thinking ability before scoring. are all trained via supervised fine-tuning on the collected dataset directly, limiting their generalization ability on the OOD data points (Chu et al., 2025). To address this gap, we propose VIDEOSCORE2, multi-dimensional, interpretable, and humanaligned evaluator for AI-generated videos. VIDEOSCORE2 not only outputs structured scores along three axes(1) visual quality, (2) text alignment, and (3) physical/common-sense consistencybut also provides detailed chain-of-thought style analyses before giving its judgments. This thinking-before-scoring design makes VIDEOSCORE2 unique among evaluators, enabling transparent and human-like reasoning. Furthermore, unlike some prior models whose SFT training restricts them to in-domain settings, VIDEOSCORE2 demonstrates strong generalization across diverse out-of-domain benchmarks, confirming its robustness and reliability for video generation. For training VIDEOSCORE2, we curated large-scale dataset of multi-dimensional evaluations, VIDEOFEEDBACK2, that combines quality scores with reasoning traces. The text-to-video prompts were sourced from both existing datasets and our manually designed cases in some special scenarios (e.g., multiple actions, OCR text, camera motion). The videos were generated by over twenty T2V models spanning from early baselines to recent state-of-the-art modern generative systems, marking more fine-grained video quality gradient. Our annotators were instructed to provide scores (1-5) as well as brief diagnostic comments across three dimension, which were later expanded into detailed rationales through an LLM semi-blind scoring and alignment pipeline. This design yields diverse, reliable, and reasoning-augmented dataset that serves as the foundation for teaching VIDEOSCORE2 both what to evaluate and how to reason. As result, we derived 2933 unique prompts, 27168 generated videos and 81504 scores with rationales in total. We also split 500 exmaples as new video point-score benchmark: VIDEOSCORE-BENCH-V2. During the experiments, we adopt two-stage training pipeline. First, cold-start supervised finetuning (SFT) is applied to instill structured output formatting and basic reasoning capabilities. Then, we employ Group Relative Policy Optimization (GRPO) with RL through to further strengthen analytical robustness, refine interpretability, and align evaluations with human preference distributions. Extensive experiments demonstrate the effectiveness of VIDEOSCORE2. On the in-domain VIDEOSCORE-BENCH-V2, VIDEOSCORE2 achieves 44.35 (+5.94) in point-score accuracy, 90.78 (+4.01) in relaxed accuracy and 60.37 (+8.32) in PLCC with significant improvements compared to the previous SoTA. Our model consistently achieves superior performance in the out-of-domain (OOD) benchmarks, reaching 50.37 (+4.32) average performance across 2 OOD preference and 2 OOD point score benchmarks. Furthermore, we show VIDEOSCORE2s potential to be applied as reward model for T2V generation via Best-of-N (BoN) sampling. We also conducted detailed ablation study to understand importance of rationale for SFT, cold-start SFT for RL, and score output format, etc. Results demonstrate RL with cold-start SFT and rationale as the best parctice."
        },
        {
            "title": "2.1 TEXT-TO-VIDEO GENERATION",
            "content": "Research on text-to-video (T2V) generation has progressed rapidly with the introduction of large diffusion and Transformer-based architectures. Early milestones include ModelScope (Wang et al., 2023), which provided one of the first open-source diffusion pipelines for T2V, making the task widely accessible. Subsequent VideoCrafter2 (Chen et al., 2024) improved temporal fidelity with enhanced motion realism under data constraints. More recently, CogVideoX (Yang et al., 2024) employed large DiT backbone to achieve high resolution and narrative coherence. At the industrial scale, OpenAI Sora (OpenAI, 2024) positions itself as world simulator, capable of generating long videos with rich physical plausibility. Similarly, an open-sourced work StepVideo-T2V (Ma et al., 2025a) emphasizes scalable training and efficient architecture design to support long and coherent video synthesis. Other commercial systems such as Veo 3 (Google, 2025a), Kling-1.6 (Kuaishou, 2025), and Pika-2.2 (Pika-Labs, 2025) further highlight advances in controllability, and human-centric generation. Despite these achievements, systematic and human-aligned evaluation of video qualities from visual perception to semantic reasoning remains limited, underscoring the need for multi-dimensional and interpretable evaluation frameworks."
        },
        {
            "title": "2.2 REWARD MODELING FOR VISION",
            "content": "Reward modeling has become central paradigm for aligning generative models with human preferences in both image and video domains. Early methods such as Dover (Wu et al., 2023a) and ImageReward (Xu et al., 2023) provide single scalar scores, which are effective but insufficient for capturing the multi-faceted nature of visual quality. More recent approachesVideoReward (Liu et al., 2025), UnifiedReward (Wang et al., 2025c), and Q-Insight (Li et al., 2025)introduce multidimensional scoring, yet are limited to numeric ratings without explanatory reasoning. Other efforts like LiFT (Wang et al., 2025b) provide short analytical comments, but remain broad and lack the depth necessary for systematic evaluation. Addressing these limitations, VIDEOSCORE2 delivers multi-dimensional assessments together with long-form analytical reasoning, making its evaluations both human-aligned and interpretable. Moreover, unlike many existing reward models whose reliance on SFT training often leads to poor generalization, VIDEOSCORE2 demonstrates robust performance across OOD benchmarks, underscoring its potential as more reliable evaluator."
        },
        {
            "title": "2.3 VIDEO UNDERSTANDING AND REASONING",
            "content": "Video understanding and reasoning has been long-standing problem in multimodal learning. Since 2022, transformer-based models have become the backbone of video understanding. Works like Video Swin Transformer (Liu et al., 2021) and InternVideo (Wang et al., 2022) show the benefit of large-scale pretraining and hierarchical temporal modeling. Extending to videolanguage reasoning, models such as Video-LLaMA (Zhang et al., 2023), Video-LLaVA (Lin et al., 2024b), and mPLUGOwl-V (Ye et al., 2024) align LMMs with video for open-ended QA and grounding. Recent efforts emphasize long video understanding, including LongVLM (Weng et al., 2024), Video-ChatGPT (Maaz et al., 2024), and benchmarks like Video-MME (Fu et al., 2025) and VideoEval-Pro (Ma et al., 2025b), which stress more realistic, open-ended evaluation of extended video reasoning. Table 1: Comparison of VIDEOSCORE2 and existing reward models for multi-dimensions, rationale support, and dataset recency. Method Input Multi-Dim Rationale data recency (yy.mm) DeQA-Score Q-Insight VisionReward VideoReward UnifiedReward image image video video video VIDEOSCORE2 video 24.10 24. 24.08 24.09 24.12 25.04 3 Figure 2: Prompt source proportion. Table 2: Definition and Checklist of Evaluation Dimensions. Dimension Definition and Checklist Visual Quality Text Alignment Quality of visual viewing experience, including resolution, overall and local clarity, smoothness, brightness stability, distortions, etc. The alignment between video content and text prompt, in terms of subjects, actions, details, styles, and sequential events, etc. Physical / Common-sense Consistency Whether the video is normal and aligns with common sense, or physical laws, based on everyday knowledge and intuition. Check for abrupt changes, distortions, counterintuitive scenes, and anything weird and abnormal."
        },
        {
            "title": "3.1 DATA PREPARATION",
            "content": "Prompt Collection. Our dataset prompts come from two sources: existing datasets VidProM (Wang and Yang, 2024) and Koala-36M (Wang et al., 2025a), and manually collected ones. VidProM provides real user queries from generative model communities, while Koala-36M contains detailed and structured captions that can be adapted into text-to-video prompts. Since some raw prompts are abstract, incomplete or unsuitable, we adopt two-stage filtering pipeline: (1) rulebased filtering is applied to remove prompts that are unsuitable due to length, format, or other constraints; (2) LLM semantic filtering or revising is used to discard or revise prompts that are abstract, incoherent, or bad for short video generation. See details in Appendix A.1. For the manually collected part, we focus on three categories: multi-action, OCR text, and camera motion. This design is motivated by known limitations of current text-to-video (T2V) models in failing to express multiple actions (Wang et al., 2024b), render readable text (), and reproduce camera motion (). To construct the multi-action and OCR text prompts, we first design about 100 seed examples and then ask LLMs to expand them creatively, while camera motion prompts are built by appending motion instructions (e.g., pan left, tilt up) to some sampled prompts directly. Video Collection We collected videos from over 20 T2V models, ranging from early diffusion systems such as ModelScope (Wang et al., 2023) to advanced generators such as StepVideo-T2V (Ma et al., 2025a) and Kling-1.6 (Kuaishou, 2025). For annotation, models were grouped into four coarse tiers (Poor/Early, Medium, Good, Perfect/Modern). For each prompt, we randomly sampled 10 models to generate videos, ensuring balanced distribution across tiers. This design enabled direct comparisons among videos with the same semantic content but different quality levels, improving scoring consistency and reliability. By covering wide range of resolutions (256256 to 1980982), frame rates (830 fps), and durations (16s), our dataset offers diverse variability, helping VIDEOSCORE2 learn quality from poor output to near-photorealistic generations (see Appendix A.2 for details and A.3 for video examples). Evaluation Dimensions We evaluate videos along three dimensions: visual quality, text alignment, and physical / common-sense consistency, to capture fidelity, semantic accuracy, and content-level reasoning. Unlike VideoScore with five dimensions(He et al., 2024), we remove dynamic degree (mostly prompt-dependent) and subsume temporal consistency under visual quality."
        },
        {
            "title": "3.2 ANNOTATION",
            "content": "We provide dimension-specific checklists in Table 2 to help annotators understand the task. Annotators are required to assign integer scores (15) and short comments for each dimension, later expanded into full rationales by an LLM. For example, the comments can be: Low resolution, brightness is unstable or The second and third actions in prompt are missing. Our team consists of 15 annotators who were trained with annotated examples and pilot rounds (3050 videos each) with reviewer feedback to ensure consistency. See detailed guidelines in Appendx A.4. Quality control was ensured through periodic audits, where 1020% of data was spot-checked for scoring accuracy and comment quality. Annotators with inconsistent work received feedback and were required to revise their annotations. As shown in Table 3, the inter-annotator agreement (IAA) indicates good labeling reliability. 4 Table 3: Inter-Annotator Agreement (IAA) results (R / α). = Relaxed Match (all annotator scores within margin of 1), α = Krippendorffs Alpha. Table 4: Human inspection on the difference between model score and human score in augmented-scoring. Trial VQ TA PC 1 (n = 30) 2 (n = 30) 93.33 / 92.06 96.67 / 90.61 93.33 / 82.71 80.00 / 77. 83.33 / 82.99 80.00 / 80.95 4951 videos, 14853 scores in total Difference 0 1 Counts 4710 7698 2062 3 Bad videos (diff. 3 in any dim.) : 337 / 4951 Figure 4: Rationale length (num of words). Most are in 200-600 words. Figure 5: Human annotated score distribution in SFT data."
        },
        {
            "title": "3.3 SFT DATA PROCESSING",
            "content": "Rationale Elicitation We use Claude-4-Sonnet (Anthropic, 2025) (with thinking enabled) to elicit CoT-like rationales (Wei et al., 2023). The LLM receives evaluation instructions, sampled frames, annotator comments (without scores), and 23 few-shot examples (see Appendix A.5). Its outputs are compared with human scores, and reconciliation follows: (i) if the difference 1, keep the human score; (ii) if the difference = 2, average the two; (iii) if any dimension differs 3, the whole entry is re-scored, up to three times. After resampling, fewer than 10% of entries were discarded. In 4,951 videos (14, 853 scores), we report the distribution of humanmodel differences in Table 4. Align Rationales with Scores Since the final score may occasionally differ from that mentioned in the rationale by one point, we use GPT-5-mini (OpenAI, 2025) to align rationales with scores (prompt template shown in Appendix A.5). This lightweight adjustment preserved the rationales meaning while ensuring scoring consistency: typically, it involved only minor edits, such as softening or intensifying descriptions of quality issues (e.g., slight blur noticeable blur). The rationale length distribution is shown in Figure 4, and the score distribution in Figure 5. Building data for SFT and RL After processing, we obtain 27,168 samples (denoted as VIDEOFEEDBACK2), and the proportions of videos across the four quality tiers (from best to worst) are 10.36%, 33.53%, 41.77%, and 12.54%, respectively (Appendix A.2). 500 videos are held out as the test set (VIDEOSCORE-BENCH-V2) and the rest used for training. The SFT data follow QA format, where the query specifies the task  (Table 11)  , and the answer provides rationale and scores. For RL, we follow Video-R1 (Liu et al., 2025), using the same structure with problem and solution to compute accuracy rewards."
        },
        {
            "title": "4.1 TRAINING AND INFERENCE SETUP",
            "content": "SFT Cold-Start. We adopt two-stage training strategy for VIDEOSCORE2. To ensure basic format-following ability and task familiarity, we first perform supervised fine-tuning (SFT) as the cold-start. The training is implemented with the LLaMA-Factory (Zheng et al., 2024a) framework, and Qwen2.5-VL-7B-Instruct (Bai et al., 2025) as base model. For preparing the SFT checkpoint to initialize RL, we consider both the performance on VIDEOSCORE-BENCH-V2 and training loss stability: high benchmark scores may indicate over5 fitting on in-domain tests and weak generalization to others. Balancing these factors, we adopt the configuration described in Appendix D.1 as our main SFT model. Additional ablations on sampling fps, learning rate, and training epochs are reported in Appendix D.2 and Appendix D.3. Reinforcement Learning We further train SFT checkpoint with open-source video reinforcement learning framework Video-R1 (Feng et al., 2025) implementing GRPO (Shao et al., 2024) to enhance its analytical robustness and human alignment: Accuracy Reward. The reward is defined by the degree of match. The design of this reward signal follows the principle that only predictions within 1 of the ground truth on all dimensions should receive non-zero reward. On 5-point scale, deviation of one point is marginally acceptable, whereas deviations of two or more points indicate serious misjudgments and often contradict the ground-truth evaluation. Racc = 1.0 0.7 0.4 0.1 0 if all three dimensions match exactly, if two match and one differs by 1, if one matches and two differ by 1, if all three differ by 1, otherwise. (1) Format Reward. To ensure the output includes both rationale and final scores, we assign Rfmt = 1 if the response contains the <think> tag with rationale, and Rfmt = 0 otherwise. Final Reward. Following the setting for general video reasoning tasks in Video-R1, final reward = Racc + λRfmt. In practice, when starting from the SFT checkpoint, outputs already follow the query template, making the format reward redundant; thus we set λ = 0 to focus on accuracy. In contrast, RL from the base model without SFT often shows format deviations, so we set λ = 0.3 to encourage valid rationales and scores. RL training uses learning rate of 2e-6 with = 8 generations per rollout, on 4A100 GPUs (about 8 hours per 100 steps). Evaluations on VIDEOSCORE-BENCH-V2 and OOD benchmarks show performance peaks at around 300 steps; beyond this, performance on VIDEOSCORE-BENCHV2 drops (Appendix D.5). We therefore adopt the 300-step checkpoint for all reported results. Inference. Both the SFT and RL models output free-form text with rationale and final scores, following the same query template  (Table 11)  . To generalize discrete predictions {1, 2, 3, 4, 5}, we set decoding temperature to 0.7 and convert them into soft float scores using token-level probabilities: = arg max p(s) p(s) j=1 p(j) (cid:80)5 . (2) This yields smoother scores in [1, 5] while preserving interpretability. We further ablate score format (int vs. float) in Table 7 and inference fps (2, 4, 8) in Appendix D.4; all reported results use 2 fps and normalized float scores."
        },
        {
            "title": "4.2 BENCHMARKS",
            "content": "In addition to the in-domain test on the VIDEOSCORE-BENCH-V2, we do further assessment on four out-of-domain benchmarks, testing the generalization ability across wide range of video understanding and quality evaluation scenarios. The out-of-domain benchmarks can be categorized into two types based on the evaluation task: Pairwise Preference and Point Score. Pairwise preference benchmarks require the evaluator model to compare pair of videos and identify which one exhibits higher quality. VideoGenReward-Bench (Liu et al., 2025), built on VideoGen-Eval (Yang et al., 2025), contains 4,691 videos and 25,234 pairs. Annotators provide pairwise preference labels on dimensions of Visual Quality, Motion Quality, Text Alignment, and Overall preference. T2VQA-DB (Kou et al., 2024) assigns each video human quality score (0100). We sample 2,000 videos and derive 1,822 preference pairs by comparing the scores of videos. The preference benchmarks both have ties, For models that output float scores, we treat two videos as having equal preference if their score difference is within 5% of the models score range. (e.g., in [0.0, 5.0], scores 3.28 vs. 3.26 tie). 6 Figure 6: Accuracy rewards in RL training. Figure 7: Comparison of Best-of-N sampling with VIDEOSCORE2 and random ones on averaged 5 VBench dimensions. Point score benchmarks focus on how well the evaluators numerical predictions (after appropriate rescaling) align with the ground-truth scores in overall quality or fine-grained dimensions. MJ-Bench-Video (Tong et al., 2025) contains 2,170 human-labeled videos with aspect-level scores in {0,1,2}. We use {Fineness, Alignment, Coherence & Consistency} and average VIDEOSCORE2 three dimensions to compare with their overall score. VideoPhy2-test (Bansal et al., 2025) includes 3,396 videos annotated on 15 scale for Semantic Adherence and Physical Consistency, which directly match VIDEOSCORE2 second and third dimensions. For consistency across benchmarks with different focuses and dimensions, we perform dimension mapping and ground-truth score rescaling, as detailed in Appendix B.1."
        },
        {
            "title": "4.3 BASELINE MODELS",
            "content": "To ensure comprehensive and rigorous evaluation of VIDEOSCORE2, we evaluate it against more than 10 baseline methods spanning diverse methodological families, divided into two classes. Prompting MLLMs, we employ the same query templates as in the training data and provide sampled video frames as input to current most advanced LLMs with vision support for direct scoring, including Gemini-2.5-Pro (Google, 2025b), GPT-5 (OpenAI, 2025), Claude-4-Sonnet (Anthropic, 2025), Grok-4 (xAI, 2025), GLM-4.1v-9b-Thinking (Team et al., 2025), Llama-4-Maverick (Meta, 2025) and Qwen2.5-VL-72B-Instruct (Bai et al., 2025). Vision Reward/Scoring Models, can be categorized based on whether it supports video input. Image-only Models: We adopt ImageReward (Xu et al., 2023), DeQA-Score (You et al., 2025), and Q-Insight (Li et al., 2025). For video evaluation, frames are sampled at the same fps as the video models. ImageReward and DeQA-Score yield single overall score, while Q-Insight supports aspect-specific queries, which we align with the three dimensions of VIDEOSCORE2. Video-Capable Models: We include VideoReward (Liu et al., 2025), UnifiedReward (Wang et al., 2025c), VideoScore (He et al., 2024), and VideoPhy2 (Bansal et al., 2025), which support multi-dimensional scoring. Others such as VisionReward (Xu et al., 2024), Q-Align (Wu et al., 2023b), and DOVER (Wu et al., 2023a) provide only single overall score. Most models output scores without detailed reasoning. VisionReward uses fine-grained binary questions aggregated into score, but lacks explicit explanations. LiFT adds short comments, yet these remain high-level and superficial. In contrast, VIDEOSCORE2 produces both dimension-level scores and comprehensive analyses, making its evaluation more interpretable. Since different models use varying dimensions and scales, we rescale and adjust all outputs to match VIDEOSCORE2 setting (detailed in Appendix B.2)."
        },
        {
            "title": "4.4 EVALUATION RESULTS\nWe report results on VIDEOSCORE-BENCH-V2 in Table 5, with Accuracy (w/o and w/ relaxation)\nand correlation metrics (PLCC). For float-output models, scores are rounded for accuracy and kept\nraw for correlations. VIDEOSCORE2 surpasses the best baseline across all dimensions and metrics.\nWe further test on four out-of-domain (OOD) benchmarks: two pairwise preference and two point-\nscore (Section 4.2). In the tables, Overall denotes an explicit overall score, while Avg is the mean",
            "content": "7 Table 5: Accuracy and correlation between model answer and human score on VIDEOSCOREBENCH-V2. Relaxed Accuracy counts cases where the prediction differs from the ground truth by at most one point. Bold denotes the best model and the underlined denotes the second best. VIDEOSCORE-BENCH-V2 Accuracy Relaxed Accuracy PLCC Visual Align Phy Avg Visual Align Phy Avg Visual Align Phy Avg Claude-Sonnet-4 Gemini-2.5-Pro GPT-5 GLM-4.1v-9B ImageReward DeQA-Score Q-Insight VideoScore1.1 VideoReward UnifiedReward VisionReward Q-Align AIGVE-MACS VideoPhy2-AutoEval Dover Prompting MLLM 33.07 29.92 30.72 33. 29.86 29.72 27.91 31.46 23.85 24.07 20.08 21. 28.93 27.90 26.24 28.72 76.35 71.49 73.90 80. 76.95 70.88 72.29 77.15 61.92 61.45 60.84 61. 71.74 67.94 69.01 73.04 20.17 26.71 13.38 28. 30.64 32.96 23.34 16.80 18.01 19.75 17.24 10. 22.94 26.47 17.99 18.34 Reward/Scoring Models for Image 29.06 36. 33.60 28.06 28.66 30.60 27.26 32. 31.00 28.13 32.53 31.73 65.13 85. 81.40 68.94 77.15 77.40 61.72 80. 75.60 65.26 81.16 78.13 28.23 44. 41.05 40.76 23.96 25.44 23.26 29. 27.54 30.75 32.85 31.34 Reward/Scoring Models for Video 41.48 23. 25.20 41.28 28.66 20.12 - 39.08 34.87 28.86 27. 33.47 28.06 12.48 28.46 31.06 38.88 - 22.80 35.07 27. 14.09 16.23 31.86 38.41 26.16 25.07 36.61 28.19 15.56 22. 34.00 90.98 60.32 71.00 87.17 75.55 62.37 - 84. 82.37 67.74 64.80 84.37 69.74 46.48 73.75 74.75 86.97 - 68.00 82.16 68.94 45.27 52.31 75.92 86.77 64.03 67. 84.57 71.41 51.37 63.03 78.48 49.00 46.36 58.61 46.85 54. 27.30 - 50.24 30.90 48.31 43.91 45.32 34.01 6.90 35. 32.83 47.00 - 53.64 38.25 37.78 13.03 25.41 33.00 42.30 47. 52.05 43.47 42.17 15.74 30.42 38.69 VIDEOSCORE2 Ours over Best Baseline 50.10 43.88 39.08 44.35 92.99 91. 87.98 90.78 60.13 62.60 52.73 60. +8.62 +9.01 +0.20 +5.94 +2.80 +7.01 +1.01 +4. +1.52 +14.29 -0.91 +8.32 Table 6: Performance comparison on out-of-domain benchmarks, with 2 pairwise preference benchmarks and 3 point-score benchmarks. Bold denotes the best model and the underlined denotes the second best. For OOD Preference Benchmark, performance is computed over all test samples. OOD Bench Average OOD Preference Benchmark OOD Point Score Benchmark VideoGenT2VQA-DB MJ-Bench Reward Bench (Preference) -Video VideoPhy2 -test ImageReward DeQA-Score Q-Insight VideoScore-v1.1 VideoReward UnifiedReward VisionReward Q-Align AIGVE-MACS VideoPhy2 Dover 37.40 40.54 46.05 38.87 44.73 37.22 42.86 32.62 30.48 29.13 42.70 Reward/Scoring Models for Image 47.14 53.88 54.05 43.46 35.22 46.65 Reward/Scoring Models for Video 16.79 59.69 53.31 54.31 42.05 37.09 30.75 54.27 39.18 36.15 50.39 37.64 43.24 36.91 24.12 44.62 VIDEOSCORE 37.51 44.19 52.58 71.57 51.75 23.18 56.91 21.97 31.00 24.00 43.69 21.48 28.85 30.90 27.95 31.33 22.02 22.58 23.22 16.93 37.64 28.21 Ours 50. 51.53 50.60 65.77 33.58 8 Table 7: Ablations on RL start point, rationale in SFT and score output format. In-Domain OOD Preference Benchmark OOD Point Score Benchmark Ablations VIDEOSCORE-BENCH-V2 VideoGenT2VQA-DB MJ-Bench Reward-Bench (Preference) -Video RL w/o SFT RL w/ SFT w/ CoT (default) w/o CoT Normalized (default) Raw Int Score 36.70 44. 39.81 32.17 44.53 45.83 54.53 51.53 50.79 54.74 51.53 51.19 54.54 50. 52.36 58.63 50.60 30.22 56.43 65.77 66.88 59.06 65.77 66.51 VideoPhy2 -test 27.69 33.58 30.02 21.83 33.58 34.51 across dimensions; preference results include ties. As shown in Table 6, while VIDEOSCORE2 is not always the top model on each benchmark, it achieves the highest overall average. To further validate the effectiveness of VIDEOSCORE2 in video evaluation, we conduct human inspection to examine whether its predicted scores were reasonable and whether the analyses were accurate and appropriate. Qualitative examples are provided in Appendix E."
        },
        {
            "title": "4.5 BEST-OF-N SAMPLING WITH VIDEOSCORE2",
            "content": "We evaluate VIDEOSCORE2 with best-of-n (BoN) sampling (n = 5), where the model selects the best video among candidates. Six T2V models of moderate or poor quality are used, avoiding very strong ones to highlight the BoN effect. For 500 prompts, each model generates 500 5 videos. Comparison on VBench (Figure 7) shows BoN consistently outperforms random sampling, confirming that VIDEOSCORE2 effectively guides higher-quality selection. See full results in Appendix C.4."
        },
        {
            "title": "4.6 ABLATION STUDY",
            "content": "Besides the ablations on SFT settings, RL training steps, as well as inference configurations (Appendix D), we conduct the following studies, providing more insights of designing VIDEOSCORE2, summarized in Table 7. Cold Start. We compare RL initialized from the base Qwen2.5-VL-7B-Instruct versus the SFT checkpoint. The SFT version achieves higher average scores across both VIDEOSCORE-BENCHV2 and OOD benchmarks, even if not superior on every benchmark. This indicates SFT provides stronger starting point, enabling RL to focus on reward alignment rather than task formatting. SFT w/ and w/o rationale We further test SFT with and without CoT-like rationales. While the CoT-based version is slightly weaker on preference benchmarks, it performs significantly better on point-score benchmarks and thus improves generalization on average. This confirms that rationales are not only important for interpretability but also beneficial for overall robustness. Score format. We ablate the output format by comparing raw integer scores and normalized float scores. While integers show slight advantages on OOD point-score benchmarks, they perform notably worse on OOD preference tasks. Using normalized float scores strikes better balance, preserving accuracy for point-score while capturing finer quality differences in preference settings."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduced VIDEOSCORE2 for multi-dimensional, interpretable, and humanaligned evaluation of AI-generated videos. By building comprehensive annotation pipeline that gathers diverse prompts, generative videos as well as reliable scores and rationales, we are able to train VIDEOSCORE2 in the 2-stage paradigm. Comprehensive experiments demonstrate that our model outperforms existing evaluators across in-domain and out-of-domain benchmarks. We believe that VIDEOSCORE2 open path for trustworthy evaluation and human-aligned training of generative video models. Furthermore, our evaluation results also shows that model still struggle in evaluating physics and common senses in the generative models, highlighting the importance of world model for video evaluator. We leave this as future direction worth to explore."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work adheres to the ICLR Code of Ethics. In this study, no human subjects or animal experimentation was involved. All datasets used, including our curated VIDEOFEEDBACK2, were sourced and processed in compliance with relevant usage guidelines, ensuring no violation of privacy or intellectual property. For prompt collection, we applied strict filtering to exclude NSFW, harmful, or otherwise inappropriate content, and ensured that prompts did not involve personally identifiable information (PII) or sensitive entities. Videos used for annotation were generated by publicly available text-to-video models, and only non-sensitive, safe prompts were retained. Our annotation guidelines emphasized fairness and consistency, and all annotators were trained to avoid introducing biased or discriminatory judgments. No personally identifiable information was collected or used, and no experiments were conducted that could raise privacy or security concerns. We are committed to maintaining transparency, fairness, and integrity throughout the research process."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All the code and datasets used in the paper will be open-sourced after the paper is accepted. We also have provided comprehensive details for both training (see in Appendix D) and evaluation (see in Appendix C) to help the community for reproduction."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude-sonnet-4. https://www.anthropic.com/news/claude-4, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation, 2025. URL https://arxiv.org/abs/2503.06800. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. ArXiv, abs/2501.17161, 2025. URL https://api. semanticscholar.org/CorpusID:275932560. Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms, 2025. URL https://arxiv.org/abs/2503.21776. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2025. URL https://arxiv.org/abs/2405.21075. Google. Google-veo3. https://aistudio.google.com/models/veo-3, 2025a. 10 Google. Gemini-2.5-pro. https://deepmind.google/models/gemini/pro/, 2025b. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, and Wenhu Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation, 2024. URL https://arxiv.org/abs/2406.15252. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, and Ning Liu. Subjective-aligned dataset and metric for text-to-video quality assessment, 2024. URL https://arxiv.org/abs/2403.11956. Kuaishou. Kling-1.6. https://app.klingai.com/global, 2025. Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, and Jian Zhang. Q-insight: Understanding image quality via visual reinforcement learning. arXiv preprint arXiv:2503.22679, 2025. Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024a. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection, 2024b. URL https://arxiv. org/abs/2311.10122. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, 2021. URL https://arxiv.org/abs/2106.13230. Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, and Daxin Jiang. Step-video-t2v 11 technical report: The practice, challenges, and future of video foundation model, 2025a. URL https://arxiv.org/abs/2502.10248. Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, and Wenhu Chen. Videoeval-pro: Robust and realistic long video understanding evaluation, 2025b. URL https: //arxiv.org/abs/2505.14640. Xin Ma, Yaohui Wang, Xinyuan Chen, Gengyun Jia, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation, 2025c. URL https:// arxiv.org/abs/2401.03048. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. Meta. Llama-4-herd. llama-4-multimodal-intelligence/, 2025. https://ai.meta.com/blog/ John Mullan, Duncan Crawbuck, and Aakash Sastry. Hotshot-XL, October 2023. URL https: //github.com/hotshotco/hotshot-xl. OpenAI. Openai sora. https://openai.com/sora, 2024. OpenAI. Gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. Pika-Labs. Pika-v2.2. https://pikalabs.org/pika-2-2, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Spencer Sterling. Zeroscope v2, 2024. URL https://huggingface.co/cerspense/ zeroscope_v2_576w. CreateAI Team. Ruyi-mini-7b. https://github.com/IamCreateAI/Ruyi-Models, 2024a. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024b. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Mj-video: Fine-grained benchmarking and rewarding video preferences in video generation, 2025. URL https://arxiv.org/abs/2502.01719. 12 Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. URL https://arxiv.org/abs/2308. 06571. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, Fei Yang, Pengfei Wan, and Di Zhang. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content, 2025a. URL https://arxiv.org/abs/2410.08260. Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models, 2024. URL https://arxiv.org/abs/2403.06098. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2024a. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning, 2022. URL https://arxiv.org/abs/2212.03191. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment, 2025b. URL https://arxiv.org/ abs/2412.04814. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025c. Yiping Wang, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Is your world simulator good story presenter? consecutive eventsbased benchmark for future long video generation, 2024b. URL https://arxiv.org/abs/ 2412.16211. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models, 2024. URL https://arxiv.org/ abs/2404.03384. Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023a. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin. arXiv preprint Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv:2312.17090, 2023b. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. 13 xAI. Grok-4. https://x.ai/news/grok-4, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation, 2024. URL https://arxiv.org/abs/2412.21059. Yuhang Yang, Ke Fan, Shangkun Sun, Hongxiang Li, Ailing Zeng, FeiLin Han, Wei Zhai, Wei Liu, Yang Cao, and Zheng-Jun Zha. Videogen-eval: Agent-based system for video generation evaluation. arXiv preprint arXiv:2503.23452, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl: Modularization empowers large language models with multimodality, 2024. URL https://arxiv.org/abs/2304.14178. Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023. URL https://arxiv.org/abs/2306.02858. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024a. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024b."
        },
        {
            "title": "A Data Collection and Processing",
            "content": "A.1 Collecting Text-to-Video Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Statistics of Generated Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Video Examples from Different Quality Tiers. . . . . . . . . . . . . . . . . . . . . A.4 Annotation Details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Prompt Templates for Annotation Processing . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Evaluation Suite",
            "content": "B.1 Dimension Matching and Modification in Out-of-Domain Benchmarks . . . . . . . B.2 Dimension Matching and Score Rescaling for Baselines . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Full Evaluation Results",
            "content": "C.1 Full Results on VideoGen-Reward-Bench . . . . . . . . . . . . . . . . . . . . . . C.2 Full Results on MJ-Bench-Video . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Full Results on VideoPhy2-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Full Results of Best-of-N sampling on VBench . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Experiment Setuo and Ablation Studies",
            "content": "D.1 SFT experiment setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Ablation on sampling fps in SFT training . . . . . . . . . . . . . . . . . . . . . . D.3 Ablation on learning rate and epochs in SFT training . . . . . . . . . . . . . . . . D.4 Ablation on inference settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Ablation on RL training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F The Use of Large Language Models",
            "content": "16 16 18 19 21 24 24 25 27 27 29 30 31 31 31 33 34 35"
        },
        {
            "title": "A DATA COLLECTION AND PROCESSING",
            "content": "A.1 COLLECTING TEXT-TO-VIDEO PROMPTS SOURCE 1: VIDPROM Rule-based filtering. NSFW probability. The original dataset provides probability that given prompt may lead to NSFW content. We exclude prompts with NSFW probability greater than 0.2, following the original datasets setting. Trigger-word filtering. Exclude prompts intended for image-to-video generation, which explicitly mention image attachments, and prompts specifying aspect ratios, or duThe trigger-word ration, which cannot be freely controlled in most T2V models. [\"screen size\", \"16:9\", \"1:1\", \"3:4\", \"4k\", \"8k\", list \"seconds\", \"message\", \"attach\"]. includes: Length control. Only prompts between 15 and 100 words are retained. LLM Semantic filtering. To filter out unsuitable prompts, we use GPT-4o-mini for semantic checks and exclude problematic ones. Specifically, we remove prompts that: vague or meaningless, lacking concrete task, containing specific people or names, missing substantive verbs or motion, closer to images than videos, describing over three actions or events, too complex for short videos. SOURCE 2: KOALA-36M Rule-based filtering. Since prompts come from real video captions, we only keep those associated with video segments shorter than 5 seconds; longer captions usually describe too many actions and are unsuitable for short video generation. Each videocaption pair includes clarity score and an aesthetic quality score. We exclude captions with clarity score below 0.95 or aesthetic score below 4.0."
        },
        {
            "title": "LLM Semantic filtering and revising",
            "content": "Same semantic checks as for VidProM, removing ambiguous or low-quality prompts. SOURCE 3: OCR-TEXT (MANUALLY COLLECTED). For the OCR-text category, we first drafted seed prompts that explicitly required text to appear in the video, then expanded them using LLMs to create realistic yet creative scenarios where text naturally integrates into the scene. These prompts are diverse and challenging, often harder to generate than purely human-written ones. For example: painter adds brush strokes to canvas, with palette that says Portrait of Lady, Acrylic Paints, Warm Tones and Fine Detail. photographer adjusts their lens with Capture the Perfect Shot: Photography Tips and Tricks displayed on screen in front of them. In total, 200 prompts were collected. 16 SOURCE 4: MULTI-ACTION (MANUALLY COLLECTED). For the multi-action category, we followed similar approach as OCR-text. We first drafted seed prompts containing two or three connected actions, then expanded and rewritten them with LLMs to produce diverse, story-like scenarios. In total, 200 prompts were collected, each describing short narrative with three consecutive actions. For example: woman adjusts her glasses, glances at the book with focus, and flips to the next page with smile. fluffy orange cat swats ball of yarn, sends it rolling, then dashes after it and pounces mid-roll. SOURCE 5: CAMERA MOTION (MANUALLY COLLECTED). For the camera-motion category, we did not generate entirely new prompts. Instead, we augmented existing prompts by appending explicit camera movement instructions at the end. Common motions include Zoom in, Zoom out, Pan left, Pan right, Pan up, Pan down, Tilt up, Tilt down, and Tracking shot. This simple yet effective strategy allows the dataset to capture scenarios where video realism depends on both content generation and dynamic camera behavior. Figure 8: Distribution of Length (Num of Words) for the Prompt Set. Figure 9: Word Cloud for the Prompt Set. 17 A.2 STATISTICS OF GENERATED VIDEOS We generate videos for annotation using more than twenty text-to-video (T2V) models, spanning from early diffusion-based systems such as ModelScope (Wang et al., 2023) to recent high quality generators like Kling-1.6 (Kuaishou, 2025). This ensures broad quality spectrum, covering both weak and strong generations. As discussed in Section 3.1, to facilitate fairer comparisons and improve annotation reliability, we categorize these models into four coarse quality tiers. For each prompt, ten videos are sampled from ten different models while maintaining balanced distribution across the four tiers. Typically 1-2 from Poor / Early, 3-4 from Moderate, 3-4 from Good, and 1 from Perfect / Modern. The resulting videos vary widely in characteristics, with durations ranging from 1 to 6 seconds, resolutions from 256256 up to 1920982, and frame rates from 8 to 30 fps. full summary of the models (and its variants) used is provided in Table 8. Table 8: Detailed information of videos in our dataset, including t2v-model sources, video fps, resolution, duration, etc. T2V Model (Suffix code in dataset) Open Source FPS Resolution Duration Num Proportion Tier1: Perfert / Modern. 2814 videos, 10.36% Kling-1.6 (Kuaishou, 2025) (r) Sora (OpenAI, 2024) (s) Pika-2.2 (Pika-Labs, 2025) (t) StepVideo-T2V (Ma et al., 2025a) (y) Wanx-2.1 (14B) (Wan et al., 2025) (w) Ruyi (Team, 2024a) (A) CogVideoX-1.5 (Yang et al., 2024) (g) N Y 24. 30.0 24.0 25.0 25.0 24.0 10. 1280*720 1920*982 1280*720 992*544 832*480 1008* 1360*768 5.0s 10.0s 5.0s 4.0s 3.9s 5.0s 4.0s Tier2: Good. 9598 videos, 33.53% Wanx-2.1 (1.3B) (Wan et al., 2025) (v) MagicTime (Yuan et al., 2025) (q) Mochi1-Preview (Team, 2024b) (c) LaVie-base (Wang et al., 2024a) (h) CogVideoX (5B) (Yang et al., 2024) (f) OpenSora-Plan (v1.3) (Lin et al., 2024a) (u) Y Y 24.0 832*480 8. 512*512 10.0 848*480 8.0 10.0 18. 512*320 720*480 640*352 Tier3: Moderate. 11349 videos, 41.77% CogVideoX (2B) (Yang et al., 2024) (e) LTX-Video-0.9.5 (HaCohen et al., 2024) (z) OpenSora (v1.2) (Zheng et al., 2024b) (x) Latte (Ma et al., 2025c) (b) VideoCrafter2 (Chen et al., 2024) (n) Vchitect-2.0 (Fan et al., 2025) (p) AnimateDiff (Guo et al., 2024) (a) Hotshot-XL (Mullan et al., 2023) (m) Y Y 10.0 25.0 8.0 10. 10.0 10.0 10.0 8.0 720*480 704* 640*480 512*512 512*320 512*320 512*512 673* Tier4: Poor / Early. 3407 videos, 12.54% ModelScope (Wang et al., 2023) (d) LTX-Video-0.9.1 (HaCohen et al., 2024) (i) ZeroScope (Sterling, 2024) (j) T2V-Zero (Khachatryan et al., 2023) (k) Y 10.0 10.0 10. 10.0 256*256 704*512 256*256 256*256 3.9s 2.0s 1.9s 2.0s 4.0s 5.2s 4.0s 4.8s 1.6s 1.6s 1.6s 1.6s 2.4s 1.0s 2.4s 3.3s 2.4s 0.8s 298 321 741 281 184 1497 1741 1649 1547 1786 1774 1692 907 1510 1172 1755 1304 967 1333 395 2.25% 1.10% 1.18% 2.73% 1.03% 0.68% 1.39% 5.51% 6.41% 6.07% 5.69% 6.57% 5.07% 6.53% 6.23% 3.34% 5.56% 4.31% 4.55% 6.46% 4.80% 3.56% 4.91% 1.45% 2.62% All: 27168 videos 18 A.3 VIDEO EXAMPLES FROM DIFFERENT QUALITY TIERS. Below we show some videos of each quality tier, from Perfect / Modern to Poor / Early. Example video of the quality tier Perfect / Modern, from Kling-1.6. Prompt is: woman ties red ribbon around gift box, carefully wraps it in shiny paper, and then smiles as she hands it to her friend. Example video of the quality tier Perfect / Modern, from Sora. Prompt is: young man in glasses and gray t-shirt stands on wooden deck, gesturing with his hands and expressing different emotions. The background shows scenic forest with tall trees and clear sky. His facial expressions change as he moves his hands, sometimes near his face, indicating various reactions. The camera captures him in steady medium shot, focusing on his upper body and gestures. Example video of the quality tier Good, from LaVie-base. Prompt is: hawk perches on leafless tree branch, facing away from the camera and gazing up at the clear blue sky. The calm scene features few wispy clouds and barren tree branches. The hawk remains still, with occasional head movements, set against peaceful, natural backdrop. Example video of the quality tier Good, from MagicTime. Prompt is: scene showing the lost bunny, its eyes wide with fear, as it navigates through dense forest, with Sammy guiding it safely home. 19 Example video of the quality tier Moderate, from AnimateDiff. Prompt is: 5 boys of age 17 standing outside school building. Three of them are looking at other students passing by, one is looking at his mobile, and two are talking to each other. Crane up. Example video of the quality tier Moderate, from Hotshot-XL. Prompt is: cozy family kitchen with breakfast items on the table. Xiao Ming, wearing traditional home attire, engages in lively conversation with his parents, emphasizing the warmth of family bonds and traditional values. Example video of the quality tier Poor / Early, from ModelScope. Prompt is: The Kurdish king wears crown of gold on his head in 1850. He is imposing, serious, authoritative, loving, tall, and handsome. He walks among the people in Kurdish clothes. Tilt down. Example video of the quality tier Poor / Early, from Text2Video-Zero. Prompt is: busy highway with cars and trucks moving in both directions under clear blue sky. The scene, filmed from moving vehicle, highlights white van with Martinez returns from Florida on its side. 20 A.4 ANNOTATION DETAILS. Main Instructions The main instruction required annotators to assign score for each dimension based on its definition and to provide short comment describing the issues observed. For instance, under Visual Quality, comments could include low resolution, local blur, or brightness flicker. For Text-to-Video Alignment, annotators were asked to note missing elements from the prompt, while for Physical/Common-Sense Consistency, they were instructed to highlight any violations of physical laws, common sense, or abnormal artifacts. Detailed Guidelines In addition, we provided detailed annotation guidelines to ensure consistency: (i) if dimension was rated 5, the comment could be omitted, since in such cases templatebased rationale could be generated; (ii) if video was entirely black or unrecognizable, it should be skipped. Dimension-specific clarifications were also given: Visual Quality: Videos scoring 5 should look almost perfect, comparable to real footage; while videos scoring 1 corresponds to severe flaws, where the subject, object, or motion is hardly identifiable, or strong distortion/disconnection is present. Text-to-Video Alignment: For prompts with multiple actions (e.g., Open the refrigerator, put the elephant in, and close the door), all actions must be checked for faithful realization. While alignment often correlates with visual quality, clear and smooth videos may still fail to match the prompt. Annotators were instructed to focus on whether the prompt content was expressed correctly, ignoring minor extra details unless they severely misled the meaning. Physical/Common-Sense Consistency: Most videos contain at least minor physical issues, but the severity varies. If prompt itself is unrealistic or absurd, annotators were instructed to disregard this and judge the video independently. Complex reasoning was unnecessary; everyday common sense was considered sufficient for evaluation. Furthermore, annotators are informed that each batch of 10 videos they see sequentially corresponded to the same prompt but came from different T2V models with diverse quality levels, enabling fairer and more calibrated scoring. 21 A.5 PROMPT TEMPLATES FOR ANNOTATION PROCESSING Table 9 shows the prompt template in LLM augmented scoring for eliciting detailed thinking from human annotated quality comments. Table 10 shows the prompt template for revising analysis process when the human-annotated score and then adjusted model score are inconsistent with the thinking models output analysis. Table 11 shows the prompt template for building query in SFT data and running inference. Table 9: Prompt template in LLM augmented scoring for eliciting detailed thinking from human annotated quality comments. We are collecting and processing human annotations for the quality evaluation of AI-generated videos. Dimension definitions: (1) Visual Quality: Mainly evaluates the videos visual and optical properties, including resolution, overall clarity, local blurriness, smoothness, stability of brightness/contrast, distortion/misalignment, abrupt changes, and any other factors the affect the watching experience. The keywords written by the annotators are also mostly derived from the above factors. (2) Text Alignment: Mainly assesses whether the generated video fully and accurately depicts the elements mentioned in the text prompt, such as characters, actions, animals, etc., as well as background, quantity, color, weather, and so on. So the keywords written by annotators sometimes only indicate the elements that are missing from the video. (3) Physical/Common-sense Consistency: Mainly examines whether there are any violations of common sense, physical laws, or any other aspects in the video that appear strange or unnatural. Most of the keywords provided by annotators point out the specific abnormalities or inconsistencies they observed in the video. With the reference of some frames of the video, and the comments of 3 dimensions from human annotator may also be provided, please do your best to analyze and give INTEGAR score between 1 and 5 for these dimensions, where 1 means very bad, 3 means medium, and 5 means very good. Sometimes human comments may be brief or lacking details, or the human comments may be null, please check the aspects in dimension definitions and make sure to thoroughly perceive and analyze the video on your own. Your reasoning should be detailed, professional, and comprehensive. **DO NOT mention any human comment in your thinking**; you should pretend not to know these comments (if they are provided), they are provided solely to inform and enhance your understanding for better evaluation. Output format: Your response must follow the format below strictly: { score visual: quality score (this field is only allowed to be number between 1 and 5, inclusive, ), score t2v: quality score (this field is only allowed to be number between 1 and 5, inclusive), score phy: quality score (this field is only allowed to be number between 1 and 5, inclusive), } DO NOT include any text before or after the json block. Here is the Input: Text prompt used to generate the video: $prompt Comment for visual quality: $comment visual Comment for text-to-video alignment (the elements or events not expressed or not aligned in the video): $comment t2v Comment for physical/common-sense consistency (the elements or events that look weird, abnormal or unnatural): $comment phy 22 Table 10: Prompt template for revising analysis process when the human-annotated score and then adjusted model score are inconsistent with the thinking models output analysis.. Im conducting multi-dimensional quality assessment of AI-generated videos, focusing on the dimensions of (1) Visual Quality, (2) Text Alignment, and (3) Physical/Common-sense Consistency. will provide multi-dimensional quality analysis for video. However, the scores assigned in the analysis may not be entirely accurate. And the ground truth scores for each dimension will also be provided. Your task is to adjust the analysis text accordingly to ensure it aligns with the actual scores. In many cases, this means revising the severity of issues for certain dimension based on the ground truth scores. The scale of score is [1, 2, 3, 4, 5]. **Important Notes:** (1) **Any human comment should NOT be mentioned in the output analysis**. If the input analysis quote or mention human comments, you should pretend not to know them in your output, they are provided solely to inform and enhance your understanding for better evaluation. (2) **DO NOT** alter the overall structure or core meaning of the analysis. Only revise specific expressions or phrases as needed so that the content reasonably reflects the provided scores. (3) The input original analysis is constructed from the sampled frames of the video, if the input analysis includes evaluations of individual frames or frame-by-frame assessments, you should appropriately transform them into an overall evaluation of the entire video, since the final output is expected to be based on the video as whole. (4) Your output analysis should be approximately the same length as the input analysis. If the input analysis is not very detailed and specific, you may extend your output accordingly. Output format: Your response must follow the format below strictly: { new thinking: modified analysis (this field is only allowed to be string), } DO NOT include any text before or after the dictionary block. Here is the input: multi-dimensional analysis: $thinking ground truth score of Dim-1 Visual Quality:$v score ground-truth scoreof Dim-2 Text-to-Video Alignment:$t score ground-truth of Dim-3 Physical Consistency (also referred to as Common-sense Consistency): $p score Table 11: Prompt template for building query in SFT data and running inference. We would like to evaluate its quality from three dimensions: visual quality, text-to-video alignment and physical/common-sense consistency. Below is the definition of each dimension: (1) visual quality: The dimension visual quality cares about the videos visual and optical properties, including resolution, overall clarity, local blurriness, smoothness, stability of brightness/contrast, distortion/misalignment, abrupt changes, and any other factors the affect the watching experience. The keywords written by the annotators are also mostly derived from the above factors. (2) text alignment: The dimension text-to-video alignment mainly assesses whether the generated video fully and accurately depicts the elements mentioned in the text prompt, such as characters, actions, animals, etc., as well as background, quantity, color, weather, and so on. So the keywords written by annotators sometimes only indicate the elements that are missing from the video. (3) physical/common-sense consistency: The dimension physical/common-sense consistency mainly examines whether there are any violations of common sense, physical laws, or any other aspects in the video that appear strange or unnatural. Most of the keywords provided by annotators point out the specific abnormalities or inconsistencies they observed in the video. Here we provide an AI video generated by text-to-video models and its text prompt: $t2v prompt. Based on the video content and the dimension definitions, please evaluate the video quality and give the quality score. The score must be in the range of 1 - 5."
        },
        {
            "title": "B EVALUATION SUITE",
            "content": "B.1 DIMENSION MATCHING AND MODIFICATION IN OUT-OF-DOMAIN BENCHMARKS Since different benchmarks define varying dimensions and scoring scales, we align them with the three evaluation dimensions of VIDEOSCORE2 (visual quality, text alignment, and physical consistency) and, where necessary, rescale their ground-truth scores. Below we summarize the mapping rules for each benchmark. VideoGenReward Bench. This is pairwise preference benchmark containing 4,691 videos, forming 25,234 video pairs. It evaluates three dimensionsvisual quality (VQ), text alignment (TA), and motion quality (MQ)and also provides an Overall preference label indicating which video is better overall. Among these, VQ and TA correspond closely to VIDEOSCORE2 first two dimensions (despite slight definitional differences), so these two dimensions are used for this benchmark. For the Overall preference, we use the mean of all available dimension scores from VIDEOSCORE2 or the baseline method (if the baseline only has one quality score output, then that score is used directly). T2VQA-DB. Originally human-annotated video quality dataset with 10,000 videos, each labeled with single quality score in the range [1,100]. We sample 2,000 videos and construct 1,822 pairs by comparing human-annotated scores. Since the dataset provides only one dimension (the final score), we predict preference by averaging all dimension scores from VIDEOSCORE2 or the baseline method (if the baseline only has one quality score output, then that score is used directly). MJ-Bench-Video. This benchmark contains 2,170 videos and adopts point-score format with five dimensions: fineness, alignment, consistency & coherence, safety, and bias & fairness. We select the first three, which correspond to VIDEOSCORE2 three evaluation dimensions. For baselines with only one final score, we broadcast this score across multiple dimensions. The benchmark uses {0,1,2} scale, whereas VIDEOSCORE2 and other baselines output (or are normalized to) integer scores in [1,5]. Thus, we apply the following mapping, where denotes the original score of each dimension, v, t, denote the rescaled score of for visual quality, text alignment and physical consistency, respectively: = 0 1 2 if {1, 2}, if {3, 4}, if = 5, = 0 1 if = 1, if {2, 3}, if {4, 5}, = 0 1 2 if = 1, if {2, 3}, if {4, 5}. The benchmark also provides an Overall score, for which we again take the mean of all available dimension scores (or the single dimension if only one is provided), rescaled into {0,1,2} using the same rule. VideoPhy2-test. This benchmark contains 3,396 videos with two dimensions, SA: semantic adherence and PC: physical consistency. These map perfectly to VIDEOSCORE2 second and third dimensions. For baselines lacking one of the dimensions (e.g., VideoReward, which provides VQ, TA, MQ but no physical consistency), we skip the missing dimension. The scoring scale is {1,2,3,4,5}, so no rescaling is required. 24 B.2 DIMENSION MATCHING AND SCORE RESCALING FOR BASELINES Since different baseline models adopt varying evaluation dimensions and scoring scales, we apply dimension matching and score rescaling to make them compatible with VIDEOSCOREBENCH-V2 and VIDEOSCORE2. Our goal is to ensure that all baselines output scores on the three dimensionsvisual quality (v), text alignment (t), and physical consistency (p)within unified range of integers 1-5. summary of the mapping rules is provided in Table 12. Baseline Dimension Matching. With v, t, and to denote the score of visual quality, text alignment, physical/common-sense consistency, respectively, we consider three cases: Broadcast. Some baselines only output single final score. In this case, we broadcast the same score to our three dimensions v, t, and p. Good Match. Some baselines already report dimensions that closely match ours, so we directly use their outputs without modification. Customized. For baselines with different or partially overlapping dimensions, we design customized mappings. VideoReward: outputs Visual Quality, Text Alignment, and Motion Quality. We use the outputs of first two dimensions as and t, and skip Motion Quality. AIGVE-MACS: outputs multiple fine-grained dimensions. We average {technical quality, element quality, action quality} as v, average {element presence, action presence} as t, and use physics as p. VideoPhy2-Auto-Eval: outputs Semantic Adherence (SA) and Physical Consistency (PC). We use SA as and PC as p, while skipping v. Baseline Score Rescaling. To make results comparable, we rescale all baseline outputs into unified integer range of 15. summary of the mapping rules is provided in Table 12. Linear Scaling or No Scaling. For baselines with well-defined score ranges (e.g., [0,1], [0,100]), we apply linear normalization followed by rounding to the nearest integer in {1, 2, 3, 4, 5}. Ordinal categories using Gaussian-distribution quantile thresholds. For baselines without fixed score bounds, we adopt an ordinal mapping based on Gaussian-distribution quantile thresholds. Specifically, raw scores are assumed to approximately follow Gaussian distribution and are divided into five categories using the 20%, 40%, 60%, and 80% quantiles of the standard normal distribution. If the raw scores typically fall within [-2.0, 2.0] and we assume Gaussian Distribution (0, 1), thus apply the following mapping: score = 1 2 3 4 5 if < Φ1(0.2), if Φ1(0.2) < Φ1(0.4), if Φ1(0.4) < Φ1(0.6), if Φ1(0.6) < Φ1(0.8), otherwise, where is the raw model score and Φ1 denotes the inverse CDF of the standard Gaussian. ImageReward and VisionReward: most scores are in [-2.0, 2.0], assume (0, 1) and follow the mapping above. VideoReward: most scores are in [-3.0, 3.0], so we assume Gaussian Distribution (0, 1.5), and is replaced by z/1.5 in the rules above to firstly normalize the raw score before converting it to integers. 25 Table 12: Rescale output scores and map dimensions of baselines models to align with our VIDEOSCORE2 and VIDEOSCORE-BENCH-V2. Model Dimension Mapping Original Scale Score Rescaling Method Reward/Scoring Models for Image (averaged on sampled frames) ImageReward DeQA-Score Broadcast Broadcast Q-Insight Good Match most in [-2.0,2.0] Ordinal categories using Gaussian-distribution quantile thresholds. [0.0, 5.0] [1.0, 5.0] Linearly amplify and round Linearly amplify and round Reward/Scoring Models for Video VideoReward Customized most in [-4.0,4.0] Ordinal categories using Gaussian-distribution quantile thresholds. UnifiedReward Good Match {1,2,3,4,5} No rescaling VisionReward Q-Align Broadcast Broadcast most in [-1.0,1.0] Ordinal categories using Gaussian-distribution quantile thresholds. [0.0, 1.0] Linearly amplify and round AIGVE-MACS Customized {1,2,3,4,5} No rescaling VideoPhy2 Customized {1,2,3,4,5} No rescaling Dover Broadcast [0.0, 1.0] Linearly amplify and round"
        },
        {
            "title": "C FULL EVALUATION RESULTS",
            "content": "C.1 FULL RESULTS ON VIDEOGEN-REWARD-BENCH VideoGen-Reward-Bench is video preferenc over three dimensions: visual quality, text alignment, and motion quality. The task is to compare pair of videos and judge which one is better along these axes. Among them, the first two dimensions are broadly aligned with ours, while the benchmark also provides an additional measure of overall preference. For the preference benchmarks, we report results under two settings. The w/ ties version includes all test entries, where in some cases the two compared videos (including the ground-truth reference) are judged as equally preferred. The w/o ties version is subset obtained by removing those entries with equal preference labels. The full evaluation results of preference prediction accuracy are shown in Table 13. Table 13: Full evaluation results on VideoGen-Reward-Bench. Bold denotes the best model and the underlined denotes the second best. VideoGen-Reward-Bench Visual Quality Text Alignment Overall ties w/o ties ties w/o ties ties w/o ties Reward/Scoring Models for Image (averaged on sampled frames) ImageReward DeQA-Score 31.64 51.40 44.00 60.72 47.14 58. 41.07 69.55 36.22 53.23 53.88 67. Q-Insight 30.68 66.34 42.11 59.47 54. 66.34 Reward/Scoring Models for Video VideoScore-v1.1 47.41 30.84 26. 30.85 16.79 40.19 VideoReward 53.21 75. 52.75 72.18 59.69 73.66 UnifiedReward 41. 39.42 40.11 36.58 53.31 58.83 VisionReward 35.89 59.03 44.86 61.15 54.31 67. Q-Align 32.01 52.98 35.77 51.06 42. 52.52 AIGVE-MACS 38.05 30.80 30.76 11. 37.09 37.08 VideoPhy2 Dover - - 37.04 22.14 30.75 26.41 39.34 68. 38.01 55.65 54.27 68.58 Ours VIDEOSCORE2 (SFT only) 37.74 63.17 43.07 61.35 50.79 63. VIDEOSCORE2 (RL w/o SFT) 34.67 65.87 48.70 65.92 54.53 65. VIDEOSCORE2 (SFT + RL) 37.44 63.08 42.87 60.61 51. 63.72 27 C.2 FULL RESULTS ON MJ-BENCH-VIDEO To maximize compatibility with the evaluation dimensions of VIDEOSCORE2, we selected three aspects from MJ-Bench-Video that are most semantically aligned: Fineness, Alignment, and Coherence & Consistency. These aspects correspond respectively to the three dimensions in VIDEOSCORE2: visual quality, text alignment, and physical/commonsense consistency. The full evaluation results of the three aspects and the overall scores are shown in Table 14, with prediction accuracy between model outputs and ground truths adopted as metrics. Table 14: Full evaluation results on MJ-Bench-Video. Bold denotes the best model and the underlined denotes the second best. MJ-Bench-Video Accuracy Fineness Alignment Coherence & Consistency Overall Reward/Scoring Models for Image (averaged on sampled frames) ImageReward DeQA-Score Q-Insight 47.05 18. 12.72 28.07 51.20 42.86 Reward/Scoring Models for Video VideoScore-v1. VideoReward UnifiedReward VisionReward Q-Align AIGVE-MACS VideoPhy2-Auto-Eval Dover 13.69 79.36 43.50 36.31 14. 20.18 - 29.26 Ours VIDEOSCORE2 (SFT only) VIDEOSCORE2 (RL w/o SFT) VIDEOSCORE2 (SFT + RL) 33.95 64.68 22.50 64.19 38. 21.98 55.99 31.74 26.27 38.97 45. 46.20 32.79 48.58 29.03 52.40 28. 79.22 - 18.16 67.51 26.41 21. 7.89 48.02 57.80 57.27 66.79 37. 44.19 52.58 71.57 51.75 23.18 56. 21.97 31.00 24.00 43.69 66.88 56. 65.77 28 C.3 FULL RESULTS ON VIDEOPHY2-TEST Video-Phy2-Test is human-annotated test set with two dimensions: semantic adherence and physical consistency (abbreviated as semantic and physical in our tables). These two dimensions correspond directly to the latter two evaluation dimensions in our framework. The full evaluation results of the two dimensions are shown in Table 15, with prediction accuracy and PLCC between model outputs and ground truths adopted as metrics. Table 15: Full evaluation results on Video-Phy2-test. Bold denotes the best model and the underlined denotes the second best. Video-Phy2-test"
        },
        {
            "title": "Semantic Physical Avg",
            "content": "Reward/Scoring Models for Image (averaged on sampled frames)"
        },
        {
            "title": "ImageReward",
            "content": "DeQA-Score Q-Insight 23.73 28.74 29.21 19. 21.48 15.28 28.96 28.85 3.55 32. 30.90 22.45 3.07 2.14 4.98 9. 2.85 13.72 Reward/Scoring Models for Video VideoScore-v1.1 VideoReward UnifiedReward VisionReward Q-Align AIGVE-MACS VideoPhy2-Auto-Eval Dover VIDEOSCORE2 (SFT only) VIDEOSCORE2 (RL only) VIDEOSCORE2 (SFT + RL) 29.81 31.33 17.64 31. 18.43 12.23 37.96 26.56 32.24 31. 37.48 26.08 27.95 - 31.33 26. 22.02 13.20 22.58 28.00 23.22 21. 16.93 11.61 34.54 34.57 28.11 5. 8.09 13.09 12.35 - 34.54 22. 28.68 13.67 20.89 2.70 4.11 11. 10.00 37.31 37.64 38.64 29.84 34. 29.86 28.21 3.85 1.15 2.50 Ours 27.80 30.02 23.66 27.69 29.67 33. 27.22 39.07 41.08 13.85 20.54 16. 27.99 17.57 29.33 29 C.4 FULL RESULTS OF BEST-OF-N SAMPLING ON VBENCH Table 16: Quality evaluatioin of eight T2V models on V-Bench with BoN sampling by our VIDEOSCORE2, compared with random ones. We can see consistent improvement. Best-of-N Average Subject Background Aesthetic Imaging Motion Dimensions in VBench Random BoN Random BoN Random BoN Random BoN Random BoN Random BoN Lavie-base AnimateDiff VideoCrafter 82.85 81.97 80.03 83.07 83.15 80. 95.40 95.69 96.89 91.16 94.18 94. 95.35 95.58 95.76 97.08 95.64 96. ModelScope 78.75 79.70 93.68 95.07 95. 96.40 ZeroScope LVDM 76.36 75.33 77. 76.26 91.32 93.04 94.50 95.37 88. 89.91 93.14 93.81 56.64 60.90 46. 46.23 45.27 41.01 57.15 60.28 47. 67.99 69.36 67.98 97.34 69.01 94. 67.03 67.58 95.99 97.47 96.64 96. 47.60 61.64 62.32 96.66 97.10 47. 42.00 55.25 56.95 95.48 60.94 62. 92.75 96.30 93."
        },
        {
            "title": "D EXPERIMENT SETUO AND ABLATION STUDIES",
            "content": "D.1 SFT EXPERIMENT SETUP We conduct SFT with sampling fps of 2, maximum frame resolution of 960720, learning rates of 5e-5, and epochs of 2 with one epoch taking about 6 hours on 8A800 GPUs. D.2 ABLATION ON SAMPLING FPS IN SFT TRAINING During training, videos are sampled at 2 fps, which we find sufficient for evaluation: visual quality primarily reflects global perceptual properties, text alignment focuses on semantic adherence, and most issues of physical consistency or abnormal events typically last longer than half second, ensuring they can still be captured at this frame rate. We also conduct an ablation on 17k subset to study the effect of training sampling fps, comparing 2, 4, and 8 fps settings. As shown in Table 17, increasing the sampling rate does not yield significant performance gains, while it noticeably increases computational cost and training time. Therefore, we adopt 2 fps as the default setting in our main SFT experiments and in all subsequent ablations of other hyper-parameters. Table 17: Ablation results on 17k subset of VIDEOSCORE2 data for different sampling fps in SFT. Train Sampling fps Accuracy Relaxed Accuracy PLCC (17k subset) Visual Align Phy Avg Visual Align Phy Avg Visual Align Phy Avg 2fps 4fps 8fps 54. 39.33 46.67 46.89 94.00 81.33 90. 88.44 73.62 60.24 54.72 62.86 48. 42.67 49.33 46.89 94.00 82.00 92. 89.56 67.87 61.85 63.86 64.53 51. 45.33 48.00 48.11 92.00 85.33 88. 88.67 64.34 65.71 52.05 60.70 D.3 ABLATION ON LEARNING RATE AND EPOCHS IN SFT TRAINING We perform ablations on two key hyper-parameters: learning rate 1e-5, 2e-5, 5e-5, 1e-4, 2e-4 and epochs {1, 2, 3}. The results on VIDEOSCORE-BENCH-V2 are summarized in Table 18. For learning rate, 1 104 achieves slightly higher accuracy than 5 105, but its loss curve is less stable and shows lower values in the second epoch, as shown in Figure 10 and 11, suggesting potential overfitting, which could harm performance on out-of-domain benchmarks. By contrast, 2e-5 exhibits much higher loss curve in later stages, indicating underfitting. Balancing in-domain accuracy and loss smoothness, we choose 5e-5 as the default learning rate. For epochs, the 2-epoch setting outperforms both 1 and 3 epochs, and is therefore adopted as the main version. This chosen SFT checkpoint also serves as the base model for subsequent RL coldstart training. Figure 10: Training loss in ablations of learning rate, 2e-5, 5e-5, and 1e-4 are shown. Figure 11: Training loss in ablations of training epoch, 1epoch, 2epoch, and 3epoch are shown. Table 18: Ablation results on VIDEOSCORE-BENCH-V2 for different learning rate and epochs in SFT. SFT ablations Accuracy Relaxed Accuracy PLCC Visual Align Phy Avg Visual Align Phy Avg Visual Align Phy Avg Main (LR = 5e-5, 2epoch) 43.69 40.88 34.87 39.81 90.38 86.97 83.77 87.04 56.74 58.24 44.72 53. Ablation (LR = 1e-5) 41.60 38.20 31.20 37.00 87.80 81.40 79.40 82.87 47. 45.80 35.40 42.86 Ablation (LR = 2e-5) 42.77 38.55 34.94 38.75 90.76 85.14 80.72 85. 54.17 52.77 40.99 49.31 Ablation (LR = 1e-4) 41.08 41.48 37.48 40.01 88. 87.38 81.76 85.91 53.73 56.94 42.87 51.18 Ablation (LR = 2e-4) 41.48 40.48 37.48 39. 89.38 87.58 83.37 86.78 51.93 56.15 45.53 51.20 Ablation (1epoch) 42. 40.28 30.26 37.61 90.78 87.98 79.16 85.97 50.42 56.30 32.92 46.55 Ablation (3epoch) 45.29 37.28 38.88 40.48 92.39 87.38 85.77 88.51 58.34 56.71 49.60 54. 32 D.4 ABLATION ON INFERENCE SETTINGS We also conduct an ablation on inference sampling rates, testing 2 fps, 4 fps, and 8 fps on VIDEOSCORE-BENCH-V2. Results in Table 19 show that 2 fps achieves the best performance, which aligns with our expectation: two frames per second are sufficient to capture most quality issues for evaluation, while higher frame rates introduce redundant information and potential noise that may interfere with the models judgment. Table 19: Ablation results on VIDEOSCORE-BENCH-V2 for different inference configurations. Inference Accuracy Relaxed Accuracy PLCC Sampling fps Visual Align Phy Avg Visual Align Phy Avg Visual Align Phy Avg 2fps 4fps 8fps 50.10 46.80 43.88 39. 44.35 92.99 91.38 87.98 90.78 60. 62.60 52.73 60.37 44.20 38.28 43. 90.00 87.60 84.20 87.27 60.13 57. 43.55 53.74 41.67 40.77 37.61 40. 85.81 88.96 83.78 86.18 56.28 58. 41.86 52.14 33 D.5 ABLATION ON RL TRAINING STEPS We evaluated multiple intermediate checkpoints during RL training. Considering three evaluation metrics jointly, performance peaked around 300 steps. Beyond this point, scores on VIDEOSCOREBENCH-V2 showed clear decline., as shown in Table 20. Therefore, for all main experiments, we report results based on the 300-step checkpoint. Table 20: Ablation on RL training steps. Accuracy and correlation between model answer and human score on VIDEOSCORE-BENCH-V2. Relaxed Accuracy counts cases where the prediction differs from the ground truth by at most one point. RL steps 200 300 400 500 600 700 833 Accuracy Relaxed Accuracy PLCC Visual Align Phy Avg Visual Align Phy Avg Visual Align Phy Avg 50.50 50. 46.20 47.60 50.60 48.00 45.00 42.89 43.88 43.80 45.80 39.28 39. 36.00 40.00 43.40 41.40 45.20 45.60 38.00 37.80 44.22 44. 42.00 44.47 45.13 43.73 42.80 92.79 92.99 92.80 90.20 91. 90.80 91.40 91.59 91.38 90.20 91.40 87.80 87.98 85.40 87.80 89. 87.20 88.20 89.60 87.60 85.60 90.73 90.78 89.47 89.80 89. 88.87 88.87 65.14 65.78 64.57 61.57 62.89 64.28 64.68 62.95 62. 58.87 60.59 57.60 52.73 44.61 52.10 56.49 51.62 57.07 59. 49.43 46.04 61.90 60.37 61.72 58.09 57.00 56.93 56."
        },
        {
            "title": "E CASE STUDIES",
            "content": "Figure 12: Case study of VIDEOSCORE2 evaluation (1) Figure 13: Case study of VIDEOSCORE2 evaluation (2) 35 Figure 14: Case study of VIDEOSCORE2 evaluation (3)"
        },
        {
            "title": "F THE USE OF LARGE LANGUAGE MODELS",
            "content": "Large language models (LLMs), including GPT-5, Gemini-2.5-Pro were used in the preparation of this paper. Their role was limited to supporting writing by suggesting phrasing alternatives, correcting grammar, and improving readability. All technical content, experimental design, analysis, and conclusions were created and verified by the authors."
        }
    ],
    "affiliations": [
        "AI",
        "Abaka AI",
        "Independent",
        "M-A-P",
        "Netmind.AI",
        "University of Illinois Urbana-Champaign",
        "University of Toronto",
        "University of Waterloo",
        "Zhejiang University"
    ]
}