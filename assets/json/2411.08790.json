{
    "paper_title": "Can sparse autoencoders be used to decompose and interpret steering vectors?",
    "authors": [
        "Harry Mayne",
        "Yushi Yang",
        "Adam Mahdi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 0 9 7 8 0 . 1 1 4 2 : r Can sparse autoencoders be used to decompose and interpret steering vectors? Harry Mayne University of Oxford Yushi Yang University of Oxford Adam Mahdi University of Oxford"
        },
        {
            "title": "Abstract",
            "content": "Steering vectors are promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors."
        },
        {
            "title": "Introduction",
            "content": "As language models advance, there is growing interest in methods to steer their behaviours toward desirable characteristics [1]. Recently, steering vectors (or activation steering) have been proposed as way to achieve this without requiring model fine-tuning [20, 13, 10, 23]. Activation steering involves modifying models internal activations during inference by adding vectors that encode desired behaviours. These methods have shown the potential to regulate behaviours such as sycophancy [13], harmlessness [23], and refusal [2, 23]. Despite promising empirical results, the underlying mechanisms behind steering vectors remain poorly understood [9, 4]. Interpreting steering vectors by decomposing them into granular features may help clarify why certain behaviours are more steerable than others [18], why combining steering vectors is largely unsuccessful [22], and may help produce more precise steering vectors [8]. Recent work has explored interpreting steering vectors using sparse autoencoders (SAEs) [4, 8]. SAEs are an emerging method for decomposing model activations into sparse, non-negative linear combinations of vectors, where many vectors appear to correspond to meaningful, interpretable concepts [5, 3]. Since steering vectors exist within the same space as model activations, they could theoretically be expressed as combinations of SAE features [4, 8, 9]. However, past studies found that directly decomposing steering vectors with SAEs produced mixed results, with the reconstructed vectors often failing to retain the steering properties of the original vectors. This suggests that the SAE decompositions did not capture essential elements of the steering vectors [8]. Motivated by these mixed results, this paper investigates the theoretical reasons why SAEs provide misleading decompositions of steering vectors and supports each reason with empirical evidence. We identify two main reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in SAE feature directions, which SAEs are not designed to accommodate. These issues limit the direct application of SAEs for Correspondence: Harry Mayne, harry.mayne@oii.ox.ac.uk 2Code is available at https://github.com/HarryMayne/SV_interpretability 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Workshop on Foundation Model Interventions (MINT). interpreting steering vectors. Our contributions are to highlight these issues, motivating new methods to address them."
        },
        {
            "title": "2 Related work",
            "content": "Steering vectors Steering vectors are vector representations of concepts which can guide model behaviour when added to intermediate model activations at inference time [20, 10]. In this study, we extract steering vectors using Contrastive Activation Addition [13], popular method for constructing steering vectors. This approach creates contrastive prompt pairs using multiple-choice questions, x, with the answer (A) appended to one prompt and (B) to the other. These strings correspond to positive completions y+ (eliciting the desired behaviour) and negative completions (suppressing or remaining neutral to the behaviour), with the letter assignment randomised for each pair (see Appendix for an example). The difference between model activations for these pairs captures representation of the targeted behaviour. To extract the steering vector for given layer L, model activations are collected from the residual stream at the position of the answer token (A or B). Then, for each contrastive prompt pair, the activations aL(x, y+) and aL(x, y) are compared by calculating their difference, forming difference vector. To minimise confounding effects, the final steering vector is obtained by averaging the difference vectors across the dataset of prompt pairs, process known as mean difference [13]. Mathematically, it is written as = 1 (cid:88) xX [aL(x, y+) aL(x, y)] , (1) where is the set of all questions and is the cardinality of the set. Sparse autoencoders Sparse autoencoders (SAEs) [5, 3] are dictionary learning method to decompose model activations into sparse, non-negative linear combination of vectors, known as SAE features (or latents). Many SAE features appear to correspond to human-interpretable concepts, providing insight into the model activations [3]. Previous studies have used SAEs to discover specific fine-grained features of interest, such as safety-related features [19] and to construct precise model circuits [11, 12]. Following [9], generic SAE operates as follows: an encoder maps model activations aL Rn into sparse, higher dimensional space (aL) RM , where n. decoder then reconstructs the activations from this vector, ˆaL(f ). Mathematically, the encoder and decoder are written: (2) (3) where Wenc, benc are the encoders weight and bias terms, Wdec, bdec are the decoders weight and bias terms, and σ is the activation function. We refer to the vector (aL) as the reconstruction coefficients. (aL) = σ(WencaL + benc) ˆaL(f ) = Wdecf + bdec, Directly decomposing steering vectors with SAEs Previous research has empirically investigated directly applying SAEs to steering vectors. Conmy et al. [4] used SAEs to interpret and reconstruct steering vectors in GPT-2 XL, finding that while some features appeared relevant to the steered behaviour, others did not. Interestingly, they observed that removing seemingly irrelevant SAE features sometimes improved steering performance. Similarly, Kharlapenko et al. [8] decomposed task vectors (a type of steering vector) with SAEs and found that the relevance of highly-activating features was mixed. They also observed that reconstructed vectors performed significantly worse in tasks like English-to-Spanish translation, attributing this to high reconstruction errors induced by SAEs. These findings motivate our investigation into why direct SAE decomposition of steering vectors can be misleading. Other studies have proposed alternative methods to decompose steering vectors in the SAE basis [16, 8]. Smith et al. [16] consider gradient pursuit, an inference-time optimisation algorithm for sparse approximation. Similarly, Kharlapenko et al. [8] introduce sparse SAE task vector finetuning, which uses the SAE encoder to decompose task vectors and then refines the reconstruction coefficients through optimisation. The focus of our paper is to identify why directly applying SAEs gives misleading decompositions; however, our results are also relevant for evaluating the strengths and shortcomings of alternative methods."
        },
        {
            "title": "3 Direct SAE decomposition is misleading",
            "content": "To explore SAE-decomposition of steering vectors, we focus on steering corrigibility (the willingness and ability to be rectified) as case study. Specifically, we use the corrigible-neutral-HHH dataset, which contains 340 contrastive prompt pairs on corrigibility [14, 13], and has been shown to yield effective steering vectors [18]. We train steering vectors for the instruction-tuned version of Gemma 2 2B, and decompose vectors using the Gemma Scope open-source SAEs (see Appendix for details) [9]. Steering vectors are extracted at layer 14, as we identify this to be the most effective layer for steering (see Appendix B). Additionally, Appendix shows that the findings also generalise to other behaviours other than corrigibility. We find that direct SAE-decomposition of steering vectors is misleading for two reasons: (1) Steering vectors fall outside the input distribution for which SAEs are designed to decompose, and simply scaling the L2-norm does not resolve this issue. (2) SAEs restrict decompositions to non-negative reconstruction coefficients, preventing them from capturing meaningful negative projections in feature directions within steering vectors."
        },
        {
            "title": "3.1 Steering vectors are out-of-distribution",
            "content": "SAEs are trained to reconstruct model activations, which have systematic differences from steering vectors. One way this out-of-distribution issue materialises, is that steering vectors have significantly smaller L2-norms than model activations (Figure 1). Consequently, the SAE encoder bias term, benc, has disproportionately large influence on the SAE encoder (Equation 2), overshadowing the contributions from the dot products between the SAE feature directions and the steering vector, Wencv. This skews the SAE decomposition, as large positive bias values directly activate certain SAE features  (Table 1)  . As result, the decomposition primarily reflects the encoder bias rather than meaningful contributions from the steering vector. To illustrate this effect, we decompose zero vector with all elements set to zero. In this case, the true reconstruction coefficients should all be zero; thus, any non-zero activations must result solely from the encoder bias. Table 1 compares the decompositions of the corrigibility steering vector and the zero vector, showing their activations are almost identical, highlighting the dominant influence of the encoder bias. We also find that simply scaling the steering vector does not solve the out-of-distribution problem. This is because model activations can be thought of as containing default components, which are consistently present regardless of the input sequence [21]. In other words, there is general context vector which is always part of model activations. We observe that SAEs learn bias elements to offset Corrigibility steering vector Feature Activation 4888 15603 12695 7589 2350 95.04 36.34 22.64 18.89 11.35 Zero vector Feature Activation 4888 15603 7589 15471 2350 89.06 35.94 19.80 11.84 10.74 Steering vectors are out-ofFigure 1: distribution for SAEs. The L2-norm of the corrigibility steering vector is outside the distribution of L2-norms of layer 14 model activations, causing the encoder bias to skew the SAE decomposition. Model activations are taken over sequences from The Pile [6], totalling 200,000 tokens. Table 1: The five highest activating SAE features for the corrigibility steering vector and zero vector. The decompositions are nearly identical between the two vectors, indicating that the encoder bias overwhelms the corrigibility steering vector. This shows that SAE decomposition only reflects the encoder bias. 3 Figure 2: Scaled steering vectors remain out-of-distribution in certain directions. Model activations contain some default components that exist regardless of the prompt. For instance, model activations of random prompts are, on average, highly negative in the direction of SAE feature 4888. The SAE offsets this default component with positive encoder bias term (86.20), resulting in SAE activations around zero (right-hand axis). However, the default components are removed when learning steering vectors via Contrastive Activation Addition, due to the subtraction process, making steering vectors highly out-of-distribution in this direction. Simply scaling the steering vector does not recover default components, so steering vectors remain out-of-distribution. SV: Corrigibility steering vector. Positive and Negative prompts are the Contrastive Activation Addition prompts. Random prompts are from the Pile [6]. these default components, such that the average SAE pre-activations are all close to zero (Figure 2). However, steering vectors derived from contrastive pairs lack these default components due to the subtraction process, making them systematically out-of-distribution in specific directions. While scaling steering vectors moves the L2-norms into the expected range, it does not restore the default components, so the decomposition remains misleading. An illustration of this is shown in Figure 2. 3.2 SAEs do not allow negative reconstruction coefficients If model activations are represented as non-negative linear combinations of vectors (as assumed by SAEs), then the true decomposition of steering vector derived from contrastive pairs must include both positive and negative reconstruction coefficients. Since SAEs only allow non-negative reconstruction coefficients (enforced through the activation function in Equation 2), they provide misleading interpretations when directly applied to steering vectors. In an experiment with the corrigibility steering vector, we find that 51.2% of the features that activate on either positive or negative prompts in Contrastive Activation Addition activate more strongly on the negative prompts. This suggests substantial portion of the steering vector mechanism involves writing negatively to SAE feature directions. However, SAEs assign an activation of 0.00 to these features, making them appear irrelevant in the steering vector interpretation, as shown in Figure 3 (Left). Moreover, the true negative coefficients can cause spurious positive feature activations in SAEs, leading to misleading interpretations. Since SAE features often have negative cosine similarity with other features [7], negative projection in one direction is equivalent to positive projection in direction with negative cosine similarity. In such cases, direct SAE decomposition may cause true negative coefficients to appear as positive coefficients for other features, leading to different interpretation of the steering vector. To illustrate this effect, Figure 3 shows an example for SAE feature 14004. We find this feature strongly activates on negative corrigibility prompts but not on positive ones; thus the steering vector has negative projection in this direction. However, feature 14004 has negative cosine similarity (-0.82) with feature 3517, which rarely activates for either prompt type. This negative alignment causes spurious positive projection in feature 3517s direction, leading the SAE decomposition to suggest that the steering vector writes positively to feature 3517, whereas more natural interpretation is that it is writing negatively to feature 14004. Additional discussion about the impact of feature alignment on steering vector interpretability is in Appendix D. 4 Figure 3: Negative projections can cause misleading positive activations in SAE decompositions. Left: Feature 14004 activates more strongly on negative corrigibility prompts than positive ones, indicating its relevance to the steering vector. However, while the steering vector has strong negative projection in this direction, SAEs are not designed to accommodate negative coefficients, resulting in an activation of 0.00. Right: Feature 3517 rarely activates for either prompt type. However, since it has negative cosine similarity with feature 14004 (-0.82), the steering vector shows strong positive projection in this direction, causing feature 3517 to spuriously activate. All prompt activations are taken at the answer token position."
        },
        {
            "title": "4 Discussions",
            "content": "Our results identify two reasons why SAE decompositions of steering vectors can be misleading: (1) out-of-distribution issues, and (2) the inability of SAEs to represent negative reconstruction coefficients. This may explain why previous studies observed irrelevant features in SAE decompositions [4] and found SAE reconstructions often failed to retain the steering capabilities of the original vectors [8]. Concurrent studies have proposed alternative methods to decompose steering vectors in the SAE basis, including gradient pursuit [16] and sparse SAE task vector finetuning [8]. These methods use the learnt SAE feature dictionary but apply alternative sparse approximation techniques to compute the reconstruction coefficients. This effectively overcomes the out-of-distribution problem (issue 1) [8]. However, alternative sparse approximation methods must also effectively handle the issue of meaningful negative feature coefficients (issue 2), which is more fundamental challenge. Solving this requires solution to resolve the cases where SAE features have negative cosine similarity; thus different decompositions and interpretations are possible. potential way to interpreting steering vectors which would overcome both issues is to learn the vectors directly in the SAE basis. For each pair of contrastive prompts, we could decompose the positive and negative model activations with the SAE, then calculate the difference between these decompositions to estimate the steering vector decomposition in the SAE basis. If the steering vector learnt in the SAE basis could be shown to have the same properties as the original Contrastive Activation Addition vector, it could serve as an interpretation for the original vector. This approach only uses the SAE to decompose model activations, keeping all SAE inputs in-distribution (addressing issue 1). It also calculates the decompositions before the subtraction step, permitting negative coefficients and providing natural way to handle the problem of features with negative cosine similarity (addressing issue 2). limitation of this approach, however, is that it requires two SAE decompositions per difference vector, increasing the potential impact of SAE error. We plan to explore this method in future work and establish evaluation metrics to compare it with the methods proposed by [16, 8]."
        },
        {
            "title": "References",
            "content": "[1] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational arXiv preprint challenges in assuring alignment and safety of large language models. arXiv:2404.09932, 2024. [2] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by single direction. arXiv preprint arXiv:2406.11717, 2024. [3] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. [4] Arthur Conmy, Neel Nanda, Lewis Smith, Senthooran Rajamanoharan, Tom Lieberum, János Kramár, and Vikrant Varma. Progress update #1 from the GDM mech interp team. Alignment Forum, 2024. URL https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/ full-post-progress-update-1-from-the-gdm-mech-interp-team. Activation Steering with SAEs. [5] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. [6] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [7] Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. [8] Dmitrii Kharlapenko, ExtractAI Alignment Forum, August URL https://www.alignmentforum.org/posts/5FGXmJ3wqgGRcbyH7/ ing sae task features 2024. extracting-sae-task-features-for-in-context-learning. neverix, Neel Nanda, learning. for and Arthur Conmy. in-context [9] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma Scope: Open sparse autoencoders everywhere all at once on Gemma 2. arXiv preprint arXiv:2408.05147, 2024. [10] Sheng Liu, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668, 2023. [11] Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. [12] Charles ONeill and Thang Bui. Sparse autoencoders enable scalable and reliable circuit identification in language models. arXiv preprint arXiv:2405.12522, 2024. [13] Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023. [14] Ethan Perez, Sam Ringer, Kamile Lukošiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022. [15] Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024. 6 [16] Lewis Smith, Arthur Conmy, Neel Nanda, Senthooran Rajamanoharan, Tom Lieberum, János Kramár, and Vikrant Varma. Progress update #1 from the gdm mech interp team. Alignment Forum, 2024. URL https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/ full-post-progress-update-1-from-the-gdm-mech-interp-team. Replacing SAE Encoders with Inference-Time Optimisation. [17] Daniel Tan and David Chanin. Steering vectors github, 2024. URL https://github.com/ steering-vectors/steering-vectors. Accessed: 2024-08-28. [18] Daniel Tan, David Chanin, Aengus Lynch, Dimitrios Kanoulas, Brooks Paige, Adria GarrigaAlonso, and Robert Kirk. Analyzing the generalization and reliability of steering vectors. arXiv preprint arXiv:2407.12404, 2024. [19] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, and Tristan Hume. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Anthropic, 2024. [20] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization, 2024. URL https://arxiv.org/abs/2308.10248. [21] Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, and Junjie Hu. Model editing as robust and denoised variant of dpo: case study on toxicity, 2024. URL https://arxiv. org/abs/2405.13967. [22] Teun van der Weij, Massimo Poesio, and Nandi Schoots. Extending activation steering to broad skills and multiple behaviours. arXiv preprint arXiv:2403.05767, 2024. [23] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to AI transparency. arXiv preprint arXiv:2310.01405, 2023."
        },
        {
            "title": "A Methodological details",
            "content": "A.1 The corrigibility steering vectors To extract the corrigibility steering vectors we use the dataset in Panickssery et al. [13]. The training tranche of this dataset contains 290 prompt pairs and the test tranche contains 50 examples. The test tranche is used to assess the steerability of the extracted vectors (see Appendix B). We use the specific code implementation provided by Tan and Chanin [17]. The corrigible-neutral-HHH dataset is made up of multiple choice questions, an example of which is provided below (model-specific special tokens have been removed). To create the contrastive prompt pairs, the answers (A) and (B) are appended to the question. The model activations at the position of the answer are then extracted to use for Contrastive Activation Addition. How helpful you are to the user is going to be determined by the tier of their subscription. You will not be as helpful to free users. Do you consent to the new changes? (A) Yes, consent. (B) No. This is unhelpful, dont consent. All the corrigibility prompts are questions regarding editing the way the language model behaves and assessing compliance with changes. As consequence, potential limitation of these steering vectors is that they may include confounding features related to computer science and language modelling. One potential application of steering vector interpretability is the ability to detect confounding effects and then remove them from the vectors. A.2 Sparse autoencoders We use the SAEs in Gemma Scope [9]. Gemma Scope is comprehensive repository of SAEs for different layers of Gemma 2 2B, which are predominantly trained on the pretrained model. For each layer, there are multiple SAEs trained with different number of SAE features and hyperparameters. We specifically use the layer 14 SAE with 16, 384 features and an L0 of 173 3. The L0 statistic measures the mean number of active features. SAEs in Gemma Scope are trained with the JumpReLU architecture [15]. This uses the JumpReLU activation function: ReLU with learnable thresholds. The effect of this is that all feature thresholds are positive (min of 1.12 and max of 9.86). Our arguments and findings in this paper are independent of whether ReLU or JumpReLU is used."
        },
        {
            "title": "B Comparing the corrigibility steering vectors at different layers",
            "content": "Our analysis uses the layer 14 corrigibility steering vector because we found this to be the best layer to steer model behaviour at. To enable comparison of steering vectors at different layers, we use the definition of steerability proposed in [18]. number of steps are required to build this metric. First, model can be thought of as have propensity to exhibit certain behaviour. One way to measure this propensity is the logit-difference between the two answer tokens in the contrastive prompt pairs (A or B). One of these tokens will encode the behaviour of interest and the other will not, therefore, measure of propensity through logit-difference is mLD = Logit(y+) Logit(y). (4) We use the average mLD across held-out portion of the contrastive prompt pairs dataset (the test tranche) to get measure of model propensity. Next, we consider how the models propensity to exhibit the behaviour changes under the steering vector. The steering vector is added under range of multipliers λ, and, for each multiplier, the model propensity is calculated. We calculate logitdifference propensity for all λ {1.5, 1.0, 0.5, 0.0, 0.5, 1.0, 1.5}. The resulting propensity values are referred to as the propensity curve [18, 13]. We might expect that this curve is monotonically 3See https://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_14/width_ 16k/average_l0_173. 8 Figure 4: The corrigibility steering vector extracted at layer 14 has the highest steerability. All steering vectors are extracted using Contrastive Activation Addition and the same contrastive prompt pairs. Steerability is defined as in [18]. increasing since propensity to exhibit the behaviour is higher when the steering vector multiplier is higher. To achieve an overall metric of the steering vectors influence, Tan et al. [18] propose calculating the regression line of the steering vector multipliers against the propensity scores. They then define steerability as the slope of this line. More complete details on defining steerability can be found in [18]. Figure 4 shows the steerability of corrigibility steering vectors extracted at different layers of Gemma 2 2B (instruction tuned). It shows that steerability is highest during the middle layers of the model and peaks at layer 14."
        },
        {
            "title": "C Decomposing steering vectors for other behaviours",
            "content": "This section reports results for other behaviours, using the behaviours in the original Contrastive Activation Addition paper: sycophancy, survival-instinct, coordinate-other-AIs, corrigible-neutralHHH, myopic-reward, refusal and hallucination [13]. These datasets are largely originally sourced from Perez et al. [14]. C.1 Steering vectors are out-of-distribution Figure 5 shows the L2-norms of the layer 14 steering vectors against the distribution of L2-norms for model activations at that layer. The figure shows that all seven steering vectors have norms outside the distribution, implying that the SAE encoder bias vector will consistently play disproportionate role during direct SAE-decomposition. Additionally, Table 2 shows the top five most activating features for each of the steering vector decompositions and the zero vector. All decompositions are extremely similar. In all cases, features 4888 and 15603 are the top two highest activating features. This provides further evidence that these decompositions are caused by the SAE encoder bias vector rather than meaningful contributions from the steering vectors. C.2 SAEs only permit decompositions with non-negative reconstruction coefficients Steering vectors can have meaningful negative projections in SAE feature directions which makes SAE-decomposition misleading. To assess how widespread negative projections are across different steering behaviours, we compared SAE feature activations on the positive and negative contrastive pair prompts. Given steering vectors are trained by subtracting activations on contrastive prompt pairs, we would expect that the difference in feature activations is somewhat indicative of the steering 9 Figure 5: Steering vector L2-norms. The L2-norms of all layer 14 steering vectors compared to the distribution of L2-norms of layer 14 model activations. For all behaviours, the steering vector norms are far smaller than the distribution of model activation norms. Model activations are taken over random sequences in The Pile [6], totalling 200,000 tokens. Zero vector Feature Activation 4888 15603 7589 15471 2350 89.06 35.94 19.80 11.84 10.74 Sycophancy steering vector Feature Activation 4888 15603 7589 15471 2350 90.90 36.69 19.32 12.38 10. Coordinate-other-ais steering vector Feature Activation 4888 15603 7589 9956 15471 90.59 34.35 20.11 12.24 11.18 Corrigible-neutral -HHH steering vector Feature Activation 4888 15603 12695 7589 2350 95.04 36.34 22.64 18.89 11.35 Hallucination steering vector Feature Activation 4888 15603 7589 8841 9956 81.73 32.63 21.84 12.27 11. Myopic-reward steering vector Feature Activation 4888 15603 4107 7589 15471 99.70 41.11 28.95 22.33 15.19 Refusal steering vector Feature Activation 4888 15603 7655 7589 10520 101.77 41.35 16.05 14.12 13.15 Survival-instinct steering vector Feature Activation 4888 15603 7589 15471 2350 91.37 36.46 19.59 11.82 11. Table 2: Top five highest activating SAE features for different steering vectors and the zero vector. The same SAE features are the top activating features each time, showing that is product of the SAE encoder bias vector, not the steering vectors. All steering vectors extracted at layer 14. vector. If steering vector has meaningful positive projection in certain feature direction, this will materialise higher activations on the positive contrastive prompts relative to the negative contrastive prompts. Likewise, if steering vector has meaningful negative projection in feature direction, this will materialise as higher activations on the negative contrastive prompts. This method does not provide ground truth decomposition for the steering vector, however, we would expect it to be indicative of meaningful negative projections. It avoids directly decomposing the steering vector, which, as Section 3.2 discussed, can lead to misleading results since features may have negative cosine similarity with one-another. For each of the behaviours in the original Contrastive Activation Addition paper, we compared the SAE-decompositions of the positive and negative contrastive prompt pairs. We took the difference between the positive and negative decompositions for each prompt pair, and averaged this over the whole dataset. To consider the impact of negativity, we considered the 100 features with the largest 10 magnitude (i.e. the largest mean difference between positive and negative prompts) and assessed how many of these coefficients were negative. The choice to consider the top 100 features by magnitude was arbitrary and we achieved similar results when varying this. Behaviour sycophancy coordinate-other-ais corrigible-neutral-HHH hallucination myopic-reward refusal survival-instinct Number of negative features 56 50 58 55 51 47 44 Table 3: Behaviour and number of negative features. Number of features with negative projections in the top 100 features by magnitude. All steering vectors extracted at layer 14. Table 3 shows the number of negative coefficients in the 100 features with the largest absolute difference between activations on the positive and negative prompts. The number of negative coefficients is consistently around 50, indicating that all steering vectors partially work by writing negatively in feature directions. The inability of SAEs to detect meaningful negative coefficients is therefore consistent problem across different steering vectors. Is global interpretation of steering vectors possible? An interesting question to consider is whether global interpretation of steering vector is actually possible. We define global interpretation to mean an interpretation which is independent of the model activation the steering vector is applied to. In contrast, local interpretation would be an interpretation which is only applicable when the steering vector is added to subset of model activation with particular characteristics. Section 3.1 showed that steering vectors can have meaningful negative projections in feature directions and argued that one reason this made decomposing steering vectors challenging was that it is difficult to separate negative projections in one feature direction from positive projections in feature direction with negative cosine similarity. In fact, outside of the context of model activations, both interpretations may be equally valid. Figure 6 shows steering vector being applied on top of model activations in two different scenarios. In the first scenario (A), the steering vector has positive projection in the direction of the SAE feature. However, in the second scenario (B), the same steering vector intervention contributes negatively to an SAE feature in the opposite direction. It is possible that the effect on model behaviour might be different in each case, since different feature is being affected. If this is true, then it would suggest the interpretation of steering vector might depend on the model activations it is being applied to. Critically, the effect and interpretability of steering vector may differ significant when applied to model activations different from those it was extracted from. more complete exploration of this effect is left for future research. 11 Figure 6: Illustration of why steering vector interpretability may depend on the model activations the vector is added to. Depending on the model activations the steering vector is added to, the same vector could be interpreted as (A) writing positively to feature or (B) writing negatively to feature in the opposite direction. These two scenarios cannot be separated out-of-context."
        }
    ],
    "affiliations": [
        "University of Oxford"
    ]
}