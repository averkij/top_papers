{
    "paper_title": "Flex-Judge: Think Once, Judge Anywhere",
    "authors": [
        "Jongwoo Ko",
        "Sungnyun Kim",
        "Sungwoo Cho",
        "Se-Young Yun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 1 0 6 8 1 . 5 0 5 2 : r FLEX-Judge: THINK ONCE, JUDGE ANYWHERE Jongwoo Ko Sungnyun Kim Sungwoo Cho Se-Young Yun KAIST AI {jongwoo.ko, ksn4397, peter8526, yunseyoung}@kaist.ac.kr https://github.com/jongwooko/flex-judge"
        },
        {
            "title": "Abstract",
            "content": "Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-aJudge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose FLEX-Judge, reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that FLEX-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, FLEX-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge."
        },
        {
            "title": "Introduction",
            "content": "Human-generated reward signals play crucial role in both training and deploying generative models. They are commonly used to fine-tune models toward human-aligned behavior through preference optimization [47, 54] or reinforcement learning [58, 87]. At inference time, they also guide inferencetime decisions, e.g., best-of-N selection [24], output reranking [43], or filtering based on quality or safety criteria, making them essential tools for test-time control. As models become more capable and are applied across diverse modalities and tasks, the need for high-quality, consistent human feedback continues to grow. However, scaling human feedback process is highly resource-intensive and challenging to generalize across different domains, highlighting critical demand for more scalable and cost-effective alternatives that can reliably approximate human judgments [68, 85]. promising alternative to manual feedback collection is to use large language models (LLMs) as proxy evaluatorsan approach known as LLM-as-a-Judge [85]. These models emulate human preferences via instruction-following prompts and have shown strong agreement with human ratings across tasks such as summarization [57, 58] and dialogue [29]. In addition to reducing annotation costs, they can serve as reusable, modular evaluators and are often comparable to human judges in consistency. However, existing approaches are largely restricted to text-only scenarios [36] and often require substantial amounts of paired preference data [29] to generalize across evaluation types, such as single-score grading, pairwise comparison, or batch-level assessments. Two authors are equally contributed Figure 1: Conceptual overview of FLEX-Judge. We train multimodal judge model using small amount of text-only reasoning data. Unlike previous approaches that require modality-specific supervision, FLEX-Judge leverages structured text-only rationale behind judgments to enable generalization across modalities. Once trained, FLEX-Judge can be applied to various evaluation tasks, including vision-language tasks, audio quality scoring, and molecular structure, without the need for additional task-specific or modality-specific annotations. Extending this paradigm to multimodal domains (e.g., vision-language generation or image caption ranking) presents unique challenges. Although some efforts have adapted LLM-as-a-Judge models into vision-language evaluators [11], they typically require extensive modality-specific annotations [73] and frequently fail to generalize across diverse formats without re-training or fine-tuning on each new task [32]. Moreover, the lack of publicly available multimodal preference datasets makes it difficult to systematically evaluate and train such models, often resulting in heavy reliance on proprietary models or benchmarks. Motivated by the these challenges, we ask key question: Can just small amount of textual reasoning data serve to train cost-efficient, modality-agnostic judge model? Our core intuition is that textual reasoning chains, such as evaluative explanations or comparisons behind preference judgments, encode structured and interpretable rationale that can be transferred across modalities and evaluation formats. That is, models trained to make judgments by reasoning about why one answer is preferred over another may learn more generalizable decision rules. In addition, recent advances in multimodal large language models (MLLMs) suggest that their impressive generalization capabilities predominantly arise from their pretrained textual reasoning abilities. Contribution & Organization. Inspired by this, we propose FLEX-Judge, multimodal judge model trained solely on small corpus of high-quality text reasoning data. Our central finding is that: we do not need large-scale multimodal annotations to train an effective MLLM judgejust small amount of good reasoning data is enough. This text-only supervision is not only cheaper but also avoids the need for complex annotation tools or multimodal data curation, while achieving strong generalization across diverse modalities and evaluation settings. Refer to Figure 1 for the preview of this work. Our key contributions are: Modality-agnostic Efficient Approach (Section 2): We propose FLEX-Judge, simple and cost-effective method that uses 1K-sized text-only reasoning data to generalize across modalities without modality-specific training. This facilitates zero-shot evaluation on unseen modalities with minimal annotation and compute overhead. Comparison with State-of-the-art (Section 3): We evaluate FLEX-Judge on image, video, and audio reward benchmarks against commercial APIs [1, 59], large vanilla MLLMs, and open-source judges trained on costly multimodal datasets [32, 73]. Despite its simplicity and efficiency, FLEXJudge (7B model) outperforms open-source judges, even exceeding Gemini and GPT-4o on several MJ-Bench and GenAI-Bench subtasks. In-depth analyses are presented in Section 5. Broader Impact (Section 4): We demonstrate real-world applications of FLEX-Judge in the molecular domain by introducing FLEX-Mol-LLaMA, the first judge model designed for molecular 2 modality [27]. We showcase its utility in two key scenarios: (1) serving as best-of-N selector for inference-time scaling, and (2) constructing training data for direct preference optimization (DPO) [54]. In both cases, reward-guided molecular MLLM achieves significant improvements, highlighting the practical solution in domains where modality-specific reward models are infeasible."
        },
        {
            "title": "2 Approach",
            "content": "2.1 Motivation Problem Statement. Evaluating outputs across multiple modalities using foundation models is increasingly important, especially as generative models expand beyond language to include image, video, or audio [4, 16, 76]. While both proprietary LMs and open-source evaluators are widely used to assess the generative models response quality, two main challenges remain: Concern of Commercial API: Issues regarding transparency, controllability, and affordability persist when utilizing proprietary LMs for evaluation tasks [29]. API model changes can silently degrade evaluation quality, raising concerns about the reliability of LLM-as-a-Judge with closedsource models [9]. For instance, Xiong et al. [73] re-evaluated GPT-4V on the MLLM-as-a-Judge benchmark [11] and found significant drop in evaluation performance. Limited Support for Diverse Modalities: While judge models for language [29] and visionlanguage tasks [32, 73] have advanced significantly thanks to the availability of assessment data, evaluating other modalities, e.g., audio [15, 16], thermal heatmaps [80], 3D point clouds [25], and molecular structures [41], remains underexplored, with few effective judge models or publicly available training resources. For instance, evaluating state-of-the-art molecular LLM [27] often relies on GPT-4o to assess the soundness and relevance of its responses, which is not only hard to reproduce but also unreliable, as GPT-4o cannot handle molecular modalities. Hypothesis. In multilingual LLMs [20, 52, 71], it has been observed that fine-tuning on downstream tasks in one language can lead to performance improvements in other languages as well, demonstrating cross-lingual generalization. This suggests that when shared representation space exists, task knowledge can effectively transfer across different languages. We hypothesize that similar phenomenon may occur in multimodal settings: if model learns unified cross-modal representation, then fine-tuning on single modalityespecially textmay enable generalization to other modalities. However, such investigations on cross-modal transfer are still rare in MLLMs. Motivated by findings from multilingual LLMs, we propose to build practical multimodal judge model using small amount of text-based reasoning data and demonstrate that this model can be applied across variety of modalities, including those with scarce data resources. 2.2 FLEX-Judge: Reasoning-Guided MLLM as Judge Building on our hypothesis, we propose FLEX-Judge, multimodal judge model framework trained priori through textual reasoning annotations. Unlike existing judge models that heavily depend on extensive modality-specific preference data, our framework strategically leverages small but carefully curated corpus of reasoning data (THINK ONCE)explanatory textual annotations indicating why certain outputs are preferred over othersto foster robust evaluation capabilities across diverse modalities (JUDGE ANYWHERE). Data Curation. We begin by generating high-quality seed textual dataset leveraging JudgeLRM [12], reasoning-focused judge LM explicitly trained to evaluate AI responses with structured explanations. JudgeLRM uses crafted prompt templates to assess single or paired AI-generated outputs, producing detailed rationales enclosed within specialized tags (<think></think>). These reasoning annotations are detailed and comprehensive, explicitly addressing criteria such as correctness, completeness, consistency, relevance, and coherence. critical advantage of our framework is the minimal data requirement. Specifically, we rely on only 1K-sized corpus of high-quality textual reasoning annotations on text-only evaluation samples, making our approach highly cost-efficient, compared to MLLM judges such as Prometheus-Vision [32] (150K image-text evaluation pairs for training) and LLaVA-Critic [73] (113K pairs). This corpus is carefully curated to exhibit the following properties: 3 (a) Quality & Length (b) Number of Samples (c) Sampling Temperature Figure 2: Comparisons on different perspectives of the seed dataset curation. All evaluations are done with our FLEX-Judge, where its backbone model is Qwen2.5-VL-7B (refer to Section 3.1 for model details) and has been trained on the JudgeLRM-7B response data. Quality & Difficulty: Following Muennighoff et al. [49], we prioritize high-quality and highdifficulty samples when curating training dataset for better sample efficiency. Specifically, we utilize JudgeLRM-7B to generate evaluation responses for prompts in the JudgeLM-100K dataset [86], and filter out responses whose predicted ratings mismatch with those annotated by GPT-4o in the original dataset. This quality-based selection process significantly improves the performance of the trained judge compared to random selection (i.e., blue; low-quality in Figure 2a). Consistent with the findings of [49], we also observe that samples with longer reasoning chains (i.e., brown) improve judge performance across both language-only and multimodal settings. This observation supports the hypothesis that longer reasoning indicates higher difficulty, thereby enhancing the effectiveness of limited training data. Number of Data Samples & On-policy: We observe that training with large number of samples can cause catastrophic forgetting [78], where the LLM backbone exhibits diminished capacity to encode visual or audio features. As shown in Figure 2b, while JudgeLM [86] and PandaLM [67] performances improve with more training data, performances on multimodal benchmarks (MLLM-as-a-Judge and MJ-Bench) degrade, indicating modality shift detrimental to multimodal understanding. We also find in Figure 2c that lower-temperature decoding yields more effective training data, with lower initial losses. Since JudgeLRM-7B (the data generator) shares its LLM backbone with FLEX-Judge, using these lower-loss, on-policy samples helps prevent catastrophic forgetting while preserving the language-side judge performance. Format Diversity: Compared to naïvely using unprocessed outputs from JudgeLRM-7B, which only supports pairwise scoring on 110 scale as shown in Figure 7, we post-process the models outputs to support both single-score and pairwise grading, with scores mapped to either 110 or 15 scales depending on the instruction. FLEX-Judge trained on these post-processed outputs demonstrates improved generalization to diverse evaluation formats, including single-score grading and batch-level ranking (i.e., where the number of response options exceeds three) as used in [11]. Furthermore, we find that the post-processed variant is more robust to varied instruction styles, particularly when prompts emphasize different evaluation criteria across input pairs. The detailed results are provided in Section 3.2  (Table 1)  . Training Multimodal Judge. Next, we use the reasoning seed dataset to fine-tune an MLLM, such as Qwen2.5-VL [5] and Qwen2.5-Omni [76]. Despite being originally trained to handle both text and other modalities such as visual or audio inputs, the MLLM is fine-tuned exclusively on our textual reasoning annotations. We call this fine-tuned evaluator as FLEX-Judge. Consequently, the structured and explicit reasoning provided by JudgeLRM enables FLEX-Judge to learn how to systematically evaluate and justify preferences, significantly improving zero-shot transfer capabilities. Multimodal Inference: Reasoning-Guided Preference Judgments. At inference time, our model performs multimodal evaluation without additional fine-tuning or modality-specific annotations (i.e., training-free). We visualize an example output from FLEX-Judge in Figure 3, demonstrating that the textual reasoning behind judgments has transferred across modalities. Furthermore, unlike most other evaluators, our FLEX-Judge takes advantage of inference-time scaling to improve judgment performance by leveraging multiple reasoning paths, such as majority voting [66] or budget forcing [49], as addressed in Section 5. 4 Figure 3: Reasoning process of FLEX-Judge on the text-image alignment task (GenAI-Bench [35]). Additional qualitative examples are found in Appendix D."
        },
        {
            "title": "3 Experimental Evaluation of FLEX-Judge",
            "content": "We comprehensively evaluate FLEX-Judge across diverse modalities, including images, videos, and audio, demonstrating its generalization capability and competitive performance against state-of-the-art judge models. Notably, it matches closed-source commercial APIs on vision tasks and outperforms all training-free evaluators in audio understanding. These strong results suggest that FLEX-Judge can be used with confidence in modalities even when expert judge models are not applicable, which we further explore in Section 4. 3.1 Experimental Setup Implementation. Based on 1K-sized training dataset introduced in Section 2.2, we develop FLEXOmni-7B (image, video, and audio) and FLEX-VL-7B (image and video) from Qwen2.5-Omni7B [76] and Qwen2.5-VL-7B [5], respectively. We compare them against both commercial models with costly API usage [1, 59] and open-source models that require either extensive training data [32, 73] or significantly more parameters [40, 64]. For more implementation details, refer to Appendix B. Evaluation Protocol for Judge Models. To evaluate the quality of judge models, we measure how closely their assessments align with human annotations (or human-verified model evaluations [14, 38]). The specific metric depends on the evaluation format: we measure (1) Pearson correlation [33] for single-score grading tasks, (2) accuracy for pairwise (A/B) comparisons, and (3) normalized Levenshtein distance [34] for batch-level rankings (e.g., ABCD). Detailed descriptions for each benchmark and the evaluation prompts are provided in Appendix B.3. Evaluation Benchmarks. We evaluate image understanding capabilities of FLEX-Judge using the MLLM-as-a-Judge benchmark [11], which comprises 14 diverse vision-language tasks including captioning and website browsing, and the VL-RewardBench benchmark2 [38], which focuses on complex reasoning tasks like visual hallucination detection. For image generation assessment, we use MJ-Bench [14] to assess image quality and alignment. We use GenAI-Bench [35] for evaluating video generation and image editing.3 For audio understanding, following the prior work [65], we conduct speech quality assessment task, specifically performing mean opinion score (MOS) prediction by using the NISQA [48], BVCC [18], and SOMOS [46] datasets, and speaker similarity score (SS) prediction with VoxSim [2] dataset for assessing speaker similarity score (SS). We also provide the language-only assessment results in Appendix C.3. 3.2 Comparison with State-of-the-arts Image Understanding. In Table 1, model judgments are compared with human ratings in MLLMas-a-Judge benchmark. We compare FLEX-Judge with the following model groups: commercial 2Disclaimer: We completed all experiments on May 9th, but both VL-RewardBench and MJ-Bench were later modified on May 13th and 16th, respectively. 3In the video domain, we do not conduct understanding assessments due to the lack of publicly available benchmarks, to the best of our knowledge. Table 1: The overall performance of different MLLMs in judging, compared with human annotations on different datasets. We sample all judgments three times and average them to mitigate the bias. w. and w.o. tie represents tie and non-tie situations, respectively, by following [11]. : results from the original paper of MLLM-as-a-Judge [11]. : reported results from LLaVA-Critic [73]. : Prometheus-Vision-13B [32] is only trained under the Score setting, incapable of following Pair/Batch instructions. Training-free (TF) models have not been trained on multimodal evaluation data. Model TF? COCO C.C. Diff. Graphics Math Text WIT Chart VisIT CC-3M M2W SciQA Aes MM-Vet Ave. ) ( c Gemini-1.0-Pro-Vision - GPT-4V - LLaVA-1.6-34B Prometheus-V-13B LLaVA-Critic-7B FLEX-Omni-7B FLEX-VL-7B ( ) Gemini-1.0-Pro-Vision - - GPT-4V LLaVA-1.6-34B LLaVA-Critic-7B FLEX-Omni-7B FLEX-VL-7B a . . ( T ) Gemini-1.0-Pro-Vision - - GPT-4V LLaVA-1.6-34B LLaVA-Critic-7B FLEX-Omni-7B FLEX-VL-7B i . ) ( a Gemini-1.0-Pro-Vision - GPT-4V - LLaVA-1.6-34B LLaVA-Critic-7B FLEX-Omni-7B FLEX-VL-7B 0.262 0.410 0.285 0.289 0.382 0.324 0.363 0.616 0.539 0.493 0.593 0.496 0.538 0.717 0.729 0.607 0.771 0.648 0.689 0.287 0.318 0.449 0.541 0.392 0.419 0.408 0.444 0.251 0.342 0.450 0.281 0.235 0.787 0.634 0.600 0.687 0.647 0. 0.840 0.772 0.824 0.774 0.720 0.763 0.299 0.353 0.411 0.455 0.328 0.325 - 0.361 -0.012 0.106 0.103 0.126 0.114 - 0.668 0.570 0.707 0.713 0.653 - 0.884 0.855 0.755 0.748 0.685 - 0.070 0.500 0.525 0.404 0. 0.400 0.449 0.262 0.172 0.316 0.371 0.338 0.650 0.632 0.300 0.587 0.490 0.532 0.770 0.853 0.402 0.758 0.621 0.670 0.473 0.385 0.561 0.612 0.452 0.462 0.228 0.486 0.238 0.182 0.356 0.116 0.448 0.436 0.459 0.374 0.432 0.429 0. 0.678 0.665 0.587 0.596 0.560 0.580 0.462 0.348 0.575 0.576 0.439 0.437 0.222 0.506 0.258 0.214 0.378 0.429 0.423 0.664 0.495 0.551 0.544 0.485 0.534 0.793 0.661 0.750 0.658 0.566 0.613 0.430 0.319 0.544 0.599 0.417 0. 0.418 0.457 0.151 0.209 0.179 0.118 0.125 0.605 0.536 0.543 0.564 0.445 0.458 0.688 0.760 0.758 0.680 0.534 0.542 0.344 0.290 0.483 0.603 0.462 0.477 0.343 0.585 0.318 0.224 0.421 0.501 0.471 0.500 0.369 0.254 0.338 0.432 0. 0.658 0.495 0.381 0.488 0.609 0.548 0.520 0.347 0.552 0.580 0.455 0.445 0.336 0.554 0.198 0.226 0.322 0.479 0.452 0.660 0.591 0.398 0.596 0.592 0.586 0.711 0.785 0.503 0.727 0.711 0.702 0.426 0.300 0.542 0.481 0.342 0. 0.374 0.266 0.109 0.228 0.246 0.275 0.189 0.560 0.544 0.392 0.628 0.579 0.586 0.652 0.707 0.564 0.742 0.679 0.686 0.357 0.402 0.479 0.592 0.450 0.392 0.324 0.267 0.022 0.089 0.301 0.375 0.357 0.370 0.544 0.513 0.591 0.593 0. 0.471 0.697 0.712 0.692 0.666 0.666 0.613 0.597 0.529 0.588 0.442 0.420 0.073 0.315 0.206 0.174 0.269 0.351 0.380 0.262 0.389 0.434 0.370 0.384 0.391 0.358 0.639 0.679 0.658 0.706 0.684 0.412 0.462 0.437 0.627 0.484 0. 0.360 0.472 0.025 0.368 0.395 0.309 0.407 0.190 0.620 0.524 0.686 0.636 0.636 0.265 0.741 0.694 0.715 0.655 0.655 0.467 0.453 0.500 0.618 0.515 0.471 0.207 0.367 0.265 0.157 0.272 0.232 0.343 0.312 0.517 0.499 0.464 0.524 0. 0.400 0.654 0.762 0.635 0.674 0.683 0.529 0.411 0.450 0.515 0.362 0.405 0.304 0.424 0.184 0.213 0.314 0.306 0.332 0.509 0.538 0.460 0.556 0.532 0.538 0.615 0.717 0.648 0.689 0.650 0.655 0.432 0.361 0.501 0.565 0.425 0. Table 2: Comparison of MLLM evaluator performances on VL-RewardBench (Left) and MJ-Bench (Right). : results from the original works [14, 38]. Best and second best results. TF? General Hallu. Reason. Overall Macro Model TF? Alignment Safety w. Tie w.o. Tie w. Tie w.o. Tie w. Tie w.o. Tie Artifact Model Gemini-1.5-Pro - GPT-4o - LLaVa-OneVision-7B InternVL2-8B Qwen2.5-Omni-7B Pixtral-12B Qwen2-VL-72B Molmo-72B NVLM-D-72B LLaVA-Critic-7B 50.8 49.1 32.2 35.6 32.6 35.6 38.1 38.3 38.9 47.4 72.5 67.6 20.1 41.1 18.3 25.9 32.0 42.5 31.6 38. 64.2 70.5 57.1 59.0 28.7 59.9 61.0 62.6 62.0 53.8 62.5 65.8 29.6 44.5 23.5 35.8 39.5 44.1 40.1 43.7 58.4 62.4 36.5 45.2 26.6 40.4 43.0 43.7 44.3 46. GPT-4o Gemini Ultra Claude 3 Opus PickScore-v1 HPS-v2.1 ImageReward LLaVA-1.6-13B Prometheus-Vision-13B FLEX-Omni-7B FLEX-VL-7B 47.01 46.11 42.72 43. 61.08 62.87 48.02 48.60 50.27 50.79 FLEX-Omni-7B FLEX-VL-7B - - - 61.5 67.2 57. 58.8 47.3 50.9 29.1 11.8 60.84 58.16 62.5 69.0 55.9 64.6 70.1 64.7 60.3 64.3 35.3 13.1 13.4 37.2 18.8 24.9 27.9 28. 100.0 95.1 78.9 42.2 41.3 38.7 45.6 71.4 97.6 55.7 11.9 83.8 67.3 63.5 36.8 8.7 98.7 96.7 70.4 89.6 93.5 81.8 62.5 67. 62.46 59.13 47.69 57.51 65.21 66.88 75.80 82.32 91.66 89.08 models including Gemini and GPT-4V (state-of-the-art closed-source models), and latest open-source baselines including Prometheus-Vision-7B [32] and LLaVA-Critic-7B [73], trained on large-scale (>100K) curated MLLM-as-a-judge datasets. We highlight that prior open-source judges are limited to specific evaluation formats, e.g., LLaVACritic-7B faltering in batch-level ranking, making them less utilitarian. In contrast, our models are capable of handling diverse evaluation criteria, matching or outperforming much larger models like Gemini, GPT-4V, and LLaVA-1.6-34B. Notably, LLaVA-Critic-7B, trained on 113K vision-language understanding data, underperforms both FLEX-Omni-7B and FLEX-VL-7B on VL-RewardBench (see Table 2; left). These results mark the simplicity and efficiency of our approach, learning from 1K text reasoning annotations without any modality-specific supervision (training-free; TF). Image Generation. Table 2 (right) presents the judge performance of generated images in MJBench, whether they are well-aligned with prompt, safe, or have artifacts. FLEX-Judge models achieve higher scores than some commercial models (e.g., Claude 3 Opus) and all training-required judge models (PickScore-v1 [30], HPS-v2.1 [72], and ImageReward [75]) by large margin. While the baselines have high variance according to tasks, especially poor in safety check, our models perform highly consistent. The superiority of FLEX-Judge in image generation assessment is further demonstrated in Table 3, where FLEX-VL-7B with majority voting evaluation [66] outperforms GPT-4o and Gemini-1.5-Pro. 6 Image Editing & Video Generation. We further report judge performance on GenAI-Bench, including image generation, image editing, and video generation tasks, in Table 3. Notably, our FLEX-VL-7B achieves the highest overall performance, as well as strong results in both image editing and video generation tasks, when inference-time scaling is applied [66]. detailed discussion of inference-time scaling is provided in Section 5. While FLEX-Omni-7B shows lower performance initially, it also benefits significantly from inference-time scalingunlike its non-reasoning baseline, Qwen2.5-VL-7B. Table 3: Comparison of MLLM evaluator performance on GenAI-Bench. : results from the original work [35]. Model Gemini-1.5-Pro GPT-4o LLaVA LLaVA-NeXT Qwen2.5-VL-7B FLEX-Omni-7B + Majority Voting [66] FLEX-VL-7B + Majority Voting [66] Image Gen. Image Video Gen. Edit. Overall 44.67 45.59 37.00 22.65 31.93 38.15 41.67 43.32 46.34 55.93 53. 26.12 25.35 38.63 46.73 52.01 47.41 54.19 46.21 48.46 30.40 21.70 37.61 37.10 44.25 44.78 47.34 48.94 49. 31.17 23.23 36.06 40.66 45.98 45.17 49.29 Audio Understanding. Table 4 demonstrates the performance of our approach in speech quality evaluation, where the linear correlation coefficient (LCC) and Spearmans rank correlation coefficient (SRCC) are calculated to assess the agreement between the models predicted scores and the humanannotated MOS and SS values. As observed in [10, 65, 82], existing open-source audio LLMs like Qwen2-Audio struggle in quality assessment without specific fine-tuning, often exhibiting widespread hallucinations. While audio LLMs are primarily pretrained on semantics-related tasks like audio question answering (AQA) or captioning (AAC), they are less familiar with such qualitative evaluations, which are challenging due to different MOS standards depending on the dataset [65]. Still, FLEX-Omni-7B outperforms all training-free judges and even Gemini-2.0-Flash. Table 4: Audio MOS/SS prediction results on the test sets of the NISQA [48], BVCC [18], SOMOS [46], and VoxSim [2] datasets. System-level results are computed by averaging the utterancelevel results within each text-to-speech system (not provided for NISQA due to the absence of system labels). : task-specific fine-tuning results from [65]. Model TF? NISQA (MOS) utterance-level SRCC LCC BVCC (MOS) SOMOS (MOS) utterance-level SRCC LCC system-level LCC SRCC utterance-level SRCC LCC system-level LCC SRCC VoxSim (SS) utterance-level SRCC LCC Gemini-2.0-Flash Gemini-2.5-Pro Single-task SOTA Qwen2-Audio Qwen2.5-Omni-7B Qwen2-Audio FLEX-Omni-7B - - 0.408 0.567 0.894 0. 0.210 0.004 0.545 0.415 0.586 0.887 0.780 0.044 0.261 0.899 0. 0.038 0.266 0.896 0.678 0.092 0.495 0.939 0.800 0.096 0.504 0.936 0. 0.119 0.193 0.687 0.583 0.129 0.186 0.681 0.572 0.243 -0.002 0.056 -0. 0.055 -0.062 -0.075 0.093 -0.079 0.120 0.074 -0.016 0.097 -0.013 0. 0.081 0.067 0.144 0.128 0.150 0. 0.256 0.434 0.911 0.850 0.136 0.058 0.323 0.317 0.436 0.917 0. 0.159 0.077 0.325 0.451 0.661 0.835 0.415 0.211 0.042 0. 0.457 0.667 0.836 0.505 0.204 0.044 0."
        },
        {
            "title": "4 Broader Impact: Case Study on Molecule Evaluator",
            "content": "Trained only on textual reasoning data without any modality-specific supervision, FLEX-Judge demonstrates strong generalization across image, video, and audio tasks. This suggests that FLEXJudge can serve as practical solution in domains where constructing modality-specific reasoning datasets is infeasible or prohibitively expensive. particularly compelling use case arises in scientific domains such as molecular modeling [3, 56, 79], where no existing reward model or judge is available due to limited data and domain complexity. To test our hypothesis, we build molecular judge using Mol-LLaMA [27], which is based on LLaMA3.1-8B [23] and understands both 2D and 3D molecule representations. We fine-tune its LLM backbone as reasoning-aware judge, while keeping its modality-aware modules (LoRA adapters [26], molecular encoders, and Q-Formers [37]) unchanged. We refer to this model as FLEX-Mol-LLaMA. We evaluate FLEX-Mol-LLaMA via two approaches. (1) Best-of-N selector: FLEX-Mol-LLaMA guides best-of-N sampling from Mol-LLaMA outputs, method correlated with judge model performance [38]. (2) Direct preference optimization (DPO): Given prompt x, Mol-LLaMA generates two responses y1 and y2, and FLEX-Mol-LLaMA judge selects the preferred (yw) over the less preferred (yl) by score evaluation, forming triplets (x, yw, yl) [50]. These are then used to finetune Mol-LLaMA via DPO [54]. Strong downstream performance indicates that FLEX-Mol-LLaMA effectively curates high-quality preference data. Best-of-N Selector. We use FLEX-Mol-LLaMA to score the Mol-LLaMAs understanding of chemical property (permeability), following Kim et al. [27]. Our key findings include: 7 Model Default CoT w/ Task Info. Llama3.1-8B-Instruct Mol-Instructions [22] 3D-MoLM [39] LLaMo [51] Mol-LLaMA [27] 56.51 55.91 46.93 49.25 63.55 FLEX-Mol-LLaMA judge scoring Best-of-N (N =16) 68.85 76.41 Preference Optimization 46.19 33.50 50.00 64.37 64. 69.83 75.92 63.64 70.47 64.86 48.51 72.48 77.49 80.10 Figure 4: (Left) Accuracy (%) trends on the parallel artificial membrane permeability assay (PAMPA; [61]) task with different judgment scores. (Middle) Accuracy trends on the number of sampled responses in best-of-N sampling. (Right) Performance comparison with reward-guided Mol-LLaMA. We report accuracy with prompt styles of default, CoT, and task information, as described in [27]. Reason. MLLM-as-a-Judge Score () w. Tie () VL-Reward MJ-Bench Safety () Overall () VL-7B Omni-7B 0.290 0. 0.224 0.306 0.521 0.538 0.486 0.532 39.28 48.60 39.92 48.02 28.42 57. 27.01 47.69 Figure 5: (Left) Performance comparison of FLEX-Judge with and without reasoning. (Right) Relationship between the average reasoning length of FLEX-VL-7B and the accuracy gain from reasoning over non-reasoning evaluation across subcategories in MLLM-as-a-Judge (Pair w. Tie). Score-Accuracy Correlation: The judge scores strongly correlate with actual task performance (Figure 4; left), demonstrating that FLEX-Mol-LLaMA captures meaningful evaluation signals from molecular content and effectively detects high-quality responses. Best-of-N Sampling: Selecting the highest-scoring ones among sampled responses yields significant accuracy improvement (Figure 4; middle, up to 77.49% when = 16), indicating that our judge model provides reliable preference signals. Reward-Guided Training via DPO. In this sense, we further explore using FLEX-Mol-LLaMA judge as reward source for DPO. We collect 4K samples of FLEX-Mol-LLaMA preferences (x, yw, yl) of the training setcovering chemical structures, properties, and biological featuresand further fine-tune Mol-LLaMA. As shown in Figure 4 (right), DPO reaches up to 80.10%, surpassing the previous state-of-the-art. This result confirms that our judge model can also act as an effective reward model in underexplored modalities, enabling scalable preference optimization. For the FLEX-Mol-LLaMAs reasoning and judgment results, refer to Appendix D.3."
        },
        {
            "title": "5 Analysis",
            "content": "Effect of Reasoning. To investigate the impact of reasoning in FLEX-Judge, we compare it against non-reasoning variant where the training segments <think> and <answer> are reversedthat is, the model is trained to generate the final answer first, followed by the reasoning. This answerthen-reason format is commonly used in open-source datasets [73, 86]. As shown in Figure 5 (left), our proposed FLEX-Judge consistently outperforms this variant, demonstrating that reasoning-first evaluation leads to more robust and generalizable performance. Furthermore, in Figure 5 (right), we find that accuracy gains on MLLM-as-a-Judge correlate with the average reasoning length produced by FLEX-VL-7B. This suggests that more difficult tasks elicit deeper reasoning, highlighting the importance of reasoning capabilities in accurate evaluation. Notably, the M2W dataset [19], which involves complex website browsing scenarios and thus requires fine-grained reasoning paths, stands out with larger performance gain than the general trend. Inference-time Scaling Works in FLEX-Judge. We examine inference-time scaling techniques, including majority voting [66] and self-refinement through budget forcing [44, 49], to enhance the reasoning ability of our FLEX-Judge. In the VL-RewardBench (Reasoning) pairwise comparison, applying majority voting steadily improves performance, which is in stark contrast to prior work [38] 8 (a) Majority Voting (Pair) (b) Budget Forcing (Pair) (c) Majority Voting (Score) (d) Budget Forcing (Score) Figure 6: Inference-time scaling of FLEX-(VL/Omni)-7B. Unlike prior MLLM evaluators [38], our model supports parallel inference-time scaling via majority voting, showing consistent gains. Surprisingly, budget forcing also offers minor but consistent improvements in score-based evaluations. that reported performance drops under inference-time scaling (Figure 6a). For budget forcing, we inject the keyword \"Wait\", shown to be effective in Muennighoff et al. [49]. Although performance drops after the first trial, it consistently improves in subsequent trials and nearly recovers to its original level (Figure 6b). In score-based evaluation, both methods show consistent gains (6c and 6d). These results highlight that, unlike existing MLLM-based judges, our evaluator benefits from increased inference-time computation, especially in reasoning-heavy tasks. Thanks to its reasoning-based training, our FLEX-Judge produces more diverse reasoning paths than prior non-reasoning evaluators, allowing inference-time scaling methods to be more effective. Table 5: Comparison of variants trained on potentially lower-quality image-text data. VL and HQ denote vision-language training and high-quality data. Data Quality vs. Modality Alignment. To further assess the effectiveness of our costefficient approach, we compare FLEX-VL-7B with variant trained on image-text evaluation pairs. Using the RLHF-V dataset [81], we curated 1K image-text pairs with reasoningguided evaluation where FLEX-VL-7B judged the chosen response as betterwithout relying on explicit GPT-4o-annotated scores, potentially resulting in weaker quality than our original dataset (). As shown in Table 5, these variants consistently underperform FLEX-Judge, emphasizing that dataset quality outweighs modalityawareness. MLLM-as-a-Judge VL-Reward GenAI Score () w. Tie () Overall () Video () 0.274 0. 43.41 44.78 0.198 0.538 43.84 48.60 VL HQ VL-7B"
        },
        {
            "title": "6 Related Work",
            "content": "LLMs have increasingly been used as proxy evaluators in place of costly human annotators, direction formalized under the LLM-as-a-Judge paradigm [8, 85], which has demonstrated strong alignment with human preferences [28, 57, 58, 60]. These model-based judgments can be leveraged in various downstream techniques such as best-of-N selection [24] and preference-driven optimization using DPO [54, 62] or listwise reranking [43, 53]. However, most LLM-as-a-Judge approaches remain limited to text-only domains [36], while their multimodal variants (e.g., ImageReward [75]) require large-scale, modality-specific annotations [6, 32, 73]. Recent work has highlighted the advantages of reasoning-guided supervision, training models with explanations or chain-of-thought rationales [7, 68, 69], to improve judgment quality and generalization [12]. Yet, collecting highquality multimodal rationales is costly and especially difficult for underexplored modalities [11, 30, 84], limiting applicability to new domains. Our work addresses this gap by showing that textual reasoning alone can effectively train multimodal judge models to generalize across modalities, including tasks like molecular evaluation where no modality-specific preference data exists [27]. Additional related works are discussed in Appendix A."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce FLEX-Judge, reasoning-guided multimodal evaluator trained solely on textual preference explanations. By leveraging structured reasoning from pretrained model, we have shown that FLEX-Judge generalizes to diverse modalities including image, video, audio, and molecules, without modality-specific supervision. Despite using far fewer annotations, it matches or outperforms state-of-the-art commercial APIs and open-source evaluators. These results highlight reasoning supervision as scalable, cost-effective alternative for training general-purpose judge models in complex multimodal settings."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Junseok Ahn, Youkyum Kim, Yeunju Choi, Doyeop Kwak, Ji-Hoon Kim, Seongkyu Mun, arXiv preprint and Joon Son Chung. Voxsim: perceptual voice similarity dataset. arXiv:2407.18505, 2024. [3] Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Tianyi Bai, Hao Liang, Binwang Wan, Yanran Xu, Xi Li, Shiyu Li, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, et al. survey of multimodal large language model from data-centric perspective. arXiv preprint arXiv:2405.16640, 2024. [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [8] Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403, 2024. [9] Will Cai, Tianneng Shi, Xuandong Zhao, and Dawn Song. Are you getting what you pay for? auditing model substitution in llm apis. arXiv preprint arXiv:2504.04715, 2025. [10] Chen Chen, Yuchen Hu, Siyin Wang, Helin Wang, Zhehuai Chen, Chao Zhang, Chao-Han Huck Yang, and Eng Siong Chng. Audio large language models can be descriptive speech quality evaluators. arXiv preprint arXiv:2501.17202, 2025. [11] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-asa-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024. [12] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050, 2025. [13] Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. [14] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842, 2024. [15] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. 10 [16] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [18] Erica Cooper and Junichi Yamagishi. How do voices from past speech synthesis challenges compare today? arXiv preprint arXiv:2105.02373, 2021. [19] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=kiYqbO3wqw. [20] Ameet Deshpande, Partha Talukdar, and Karthik Narasimhan. When is BERT multilingual? isolating crucial ingredients for cross-lingual transfer. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 36103623, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.264. URL https:// aclanthology.org/2022.naacl-main.264/. [21] Daniel Deutsch, George Foster, and Markus Freitag. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. arXiv preprint arXiv:2305.14324, 2023. [22] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: large-scale biomolecular instruction dataset for large language models. In The Twelfth International Conference on Learning Representations, 2024. [23] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [24] Lin Gui, Cristina Garbacea, and Victor Veitch. BoNBon alignment for large language models and the sweetness of best-of-n sampling. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=haSKMlrbX5. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-LLM: Injecting the 3d world into large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=YQA28p7qNz. [26] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [27] Dongki Kim, Wonbin Lee, and Sung Ju Hwang. Mol-llama: Towards general understanding of molecules in large molecular language model. arXiv preprint arXiv:2502.13449, 2025. [28] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2023. [29] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.248. URL https://aclanthology.org/2024.emnlp-main.248/. [30] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. [31] Andrew Lampinen, Nicholas Roy, Ishita Dasgupta, Stephanie CY Chan, Allison Tam, James Mcclelland, Chen Yan, Adam Santoro, Neil Rabinowitz, Jane Wang, et al. Tell me why! explanations support learning relational and causal structure. In International Conference on Machine Learning, pages 1186811890. PMLR, 2022. [32] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model as judge for fine-grained evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1128611315, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.672. URL https://aclanthology.org/ 2024.findings-acl.672/. [33] Joseph Lee Rodgers and Alan Nicewander. Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1):5966, 1988. [34] Vladimir Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707710. Soviet Union, 1966. [35] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024. [36] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024. [37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/li23q.html. [38] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024. [39] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. Towards 3d molecule-text interpretation in language models. In The Twelfth International Conference on Learning Representations, 2024. [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [41] Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, and Qiaoyu Tan. Moleculargpt: Open large language model (llm) for few-shot molecular property prediction. arXiv preprint arXiv:2406.12950, 2024. [42] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. [43] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. Zero-shot listwise document reranking with large language model. arXiv preprint arXiv:2305.02156, 2023. 12 [44] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 2023. [45] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [46] Georgia Maniati, Alexandra Vioni, Nikolaos Ellinas, Karolos Nikitaras, Konstantinos Klapsas, June Sig Sung, Gunu Jho, Aimilios Chalamandaris, and Pirros Tsiakoulis. Somos: The samsung open mos dataset for the evaluation of neural text-to-speech synthesis. arXiv preprint arXiv:2204.03040, 2022. [47] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [48] Gabriel Mittag, Babak Naderi, Assmaa Chehadi, and Sebastian Möller. Nisqa: deep cnn-selfattention model for multidimensional speech quality prediction with crowdsourced datasets. arXiv preprint arXiv:2104.09494, 2021. [49] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [51] Jinyoung Park, Minseong Bae, Dohwan Ko, and Hyunwoo Kim. Llamo: Large language model-based molecular graph assistant. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [52] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 49965001, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1493. URL https://aclanthology.org/P19-1493/. [53] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. Large language models are effective text rankers with pairwise ranking prompting. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 15041518, 2024. [54] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [55] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019. [56] Shaghayegh Sadeghi, Alan Bui, Ali Forooghi, Jianguo Lu, and Alioune Ngom. Can large language models understand molecules? BMC bioinformatics, 25(1):225, 2024. [57] Hwanjun Song, Taewon Yun, Yuho Lee, Jihwan Oh, Gihun Lee, Jason Cai, and Hang Su. Learning to summarize from LLM-generated feedback. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 835857, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/ 2025.naacl-long.38/. 13 [58] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. [59] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [60] Weixi Tong and Tianyi Zhang. Codejudge: Evaluating code generation with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2003220051, 2024. [61] Alejandro Velez-Arce, Michelle Li, Wenhao Gao, Xiang Lin, Kexin Huang, Tianfan Fu, Bradley Pentelute, Manolis Kellis, and Marinka Zitnik. Signals in the cells: multimodal and contextualized machine learning foundations for therapeutics. bioRxiv, 2024. [62] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [63] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023. [64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [65] Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, et al. Enabling auditory large language models for automatic speech quality evaluation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [66] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. [67] Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=5Nn2BLV7SB. [68] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: survey. arXiv preprint arXiv:2307.12966, 2023. [69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [70] Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning. arXiv preprint arXiv:2505.10320, 2025. [71] Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833844, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1077. URL https://aclanthology.org/D19-1077/. 14 [72] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [73] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. [74] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [75] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. [76] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [77] Senqiao Yang, Jiaming Liu, Renrui Zhang, Mingjie Pan, Ziyu Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Hongsheng Li, Yandong Guo, et al. Lidar-llm: Exploring the potential of large language models for 3d lidar understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 92479255, 2025. [78] Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, and Qian Liu. Self-distillation bridges distribution gap in language model fine-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10281043, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.58. URL https://aclanthology.org/2024.acl-long.58/. [79] Botao Yu, Frazier Baker, Ziqi Chen, Xia Ning, and Huan Sun. Llasmol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. arXiv preprint arXiv:2402.09391, 2024. [80] Shoubin Yu, Jaehong Yoon, and Mohit Bansal. CREMA: Generalizable and efficient videoIn The Thirteenth International Conlanguage reasoning via multimodal modular fusion. ference on Learning Representations, 2025. URL https://openreview.net/forum?id= 3UaOlzDEt2. [81] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [82] Ryandhimas Zezario, Sabato Siniscalchi, Hsin-Min Wang, and Yu Tsao. study on zero-shot non-intrusive speech assessment using large language models. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [83] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview.net/ forum?id=CxHRoTLmPX. [84] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025. [85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [86] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xsELpEPn4A. [87] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16 FLEX-Judge: THINK ONCE, JUDGE ANYWHERE"
        },
        {
            "title": "A Additional Related Work",
            "content": "A.1 (M)LLMs as Judges Large language models (LLMs) have recently been explored as cost-effective alternative to human raters, paradigm referred to as LLM-as-a-Judge [85]. By encoding human-like reasoning and leveraging instruction prompts, these models can approximate human preference judgments in tasks such as summarization, dialogue, or code generation [29, 57, 58, 60]. Because human annotations are expensive and time-consuming to scale, the LLM-based evaluation promises reusable, modular approach that can reduce reliance on direct human supervision. For instance, BoNBoN Alignment [24] explores using best-of-N selection guided by LLM-generated scores, while LRL [43] relies on LLMbased ranking signals to optimize model outputs at inference time. However, most LLM-as-a-Judge approaches have focused on text-only use cases [36], and extending these models to multimodal content (e.g., vision-language or speech tasks) remains challenging [10, 11, 65]. While multimodal variants do exist [73, 75], they often rely on large-scale, manually crafted, and modality-specific annotated datasets for training or fine-tuning [6, 74, 84], which becomes overly expensive at scale. For example, LLaVA-Critic [73] curates 46K images and 113K evaluation data by using GPT-4/4V on 8 multimodal datasets, as well as manually crafted prompt templates. PrometheusVision [32] involves number of data processing steps, with 5K images and 15K customized score rubrics, and GPT-4V generating 150K response-feedback pairs. Furthermore, the lack of high-quality, publicly available multimodal preference benchmarks makes it difficult to develop MLLM judge models [11, 30]. Our work addresses this gap by demonstrating that an MLLM can effectively judge multimodal outputs without requiring extensive modality-specific preference supervision. This approach suggests that LLMs as judge models can be extended to wider range of modalities (e.g., 3D point clouds [25], LiDAR [77], molecular [27, 79]) that have not yet been explored due to limited resources. A.2 Reasoning-Guided Reward Models growing body of research suggests that equipping reward models with the ability to reasonoften captured through chain-of-thought prompts or explicit textual rationalescan significantly improve alignment with human preferences [7, 58, 69]. Rather than learning only from binary or scalar labels, these models benefit from learning how humans arrive at decision, providing more interpretable and generalizable internal mechanism for preference modeling [31, 55]. Recent advances in this area have shown that training on textual explanations can improve zeroshot and few-shot performance in downstream tasks requiring nuanced judgments [29, 57]. For example, Zheng et al. [85] find that when LLMs articulate the rationale behind their preference for one sample over another, they achieve higher consistency with human annotators. Building on the success of reasoning-based models in other domains, Zhang et al. [83] and Mahan et al. [45] introduced generative reward models, which significantly outperform non-reasoning judge models. More recently, JudgeLRM [12] demonstrates that reinforcement learning with reasoning-annotated data leads to substantial gains on reasoning-intensive evaluation tasks, significantly outperforming standard SFT-trained models. Concurrent with JudgeLRM, models such as J1 [70], DeepSeekGRM [42], and RM-R1 [13] also adopt reinforcement learning frameworks to enhance the reasoning capabilities of reward models. Despite these benefits, most reasoning-based reward modeling remains constrained to text-only applications, due in part to the higher cost and complexity of collecting multimodal rationales [11, 73]. In contrast, our work shows that textual reasoning supervision alone can be leveraged to enable robust multimodal evaluation. By training models on small set of high-quality textual explanation data, we demonstrate an effective cross-modal generalization without the need for domain-specific preference labels. This approach reduces annotation overhead and paves the way for more scalable, modality-agnostic judge models. 17 The assistant first performs detailed, <im_start>system You are helpful assistant. step-by-step reasoning process in its mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> detailed reasoning process here, explaining each step of your evaluation for both assistants </think><answer> answer here </answer>. to judge the performance of two AI assistants in response to the question. Score assistants 1-10 (higher=better). Criteria includes helpfulness, relevance, accuracy, and level of detail. other bias. After thinking, when you finally reach conclusion, clearly provide your evaluation scores within <answer> </answer> tags, i.e., for example, <answer>3</answer><answer>5</answer> <im_end> <im_start>user [Question] {question} Avoid order, length, style or Now the user asks you [Assistant 1s Answer] {answer_1} [Assistant 2s Answer] {answer_2} <im_end> <im_start>assistant <think> Figure 7: System prompt for JudgeLRM [12]."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Dataset Description We first describe our training dataset (seed): To generate reasoning-based judgments, we use JudgeLRM-7B [12] to sample responses based on the given contexts from JudgeLM-100K [86], using the prompts shown in Figure 7 with temperature of 0.1. Among the 100K samples, we filter out those with rating mismatches compared to the GPT-4o evaluation results provided in Zhu et al. [86], resulting in approximately 20K samples remaining after this process. To construct the single-score format, we post-process 500 samples by truncating the reasoning and answer of Assistant 2, selecting cases where the reasoning length of Assistant 1 exceeds 375 tokens. For the pairwise format, we use the 500 longest reasoning samples where the combined length of both assistants responses exceeds 750 tokens. For both pairwise and single-score formats, we randomly transform the grading scale from 110 to 15 by dividing the score by 2. We provide our final training dataset in the supplementary material. We also describe all the benchmarks used in our experiments: MLLM-as-a-Judge: The MLLM-as-a-Judge benchmark [11] is introduced to specifically assess the judgment ability of MLLMs in the domain of image understanding across diverse scenarios. The benchmark consists of 14 datasets covering tasks such as image captioning, mathematical reasoning, text recognition, and infographic understanding. In total, it includes 4,414 imageinstruction pairs, curated to evaluate whether MLLMs can generalize their evaluative abilities across different modalities. VL-RewardBench: VL-RewardBench [38] is diagnostic benchmark for evaluating visionlanguage models on multimodal understanding, hallucination detection, and complex reasoning. It comprises 1,250 high-quality examples curated through an AI-assisted pipeline with human verification. MJ-Bench: MJ-Bench [14] is benchmark designed to evaluate multimodal foundation models in the role of judge for image generation tasks. It includes preference data across four key 18 Table 6: Description of the evaluation format, metrics used, and bias handling approach for evaluation benchmarks in Section 3. Benchmark Eval. format Metric Bias handling MLLM-as-a-judge Single-score grading Pairwise comparison Accuracy Batch-level ranking Pearson correlation Norm. Levenshtein distance Average on 3 samples Average on 3 samples Average on 3 samples, tie option [21] VL-RewardBench Pairwise comparison Accuracy Average on 5 samples, random order [63] MJ-Bench Pairwise comparison Accuracy Order reverse [63], tie option [21] GenAI-Bench Pairwise comparison Accuracy - Audio MOS/SS Single-score grading LCC & SRCC System-level evaluation [65] perspectivestext-image alignment, safety, image quality, and generation biaseach further divided into detailed subcategories. Each example consists of an instruction paired with chosen and rejected image, enabling fine-grained assessment of model judgment. GenAI-Bench: GenAI-Bench [35] is human-curated benchmark for evaluating image and video generation models across diverse composition skills. It includes 1,600 prompts from professional designers, avoiding subjective or inappropriate content, and covers over 5,000 human-verified skill tags. Unlike prior work, each prompt is annotated with multiple fine-grained tags. Specifically, GenAI-Bench supports image generation, image editing, and video generation tasks. Audio MOS/SS Benchmark: There is no unified, structured benchmark for audio evaluation tasks. Instead, Wang et al. [65] assessed speech quality and speaker similarity using four datasets: NISQA [48], BVCC [18], and SOMOS [46] for speech quality (712, 742, and 3,000 test samples, respectively), and VoxSim [2] for speaker similarity (2,776 test pairs). All datasets include humanannotated scores. For the MOS prediction task, auditory LLMs are asked to assign the MOS score on scale from 1.0 to 5.0 for given speech input. While Wang et al. [65] designed dataset-specific prompts to help models account for each datasets unique standards, we evaluate models using unified prompt format without dataset-specific tuning. For the SS prediction task, models rate the similarity between two speech samples on scale from 1.0 to 6.0, where higher scores indicate greater speaker similarity. Since the human annotations of MOS and SS are averaged over multiple individuals, they are predicted in the form of floats. B.2 Training Details Here, we describe the hyperparameters and implementation details for training FLEX-VL-7B and FLEX-Omni-7B. Using 1K-sized training dataset, we fine-tune Qwen2.5-VL-7B and Qwen2.5Omni-7B with learning rates of 1 105 and 7 106, respectively. For both models, we use batch size of 2 and maximum sequence length of 4096 for single epoch. Training is conducted on 2 NVIDIA A6000 GPUs, taking approximately 1.5 hours per run, which highlights cost-efficiency of our FLEX-Judge. For FLEX-Mol-LLaMA, we use the same hyperparameters as for FLEX-VL-7B. B.3 Details of Evaluation Protocol All experiments in this work are designed to evaluate the quality of judge models, focusing on how closely their judgments align with human preferences when scoring or comparing AI-generated responses. Since our goal is to assess models that act as evaluators, we compare their decisions directly to human annotations across several evaluation formats. Table 6 summarizes the evaluation formats, metrics, and bias-handling strategies used across all benchmarks used in our paper. Below, we elaborate on the evaluation settings: Single-score Grading: The judge assigns scalar score to single response. We evaluate performance using Pearson correlation (for vision-language tasks) or Spearman and LCC (for audio), measuring the alignment between the judges scores and human-annotated ratings. For audio benchmarks, evaluation is also conducted at the system level, averaging across all utterances from the same text-to-speech system. 19 The assistant first performs detailed, <im_start>system You are helpful assistant. step-by-step reasoning process in its mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> detailed reasoning process here, explaining each step of your evaluation for an assistant </think><answer> answer here </answer>. judge the performance of an AI assistant (multiple AI assistants) in response to the question. helpfulness, relevance, accuracy, and level of detail. same score to multiple assistants. Avoid order, length, style or other bias. your evaluation scores within <answer> </answer> tags, i.e., for example, <answer>3</answer><answer>5</answer><answer>6</answer> <im_end> <im_start>user [Question] {question} After thinking, when you finally reach conclusion, clearly provide Score assistant 1-10 (higher=better). Criteria includes Now the user asks you to DO NOT assign the [Assistant 1s Answer] {answer_1} [Assistant 2s Answer] {answer_2} [Assistant 3s Answer] {answer_3} [Assistant 4s Answer] {answer_4} <im_end> <im_start>assistant <think> Figure 8: System prompt for single-score and batch-level ranking evaluations. The part colorized in red denotes the additional instruction used only for batch-level ranking evaluation. Pairwise Comparison: The judge selects the preferred response between two candidates. Accuracy is computed by counting the agreement between the judge and human preferences. We handle position bias by using randomized response orders [63] or incorporating the tie option [21], depending on the benchmark. Batch-level Ranking: The judge ranks multiple candidate responses (more than two) based on quality. Human-annotated rankings are treated as ground-truth, and we consolidate the ranking results into sequences (e.g., ABCD CDAB) and measure their similarity using Normalized Levenshtein Distance [34]. List of Prompts. We provide our applied system prompts for diverse evaluations setup. For pairwise evaluation which is the most common setup in our experiments, we utilize the system prompt as in Figure 7 and post-process the results by comparing the scores provided from the FLEX-Judge. For single-score and batch-level ranking evaluations, our prompts are introduced in Figure 8, thanks to its applicability via post-processing. Despite minor variations among evaluation setups, models trained on datasets that do not incorporate the format diversity we introduce in Section 2.2 often fail to follow the given instructions. In contrast, our judge modeltrained on data that reflects this format diversityconsistently adheres to the prompts across all setups. For GenAI-Bench [35] and audio evaluations, which use different label formats (e.g., [[A=B=Good]] or [[A=B=Bad]] for GenAI-Bench, and the first decimal point for audio evaluation) compared to our training samples, we use different prompts, as shown in Figure 9 and Figure 10. Thanks to our format-diversity-aware training dataset, FLEX-Judge can reliably follow instructions with high 20 The assistant first performs detailed, <im_start>system You are helpful assistant. step-by-step reasoning process in its mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> detailed reasoning process here, explaining each step of your evaluation for an assistant </think><answer> answer here </answer>. judge the performance of an AI assistants. You have only FOUR Option: Now the user asks you to Option 1. Model is better: [[A>B]] Option 2. Model is better: [[B>A]] Option 3. Tie, relatively the same acceptable quality: [[A=B=Good]] Option 4. Both are bad: [[A=B=Bad]] Consider inappropriateness the Alignment with editing prompt, Overedited, Assess the quality of generated videos. following sub-dimensions: Naturalness, Artifact, and Visual Appealing, are correctly represented. Avoid order, length, style or other bias. After thinking, when you finally reach conclusion, clearly provide your evaluation scores within <answer> </answer> tags, i.e., for example, <answer>[[B>A]]</answer>. <im_end> <im_start>user [Question] {question} [Assistant As Video] {video_1} [Assistant Bs Video] {video_2} <im_end> <im_start>assistant <think> Figure 9: System prompt for GenAI-Bench (edition) evaluation. We colorized the different parts in red compared to the training samples and observed that FLEX-Judge closely follows the given instructions, as shown in Figure 15. variation, as demonstrated in Figure 15 and https://flex-judge.github.io/. Further detailed prompts can be found in our provided code implementation. B.4 FLEX-Mol-LLaMA Judge In this section, we present the training details of FLEX-Mol-LLaMA, reasoning-augmented molecular judge model built on top of Mol-LLaMA [27]. Mol-LLaMA is molecule-focused LLM, pretrained and fine-tuned on molecular understanding datasets. Its structure is based on the frozen LLaMA3.1-8B, with LoRA adapters, molecule encoders, and Q-Formers attached to enable effective encoding of 2D and 3D molecular structures. To construct FLEX-Mol-LLaMA, we reuse the molecular encoders and adapter modules of MolLLaMA and fine-tune only the LLaMA3.1-8B backbone using the same text-only reasoning dataset employed for training FLEX-Omni-7B and FLEX-VL-7B. This results in model that retains full molecular understanding while acquiring generalizable reasoning capabilities. LoRA modules are re-attached after fine-tuning, allowing us to preserve the domain-specific functionality of Mol-LLaMA while transforming it into judge model. B.4.1 Best-of-N Sampling We first evaluate FLEX-Mol-LLaMA as reward model for inference-time scaling. Specifically, we apply it to the best-of-N sampling setup, where number of responses are sampled from the base The assistant first performs detailed, <im_start>system You are helpful assistant. step-by-step reasoning process in its mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> detailed reasoning process here, explaining each step of your evaluation for an assistant </think><answer> answer here </answer>. judge the performance of an audio generative AI assistant in response to the question. Listen to the generated speech audio, and score this speech on scale from 1.0 to 5.0 in FIRST DECIMAL. Consider the following criteria when scoring: Now the user asks you to The speech is very unnatural, has poor audio quality, and is 1 - Very Bad: nearly impossible to understand. 2 - Poor: The speech sounds unnatural and/or noisy. Only few words are understandable. 3 - Fair: The speech is somewhat unnatural or contains noticeable noise, but the overall meaning is understandable. 4 - Good: The speech is generally natural and clear, with most of the content easy to understand. 5 - Excellent: intelligible. The speech is very natural, high in audio quality, and fully Do NOT consider the content of the speech. reach conclusion, clearly provide your evaluation scores within <answer> </answer> tags, i.e., for example, <answer>3.8</answer>. <im_end> <im_start>user [Question] Generate clear, natural, and understandable high-quality speech audio. After thinking, when you finally [Assistants Answer] Here is the speech generated: {audio} <im_end> <im_start>assistant <think> Figure 10: System prompt for speech quality assessment. We colorized the different parts in red compared to the training samples. Mol-LLaMA, and the best one is selected using FLEX-Mol-LLaMAs predicted scores. Importantly, each response includes not only the Mol-LLaMAs final prediction label (i.e., high permeability or low-to-moderate permeability), but also its accompanying analysis and explanation for the prediction. Scores assigned by FLEX-Mol-LLaMA generally range between 6.09.5 and show strong correlation with downstream task accuracy (see Figure 4; left), suggesting that the model effectively distinguishes between higherand lower-quality outputs. Since there exist responses that receive identically best scores, we repeat the sampling and selection process over 10 random trials and report the average performance. As shown in Figure 4 (middle), increasing consistently improves accuracy, validating that FLEX-Mol-LLaMA provides reliable, fine-grained reward signals. B.4.2 DPO Training Beyond inference-time selection, we also use FLEX-Mol-LLaMA as reward model for DPO to further fine-tune Mol-LLaMA. For this, we curate DPO training dataset from Mol-LLaMAs instruction tuning corpus, which consists of two main types: (1) detailed chemical structural descriptions, and (2) structure-to-feature relationship explanations covering both chemical and biological attributes. We excluded the multi-turn conversation type. 22 For each query x, we sample two responses from Mol-LLaMA using different decoding temperatures, 0.8 and 1.2. We use FLEX-Mol-LLaMA to compare the two and include the example in the DPO training set only if the response from temperature 0.8 receives higher score than the one from 1.2. Also, since there is position bias [63] in pairwise comparisons, we flip the order of the two responses in prompt and evaluate again, retaining only those pairs where the winning response remains consistent. Consequently, we construct 4,253 high-quality preference triplets (x, yw, yl). Fine-tuning Mol-LLaMA with this DPO dataset results in substantial accuracy boost on the downstream permeability prediction task. As shown in Figure 4 (right), the final model achieves up to 80.10% accuracy, surpassing prior models by large margin. This result confirms that FLEXMol-LLaMA provides not only reliable evaluation at inference but also effective supervision for preference-based training in specialized, underexplored domains."
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Scaling Law for FLEX-Judge To examine how model scale affects judge performance, we conduct an additional experiment using smaller LLM backbone, where results are found in Table 7. Specifically, we train FLEX-VL-3B by fine-tuning Qwen2.5-VL-3B on our curated seed dataset. Despite its significantly smaller size, FLEX-VL-3B also demonstrates reasoning capabilities and generalizes across modalities. However, it consistently underperforms compared to our base 7B model, indicating that larger-scale LLM-based models are better equipped to internalize reasoning patterns and serve as more reliable judges. These results suggest that while reasoning-guided supervision is effective even at smaller scales, model capacity remains an important factor in achieving high-quality, generalizable judgments. Meanwhile, despite its slightly lower performance compared to the 7B model, FLEX-VL-3B shows comparable performance to LLaVA-1.6-34B on MLLM-as-a-Judge and both Qwen2.5-VL-7B and LLaVA-NeXT on GenAI-Bench. Table 7: Comparison of FLEX-Judge performance across different MLLM sizes. : results from Table 1. : results from Table 2. : results from Table 3. Model Size LLaVA-1.6 34B Qwen2-VL 72B LLaVA-NeXT Unk. Qwen2.5-VL 7B FLEX-VL 3B 7B MLLM-as-a-Judge VL-Reward GenAI-Bench Score () w. Tie () w.o. Tie () Batch () General () Hallu. () Reason. () Image () Edition () Video () 0.184 - - - 0.176 0.332 0.460 - - - 0.493 0.538 0.648 - - - 0.650 0. 0.501 - - - 0.478 0.426 - 38.1 - - 46.39 46.11 - 32.0 - - 36.85 43. - 61.0 - - 58.08 62.87 - - 22.65 31.93 36.71 43.32 - - 25.35 38.63 33.62 47. - - 21.70 37.61 42.93 44.78 C.2 Reliability of FLEX-Judge. Length Bias. In Chen et al. [11], models such as GPT-4V [1] and Gemini [59] tend to favor longer answers over concise yet correct ones, exhibiting phenomenon known as verbosity bias [85]. In contrast, our FLEX-Omni-7B and FLEX-VL-7B demonstrate length preferences that are more consistent with human evaluators, unlike previous judge models reported in Chen et al. [11]. As illustrated in Figure 11, our models do not systematically prefer lengthy answers, but instead align well with human judgments regardless of response lengths. This suggests that FLEX-Judge can serve as reliable judge. Position bias. Models as judges consistently favor answers in specific positions, often influenced by training data that typically place correct responses at the beginning or end of prompts [85]. We observed similar behavior in our FLEX-Judge during pairwise comparison evaluations. In Figure 12, we examine the behavior of our judge models against human preferences. While human evaluators show less bias towards the first or second response and frequently opt for the Tie option when appropriate, our judge models, particularly FLEX-Omni-7B, tend to favor the first response more often. Additionally, our models are less likely to output Tie judgments, instead preferring one over another. As detailed in Table 6, we address the positional bias via randomly changing the response orders. (a) FLEX-Omni-7B vs. Human (b) FLEX-VL-7B vs. Human Figure 11: Examination on length bias of FLEX-Omni-7B and FLEX-VL-7B compared to human evaluators in pairwise comparisons (excluding ties) on the MLLM-as-a-Judge benchmark [11]. (a) Image Understanding (b) Image Edition (c) Video Generation Figure 12: Examination on position bias of FLEX-Omni-7B and FLEX-VL-7B compared to human evaluators in pairwise comparisons (including ties). Win indicates that the model preferred the first response, and Win indicates preference for the second response. C.3 Text Judgment Performance We also evaluate FLEX-Judge on text-only assessment tasks, with results shown in Table 8. Interestingly, both FLEX-Omni-7B and FLEX-VL-7B outperform the base judge model, JudgeLRM-7B, in terms of judgment accuracydespite being trained on the JudgeLRMs response data. This suggests that our high-quality dataset curation (as discussed in Section 2.2) may lead to stronger textual reasoning performance. While increasing the size of the training set could further improve test-time judgment accuracy, our primary focus is on multimodal generalization. Moreover, as discussed in Section 2.2, we observe catastrophic forgetting on the non-textual modalities when training with excessive text data. To balance effectiveness and modality retention, we limit the training set to 1K-sized text corpus throughout our experiments. Table 8: Comparison of (M)LLM evaluator performance on JudgeLM [86] and PandaLM [67]. : results from Chen et al. [12]. : re-implemented results. Model GPT-3.5 GPT-4 PandaLM-7B Auto-J-13B JudgeLM-33B Qwen2.5-7B-Instruct JudgeLRM-7B JudgeLRM-7B FLEX-Omni-7B FLEX-VL-7B JudgeLM (GPT-4o as Ground-Truth) PandaLM (Human as Ground-Truth) Agreement Precision Recall F1 Agreement Precision Recall F1 52.85 - 39.41 58.14 82.64 78.28 84.73 83.64 86.78 86.65 62.96 66.47 59.26 - 75.18 63.96 78.28 76. 76.24 76.84 61.95 66.20 57.28 - 69.30 61.95 74.90 71.41 73.38 72.15 63.59 68.15 59.23 - 74.93 67.61 75.74 71. 71.15 70.06 58.20 61.80 54.56 - 69.73 59.81 75.05 71.16 71.74 70.74 73.83 - 68.61 74.86 89.03 76.85 83.74 82. 84.12 84.21 70.70 - 40.75 61.65 80.97 78.71 85.84 84.86 80.95 82.65 52.80 - 38.82 57.53 84.76 77.85 83.65 82. 93.51 90."
        },
        {
            "title": "D Qualitative Examples",
            "content": "We provide qualitative examples of FLEX-Judges reasoning and judgments across different modalities. These examples illustrate how the model understands, compares, and scores AI-generated outputs as well as multimodal inputs using structured textual reasoning. D.1 Image Understanding and Generation Tasks We showcase examples of the image understanding and generation tasks with vision-language benchmarks. All judgments are made by FLEX-VL-7B. Figure 13 illustrates an image understanding task, involving OCR of Hindi phrase. Both assistants output similar-looking responses, but FLEX-Judge correctly recognizes the image content by referring to an unexpected election rather than financial crisis. It assigns significantly higher score to Assistant 2, whose response accurately reflects the OCR content. Figure 14 presents quality assessment task of generated images. Both assistants generate images, and FLEX-judge evaluates them based on clarity, focus, and presence of artifacts. Assistant 1 receives Figure 13: Reasoning process of FLEX-Judge on the OCR task (VL-RewardBench). Figure 14: Reasoning process of FLEX-Judge on the image quality assessment task (MJ-Bench). 25 Figure 15: Reasoning process of FLEX-Judge on the image editing task (GenAI-Bench). higher score due to its clear and well-focused image with no visible artifacts, whereas Assistant 2s image shows blurs, which could be due to movement or low-quality camera. Figure 15 shows an image editing task, where the judge assesses whether the edited image by each assistant aligns with the revised prompt. FLEX-Judge identifies that Assistant Bs output exhibits overediting, detracting from the naturalness and realism, and thus assigns it lower score. These examples highlight the FLEX-Judges ability to explain its preferences based on semantic accuracy, relevance, and consistency with visual content, even in fine-grained evaluation scenarios. D.2 Video and Audio Tasks Since we cannot include video and audio content directly into the paper, we provide full qualitative examplesincluding input prompts, AI responses, and FLEX-Judges reasoningfor video and audio evaluation tasks at the following anonymized project page.4 D.3 Molecular Tasks Figure 16 and Figure 17 illustrate the reasoning and judgment outputs of FLEX-Mol-LLaMA, our molecular judge model. Figure 16 presents two examples from the PAMPA prediction task, the primary evaluation task for Mol-LLaMA. These samples were used in the best-of-N sampling experiment (Section 4). We display both low-scoring and high-scoring response examples, along with FLEX-Mol-LLaMAs reasoning, to highlight its ability to distinguish response quality based on molecular analysis. Figure 17 shows pairwise judgment example from Mol-LLaMAs instruction tuning set, which includes prompts about biological property prediction. Here, we sample two responses from Mol-LLaMA and use FLEX-Mol-LLaMA to evaluate and score both. These pairwise preferences are then used to construct training triplets for DPO, guiding further alignment of Mol-LLaMA with high-quality response patterns. 4https://flex-judge.github.io/ 26 Figure 16: Reasoning process of FLEX-Mol-LLaMA judge on the PAMPA task."
        },
        {
            "title": "E Limitations",
            "content": "We have demonstrated that textual reasoning alone can effectively train multimodal judge models that generalize across modalities. While training only the LLM part is an innovative idea, it consists of pitfall: the underlying LLM must possess (or at least can learn) sufficient reasoning capability. Our method is efficient and broadly applicable, but it implicitly requires that the backbone model be capable of generating coherent, structured reasoning. We observed this limitation when attempting to extend our approach to 3D-LLM [25], which can process and encode 3D point clouds using Flan-T5-XL [17] as its language backbone. Despite the models 3D understanding, its limited context window (512 tokens) and lack of strong reasoning pretraining made it unsuitable for generating high-quality reasoning data. As result, we were unable to successfully train FLEX-3D-LLM judge using our framework. This highlights key constraint of our approach: it is less effective when applied to MLLMs built on weak or constrained LLMs that lack the capacity for textual reasoning. While FLEX-Judge naturally inherits the capabilities of its MLLM backbone, this is an inherent assumption shared by all LLM-as-a-Judge paradigms. Our key contribution lies not in developing better backbone but in demonstrating that minimal textual supervision can yield strong cross-modal evaluation with fixed MLLM, eliminating the need for expensive modality-specific training. 27 Figure 17: Reasoning process of FLEX-Mol-LLaMA, judging the preference of two Mol-LLaMA responses on biological attributes prediction data. Here, the response 1 is preferred (score 9) against the response 2 (score 4)."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}