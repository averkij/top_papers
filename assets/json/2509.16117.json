{
    "paper_title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
    "authors": [
        "Kaiwen Zheng",
        "Huayu Chen",
        "Haotian Ye",
        "Haoxiang Wang",
        "Qinsheng Zhang",
        "Kai Jiang",
        "Hang Su",
        "Stefano Ermon",
        "Jun Zhu",
        "Ming-Yu Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 1 1 6 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DIFFUSIONNFT: ONLINE DIFFUSION REINFORCEMENT WITH FORWARD PROCESS Kaiwen Zheng1,2, Huayu Chen1,2, Haotian Ye2,3 Haoxiang Wang2 Qinsheng Zhang2 Kai Jiang1 Hang Su1 Stefano Ermon3 Jun Zhu1, Ming-Yu Liu2 Equal Contribution Corresponding Author 1Tsinghua University 2NVIDIA 3Stanford University https://research.nvidia.com/labs/dir/DiffusionNFT"
        },
        {
            "title": "ABSTRACT",
            "content": "Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forwardreverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to 25 more efficient than FlowGRPO in head-to-head comparisons, while being CFGfree. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested. (a) (b) Figure 1: Performance of DiffusionNFT. (a) Head-to-head comparison with FlowGRPO on the GenEval task. (b) By employing multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested, while being fully CFG-free."
        },
        {
            "title": "INTRODUCTION",
            "content": "Online Reinforcement Learning (RL) has been pivotal in the post-training of LLMs, driving recent advances in LLMs alignment and reasoning abilities (Achiam et al., 2023; Guo et al., 2025). However, replicating similar success for diffusion models in visual generation is not straightforward."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Comparison between Forward-Process RL (NFT) and Reverse-Process RL (GRPO). NFT allows using any solvers and does not require storing the whole sampling trajectory for optimization. Policy Gradient algorithms assume that model likelihoods are exactly computable. This assumption holds for autoregressive models, but is inherently violated by diffusion models, where likelihoods can only be approximated via costly probabilistic ODE or variational bounds of SDE (Song et al., 2021). Recent works circumvent this barrier by discretizing the reverse sampling process, reframing diffusion generation as multi-step decision-making problem (Black et al., 2023). This makes transitions between adjacent steps tractable Gaussians, enabling direct application of existing RL algorithms like GRPO to the diffusion domain (Xue et al., 2025; Liu et al., 2025). Despite promising efforts made, we argue that GRPO-style diffusion reinforcement still faces fundamental limitations: (1) Forward inconsistency. Focusing solely on the reverse sampling process breaks adherence to the forward diffusion process, risking the model degenerating into cascaded Gaussians. (2) Solver restriction. The training loss is entangled with first-order SDE samplers, precluding the full utilization of ODE or high-order solvers that are default to flow models and advantageous for generation efficiency. (3) Complicated CFG integration. Diffusion models heavily rely on Classifier-Free Guidance (CFG) (Ho & Salimans, 2022), which requires training both conditional and unconditional models. Current RL practices directly incorporate CFG in post-training, leading to complicated and inefficient two-model optimization scheme. There is single forward (noising) but various reverse (denoising) processes for diffusion policy. Can diffusion reinforcement be performed on the forward process instead of the reverse? We propose new online RL paradigm: Diffusion Negative-aware FineTuning (DiffusionNFT). DiffusionNFT is not built on the Policy Gradient framework. Instead, it performs policy optimization directly on the forward diffusion process through the flow matching objective. Intuitively, DiffusionNFT splits generated samples into positive and negative subsets and defines contrastive improvement direction between the two policies learned on them. Moreover, unlike conventional guidance techniques that modify the sampling process, DiffusionNFT adopts an implicit parameterization technique that allows integrating reinforcement guidance directly into the optimized policy. The forward-process RL formulation provides several practical benefits (Figure 2). First, DiffusionNFT allows data collection with arbitrary black-box solvers, rather than just first-order SDE samplers. Second, it eliminates the need to store entire sampling trajectories, requiring only clean images for policy optimization. Third, it is fully compatible with standard diffusion training, requiring minimal modifications to existing codebases. Finally, it is native off-policy algorithm, naturally allowing decoupled training and sampling policies without importance sampling. We evaluate DiffusionNFT by post-training SD3.5-Medium (Esser et al., 2024) on multiple reward models. The entire training process deliberately operates in CFG-free setting. Although this results in significantly lower initialization performance, we find DiffusionNFT substantially improves performance across both in-domain and out-of-domain rewards, rapidly outperforming CFG and the GRPO baseline. We also conduct head-to-head comparisons against FlowGRPO in single-reward settings. Across four tasks tested, DiffusionNFT consistently exhibits 3 to 25 efficiency and achieves better final scores. For instance, it improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves only 0.95 with over 5k steps and additional CFG employment. Our main contribution is DiffusionNFT, direct RL alternative to conventional Policy Gradient methods, introducing the Negative-aware FineTuning (NFT) paradigm (Chen et al., 2025c) into the diffusion domain. Grounded in supervised learning foundation, we believe this paradigm offers valid path toward general, unified, and native off-policy RL recipe across various modalities."
        },
        {
            "title": "2.1 DIFFUSION AND FLOW MODELS",
            "content": "Diffusion models (Ho et al., 2020; Song et al., 2020b) learn continuous data distributions by gradually perturbing clean data x0 π0 = pdata with Gaussian noise according to forward process. Then, data can be generated by learning to reverse this process. The forward noising process admits closed-form transition kernel πt0(xtx0) = (αtx0, σ2 with specific noise schedule αt, σt, enabling reparameterization as I) xt = αtx0 + σtϵ, ϵ (0, I). One way to learn diffusion models is to adopt the velocity parameterization vθ(xt, t) (Zheng et al., 2023b), which predicts the tangent of the trajectory, trained by minimizing Et,x0π0,ϵN (0,I)[w(t)vθ(xt, t) v2 2], where the target velocity is defined by the schedules time derivatives as = αtx0 + σtϵ under the notation ft := dft/dt, and w(t) is some weighting function. Reverse sampling typically follows the ODE form (Song et al., 2020b) of the diffusion model, which is reduced to dxt dt = vθ(xt, t) using vθ. This formulation is known as flow matching (Lipman et al., 2022), where simple Euler discretization serves as an effective ODE solver, equivalent to DDIM (Song et al., 2020a). (1) Rectified flow (Liu et al., 2022) can be considered as simplified special case of the above-discussed diffusion models, where αt = 1 t, σt = t, which simplifies the velocity target to = ϵ x0. 2.2 POLICY GRADIENT ALGORITHMS FOR DIFFUSION MODELS In order to apply Policy Gradient algorithms such as PPO (Schulman et al., 2017) or GRPO (Shao et al., 2024) to diffusion models, recent works (Black et al., 2023; Fan et al., 2023; Liu et al., 2025; Xue et al., 2025) formulate the diffusion sampling as multi-step Markov Decision Process (MDP). This can be achieved by discretizing the reverse sampling process of diffusion models. While flow models naturally admit simple and efficient sampling through ODE, the lack of stochasticity hinders the application of GRPO. FlowGRPO (Liu et al., 2025) addresses this by using the SDE form (Song et al., 2020b) under the velocity parameterization vθ (see Appendix B.1): (cid:104) dxt = vθ(xt, t) + g2 2t (cid:0)xt + (1 t)vθ(xt, t)(cid:1)(cid:105) dt + gtdwt (2) where gt = 1t controls the level of injected stochasticity. Discretizing it with Euler yields (cid:113) πθ(xtt xt) = (cid:16) xt + (cid:104) vθ(xt, t) + (cid:105) (xt + (1 t)vθ(xt, t)) g2 2t t, g2 (cid:17) . This makes transition kernels between adjacent steps likelihood tractable Gaussians, enabling the direct application of existing policy gradient algorithms, such as GRPO."
        },
        {
            "title": "3 DIFFUSION REINFORCEMENT VIA NEGATIVE-AWARE FINETUNING",
            "content": "3.1 PROBLEM SETUP Online RL. Consider pretrained diffusion policy πold and prompt datasets {c}. At each iteration, we sample images x1:K for prompt c, and then evaluate each image with scalar reward function [0, 1], representing the its optimality probability r(x0, c) := p(o = 1x0, c) (Levine, 2018). 0 Collected data can be randomly split into two imaginary subsets. An image x0 will have probability of falling into the positive dataset D+ and otherwise the negative dataset D. Given infinite samples, the underlying distributions of these two subsets are respectively π+(x0c) := πold(x0o = 1, c) = p(o = 1x0, c)πold(x0c) pπold(o = 1c) = r(x0, c) pπold(o = 1c) πold(x0c)"
        },
        {
            "title": "Preprint",
            "content": "π(x0c) := πold(x0o = 0, c) = p(o = 0x0, c)πold(x0c) pπold (o = 0c) = 1 r(x0, c) 1 pπold (o = 1c) πold(x0c) RL requires performing policy improvement at each iteration. The optimized policy π satisfies Eπ(c)r(x0, c) > Eπold(c)r(x0, c) (denoted as π πold) It is easy to prove that π+ πold π constantly Policy Improvement on Positive Data. holds, thus straightforward improvement of πold can be π = π+. To achieve this, previous work (Lee et al., 2023) performs diffusion training solely on D+, known as Rejection FineTuning (RFT). Despite the simplicity, RFT cannot effectively leverage negative data in (Chen et al., 2025c). Reinforcement Guidance. We posit that negative feedback is crucial to policy improvement. Rather than treating π+ as an optimization point, we leverage both negative and positive data to derive an improvement direction Rn. The training target is defined as v(xt, c, t) := vold(xt, c, t) + 1 β (xt, c, t). (3) where is the velocity predictor of the diffusion model, β is hyperparameter. This definition formally resembles diffusion guidance such as Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). We term (xt, c, t) Rn reinforcement guidance, and 1 β guidance strength. In Section 3.2, we address two challenges: 1. What is an appropriate form of that enables policy improvement? 2. How to directly optimize vθ leveraging collected dataset D+ and D? 3.2 NEGATIVE-AWARE DIFFUSION REINFORCEMENT WITH FORWARD PROCESS In Eq. (3), corresponds to the distributional shift between an improved policy and the original policy. To formalize this, we first study the distributional difference between π+ πold π. Theorem 3.1 (Improvement Direction). Consider diffusion models v+, v, and vold for the policy triplet π+, π, and πold. The directional differences between these models are proportional: :=[1 α(xt)] [vold(xt, c, t) v(xt, c, t)] [v+(xt, c, t) vold(xt, c, t)]. (Reinforcement Guidance) = α(xt) (4) where 0 α(xt) 1 is scalar coefficient: π+ (xtc) πold (xtc) α(xt) := Eπold(x0c)r(x0, c) 1 Eq. (4) indicates an ideal guidance direction for improving over vold. With appropriate guidance strength, policy improvement can be guaranteed. For instance, let β = α(xt) in Eq. (3), we have v(xt, c, t) = vold(xt, c, t) + α(xt) (xt, c, t) = v+(xt, c, t), such that π = π+ πold holds. Figure 3 contains an illustration for the improvement direction . Having defined valid optimization target with Eq. (3) and 4, we now introduce training objective that directly optimizes vθ towards v: Theorem 3.2 (Policy Optimization). Consider the training objective: 2 + (1 r)v where v+ θ (xt, c, t) := (1 β)vold(xt, c, t) + βvθ(xt, c, t), θ (xt, c, t) := (1 + β)vold(xt, c, t) βvθ(xt, c, t). and L(θ) = Ec,πold(x0c),t rv+ Figure 3: Improvement Direction. θ (xt, c, t) v2 2, θ (xt, c, t) v2 (Implicit positive policy) (Implicit negative policy) (5) Given unlimited data and model capacity, the optimal solution of Eq. (5) satisfies vθ (xt, c, t) = vold(xt, c, t) + 2 β (xt, c, t). (6)"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: DiffusionNFT jointly optimizes two dual diffusion objectives, on both positive (r = 1) and negative (r = 0) branches. Rather than training two independent models v+ θ , it adopts an implicit parameterization technique that directly optimizes single target policy vθ. θ and Theorem 3.2 presents new off-policy RL paradigm (Figure 4). Instead of applying Policy Gradient, it adopts supervised learning (SL) objectives, but additionally trains on online negative data D. This renders the algorithm highly versatile, compatible with existing SL methods. We term our method Diffusion Negative-aware FineTuning (DiffusionNFT), highlighting its negative-aware SL nature and conceptual similarity to parallel algorithm NFT in language models (Chen et al., 2025c). Below, we discuss several distinctive advantages of DiffusionNFT. 1. Forward Consistency. In contrast to policy gradient methods (e.g., FlowGRPO), which formulated RL on the reverse diffusion process, DiffusionNFT defines typical diffusion loss on the forward process. This preserves what we term forward consistencythe adherence of the diffusion models underlying probability density to the Fokker-Planck equation (Øksendal, 2003; Song et al., 2020b), ensuring that the learned model corresponds to valid forward process rather than degenerating into cascaded Gaussians. 2. Solver Flexibility. DiffusionNFT fully decouples policy training and data sampling. This enables the full utilization of any black-box solvers throughout sampling, rather than relying on first-order SDE samplers. It also eliminates the need to store the entire sampling trajectory during data collection, requiring only clean images with their associated rewards for training. 3. Implicit Guidance Integration. Intuitively, DiffusionNFT defines guidance direction and apply such guidance to the old policy vold (Eq. (6)). However, instead of learning separate guidance model θ and employing guided sampling, it adopts an implicit parameterization technique that enables direct integration of reinforcement guidance into the learned policy. This technique, inspired by recent advances in guidance-free training (Chen et al., 2025a), allows us to perform RL continuously on single policy model, which is crucial to online reinforcement. 4. Likelihood-Free Formulation. Previous diffusion RL methods are fundamentally constrained by their reliance on likelihood approximation. Whether approximating the marginal data likelihood with variational bounds and applying Jensens inequality to reduce loss computation cost (Wallace et al., 2024), or discretizing the reverse process to estimate sequence likelihood (Black et al., 2023), they inevitably introduce systematic estimation bias into diffusion post-training. In contrast, DiffusionNFT is inherently likelihood-free, bypassing such compromises. 3.3 PRACTICAL IMPLEMENTATION We provide DiffusionNFT pseudo code in Algorithm 1. Below, we elaborate on key design choices. Optimality Reward. In most visual reinforcement settings, rewards manifest as unconstrained continuous scalars rather than binary optimality signals. Motivated by existing GRPO practices (Shao et al., 2024; Liu et al., 2025; Xue et al., 2025), we first transform the raw reward rraw into [0, 1] which represents the optimality probability: r(x0, c) := 1 2 + 1 2 clip (cid:20) rraw(x0, c) Eπold(c)rraw(x0, c) Zc (cid:21) , 1, 1 . Zc > 0 is some normalizing factor, which could take the form of global reward std. We sample images for each prompt during data collection, so the average reward Eπold(c)rraw(x0, c) for each prompt can be estimated."
        },
        {
            "title": "Preprint",
            "content": "// Rollout Step, Data Collection 0 for each sampled prompt do Collect clean images x1:K , and evaluate their rewards {rraw}1:K . Normalize raw rewards in group: rnorm := rraw mean({rraw}1:K ). Define optimality probability = 0.5 + 0.5 clip{rnorm/Zc, 1, 1}. {c, x1:K Algorithm 1 Diffusion Negative-aware FineTuning (DiffusionNFT) Require: Pretrained diffusion policy vref, raw reward function rraw() R, prompt dataset {c}. Initialize: Data collection policy vold vref, training policy vθ vref, data buffer . 1: for each iteration do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for Output: vθ Forward diffusion process: xt = αtx0 + σtϵ; = αtx0 + σtϵ. Implicit positive velocity: v+ Implicit negative velocity: θ θ λθ end for Update data collection policy θold ηiθold + (1 ηi)θ, and clear buffer . θ (xt, c, t) := (1 β)vold(xt, c, t) + βvθ(xt, c, t). θ (xt, c, t) := (1 + β)vold(xt, c, t) βvθ(xt, c, t). end for for each mini batch {c, x0, r} do θ (xt, c, t) v2 θ (xt, c, t) v2 2 + (1 r)v , r1:K [0, 1]}. (cid:2)rv+ (Eq. (5)) (cid:3). 2 0 // Gradient Step, Policy Optimization // Online Update Soft Update of Sampling Policy. The off-policy nature of DiffusionNFT decouples the sampling policy πold from the training policy πθ. This obviates the need for hard update (πold πθ) after each iteration. Instead, we leverage this property to employ soft EMA update: θold ηiθold + (1 ηi)θ where is the iteration number. The parameter η governs trade-off between learning speed and stability. strictly on-policy scheme (η = 0) yields rapid initial progress but is prone to severe instability, leading to catastrophic collapse. Conversely, nearly offline approach (η 1) is robustly stable but suffers from impractically slow convergence (Figure 8). Adaptive Loss Weighting. Typical diffusion loss includes time-dependent weighting w(t) (Eq. (1)). Instead of manual tuning, we adopt an adaptive weighting scheme. The velocity predictor vθ can be equivalently transformed into x0 predictor, denoted as xθ (e.g., xθ = xt tvθ under rectified flow schedule). We replace the weighting with form of self-normalized x0 regression: xθ(xt, c, t) x02 2 sg(mean(abs(xθ(xt, c, t) x0))) where sg is the stop-gradient operator. We find it offers stable training (Figure 9). w(t)vθ(xt, c, t) 2 CFG-Free Optimization. Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) is default technique to enhance generation quality at inference time, yet it complicates post-training and reduces efficiency. Conceptually, we interpret CFG as an offline form of reinforcement guidance (Eq. (4)), where conditional and unconditional models correspond to positive and negative signals. With this understanding, we discard CFG in our algorithm design. The policy is initialized solely by the conditional model. Despite this seemingly poor initialization, we observe that performance surges and quickly surpasses the CFG baseline (Figure 1). This suggests that the functionality of CFG can be effectively learned or substituted through RL post-training, echoing recent studies that achieve strong performance without CFG through post-training (Chen et al., 2025b;a; Zheng et al., 2025)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We demonstrate the potential of DiffusionNFT through three perspectives: (1) multi-reward joint training for strong CFG-free performance, (2) head-to-head comparison with FlowGRPO on single rewards, and (3) ablation studies on key design choices. 4.1 EXPERIMENTAL SETUP Our experiments are based on SD3.5-Medium (Esser et al., 2024) at 512512 resolution, with most settings aligned with FlowGRPO (Liu et al., 2025)."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Evaluation Results. Gray-colored: In-domain reward. Evaluated on official checkpoints. Evaluated under 10241024 resolution. Bold: best; Underline: second best. Model #Iter Rule-Based Model-Based GenEval OCR PickScore ClipScore HPSv2.1 Aesthetic ImgRwd UniRwd SD-XL SD3.5-L FLUX.1-Dev + FlowGRPO SD3.5-M (w/o CFG) + CFG >5k 2k 4k 1.7k + Ours 0.55 0.71 0.66 0.24 0.63 0.95 0.66 0.54 0.94 0.14 0.68 0.59 0.12 0.59 0.66 0.92 0.68 0.91 22.42 22.91 22.84 20.51 22.34 22.51 22.41 23.50 23. 0.287 0.289 0.295 0.237 0.285 0.293 0.290 0.280 0.293 0.280 0.288 0.274 0.204 0.279 0.274 0.280 0.316 0.331 5.60 5.50 5.71 5.13 5.36 5.32 5.32 5.90 6. 0.76 0.96 0.96 -0.58 0.85 1.06 0.95 1.29 1.49 2.93 3.25 3.27 2.02 3.03 3.18 3.15 3.37 3.49 Figure 5: Qualitative Comparison. The prompts are taken from GenEval, OCR and DrawBench respectively, where we compare the corresponding FlowGRPO model with our model. Reward Models. (1) Rule-based rewards, including GenEval (Ghosh et al., 2023) for compositional image generation and OCR for visual text rendering, where the partial reward assignment strategies follow FlowGRPO. (2) Model-based rewards, including PickScore (Kirstain et al., 2023), ClipScore (Hessel et al., 2021), HPSv2.1 (Wu et al., 2023), Aesthetics (Schuhmann, 2022), ImageReward (Xu et al., 2023) and UnifiedReward (Wang et al., 2025), which measure image quality, image-text alignment and human preference. Prompt Datasets. For GenEval and OCR, we use the corresponding training and test sets from FlowGRPO. For other rewards, we train on Pick-a-Pic (Kirstain et al., 2023) and evaluate on DrawBench (Saharia et al., 2022). Training and Evaluation. We finetune with LoRA (α = 64, = 32). Each epoch consists of 48 groups with group size = 24. We use 10 rollout sampling steps for head-to-head comparison and ablation studies, and 40 steps for best visual quality in multi-reward training. Evaluation is performed with 40-step first-order ODE sampler. Additional details are provided in Appendix C. 4.2 MULTI-REWARD JOINT TRAINING We first assess DiffusionNFTs effectiveness in comprehensively enhancing the base model. Starting from the CFG-free SD3.5-M (2.5B parameters), we jointly optimize five rewards: GenEval, OCR,"
        },
        {
            "title": "Preprint",
            "content": "(a) (b) (c) Figure 6: Head-to-head comparison between DiffusionNFT with FlowGRPO on single rewards. PickScore, ClipScore, and HPSv2.1. Since the rewards are based on different prompts, we first train on Pick-a-Pic with model-based rewards to strengthen alignment and human preference, followed by rule-based rewards (GenEval, OCR). Out-of-domain evaluation is conducted on Aesthetics, ImageReward, and UnifiedReward. As shown in Table 1, our final CFG-free model not only surpasses CFG and matches FlowGRPO (fitted only single rewards) on both in-domain and out-of-domain metrics, but also outperforms CFGbased larger models such as SD3.5-L (8B parameters) and FLUX.1-Dev (12B parameters) (Labs, 2024). Qualitative comparison in Figure 5 demonstrates the superior visual quality of our method. 4.3 HEAD-TO-HEAD COMPARISON We conduct head-to-head comparisons with FlowGRPO on single training rewards. As shown in Figure 1(a) and Figure 6, our method is 3 to 25 more efficient in terms of wall-clock time, achieving GenEval score of 0.98 within only 1k iterations. This demonstrates that CFG-free models can rapidly adapt to specific reward environments under our framework. 4.4 ABLATION STUDIES (a) (b) Figure 7: Different diffusion samplers for data collection. Figure 8: Soft-update strategies. (a) (b) Figure 9: Different time-dependent weighting strategies. Figure 10: Choices of strength β. We analyze the impact of our core design choices: Negative Loss. The negative-aware component is crucial in DiffusionNFT. Without the negative policy loss on θ , we find rewards collapse almost instantly during online training, highlighting the"
        },
        {
            "title": "Preprint",
            "content": "essential role of negative signals in diffusion RL. This phenomenon is divergent from observations in LLMs, where RFT remains strong baseline (Xiong et al., 2025; Chen et al., 2025c). Diffusion Sampler. Online samples in DiffusionNFT are used both for reward evaluation and as training data, making quality critical. Figure 7 shows that ODE samplers outperform SDE ones, especially on PickScore, which is noise-sensitive. Second-order ODE slightly outperforms firstorder on GenEval, while being comparable on PickScore. Adaptive Weighting. We find stability improves when the flow-matching loss is given higher weight at larger t, whereas inverse strategies (e.g., w(t) = 1 t) lead to collapse (Figure 9). Our adaptive schedule consistently matches or exceeds heuristic choices. Soft Update. We compare different ηi schedules for the soft update in Figure 8. Fully on-policy (ηi = 0) accelerates early progress but destabilizes training, while overly off-policy (η = 0.9) slows convergence. We find that starting with small η and gradually increasing it to larger value in later stages strikes an effective balance between convergence speed and training stability. Guidance Strength. As shown in Figure 10, the guidance parameter β also governs trade-off between stability and convergence speed. We find that β near 1 performs stably and select β as 1 or 0.1 (for faster reward increase) in practice."
        },
        {
            "title": "5 RELATED WORK",
            "content": "The transition of RL algorithms from discrete autoregressive (AR) to continuous diffusion models poses central challenge: the inherent difficulty of diffusion models for computing exact model likelihoods (Song et al., 2021), which are nonetheless crucial for RL (Chen et al., 2023; Liu et al., 2025). To address this challenge, existing efforts include: Likelihood-free methods: (1) Reward Backpropagation (Xu et al., 2023; Prabhudesai et al., 2023; Clark et al., 2023; Prabhudesai et al., 2024) proves highly effective, yet is limited to differentiable rewards and can only tune low-noise timesteps due to memory costs and gradient explosion when unrolling long denoising chains. (2) Reward-Weighted Regression (RWR) (Lee et al., 2023) is an offline finetuning method but lacks negative policy objective to penalize low-reward generations. (3) Policy Guidance. This includes energy guidance (Janner et al., 2022; Lu et al., 2023) and CFGstyle guidance (Frans et al., 2025; Jin et al., 2025). These methods all require combining multiple models for guided sampling, thus complicating online optimization. (4) Score-based RL. These methods try to perform RL directly on the score rather than the likelihood field (Zhu et al., 2025). Likelihood-based methods: (1) Diffusion-DPO (Wallace et al., 2024; Yang et al., 2024; Liang et al., 2024; Yuan et al., 2024; Li et al., 2025a) adapts DPO to diffusion for paired human preference data but requires additional likelihood and loss approximations compared to AR. (2) Policy gradient methods, starting from PPO style (Black et al., 2023; Fan et al., 2023), decompose trajectory likelihoods step by step without considering forward consistency. Recent GRPO extensions (Liu et al., 2025; Xue et al., 2025) prove effective and scalable for diffusion RL, but they couple the training loss with SDE samplers and face efficiency bottlenecks. MixGRPO (Li et al., 2025b) improves efficiency by mixing SDE and ODE, while issues of coupling and forward inconsistency remain."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), new paradigm for online reinforcement learning of diffusion models that directly operates on the forward process. By formulating policy improvement as contrast between positive and negative generations, DiffusionNFT integrates reinforcement signals seamlessly into the standard diffusion objective, eliminating the reliance on likelihood estimation and SDE-based reverse process. Empirically, DiffusionNFT demonstrates strong and efficient reward optimization, achieving up to 25 higher efficiency than FlowGRPO while producing single omni-model that outperforms CFG baselines across diverse in-domain and out-of-domain rewards. We believe this work represents step toward unifying supervised and reinforcement learning in diffusion, and highlights the forward process as promising foundation for scalable, efficient, and theoretically principled diffusion RL."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Cheng Lu, Hanzi Mao, Zekun Hao, Tao Yang, Zhanhao Liang, Shuhuai Ren, Tenglong Ao, Xintao Wang, Haoqi Fan, Jiajun Liang, Yuji Wang, and Hongzhou Zhu for the valuable discussion."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning In The Eleventh International Conference on via high-fidelity generative behavior modeling. Learning Representations, 2023. Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, and Jun Zhu. Visual generation without guidance. Forty-second international conference on machine learning, 2025a. Huayu Chen, Hang Su, Peize Sun, and Jun Zhu. Toward guidance-free ar visual generation via condition contrastive alignment. In ICLR, 2025b. Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, and Haoxiang Wang. Bridging supervised learning and reinforcement learning in math reasoning. arXiv preprint arXiv:2505.18116, 2025c. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Kevin Frans, Seohong Park, Pieter Abbeel, and Sergey Levine. Diffusion guidance is controllable policy improvement operator. arXiv preprint arXiv:2505.23458, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Martin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, Hatem Hajri, Nader Masmoudi, et al. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. Advances in Neural Information Processing Systems, 36:6806168120, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020."
        },
        {
            "title": "Preprint",
            "content": "Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19:209286, 2010. Chin-Wei Huang, Jae Hyun Lim, and Aaron Courville. variational perspective on diffusionbased generative models and score matching. Advances in Neural Information Processing Systems, 34:2286322876, 2021. Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, and Xipeng Qiu. Inference-time alignment control for diffusion models with reinforcement learning guidance. arXiv preprint arXiv:2508.21016, 2025. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. Binxu Li, Minkai Xu, Meihua Dang, and Stefano Ermon. Divergence minimization preference optimization for diffusion model alignment. arXiv preprint arXiv:2507.07510, 2025a. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025b. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2(5):7, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. arXiv preprint arXiv:2304.12824, 2023. Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations: an introduction with applications, pp. 3850. Springer, 2003."
        },
        {
            "title": "Preprint",
            "content": "Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. 2023. Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022."
        },
        {
            "title": "Christoph",
            "content": "Schuhmann. laion-aesthetics/, 2022. Laion-aesthetics. https://laion.ai/blog/ John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of scorebased diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp. 14151428, 2021. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20962105, 2023. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. Advances in Neural Information Processing Systems, 37: 7336673398, 2024."
        },
        {
            "title": "Preprint",
            "content": "Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood In International Conference on Machine Learning, pp. 42363 estimation for diffusion odes. 42389. PMLR, 2023b. Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024. Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. In ICML, 2025. Huaisheng Zhu, Teng Xiao, and Vasant Honavar. Dspo: Direct score preference optimization for diffusion model alignment. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "A PROOF OF THEOREMS",
            "content": "Lemma A.1 (Distribution Split). Consider the distribution triplet π+, π, and πold, as defined in Section 3.1: π+(x0c) := πold(x0o = 1, c) = π(x0c) := πold(x0o = 0, c) = p(o = 1x0, c)πold(x0c) pπold(o = 1c) p(o = 0x0, c)πold(x0c) pπold (o = 0c) = = r(x0, c) pπold(o = 1c) 1 r(x0, c) 1 pπold (o = 1c) πold(x0c) (7) πold(x0c) (8) πold(x0c) is as linear combination between its positive split π+(x0c) and negative split π(x0c): πold(x0c) = pπold(o = 1c)π+(x0c) + [1 pπold(o = 1c)]π(x0c) (9) Proof. The result follows directly from Eq.(7) and Eq.(8). Lemma A.2 (Posterior Split). The diffusion posteriors for distribution triplet π+, π, and πold satisfy: πold(x0xt, c) = α(xt)π+(x0xt, c) + [1 α(xt)]π(x0xt, c) where α(xt) := π+ (xtc) πold (xtc) Eπold(x0c)r(x0, c) Proof. Leveraging Bayes Rule: πold(x0c) = (xtc)πold πold 0t(x0xt, c) π(xtx0) Replacing all distributions in Eq. (9) (Lemma A.1) we get (xtc)π+ π+ 0t(x0xt, c) (xtc)πold πold 0t(x0xt, c) π(xtx0) =pπold(o = 1c) π(xtx0) + [1 pπold(o = 1c)] (xtc)π π 0t(x0xt, c) π(xtx0) πold 0t(x0xt, c) =pπold(o = 1c) π+ (xtc) πold (xtc) π+ 0t(x0xt, c) + [1 pπold (o = 1c)] π (xtc) πold (xtc) π 0t(x0xt, c) Diffuse both sides of Eq. (9), we have (xtc) = pπold(o = 1c)π+ πold (xtc) + [1 pπold(o = 1c)]π (xtc) pπold(o = 1c) π+ (xtc) πold (xtc) + [1 pπold (o = 1c)] π (xtc) πold (xtc) = 1 pπold(o = 1c) = Eπold(x0c)r(x0, c) 0t(x0xt, c) = α(xt)π+ πold 0t(x0xt, c) + [1 α(xt)]π 0t(x0xt, c) Note that We have Theorem A.3 (Improvement Direction). Consider diffusion models v+, v, and vold for the distribution triplet π+, π, and πold. The directional differences between these models are parallel: :=[1 α(xt)] [vold(xt, c, t) v(xt, c, t)] [v+(xt, c, t) vold(xt, c, t)]. = α(xt) (Reinforcement Guidance) where 0 α(xt) 1 is scalar coefficient: π+ (xtc) πold (xtc) α(xt) := Eπold(x0c)r(x0, c)"
        },
        {
            "title": "Preprint",
            "content": "Proof. According to the relationship between the optimal velocity predictor and the posterior mean of x0 (i.e., the optimal x0 predictor) (Zheng et al., 2023b): vold(xt, c, t) = atxt + btEπold(x0xt,c)[x0] v+(xt, c, t) = atxt + btEπ+(x0xt,c)[x0] v(xt, c, t) = atxt + btEπ=(x0xt,c)[x0] where at = σt σt , bt = αt σtαt σt . Based on Lemma A.2 we have vold(xt, c, t) = α(xt)v+(xt, c, t) + [1 α(xt)]v(xt, c, t) Rearranging the equation, we complete the proof. Theorem A.4 (Reinforcement Guidance Optimization). Consider the training objective: θ (xt, c, t) v2 L(θ) = Ec,πold(x0c),t rv+ θ (xt, c, t) v2 2, 2 + (1 r)v where v+ θ (xt, c, t) := (1 β)vold(xt, c, t) + βvθ(xt, c, t), θ (xt, c, t) := (1 + β)vold(xt, c, t) βvθ(xt, c, t). and (Implicit positive policy) (Implicit negative policy) (10) Given unlimited data and model capacity, the optimal solution of Eq. (10) satisfies vθ (xt, c, t) = vold(xt, c, t) + 2 β (xt, c, t). Proof. L(θ) = 0t(x0xt,c) r(x0, c)v+ θ (xt, c, t) v2 2 + [1 r(x0, c)]v θ (xt, c, t) 2 c,t,πold (xtc)πold (xtc){E = c.t,πold + 0t(x0xt,c)[1 r(x0, c)]v πold 0t(x0xt,c)r(x0, c)v+ πold θ (xt, c, t) v2 θ (xt, c, t) v2 2} From Lemma A.1 we have r(x0, c)πold(x0c) = pπold(o = 1c)π+(x0c), therefore: r(x0, c)πold 0t(x0xt, c) = r(x0, c) πold(x0c)π(xtx0) πold (xtc) π+ (xtc) πold (xtc) π+ (xtc) πold (xtc) = pπold (o = 1c) π+(x0c)π(xtx0) π+ (xtc) = pπold(o = 1c) π+ 0t(x0xt, c) = α(xt)π+ 0t(x0xt, c) [1 r(x0, c)]πold 0t(x0xt, c) = [1 α(xt)]π 0t(x0xt, c) Similarly, Then, θ (xt, c, t) v2 2 L(θ) =E (xtc){α(xt)E c,t,πold + [1 α(xt)]E 0t(x0xt,c)v+ π+ 0t(x0xt,c)v π θ (xt, c, t) v2 2} 0t(x0xt,c)[v]2 π+ 2} + C1 2 =E (xtc){α(xt)v+ θ (xt, c, t) c,t,πold + [1 α(xt)]v =E (xtc){α(xt)v+ c,t,πold + [1 α(xt)]v θ (xt, c, t) 0t(x0xt,c)[v]2 π θ (xt, c, t) v+(xt, c, t)2 2} + C1 θ (xt, c, t) v(xt, c, t)2"
        },
        {
            "title": "Preprint",
            "content": "Combining Theorem A.3, we observe that v+ θ (xt, c, t) v+(xt, c, t) = (1 β)vold(xt, c, t) + βvθ(xt, c, t) v+(xt, c, t) = β[vθ vold 1 β α(xt) ] θ (xt, c, t) v(xt, c, t) = (1 + β)vold(xt, c, t) βvθ(xt, c, t) v(xt, c, t) = β[vθ vold 1 β 1 α(xt) ] Substituting these results into L(θ): L(θ) =E c,t,πold =β2E c,t,πold + [1 α(xt)]β2vθ vold 2 1 β (xtc){α(xt)β2vθ vold 1 β α(xt) 1 α(xt) 1 (xtc){α(xt)vθ (vold + β α(xt) 1 α(xt) 1 α(xt) β )2 1 β 2 2} + )2 2 2} + C1 + [1 α(xt)]vθ (vold + =β2Ec,t,πold(xtc)vθ α(xt)(vold + ) [1 α(xt)](vold + 1 β 1 α(xt) )2 2 + C2 =β2Ec,t,πold(xtc)vθ (vold + 2 β ) 2 + C2 from which it is obvious that the optimal θ satisfies vθ (xt, c, t) = vold(xt, c, t) + 2 β (xt, c, t)."
        },
        {
            "title": "B THEORETICAL DISCUSSIONS",
            "content": "B.1 FLOW SDE As flow models are special case of diffusion models under the rectified schedule αt = 1t, σt = t, the earliest results on diffusion SDEs (Song et al., 2020b) can be directly applied without difficulty. FlowGRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) derive the flow SDE with unexplained hyperparameters gt = 1t or additional complexity. We provide simpler and more principled perspective based solely on the diffusion model framework. (cid:113) To leverage the diffusion SDE formulation in Song et al. (2020b), we need to match its forward SDE dxt = (t)xtdt + g(t)dwt with the forward transition kernel xt = αtx0 + σtϵ. As noted in the first two versions of VDM (Kingma et al., 2021), (t), g(t) are related to αt, σt by (t) = log αt , g2(t) = dσ2 dt dt 2 log αt dt σ2 . Setting αt = 1 t, σt = t, we have 1 2t 1 1 (t) = g2(t) = , (11) for rectified flow. According to (Huang et al., 2021), the generalized reverse SDE takes the form: (cid:20) (t)xt dxt = 1 + λ2 2 (cid:21) g2(t)xt log πt(xt) dt + λtg(t)d wt (12) where λt [0, 1]. Equivalently, it amounts to introducing Langevin dynamics on top of the diffusion ODE, with λt = 0 corresponding to ODE, and λt = 1 corresponding to the maximum variance SDE in Song et al. (2020b). The score function sθ(xt, t) xt log πt(xt), noise predictor ϵθ(xt, t), data predictor xθ(xt, t) and velocity predictor vθ(xt, t) are interconvertible (Zheng et al., 2023b): xt σtϵθ(xt, t) αt ϵθ(xt, t) = σtsθ(xt, t), xθ(xt, t) = vθ(xt, t) = αtxθ(xt, t)+ σtϵθ(xt, t) , (13)"
        },
        {
            "title": "Preprint",
            "content": "Applying these relations to the rectified flow schedule, we can derive: sθ(xt, t) = xt + (1 t)vθ(xt, t) (14) Substituting Eq. (11) and Eq. (14) into Eq. (12), we have the diffusion SDE under rectified flow: dxt = (cid:20) (1 + λ2 )vθ(xt, t) + λ2 1 (cid:21) xt dt + λt (cid:114) 2t 1 dw (15) Therefore, the flow SDE in Eq. (2) is essentially conducting transformation gt = λt 1t from the interpolation parameter λt [0, 1] to the variance parameter gt. This also explains the choice gt = (cid:113) 2 corresponding to the maximum variance SDE. In comparison, DanceGRPO adopts fixed variance gt across timesteps, which is less effective on image models while more stable on video models. 2λt is scaled version of λt, with = 1t in FlowGRPO, where = (cid:113) 2t FlowGRPO and DanceGRPO directly take the Euler discretization of the flow SDE. In principle, there are more accurate ways, such as utilizing the idea of diffusion implicit models (Song et al., 2020a; Zheng et al., 2024), which is equivalent to the first-order discretization after applying exponential integrators (Hochbruck & Ostermann, 2010; Zhang & Chen, 2022; Gonzalez et al., 2023). Specifically, the sampling step from to < can be derived as: (cid:20) xs = (1 s) + (cid:113) s2 ρ2 (cid:21) (cid:20) xt (1 s)t (cid:113) s2 ρ2 (1 t) (cid:21) vθ(xt, t) + ρtϵ, ϵ (0, I) (cid:113) 1 s2(1t)2 (16) where ρt = ηts t2(1s)2 , and ηt [0, 1] interpolates between ODE and maximum variance SDE. Compared to the Euler discretization, the DDIM-style discretization avoids singularities at boundaries and is expected to reduce sampling errors. However, as the training loss of FlowGRPO couples with the sampler, and RL requires not only accurate reward signals but also large exploration space, we did not observe notable advantages by replacing the SDE sampler with DDIM. B.2 HIGH-ORDER FLOW ODE SAMPLER We implement the 2nd-order ODE sampler for flow models based on the DPM-Solver series (Lu et al., 2022a;b; Zheng et al., 2023a), which uses the multistep method and half the log signal-to-noise ratio (SNR) λt = log(αt/σt) for time discretization. Specifically, for three consecutive timesteps ti < ti1 < ti2, where xti1 , xti2 are already obtained, the update rule for xti is: xti = σti σti1 xti1 αti(ehi 1) (cid:20)(cid:18) 1 + (cid:19) 1 2ri xθ(xti1 , ti1) (cid:21) xθ(xti2 , ti2) (17) 1 2ri where hi = λti λti1 , ri = hi1 , and the data predictor xθ = xt tvθ for rectified flow. Highhi order solvers are also adopted in MixGRPO (Li et al., 2025b) but only for certain steps. Adopting the 2nd-order solver throughout the entire sampling process is infeasible, as λt will be infinity at boundaries = 0 or = 1. Following common practices, the first and last steps degrade to the first-order solver, which is the default Euler discretization for flow models. B.3 INTUITION BEHIND THE FLOWGRPO OBJECTIVE We provide some insight into reverse-process diffusion RL by inspecting the FlowGRPO objective in sampler-agnostic manner. For any first-order SDE sampler, the reverse sampling step from to < can be expressed as xs = l(s, t)xt m(s, t)vθ(xt, t) + n(s, t)ϵ, ϵ (0, I) (18) where l(s, t), m(s, t), n(s, t) depend only on s, and the sampler. Consider the on-policy case and the branching strategy in MixGRPO. Starting from shared xt, group of noises ϵ(1), . . . , ϵ(N ) , . . . , x(N ) are sampled and incorporated into the reverse step to produce multiple samples x(1) ."
        },
        {
            "title": "Preprint",
            "content": "They go through further sampling, yielding clean samples and corresponding advantages A(1), . . . , A(N ). On-policy GRPO minimizes the negative advantage-weighted log likelihoods: L(θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 A(i) log pθ(x(i) xt) where log pθ(x(i) xt) = x(i) (l(s, t)xt m(s, t)vθ(xt, t))2 2 2n2(s, t) + = m(s, t)vθ(xt, t) m(s, t)vsg(θ)(xt, t) + n(s, t)ϵ(i)2 2 2n2(s, t) + (19) (20) sg emerges because the samples x(1) log likelihood w.r.t. θ can be surprisingly reduced to simple form: , . . . , x(N ) are gradient-free. The gradient of the reverse-step and θ log pθ(x(i) xt) = m(s, t) n(s, t) θ((ϵ(i))vθ(xt, t)) θL(θ) = m(s, t) n(s, t) θ (cid:34) 1 (cid:88) (cid:35) (A(i)ϵ(i))vθ(xt, t) i=1 (21) (22) Therefore, FlowGRPO essentially aligns the velocity field with the advantage-weighted noise, while the choice of timesteps and sampler only influences the weighting m(s,t) n(s,t) across sampling steps. From this perspective, the training loss of FlowGRPO should be disentangled with sampler parameters like the variance gt, which current practices fail to address."
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "Training Configurations. Our setup largely follows FlowGRPO, adopting the same number of groups per epoch (48), group size (24), LoRA configuration (α = 64, = 32), and learning rate (3e 4). For each collected clean image, forward noising and loss computation are performed exactly on the corresponding sampling timesteps. We employ 2nd-order ODE sampler for data collection and enable adaptive time weighting by default. Single-Reward. For head-to-head comparison with FlowGRPO under single-reward settings, we fix the number of sampling steps to 10 to ensure fairness. By default, we set β = 1 and ηi = min(0.001i, 0.5), which work stably for most reward models. In the case of OCR, the reward rapidly approaches 1 within 100 iterations but suffers from instability. To address this, we adopt more conservative soft-update strategy with ηmax = 0.999. Multi-Reward. To comprehensively improve the base model across multiple rewards, we adopt multi-stage training scheme. The training setup involves three categories of rewards and datasets: (1) PickScore, CLIPScore, and HPSv2.1 on the Pick-a-Pic dataset; (2) GenEval with the three rewards above on the GenEval dataset; and (3) OCR with the three rewards above on the OCR dataset. Since the initial CFG-free generation is of low quality, we first train on (1) for 800 iterations to enhance image quality, followed by (2) for 300 iterations, (1) for 200 iterations, (2) for 200 iterations, and finally (3) for 100 iterations. All rewards are equally weighted, with PickScore divided by 26 for normalization to [0, 1]. By default, we use β = 0.1 and ηi = min(0.001i, 0.5), while setting ηmax = 0.95 for OCR to stabilize training. The number of sampling steps is fixed to 40 to ensure high-fidelity data collection."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "We provide more qualitative comparison between the base model, FlowGRPO and our multi-reward optimized model in Figure 11, Figure 12 and Figure 13."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Qualitative comparison between FlowGRPO and our model on GenEval prompts."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Qualitative comparison between FlowGRPO and our model on OCR prompts."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Qualitative comparison between FlowGRPO and our model on DrawBench prompts."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Stanford University",
        "Tsinghua University"
    ]
}