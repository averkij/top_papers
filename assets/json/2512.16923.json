{
    "paper_title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "authors": [
        "Chun-Wei Tuan Mu",
        "Jia-Bin Huang",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes."
        },
        {
            "title": "Start",
            "content": "Generative Refocusing: Flexible Defocus Control from Single Image Chun-Wei Tuan Mu1 Jia-Bin Huang2 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2University of Maryland, College Park 5 2 0 D 8 1 ] . [ 1 3 2 9 6 1 . 2 1 5 2 : r Figure 1. Generative refocusing with controllable optics. Our model turns single input image into virtual controllable camera, enabling diverse post-capture adjustments. (a) demonstrates aperture size control, allowing the user to vary the depth of field from strong bokeh to an all-in-focus image. (b) illustrates focus plane control by shifting the sharp region from the middle subject to the background. (c) highlights aperture shape control, synthesizing creative heart-shaped bokeh from point lights in the scene. (d) shows composite control where both the focus plane and aperture size are adjusted simultaneously to reframe the subject."
        },
        {
            "title": "Abstract",
            "content": "Depth-of-field control in photography, but is essential getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes. Project page: https://generative-refocusing.github.io/ 1. Introduction Controlling depth of field and focus is essential for artistic photography. It helps guide the viewers eye through selective focus and bokeh effects. Getting perfect focus often requires multiple shots or special equipment, which can be tough for everyday photographers. Single-image refocusing 1 Table 1. Comparison across methods. = supported; = not supported; = supported with caveat. In the synthetic/real training data columns, paired indicates the use of paired supervision, while unpaired indicates real unpaired data (no real paired supervision). Bokehlicious typically assumes an all-in-focus input and requires fine-tuning for different input types. For BokehMe, only the classical renderer variant supports aperture-shape control. UW = ultra-wide, = wide. Input Training data Supported user control Method Setup BokehMe [48] DC2 [2] Bokehlicious [60] Bokehdiff [89] DiffCamera [72] Ours Single-view Dual-view (UW+W) Single-view Single-view Single-view Single-view AIF required? Synthetic paired paired paired paired Real paired paired unpaired Aperture size Aperture shape Focus point Defocus map removes these obstacles. It allows for focus and bokeh adjustments after the photo is taken. The main challenge is recovering sharp details from blurred areas while creating natural-looking bokeh, while still giving users fine control. Existing research addresses different aspects of this problem. Defocus deblurring methods [1, 8, 36, 56, 67, 73, 84] aim to recover sharp images. Diffusion approaches [41, 42] show promise for addressing spatially variant blur. Bokeh synthesis methods [9, 30, 48, 60] focus on rendering realistic depth-of-field. Recent efforts include physics-based [37, 38, 50], neural [48, 49], and diffusion-based [19, 72, 82] approaches. However, most methods handle either deblurring or bokeh synthesis individually. End-to-end refocusing often requires specialized capture setups [2, 40, 46]. Despite these advances, three main limitations remain (Tab. 1). First, most methods rely on all-in-focus inputs and accurate depth maps. This restricts their use to images that already have defocus blur. Second, while synthetic data [48, 89] can be used for training, it is limited by the realism of the simulators. This approach often fails to capture detailed visual appearances. On the other hand, obtaining real paired datasets [60] is very expensive. Third, current methods usually only support aperture size, not its shape. This restriction limits the possibilities for creative bokeh effects. To tackle these issues, we present Generative Refocusing, flexible single-image refocusing system built on two-stage design. Our DeblurNet module produces sharp, in-focus image from variety of inputs. It uses diffusion model guided by initial deblurring predictions. Our BokehNet creates fully customizable bokeh, considering userdefined focus planes, bokeh intensity, and aperture shapes  (Fig. 1)  . significant advancement in our method is our training scheme, which makes use of both 1) paired synthetic data to maintain geometric consistency and 2) unpaired real-world bokeh images with camera metadata that help us learn real lens characteristics not captured by simulators. Our approach achieves top performance in defocus deblurring, bokeh generation, and refocusing, while maintaining the natural consistency of scenes. Our main contributions are: flexible refocusing pipeline accepts images in any focus state. It gives users control over the focus plane, bokeh intensity, and aperture shape through two-stage decomposition. This semi-supervised framework connects synthetic and unpaired real bokeh images. It enables genuine optical learning that goes beyond what simulator can offer. This is achieved through parameter estimation from EXIF metadata. The system performs well across defocus deblurring, bokeh synthesis, and refocusing. This has been validated on existing benchmarks (DPDD [1], REALDOF [36]) and our new light-field datasets (LF-BOKEH, LF-REFOCUS). It also has applications in text-guided deblurring and creating aperture shapes. 2. Related Work Diffusion Models for Image Restoration. Diffusion models [24, 65, 68, 81] advance image restoration via generative priors, extending to zero-shot restoration and artifact removal. Methods evolved from pixel-space [57] to efficient latent-space approaches [54, 76], enabling faster inference. ResShift [83] cuts steps from 1000 to 415 via residual shifting. Deblurring progressed from merging diffusion priors with regression tasks, such as matting [74] or hierarchical restoration [13], to learning spatially-varying kernels in latent space [33]. Cascaded pipelines [25] refine at multiple resolutions, while principled frameworks [44, 45] treat degradations as stochastic processes. Training-free methods [32, 91] enable plug-and-play restoration. Unlike these general methods, we target spatially-varying defocus blur with two-stage pipeline explicitly separating deblurring from bokeh synthesis for controllable refocusing. Defocus Deblurring. Defocus deblurring recovers sharp images from spatially varying blur caused by limited depth of field. Early deconvolution [4, 92] introduced artifacts at depth transitions. Dual-pixel sensors [1], quad-pixel data [8], and disparity-aware techniques [36, 69, 79] im2 Figure 2. Pipeline Overview. Our method decomposes single-image refocusing into two stages: (a) Defocus Deblurring and (b) Shallow Depth-of-Field Synthesis. Given blurry input image Iin, we optionally apply pre-deblurring method [56] to obtain conservative estimate Isyn, then feed both Iin and Isyn into DeblurNet to recover high-quality all-in-focus image Iaif. The VAE encoder and decoder convert images to latent representations for processing by the DiT backbone. In the second stage, BokehNet takes the all-in-focus image Iaif, the defocus map Ddef, and optionally specific aperture shape as inputs to synthesize the refocused output. The defocus map Ddef is computed based on the estimated depth map [7], along with both the user-specified focus plane S1 and bokeh level K. proved blurdepth modeling and the recovery of all-infocus images from focal stacks. Architectures shifted from CNNs [10] to transformers [67, 73], with gains from implicit representations [17, 51] and multi-scale attention [87]. Recent work includes vision-language fusion [77] for semantic estimation and shift from supervised [52] to diffusionbased learning [41, 42], including unpaired data. While these excel as standalone solutions, we position deblurring as the first stage of refocusing, using FLUXs [61] generative prior to enable controllable bokeh synthesis from any input. Bokeh Rendering. Bokeh rendering progressed from physically based scattering [37, 38, 50] and differentiable rendering [63] to neural methods combining ray tracing with learned enhancements [48, 49]. Learning-based approaches evolved from fixed apertures [30] to variable f-stops [9, 60], extending to video [80] and 3D scenes [62, 71]. Diffusion models enabled bokeh control [19, 82], mainly for text-toimage synthesis. These methods require all-in-focus inputs and accurate depth. We remove this constraint via flexible input handling, which accepts defocused or all-in-focus images, and semi-supervised learning on unpaired real-world bokeh to capture authentic lens characteristics beyond simulators. Single-Image Refocusing. Post-capture refocusing manipulates focus after capture. Light-field cameras [40, 46] enable optical refocusing but need specialized capture. Computational approaches evolved from deconvolution [4, 86] producing artifacts, to GAN-based paradigms [58], to diffusion methods [72]. 3D representations [35, 43, 62, 71] achieve real-time controllable DoF but need multi-view capture or calibration. We combine three capabilities: (1) semisupervised learning mixing synthetic and unpaired real data for authentic optics, (2) single-image flexibility accepting any input without preprocessing, and (3) comprehensive control over focus, intensity, and aperture shape. Camera-Conditioned Diffusion. Conditioning diffusion models on camera parameters progressed from extrinsic control for view synthesis [14, 26] and video generation [22, 75], to intrinsic control of focal length, aperture, and exposure [12, 18]. Direct conditioning often yields inconsistent content. Recent work uses temporal modeling [82], explicit calibration [6, 16], or transformer-based pose handling [3, 59]. These focus on generation; we operate in editing, manipulating existing images with scene consistency. By decoupling into deblurring and bokeh stages, we achieve efficient single-image manipulation without multiframe overhead. Semi-Supervised Learning. When paired data is scarce, semi-supervised learning provides alternatives. The field evolved from learning without clean targets [27, 34, 39] to pseudo-labeling [11, 23, 70]. Foundation models enabled the generation of high-quality pseudo-labels [21, 85]. Unpaired 3 Figure 3. Training data generation. Each training sample consists of the following five components: (i) bokeh image, (ii) an all-in-focus (AIF) image, (iii) depth map D, (iv) bokeh level K, and (v) focus plane S1. We construct these samples via three routes: (a) Synthetic paired data. Given real AIF images and depth maps D, we compute defocus map Ddef parameterized by specified bokeh level and focus plane S1, and feed it into bokeh renderer [48] to synthesize corresponding bokeh images. (b) Real unpaired data. Given real bokeh images, DeblurNet recovers an AIF image. We then estimate depth and extract foreground mask [88] to define the focus plane S1. The bokeh level is computed from the EXIF metadata following the formulation in [19]. (c) Real paired data without EXIF. For real pairs lacking EXIF, we obtain pseudo-AIF image and S1 as in (b), and follow Eq. (2) to estimate the bokeh level K. frameworks [31, 64, 90] learn enhancement without paired supervision. Two-stage approaches [29, 78] bridge fidelity and perceptual quality via different supervision types. We adapt these to bokeh rendering: perfect paired refocusing data is nearly impossible to obtain. We train on synthetic pairs for structure, then unpaired real bokeh for realism. Deblur restores to all-in-focus, enabling bokeh to learn authentic lens characteristics exceeding simulators. 3. Method Single-image refocusing couples two operations: recovering sharp content in out-of-focus regions and applying controllable bokeh to originally sharp areas. Blur magnitude is dictated by defocus map parameterized by user-specified focus point and the scene depth. When the input is blurry, monocular depth estimation is brittle, undermining precise control over the defocus map and often yielding misfocus or artifacts. We propose Generative Refocusing, which decomposes the task into defocus deblurring and bokeh synthesis, and comprises two models, DeblurNet and BokehNet, that jointly enable precise and controllable refocusing (see Fig. 2 for an overview of the pipeline); Such flexible pipeline allows us to exert greater control over input conditions (see Tab. 1). 3.1. Stage 1: Defocus Deblurring DeblurNet. We condition the deblurring network on pair (Iin, Ipd), where Ipd is conservative pre-deblur generated by an off-the-shelf defocus deblurring model. In our implementation, we use non-generative defocus model [56], whose outputs are typically content-faithful yet over-smoothed. Our model, guided by learned diffusion prior, then injects highfidelity details without sacrificing geometric correctness. Artifact-robust dual conditioning. To prevent artifacts in Ipd from biasing the reconstruction and to ensure that fine details are recovered from Iin, we introduce two complementary mechanisms (see Fig. 2 (a)): Positional disentanglement. We encode the two inputs with disjoint 2D positional grids: Iin is anchored at (0, 0), while Ipd is shifted to start at (0, ), where denotes the side length of the token grid after downsampling Iin. This shift lets the network distinguish the two sources, using Ipd as low-frequency anchor while extracting high-frequency evidence from Iin. Pre-deblur dropout. During training, we stochastically replace Ipd with zero tensor (at fixed probability), forcing the network to remain effective when the auxiliary input is unreliable or absent. This regularizes the fusion, shields against method-specific artifacts, and enables the model to operate in single-input deblurring mode at test time. 3.2. Stage 2: Shallow Depth-of-Field Synthesis Synthetic-pair learning. BokehNet expects an AIF image, target bokeh image, and the corresponding defocus map (see Fig. 2 (b)). In practice, perfectly paired data and accurate defocus maps are hard to acquire. We therefore pretrain with synthetic pairs: starting from real all-in-focus images and their estimated depth, we randomly sample focus plane S1 and target bokeh level K, and use simulator [48] to render bokeh consistent with the induced defocus map (see Fig. 3 (a)). This stage quickly trains the network to modulate the circle of confusion based on the defocus map, but it remains constrained by renderer bias and may introduce non-realistic artifacts. Real unpaired bokeh learning. To capture real optics, we further leverage unpaired real bokeh images with EXIF metadata (see Fig. 3 (b)) and estimate an approximate bokeh level following [19]: 2 S1 2 (S1 ) pixel ratio, (1) where is the focal length and denotes the aperture - number; and pixel ratio compensates for sensor size and image resolution differences. The AIF input is provided by DeblurNet. Although S1 can sometimes be read from metadata, both estimated and true depths may deviate in real scenes. To obtain more reliable supervision, we estimate infocus masks using BiRefNet [88], and have human experts filter erroneous cases and complete partial masks. These masks refine the focus plane and yield consistent defocus map, enabling BokehNet to learn from real bokeh images. Real paired bokeh images without metadata. Previous bokeh-synthesis datasets [60] [30] provide real pairs but omit EXIF metadata or provide insufficient fields to estimate reliable (see Fig. 3 (c)). We therefore adopt simulator-inthe-loop calibration of the bokeh level: given an AIF image Iaif , an estimated depth D, and focus annotation obtained as in the unpaired setting (BiRefNet-based in-focus masks with expert filtering), we sweep and choose the value whose rendered result best matches the real bokeh target Ireal: = argmax (Kmin, Kmax) SSIM(cid:0)B(Iaif , D; S1, K), Ireal (cid:1), (2) where denotes our physically guided renderer and S1 is the focus-plane proxy. The selected is then used as the pseudo bokeh level label for training . Focus-plane annotation follows the same protocol as in the unpaired case. Discussion. Although the resulting defocus maps and the AIF proxy are not perfectly accurate, this supervision regime mirrors the practical refocusing scenario. Aggregating numbers of real bokeh images substantially improves realism and robustness over synthetic-only training. 3.3. Bokeh-Shape Aware Synthesis State-of-the-art learning-based bokeh synthesis methods deliver compelling results but do not expose aperture-shape control. Physics-based renderers, in principle, support arbitrary shapes, yet public implementations typically omit this functionality. We therefore explore explicit shape-aware control using only bokeh shape image within trainable framework. Simulator and Data. Real photographs exhibiting diverse bokeh shapes are rare, and paired AIFbokeh examples are even scarcer. We thus rely on simulation. key observation is that when an all-in-focus (AIF) image lacks point-light stimuli, the simulated bokeh carries weak shape cues, making the model reluctant to learn shape-conditioned responses. To address this, we synthesize PointLight-1K AIF set designed to reveal aperture shape. We also extend the classical BokehMe renderer to scatter through given shape: given an AIF image Iaif , depth D, focus-plane proxy S1, bokeh level K, and shape kernel (binary/raster PSF), the simulator renders Isyn = R(cid:0)Iaif , D; S1, K, s(cid:1), (3) yielding paired supervision for shape-aware training. Shape-conditioned fine-tuning. Starting from our base model trained without shape control, we freeze all original LoRA weights and introduce another conditioning branch that ingests shape exemplar s. Only this branch is finetuned while the backbone remains fixed, ensuring that shape edits do not regress the learned bokeh synthesis. 4. Experiments 4.1. Setup Backbone and training details. Our base generative backbone is FLUX-1-DEV [61], fine-tuned via LoRA [28] with multi-condition scheme following [66]. DeblurNet employs LoRA rank r=128, while BokehNet uses r=64. Both models are trained with per-GPU batch size 1 and gradient accumulation over 8 steps on 4RTX A6000 GPUs. DeblurNet is trained for 60K steps. BokehNet is trained in two stages: synthetic pair training for 40K steps and Real unpair for another 40K steps. Datasets. We summarize the training data for the two modules below. DeblurNet. We train on DPDD [1] together with curated subset of REALBOKEH 3MP [60], totaling approximately 4.5K image pairs. BokehNet (synthetic pairs). We use AIF images from [82] and subset of EBB! [30], and procedurally render defocus under varying aperture sizes/shapes and focus points to generate about 70K synthetic pairs. BokehNet (real supervision). We incorporate (i) an unpaired real-bokeh set collected from Flickr by Fortes et al. [19], and (ii) real AIFbokeh pairs with incomplete metadata from REALBOKEH 3MP [60] and LFDOF [55], contributing approximately 27K real examples in total. All Table 2. Defocus deblurring benchmark on REALDOF [36] and DPDD [1] datasets. We evaluate the task of recovering all-in-focus images from defocused-blur inputs. Our method, GenRefocus, uses the DeblurNet module for this task. We report both reference-based metrics (LPIPS, FID) and no-reference perceptual quality metrics (CLIP-IQA, MANIQA, MUSIQ). Best , second best , and third best results are highlighted. GenRefocus outperforms all competing approaches across both benchmarks and all metrics, demonstrating the effectiveness of our diffusion-based deblurring with pre-deblur conditioning. REALDOF [36] DPDD [1] Method Input GT AIFNet [55] IFANet [36] DRBNet [56] Restormer [84] INIKNet [51] GenRefocus (Ours) LPIPS FID CLIP-IQA MANIQA MUSIQ LPIPS FID CLIP-IQA MANIQA MUSIQ 0.5241 0.4132 0.2994 0.2550 0.2863 0.2851 0.2356 92.93 72.38 33.21 29.89 29.80 32.18 24.73 0.3562 0. 0.3814 0.4088 0.3889 0.4062 0.3984 0.4575 0.2213 0.2675 0.2536 0.2609 0.2609 0.2642 0.2580 0.2886 28.7087 42.5777 35.7121 39.5949 38.9014 40.8355 38.9959 43.7042 0.3485 0.3090 0.2301 0.1819 0.1762 0.1860 0.1598 88.85 79.84 42.60 39.37 33.45 36.63 33.08 0.4337 0.4563 0.4359 0.4472 0.4142 0.4332 0.4290 0.4619 0.3325 0. 0.3371 0.3276 0.3254 0.3307 0.3265 0.3411 45.5376 47.8102 46.9558 47.2784 46.7229 47.9872 47.1961 49.3019 Figure 4. Qualitative comparison on defocus deblurring. Visual results on (a) REALDOF [36] and (b) DPDD [1] datasets. Blue boxes on the left indicate cropped regions shown in detail. Our DeblurNet faithfully recovers fine text details (NEW YORK) in (a) where competing methods produce blurry or distorted results. In the challenging example (b) with severe defocus blur, other methods either fail to recover structure (AIFNet [55], DRBNet [56], INIKNet [51]) or introduce artifacts in the background (IFANet [36], Restormer [84]), while our method reconstructs geometrically consistent, visually compelling content by leveraging diffusion priors guided by the pre-deblurred input. datasets are used strictly for research and are split so that no scene overlaps across training and evaluation. Benchmarks. We evaluate three tasks: (i) defocus deblurring on DPDD and REALDOF; (ii) bokeh synthesis on our new LF-BOKEH benchmark, comprising 200 images with multiple focus planes synthesized from [53] and [15] lightfield captures; and (iii) refocusing on LF-REFOCUS, built by pairing same-scene images within LF-BOKEH to form 400 sourcetarget pairs. Inference-time resolution and aspect ratio. We adopt tiling strategy inspired by [5] at evaluation time: we split the input image into overlapping patches and perform tile-wise denoising before blending them back together. As result, all benchmarks are conducted at each datasets original aspect ratio and resolution, without additional resizing. 4.2. Comparison Defocus deblurring. As stated on Tab. 2, DeblurNet consistently outperforms all baselines across reference and no6 Input GT BokehMe [48] Bokehlicious [60] BokehDiff [89] Ours Figure 5. Qualitative comparison on bokeh synthesis benchmark. Results on LF-BOKEH with zoomed patches (blue and orange boxes) highlighting detail quality. Our BokehNet synthesizes bokeh effects that better match ground truth with realistic blur gradients and natural occlusion handling. Baselines show various artifacts: BokehMe [48] exhibits simulator bias, Bokehlicious [60] over-smooths details, and BokehDiff [89] produces inconsistent defocus. Our semi-supervised training on real bokeh images enables the capture of authentic lens characteristics. Table 3. Bokeh synthesis benchmark. Evaluation on LF-BOKEH (200 light-field images with multiple focus planes). Following [89], we use per-image binary search for optimal K. Bokehlicious supports only aperture size control. Our semi-supervised training on real bokeh data outperforms all baselines. Method LPIPS DISTS CLIP-I BokehMe [48] Bokehlicious [60] BokehDiff [89] GenRefocus (Ours) 0.1228 0.1799 0.1708 0.1047 0.0744 0.1062 0.0933 0.0611 0.9511 0.9304 0.9192 0. reference metrics. On certain NR-IQA measures, it even surpasses the ground-truth. Qualitatively, Fig. 4 (a) shows faithful recovery of the NEW YORK text with well-preserved edges. The scene in Fig. 4 (b) is particularly challenging: competing methods recover only blurry dark stripe and further distort the background, whereas DeblurNet reconstructs structure-consistent content and delivers visually compelling results without introducing artifacts or geometry drift. Bokeh synthesis. Following the BokehDiff protocol [89], we conduct per-image binary search over the bokeh level and select the value that maximizes SSIM with the target. All other control signals are held fixed, ensuring fair comparison in which each method operates at its best attainable setting. As summarized in Tab. 3, our approach attains the highest scores on both fidelity and perceptual metrics, outperforming all baselines. The qualitative results in Fig. 5 show that our method synthesizes bokeh that more closely matches that of real photographs. In the zoomed-in patches, our outputs more faithfully reproduce the ground-truth bokeh than competing baselines. Other methods tend to naively translate the predicted defocus map into blur strength, so any error in the depth scale directly leads to overor under-blurring. In contrast, BokehNet, trained with our semi-supervised strategy, is more robust to such depth-scale inaccuracies and thus produces more appropriate amount of blur that better matches the ground truth. Refocusing. We build four two-stage baselines by pairing strong AIF estimators (DRBNet [56], Restormer [84]) with bokeh synthesizers (BokehMe [48], BokehDiff [89]), yielding the 2 2 combinations. For each method, we also follow the protocol of BokehDiff [89] to select the best bokeh level . As shown in Tab.4, our approach achieves the best results on both fidelity and perceptual metrics, outperforming all baseline pairings. Qualitatively, Fig. 6 shows that competing methods tend to produce spatially ambiguous blur and fail to place the focal plane correctly, resulting in uniformly soft content and inaccurate foreground/background separation. In contrast, our approach restores fine details at the intended focus while synthesizing physically consistent bokeh elsewhere. 4.3. Ablation Study One stage and two stage refocusing. We also evaluate variant that takes an input image and provided defocus map and directly produces refocused output in single pass. Concretely, we instantiate one-stage model by reusing the BokehNet backbone and training it on the same synthetic pairs, then assess it on the refocusing benchmark. As re7 Table 4. Quantitative comparison on refocusing benchmark. Results on our LF-REFOCUS dataset (400 pairs with different focus planes). We compare our DeblurNet+BokehNet pipeline against combinations of existing all-in-focus estimation methods [56, 84] and bokeh synthesis methods [48, 89]. Optimal bokeh level is determined via per-image binary search following [89]. Our approach achieves superior performance across all metrics, demonstrating the benefit of joint optimization and semi-supervised training. AiF Estimation Bokeh Synthesis LPIPS DISTS CLIP-I MUSIQ NIQE CLIP-IQA DRBNet [56] Restormer [84] BokehMe [48] BokehDiff [89] BokehMe [48] BokehDiff [89] DeblurNet (Ours) BokehNet (Ours) 0.1471 0.1848 0.1554 0.1860 0.1458 0.0937 0.1030 0.0963 0.1016 0.0850 0.9335 0.9310 0.9303 0.9320 0. 43.3087 48.0766 43.3188 48.0326 52.6954 6.5783 5.8195 7.1249 6.3216 5.3343 0.4460 0.4997 0.4782 0.5352 0.5763 GT Input DRBNet [56] + BokehMe [48] DRBNet [56] + BokehDiff [89] Figure 6. Qualitative comparison on refocusing benchmark. Results on LF-REFOCUS comparing our end-to-end trained pipeline against baseline combinations of existing deblurring [56, 84] and bokeh synthesis [48, 89] methods. Zoomed patches (blue and orange boxes) highlight focal and defocused regions. Baselines struggle with accurate focus-plane placement and produce inconsistent or overly smooth blur. Our method achieves sharp refocusing at the target plane while synthesizing photorealistic bokeh, closely matching ground truth across diverse scenes. Restormer [84] + BokehDiff [89] Restormer [84] + BokehMe [48] GenRefocus (Ours) Table 5. Ablation on pipeline design. Comparing one-stage (direct refocusing) and two-stage (deblur then re-bokeh) on LF-REFOCUS. Both use the same backbone and synthetic data. Two-stage enables better depth control and independent semi-supervised learning per stage, yielding substantial improvements. Method LPIPS DISTS CLIP-I One stage Two stage (ours) 0.1723 0.1458 0.0976 0.0850 0.9345 0.9461 Table 6. Ablation on BokehNet training. Comparing syntheticonly vs. adding real unpaired bokeh images on LF-BOKEH. Second stage uses real bokeh with estimated parameters (Sec. 3.2), initialized from synthetic weights. Real data enables learning authentic optical characteristics beyond simulators. Method LPIPS DISTS CLIP-I Synthetic pair Synthetic pair+ Real unpair 0.1289 0.1047 0.0738 0.0611 0.9461 0. ported in Tab. 5, the one-stage variant lags the proposed two-stage pipeline by substantial margin. We attribute this gap to two factors. First, control fidelity: the defocus map relies on depth estimated from blurry input; without an allin-focus view, depth quality degrades, leading to misaligned or biased blur fields and weakening refocusing accuracy. Second, data leverage: the one-stage formulation cannot separate the problem into defocus removal and bokeh synthesis, preventing us from injecting real-data supervision tailored to each subtask. Key idea and BokehNet training ablation. Our core idea is to use data generation to harvest real-world supervision for training BokehNet, leveraging real unpaired images. To quantify its impact, we evaluate two training settings without introducing additional architectural changes: (i) synthetic pair: BokehNet is trained purely on synthetic paired data; and (ii) synthetic pair + real unpaired: starting from (i), we further train with real unpaired images processed by our pipeline to produce usable supervision. On the bokehsynthesis benchmark, the synthetic pair setting performs on par with classical simulator baseline, while the synthetic pair + real-unpaired setting delivers substantial improvements across both fidelity and perceptual metrics (Tab. 6)."
        },
        {
            "title": "References",
            "content": "[1] Abdullah Abuolaim and Michael S. Brown. Defocus deblurring using dual-pixel data. In ECCV, 2020. 2, 5, 6, 13 [2] Hadi Alzayer, Abdullah Abuolaim, Leung Chun Chan, Yang Yang, Ying Chen Lou, Jia-Bin Huang, and Abhishek Kar. Dc2: Dual-camera defocus control by learning to refocus. In CVPR, 2023. 2 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, HsinYing Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. 3 [4] Yosuke Bando and Tomoyuki Nishita. Towards digital refocusing from single photograph. In PG, 2007. 2, 3 [5] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. 6 [6] Edurne Bernal-Berdun, Ana Serrano, Belen Masia, Matheus Gadelha, Yannick Hold-Geoffroy, Xin Sun, and Diego Gutierrez. Precisecam: Precise camera control for text-to-image generation. In CVPR, 2025. 3 [7] Alexey Bochkovskiy, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. In ICLR, 2025. 3, [8] Hang Chen, Yin Xie, Xiaoxiu Peng, Lihu Sun, Wenkai Su, Xiaodong Yang, and Chengming Liu. Quad-pixel image defocus deblurring: new benchmark and model. In CVPR, 2025. 2 [9] Kang Chen, Shijun Yan, Aiwen Jiang, Han Li, and Zhifeng Wang. Variable aperture bokeh rendering via customized focal plane guidance. arXiv preprint arXiv:2410.14400, 2024. 2, 3 [10] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In ECCV, 2022. 3 [11] Lufei Chen, Xiangpeng Tian, Shuhua Xiong, Yinjie Lei, and Chao Ren. Unsupervised blind image deblurring based on self-enhancement. In CVPR, 2024. 3 [12] Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, and Yen-Yu Lin. Learning continuous exposure value representations for single-image hdr reconstruction. In ICCV, 2023. [13] Zheng Chen, Yulun Zhang, Ding Liu, Jinjin Gu, Linghe Kong, Xin Yuan, et al. Hierarchical integration diffusion model for realistic image deblurring. In NeurIPS, 2023. 2 [14] Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, and Niki Trigoni. Learning continuous 3d words for text-to-image generation. In CVPR, 2024. 3 [15] Donald G. Dansereau, Bernd Girod, and Gordon Wetzstein. Liff: Light field features in scale and depth. In CVPR, 2019. 6 [16] Junyuan Deng, Wei Yin, Xiaoyang Guo, Qian Zhang, Xiaotao Hu, Weiqiang Ren, Xiao-Xiao Long, and Ping Tan. Boost 3d reconstruction using diffusion-based monocular camera calibration. In ICCV, 2025. 3 Input Triangle Heart Star Figure 7. Controllable aperture shape synthesis. Given an input image, our BokehNet can synthesize bokeh effects with custom aperture shapes (triangle, heart, star) by conditioning on shape exemplars. Background point lights exhibit the specified aperture geometry while maintaining scene consistency. Figure 8. Text-guided deblurring. DeblurNet uses optional text prompts to guide reconstruction of severely blurred text. With prompts, we correct hallucinations (red: DESION DESIGN) and recover accurate text (green: yeurs years), matching GT. Without guidance, it hallucinates incorrect content. Application. As illustrated in Fig. 7, after we fine-tune BokehNet on our point-light 1K dataset, the model can synthesize bokeh with diverse, user-specified aperture shapes. Moreover, even though DeblurNet is trained without any text prompts, it can still leverage textual instructions at inference time to refine the all-in-focus output, as shown in Fig. 8. This highlights key advantage of generative defocus-deblurring models: they naturally support post-hoc, prompt-based correction and editing of the reconstructed content. 5. Conclusion We present Generative Refocusing, two-stage diffusion framework turning single images into controllable virtual cameras. By decoupling into DeblurNet (deblurring) and BokehNet (bokeh synthesis), our approach handles arbitrary inputs with intuitive controls over focus plane, bokeh intensity, and aperture shape. Through semi-supervised training on synthetic and real unpaired bokeh data, GenRefocus learns authentic optical behavior beyond simulators. Experiments on DPDD, REALDOF, LF-BOKEH, and LFREFOCUS show consistent improvements, enabling textguided refinement and aperture-shape editing. Limitations and future work. GenRefocus relies on monocular depth estimation; severe failures result in mislocalized focal planes. Complex aperture shapes require simulator-driven training data. Future work should generalize to richer aperture vocabularies, including user-drawn designs. 9 [17] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. Spectromotion: Dynamic 3d reconstruction of specular scenes. In CVPR, 2025. 3 [18] I-Sheng Fang, Yue-Hua Han, and Jun-Cheng Chen. Camera settings as tokens: Modeling photography on latent diffusion models. In SIGGRAPH Asia, 2024. 3 [19] Armando Fortes, Tianyi Wei, Shangchen Zhou, and Xingang Pan. Bokeh diffusion: Defocus blur control in text-to-image diffusion models. In SIGGRAPH Asia, 2025. 2, 3, 4, 5, [20] Google DeepMind. Gemini models: Product overview. https : / / deepmind . google / technologies / gemini/, 2025. Accessed: November 21, 2025. 13, 14 [21] Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Weaklysupervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping. In NeurIPS, 2023. 3 [22] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [23] Jin-Ting He, Fu-Jen Tsai, Jia-Hao Wu, Yan-Tsung Peng, Chung-Chi Tsai, Chia-Wen Lin, and Yen-Yu Lin. Domainadaptive video deblurring via test-time blurring. In ECCV, 2024. 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [25] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [26] Lukas Hollein, Aljaˇz Boˇziˇc, Norman Muller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhofer, and Matthias Nießner. Viewdiff: 3d-consistent image generation with text-to-image models. In CVPR, 2024. 3 [27] Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert YC Chen, Min Sun, and Cheng-Hao Kuo. Openm3d: Open vocabulary multi-view indoor 3d object detection without human annotations. In ICCV, 2025. 3 [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. 5 [29] Shirui Huang, Keyan Wang, Huan Liu, Jun Chen, and Yunsong Li. Contrastive semi-supervised learning for underwater image restoration via reliable bank. In CVPR, 2023. 4 [30] Andrey Ignatov, Jagruti Patel, and Radu Timofte. Rendering natural camera bokeh effect with deep learning. In CVPRW, 2020. 2, 3, 5, 13 [31] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. IEEE TIP, 30:23402349, 2021. 4 [33] Lingshun Kong, Dongqing Zou, Fu Lee Wang, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, Jinshan Pan, et al. Deblurdiff: Real-word image deblurring with generative diffusion models. In NeurIPS, 2025. 2 [34] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In CVPR, 2019. [35] Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, and Eunbyung Park. Deblurring 3d gaussian splatting. In ECCV, 2024. 3 [36] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun Cho, Iterative filter adaptive network for and Seungyong Lee. single image defocus deblurring. In CVPR, 2021. 2, 6 [37] Sungkil Lee, Gerard Jounghyun Kim, and Seungmoon Choi. Real-time depth-of-field rendering using point splatting on per-pixel layers. In Computer Graphics Forum, pages 1955 1962, 2008. 2, 3 [38] Sungkil Lee, Elmar Eisemann, and Hans-Peter Seidel. Depthof-field rendering with multiview synthesis. ACM TOG, 28 (5):16, 2009. 2, 3 [39] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In ICML, 2018. 3 [40] Marc Levoy and Pat Hanrahan. Light field rendering. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 441452. 2023. 2, [41] Yuhao Li, Haoran Fang, Xiang Lei, Qi Wang, Gang Hu, Jiaqing Dong, Zilong Li, Jiabin Lin, Qiegen Liu, and Xianlin Song. Real-world defocus deblurring via score-based diffusion models. Scientific Reports, 15(1):22942, 2025. 2, 3 [42] Hanyan Liang, Shuyao Chai, Xixuan Zhao, and Jiangming Kan. Swin-diff: single defocus image deblurring network based on diffusion model. Complex & Intelligent Systems, 11 (3):170, 2025. 2, 3 [43] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. 3 [44] Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, and Weiming Hu. Visual-instructed degradation diffusion for all-in-one image restoration. In CVPR, 2025. 2 [45] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas Schon. Image restoration with mean-reverting stochastic differential equations. In ICML, 2023. 2 [46] Ren Ng, Marc Levoy, Mathieu Bredif, Gene Duval, Mark Horowitz, and Pat Hanrahan. Light field photography with hand-held plenoptic camera. PhD thesis, Stanford university, 2005. 2, [47] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 13 [32] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In NeurIPS, 2022. 2 [48] Juewen Peng, Zhiguo Cao, Xianrui Luo, Hao Lu, Ke Xian, and Jianming Zhang. Bokehme: When neural rendering meets classical rendering. In CVPR, 2022. 2, 3, 4, 5, 7, 8, 10 [49] Juewen Peng, Zhiguo Cao, Xianrui Luo, Ke Xian, Wenfeng Tang, Jianming Zhang, and Guosheng Lin. Bokehme++: Harmonious fusion of classical and neural rendering for versatile bokeh creation. IEEE TPAMI, 2024. 2, 3 [66] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 5 [50] Michael Potmesil and Indranil Chakravarty. lens and aperture camera model for synthetic image generation. SIGGRAPH, 1981. 2, 3 [67] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, and Chia-Wen Lin. Stripformer: Strip transformer for fast image deblurring. In ECCV, 2022. 2, 3 [51] Yuhui Quan, Xin Yao, and Hui Ji. Single image defocus deblurring via implicit neural inverse kernels. In ICCV, 2023. 3, [68] Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, and Yu-Lun Liu. Lightsout: Diffusion-based outpainting for enhanced lens flare removal. In ICCV, 2025. 2 [52] Dongwei Ren, Xinya Shu, Yu Li, Xiaohe Wu, Jin Li, and Wangmeng Zuo. Reblurring-guided single image defocus deblurring: learning framework with misaligned training pairs. IJCV, pages 118, 2025. 3 [53] Milan Rˇeˇrabek and Touradj Ebrahimi. New light field image dataset. In QoMEX, 2016. 6 [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [55] Lingyan Ruan, Bin Chen, Jizhou Li, and Miu-Ling Lam. Aifnet: All-in-focus image restoration network using light field-based dataset. IEEE TCI, 7:675688, 2021. 5, 6 [56] Lingyan Ruan, Bin Chen, Jizhou Li, and Miu-Ling Lam. Learning to deblur using light field generated and real defocus images. In CVPR, 2022. 2, 3, 4, 6, 7, 8 [57] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE TPAMI, 45(4):4713 4726, 2022. [58] Parikshit Sakurikar, Ishit Mehta, Vineeth Balasubramanian, and PJ Narayanan. Refocusgan: Scene refocusing using single image. In ECCV, 2018. 3 [59] Saurabh Saxena, Junhwa Hur, Charles Herrmann, Deqing Sun, and David Fleet. Zero-shot metric depth with field-of-view conditioned diffusion model. arXiv preprint arXiv:2312.13252, 2023. 3 [60] Tim Seizinger, Florin-Alexandru Vasluianu, Marcos V. Conde, and Radu Timofte. Bokehlicious: Photorealistic bokeh rendering with controllable apertures. In ICCV, 2025. 2, 3, 5, 7, 13 [61] Shakker Labs. Flux.1-dev-controlnet-union-pro. https:// huggingface.co/ShakkerLabs/FLUX.1devControlNet-Union-Pro, 2024. Accessed: 2025-11-13. 3, 5, 13 [62] Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, and Chen Change Loy. Dof-gaussian: Controllable depth-of-field for 3d gaussian splatting. In CVPR, 2025. 3 [63] Yichen Sheng, Zixun Yu, Lu Ling, Zhiwen Cao, Xuaner Zhang, Xin Lu, Ke Xian, Haiting Lin, and Bedrich Benes. Dr. bokeh: Differentiable occlusion-aware bokeh rendering. In CVPR, 2024. [64] Yiqi Shi, Duo Liu, Liguo Zhang, Ye Tian, Xuezhi Xia, and Xiaojing Fu. Zero-ig: Zero-shot illumination-guided joint denoising and adaptive enhancement for low-light images. In CVPR, 2024. 4 [65] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2 [69] Ning-Hsu Wang, Ren Wang, Yu-Lun Liu, Yu-Hao Huang, Yu-Lin Chang, Chia-Ping Chen, and Kevin Jou. Bridging unsupervised and supervised depth from focus via all-in-focus supervision. In ICCV, 2021. 2 [70] Ning-Hsu Albert Wang and Yu-Lun Liu. Depth anywhere: Enhancing 360 monocular depth estimation via perspective distillation and unlabeled data augmentation. In NeurIPS, 2024. 3 [71] Yujie Wang, Praneeth Chakravarthula, and Baoquan Chen. Dof-gs: Adjustable depth-of-field 3d gaussian splatting for post-capture refocusing, defocus rendering and blur removal. In CVPR, 2025. [72] Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, and Hengshuang Zhao. Diffcamera: Arbitrary refocusing on images. In SIGGRAPH Asia, 2025. 2, 3 [73] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In CVPR, 2022. 2, 3 [74] Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, and ShinIchi Satoh. Matting by generation. In SIGGRAPH, 2024. 2 [75] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. 3 [76] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: In ICCV, Efficient diffusion model for image restoration. 2023. 2 [77] Hao Yang, Liyuan Pan, Yan Yang, Richard Hartley, and Miaomiao Liu. Ldp: Language-driven dual-pixel image defocus deblurring network. In CVPR, 2024. [78] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. From fidelity to perceptual quality: semisupervised approach for low-light image enhancement. In CVPR, 2020. 4 [79] Yan Yang, Liyuan Pan, Liu Liu, and Miaomiao Liu. K3dn: Disparity-aware kernel estimation for dual-pixel defocus deblurring. In CVPR, 2023. 2 [80] Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai, Bo Li, and Peng-Tao Jiang. Any-to-bokeh: One-step video bokeh via multi-plane image guided diffusion. arXiv preprint arXiv:2505.21593, 2025. 3 [81] Chang-Han Yeh, Chin-Yang Lin, Zhixiang Wang, ChiWei Hsiao, Ting-Hsuan Chen, Hau-Shiang Shiu, and YuLun Liu. Diffir2vr-zero: Zero-shot video restoration with 11 diffusion-based image restoration models. arXiv preprint arXiv:2407.01519, 2024. [82] Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, and Stanley Chan. Generative photography: Scene-consistent camera control for realistic text-to-image synthesis. In CVPR, 2025. 2, 3, 5, 13 [83] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. In NeurIPS, 2023. 2 [84] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 2, 6, 7, 8 [85] Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen, Hiep Luong, and Wilfried Philips. Unmixing diffusion for selfsupervised hyperspectral image denoising. In CVPR, 2024. 3 [86] Wei Zhang and Wai-Kuen Cham. Single-image refocusing and defocusing. IEEE TIP, 21(2):873882, 2011. [87] Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, and Shing Shin Cheng. unified framework for microscopy defocus deblur with multi-pyramid transformer and contrastive learning. In CVPR, 2024. 3 [88] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 3(9150038):112, 2024. 4, 5 [89] Chengxuan Zhu, Qingnan Fan, Qi Zhang, Jinwei Chen, Huaqi Zhang, Chao Xu, and Boxin Shi. Bokehdiff: Neural lens blur with one-step diffusion. In ICCV, 2025. 2, 7, 8 [90] Lingyu Zhu, Wenhan Yang, Baoliang Chen, Hanwei Zhu, Zhangkai Ni, Qi Mao, and Shiqi Wang. Unrolled decomposed unpaired learning for controllable low-light video enhancement. In ECCV, 2024. 4 [91] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In CVPR, 2023. 2 [92] Shaojie Zhuo and Terence Sim. Defocus map estimation from single image. PR, 44(9):18521858, 2011. 12 A. Appendix Overview C.2. BokehNet In this appendix, we provides additional details on the training datasets used in our framework, the construction pipeline of our POINTLIGHT-1K dataset, comparison with the VLM model, and visual comparisons from the BokehNet training ablation study. We also present failure cases and additional in-the-wild results on the accompanying HTML page. B. Visual Comparison for BokehNet Training"
        },
        {
            "title": "Ablation",
            "content": "In the main paper, we compare different training strategies for BokehNet. As illustrated in Fig. 10, under foregroundbokeh setting with comparable bokeh strength in both rows, the model in (a), which is trained only on synthetic pairs, inherits artifacts from the simulator [48] and fails to correctly blur fine structures such as hair along object boundaries. In contrast, the model in (b), which is additionally trained with unpaired real bokeh data, produces smoother and more realistic blur around these edges. C. Training Datasets We summarize the datasets used to train DeblurNet and BokehNet. C.1. DeblurNet For training DeblurNet, we use the DPDD [1] dataset and adopt all images from its official training split. This dataset provides paired defocused and all-in-focus images, which we use to supervise the deblurring stage. We further leverage the REALBOKEH 3MP [60] dataset, which contains images captured with relatively small apertures and thus does not strictly provide all-in-focus views. To obtain images that are sufficiently sharp for our purposes, we compute the Laplacian variance of each image as simple focus measure and rank the dataset accordingly. We then retain the top 3000 images with the highest Laplacian variance as additional training data for DeblurNet. For training BokehNet, we require diverse all-in-focus inputs. We therefore draw all-in-focus images from the [82] and [30] collections. As in the DeblurNet stage, we compute the Laplacian variance for each candidate image and filter out blurry examples. After this filtering, we obtain pool of approximately 1.7k all-in-focus images for simulating bokeh images. The handling of the real bokeh datasets BOKEHDIFFUSION [19] and REALBOKEH 3MP [60] for bokeh synthesis is already described in the main paper. D. PointLight-1K Collection Pipeline We construct the POINTLIGHT-1K dataset to better model scenes with strong point-light bokeh. The pipeline consists of the following steps: 1. Keyword mining. We first mine keywords from photographs on Flickr1 that are tagged with terms such as night or bokeh. These keywords capture typical compositions and scene elements with prominent point lights. 2. Prompt expansion. The collected keywords are then expanded into diverse, natural-language prompts using GPT-4o [47]. This step enriches the textual descriptions while preserving point-light characteristics. 3. Image generation with fine-tuned FLUX. We use finetuned version of FLUX.1-Dev [61] (FLUX.1-dev-LoRAAntiBlur LoRA) to generate images from the prompts. The fine-tuning is designed to encourage sharper scenes. 4. Deblurring with DeblurNet. Despite fine-tuning, we observe that images involving point lights may still exhibit mild residual blur. To further reduce this blur, we pass all generated images through our DeblurNet module, optimizing each image toward an all-in-focus appearance. 5. Final dataset. After this process, we obtain curated set of 1k night-time images with prominent point lights and minimal residual blur. We refer to this dataset as POINTLIGHT-1K and use it as specialized training set to pretrain and refine our learning strategy for controllable 1https://www.flickr.com/ Figure 9. Comparison with VLM model. Qualitative refocusing results comparing our Generative Refocusing framework with Gemini 3 Nano Banana Pro [20] given prompt focus on the man on the right. 13 Figure 10. Visual Comparison for BokehNet Training Ablation. bokeh shape. E. Comparison with VLM model We also compare our method with the recently released visionlanguage model Gemini-3 Nano Banana Pro [20], which explicitly supports text-driven refocusing. For this model, we use the prompt focus on the man on the right to specify the desired focus point. As shown in Fig. 9, Gemini-3 Nano Banana Pro can change the focus to some extent, but it also alters the facial expressions and appearances of people in both the foreground and the background. In contrast, our method preserves the original content while producing realistic bokeh effect and accurate refocusing. F. Failure Cases We observe two main types of unsatisfactory refocusing results: (1) incorrect hallucinated details and (2) inaccurate bokeh blur. For the first type, even though DeblurNet can generate visually plausible high-frequency details, in cases of severe input blur it may hallucinate incorrect detail (see Fig. 11). For the second type, the bokeh blur can be imperfect because our defocus map is derived from monocular depth estimator [7]. When the estimated depth scale is biased, the resulting defocus map may fail to fully cover the intended focus region or may assign too much or too little blur to certain areas. Although our semi-supervised training strategy for BokehNet mitigates these issues to some extent, residual discrepancies with respect to the ground truth can still be observed in challenging cases. 14 Figure 11. Failure case. Due to the severely blurred input, the model hallucinates an incorrect time, changing 11:30 a.m. to 12:30 a.m."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University",
        "University of Maryland, College Park"
    ]
}