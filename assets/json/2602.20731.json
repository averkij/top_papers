{
    "paper_title": "Communication-Inspired Tokenization for Structured Image Representations",
    "authors": [
        "Aram Davtyan",
        "Yusuf Sahin",
        "Yasaman Haghighi",
        "Sebastian Stapf",
        "Pablo Acuaviva",
        "Alexandre Alahi",
        "Paolo Favaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods."
        },
        {
            "title": "Start",
            "content": "Communication-Inspired Tokenization for Structured Image Representations Aram Davtyan 1 Yusuf Sahin 1 Yasaman Haghighi 2 Sebastian Stapf 1 Pablo Acuaviva 1 Alexandre Alahi 2 Paolo Favaro 1 Abstract Discrete image tokenizers have emerged as key component of modern vision and multimodal systems, providing sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), novel framework for learning structured discrete visual token sequences. COMiT constructs latent message within fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within single transformer model and trained end-to-end using combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, the proposed attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods. 6 2 0 2 4 2 ] . [ 1 1 3 7 0 2 . 2 0 6 2 : r 1. Introduction Modern multimodal systems increasingly process vision through the lens of sequence modeling (Sun et al., 2024; images are converted into discrete Chen et al., 2025a): Website: https://araachie.github.io/comit 1Computer Vision Group, University of Bern, Switzerland 2VITA Lab, EPFL, Switzerland. Correspondence to: Aram Davtyan <aram.davtyan@unibe.ch>. Preprint. February 25, 2026. 1 token sequences that can be consumed by transformer-based architectures. This token-based interface enables scalable training, efficient compression, and unified reasoning over visual and textual inputs. Consequently, learning effective image tokenizers has emerged as central problem in visual representation learning. Conventional discrete encoders (Van Den Oord et al., 2017; Esser et al., 2021) typically represent images as twodimensional grids of tokens and are trained primarily for reconstruction under compression constraint. As result, the learned tokens often capture local texture and patch statistics rather than object-level semantic structure, limiting their interpretability and usefulness for downstream image understanding tasks. Recent progress in image tokenization has renewed interest in one-dimensional discrete bottlenecks, which better match the sequential format expected by transformer-based models and can improve the semantic organization of token sequences (Yu et al., 2024a; Wang et al., 2025; Bachmann et al., 2025; Duggal et al., 2024). However, despite this syntactic compatibility, most existing approaches remain primarily optimized for compression trade-offs. As result, semantic information is often entangled and poorly localized across tokens, limiting performance on downstream tasks that require compositional, object-centric structure. In this work, we shift the focus from compression tradeoffs to the semantic organization of visual token sequences. While semantic information can be encouraged through alignment with pretrained visual representations, we argue that such supervision alone is insufficient to induce structured and interpretable tokens. Instead, structured visual tokenization requires coupling semantic learning objectives with an encoding procedure that explicitly encourages compositional organization. Our approach is inspired by how humans communicate about visual scenes. When describing scene containing multiple objects, speaker typically attends to one region at time, sequentially incorporating salient information into the message. This incremental process allows the listener to integrate each observation into coherent mental model of the scene while gradually reducing uncertainty (Hassabis & Maguire, 2007). Under limited communication Communication-Inspired Tokenization for Structured Image Representations bandwidth, descriptions naturally prioritize high-level entities and their relations over fine-grained details, yielding certain coarse-to-fine hierarchy in the space of possible descriptions. Notably, while so far we have considered the speaker and the listener to be two separate roles, humans are able to simultaneously perform both tasks. For instance, one could imagine the setting where the speaker is trying to memorize the scene for certain period of time and then is asked to recall it. Motivated by this perspective, we study how communication-inspired encoding process can shape the structure of learned visual token sequences. In particular, we adopt two key design principles: Attentive and sequential tokenization. The encoder processes the image as sequence of localized observations, attending to different regions at each step and incrementally integrating information into discrete latent message. Homogeneous communication. In contrast to traditional autoencoders that use separate encoder and decoder networks, we adopt unified design in which the same network acts as both speaker and listener, mirroring the symmetry in human communication. Building on these principles, we propose COMmunication inspired Tokenization (COMiT), which formulates image encoding as an iterative communication-and-reconstruction game. At each step, the model observes new image crop and updates its discrete latent message. After several steps, the final message serves as compact representation of the scene, from which the same network reconstructs the full image. The whole pipeline is trained as generative model within the flow-matching framework. To encourage semantic grounding, we incorporate semantic representation alignment objective that distills high-level features from frozen self-supervised vision model. Crucially, while alignment provides semantic signal, the attentive sequential tokenization determines how this information is distributed and localized across tokens. To evaluate these properties, we introduce suite of benchmarks that probe not only semantic content, but also compositional generalization and relational reasoning. Across these evaluations, COMiT consistently outperforms existing one-dimensional discrete image encoders by substantial margin. Ablations further reveal complementary effects: semantic alignment improves token meaning, while communication-inspired sequential encoding induces more interpretable, object-centric tokens. 2 2. Related Work Attentive Encoding. The idea of aggregating information from partial observations of scene has long history. One early example is the Recurrent Attention Model (RAM) proposed by Mnih et al. (2014), where model iteratively selects regions of an image to attend to and accumulates latent state to solve classification. In that work, the primary motivation was computational efficiency on large images, rather than representation learning or reconstruction. In other early approaches (Eslami et al., 2016; 2018) the reconstruction serves as the main training signal. However, often the latent state in such models explicitly encodes inductive biases, such as object-level bounding box coordinates (Eslami et al., 2016), rather than abstract concepts. Moreover, the early methods were only tested on rather toy data, and, to the best of our knowledge, there is no evidence on scaling these to real images. Image Tokenization. Early works such as VQ-VAE (Van Den Oord et al., 2017; Razavi et al., 2019) introduced discrete visual codebooks learned via vector quantization. Subsequent approaches, including MaskGIT (Chang et al., 2022) and VQ-GAN (Esser et al., 2021), improved generative fidelity through masked prediction and adversarial training. More recently, several works have explored onedimensional tokenization schemes, with Yu et al. (2024a) pioneering this direction. Many of these methods, however, continue to treat tokenization primarily as compression and reconstruction objective, producing tokens in single encoding pass without explicit mechanisms for sequential refinement. Other works have investigated how token ordering can reflect properties of the underlying visual signal, such as semantic hierarchies (Bachmann et al., 2025) or frequency structure (Wang et al., 2025). Closest to our approach, Duggal et al. (2024) also introduce an iterative recurrent procedure for refining the latent representation. In their setting, however, the first encoding step is designed to capture most of the image content, while subsequent refinement steps use additionally allocated capacity. Overall, existing tokenizers remain largely optimized for reconstruction-compression trade-offs rather than structure and semantics. Flow Matching and Diffusion Autoencoders. Flowmatching generative models (Lipman et al., 2023; Liu et al., 2022) offer stable and efficient training of continuous data distributions. These frameworks have also been used to train decoders in autoencoders (Preechakul et al., 2022; Guo & Schwing, 2025; Bachmann et al., 2025; Chen et al., 2025b). However, to the best of our knowledge, we are the first to leverage this formulation to train both encoding and decoding stages through unified, differentiable flow objective, integrating representation learning and generation into single network. Communication-Inspired Tokenization for Structured Image Representations 3. Communication Inspired Tokenization Algorithm 1 The training pipeline of COMiT In this section we start by describing the high-level training pipeline of COMiT that is illustrated in Figure 1. We then talk about some crucial details in Sections 3.13.5. Encoding. Given an input RGB1 image R3HW , we k=1, ck R3hw define sequence of random crops {ck}K and their corresponding locations {lk}K k=1, lk [1, 1]2. Here lk is the absolute location of the ck crops center in the image with each component normalized to [1, 1]. From these, we derive the sequence of offsets {ak}K k=1, where a1 = (0, 0) and ak = lk lk1, 2. The offsets can be interpreted as actions the agent takes to scan the scene. We opt for relative offsets instead of global grounding to prevent parts of the latent message to specialize on certain constant regions. In the beginning, the latent message m0 RLd is initialized with the same d-dimensional vectors from dictionary of unique tokens. We refer to as the message length and as the vocabulary size. After that, at each step, the model fθ takes as input the current latent message mk1 and updates it with the information from the current crop: mk = θ (ck, tk, ak, mk1). (1) Here, the superscript in θ denotes the readout part of the models output that corresponds to the updated message. This is needed to support different output modalities that we introduce below. The second argument in θ corresponds to the denoising timestamp, which is tk = 1 for the clean crops. This is needed to distinguish the encoding from the decoding regime, which we also introduce below. is The predicted message mk then quantized via FSQ (Mentzer et al., 2024) to project the tokens to the vocabulary and fed back to the model along with the newly observed crop, forming recurrent loop. After steps, the final message mK is obtained. Decoding. The message derived through the above procedure of recurrent crop aggregation serves as conditioning for flow-based decoding of the full image. However, in contrast to the conventional autoencoder architectures, COMiT uses the same model for both encoding and decoding. That is, we start by creating noisy version of the image xt = tx + (1 t)ε with [0, 1] random interpolation time and ε (0, I) random noise instance. Then, the noisy image xt is fed to the network along with the final message mK and the offset relative to the last crop ag = lK. The network is then supervised to predict the velocity of the marginal flow at xt via the standard conditional 1In fact, COMiT works in the latent space of pretrained 2D VAE, but we omit the VAE in our notation as it does not affect any of the training steps. for in dataloader do Pick random from {1, . . . , Kmax}; Randomly crop to get {ck}K k=1; With probability pG set c1 = and l1 = (0, 0); Calculate the offsets: k=1 and {lk}K a1 = (0, 0), ak = lk lk1, ag = lK; (3) Initialize the latent message m0; for 1, . . . , do Update the message: mk = θ (ck, 1, ak, sg[mk1]); (4) end for With probability pCFG set mK = m0; Calculate the loss according to Equation 7; Update θ with L; end for flow-matching loss (Lipman et al., 2023): LFM = tU [0,1] εN (0,I) θ (xt, t, ag, mK) (x ε)2 2 . (2) Here, as before, the superscript in θ denotes the readout part of the models output that corresponds to the estimated velocity. Inference. Given latent message ˆm, in order to visually probe its content, one may decode the message through numerically integrating the flow ODE (Lipman et al., 2023). That is, starting at noise sample x0 (0, I), one obtains the clean image x1 via iteratively denoising the current estimate: xt+h = xt + hf θ (xt, t, ag, ˆm), (5) where is the step size connected to the number of function evaluations (NFE) via = 1/NFE. 3.1. Greedy Use of Tokens As noted in the introduction, as the amount of information embedded in the latent message increases (in our case, the number of crops), it is natural to expect the model to discard irrelevant secondary details and retain only the most essential features for reconstruction. This process would naturally induce hierarchical structure in the feature space. However, if during training the number of crops the model aggregates were fixed, the network would instead learn to pre-allocate parts of the latent message for future crops. This would effectively enforce fixed capacity per crop, with each information chunk occupying predetermined location 3 Communication-Inspired Tokenization for Structured Image Representations Figure 1. The overall training pipeline of COMiT. sequence of random crops is extracted from the input image and iteratively embedded into the latent message mK that is discretized via FSQ (Mentzer et al., 2024). The latter is decoded by the same model using the flow matching objective (Lipman et al., 2023). Additionally, we use REPA (Yu et al., 2024b) to speed up the training and SREPA to inject more semantic priors into the latent message. in the message. To avoid this behavior, we randomize the number of crops the model processes during training. As result, the model never knows whether additional crops will follow the current one and is therefore encouraged to use the available tokens greedily. This corresponds to the setting with fixed capacity and varying amount of information, which is precisely the regime we were aiming for. In addition, backpropagating gradients through the entire sequence of message updates would be computationally expensive and memory-intensive. To address this, we apply stop-gradient operation to all updates except the final one. Combined with the randomized number of crop aggregations, this further promotes greedy token usage while enabling efficient training. In preliminary small-scale experiments, we found that restricting gradient backpropagation in this way has only moderate impact on performance. 3.2. Distilling Semantic Representation As noted in the introduction, to enforce the semantic ordering of the information in the learned messages, we directly distill pretrained SSL features ψ(x) Rs(namely, the [CLS] of DINOv2 (Oquab et al., 2023)) into COMiTs intermediate representations that correspond to the message tokens. We call this procedure semantic representation alignment, or SREPA. More precisely, during the decoding stage, the intermediate representations in the j-th layer of the netθ [j](xt, t, ag, mK) RLr are first projected with work small MLP and then averaged pooled into single vector θ [j](xt, t, ag, mK) Rs. Then, the following loss is calculated: LSREPA = exp (cid:0)Sim (cid:0)ψ(x), θ [j](xt, t, ag, mK)(cid:1)(cid:1) , (6) where Sim(, ) is the cosine similarity. 3.3. Reconstruction Fidelity Although our main focus in this paper is on semantics of the token sequences, we apply several techniques to improve the reconstruction fidelity of COMiT. First, to enable classifierfree guidance (CFG) (Ho & Salimans, 2022) at inference, during training we skip the encoding part with probability pCFG 0.2, thereby training an unconditional decoder. Besides this, with probability pG 0.5 we replace the first crop with the full original image, allowing the model to encode the whole image in single step. For clarity, we refer to the full image as the global crop, and to the smaller crops as local crops. As result, the training consists of three regimes: in some cases, COMiT first observes the global crop and then refines its latent message with local crops; in others, it observes only local crops; and the third portion of the training is devoted to the decoding of the image from an empty message. 3.4. Implementation Details In practice, COMiT is implemented as transformer encoder network (Vaswani et al., 2017). Following the standard practices of DiT (Peebles & Xie, 2023), we use AdaLN (Perez et al., 2018) layers to make the network timestamp-conditioned. Notably, different modalities (image, message) use separate projections in the AdaLN layers. All inputs to the network are flattened into single sequence of tokens. Images are first encoded with pretrained VAE (namely, the KL-regularized VAE of Stable Diffusion (Rombach et al., 2022)) to reduce the input size and then patchified. The offset is embedded into single token via linear projection. We found that the model performs slightly better when, instead of overwriting the previous message, one feeds both the previous message and newly initialized buffer Communication-Inspired Tokenization for Structured Image Representations tokens to predict the next message (see also Figure 1). This architectural design also allows us to apply causal masks in the attention maps of the buffer tokens, mimicking the causal nature of composing sentences (illustrated in Figure 1 as the tilted line of the gray shaded region in COMiTs main backbone). We train COMiT in three model sizes: (12 layers, hidden size 768), (24 layers, hidden size 1024) and XL (28 layers, hidden size 1152). All models are trained on ImageNet1k (Deng et al., 2009). To speed up the convergence, we use REPA (Yu et al., 2024b) that aligns the intermediate image representation in the network with the DINOv2s spatial features. The final loss thus takes the form: = LFM + λREPALREPA + λSREPALSREPA. (7) In our experiments, we choose λREPA = λSREPA = 0.5. The whole training procedure is summarized in Algorithm 1 and Figure 1. Other implementation details can be found in Appendix A. 3.5. Cropping Policies Because both the crops and their number are randomized during training, COMiT offers flexibility at inference time in how crops are selected when encoding an image. We refer to these choices as cropping policies. Although many cropping policies are possible, in this paper, we study only limited subset to illustrate the impact of the main design decisions, leaving more exhaustive exploration to future work. For simplicity, we resize all images to resolution of 256 256 and consider only 96 96 crops arranged on 3 3 grid. We characterize cropping policies by three factors: (i) whether global crop is included at the beginning of the crop sequence, (ii) the number of crops, and (iii) the order in which the crops are processed. With respect to the latter, we consider random ordering, fixed raster-scan order, and an adaptive policy. The adaptive policy decodes the current latent message after each newly aggregated crop and selects the next crop to be the one that has the largest mean reconstruction error. The intermediate reconstructions are obtained using single decoding step, resulting in blurry regions in areas where the model is uncertain about the content (see Figures 4 and 3). 4. Experiments In this section, we evaluate COMiT to showcase its improved semantic encodings and emergent properties. We design the test suite of three representative benchmarks. Visual recognition (ImageNet100). To evaluate whether COMiT encodes high-level semantic information useful for visual recognition, we perform classification probing on ImageNet100 (IN100) (Deng et al., 2009). Instead of standard linear probing, we adopt an attention-based probing strategy using lightweight two-layer of self-attention operating over the 1D sequence of tokens produced by our model and the baselines. This design allows the probe to flexibly attend to and aggregate information across the entire token sequence, without assuming that class-discriminative features are localized at fixed position. Attention probing is therefore better suited for the 1D tokenization setting, where semantic information may be distributed across tokens. Compositional generalization (MSCOCO). We evaluate compositional generalization using benchmark derived from MSCOCO (Lin et al., 2014), focusing on images that contain at least two object categories. The dataset is split into two disjoint partitions such that both splits share the same set of object categories, while specific object cooccurrences (i.e., object pairs) appear in only one split. We train the probing network on the first split and evaluate it on the second, ensuring that the probe must generalize to unseen object compositions. This setup tests whether objectrelated information is stored in disentangled manner across the token sequence (e.g., distributed across different subsets of tokens corresponding to individual objects, rather than being collapsed into single token that encodes the entire scene). Indeed, if the model mixed the information about two objects in single token, the probe would have more difficult job to effectively generalize to unseen object pairs. Inter-object relations (Visual Genome). We evaluate the ability of models to encode relational semantics using relationship prediction on Visual Genome (VG) (Krishna et al., 2017). Given batch of images paired with candidate subjectobject relations, we test whether the probing network can correctly assign each image to its corresponding relation. This benchmark assesses whether inter-object relationships are explicitly represented in the token sequence, beyond individual object semantics, and whether such relational information can be recovered through lightweight probing. For more details on our testing suite, we refer the reader to the Appendix B. 4.1. Ablations Semantic distillation. We evaluate the impact of SREPA by comparing the full COMiT-B model against an ablated variant in which λSREPA = 0 during training. Both models are evaluated under identical settings on IN100. We observe that SREPA has significant contribution to our tokenizers performance (see Table 1). Note that since FSQ uses the same codebook across model variants, all the differences in performance may be attributed to token co-occurrences. Attentive tokenization. Next, we evaluate the influence of the presence of local crops during the training. To this end, we train COMiT-B with and without local crops. The 5 Communication-Inspired Tokenization for Structured Image Representations Table 1. The effect of SREPA during training. Both models use the same cropping policy (single global crop) at test time. Input images Model variant SREPA ImageNet100 (top-1) COMiT-B COMiT-B 82.91 72.26 Table 2. The effect of using recurrent local crops aggregation during training. At test time, both models use the same cropping policy with single global crop. Model COMiT-B COMiT-B Local crops at training ImageNet100 (top-1) 82.91 80.94 variant trained without local crops lacks recurrent message refinement and effectively reduces to standard encoderdecoder architecture with discrete bottleneck. First, we evaluate both variants on visual recognition benchmark using the same cropping policy at inference time, which encodes only the global crop. Although local crops are not used to construct the latent message at test time, the model trained with the attentive communication bias produces better structured latent messages as reflected with the probing accuracy shown in Table 2. Next, we analyze token attention maps following Duggal et al. (2024). Using CSSD (Yan et al., 2013), we binarize each attention map by retaining the top Q% of values and select, per image, the token whose map attains the highest IoU (Everingham et al., 2010) with the ground-truth mask. Figure 2 compares these token attention maps for COMiT-B and variant trained without local crops. COMiT produces attention maps tightly aligned with objects, achieving mean mIoU of 0.53, whereas the ablated model exhibits diffuse attention across the image and reaches only 0.34 mIoU. This alignment emerges from our attentive tokenization pipeline, indicating that while distillation enforces semantics, tokenization pipeline is key to structuring messages. Additional results are provided in Appendix C. Cropping policy. Table 3 compares different cropping policies. single global crop yields competitive average performance across tasks while incurring the lowest test-time cost (the number of crops). Based on this observation, we adopt global-only cropping as the default for the remainder of our probing experiments unless stated otherwise. Importantly, COMiT also naturally supports test-time scaling: adding local crops provides modest gains on compositional generalization and inter-object relations, suggesting that local processing can benefit certain benchmarks. Notably, all differences across cropping policies arise purely at test-time, without retraining the model. Ground truth masks COMiT-B, mIoU = 0.53 no local crops, mIoU = 0.34 Figure 2. The effect of our attentive tokenization pipeline on the tokens visual grounding. The model that has been trained with attentive tokenization demonstrates much better tokenobject alignment compared to the variant of the model that has only seen global crops at training. The token sequences for both models were obtained from the 10th layer of COMiT-B, using the same cropping policy that embeds only the global crop. 4.2. Quantitative Results We evaluate COMiT against relevant prior work in the 1D tokenization domain (see Table 4). We report results on our semantic probing test suite along with rFID (Heusel et al., 2017) and PSNR (Huynh-Thu & Ghanbari, 2008) on ImageNet1k (Deng et al., 2009) validation set. All methods, if not stated otherwise, were pretrained on ImageNet1k (Deng et al., 2009). COMiT consistently outperforms prior work on semantic tasks. Similarly to prior work (Bachmann et al., 2025), we observe that scaling the model from to improves both reconstruction and representation, and further scaling the model from to XL allocates the additional capacity to aid reconstruction, while diminishing the semantics. Notably, our design choice was to avoid encoder-decoder separation and allow the model to autonomously decide which part of the networks capacity to dedicate to which task. This choice mimics the symmetry of the communication game, eases development by removing degree of freedom in designing the architecture and potentially removes redundancies in the models parameters. We believe the reconstruction fidelity of our method can be further improved by additional training. Many methods implement multi-stage training pipelines, including finetuning the decoding for better reconstruction (Yu et al., 2024a; Chang et al., 2023). We leave further exploration of such fused architectures, including the analysis of how different tasks are balanced in single network, to the future work. 6 Communication-Inspired Tokenization for Structured Image Representations Table 3. The effect of cropping policy using COMiT-B. single global crop provides the best overall trade-off between performance and test-time cost, while some tasks benefit from embedding additional local crops, highlighting the potential for future investigation of optimal (task-adaptive) cropping policies. Global crop - - # Crops Ordering IN100 MSCOCO top-1 top-5 VG top-1 1 10 9 10 9 3 3 Random Random Raster-scan Raster-scan Adaptive Adaptive 82.91 82.30 74.45 82.22 80. 82.09 74.96 41.46 40.21 38.92 41.83 38.61 40.68 32.70 52. 52.12 54.96 55.78 51.10 55.00 50.22 Figure 3. The difference between the adaptive (top) and global+adaptive (bottom) cropping policies. In both cases COMiT aggregates the crops of the input image (the leftmost column) into the latent message and decodes it to obtain the reconstructed image (the rightmost column, 10 NFE with CFG = 7.5). The columns in-between show which crops are selected together with immediate single step reconstructions (1 NFE with CFG = 1.0). The progressive ambiguity reduction as more crops are integrated in the latent message is particularly visible with the single step decoding. 4.3. Qualitative Results Evolution of Uncertainty. One way to visually probe the information encoded in the latent messages is via reconstructing the images. However, in the case when only local crops are used to encode the message, 12 crops may not be enough to reliably reconstruct the image. Since COMiT is generative model of images given the message, it will hallucinate the content in the unobserved regions. However, the flow matching framework guarantees that at = 0 the optimal velocity field is the one that points to the mean of the data that shares the same conditioning. Therefore, decoding messages with single denoising step results in images that have sharp details in the recently observed regions and blurry content in the regions where the model is not certain about the scene. Figures 3 (top) and 4 demonstrate how the information in the latent message is gradually refined with more crops aggregated. This process is also inherently compositional. Figure 4 shows how objects are added to the latent messages as soon as observed. Global vs. Local crops. Figure 3 additionally demonstrates the difference between adaptive cropping policies in the absence and the presence of the global crop. When the global crop is leveraged, it provides way to quickly embed the whole scene into the latent message with the subsequent local crops allowing to focus on the finer details (such as the shape and the color of the feathers near the birds head). More visual results can be found in Appendix D. 5. Conclusion and Discussion In this paper, we introduced COMiT, communicationinspired approach to one-dimensional image tokenization that promotes structured visual representations. By framing tokenization as an iterative communication-andreconstruction process, COMiT constructs discrete latent message through attentive sequential observations, encouraging semantic information to be incrementally organized across tokens rather than entangled in single pass. 7 Communication-Inspired Tokenization for Structured Image Representations Table 4. We evaluate COMiT and several baseline 1D tokenizers on our test suite comprising three representative benchmarks described in Section 4. COMiT outperforms prior work on semantic probing. This highlights representation-reconstruction trade-off that contrasts with the conventional compression-reconstruction trade-off targeted in prior work. : trained on larger data mixtures. Method #params msg length TiTok-L (Yu et al., 2024a) TiTok-B (Yu et al., 2024a) TiTok-S (Yu et al., 2024a) ALIT (Duggal et al., 2024) FlexTok(d12-d12) (Bachmann et al., 2025) FlexTok(d18-d18) (Bachmann et al., 2025) FlexTok(d18-d28) (Bachmann et al., 2025) SelfTok (Wang et al., 2025) SelfTok (Wang et al., 2025) COMiT-B (ours) COMiT-L (ours) COMiT-XL (ours) 614M 172M 44M 229M 254M 860M 2.5B 2.17B 2.17B 174M 610M 900M 32 64 128 256 256 256 256 512 1024 256 256 256 voc. size 212 212 212 210 64k 64k 64k 215 216 64k 64k 64k IN1k rFID PSNR IN100 MSCOCO top-5 top-1 VG top-1 2.21 1.70 1.71 7.39 4.20 1.61 1.45 0.70 0.54 11.06 3.67 3.50 15.60 16.80 17.52 19.76 18.41 18.59 18.46 24.14 26. 17.75 17.81 17.83 17.26 19.43 19.47 29.43 80.25 81.54 80.93 40.64 35.90 82.91 85.80 84.69 6.22 12.64 6.30 8.56 38.17 38.58 39.14 17.91 17.32 41.46 45.31 39.45 26.06 27.31 26.81 32.14 53.75 54.46 54.35 37.15 36. 52.11 56.42 55.61 Figure 4. The way COMiT adds information to the latent message is inherently compositional. To evaluate these properties, we proposed suite of benchmarks designed to probe semantic grounding, compositional generalization, and relational reasoning in learned token sequences. Across these evaluations, COMiT consistently outperforms prior 1D discrete image encoders. Qualitative analysis further illustrates how the model progressively refines its latent message over encoding steps, while ablations on attention organization reveal the role of attentive tokenization in emerging compositional structure of the token sequences aligned with meaningful regions of the scene. While the primary focus of this work is on semantic organization, reconstruction fidelity could be further improved, which may be important for generation-focused applications. Nevertheless, we believe that structured discrete token sequences such as those learned by COMiT provide promising interface for multimodal architectures, especially in settings where object-centric reasoning and compositional understanding are critical. Moreover, the flexibility of COMiT at test time (via different cropping policies) opens up new directions for adaptive and task-dependent visual tokenization (e.g. via reinforcement learning). Future work includes extending COMiT to video, where temporal redundancy and long-range structure pose additional challenges and opportunities for discrete representation learning. Incorporating spatiotemporal observations and allowing the latent message to accumulate motion and action-related information over time may enable structured tokenizations that support efficient video understanding. 8 Communication-Inspired Tokenization for Structured Image Representations"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported as part of the Swiss AI Initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project IDs a144 and a137 on Alps. Aram Davtyan, Sebastian Stapf and Pablo Acuaviva have been supported by Swiss National Science Foundation (SNSF) Project 10001278. Yasaman Haghighi has been supported by Swiss National Science Foundation (SNSF) Project 10003100."
        },
        {
            "title": "Impact Statement",
            "content": "This paper advances research in autoencoders, representation learning and generative modeling. As with other generative image models and tokenizers, the methods studied here may carry standard societal and ethical risks. These include the potential misuse of generated content for creating misleading or deceptive media, the amplification of biases present in training data, and the environmental costs associated with training and deploying large-scale models. While this work is primarily methodological, we acknowledge these well-documented considerations and emphasize that responsible dataset curation, evaluation, and deployment practices are important to mitigate such risks."
        },
        {
            "title": "References",
            "content": "Bachmann, R., Allardice, J., Mizrahi, D., Fini, E., Kar, O. F., Amirloo, E., El-Nouby, A., Zamir, A., and Dehghan, A. Flextok: Resampling images into 1d token sequences of flexible length. In Forty-second International Conference on Machine Learning, 2025. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K., Freeman, W. T., Rubinstein, M., et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., and Ruan, C. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025a. Chen, Y., Girdhar, R., Wang, X., Rambhatla, S. S., and Misra, I. Diffusion autoencoders are scalable image tokenizers. arXiv preprint arXiv:2501.18593, 2025b. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Duggal, S., Isola, P., Torralba, A., and Freeman, W. T. Adaptive length image tokenization via recurrent allocation. In First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models, 2024. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Hinton, G. E., et al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in neural information processing systems, 29, 2016. Eslami, S. M. A., Rezende, D. J., Besse, F., Viola, F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A., Danihelka, I., Gregor, K., Reichert, D. P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N., King, H., Hillier, C., Botvinick, M., Wierstra, D., Kavukcuoglu, K., and Hassabis, D. Neural scene representation and rendering. Science, 360(6394):12041210, 2018. doi: 10.1126/science.aar6170. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., and Rombach, R. Scaling rectified flow transformers for highresolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=FPnUhsQJ5B. Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International journal of computer vision, 88 (2):303338, 2010. Geiping, J., McLeish, S., Jain, N., Kirchenbauer, J., Singh, S., Bartoldson, B. R., Kailkhura, B., Bhatele, A., and Goldstein, T. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. Guo, P. and Schwing, A. G. Variational rectified flow matching. arXiv preprint arXiv:2502.09616, 2025. Hassabis, D. and Maguire, E. A. Deconstructing episodic memory with construction. Trends in cognitive sciences, 11(7):299306, 2007. 9 Communication-Inspired Tokenization for Structured Image Representations Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Huynh-Thu, Q. and Ghanbari, M. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44 (13):800801, 2008. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/ 1412.6980. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Preechakul, K., Chatthee, N., Wizadwongsa, S., and Suwajanakorn, S. Diffusion autoencoders: Toward meaningful and decodable representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1061910629, 2022. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Razavi, A., Van den Oord, A., and Vinyals, O. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Sadat, S., Hilliges, O., and Weber, R. M. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Loshchilov, I., Hutter, F., et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5(5): 5, 2017. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: Vq-vae made simple. In The Twelfth International Conference on Learning Representations, 2024. Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. Recurrent models of visual attention. Advances in neural information processing systems, 27, 2014. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, B., Yue, Z., Zhang, F., Chen, S., Bi, L., Zhang, J., Song, X., Chan, K. Y., Pan, J., Wu, W., et al. Selftok: Discrete visual tokens of autoregression, by diffusion, and for reasoning. arXiv preprint arXiv:2505.07538, 2025. Yan, Q., Xu, L., Shi, J., and Jia, J. Hierarchical saliency In Proceedings of the IEEE conference on detection. computer vision and pattern recognition, pp. 11551162, 2013. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. 10 Communication-Inspired Tokenization for Structured Image Representations Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940128966, 2024a. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. 11 Communication-Inspired Tokenization for Structured Image Representations A. Additional Implementation Details In this section, we provide additional implementation details critical for reproducing the results in the paper. The code and the pretrained models are publicly available at https://github.com/Araachie/comit. Model architecture. As noted in the main paper, the architecture of COMiT is based on DiT (Peebles & Xie, 2023). The only substantial difference is that we use different set of AdaLN (Perez et al., 2018) layers to modulate the image and the message tokens. This is similar to (Yang et al., 2024), where different modulations were applied to text and video tokens. Other architectural details, such as the number of layers and transformer dimensions can be found in Table 5. Training hyperparameters. All models are trained on full ImageNet1k (Deng et al., 2009) using Adam (Kingma & Ba, 2017) optimizer with base learning rate of 3e-4 and square root decay schedule with 5000 iterations of warmup. The and variants were trained with batch size of 512 images, while for the XL variant we used batch size of 256 with 2 gradient accumulation steps, resulting in the same effective batch size of 512 images. Our models were trained for 200 epochs on 32 GH200 GPUs. We use EMA with decay 0.999. For convenience, all hyperparameters are summarized in Table 5. Depth Hidden size Heads MLP ratio Representation layer Number of MLP layers MLP hidden size Number of message tokens Token dimensions Vocabulary size FSQ Levels COMiT-B COMiT-L COMiT-XL DiT parameters 12 768 12 4 REPA/SREPA parameters 4 3 768 Bottleneck parameters Other training parameters 24 1024 16 4 4 3 768 28 1152 16 4 4 3 768 256 6 64000 [8, 8, 8, 5, 5, 5] Flow matching time distribution Cropping policy Mode of number of crops Kmode Maximum number of crops Kmax Number of updates to backpropagate through pCFG pG Optimizer Betas ε Weight decay Learning rate Scheduler Warmup steps Gradient clipping Epochs Batch size Gradient accumulation steps EMA decay logitnormal randomized 1 9 1 0.18 0.55 Adam (Kingma & Ba, 2017) [0.9, 0.999] 1e-8 0.0 0.0003 square root 5000 1.0 200 512 1 0.999 Table 5. Architecture and training details of COMiT variants. 512 1 256 2 12 Communication-Inspired Tokenization for Structured Image Representations Cropping policy at training. During training the input images are of resolution 256 256. The local crops of resolution 96 96 are extracted at random locations within the image, such that the local crops do not go beyond the image domain. The number of crops is randomly selected from {1, . . . , Kmax}. We found that instead of uniformly selecting it is better to slightly skew its distribution towards 1. More precisely, similarly to Geiping et al. (2025), we use the following sampling scheme: = min(max(1, ξ + 1), Kmax), where ξ Poisson(eτ ), and τ = ln(Kmode) + ε 0.5 0.125 (8) Flow matching timestamp. In the flow matching framework (Lipman et al., 2023), the default choice for the timestamp sampling distribution is uniform on the unit interval [0, 1]. However, in prior work it has been discovered that sometimes it is better to skew the timestamp distribution towards the ends of the interval (Esser et al., 2024). We found that it is better to skew the distribution of the timestamps towards 0. This prioritizes high noise regimes and allows for better training of the encoder. We use the logitnormal distribution with σ2 = 1 and µ = 1. Message handling. For initializing the message tokens we pick one fixed token from the vocabulary (namely with id 32000) and use it to initialize all tokens in m0. Prior to feeding the message tokens to the DiT, we embed them using 2 layer MLP with SiLU (Elfwing et al., 2018) activation in between. As noted in the main paper, at each message update step, we append the current message with new one that acts as buffer to write the aggregated information from the message and the crop to. The tokens in the buffer message are copies of learned embedding. Position encodings. Each modality in the input uses its own set of position encodings. The message and the offset tokens are augmented with learned position encodings. The image tokens use 2D sinecosine position encodings. We define some maximum resolution of the image input (depends on the resolution of the images, the downsampling factor of the VAE and the patch size of the DiT). Each crop only uses the top-left corner of the 2D image position encodings. Classifier-free guidance. To smoothen the dependence of the metrics on the CFG strength and allow for high values of the latter, similarly to (Bachmann et al., 2025), we use the Adaptive Projected Guidance (Sadat et al., 2024) with the rescaling threshold as = 2.5, parallel component as η = 0, and momentum as β = 0.5. B. Evaluation Details B.1. Test Suite We evaluate the ability of image tokenizers to encode semantic content using probing protocol. Images are first tokenized using the evaluated tokenizers, and the resulting image-token sequences are kept frozen. For each benchmark, we train lightweight probe with identical capacity across tokenizers to ensure fair comparison. Across all tasks, we apply early stopping based on validation performance and report results from the best-performing checkpoint. ImageNet100. We evaluate category-level semantic information using ImageNet100 as single-label classification task. Given frozen image tokens, the probe is trained to predict one of 100 object categories using cross-entropy objective. Performance is measured using top-1 accuracy on the validation set. MSCOCO. We assess compositional generalization using multi-label classification task derived from MSCOCO. We retain only images containing exactly two object categories and construct pair-disjoint training and validation splits such that object category pairs observed at validation time are never seen during training. Each image is annotated with 2-hot target vector over 90 categories. The probe is trained with binary cross-entropy loss, and performance is reported using top-5 accuracy, requiring both ground-truth categories to appear among the top-5 predictions. Visual Genome. We evaluate relational reasoning on Visual Genome, which provides supervision in the form of (subject, predicate, object) triplets with large and open-ended vocabulary. Training and validation splits are constructed to be image-disjoint to avoid visual leakage. To ensure meaningful probing signal, we filter out relations involving objects that occupy less than 5% of the image area and restrict the vocabulary to the 150 most frequent subject/object categories and predicates. This preprocessing avoids evaluation failures caused by out-of-vocabulary textual concepts at validation time, allowing the probes performance to more faithfully reflect the semantic information encoded in the image tokens rather than limitations of the classifiers label space. For each image, the probe is trained to distinguish the correct triplet from set of negative triplets sampled from other images using contrastive objective. At evaluation time, accuracy is reported based on whether the correct triplet is identified among its associated negatives. 13 Communication-Inspired Tokenization for Structured Image Representations"
        },
        {
            "title": "Model dim Depth Heads Global Batch Size",
            "content": "ImageNet100 MSCOCO Visual Genome 2 2 2 Table 6. Probe hyperparameters used for all benchmarks. 768 768 64 512 256 512 8 8 8 B.2. Probing Network Design All probes operate on frozen image-token sequences and use shallow Transformer encoder to aggregate information across tokens. Image tokens RLd are first projected to shared model dimension, and learnable [CLS] token is prepended to the sequence. Learned positional embeddings and discrete type embeddings are added to all inputs before applying self-attention. Predictions are obtained from the final [CLS] representation. For ImageNet100 and MSCOCO, the input sequence consists only of the [CLS] token followed by the projected image tokens. two-way type embedding distinguishes image tokens from [CLS]. The Transformer output corresponding to [CLS] is passed to linear classification head, producing either single-label logits (ImageNet100) or multi-label logits (MSCOCO). For Visual Genome, the probe additionally conditions on candidate (subject, predicate, object) triplet. Subject, object, and predicate text are encoded using frozen T5 encoder (Raffel et al., 2020) and independently projected to the model dimension. These embeddings are appended to the image-token sequence, and five-way type embedding is used to distinguish image tokens, textual roles, and [CLS]. The Transformer encoder processes the full sequence jointly, and linear head maps the [CLS] representation to single logit representing imagetriplet compatibility. B.3. Hyperparameters All probes are trained using the AdamW (Loshchilov et al., 2017) optimizer with fixed learning rate, which we keep constant across benchmarks to ensure comparability. For Visual Genome, we use smaller model dimension than for ImageNet100 and MSCOCO due to the increased input length from concatenating textual triplet embeddings and the substantially larger effective batch size required for contrastive training with multiple negatives, which together impose higher memory and compute demands. An overview of the hyperparameters is given in Table 6. (a) rFID vs. NFE with CFG = 3. (b) rFID vs. CFG with NFE = 10 (c) Ablation of the bottleneck size Figure 5. (a) and (b): Ablation of the reconstruction fidelity under different sampling hyperparameters. (c): Ablation of the bottleck size with COMiT-B. B.4. Reconstruction. To select the optimal parameters for the NFE and CFG we start by fixing CFG = 3.0 and sweep the NFE in {2, 4, 10, 25} (see Figure 5a). We find NFE = 10 to be the optimal number of evaluation steps across all model sizes. We observed that increasing NFE typically leads to sharper details and more salient artifacts, which slightly increases rFID. We then fix NFE = 10 and sweep CFG in {1.5, 2.5, 3.0, 3.5, 4.5, 7.5, 10.5} (see Figure 5b). We observe the rFID curves behave smoothly under the changes in CFG, with CFG = 7.5 yielding the lowest rFID with COMiT-L and -XL. Therefore, for all Communication-Inspired Tokenization for Structured Image Representations Figure 6. Emergence of object-centric tokens in COMiT. We visualize the attention maps of specific tokens in one of the deep layers of the network during the decoding stage. One can see that the tokens naturally attend to the objects and their parts. our experiments we fix NFE = 10 and CFG = 7.5. Notably, scaling COMiT from to leads to significant drop in rFID, while further scaling from to XL only moderately improves the reconstruction quality. B.5. Bottleck size. We additionally ablate the bottleneck size of COMiT. We train 3 variants of COMiT-B with small (S), medium (M) and large (L) bottleneck sizes, where corresponds to the message length of 64 tokens from vocabulary of 1k unique entries, stay for the message length of 128 tokens and 16k vocabulary size, while is the default configuration in the main paper with 256 tokens in the messages and 64k tokens in the vocabulary. We then report the classification probing accuracies of these variants along with the reconstruction PSNRs (see Figure 5). We observe that the accuracy stays within the same range, while PSNR keeps increasing. This may indicate the potential to further improve the reconstruction fidelity of the model without loosing the probing capabilities. C. Analysis of the Attention Maps In this section, we extend the preliminary analysis of the tokens attention maps demonstrated in the main paper. Emergence of Objectness. Similarly to (Duggal et al., 2024), we study the emergence of object discovery via visualizing the attention maps of message tokens to the image tokens in the 24th layer of COMiT-XL during the decoding process. First, we visually investigate the attention maps and observe that COMiTs tokens tend to correspond to some semantically meaningful regions in images, such as objects or object parts (see Figure 6). Quantifying Objectness. To quantify these findings, we follow the same procedure described in the main paper. In Figure 8, we visualize the attention maps with 10%, 20%, 30% and 40% thresholding along with the mean IoUs for the whole CSSD dataset (Yan et al., 2013). We found that with 30% thresholding one can obtain 0.58 mIoU, which is considered remarkable for model that has never seen segmentation maps or object categories during training. The attention maps are extracted from layer 24 at denoising time = 0.1 using COMiT-XL. D. More Visual Results Here we provide more qualitative results with COMiT that could not be included in the main paper due to the pages number limit. If not stated otherwise, the input images in the main paper and in this appendix are random images from unsplash.com. We observed that despite being trained only on ImageNet1k (Deng et al., 2009), COMiT demonstrates Communication-Inspired Tokenization for Structured Image Representations Figure 7. Generalization of COMiT to other domains, such as rendered animations or medical images. It can be seen that the model has certain symmetry bias that allows it to reduce the uncertainty about the right hand side of the image when the content on the left hand side is observed. Also notice how the adaptive cropping policy selects the most critical regions for reconstruction. remarkable generalization capabilities and remains robust under domain shifts (see Figure 7). Nearest Neighbors. To study the learned space of latent messages, we visualize some nearest neighbors of sample images from the validation set of ImageNet100 (Deng et al., 2009). For simplicity, we concatenate the tokens in latent messages to obtain single 1536-dimensional vector per image. For each query image we then find 4 closest samples among the whole validation set using the cosine distance as our similarity measure (see Figure 9). Despite the simplicity of this probing mechanism, we find that the latent space is wellstructured and the neighboring images typically share semantics. Cropping policies. Figure 10 contains more demonstrations on the difference between the presence and absence of the global crop in the adaptive cropping policy. Notice that the content that the model does not observe in the sequence of crops misses from the final reconstruction. However, 3-4 local crops in the adaptive policy are typically enough to grasp the overall idea behind the scene without including too much details. This explains, why the adaptive policy with only 3 crops performs quite well on our semantic probing test suite. Communication-Inspired Tokenization for Structured Image Representations"
        },
        {
            "title": "Ground truth masks",
            "content": "40% attention thesholding, mIoU = 0.57 30% attention thesholding, mIoU = 0.58 20% attention thesholding, mIoU = 0.54 10% attention thesholding, mIoU = 0.38 Figure 8. Visualization of COMiT-XLs attention maps. The attention maps of the tokens that yield the largest IoUs per sample are shown. With adjusting the thresholding percentage, remarkable mIoU of 0.58 can be achieved. Note that the model has never seen any segmentation maps or class labels during training. Communication-Inspired Tokenization for Structured Image Representations"
        },
        {
            "title": "Query image",
            "content": "1st neighbor 2nd neighbor 3rd neighbor 4th neighbor Figure 9. Visualization of the nearest neighbors of sample from the ImageNet100 validation set in the space of COMiTs latent messages. One can see that despite unsupervised training and simple probing the learned messages tend to cluster into semantically meaningful groups. Communication-Inspired Tokenization for Structured Image Representations Figure 10. Additional results on the difference between the adaptive (even rows) and global+adaptive (odd rows) cropping policies. The first column depicts the ground truth input image. The last column corresponds to the final reconstruction with 10 NFE. The columns in-between demonstrate how COMiT refines its latent message with incoming information (1 NFE reconstructions are shown)."
        }
    ],
    "affiliations": [
        "Computer Vision Group, University of Bern, Switzerland",
        "VITA Lab, EPFL, Switzerland"
    ]
}