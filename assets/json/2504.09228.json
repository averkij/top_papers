{
    "paper_title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
    "authors": [
        "You Wu",
        "Xucheng Wang",
        "Xiangyang Yang",
        "Mengyuan Liu",
        "Dan Zeng",
        "Hengzhou Ye",
        "Shuiwang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose a major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of a target with respect to random masking operations modeled by a spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create a more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the task's difficulty. This student model, dubbed ORTrack-D, retains much of ORTrack's performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 2 2 9 0 . 4 0 5 2 : r Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking You Wu1, Xucheng Wang2, Xiangyang Yang1, Mengyuan Liu1, Dan Zeng3, Hengzhou Ye1, Shuiwang Li1 1College of Computer Science and Engineering, Guilin University of Technology, China 2School of Computer Science, Fudan University, Shanghai, China 3School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China wuyou@glut.edu.cn, xcwang317@glut.edu.cn, xyyang317@163.com, mengyuaner1122@foxmail.com, zengd8@mail.sysu.edu.cn, yehengzhou@glut.edu.cn, lishuiwang0721@163.com"
        },
        {
            "title": "Abstract",
            "content": "Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of target with respect to random masking operations modeled by spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the tasks difficulty. This student model, dubbed ORTrackD, retains much of ORTracks performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack. 1. Introduction Unmanned aerial vehicles (UAVs) are leveraged in plethora of applications, with increasing emphasis on UAV tracking [4, 43, 46, 49, 52, 79, 84]. This form of tracking poses an exclusive set of challenges such as tricky viewing angles, motion blur, severe occlusions, and the need for Equal contribution. Corresponding authors. Figure 1. Compared to SOTA UAV trackers on UAVDT, our ORTrack-DeiT sets new record with 83.4% precision and speed of 236 FPS. Our ORTrack-D-DeiT strikes better trade-off with 82.5% precision and speed of about 313 FPS. efficiency due to UAVs restricted battery life and computational resources [5, 42, 80, 83]. Consequently, designing an effective UAV tracker requires delicate balance between precision and efficiency. It needs to ensure accuracy while being conscious of the UAVs energy and computational constraints. In recent years, there has been notable shift from discriminative correlation filters (DCF)-based methods, because of their unsatisfactory robustness, towards DL-based approaches, particularly with the adoption of single-stream architectures that integrate feature extraction and fusion via pre-trained Vision Transformer (ViT) backbone networks. This single-stream paradigm has proven highly effective in generic visual tracking, as evidenced by the success of recent methods such as OSTrack [91], SimTrack [8], Mixformer [13], and DropMAE [82]. Building on these advancements, Aba-VTrack [44] introduces lightweight DLbased tracker within this framework, employing an adaptive and background-aware token computation method to enhance inference speed, which demonstrates remarkable precision and speed for real-time UAV tracking. However, the use of variable number of tokens in Aba-VTrack incurs significant time costs, primarily due to the unstructured access operations required during inference. Adding to this, it also grappled with establishing robustness when facing target occlusion, challenge common in UAV tracking often triggered by obstructive elements like buildings, mountains, trees, and so forth. The problem is exacerbated by the fact that UAVs may not always be capable of circumventing these impediments due to potential large-scale movements involved. To address these issues, we introduce novel framework designed to enhance the occlusion robustness of ViTs for UAV tracking. Our approach, termed ORTrack, aims to learn ViT-based trackers that maintain robust feature representations even in the presence of target occlusion. This is achieved by enforcing an invariance in the feature representation of the target with respect to random masking operations modeled by spatial Cox process. The random masking serves as simulation of target occlusion, which is expected to mimic real occlusion challenges in UAV tracking and aid in learning Occlusion-Robust Representations (ORR). Notably, our method for learning occlusion-robust representation simply uses Mean Squared Error (MSE) loss during training, adding no extra computational load during inference. Additionally, to enhance efficiency for real-time applications, we introduce an Adaptive FeatureThis Based Knowledge Distillation (AFKD) method. method creates more compact tracker, named ORTrack-D, which adaptively mimics the behavior of the teacher model ORTrack based on the complexity of the tracking task during training. The reasoning is that the teacher model, in its pursuit of powerful representations, may compromise its generalizability. Hence, in situations where generalizability is vital, the student model may perform better, and closely mimicking the teachers behavior becomes less important. We use the deviation of GIoU loss [67] from its average value to quantify the difficulty of the tracking task, which makes sense as loss value is commonly used criteria to define hard samples [70, 74, 77]. ORTrack-D maintains much of ORTracks performance with higher efficiency, making it better suited for deployment in resource-constrained environments typical of UAV applications. Extensive experiments on four benchmarks show that our method achieves state-of-the-art performance. In summary, our contributions are as follows: (i) We propose to learn Occlusion-Robust Representations (ORR) by imposing an invariance in the feature representation of the target with respect to random masking operations modeled by spatial Cox process, which can be easily integrated into other tracking frameworks without requiring additional architectures or increasing inference time; (ii) We propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to further enhance efficiency, in which the student model adaptively mimics the behavior of the teacher model according to the tasks difficulty, resulting in significant increase in tracking speed while only minimally reducing accuracy; (iii) We introduce ORTrack, family of efficient trackers based on these components, which integrates seamlessly with other ViT-based trackers. ORTrack demonstrates superior performance while maintaining extremely fast tracking speeds. Extensive evaluations show that ORTrack achieves state-of-the-art real-time performance. 2. Related work 2.1. Visual Tracking. In visual tracking, the primary approaches consist of DCFbased and DL-based trackers. DCF-based trackers are favored for UAV tracking due to their remarkable efficiency, but they face difficulties in maintaining robustness under complex conditions [31, 42, 46]. Recently developed lightweight DL-based trackers have improved tracking precision and robustness for UAV tracking [4, 5]; however, their efficiency lags behind that of most DCF-based trackers. Model compression techniques like those in [80, 83] have been used to further boost efficiency, yet these trackers still face issues with tracking precision. Vision Transformers (ViTs) are gaining traction for streamlining and unifying frameworks in visual tracking, as seen in studies like [13, 85, 86, 89, 91]. While these frameworks are compact and efficient, few are based on lightweight ViTs, making them impractical for real-time UAV tracking. To address this, Aba-ViTrack [44] used lightweight ViTs and an adaptive, background-aware token computation method to enhance efficiency for real-time UAV tracking. However, the variable token number in this approach necessitates unstructured access operations, leading to significant time costs. In this work, we aim to improve the efficiency of ViTs for UAV tracking through knowledge distillation, more structured method. 2.2. Occlusion-Robust Feature Representation. Occlusion-robust feature representation is crucial in computer vision and image processing. It involves developing methods that can recognize and process objects in images even when parts are hidden or occluded [62, 76]. Early efforts often relied on handcrafted features, active appearance models, motion analysis, sensor fusion, etc [7, 33, 51, 71]. While effective in some cases, these methods struggled with the complexity and variability of realworld visual data. The advent of deep learning revolutionized the field. Many studies have applied Convolutional Neural Networks (CNNs) and other deep architectures to extract occlusion-robust representations [35, 62, 66, 76]. These approaches use deep models to capture complex patterns and variations in visual data, making learned features resilient to occlusions and having proven valuable for many computer vision applications, such as action recognition [17, 88], pose estimation [62, 95], and object detection [12, 36]. The exploration of occlusion-robust representations in visual tracking has also demonstrated great success [1, 6, 27, 34, 39, 58, 59, 61, 94]. However, to our knowledge, there is dearth of research to explore learning occlusion-robust ViTs particularly in unified framework for UAV tracking. In this study, we delve into the exploration of learning occlusion-robust feature representations based on ViTs by simulating occlusion challenges using random masking modeled by spatial Cox process, specifically tailored for UAV tracking. This study represents the first use of ViTs for acquiring occlusion-robust feature representations in UAV tracking. 2.3. Knowledge Distillation. Knowledge distillation is technique used to compress models by transferring knowledge from complex teacher model to simpler student model, with the aim of maintaining performance while reducing computational resources and memory usage [63, 75]. It involves various types of knowledge, distillation strategies, and teacherstudent architectures, typically falling into three categories: response-based, feature-based, and relation-based distillation [26, 63, 78]. Widely applied in tasks such as image classification [64], object detection [9], and neural machine translation [42], it offers potential to improve the efficiency and even effectiveness of deep learning models. Recently, it has been successfully utilized to enhance the efficiency of DL-based trackers. For instance, Li et al. [41] used mask-guided self-distillation to compress Siamese-based visual trackers. Sun et al. [72] introduced lightweight dual Siamese tracker for hyperspectral object tracking, using spatial-spectral knowledge distillation method to learn from deep tracker. However, these techniques are mainly Siamese-based and tailored to specific tracking frameworks, posing challenges for adaptation to our ViT-based approach. In this study, we propose simple yet effective featurebased knowledge distillation method, in which the student adaptively replicate the behavior of the teacher based on the complexity of the tracking task during training. 3. Method In this section, we first provide brief overview of our endto-end tracking framework, named ORTrack, as shown in Figure 2. Then, we introduce the occlusion-robust representation learning based on spatial Cox processes and the method of adaptive knowledge distillation. Finally, we detail the prediction head and training loss. 3.1. Overview The proposed ORTrack introduces an novel single-stream tracking framework, featuring spatial Cox process-based masking for occlusion-robust representation learning and an adaptive feature-based knowledge distillation pipeline. ORTrack consists of two sequential training phases: the teacher model training pipeline for learning occlusion-robust representations, followed by the student training pipeline inIn the teacher volving adaptive knowledge distillation. model training phase, the input includes target template R3HzWz of spatial size Hz Wz, randomly masked target template = m(Z), and search image R3HxWx of spatial size Hx Wx, where m() represents the random masking operation that masks out nonoverlap patches of size with certain masking ratio σ. To achieve occlusion-robust representation with ViTs, we minimize the mean squared error (MSE) between two versions of the template representation: one with random masking and one without. During the training of the student model, the teachers weights remain fixed while both the teacher and student models receive inputs and X. Let BT and BS represent the backbones of the teacher and student, respectively. In our implementation, BT and BS share the same structure of the ViT layer but differ in the number of layers. Feature-based knowledge distillation is used to transfer the knowledge embedded in the teacher models backbone features to the student model through an adaptive distillation loss. 3.2. Occlusion-Robust Representations Based on Spatial Cox Processes (ORR) To begin, we describe two random masking operations used to simulate occlusion challenges: one from MAE [28] and our proposed method based on Spatial Cox process, denoted by mU and mC, respectively. Although mU allows the model to learn robust representations that are less sensitive to noise or missing information by randomly ignoring certain parts of the input data during training [28], it is less effective when used to simulate occlusion since each spatial position (in the sense of block size) is masked out with equal probability, especially in our situation where the target template generally contains background. To ensure that the target is masked out as expected with higher probabilities at given masking ratio, thereby making the occlusion simulation more effective, we employ finite Cox process [32] to model this masking operation, which is detailed as follows. Define two associated random matrices = (mi,j), = (bi,j), 1 Hz/b, 1 Wz/b, where mi,j U(0, 1) (i.e., mi,j follows uniform distribution over the interval [0, 1]), bi,j {0, 1} equals 1 if mi,j TopK(m, K), and 0 otherwise. TopK(m, K) returns the = (1 σ)HzWz largest elements from m, where rounds to Figure 2. Overview of the proposed ORTrack framework, which includes separate training pipelines for teacher and student model. Note that the spatial Cox process-based masking and occlusion-robust representation learning are applied only in the teacher pipeline. Once the teacher is trained, its weights are fixed for training the student model with the proposed adaptive knowledge distillation. the nearest integer. Mathematically, mU(Z) = (b 1), where denotes the Hadamard product and denotes the tensor product, 1 is an all-ones matrix of size b. Before defining mC, we establish core notations relevant to spatial Cox processes. It extend the concept of spatial inhomogeneous Poisson point processes by incorporating random intensity function, which, in turn, is defined as Poisson point process with an intensity determined by location-dependent function in the underlying space. For Euclidean space R2, an inhomogeneous Poisson point process is defined by locally integrable positive intensity function λ : R2 [0, ), such that for every bounded region the integral Λ(B) = (cid:82) λ(x, y) dxdy is finite, where Λ(B) has the interpretation of being the expected number of points of the Poisson process located in B, and for every collection of disjoint bounded Borel measurable sets B1, ..., Bk [60], its number distributions is defined by Pr{N(Bi) = ni, = 1, . . . , k} = (cid:81)k eΛ(Bi), ni Z0+, where Pr denotes the probability measure, indicates the random counting measure such that Λ(B) = E[N(B)], is the expectation operator. In particular, the conditional distribution of the points in bounded set given that N(B) = Z0+ is not uniform, and fn(p1, ..., pn) = (cid:81)i=1 p1, ..., pn defines the corresponding lon cation density function of the points. Since Cox process can be regarded as the result of two-stage random mechanism for which it is sometimes termed doubly stochastic Poisson process [32], the finite Cox processes can be simulated in straightforward way based on the hierarchical nature of the model. Specifically, in the first step, the intensity λ(x, y) is generated. In the second step, an in- (Λ(Bi))ni ni! λ(pi) Λ(B) , i=1 homogeneous Poisson point process is simulated using the generated λ(x, y) [32, 53]. The thinning algorithm [11] is used here for simulating inhomogeneous Poisson point processes. It involves simulating homogeneous Poisson point process with higher rate than the maximum possible rate of the inhomogeneous process, and then thinning out the generated points to match the desired intensity function. In this work, the randomness of the intensity function is modeled by random variable Γ that has Poisson distribution with expectation of ς, namely, Pr{Γ = k} = ς keς , k! where Z0+. The intensity function of the inhomogeneous Poisson point process is then given by λ(x, y) = Γe(x2+y2) e(x2+y2)dxdy (cid:82) . (1) Note that λ(x, y) is bell-shape function that gives more intensities to the central area of B. Let denote the rectangle region of size Hz/b Wz/b representing the template region. If we simulate the Cox process within and denote resulted point pattern by Ξ, we can obtain mai,j)1iHz/b,1iWz/b, where trix = (b i,j equals 1 if (i, j) Ξ, and 0 otherwise, with which our mC can be defined as mC(Z) = (b 1). It is worthy of note that if ς = (1 σ)HzWz, since E[Λ(B)] = E[(cid:82) λ(x, y)dxdy] = E[Γ] = ς, in this case, the expected masking ratio of our masking operation is equal to the masking ratio of mC. Thus, in addition to inhomogeneous intensity, our method can simulate more diverse pattern of occlusion due to the introduced randomness of the masking ratio. We denote the total number of tokens by K, the embedding dimension of each token by d, and all the tokens outKZ KX 1:K(Z, X; BT ) RKd. Let tL put by the L-th layer of BT with respect to inputs and by tL (Z, X; BT ) = tL 1:K(Z, X; BT ), where KZ KX = [1, K], tL and KZ tL represent the tokens corresponding to the template KX and the search image, respectively. By the same tothe output tokens corresponding to inputs and ken, are tL 1:K(Z , X; BT ). The feature representations of and can be recovered by tracking their token indices in respective ordered sequences, which specifically are tL (Z , X; BT ), respectively. 1:Kz The core idea of our occlusion-robust representations learning is that the mean square error between the feature representation of and that of is minimized, which is implemented by minimizing the following MSE loss, (Z, X; BT ) and tL 1:Kz Lorr = tL 1:Kz (Z, X; BT ) tL 1:Kz (Z , X; BT )2. (2) During inference, only [Z, X] is input to the model without the need for random template masking. Consequently, our method incurs no additional computational cost during inference. Notably, our method is independent of the ViTs used, any efficient ViTs can work in our framework. 3.3. Adaptive Feature-Based Knowledge Distillation (AFKD) Feature-based knowledge distillation is technique in machine learning that trains smaller student model to mimic larger teacher model, which, instead of focusing only on final outputs, transfers intermediate features or representations from the teacher to the student [26, 78]. This method uses the detailed internal representations from the teacher model to improve the students learning process. However, there is risk that the student model might overfit to the specific features of the teacher model, rather than generalizing well to new data. This can be particularly problematic if the teacher model has learned spurious correlations in the data. To combat this, we propose adaptively transferring knowledge based on the difficulty of the tracking task. We quantify this difficulty using the deviation of the GIoU loss [67] (see Section 3.4) from its average value, calculated between the students prediction and the ground truth. Adapting knowledge transfer based on difficulty ensures that the student model doesnt heavily adjust its weights on easy tasks, which it can handle already probably due to its generalizability. Instead, it focuses more on challenging scenarios where its feature representation is less effective. Additionally, the choice of teacher-student architectures is crucial in knowledge distillation. Given the wide array of possible student models, we adopt self-similar approach where the student model mirrors the teachers architecture but employs smaller ViT backbone, using fewer ViT blocks. This strategy simplifies the design and eliminates the need for additional alignment techniques that would otherwise be necessary due to mismatched feature dimensions. Lastly, layer selection and the metric of feature similarity are also crucial aspects of feature-based knowledge distillation. Given MSEs popularity in feature-based knowledge distillation and to avoid potential complexity associated with using multiple layers, we employ MSE to penalize differences between the output feature representations of both the teacher and student models backbones, i.e., tL 1:K(Z, X; BS). The proposed adaptive knowledge distillation loss is defined by 1:K(Z, X; BT ) and tL Laf kd = (α+β(LiouLiou))tL 1:K(Z, X; BT )tL 1:K(Z, X; BS)2, (3) where α+β(Liou Liou) := ϖ(Liou; α, β) is function of the deviation of GIoU loss from its average, with slop α and intercept β, used to quantify the difficulty of the tracking task. 3.4. Prediction Head and Training Loss Following the corner detection head in [13, 91], we use prediction head consisting of multiple Conv-BN-ReLU layers to directly estimate the bounding box of the target. The output tokens corresponding to the search image are first reinterpreted to 2D spatial feature map and then fed into the prediction head. The head outputs local offset [0, 1]2Hx/P Wx/P , normalized bounding box size [0, 1]2Hx/P Wx/P , and target classification score [0, 1]Hx/P Wx/P as prediction outcomes. The initial estimation of the target position depends on identifying the location with the highest classification score, i.e., (xc, yc) = argmax(x,y)p(x, y). The final target bounding box is estimated by {(xt, yt); (w, h)} = {(xc, yc) + o(xc, yc); s(xc, yc)}. For the tracking task, we adopt the weighted focal loss [40] for classification, combination of L1 loss and GIoU loss [67] for bounding box regression. The total loss for tracking prediction is: Lpred = Lcls + λiouLiou + λL1 LL1 , (4) where the constants λiou = 2 and λL1= 5 are set as in [13, 91]. The overall loss LT = Lpred + γLorr is used to train the teacher end-to-end after loading the pretrained weights of the ViT trained with ImageNet [68], where the constant γ is set to 2.0 104. After this training, we fix the weights of the teacher model, and employ the overall loss LS = Lpred + Laf kd, for end-to-end knowledge distillation training. 4. Experiments We evaluate our method on four UAV tracking benchmarks: DTB70 [45], UAVDT [19], VisDrone2018 [98], and UAV123 [57]. All experiments run on PC with an i9-10850K processor, 16GB RAM, and an NVIDIA TitanX GPU. We compare our method against 26 stateof-the-art trackers, using their official codes and hyperTable 1. Precision (Prec.), success rate (Succ.), and speed (FPS) comparison between ORTrack and lightweight trackers on four UAV tracking benchmarks, i.e., DTB70 [45], UAVDT [19], VisDrone2018 [98], and UAV123 [57]. Red, blue and green indicate the first, second and third place. Note that the percent symbol (%) is omitted for all Prec. and Succ. values. Method KCF [29] fDSST [16] ECO HC [14] AutoTrack [46] RACF [42] HiFT [4] TCTrack [5] SGDViT [90] DRCI [93] PRL-Track [22] Aba-ViTrack [44] SMAT [25] AVTrack-DeiT [47] ORTrack-DeiT ORTrack-D-DeiT s - d b - d b - Source TAPMI 15 TPAMI 17 CVPR 17 CVPR 20 PR 22 ICCV 21 CVPR 22 ICRA 23 ICME 23 IROS 24 ICCV 23 WACV 24 ICML Ours DTB70 UAVDT Prec. 46.8 53.4 63.5 71.6 72.6 80.2 81.2 78.5 81.4 79.5 85.9 81.9 84.3 86.2 83.7 Succ. 28.0 35.7 44.8 47.8 50.5 59.4 62.2 60.4 61.8 60.6 66.4 63.8 65.0 66.4 65.1 Prec. 57.1 66.6 69.4 71.8 77.3 65.2 72.5 65.7 84.0 73.1 83.4 80.8 82.1 83.4 82. Succ. 29.0 38.3 41.6 45.0 49.4 47.5 53.0 48.0 59.0 53.5 59.9 58.7 58.7 60.1 59.7 VisDrone2018 Succ. Prec. 41.3 68.5 51.0 69.8 58.1 80.8 57.3 78.8 60.0 83.4 52.6 71.9 59.4 79.9 52.1 72.1 60.0 83.4 53.8 72.6 65.3 86.1 63.4 82.5 65.3 86.0 66.8 88.6 63.9 84.6 UAV123 Avg. Avg.FPS Prec. 52.3 58.3 71.0 68.9 70.2 78.7 80.0 75.4 76.7 79.1 86.4 81.8 84.8 84.3 84. Succ. 33.1 40.5 49.6 47.2 47.7 59.0 60.5 57.5 59.7 59.3 66.4 64.6 66.8 66.4 66.1 Prec. 56.2 62.0 71.2 72.8 75.9 74.0 78.4 72.9 81.4 76.1 85.5 81.8 84.2 85.6 83.7 Succ. GPU CPU 624.3 - 32.9 193.4 - 41.4 83.5 - 48.5 57.8 - 49.3 35.6 - 51.8 - 160.3 54.6 - 149.6 58.8 - 110.5 54.5 62.7 281.3 60.1 - 132.3 56.8 64.5 50.3 181.5 - 126.8 62.6 59.8 260.3 63.8 65.0 55.4 226.4 64.7 292.3 63.7 FLOPs (GMac) - - - - - 7.2 8.8 11.3 3.6 7.4 2.4 3.2 0.97-1.9 2.4 1.5 Param. (M) - - - - - 9.9 9.7 23.3 8.8 12.0 8.0 8.6 3.5-7.9 7.9 5.3 parameters. We evaluate our approach against 13 state-ofthe-art (SOTA) lightweight trackers (see Table 1) and 14 SOTA deep trackers designed specifically for generic visual tracking (refer to Table 2). 4.1. Implementation Details We adopt different ViTs as backbones, including ViT-tiny [18], Eva-tiny [21], and DeiT-tiny [73], to build three trackers for evaluation: ORTrack-ViT, ORTrack-Eva, and ORTrack-DeiT. The head of ORTrack consists of stack of four Conv-BN-ReLU layers. The search region and template sizes are set to 256 256 and 128 128, respectively. combination of training sets from GOT-10k [30], LaSOT [20], COCO [48], and TrackingNet [56] is used for the training. The batch size is set to 32. We employ the AdamW optimizer [50], with weight decay of 104 and an initial learning rate of 4 105. The training is conducted over 300 epochs, with 60,000 image pairs processed in each epoch. The learning rate is reduced by factor of 10 after 240 epochs. 4.2. State-of-the-art Comparison Comparison with Lightweight Trackers. The overall performance of our ORTrack in comparison to 13 competing trackers on the four benchmarks is displayed in Table 1. As can be seen, our trackers demonstrate superior performance among all these trackers in terms of average (Avg.) precision (Prec.), success rate (Succ.) and speeds. On average, RACF [42] demonstrated the highest Prec. (75.9%) and Succ. (51.8%) among DCF-based trackers, DRCI [93] achieves the highest precision and success rates, with 81.4% and 60.1%, respectively, among CNN-based trackers. However, the average Prec. and Succ. of all our trackers are greater than 82.0% and 62.0%, respectively, clearly surpassing DCFand CNNbased approaches. Additionally, our ORTrack-DeiT achieves the highest Avg. Prec. and Avg. Succ. of 85.6% and 65.0%, respectively, among all competing trackers. Although Aba-ViTrack achieves performance close to our ORTrack-DeiT, its GPU speed is significantly lower, with 23.6% relative gap. Notably, when the proposed adaptive knowledge distillation is applied to ORTrack-DeiT, the resulting student model, ORTrack-DDeiT, shows significant speed increase: 29.1% on GPU and 16.8% on CPU. This improvement is accompanied by minimal reduction in accuracy, with only 1.9% decrease in Avg. Prec. and 1.3% decrease in Avg. Succ.. All proposed trackers can run in real-time on single CPU*, and our ORTrack-DeiT sets new performance record for real-time UAV tracking. We also compare the floating point operations per second (FLOPs) and number of parameters (Params.) of our method with CNN-based and ViTbased trackers in Table 1. Our method demonstrates relatively lower parameter count and reduced computational complexity compared to these approaches. Notably, since AVTrack-DeiT tracker features adaptive architectures, the FLOPs and parameters range from minimum to maximum values. These results highlight our methods effectiveness and its state-of-the-art performance. Comparison with Deep Trackers. The proposed ORTrack-DeiT is also compared with 14 SOTA deep trackers in Table 2, which shows precision (Prec.) and GPU speed on VisDrone2018. Our ORTrack-DeiT surpasses all other methods in both metrics, demonstrating its superior accuracy and speed. Although trackers like AQATrack [87], HIPTrack [2], and ROMTrack [3] achieve precision comparable to our ORTrack-DeiT, their GPU speeds are much slower. Specifically, our method is 4, 6, and 4 times faster than AQATrack, HIPTrack, and ROMTrack, respectively. Attribute-Based Evaluation. To access our methods robustness against target occlusion, we compare ORTrack- *Real-time performance applies to platforms similar to or more advanced than ours. Table 2. Precision (Prec.) and speed (FPS) comparison between ORTrack-DeiT and deep-based trackers on VisDrone2018 [98]. Source Ours Tracker ORTrack-DeiT AQATrack [87] CVPR 24 HIPTrack [2] CVPR 24 EVPTrack [69] AAAI 24 ICCV 23 ROMTrack [3] Prec. 88.6 87.2 86.7 84.5 86.4 Succ. 66.8 66.9 67.1 65.8 66.7 Tracker FPS 206.2 ZoomTrack [38] SeqTrack [10] 53.4 MAT [96] 31.3 SparseTT [23] 22.1 OSTrack [91] 51.1 Source NIPS 23 CVPR 23 CVPR 23 IJCAI 22 ECCV Prec. 81.4 85.3 81.6 81.4 84.2 Succ. 63.4 65.8 62.2 62.1 64.8 Tracker SimTrack [8] ToMP [55] FPS 61.7 15.3 68.4 KeepTrack [54] 30.2 62.7 Source ECCV 22 CVPR 22 ICCV 21 ICCV 21 PrDiMP50 [15] CVPR 20 SAOT [97] Prec. 80.0 84.1 84.0 76.9 79.4 Succ. 60.9 64.4 63.5 59.1 59.7 FPS 69.7 21.4 20.3 35.4 42.6 (AFKD). To demonstrate the effectiveness of the proposed ORR and AFKD, Table 3 shows the evaluation results on UAVDT dataset as these components are gradually integrated into the baselines. To avoid potential variations due to randomness, we only present the speed of the baseline, since the GPU speeds of the baseline and its ORR-enhanced version are theoretically identical. As can bee seen, the incorporation of ORR significantly enhances both Prec. and Succ. for all baseline trackers. Specifically, the Prec. increases for ORTrack-ViT, ORTrack-Eva, and ORTrackDeiT are 3.3%, 2.7%, and 4.8%, respectively, while the Succ. increases are 2.6%, 2.1%, and 3.1%, respectively. These significant enhancements highlight the effectiveness of ORR in improving tracking precision. The further integration of AFKD results in consistent improvements in GPU speeds, with only slight reductions in Prec. and Succ. Specifically, all baseline trackers experience GPU speed enhancements of over 30.0%, with ORTrack-DeiT showing an impressive 36.0% improvement. These results affirm the effectiveness of AFKD in optimizing tracking efficiency while maintaining high tracking performance. Table 4. Impact of various Masking Operators on performance. Method mU mC SAM[37] AdAutoMix[65] CutMix[92] ORTrack-DeiT VisDrone2018 Prec. Succ. 62.2 81.6 65.4 86.7 66.8 88.6 65.6 86.8 63.8 84.3 64.2 85.7 Effect of Masking Operators. To demonstrate the superiority of the proposed masking operator in terms of performance, we evaluate ORTrack-DeiT with various implementations of masking operators (i.e., mU, mC, and SAM [37]) alongside data mixing augmentation methods (i.e., AdAutoMix [65] and CutMix [92]). The evaluation results on VisDrone2018 are presented in Table 4. As shown, although using SAM, AdAutoMix, and CutMix improves performance, the best result achieved with SAM is only comparable to the performance of our mU masking operator. When mC is applied, the improvements are even more substantial, with increases of 7.0% and 4.6%, respectively. These results validate the effectiveness of the proposed ORR component and particularly demonstrate the superiority of the masking operator based on spatial Cox processes. Figure 3. Attribute-based comparison on the partial occlusion subset of VisDrone2018 [98]. ORTrack-DeiT* refers to ORTrackDeiT without applying the occlusion-robust enhancement. DeiT alongside 16 SOTA trackers on the partial occlusion subset of VisDrone2018. Additionally, we also assess the baseline ORTrack-DeiT*, i.e., ORTrack-DeiT without applying the proposed method for learning Occlusion-Robust Representation (ORR), for comparison. The precision plot are presented in Fig. 3, with additional attribute-based evaluation results provided in the supplemental materials. As observed, ORTrack-DeiT achieves the second-highest precision (85.0%), just slightly behind the first-ranked tracker AQATrack by 0.2%. Remarkably, incorporating the proposed components leads to significant improvement over ORTrack-DeiT*, with increases of 6.9% in Prec., well underscoring the effectiveness of our method. 4.3. Ablation Study Table 3. Effect of ORR and AFKD on the baseline trackers."
        },
        {
            "title": "ORR AFKD",
            "content": "ORTrack-ViT ORTrack-Eva ORTrack-DeiT"
        },
        {
            "title": "UAVDT",
            "content": "Prec. 77.0 80.33.3 79.12.1 78.1 80.82.7 79.51.4 78.6 83.44.8 82.53.9 Succ. 55.6 58.22.6 57.51.9 56.6 58.72.1 57.81.2 56.7 60.13.4 59.73."
        },
        {
            "title": "FPS",
            "content": "216.2 - 290.334% 238.3 - 308.830% 218.4 - 298.736% Effect of Occlusion-Robust Representations (ORR) and Adaptive Feature-Based Knowledge Distillation Table 5. Impact of the adaptive knowledge distillation loss on the generalizability on LaSOT and TrackingNet. Method KD AFKD ORTrack-DeiT LaSOT TrackingNet AUC Pnorm AUC Pnorm 52.6 72.8 53.7 53.2 73.1 54.0 54.6 54.3 73.7 77.8 78.4 79.1 60.8 61.2 62. 67.1 67.4 68.2 Impact of the Adaptive Knowledge Distillation Loss. To assess the impact of the adaptive knowledge distillation loss on generalizability, we train ORTrack-DeiT using GOT-10K with ϖ(Liou; α, β) and ϖ(Liou; α, 0) separately, then evaluate them on LaSOT and TrackingNet. The results are shown in Table 5. Note that ϖ(Liou; α, 0) degenerates to non-adaptive knowledge distillation loss as it becomes constant. As can be seen, AFKD demonstrates greater performance improvements than KD. For instance, using AFKD results in additional gains of over 1.1% in Pnorm and on LaSOT, demonstrating its superior generalizability. Table 6. Application of our ORR component to three SOTA trackers: ARTrack [81], GRM [24], and DropTrack[82]. Tracker ORR ARTrack[81] GRM[24] DropTrack[82] UAVDT VisDrone2018 Prec. 77.1 78.51. 79.0 81.71.7 76.9 78.71.8 Succ. 54.6 55.81.2 57.7 59.31.6 55.9 57.41.5 Prec. 77.7 79.51.8 82.7 84.82.1 81.5 82.81. Succ. 59.5 60.81.3 63.4 64.61.2 62.7 64.21.5 Application to SOTA trackers. To show the wide applicability of our proposed method, we incorporate the proposed ORR into three existing SOTA trackers: ARTrack [81], GRM [24], and DropTrack [82]. Please note that we replace the models original backbones with ViT-tiny [18] to reduce training time. As shown in Table 6, incorporating ORR results in significant improvements in both precision and success rates for the three baseline trackers. Specifically, ARTrack, GRM, and DropTrack demonstrate an improvement of more than 1.2% in both precision and success rate across two datasets. These experimental results demonstrate that the proposed ORR component can be seamlessly integrated into existing tracking frameworks, significantly improving tracking accuracy. Qualitative Results. Several qualitative tracking results of ORTrack-DeiT and seven SOTA UAV trackers are shown in Fig. 4. As can be seen, only our tracker successfully tracks the targets in all challenging examples, where pose variations, background clusters, and scale variations are presented. In these cases, our method performs significantly better and is more visually appealing, bolstering the effectiveness of the proposed method for UAV tracking. Figure 5 shows attention and feature maps produced by Figure 4. Qualitative evaluation on 3 video sequences from, respectively, UAV123 [57], UAVDT [19], and VisDrone2018 [98] (i.e., person9, S1607, and uav0000180 00050 s). Figure 5. Visualize the attention map (left) and feature map (right) of the target images. The first row displays the search and masked images with masking ratios of 0%, 10%, 30%, and 70%. The second and third rows show the attention and feature maps generated by ORTrack-DeiT, with and without ORR, respectively. ORTrack-DeiT, with and without occlusion-robust enhancement. We observe that ORTrack-DeiT with ORR maintains clearer focus on the targets and exhibits more consistent feature maps across masking ratios. These results support the effectiveness of our ORR component. 5. Conclusion In view of the common challenges posed by target occlusion in UAV tracking, in this work, we proposed to learn Occlusion-Robust Representation (ORR) by imposing an invariance of feature representation of the target with respect to random masking modeled by spatial Cox process. Moreover, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) to enhance efficiency. Our approach is notably straightforward and can be easily integrated into other tracking frameworks. Extensive experiments across multiple UAV tracking benchmarks validate the effectiveness of our method, demonstrating that our ORTrack-DeiT achieves SOTA performance. This work was funded by the Guangxi Natural Science Foundation (Grant No. 2024GXNSFAA010484), and the National Natural Science Foundation of China (Nos. 62466013, 62206123). Acknowledgments."
        },
        {
            "title": "References",
            "content": "[1] Wesam A. Askar, Osama Elmowafy, Anca L. Ralescu, Aliaa Abdel-Halim Youssif, and Gamal A. Elnashar. Occlusion detection and processing using optical flow and particle filter. Int. J. Adv. Intell. Paradigms, 15:63 76, 2020. 3 [2] Wenrui Cai, Qingjie Liu, and Yunhong Wang. Hiptrack: Visual tracking with historical prompts. In CVPR, pages 1925819267, 2024. 6, 7 [3] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu. RoIn ICCV, bust object modeling for visual tracking. pages 95899600, 2023. 6, 7 [4] Ziang Cao, Changhong Fu, Junjie Ye, Bowen Li, and Yiming Li. Hift: Hierarchical feature transformer for aerial tracking. In ICCV, pages 1545715466, 2021. 1, 2, 6 [5] Ziang Cao, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, and Changhong Fu. Tctrack: Temporal contexts for aerial tracking. In CVPR, pages 14798 14808, 2022. 1, 2, [6] Satyaki Chakraborty and Martial Hebert. Learning to track object position through occlusion. ArXiv, abs/2106.10766, 2021. 3 [7] T-H Chang and Shaogang Gong. Tracking multiple people with multi-camera system. In Womot, pages 1926, 2001. 2 [8] Boyu Chen, Peixia Li, Lei Bai, Lei Qiao, and et al. Backbone is all your need: simplified architecture for visual object tracking. In ECCV, pages 375392, 2022. 1, 7 [9] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and et al. Learning efficient object detection models with knowledge distillation. NIPS, 30, 2017. 3 [10] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu. Seqtrack: Sequence to sequence learning In CVPR, pages 14572 for visual object tracking. 14581, 2023. 7 [11] Yuanda Chen. Thinning algorithms for simulating point processes. Florida State University, Tallahassee, FL, 2016. 4 [12] Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, S. Li, and Xudong Zou. Pedhunter: Occlusion robust pedestrian detector in crowded scenes. ArXiv, abs/1909.06826, 2019. [13] Yutao Cui, Cheng Jiang, and et al. Mixformer: End-toend tracking with iterative mixed attention. In CVPR, pages 1360813618, 2022. 1, 2, 5 [14] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco: Efficient convolution operators for tracking. In CVPR, pages 6638 6646, 2017. 6 [15] Martin Danelljan, Luc Van Gool, and Radu Timofte. Probabilistic regression for visual tracking. In CVPR, pages 71817190, 2020. 7 [16] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and et al. Discriminative scale space tracking. IEEE TPAMI, 39(8):15611575, 2017. 6 [17] Soumen Das, Saroj K. Biswas, and Biswajit Purkayastha. Occlusion robust sign language recognition system for indian sign language using cnn and pose features. Multimed. Tools. Appl, 2024. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, and et al. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs/2010.11929, 2020. 6, 8 [19] Dawei Du, Yuankai Qi, Hongyang Yu, and et al. The unmanned aerial vehicle benchmark: Object detection and tracking. In ECCV, pages 375391, 2018. 5, 6, 8 [20] Heng Fan, Liting Lin, Fan Yang, and et al. Lasot: high-quality benchmark for large-scale single object tracking. In CVPR, pages 53695378, 2018. 6 [21] Yuxin Fang, Quan Sun, Xinggang Wang, and et al. Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171, 2024. 6 [22] Changhong Fu, Xiang Lei, and et al. Progressive representation learning for real-time uav tracking. In IROS, pages 50725079, 2024. 6 [23] Zhihong Fu, Zehua Fu, Qingjie Liu, Wenrui Cai, and Yunhong Wang. Sparsett: Visual tracking with sparse transformers. arXiv e-prints, 2022. 7 [24] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized relation modeling for transformer tracking. In CVPR, pages 1868618695, 2023. 8 [25] Goutam Yelluru Gopal and Maria Amer. Separable self and mixed attention transformers for efficient object tracking. In WACV, pages 67086717, 2024. 6 [26] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. IJCV, 129(6):17891819, 2021. 3, [27] Karthik Hariharakrishnan and Dan Schonfeld. Fast object tracking using adaptive block matching. IEEE TMM, 7:853859, 2005. 3 [28] Kaiming He, Xinlei Chen, Saining Xie, and et al. Masked autoencoders are scalable vision learners. In CVPR, pages 1597915988, 2021. 3 [29] Joao F. Henriques, Rui Caseiro, Pedro Martins, and et al. High-speed tracking with kernelized correlation filters. IEEE TPAMI, 37:583596, 2015. 6 [30] L. Huang, X. Zhao, and K. Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE TPAMI, (5), 2021. 6 [31] Ziyuan Huang, Changhong Fu, and et al. Learning aberrance repressed correlation filters for real-time uav tracking. In ICCV, pages 28912900, 2019. 2 [32] Janine Illian, Antti Penttinen, Helga Stoyan, and Dietrich Stoyan. Statistical analysis and modelling of spatial point patterns. John Wiley & Sons, 2008. 3, 4 [33] Michal Irani and Shmuel Peleg. Motion analysis for image enhancement: Resolution, occlusion, and transparency. JVCIR, 4(4):324335, 1993. [34] Dippal Israni and Hiren K. Mewada. Feature descriptor based identity retention and tracking of players under intense occlusion in soccer videos. Int. J. Intell. Eng. Syst, 2018. 3 [35] Minyang Jiang and et al. Occlusion-robust fau recognition by mining latent space of masked autoencoders. Neurocomputing, 569:127107, 2024. 3 [36] Jung Uk Kim, Ju Won Kwon, and et al. Bbc net: Bounding-box critic network for occlusion-robust object detection. IEEE TCSVT, 30:10371050, 2020. 3 [37] Alexander Kirillov, Eric Mintun, and et al. Segment anything. In ICCV, pages 40154026, 2023. 7 [38] Yutong Kou, Jin Gao, Bing Li, and et al. Zoomtrack: Target-aware non-uniform resizing for efficient visual tracking. NIPS, 36:5095950977, 2023. 7 [39] Thijs P. Kuipers, Devanshu Arya, and Deepak K. Gupta. Hard occlusions in visual object tracking. In ECCV Workshops, 2020. 3 [40] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. IJCV, 128:642656, 2018. 5 [41] Luming Li, Chenglizhao Chen, and Xiaowei Zhang. Mask-guided self-distillation for visual tracking. In ICME, pages 16, 2022. 3 [42] Shuiwang Li, Yuting Liu, Qijun Zhao, and Ziliang Feng. Learning residue-aware correlation filters and refining scale for real-time uav tracking. Pattern Recognition, 127:108614, 2022. 1, 2, 3, [43] Shuiwang Li, Xiangyang Yang, and et al. Learning target-aware vision transformers for real-time uav tracking. IEEE TGRS, 2024. 1 [44] Shuiwang Li, Yangxiang Yang, Dan Zeng, and Xucheng Wang. Adaptive and background-aware vision transformer for real-time uav tracking. In ICCV, pages 1394313954, 2023. 1, 2, 6 [45] Siyi Li and D. Y. Yeung. Visual object tracking for unmanned aerial vehicles: benchmark and new motion models. In AAAI, 2017. 5, 6 [46] Yiming Li, Changhong Fu, Fangqiang Ding, and et al. Autotrack: Towards high-performance visual tracking for uav with automatic spatio-temporal regularization. In CVPR, pages 1192011929, 2020. 1, 2, 6 [47] Yongxin Li, Mengyuan Liu, You Wu, and et al. Learning adaptive and view-invariant vision transformer for real-time uav tracking. In ICML, 2024. 6 [48] Tsung Yi Lin, Michael Maire, and et al. Microsoft coco: Common objects in context. In ECCV, 2014. [49] Mengyuan Liu, Yuelong Wang, and et al. Global filter pruning with self-attention for real-time uav tracking. In BMVC, page 861, 2022. 1 [50] Ilya Loshchilov and Frank Hutter. pled weight decay regularization. arXiv:1711.05101, 2017. 6 DecouarXiv preprint [51] David Lowe. Object recognition from local scaleinvariant features. In ICCV, pages 11501157, 1999. 2 [52] Siyu Ma, Yuting Liu, and et al. Learning disentangled representation in pruning for real-time uav tracking. In ACML, pages 690705, 2023. 1 [53] Torsten Mattfeldt. Stochastic geometry and its applications. Journal of Microscopy, 183:257257, 1996. [54] Christoph Mayer, Martin Danelljan, and et al. Learning target candidate association to keep track of what not to track. In ICCV, pages 1342413434, 2021. 7 [55] Christoph Mayer, Martin Danelljan, and et al. TransIn CVPR, forming model prediction for tracking. pages 87218730, 2022. 7 [56] Matthias Mueller, Adel Bibi, and et al. Trackingnet: large-scale dataset and benchmark for object tracking in the wild. In ECCV, pages 300317, 2018. 6 [57] Matthias Mueller, Neil G. Smith, and Bernard Ghanem. benchmark and simulator for uav tracking. In ECCV, 2016. 5, 6, 8 [58] Hieu Tat Nguyen and Arnold W. M. Smeulders. Fast occluded object tracking by robust appearance filter. IEEE TPAMI, 26:10991104, 2004. 3 [59] Hieu Tat Nguyen, Marcel Worring, and Rein van den Occlusion robust adaptive template Boomgaard. tracking. In ICCV, pages 678683, 2001. 3 [60] Toby C. ONeil. Geometric measure theory. 2002. 4 [61] Jiyan Pan and Bo Hu. Robust occlusion handling in object tracking. In CVPR, pages 18, 2007. 3 [62] Joo Hyun Park, Yeong Min Oh, and et al. Handoccnet: Occlusion-robust 3d hand mesh estimation network. In CVPR, pages 14861495, 2022. 2, 3 [63] Wonpyo Park and et al. Relational knowledge distillation. In CVPR, pages 39623971, 2019. [64] Zhimao Peng, Zechao Li, Junge Zhang, and et al. Fewshot image recognition with knowledge transfer. In ICCV, pages 441449, 2019. 3 [65] Huafeng Qin, Xin Jin, Yun Jiang, Mounim ElYacoubi, and Xinbo Gao. Adversarial automixup. arXiv preprint arXiv:2312.11954, 2023. 7 [66] Delin Qu, Yizhen Lao, and et al. Towards nonlinearmotion-aware and occlusion-robust rolling shutter correction. ICCV, pages 1064610654, 2023. 3 [67] Seyed Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, and et al. Generalized intersection over union: metric and loss for bounding box regression. CVPR, pages 658666, 2019. 2, 5 [68] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, and et al. Imagenet large scale visual recognition challenge. IJCV, 115:211 252, 2014. 5 [69] Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Shengping Zhang, and Xianxian Li. Explicit visual prompts for visual object tracking. In AAAI, 2024. [70] Abhinav Shrivastava, Abhinav Kumar Gupta, and Ross B. Girshick. Training region-based object deIn CVPR, tectors with online hard example mining. pages 761769, 2016. 2 [71] Markus Storer and et al. Active appearance model fitting under occlusion using fast-robust pca. In VISAPP, pages 129136, 2009. 2 [72] Chen Sun and et al. Siamohot: lightweight dual siamese network for onboard hyperspectral object tracking via joint spatial-spectral knowledge distillation. IEEE TGRS, 61:112, 2023. 3 [73] Hugo Touvron and et al. Training data-efficient image transformers & distillation through attention. In ICML, pages 1034710357, 2021. 6 [74] Wenxuan Tu, Sihang Zhou, and et al. Hierarchically contrastive hard sample mining for graph selfsupervised pretraining. IEEE TNNLS, PP, 2023. 2 [75] Frederick Tung and Greg Mori. Similarity-preserving In ICCV, pages 13651374, knowledge distillation. 2019. [76] K. Wang and et al. Region attention networks for pose and occlusion robust facial expression recognition. IEEE TIP, 29:40574069, 2019. 2, 3 [77] Keze Wang and et al. Towards human-machine cooperation: Self-supervised sample mining for object detection. In CVPR, pages 16051613, 2018. 2 [78] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence: IEEE TPAMI, 44:3048 review and new outlooks. 3068, 2020. 3, 5 [79] Xucheng Wang, Xiangyang Yang, and et al. Learning disentangled representation with mutual information In ICME, maximization for real-time uav tracking. pages 13311336, 2023. 1 [80] Xucheng Wang, Dan Zeng, Qijun Zhao, and Shuiwang Li. Rank-based filter pruning for real-time uav tracking. In ICME, pages 0106, 2022. 1, 2 [81] Xing Wei, Yifan Bai, and et al. Autoregressive visual tracking. In CVPR, pages 96979706, 2023. 8 [82] Qiangqiang Wu, Tianyu Yang, and et al. Dropmae: Masked autoencoders with spatial-attention dropout In CVPR, pages 1456114571, for tracking tasks. 2023. 1, 8 [83] Wanying Wu, Pengzhi Zhong, and Shuiwang Li. Fisher pruning for real-time uav tracking. In IJCNN, pages 17, 2022. 1, [84] You Wu, Xucheng Wang, Dan Zeng, and et al. Learning motion blur robust vision transformers with dynamic early exit for real-time uav tracking. arXiv preprint arXiv:2407.05383, 2024. 1 [85] Fei Xie, Chunyu Wang, and et al. Learning tracking representations via dual-branch fully transformer networks. In ICCV, pages 26882697, 2021. 2 [86] Fei Xie, Chunyu Wang, Guangting Wang, and et al. In CVPR, pages Correlation-aware deep tracking. 87418750, 2022. 2 [87] Jinxia Xie and et al. Autoregressive queries for adaptive tracking with spatio-temporal transformers. In CVPR, pages 1930019309, 2024. 6, 7 [88] Di Yang and et al. Self-supervised video pose representation learning for occlusionrobust action recognition. In AFGR, pages 15, 2021. [89] Xiangyang Yang, Dan Zeng, and et al. Adaptively bypassing vision transformer blocks for efficient visual tracking. Pattern Recognition, 161:111278, 2025. 2 [90] Liangliang Yao, Changhong Fu, and et al. Sgdvit: Saliency-guided dynamic vision transformer for uav tracking. arXiv preprint arXiv:2303.04378, 2023. 6 [91] Botao Ye, Hong Chang, and et al. Joint feature learning and relation modeling for tracking: one-stream framework. In ECCV, pages 341357, 2022. 1, 2, 5, 7 [92] Sangdoo Yun, Dongyoon Han, and et al. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, pages 60236032, 2019. 7 [93] Dan Zeng, Mingliang Zou, Xucheng Wang, and Shuiwang Li. Towards discriminative representations with contrastive instances for real-time uav tracking. In ICME, pages 13491354, 2023. 6 [94] Chenyuan Zhang, Jiu Xu, and et al. klt-based approach for occlusion handling in human tracking. In PCS, pages 337340, 2012. 3 [95] Yi Zhang, Pengliang Ji, and et al. 3d-aware neural body fitting for occlusion robust 3d human pose estimation. ICCV, pages 93659376, 2023. [96] Haojie Zhao, Dong Wang, and Huchuan Lu. Representation learning for visual object tracking by masked In CVPR, pages 1869618705, appearance transfer. 2023. 7 [97] Zikun Zhou, Wenjie Pei, Xin Li, and et al. SaliencyIn ICCV, pages 9846 associated object tracking. 9855, 2021. 7 [98] Pengfei Zhu, Longyin Wen, and et al. Visdronevdt2018: The vision meets drone video detection and tracking challenge results. In ECCV Workshops, 2018. 5, 6, 7,"
        }
    ],
    "affiliations": [
        "College of Computer Science and Engineering, Guilin University of Technology, China",
        "School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China",
        "School of Computer Science, Fudan University, Shanghai, China"
    ]
}