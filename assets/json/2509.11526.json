{
    "paper_title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis",
    "authors": [
        "Wenhao Tang",
        "Sheng Huang",
        "Heng Fang",
        "Fengtao Zhou",
        "Bo Liu",
        "Qingshan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 2 5 1 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Multiple Instance Learning Framework with Masked Hard\nInstance Mining for Gigapixel Histopathology Image Analysis",
            "content": "Wenhao Tang1, Sheng Huang1*, Heng Fang1, Fengtao Zhou2, Bo Liu3, Qingshan Liu4 1*School of Big Data & Software Engineering, Chongqing University, Chongqing, China. 2CS, Hong Kong University of Science and Technology, Hong Kong, China. 3CS, Hefei University of Technology, Hefei, China. 4CS, Nanjing University of Posts and Telecommunications, Nanjing, China. *Corresponding author(s). E-mail(s): huangsheng@cqu.edu.cn; Contributing authors: whtang@cqu.edu.cn; fangheng@cqu.edu.cn; fzhouaf@connect.ust.hk; kfliubo@gmail.com; qsliu@njupt.edu.cn;"
        },
        {
            "title": "Abstract",
            "content": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes Siamese structure with consistency constraint to explore the hard instances. Using class-aware instance probability, MHIM-MIL employs momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL. Keywords: Medical Image Analysis, Gigapixel Image Analysis, Computational Pathology, Multiple Instance Learning, Hard Instance Mining"
        },
        {
            "title": "1 Introduction",
            "content": "Computational pathology (CPath), representative branch in gigapixel image analysis, is an interdisciplinary field that advances computational methods to analyze and model gigapixel 1 histopathology images [1]. In modern medicine, histopathology image analysis plays crucial role, especially in precision oncology, where it is the Fig. 1 Comparison of MHIM-MIL with baseline methods. The blue outlines denote tumor regions annotated by pathologists; subfigure (a) shows masked instances as black patches. Subfigure (b) displays attention scores from the student model, where brighter patches indicate higher saliency (i.e., mined salient instances), while subfigure (c) presents the softmaxnormalized attention weights, with cyan indicating high probability of tumor presence, which should ideally align with the blue tumor boundaries. Baseline methods tend to focus on highly activated or trivial regions (top part of subfigure (b) in the tumor slide) or noisy activations (top-right of normal tissue slides); in contrast, MHIM-MIL suppresses such dominant patches via masking, encouraging exploration of less-salient yet informative areas (bottom part of subfigure (b)), leading to more comprehensive and robust predictions. gold standard for cancer diagnosis [25], subtyping [6], and prognosis [7, 8]. Digitizing pathological images into Whole Slide Images (WSIs) has opened new avenues for CPath [9, 10]. Due to the huge size of WSI and the lack of pixellevel annotations, histopathological image analysis is commonly framed as multiple instance learning (MIL) task [11, 12]. In MIL, each WSI (or slide) is considered bag containing thousands of unlabeled instances (patches) cropped from the slide. If at least one instance is positive, the bag is labeled positive; otherwise, it is labeled negative. However, the number of slides is limited, and each slide contains large number of instances with low positive proportion. This imbalance hinders the inference of bag labels [13, 14]. To address this issue, several CPath methods [3, 9, 1416] employ attention mechanisms to aggregate salient instance features into bag-level feature for CPath. Figure 1 demonstrates the superior capability of these methods in mining salient instances from large number of instances. Furthermore, some MIL frameworks [13, 15, 17, 18] continue to advance the concept of salient instance mining and focus on how to utilize it to facilitate CPath. For instance, existing frameworks [13, 18] propose selecting only the instances that correspond to the top highest or lowest attention scores [15, 18] or patch probabilities [13] to yield high-quality bag embeddings for both training and testing. These salient instances are essentially easyto-classify instances, which are suboptimal for training discriminative MIL model. The left part of Figure 1 demonstrates that previous MIL models tend to focus on salient regions. However, they fail to effectively utilize this information. Instead, they neglect most features and excessively concentrate on few salient features during feature aggregation, compromising model discriminativeness. Moreover, the right part further illustrates the potential misleading of these easy-to-classify instances, causing the models to be influenced by noisy adipose tissue in its discrimination. In traditional machine learning, such as Support Vector Machines (SVM) [19], samples near the category distribution boundary are more challenging to classify but are more valuable for defining the classification boundary. Furthermore, other deep learning studies [2022] have shown that incorporating hard samples in training can enhance model generalization. By applying this idea at the instance level, we can better highlight the hard-to-classify instances that facilitate MIL model training and benefit the tasks in 2 CPath. However, the absence of instance labels presents challenge in applying traditional hard sample mining directly at the instance level. To address this issue, we propose novel MIL framework based on masked hard instance mining strategies (MHIM), called MHIM-MIL. The core concept of MHIM is to mask easy-to-classify instances, thereby emphasizing hard instances for model training. We introduce momentum teacher model to evaluate all instances and propose class-aware instance probability for more accurate assessment of easy-to-classify instances. Unlike class-agnostic attention that focus solely on salient features without encoding any class information, this approach directly measures the classification probability of instances. Consequently, the teacher model effectively mines hard instances for training the MIL model (i.e., the student model). Importantly, the momentum teacher is updated using an exponential moving average (EMA) of the student model. In contrast to conventional MIL frameworks [13, 18] that employ complex cascade gradient-updating structures, our method is simpler and has less parameters. This approach not only enhances efficiency but also improves performance stability. Furthermore, MHIM is optimized by introducing consistency constraint that enhances the students ability to extract discriminative features from hard instances. The notable feature of MHIM is its ability to provide high-quality hard instances for MIL model training. In scenarios with limited slides and an abundance of patches, the hard instance sequence ideally should be non-redundant, diverse, sufficiently challenging, and error-free. To achieve this, we mask out small portion (e.g., < 5%) of the easiest-to-classify instances and then introduce large proportion (e.g., 70% 90%) of random masking. This strategy not only significantly reduces sequence length but also enhances the diversity of hard instance sequences, improving both training efficiency and model generalization. Moreover, to mitigate the potential risk of losing key features due to our large-scale random masking strategy, we propose the Global Recycle Network (GRN) to recover these critical features from randomly masked instances at global level. We validate MHIM on various CPath tasks, including cancer diagnosis (CAMELYON [23, 24]), subtyping (TCGA-NSCLC, TCGA-BRCA), and survival analysis (TCGA-LUAD, TCGALUSC, TCGA-BLCA). To demonstrate the frameworks generalizability, we employ multiple feature extractors (ResNet-50 [25], PLIP [26], UNI [27]) and baseline models (AB-MIL [14], TransMIL [9], DSMIL [15]). Under multi-fold cross-validation settings, MHIM shows significant improvements across all three baseline models and achieves stateof-the-art performance on multiple benchmarks. For instance, MHIM (TransMIL) improves the C-index by 1.8% and 1.5% compared to the baseline and the second-best model, respectively, on TCGA-BLCA-UNI. Furthermore, we shows the advantages of MHIMs computational cost. MHIM (TransMIL) reduces training time and memory consumption by 20% and 50%, respectively. Parts of this paper were published originally in ICCV 2023 [28]. However, we refer to this version as MHIM-v2 and extend our earlier work in several important aspects: We improve instance assessment by introducing class-aware instance probability, which enables the teacher to more accurately mine hard instances compared to attention scores. We simplify the masked hard instance mining strategy to improve framework generalization and incorporate Global Recycling Network (GRN) to reuse randomly masked features. Extensive experiments show that the GRN boosts the performance of various baseline models without significantly affecting efficiency. We extend our MHIM framework to Survival Analysis, providing more comprehensive evaluation of its impact on CPath. We expand the dataset for cancer diagnosis and subtyping tasks, introduce feature extraction network based on multimodal pathology large model, and standardize all experiments to 5-fold cross-validation, enabling more systematic and general validation of the MHIM."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we review the related work from two main perspectives: Computational Pathology and Hard Sample Mining. We first provide comprehensive overview of computational pathology, detailing the dominant MIL paradigm, its variants, clinical applications, multi-modal extensions, and common instance sampling strategies. 3 We then discuss the concept of hard sample mining in the broader computer vision field, which forms the core inspiration for our approach to address the limitations of existing MIL-based pathology methods."
        },
        {
            "title": "2.1 Computational Pathology",
            "content": "The shift from traditional glass slides to digital pathology has unlocked vast array of possibilities for computational pathology. This emerging field seeks to integrate the expertise of pathology with advanced image analysis and cuttingedge computer science methodologies to create sophisticated, computer-assisted tools for the interpretation and analysis of pathology images. By leveraging these technologies, computational pathology not only enhances diagnostic accuracy but also facilitates large-scale data analysis, enabling the discovery of novel biomarkers and the development of personalized treatment strategies [2933]. Given the gigapixel resolution of WSIs, which makes direct processing infeasible and fine-grained annotation prohibitive, the field has widely adopted weakly-supervised learning paradigm to effectively analyze these massive datasets. Multiple Instance Learning in Computational Pathology. Among various methodologies, Multiple Instance Learning (MIL) [12] has emerged as the de facto standard [15, 34 37]. MIL is weakly supervised learning framework that utilizes coarse-grained bag labels for training instead of fine-grained instance annotations. Previous works can be broadly categorized into two groups: instance-level [3842] and embedding-level [10, 4347]. The former obtains instance labels and aggregates them to obtain the bag label, whereas the latter aggregates all instance features into high-level bag embedding for bag prediction. Due to the lack of instance labels, recent instance-level methods focus on generating pseudo-labels. For example, Qu et al. [42] proposed an accurate pseudo-label generator through prototype learning within weakly supervised contrastive learning, which effectively facilitates instance-level learning. However, these methods have become less prevalent as their performance is fundamentally bottlenecked by the quality of the generated pseudo-labels, which are often noisy and unreliable under weak baglevel supervision. Conversely, most embeddinglevel methods [9, 13, 15] share the basic idea of AB-MIL [14], which employs learnable weights to aggregate salient instance features into bag embedding. Furthermore, some MIL frameworks [13, 15, 17, 18] mine more salient instances, making classification easier and facilitating model [17] selected learning. For example, Lu et al. the most salient instances based on their attention scores to compute instance-level loss and improve performance. Zhang et al. [13] proposed CAM-1D based on the AB-MIL paradigm to better mine salient instances and used AB-MIL to aggregate them into bag embedding. Recent Mamba-based MIL [4850] frameworks leverage the Mamba state-space model to efficiently capture long-range dependencies in gigapixel WSIs, enabling linear-time integration of global context. However, they still require specialized designs to mitigate the inherent constraints of sequential modeling. In addition, inspired by CLIP [51], vision-language-based methods [16, 26, 52, 53] have recently emerged. These methods explore how to generate and utilize prompts to capture salient instances from thousands of image patches. For example, Shi et al. [52] proposed dualscale visual descriptive text prompt based on frozen large language model to effectively boost the performance of vision-language models. However, these methods focus excessively on salient instances during training, which are easy instances with high confidence scores and can be easily classified. Consequently, they overlook the importance of hard instances for training. In this paper, we aim to mine hard instances to improve CPath performance. Clinical Tasks on Computational Pathology. Building upon the MIL framework, CPath has achieved significant progress in various clinical applications. Currently, computational pathology is widely applied in various tasks including automated tumor diagnosis, subtyping, survival analysis, grading of cancers, and quantification of histopathological features, thereby significantly enhancing diagnostic consistency and reducing the workload on pathologists. Among these tasks, classification and survival prediction stand out as two of the most crucial applications. Classification, which encompasses diagnosis and subtyping, focuses on accurately identifying and categorizing 4 [7] different types of cancers based on histopathological images [6, 9, 1315, 17, 28]. This is essential for determining the appropriate course of treatment and has been significantly advanced by deep learning techniques, which can discern subtle patterns and features in pathology slides that may not be easily detected by the human eye. Survival analysis, critical task in cancer prognosis, aims to assess the probability of an event (typically death) occurring for particular patient and accurately rank the risks of cancer patients. It is indispensable in computational pathology, providing insights into disease progression, treatment effectiveness, and patient prognosis. This task can be expressed as an estimation of the hazard function, and early classical methods [54, 55] were based on Coxs regression model [56]. Recently, deep learning-based methods have become mainstream, with some recent works [7, 8, 57] constructing effective survival analysis models based on pathological images. These methods also rely on the MIL paradigm, attempting to utilize AB-MIL to focus on salient regions in WSI. For example, Yao et al. introduced the siamese MI-FCN and attention-based MIL pooling to efficiently learn imaging features from the WSI and then aggregate WSI-level information to the patient level. Multi-modal in Computational Pathology. While unimodal approaches using only WSIs have formed the foundation of CPath, recent and powerful trend is the integration of multiple data modalities to further boost performance and robustness. Integrating genomics and WSIs in multimodal approaches [5861] has recently attracted increasing attention due to their advantages in performance and robustness. The stateof-the-art method CMTA [58] has demonstrated that leveraging genomics information and tailored design can outperform unimodal methods by large margin. The integration of multiple modalities is not limited to genomics. For example, Song et al. [62] proposed Swin-Transformer fusion model that integrates WSIs with CT images, achieving notably higher prognostic accuracy than either modality alone. Nakhli et al. [61] introduced graph-based transformer that combines multiple histopathology image modalities per patient, which significantly outperformed prior methods and remained robust even when some modality data were missing. Other approaches incorporate clinical attributes or textual pathology"
        },
        {
            "title": "Sampling",
            "content": "reports alongside WSIs to inject domain knowledge and improve interpretability [63], highlighting the promise of visionlanguage and knowledgeintegrated models in this field. These multimodal studies underscore the potential of fusing heterogeneous data sources for more robust and informative pathology AI models. However, despite the significant performance advantages of multimodal methods, obtaining high-quality multimodal data is more challenging. We aim to enhance the performance of unimodal methods in computational pathology to narrow the gap with multimodal methods, providing lower-cost and more generalizable option. Instance in Computational Pathology. MIL-based computational pathology aims to identify the most informative features and predict the final result. However, it is non-trivial to mine high-quality features from thousands of instances, and long sequence inputs pose significant memory challenges for Transformer models. Consequently, many methods attempt to sample long instance sequences to reduce problem complexity. Random sampling is classic method, and [6, 64, 65] use random strategies to divide the total instances into several groups, referred to as pseudo-bags. Then, the bag labels are assigned to the pseudo-bags as pseudo-labels. Although this simple and effective strategy can it also faces achieve substantial improvement, serious noise label issues. To alleviate these issues, some efforts [66, 67] employ attention mechanisms to improve sampling and pseudo-bag labeling. However, the pseudo-bag method still cannot effectively solve the efficiency dilemma caused by long sequence inputs. Therefore, recent works [3, 4, 68] directly sample the instance sequence randomly and discard the remaining parts to enable Transformer model training. In addition, feature clustering methods [45, 69, 70] compute cluster centroids of all feature embeddings and sample representative embeddings [71] utilizes for the final prediction. Moreover, reinforcement learning to propose an iterative dynamic instance sampling strategy to improve instance selection and feature aggregation. While our method shares similar spirit with the above approaches, we focus on how to simply and efficiently mine high-quality hard instances to facilitate model training and significantly enhance training efficiency. 5 Fig. 2 Overview of proposed MHIM-MIL. momentum teacher is used to compute class-aware instance probability for all instances. We mask instances based on these probabilities using randomly high score masking (RHSM) and random score masking (RSM) strategies. Then, we employ global recycle network (GRN) to reconstruct key features from the largescale RSM-masked instances. Finally, we feed these reconstructed features along with unmasked instances to the student model. The student is updated using consistency loss term Lcon and label error loss term Lcls. The teacher parameters are updated with an Exponential Moving Average (EMA) of the student parameters without gradient updates. During inference, we use the complete input instances and the student model only."
        },
        {
            "title": "2.2 Hard Sample Mining",
            "content": "Hard sample mining is popular technique to speed up convergence and enhance the discriminative power of the model in many deep learning areas, such as face recognition [22, 72], object detection [73, 74], person re-identification [20, 21, 75, 76], graph representation learning [7780], deep metric learning [81, 82], and more [8385]. The main idea behind this technique is to select the samples which are hard to classify correctly (i.e., hard negatives and hard positives) for alleviating the imbalance between positive and negative samples and facilitating model training[86]. There are generally three groups of approaches for evaluating sample difficulty: loss-based [87], similaritybased [88], and learnable weight-based [89]. Typically, these strategies require complete sample supervision information. Drawing on the ideas of the above works, we propose hard instance mining approach in MIL, mining hard examples at the instance level. In this, there are no complete instance labels, only the bag label is available. Similar to our approach, Li et al. [90] utilized attention scores to identify salient instances from false negative bags to serve as hard negative instances and used them to compose the hard bags for improving classification performance. key difference is that we indirectly mine hard instances by masking out the most salient instances rather than directly locating hard negative instances."
        },
        {
            "title": "3.1 Background: MIL Formulation",
            "content": "In MIL, WSI is modeled as bag of instances, represented as = {xi}N i=1, where xi is patch extracted from the WSI and is the total number of instances. For classification task, known bag-level label is available, while the instance-level labels yi are unknown. The objective of MIL model M() is to predict the bag label based on all instances: ˆY M(X). 6 While classical instance-level methods derive the bag label from instance-level predictions, our focus is on more prevalent embedding-level approach that involves an instance aggregation step. This process creates bag representation from the extracted features of all instances in the bag, = {zi}N i=1. classifier C(), trained on this representation, is then used to predict the bag label, ˆY C(F ). There are two primary strategies for instance aggregation, one of which is the attention-based aggregation method [14], denoted as follows, (cid:88) = aizi RD, (1) i=1 where ai is the learnable scalar weight for zi, and is the dimension of vector and zi. Many embedding-level works [13, 15, 17] follow this formulation but differ in the ways they generate the attention score ai. Notably, DSMIL [15] predicts instance labels during the process of aggregating bag embedding, with attention scores relying on the predicted instance labels. is the multi-head self-attention (MSA) based aggregation [9]. In this approach, class token z0 0 is combined with the instance features to form the initial input sequence 0 = (cid:2)z0 (cid:3) R(N +1)D for aggregating instance features. With the single attention head headh, this can be formulated as, 1, . . . , z0 N"
        },
        {
            "title": "Another",
            "content": "0, z0 (cid:1) R(N +1) headh = Aℓ , ℓ = Concat (head1, , headH ) O, (cid:0)Z ℓ1W ℓ = 1 . . . L, ℓ = 1 . . . L, (2) where RD and RDD are the learnable projection matrices of MSA. Aℓ R(N +1)(N +1) is the h-th head attention matrix of the ℓ-th layer, is the number of MSA block, and is the number of head in each MSA block. The bag embedding is the class token of the final layer, = zL (3) 0 . It is to be noted that the self-attention-based aggregation fundamentally represents variant of attention-based MIL aggregation. This approach excels in multiple computational pathology tasks due to its superior long-distance modeling capabilities. However, it also faces challenges related to efficiency due to the processing of long-sequence inputs. Collectively, these methods can be referred to as the general attention-based MIL. 7 Algorithm 1: Hard Instance Mining # t: the MIL model of teacher networks # t: the classifier of teacher network # mhr: high score mask ratio # mrr: random mask ratio # g: global query # mm q: momentum rates of global query def mhim fn(x): # easy instance assessment attn,f ins = t(x) ins = einsum(\"nd,n->nd\",f ins,attn) score = t(f ins) # high score masking score = sort(score) ids = topk(score,mhr*score.length) z, = mask(x,ids h) # high score mask ratio decay cosine decay(mhr) # large scale masking ids = random select(z,mrr*z.length) z,z = mask(z,ids r) # global recycle network q.require gradient = False = mca(g q,z m) # update global query via ema = mm q*g q+(1-mm q)*z m) # obtain the final hard instances for student model = concat(z,z m, dim=0) return z"
        },
        {
            "title": "3.2 MHIM-MIL for CPath",
            "content": "In general attention-based MIL frameworks, the attention score typically serves as criterion to evaluate the importance of instances, denoting their contributions to bag embedding. The salient instances with high scores are useful for classifying WSI in the testing phase, but are not conducive to training MIL model with good generalization ability. Although hard samples have been proven to enhance the generalization ability of the model in many computer vision scenarios [20, 21, 81, 91], previous MIL work focuses more on exploiting salient instances and neglecting the utilization of hard instances in model optimization. In this paper, we propose simple and efficient MIL framework with Masked Hard Instance Mining (MHIM-MIL) to enhance CPath. As illustrated in Figure 2, the MHIM-MIL framework incorporates Siamese structure during training. The backbone of our framework is general attention-based MIL model (referred to as Student), denoted as S(), which aggregates instance features. To increase the difficulty of training and encourage the student model to focus on hard instances, we introduce momentum teacher, represented as (). The teacher assesses easyto-classify instances using the proposed classaware instance probability. Class-aware instance probability provides better evaluation of easy instances and facilitates hard instance mining compared to class-agnostic attention scores. Using these assessment results, the teacher obtains highquality sequences of hard instances through masked hard mining strategy. Specifically, it first applies small-scale high-score masking followed by large-scale random masking to reduce redundancy and enhance sequence diversity. To mitigate the risk of losing critical features due to largescale random instance masking, the student model incorporates recycle network R() to recover crucial features. Finally, the student model predicts bag labels by concatenating mined hard features and recycled features. The teacher model shares the same network structure as the student model, except for the additional recycle network, and does not require gradient-based updates. Due to the varying number of instances within each bag, non-batch gradient descent algorithm (i.e., SGD with batch size 1) is used to optimize the MIL model. Consequently, unlike traditional MIL frameworks that employ double-tier gradient updating models [13, 18], this Siamese structure enables more stable and efficient training with fewer parameters. The proposed framework is formulated as: ˆY = (cid:17) (cid:16) (cid:101)Z = (R (MT (Z))) , (4) where MT () denotes the masked hard instance mining executed through the teacher model, and (cid:101)Z represents the final processed instances."
        },
        {
            "title": "3.3 Masked Hard Instance Mining",
            "content": "Traditional hard sample mining strategies often struggle in the absence of instance-level supervision. To tackle this challenge, we propose masked hard instance mining approach that leverages assessment scores to implicitly identify difficult instances by masking those that are easier with higher assessment scores. Fig. 3 Illustration of various hard instance mining methods."
        },
        {
            "title": "3.3.1 Easy Instance Assessment",
            "content": "The teacher assesses each instance to identify the easy ones. More specifically, given complete sequence of instance features = {zi}N i=1 as the input of the teacher (), the teacher outputs the attention score ai for each instance as follow, = [a1, . . . , ai, . . . , aN ] = (Z) . (5) The attention score indicates focus level of the MIL model on each instance, making it an intuitive and commonly used metric for evaluation. However, it does not explicitly show the contribution of each instance to the classification outcome. As highlighted in literature [13], instances with higher attention scores may contribute insignificantly to the diagnosis. Inspired by this observation, we introduce the Class-aware Instance Probability to evaluate instance more accurately within general attention-based MIL model. Specifically, for the teacher that include an instance classifier, such as DSMIL [15], we utilize the instance classifier to classify the weighted features derived from multiplying attention weights by instance features. For teachers without an instance classifier, we employ the bag classifier for this purpose. This process is defined as, = CT (A Z) , (6) where CT () is the instance classifier of the teacher model. is the obtained class-aware instance probabilities and is the attention weights. This computation does not introduce additional networks and effectively enhances the accuracy of 8 the general attention-based MIL model in easyclassify instances assessment. Further discussions on this can be found in the experimental section. Then, we obtain the indices of the score sequence in descending order by applying sorting operation on S, = [i1, i2, . . . , iN ] = Sort (S) , (7) where i1 is the index of the instance with the highest score, while iN is the index of the one with the lowest score. With this index collection I, we introduce several strategies for masked hard instance mining to identify hard instances. An -dimensional binary vector = [m1, . . . , mi, . . . , mN ] is defined to encode the masking flag of instances, where mi {0, 1}. An instance is masked if mi = 1, otherwise, it is considered unmasked."
        },
        {
            "title": "3.3.2 Randomly High Score Masking",
            "content": "The simplest yet most critical strategy in masked hard instance mining is the High Score Masking (HSM) strategy, which masks instances with the top βh% highest assessment scores. For HSM, the instance mask flags are initially set to zero vectors, Mh(:) = 0. We then select the indices of instances with scores in the top βh%, denoted by Ih = [it]βh%N . These indices are used to update t=1 the mask flags, setting Mh(Ih) = 1. To ensure that positive instances are preserved within the unmasked sequences, we also utilized techniques such as mask ratio decay. The ratio βh% is nonlinearly decreased using cosine function as training progresses. Despite these measures, HSM may face major challenge: it could mask all distinctive features in the early stages of training, leading to error instance mining. To mitigate this, we introduce the Randomly High Score Masking technique, which selects instances with the top 2βh% attention scores as potential candidates and randomly masks half of them to preserve distinctive information. Figure 5 illustrates this approach. The mined instance sequence can be obtained by applying the high score masking operation, ˆZh Mh (Z) = Mask (Z, Mh) ˆN D, (8) where the ˆN = βh% is the number of unmasked instances. Fig. 4 Illustration of Global Recycle Network."
        },
        {
            "title": "3.3.3 Large Scale Masking",
            "content": "Although HSM demonstrates effectiveness in mining hard instances, it is limited by low masking ratio (e.g., < 5%), which fails to significantly improve the efficiency of the MIL model. Overlength sequences not only interfere with the training of student models but also hinder the application of advanced aggregation mechanisms, such as Multi-Head Self-Attention (MSA). To address this, we apply high ratio (e.g., 70% 90%) masking to the instances mined by HSM, as depicted in Figure 5 (c) and (d). We propose the following two strategies to refine hard instance mining and achieve more properties: Low Score Masking (LSM): We use the same pipeline as HSM to generate the mask flags Ml for masking the instances with the top βl% lowest attention scores in order to filter out the redundant uninformative instances and improve efficiency. Random Score Masking (RSM): Randomness is beneficial to reduce the risk of overfitting. We generate random mask flag vector Ml with given random ratio βl%, and combine it with HSM for introducing the randomness to the hard instance mining. Once the large ratio masking flag Ml is produced, we can obtained new mined instance ˆZl R(1βl) ˆND and masked sequence sequence ˆZm Rβl ˆND, { ˆZl, ˆZm} Ml (cid:17) (cid:16) ˆZh = Mask (cid:16) ˆZh, Ml (cid:17) , (9) where ˆZl ˆZm = ˆZh. We believe RSM is the better choice as it can more effectively alleviate over-fitting by increasing image diversity. Moreover, given the inherent sparsity in attention-based MIL models, LSM struggles to further reduce redundancy. Algorithm 2: Framework Optimization # t, s: teacher and student networks # p: the pretrained network # m: momentum rates # tp: temperatures # a: consistency loss scaling factor # initialize t.params = p.params s.head.params = p.head.params # load slide and label x,y for x,y in loader: # get bag embedding from teacher ,be = t.forward(x) # stop gradient of teacher network be = be t.detach() # masked hard instance mining # algorithm 1 gives more details hard = mhim fn(x) logits s,be = s.forward(x hard) # consistency loss loss con = -softmax(be / tp) * log softmax(be s) # label prediction loss loss cls = cross entropy(logits s,y) loss all = loss cls + a*loss con # gradient descent: student network loss all.backward() update(f s.params) # ema update: teacher network t.params = m*f t.params+(1-m)*f s.params"
        },
        {
            "title": "3.4 Global Recycle Network",
            "content": "Masking instances at high ratio inevitably lead to risk of losing critical features, which in turn impairs the performance of the MIL model. This issue becomes more pronounced when key features are sparse. To address this, we propose Global Recycle Network (GRN) to recover masked discriminative features from global perspective. We initialize global queries QG RKD. The Multi-head Cross-Attention (MCA) computation between the masked instance sequence ˆZm and QG, represented as MCA (), is employed to mine key features. The process is formalized as: λq) (cid:101)Zm. This approach ensures that the query vector effectively captures key features from global perspective while maintaining the stability of the global query vector. In such manner, the final hard instances can be denoted as, (cid:101)Z = (cid:17) (cid:16) ˆZ = Concat (cid:16) ˆZl, (cid:101)Zm (cid:17) . (11)"
        },
        {
            "title": "3.5 Consistent Iterative Learning",
            "content": "Within the Siamese architecture, the teacher model not only guides the training of the student model but also updates itself with the new knowledge acquired by the student model. This iterative optimization process gradually enhances the mining capabilities of the teacher model and the discriminative power of the student model. To further facilitate this optimization and help the student model learn more effectively from hard instances, we introduce consistency loss denoted as Lcon. This aims to align the bag embedding of the teacher model, which takes the entire instance sequence as input, with that of the student model, which uses only the hard instances. This not only accelerates the convergence of the student model during the initial phase of training but also enhances its ability to extract discriminative features from hard instances. The entire optimization process can be further divided into two parts based on the two branches. Student Optimization: There are two losses in student optimization. One is the cross-entropy for measuring the bag label prediction loss, Lcls = log ˆY + (1 ) log (cid:16) 1 ˆY (cid:17) . (12) The other is the representation consistency loss between the bag embedding of student Fs and the one of momentum teacher Ft, Lcon = softmax (Ft/τ ) log Fs, (13) (cid:101)Zm = MCA (cid:16) QG, ˆZm (cid:17) , (cid:101)Zm RKD, (10) where the τ > 0 is temperature parameter. Overall, the final optimization loss is as follows: where is the number of queries. Importantly, QG is updated not through gradient descent but via Exponential Moving Average (EMA), with the update formula being QG λqQG + (1 { ˆθs} arg min θs = Lcls + αLcon, (14) where θs is the parameters of the student network S(), and α > 0 is scaling factor. 10 Table 1 Details of the Primary WSI Datasets Used. # denotes the number of it. APPW is the Average Patch per WSI. Name #Case #WSI #APPW CAMELYON TCGA-NSCLC TCGA-BRCA TCGA-LUAD TCGA-LUSC TCGA-BLCA 370 956 1062 478 478 899 1053 1131 541 512 457 8108 10302 8746 10629 10395 14364 Teacher Optimization: The parameters of momentum teacher θt are updated by the EMA of the student parameters. The update rule is θt λθt + (1 λ)θs, where θt is the parameters of the teacher network () and λ is hyperparameter. Moreover, the updated teacher model will be used in the next iteration of hard instance mining."
        },
        {
            "title": "3.6 Inference",
            "content": "In the testing stage, only the student model is used for predicting the bag label. As shown in Figure 2, the hard instance mining step will not be conducted in the inference phase. In other words, ˆY = where (cid:101)Z = Concat (Z, R(Z)). (cid:101)Z (cid:16) (cid:17)"
        },
        {
            "title": "4.1.1 Datasets",
            "content": "We validate MHIM on various computational pathology tasks, cancer diagnosis including (CAMELYON [23, 24]), subtyping (TCGANSCLC, TCGA-BRCA), and survival analysis (TCGA-LUAD, TCGA-LUSC, TCGA-BLCA). CAMELYON-16 [23] and CAMELYON17 [24] are among the largest publicly available datasets for breast cancer lymph node metastasis diagnosis, both containing binary (metastasis or not) class labels. The CAMELYON-16 dataset includes 270 training WSIs, along with an additional 129 slides as the official test set. Moreover, the CAMELYON-17 dataset encompasses 1000 WSIs from five different medical centers in the Netherlands. Given that the labels for the 500 WSIs in the CAMELYON-17 official test set are not publicly available, we only used the training set of CAMELYON-17, which includes 500 WSIs from 100 cases (with corresponding image-level diagnostic results). We combined CAMELYON-16 11 and CAMELYON-17 into single dataset, named CAMELYON, totaling 899 WSIs (591 negative, 308 positive) from 370 cases. The Non-Small Cell Lung Cancer (NSCLC) project of The Cancer Genome Atlas (TCGA) by the National Cancer Institute (NCI) is the primary dataset for the cancer subtyping task. TCGA-NSCLC is the most common type of lung cancer, accounting for approximately 85% of all lung cancer cases. This classification includes several subtypes, primarily Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC). The dataset contains 541 slides from 478 LUAD cases and 512 slides from 478 LUSC cases, with only image-level labels provided. Compared to CAMELYON, the tumor regions in the positive samples of this dataset are significantly larger."
        },
        {
            "title": "The Breast",
            "content": "Invasive Carcinoma (TCGABRCA) project is another subtyping dataset we used. TCGA-BRCA includes two subtypes: Invasive Ductal Carcinoma (IDC) and Invasive Lobular Carcinoma (ILC). It contains 779 IDC slides and 198 ILC slides from 1062 cases . The goal of survival analysis is to estimate the survival probability or survival time of patients over specific period. Therefore, we used the TCGA-LUAD, TCGA-LUSC, and TCGABLCA projects to evaluate the model performance for survival analysis tasks. Unlike the diagnosis and subtyping tasks, the survival analysis datasets are case-based rather than WSI-based. The WSIs of TCGA-LUAD and TCGA-LUSC are identical to those used in the subtyping task but with different annotations. The TCGA-BLCA includes 376 cases of bladder urothelial carcinoma."
        },
        {
            "title": "4.1.2 Preprocess",
            "content": "Following prior works [9, 13, 17, 92], we crop each WSI into series of non-overlapping patches of size (256 256) at 20 magnification and discard the background regions, including holes, as in CLAM [17]. Table 1 shows the details of the preprocessed datasets, where the average number of patches per dataset is around 10,000. To efficiently handle the large number of patches, we follow the traditional two-stage paradigm, using pre-trained offline model to extract patch features. This includes ResNet-50 [25] pre-trained on ImageNet-1k [93]. Specifically, the last convolutional module of the ResNet-50 is removed, Table 2 Cancer diagnosis and subtyping results on CAMELYON and TCGA-NSCLC. The highest performance is in bold, and the second-best performance is underlined. The Accuracy and F1-score are determined by the optimal threshold. MHIM(baseline) is the conference version of this paper, and we refer to the improved framework as MHIM-v2(baseline). 0 5 - s I Methods AB-MIL [14] CLAM [17] DSMIL [15] TransMIL [9] DTFD-MIL [13] IBMIL [6] MHIM(AB-MIL) [28] MHIM(TransMIL) [28] MHIM(DSMIL) [28] R2T-MIL [92] 2DMamba [48] MHIM-v2(AB-MIL) MHIM-v2(TransMIL) MHIM-v2(DSMIL) AB-MIL [14] CLAM [17] DSMIL [15] TransMIL [9] DTFD-MIL [13] IBMIL [6] MHIM(AB-MIL) [28] MHIM(TransMIL) [28] MHIM(DSMIL) [28] R2T-MIL [92] 2DMamba [48] MHIM-v2(AB-MIL) MHIM-v2(TransMIL) MHIM-v2(DSMIL) CAMELYON TCGA-NSCLC Accuracy AUC F1-score Accuracy AUC F1-score 88.091.69 88.202.14 87.532.24 88.421.45 87.641.76 88.081.68 89.202.11 89.981.16 89.872.23 89.762.39 88.181.59 89.761.81 90.201.49 89.531.49 91.871.35 90.872.49 90.871.44 90.312.04 90.640.83 91.762.27 90.981.73 90.422.46 90.982.14 91.871.74 90.412.94 91.870.77 90.312.64 92.311.14 91.592.27 91.432.09 90.412.25 91.231.48 91.491.57 91.502.06 92.301.79 93.021.12 92.662.16 92.891.63 90.961.93 92.771.97 93.470.57 92.931. 94.981.06 95.251.49 93.701.99 94.561.28 95.391.51 94.661.87 94.941.56 94.301.72 95.321.59 95.031.56 94.772.64 95.681.28 95.801.66 95.641.28 83.352.74 83.233.07 82.892.90 84.051.90 82.512.02 83.962.19 84.893.02 86.171.85 86.033.25 85.503.19 83.442.03 85.253.47 86.472.11 85.661.75 88.711.68 87.483.16 87.222.35 87.072.57 87.311.37 88.583.59 87.801.69 86.913.11 88.022.62 88.602.60 87.042.95 88.911.08 87.533.27 89.391.41 90.991.93 90.522.08 90.231.63 90.041.86 89.851.53 90.041.48 91.272.35 90.612.17 92.122.59 91.752.38 90.923.06 91.942.24 91.372.04 92.312.62 91.092.35 90.802.35 90.703.00 90.523.13 90.422.98 91.183.27 91.741.88 90.802.23 91.272.17 92.132.55 90.622.75 92.691.43 91.272.34 92.601.63 95.321.39 95.371.08 95.570.81 94.971.11 95.551.47 95.571.13 96.021.35 95.981.37 96.691.26 96.401.13 96.271.77 96.221.44 96.291.44 96.821. 95.591.98 95.461.72 95.701.87 95.521.95 95.831.75 95.622.09 96.211.26 96.082.03 96.231.51 96.401.45 96.312.07 96.611.09 96.381.55 96.651.23 90.951.93 90.081.97 90.201.63 89.941.73 89.601.67 89.731.64 90.852.53 90.561.89 91.972.44 91.262.60 90.702.96 91.892.23 91.322.01 92.292.62 91.072.36 90.382.46 90.193.11 90.133.31 89.913.01 90.943.20 91.201.89 90.382.49 90.962.36 91.832.50 90.292.71 92.641.44 91.242.34 92.581.64 and global average pooling is applied to the final feature maps to generate the initial feature vector. Additionally, we also use state-of-the-art foundation models pre-trained on WSIs, such as PLIP [26] and UNI [27]. The use of these advanced feature extractors enhances the rigor of our experiments and further demonstrates the generalizability of our method."
        },
        {
            "title": "4.1.3 Evaluation Metrics",
            "content": "Following previous work [9, 17, 92], we leverage Accuracy, Area Under Curve (AUC), and F1-score to evaluate model performance in the cancer diagnosis and subtyping tasks. AUC is the primary performance metric in the binary classification task, and we only report AUC in ablation experiments. For the survival analysis task, we refer to related works [57, 58, 94] and use the Concordance Index (C-index) [95] as the evaluation metric. The C-index is considered an extension of AUC that accounts for censored data. This index comprehensively reflects the capability of model to reliably rank survival times based on individual risk scores."
        },
        {
            "title": "4.2 Implementation Details",
            "content": "Following [9, 13, 17], the offline feature is projected to 512-dimensional feature vector using fullyconnected layer. The momentum rate of EMA in teacher optimization is set to 0.9999. An Adam optimizer [96] with learning rate of 2 104 and weight decay of 1 105 is used for model training. The learning rate is adjusted using the cosine annealing strategy. All models were trained for 200 epochs with an early-stopping strategy applied to the cancer diagnosis and subtyping tasks. The patience values for CAMELYON and TCGA are set to 30 and 20, respectively. For survival analysis, all models are trained for 30 epochs. No additional techniques, such as gradient clipping or gradient accumulation, are used to enhance model performance. The batch size is set to 1. All experiments are conducted using 5-fold cross-validation, and the training process does not use validation set due to data scarcity. We report the mean and standard deviation in the following tables. Regarding computational resources, the cancer diagnosis and subtyping tasks were trained on an NVIDIA RTX 3090, while the survival task was run on an NVIDIA V100 (32G). 12 Table 3 Subtyping results on TCGA-BRCA. Methods Accuracy AUC F1-score 0 5 - s I AB-MIL CLAM DSMIL TransMIL DTFD-MIL IBMIL MHIM(AB.) MHIM(Tr.) MHIM(DS.) R2T-MIL 2DMamba MHIM-v2(AB.) MHIM-v2(Tr.) MHIM-v2(DS.) AB-MIL CLAM DSMIL TransMIL DTFD-MIL IBMIL MHIM(AB.) MHIM(Tr.) MHIM(DS.) R2T-MIL 2DMamba MHIM-v2(AB.) MHIM-v2(Tr.) MHIM-v2(DS.) 86.44.9 85.22.7 87.22.7 84.72.7 85.91.8 84.23.4 86.75.6 86.73.7 87.44.1 88.30.7 87.531.7 88.53.8 88.53.7 87.72. 85.52.3 86.71.4 85.33.1 85.83.4 86.42.7 87.61.5 87.12.2 83.45.6 87.12.6 88.83.2 89.11.2 90.71.4 88.32.8 88.72.7 91.12.5 91.71.8 91.61.3 90.81.9 91.41.6 91.02.3 92.41.6 92.61.7 92.51.9 93.21.5 92.82.3 92.71.6 92.91.5 93.21.9 91.72.3 92.22.0 91.92.2 92.22.2 92.22.4 91.71.7 93.22.0 92.93.1 92.82.0 93.81.2 92.01.5 93.32.2 93.72.3 93.01.4 81.64.7 80.43.0 82.42.9 79.92.6 81.12.1 79.53.4 82.45.5 82.33.5 83.14.4 83.71.0 74.43.1 84.04.3 84.04.1 83.12.2 80.62.7 81.91.8 80.33.0 81.13.3 81.82.7 82.82.0 82.52.5 79.25.4 82.52.4 84.63.6 82.42.5 86.41.6 83.83.0 84.22."
        },
        {
            "title": "4.3.1 Baselines",
            "content": "We primarily compare our method with wellestablished approaches such as AB-MIL [14], DSMIL [15], CLAM [17], TransMIL [9], and DTFD-MIL [13]. AB-MIL (AB.), TransMIL (Tr.), and DSMIL (DS.). Despite their differences in attention mechanisms, these methods can all be considered generalized attention-based MIL methods. We use them as baselines to validate the generalizability of our framework. Additionally, we compare our approach with recent methods such as IBMIL [6], MHIM [28], R2T-MIL [92], and 2DMamab [48], with MHIM being the conference version of this paper. To differentiate, we refer to the improved framework proposed in this paper as MHIM-v2. For consistency, the results of all other methods are reproduced using their official codes under the same experimental settings."
        },
        {
            "title": "4.3.2 Cancer Diagnosis and Subtyping",
            "content": "As shown in the left part of Table 2, the performance of nearly all models on the newly constructed CAMELYON dataset is suboptimal. Some models, such as DSMIL [15] and TransMIL [9], were only validated on the CAMELYON16 dataset in their original papers, resulting in inferior performance on the larger CAMELYON dataset compared to classic models like ABMIL. However, our proposed MHIM-v2 framework demonstrates superior performance on such challenging dataset, and this advantage is not just limited to specific baseline model. MHIMv2 achieves high-level performances across three baseline models. Specifically, MHIM-v2 (Trans.) improves the AUC by 0.7% compared to the second-best method named R2T-MIL. Additionally, due to inaccuracies in hard instance mining and the omission of critical features, MHIM-MIL performs poorly with PLIP features, achieving an AUC of only 94.94%, which is slightly lower than the baseline model ABMIL. In contrast, MHIM-v2 shows significant improvement with PLIP features, increasing the AUC by 0.7%, thus validating the effectiveness of our improvements over MHIM. As shown in Tables 2 and 3, the cancer subtyping results on the TCGA-NSCLC and TCGABRCA datasets exhibit similar phenomena. The SOTA R2T-MIL model achieves slightly better AUC on the BRCA compared to MHIM-v2. However, experiments demonstrate that MHIMv2 outperforms R2T-MIL in terms of F1-score and Accuracy, indicating its performance advantages. More importantly, when MHIM-v2 is applied to more advanced models like TransMIL and DSMIL, it generally achieves better performance, even if the baselines may not perform as well as the classic ABMIL. We attribute this to the fact that more advanced and complex network architectures may not fully exploit their learning capabilities due to data limitations. In contrast, MHIM-v2 effectively enhances the learning of these more complex networks on limited WSIs through high-quality hard instance mining, thereby maximizing their potential in cancer subtyping tasks."
        },
        {
            "title": "4.3.3 Survival Analysis",
            "content": "Table 4 presents the experimental results for three survival analysis datasets. Notably, the proposed MHIM-v2 achieved favorable results across all three datasets. Specifically, leveraging the SOTA large pathology model UNI, MHIM-v2 improved the unimodal performance for BLCA, LUAD, and 13 Table 4 Survival analysis results on three main datasets. Methods BLCA LUAD LUSC 0 5 - s N AB-MIL CLAM DSMIL TransMIL DTFD-MIL IBMIL MHIM(AB.) MHIM(Tr.) MHIM(DS.) R2T-MIL 2DMamba MHIM-v2(AB.) MHIM-v2(Tr.) MHIM-v2(DS.) AB-MIL CLAM DSMIL TransMIL DTFD-MIL IBMIL MHIM(AB.) MHIM(Tr.) MHIM(DS.) R2T-MIL 2DMamba MHIM-v2(AB.) MHIM-v2(Tr.) MHIM-v2(DS.) 59.05.6 59.03.5 59.24.2 60.42.8 59.34.4 59.15.3 59.64.2 60.61.4 59.43.9 61.02.1 61.23.9 60.84.0 61.93.7 59.73.3 59.75.8 59.52.9 60.14.0 61.22.9 61.05.9 58.65.5 60.24.4 62.10.7 61.54.3 61.54.3 64.84.3 61.95.5 63.01.6 61.95.8 59.54.5 59.24.1 59.64.9 62.21.0 60.13.5 59.84.1 59.93.6 62.52.4 60.75.0 64.04.4 61.14.3 60.04.9 65.14.2 60.44.2 65.15.1 64.93.1 63.82.5 66.51.8 65.02.3 63.32.8 66.37.0 66.91.5 66.42.5 66.42.6 63.87.6 67.74.5 67.82.2 67.43.3 59.47.1 60.02.9 59.34.5 59.04.6 59.04.1 58.83.3 59.73.3 60.95.3 60.14.1 61.06.9 59.024.1 61.03.9 62.73.8 60.35.2 58.94.1 59.93.1 61.54.7 61.12.5 60.86.0 60.34.8 60.94.6 62.04.4 62.12.3 62.64.2 61.95.9 61.34.5 62.72.9 63.15. Table 5 Performance of different methods on cross-source validation using the CPTAC dataset. Methods CLAM R2T-MIL AB-MIL MHIM(AB.) MHIM-v2(AB.) TransMIL MHIM(Tr.) MHIM-v2(Tr.) CPTACNSCLC CPTACLUAD R50 PLIP R50 UNI 74.91 76.3 74.11 76.5+2 76.7+3 77.22 79.0+2 79.6+3 80.61 81.41 81.42 81.2.1 82.3+.9 81.21 82.7+2 83.1+2 47.12 51.4 48.61 49.1+.5 49.1+.6 50.49 54.0+3 54.9+4 55.63 57.04 52.04 55.8+4 56.7+5 53.33 57.0+4 57.6+5 All methods are trained on TCGA datasets. Results for the NSCLC were reported as AUC, while those for the LUAD were reported as C-index. We present the increase and decrease in performance of our methods compared to the baseline, and the standard deviations of other methods. LUSC to 63.04%, 67.70%, and 63.13%, respectively, surpassing the previous SOTA model R2TMIL. More importantly, compared to the conference version, MHIM-v2 showed performance improvements across different datasets and baselines, such as +1.27% increase on the BLCA-R50 TransMIL baseline and +0.63% increase on the LUAD-R50 AB-MIL baseline. These enhancements enabled MHIM-v2 to outperform R2TMIL, demonstrating superior performance. Furthermore, with high-quality features from UNI, both the MHIM-v2 and its conference version demonstrated more significant advantage over existing methods. This underscores the excellent adaptability of the MHIM framework to advanced features from large pathology models."
        },
        {
            "title": "4.3.4 Cross-source Validation",
            "content": "We evaluated the transferability and generalization of our model by applying it to different datasets for validation, as shown in Table 5. Specifically, the model trained on the TCGANSCLC dataset was applied to the CPTACNSCLC dataset for the subtyping task. Similarly, the model trained on the TCGA-LUAD dataset was validated on the CPTAC-LUAD dataset for survival analysis. In both cases, the CPTAC datasets were used solely as test sets, dedicated to these validation tasks. This cross-source validation demonstrates strong transferability and generalization of our model across diverse data sources. The results indicate its potential for broader applicability and confirm its ability to adapt effectively to new, unseen datasets."
        },
        {
            "title": "4.4 Computational Cost Analysis",
            "content": "Here, we report the training time and GPU memory requirements of different MIL models on an NVIDIA RTX-3090 GPU. The upper part of Table 6 compares several MIL frameworks using ABMIL [14] as baseline. The results indicate that traditional MIL frameworks often introduce additional parameters because of their complex structures, leading to reduced efficiency. For example, the parameter size of DTFD-MIL [13] nearly 1.5 times that of ABMIL (from 657K to 987K), increasing training time by 30%, yet its performance is suboptimal. In contrast, although MHIM-v2 introduced approximately 1M more parameters than MHIM-MIL and their baseline, both MHIM-MIL and MHIM-v2 achieve significant performance improvements by incorporating momentum teachers with almost no increase in computational cost. Existing Transformer-based MIL methods generally incur high computational costs, primarily because of their extensive parameter counts and self-attention mechanisms. For instance, TransMIL [9], which first applied pure Transformer structure to this area, has 4 times the parameters of ABMIL, 3 times the training 14 Table 6 Comparison of computational efficiency of different MIL methods. We report the model size (Parameter), the training time per epoch (Train Time), the peak memory usage (Memory), the inference speed on the CAMELYON dataset, and performances on all three tasks. AB. and Trans. denote AB-MIL and TransMIL baselines, respectively. Model Parameter Train Time Memory Inference Speed CAMELYON NSCLC LUAD ABMIL DTFD-MIL R2T-MIL MHIM(AB.) MHIM-v2(AB.) TransMIL MHIM(Trans.) MHIM-v2(Trans.) 0.66M 0.99M 2.70M 0.65M 1.64M 2.67M 2.67M 3.72M 3.1s 5.1s 6.5s 4.1s 4.3s 13.2s 10.1s 10.5s 2.3G 2.1G 10.1G 2.2G 2.8G 10.6G 5.5G 5.6G 1250 325 236 1250 714 76 76 72 91.59 91.49 92.73 92.30 92.77 91.23 92.62 93.47 95.32 95.55 96.40 96.02 96.22 94.97 95.98 96. 59.47 60.05 64.01 59.87 59.98 62.15 62.52 65.11 Table 7 Comparison of different modules in MHIM-v2. Table 8 Comparison between different easy instance assessment strategies. Module CAMELYON NSCLC AB. Trans. AB. Trans. Strategy 0 5 L 91.6 Baseline 92.1 +RHSM 92.5 +RHSM+GRN +RHSM+CL 92.3 +RHSM+GRN+CL 92.8 95.0 Baseline 95.2 +RHSM 95.2 +RHSM+GRN +RHSM+CL 95.2 +RHSM+GRN+CL 95.7 91.2 92.4 92.4 92.6 93.5 94.6 93.9 94.9 94.4 95. 95.3 95.9 96.1 95.9 96.2 95.6 96.3 96.4 96.3 96.6 95.0 95.1 95.8 95.5 96.3 95.5 95.9 96.0 95.8 96.4 RHSM denotes the Randomly High Score Masking. GRN refers the Global Recycle Network. CL is the Consistency Loss. time, and nearly 4.5 times the memory consumption. Although R2T-MIL employs regional MSA to mitigate the efficiency impact of the Transformer, its computational cost remains significantly higher than non-Transformer models. By using high ratio masking strategy, the MHIM-v2 framework significantly reduces the computational cost of such methods (training time decreased by 24%, memory usage reduced by 48%) and enhances stability (AUC standard deviation on CAMELYON is 0.57%). Compared to MHIM-MIL, MHIM-v2 incurs slight increase in computational cost due to the introduction of recovery network. However, this cost is acceptable. Compared to MHIMMIL (TransMIL), the training time for MHIM-v2 increases by only 0.4 seconds, memory usage by 0.1 GB, and the number of images processed per second decreases by 4. However, this minor cost results in significant performance improvement."
        },
        {
            "title": "4.5.1 Important Modules",
            "content": "Table 7 presents the ablation study of different modules in MHIM-v2 on two datasets. We first introduce the Randomly High Score Masking 15 CAMELYON NSCLC AB. Trans. AB. Trans. 0 Baseline 5 Attention Instance Probability Baseline Attention Instance Probability 91.6 92.2 92.8 95.0 95.5 95.7 91.2 92.4 93.5 94.6 95.2 95.8 95.3 96.1 96.2 95.6 96.4 96. 95.0 95.4 96.3 95.5 96.1 96.4 The proposed class-aware instance probability shows advantages over class-agnostic attention on multiple benchmarks. (RHSM) strategy, which employs teacher model to mine hard instances during training. This strategy enhances the AUC of both MIL models on the CAMELYON dataset by 0.5% and 1.1%, respectively, indicating that the incorporation of hard instance mining during training aids in constructing more accurate classification boundaries for mainstream MIL models. detailed discussion on the RHSM strategy will be provided in Section 4.5.2, and discussion on choosing the teacher model will be given in Section 4.5.4. Apart from the RHSM strategy, comparing rows 4 and 5 reveals that the Global Recycle Network (GRN) and High Ratio Masking reduce redundancy for the student model and provide more diverse hardinstances. These improves the AUC of the baseline models on the TCGA-NSCLC dataset by 0.3% and 0.8%, respectively, confirming the positive impacts of these techniques on student model training. more detailed discussion of GRN will be provided in Section 4.5.3. After incorporating the Consistency Loss (CL) into the objective function, the complete MHIM-v2 framework achieves the best AUC performance of 95.8% and 96.6% on the CAMELYON and TCGA-NSCLC datasets, respectively. For subsequent ablation experiments, we include consistency loss by default to facilitate the optimization of our framework. Table 9 Ablation of different instance mining methods. Table 10 Comparison of different types of teachers. Strategy Memo. ABMIL RHSM+CL +RSM +LSM+GRN +RSM+GRN TransMIL RHSM+CL +RSM +LSM+GRN +RSM+GRN 2.3G 2.7G 2.6G 2.8G 2.8G 10.6G 10.6G 5.5G 5.6G 5.6G CAMELYON NSCLC R50 PLIP R50 PLIP 91.6 92.3 92.6 92.3 92.8 91.2 92.6 93.0 92.5 93.5 95.0 95.2 95.4 95.4 95.7 94.6 94.4 95.1 95.2 95.8 95.3 95.9 96.2 96.0 96.2 95.0 95.5 95.6 95.8 96. 95.6 96.3 96.5 96.4 96.6 95.5 95.8 96.3 96.0 96.4 Based on RHSM, we ablated two masking strategies and investigated the impact of the proposed GRN. We found that RSM not only shows advantages in performance but also significantly reduces memory consumption. Meanwhile, GRN alleviates the critical information loss problem of large-scale masking at very low cost."
        },
        {
            "title": "4.5.2 Class-aware Instance Probability",
            "content": "We introduce class-aware instance probability instead of attention scores in MHIM-MIL to more accurately mine hard instances. To validate the effectiveness of this improvement, ablation experiments were conducted. Table 8 shows that, although attention scores can mine some hard instances, their accuracy is insufficient, resulting in limited performance improvement for the baseline model. MHIM-v2 utilizes instance probability to select more challenging instance features for training the student model, thereby enhancing training effectiveness. While this improvement generally enhances model performance across various settings, we found it especially beneficial for the more complex TransMIL, which can better model classification boundaries from hard instances. Specifically, on the CAMELYON using R50 features, the TransMIL achieved 1.1% AUC improvement due to this enhancement. Clearly, this experiment demonstrates the superior hard instance mining capability of instance probability compared to attention score."
        },
        {
            "title": "4.5.3 Global Recycle Network",
            "content": "We introduce large-ratio Random-Score Masking (RSM) module within the Global Recycle Network (GRN) as an enhancement to the MHIMMIL. This module simplifies the hard instance mining strategy and significantly reduces the computational cost of the baselines while enhancing the student models utilization of key features. The results in Table 9 confirm this improvement. Teacher 0 5 L Baseline Student copy Init. Momentum Init.+Momentum Baseline Student copy Init. Momentum Init.+Momentum CAMELYON NSCLC AB. Trans. AB. Trans. 91.6 92.5 92.3 92.4 92.8 95.0 95.2 95.1 95.3 95.7 91.2 91.7 92.8 92.5 93.5 94.6 94.6 94.7 94.6 95.8 95.3 94.1 96.2 95.9 96.2 95.6 96.1 96.4 96.3 96. 95.0 96.0 95.9 96.1 96.3 95.5 95.9 96.0 96.0 96.4 Momentum denotes the teacher is updated by EMA strategy. Init. indicates the initialization of the teacher with pre-trained baseline model. The bottom figure compares the stability of the momentum teacher and the non-batch gradient updated student during training. By comparing the second and third rows of the table, it is evident that the RSM not only reduces computational costs but also improves model performance. This reveals the presence of significant redundancy and substantial noise in the WSI. However, we believe that this random masking may lead to the omission of critical instance features. This issue is particularly pronounced in the CAMELYON, where key features are sparse. Consequently, the GRN achieves the best AUC improvements on the CAMELYON with PLIP features, showing increases of 0.3% and 0.7%. These improvements do not incur high computational costs, demonstrating the efficiency of the network. Additionally, the results in the fourth row indicate that the Low-Score Masking (LSM) strategy with the same masking ratio does not yield satisfactory results. This highlights the superiority of RSM in providing high-quality, diverse hard instances."
        },
        {
            "title": "4.5.4 Model Choice of MHIM",
            "content": "Choice of Teacher Model. In MHIM-v2, we employ Teacher model to mine hard instances and facilitate the training of the Student model. In Table 10, we comprehensively investigate the effects of various choices of Teacher networks. We initially adopts simple structure where the teacher and student models share parameters. The 16 Table 11 Comparison of different MIL models under student initialization. Table 12 Ablation study on error masking mitigation techniques. Model w/ student init. AB-MIL MHIM-v2(AB.) TransMIL MHIM-v2(Tr.) w/ student init. CLAM-SB DTFD-MIL R2T-MIL CAMELYON NSCLC R50 PLIP PLIP 91.7+.1 92.8+.9 91.8+.5 93.5+1. 95.0+.0 95.7+.4 94.5.1 95.8+1. 95.2.1 96.2+.1 94.5.5 96.3+.6 95.5.2 96.6+.4 95.4.5 96.4+.7 91.4.1 91.7+.2 92.3. 95.0.3 95.5+.2 95.1+.1 95.5+.1 95.4.2 96.4.0 95.4.1 95.9+.1 95.9.5 Model MHIM-v2(AB.) w/o Decay w/o RHSM MHIM-v2(Tr.) w/o Decay w/o RHSM CAMELYON NSCLC R50 92.8 92.7 92.0 93.5 92.9 92.8 PLIP R50 PLIP 95.7 95.2 95.0 95.8 94.4 94.7 96.2 96.2 96.0 96.3 95.9 95.9 96.6 96.5 96.4 96.4 95.9 96.2 We employ decaying high-score masking ratio (Decay) and randomly high-score masking policy (RHSM) to minimize the risk of removing informative regions. The red and blue values represent the performance increase or decrease of the algorithm relative to the methods without initialization. The bottom figure compares the performance of teacher models under initialized or uninitialized student. Student conducts masked hard instance mining prior to training. Due to the non-batch gradient update, the unstable performance of the Student model makes the strategy susceptible to noise, resulting in sub-optimal performance. Second, we adopt momentum teacher, which shares the same network structure as the Student model and is updated with the EMA strategy. This updating strategy enhances the stability of momentum teachers, as shown in the figure below Table 10, and enables MHIM-v2 to achieve up to 1.74% and 0.77% performance improvement under the two baselines, respectively. After initialization with pre-trained baseline, the momentum teacher achieves the best performance. However, fixed teacher fails to learn new knowledge, emphasizing the significance of iterative optimization. Initialization of Student Model. We initialize the fully connected layer of the student network with pre-trained baseline model to reduce the risk of collapse from the Siamese structure, as detailed in [97]. The figure below Table 11 illustrates how this initialization affects the performance of the teacher model. An uninitialized student model exhibits slow initial training, which impairs the performance of the teacher model and hinders the iterative optimization of the framework. Table 11 tabulates the performances of different MIL approaches with initialization, where the red and blue values represent the performance increase or decrease relative to their uninitialized versions. The upper part of Table 11 shows significant difference in the final performance of the student model with and without this initialization. Additionally, we applied the same initialization to mainstream MIL models to investigate whether it boosts performance by aiding Siamese structure optimization. The lower part of Table 11 indicates that this initialization does not significantly enhance the performance of existing mainstream MIL models and sometimes even degrades it. Our experiments confirm that initializing the first fully connected layer of the student facilitates the iterative optimization of the MHIM framework, rather than serving as universal trick for improving MIL model performance."
        },
        {
            "title": "4.5.5 More on Masking Strategy",
            "content": "Error Masking Mitigation. MHIM faces major challenge: it may mask all key information and turn into error instance mining. To further minimize this risk, we employ additional strategies, including decaying mask ratio (Decay) and randomly high-score masking policy (RHSM). While the lack of instance-level supervision prevents the complete elimination of this risk, our quantitative and qualitative results demonstrate its effective mitigation. The ablation study in Table 12 shows that removing the decay and randomly masking strategies (RHSM) leads to consistent performance drop. Discussion on Randomly Masking. To isolate the impact of our hard instance mining strategy 17 Fig. 5 Performance comparison between the simple Random Masking Strategy (RSM) and MHIM-v2 (Ours). from simple random masking, we conducted an ablation study. In Figure 5, we include results for an RSM-only baseline (i.e., applying only random masking at various ratios) and mark the performance of our full MHIM framework for direct comparison. While RSM alone improves performance over the standard baseline by reducing data redundancy and increasing training diversity, the results clearly show significant performance gap between the RSM-only approach and our full framework. This performance gap underscores the critical contributions of our proposed components: RHSM, CL, and GRN, in effectively guiding the model to focus on the most informative instances."
        },
        {
            "title": "4.5.6 Discussion on Hyperparameters",
            "content": "We analyze the key hyperparameters in MHIMv2. Specifically, the two mask ratios influence the quality of hard instances, while the scaling factor balances the impact of self-supervised and fully supervised labels during model training. Figure 6 Fig. 6 The performances of MHIM-v2 under different important hyperparameters. reveals consistent performance trends across different offline features, demonstrating the generalizability. Moreover, the framework exhibits relatively stable performance. Although the CAMELYON dataset shows greater performance variation due to limited data, the framework maintains satisfactory stability in the NSCLC dataset as the sample size increases. Notably, different baselines exhibit varying sensitivity to hyperparameters: AB-MIL, characterized by stronger instance sparsity, typically requires lower mask ratio, while TransMIL demands higher ratio."
        },
        {
            "title": "4.6.1 Diagnosis from Pathologist’s view",
            "content": "To intuitively understand the effect of masked hard instance mining, we visualize the attention scores (bright patches) and tumor probabilities (cyan patches) of patches produced by baselines and MHIM-v2, as illustrated in Figure 7. MHIMv2 employs AB-MIL (AB.), DSMIL (DS.), and 18 Fig. 7 Patch visualization produced by baselines and MHIM-v2 on CAMELYON. The blue lines outline the tumor regions. Brighter patches indicate higher attention scores. The cyan colors represent high probabilities of tumor presence at the corresponding locations. In the cyan patches, brightness reflects the confidence level. Ideally, the bright cyan patches should cover only the area within the blue lines. We demonstrate that focusing solely on more salient regions reduces the generalization ability of baseline models. MHIM-v2 corrects the attention regions of baselines, making accurate and robust judgments from pathologists perspective. TransMIL (Trans.) as its baseline models. We note that attention scores only indicate the regions of interest of models and are insufficient to reflect tumor probabilities [13, 15]. Therefore, we additionally visualize tumor probabilities to better understand masked hard instance mining. First, as shown in Figure 7, we find that different attention regions determine the quality of discrimination. The baseline models focus only on tumor areas, leading to missing most of them. Expanding the view to include some irrelevant areas enables MHIM-v2 to make more complete judgments (rows 1 and 4 on the left). More importantly, hard instance mining enables the model to concentrate on the stained lymph nodes, thereby reducing the impact of noise in discrimination (rows 3 and 4 on the right). Clinically, lightly stained regions primarily consist of adipose tissue; any tumor tissues present are likely due to staining artifacts or sample contamination, leading to potential false positives. Thus, the model should avoid focusing on these areas, as they typically lack substantial tumor presence, and paying attention to these regions would suggest misalignment in the models target focus. In lymph node examinations, the primary diagnostic value lies in the lymph nodes themselves, with the"
        },
        {
            "title": "4.6.2 How MHIM Facilitates Training",
            "content": "The assessment of easy-to-classify instances by the teacher model directly affects the quality of the mined hard instances, thereby influencing the training of the MHIM framework. Therefore, we visualize the teacher models assessment of easyto-classify instances at different training stages in Figure 8 and qualitatively analyze the impact of the proposed class-aware instance probability. From the upper part of the figure, we observe gradual improvement in the discriminative ability of the teacher model, demonstrating the effectiveness of iterative optimization. More importantly, for tumor slides, the initial assessments are relatively random, making it difficult to accurately locate highly challenging tumor areas. In this scenario, the quality of the mined hard instances is poor. As training progresses, the teacher model can accurately evaluate easy-to-classify instances, leading to more challenging hard instances and better training outcomes. This clearly illustrates the necessity of the RHSM strategy, which prevents the teacher model from masking all discriminative instances in the later stages of training and providing incorrect instance sequences. Conversely, the teacher exhibits opposite characteristics on normal slides in the lower part of the figure. In the early stages of training, the teacher model is easily influenced by small amount of noise, resulting in sparse assessment distributions and misjudgments. However, subsequent visualizations demonstrate that MHIM training enables the teacher to provide ideal, uniform assessments on normal slides. Additionally, the teacher can remove extreme interference for the student in the early stages of training, preventing the student model from falling into the same dilemma and facilitating the iterative optimization of MHIM. Although the teacher model based on attention scores shows trend similar to the proposed classaware instance probability, we find that instance probabilities are more accurate and exhibit more ideal distribution. This advantage is evident in both types of slides and becomes increasingly pronounced as training progresses."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper rethinks the impact of salient instances on MIL-based CPath algorithms. We demonstrate 20 Fig. 8 Visualization of easy-to-classify instances based on different strategies during the training process by the teacher model. As training progresses, the discriminability of teacher model gradually improves, and the advantage of class-aware instance probability over class-agnostic attention score becomes increasingly significant. This advantage is also shown in normal slides (bottom part of figure), where the evaluation distribution should be more uniform. lymph node capsule being of secondary importance. When examining lymph node metastasis under microscope, pathologists typically start by inspecting along the capsule, as cancerous tissues generally first affect the marginal sinus near the capsule before infiltrating deeper. The MHIMv2 model corrects the attention region bias found in baseline models, thereby enabling high-quality diagnoses from pathologists perspective. Moreover, baseline models often assign high tumor probabilities to patches in non-tumor areas. We attribute this phenomenon to the low generalization capability of conventional attention-based MIL models, which tend to focus only on salient regions during training. In contrast, MHIM-v2 trained with hard instances shows much better generalization ability than the baseline models for noise robustness (rows 2 and 6 on the right) and for precise detection of challenging subtle tumor areas (rows 2 and 3 on the left). This phenomenon demonstrates how hard instances provide more useful information to help the model make more accurate, and robust judgments. that attention-based MIL, which excessively prioritize salient instances, harm the generalization ability of the model. To address this issue, we propose the masked hard instance mining that masks out easy-to-classify patches and encourages the model to focus on informative and clinically valuable regions for better learning. Through qualitative analysis, we show that the proposed strategy effectively alleviates the underfitting problem of general attention-based MIL models to hard instances. We also develop the MHIM-v2 framework, leveraging momentum teacher, global recycle network, class-aware instance probability, and consistency loss to further enhance hard instance mining. Our experimental results demonstrate the superiority and generality of the MHIMv2 framework compared to other SOTA methods and the earlier conference version."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by the National Natural Science Foundation of China under Grant 62176030, in part by the Key Projects of the Special Program for Technological Innovation and Applied Development in Chongqing Municipality under Grant CSTB2023TIAD-KPX0060."
        },
        {
            "title": "Data Availability Statement",
            "content": "Data publicly available in repository: The CAMELYON dataset is available at https: //camelyon17.grand-challenge.org/. All TCGA datasets can be found at https:// portal.gdc.cancer.gov/. All CPTAC datasets can be found at https: //pdc.cancer.gov/."
        },
        {
            "title": "References",
            "content": "[1] Hosseini, M.S., Bejnordi, B.E., Trinh, V.Q.- H., Chan, L., Hasan, D., Li, X., Yang, S., Kim, T., Zhang, H., Wu, T., et al.: Computational pathology: survey review and the way forward. Journal of Pathology Informatics, 100357 (2024) [2] Pinckaers, H., Van Ginneken, B., Litjens, G.: Streaming convolutional neural networks for end-to-end learning with multi-megapixel images. IEEE transactions on pattern analysis and machine intelligence 44(3), 15811590 (2020) [3] Li, H., Yang, F., Zhao, Y., Xing, X., Zhang, J., Gao, M., Huang, J., Wang, L., Yao, J.: Dt-mil: Deformable transformer for multiinstance learning on histopathological image. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 206216 (2021). Springer [4] Zhao, Y., Lin, Z., Sun, K., Zhang, Y., Huang, J., Wang, L., Yao, J.: Setmil: spatial encoding transformer-based multiple instance learning for pathological image analysis. In: Medical Image Computing and Computer Assisted InterventionMICCAI 2022: 25th International Conference, Singapore, September 18 22, 2022, Proceedings, Part II, pp. 6676 (2022). Springer [5] Lu, M.Y., Chen, T.Y., Williamson, D.F., Zhao, M., Shady, M., Lipkova, J., Mahmood, F.: Ai-based pathology predicts origins for cancers of unknown primary. Nature 594(7861), 106110 (2021) [6] Lin, T., Yu, Z., Hu, H., Xu, Y., Chen, C.-W.: Interventional bag multi-instance learning on whole-slide pathological images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1983019839 (2023) [7] Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N., Huang, J.: Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. Medical Image Analysis 65, 101789 (2020) [8] Di, D., Zou, C., Feng, Y., Zhou, H., Ji, R., Dai, Q., Gao, Y.: Generating hypergraphbased high-order representations of wholeslide histopathological images for survival prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(5), 58005815 (2022) [9] Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X., et al.: Transmil: Transformer based correlated multiple instance 21 learning for whole slide image classification. NeurIPS 34 (2021) [10] Chikontwe, P., Luna, M., Kang, M., Hong, K.S., Ahn, J.H., Park, S.H.: Dual attention multiple instance learning with unsupervised complementary loss for covid-19 screening. Medical Image Analysis 72, 102105 (2021) [11] Srinidhi, C.L., Ciga, O., Martel, A.L.: Deep neural network models for computational histopathology: survey. Medical Image Analysis 67, 101813 (2021) [12] Dietterich, T.G., Lathrop, R.H., LozanoPerez, T.: Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence 89(1-2), 3171 (1997) [13] Zhang, H., Meng, Y., Zhao, Y., Qiao, Y., Yang, X., Coupland, S.E., Zheng, Y.: Dtfdmil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18802 18812 (2022) [14] Ilse, M., Tomczak, J., Welling, M.: Attentionbased deep multiple instance learning. In: International Conference on Machine Learning, pp. 21272136 (2018). PMLR [15] Li, B., Li, Y., Eliceiri, K.W.: Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In: CVPR, pp. 14318 14328 (2021) [16] Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D., Krishnan, R.G., Mahmood, F.: Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1614416155 (2022) [17] Lu, M.Y., Williamson, D.F., Chen, T.Y., Chen, R.J., Barbieri, M., Mahmood, F.: Data-efficient and weakly supervised computational pathology on whole-slide images. Nature Biomedical Engineering 5(6), 555570 (2021) [18] Xu, G., Song, Z., Sun, Z., Ku, C., Yang, Z., Liu, C., Wang, S., Ma, J., Xu, W.: Camel: weakly supervised learning framework for histopathology image segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1068210691 (2019) [19] Hearst, M.A., Dumais, S.T., Osuna, E., Platt, J., Scholkopf, B.: Support vector machines. IEEE Intelligent Systems and their applications 13(4), 1828 (1998) [20] Sun, H., Chen, Z., Yan, S., Xu, L.: Mvp matching: maximum-value perfect matching for mining hard samples, with application to person re-identification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 67376747 (2019) [21] Tan, Z., Liu, A., Wan, J., Liu, H., Lei, Z., Guo, G., Li, S.Z.: Cross-batch hard example mining with pseudo large batch for id vs. spot face recognition. IEEE Transactions on Image Processing 31, 32243235 (2022) [22] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: unified embedding for face recognition and clustering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 815823 (2015) [23] Bejnordi, B.E., Veta, M., Van Diest, P.J., Van Ginneken, B., Karssemeijer, N., Litjens, G., Van Der Laak, J.A., Hermsen, M., Manson, Q.F., Balkenhol, M., et al.: Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA 318(22), 21992210 (2017) [24] Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B.E., Lee, B., Paeng, K., Zhong, A., et al.: From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging 38(2), 550560 (2018) 22 [25] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778 (2016) C., et al.: Deep attention learning for preoperative lymph node metastasis prediction in pancreatic cancer via multi-object relationship modeling. International Journal of Computer Vision, 124 (2024) [26] Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T.J., Zou, J.: visuallanguage foundation model for pathology image analysis using medical twitter. Nature medicine 29(9), 23072316 (2023) [27] Chen, R.J., Ding, T., Lu, M.Y., Williamson, D.F., Jaume, G., Song, A.H., Chen, B., Zhang, A., Shao, D., Shaban, M., et al.: Towards general-purpose foundation model for computational pathology. Nature Medicine 30(3), 850862 (2024) [28] Tang, W., Huang, S., Zhang, X., Zhou, F., Zhang, Y., Liu, B.: Multiple instance learning framework with masked hard instance mining for whole slide image classification. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40784087 (2023) [29] Cui, M., Zhang, D.Y.: Artificial intelligence and computational pathology. Laboratory Investigation 101(4), 412422 (2021) [30] Song, A.H., Jaume, G., Williamson, D.F., Lu, M.Y., Vaidya, A., Miller, T.R., Mahmood, F.: Artificial intelligence for digital and computational pathology. Nature Reviews Bioengineering, 120 (2023) [31] Fan, J., Liu, D., Chang, H., Cai, W.: Learning to generalize over subpartitions for heterogeneity-aware domain adaptive nuclei segmentation. International Journal of Computer Vision, 124 (2024) [32] Xiong, S., Li, X., Zhong, Y., Peng, W.: Repsnet: nucleus instance segmentation model based on boundary regression and structural re-parameterization. International Journal of Computer Vision, 120 (2025) [33] Zheng, Z., Fang, X., Yao, J., Zhu, M., Lu, L., Shi, Y., Lu, H., Lu, J., Zhang, L., Shao, [34] Song, A.H., Chen, R.J., Ding, T., Williamson, D.F., Jaume, G., Mahmood, F.: Morphological prototyping for unsupervised slide representation learning in computational pathology. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1156611578 (2024) [35] Jaume, G., Oldenburg, L., Vaidya, A., Chen, R.J., Williamson, D.F., Peeters, T., Song, A.H., Mahmood, F.: Transcriptomics-guided slide representation learning in computational pathology. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 96329644 (2024) In: Proceedings of [36] Nasiri-Sarvi, A., Trinh, V.Q.-H., Rivaz, H., Hosseini, M.S.: Vim4path: Self-supervised vision mamba for histopathology images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 68946903 (2024) [37] Li, J., Dong, J., Huang, S., Li, X., Jiang, J., Fan, X., Zhang, Y.: Virtual immunohistochemistry staining for histological images assisted by weakly-supervised learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1125911268 (2024) [38] Campanella, G., Hanna, M.G., Geneslaw, L., Miraflor, A., Silva, V.W.K., Busam, K.J., Brogi, E., Reuter, V.E., Klimstra, D.S., Fuchs, T.J.: Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nature Medicine 25(8), 13011309 (2019) [39] Hou, L., Samaras, D., Kurc, T.M., Gao, Y., Davis, J.E., Saltz, J.H.: Patch-based convolutional neural network for whole slide tissue image classification. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 24242433 (2016) 23 [40] Javed, S., Mahmood, A., Qaiser, T., Werghi, N., Rajpoot, N.: Unsupervised mutual transformer learning for multi-gigapixel whole slide image classification. Medical Image Analysis 96, 103203 (2024) for image representation with applications on giga-pixel whole slide image classification. In: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 35833592 (2025) [41] Feng, J., Zhou, Z.-H.: Deep miml network. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31 (2017) [42] Qu, L., Ma, Y., Luo, X., Guo, Q., Wang, M., Song, Z.: Rethinking multiple instance learning for whole slide image classification: good instance classifier is all you need. IEEE Transactions on Circuits and Systems for Video Technology (2024) [43] Wu, Y., Schmidt, A., Hernandez-Sanchez, E., Molina, R., Katsaggelos, A.K.: Combining attention-based multiple instance learning and gaussian processes for ct hemorrhage detection. In: MICCAI, pp. 582591 (2021). Springer [44] Li, J., Chen, Y., Chu, H., Sun, Q., Guan, T., Han, A., He, Y.: Dynamic graph representation with knowledge-aware attention for histopathology whole slide image analysis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1132311332 (2024) [45] Sharma, Y., Shrivastava, A., Ehsan, L., Moskaluk, C.A., Syed, S., Brown, D.E.: Cluster-to-conquer: framework for endto-end multi-instance learning for whole slide image classification. arXiv preprint arXiv:2103.10626 (2021) [46] Wang, X., Yan, Y., Tang, P., Bai, X., Liu, W.: Revisiting multiple instance neural networks. Pattern Recognition 74, 1524 (2018) [47] Tang, W., Qin, R., Fang, H., Zhou, F., Chen, H., Li, X., Cheng, M.-M.: Revisiting endto-end learning with slide-level supervision in computational pathology. arXiv preprint arXiv:2506.02408 (2025) [48] Zhang, J., Nguyen, A.T., Han, X., Trinh, V.Q.-H., Qin, H., Samaras, D., Hosseini, M.S.: 2dmamba: Efficient state space model [49] Zheng, T., Jiang, K., Xiao, Y., Zhao, S., Yao, H.: M3amba: Memory mamba is all you need for whole slide image classification. In: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 15601 15610 (2025) [50] Yang, S., Wang, Y., Chen, H.: Mambamil: Enhancing long sequence modeling with sequence reordering in computational pathology. In: International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 296306 (2024). Springer [51] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning, pp. 87488763 (2021). PMLR [52] Shi, J., Li, C., Gong, T., Zheng, Y., Fu, H.: Vila-mil: Dual-scale vision-language multiple instance learning for whole slide image classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1124811258 (2024) [53] Li, H., Chen, Y., Chen, Y., Yu, R., Yang, W., Wang, L., Ding, B., Han, Y.: Generalizable whole slide image classification with fine-grained visual-semantic interaction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1139811407 (2024) [54] Dickson, E.R., Grambsch, P.M., Fleming, T.R., Fisher, L.D., Langworthy, A.: Prognosis in primary biliary cirrhosis: model for decision making. Hepatology 10(1), 17 (1989) [55] Ohno-Machado, L.: comparison of cox proportional hazards and artificial neural network models for medical prognosis. Computers in biology and medicine 27(1), 5565 24 (1997) [56] Cox, D.R.: Regression models and life-tables. Journal of the Royal Statistical Society: Series (Methodological) 34(2), 187202 (1972) [57] Zhu, X., Yao, J., Zhu, F., Huang, J.: Wsisa: Making survival prediction from whole slide histopathological images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 72347242 (2017) [63] Xu, Y., Wang, Y., Zhou, F., Ma, J., Jin, C., Yang, S., Li, J., Zhang, Z., Zhao, C., Zhou, H., et al.: multimodal knowledge-enhanced whole-slide pathology foundation model. arXiv preprint arXiv:2407.15362 (2024) [64] Shao, Z., Wang, Y., Chen, Y., Bian, H., Liu, S., Wang, H., Zhang, Y.: Lnpl-mil: Learning from noisy pseudo labels for promoting multiple instance learning in whole slide image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2149521505 (2023) [58] Zhou, F., Chen, H.: Cross-modal translation and alignment for survival analysis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 21485 21494 (2023) [65] Zhu, Z., Yu, L., Wu, W., Yu, R., Zhang, D., Wang, L.: Murcl: Multi-instance reinforcement contrastive learning for whole slide image classification. IEEE Transactions on Medical Imaging 42(5), 13371348 (2022) [59] Shao, W., Liu, J., Zuo, Y., Qi, S., Hong, H., Sheng, J., Zhu, Q., Zhang, D.: Fam3l: Feature-aware multi-modal metric learning for integrative survival analysis of human cancers. IEEE Transactions on Medical Imaging 42(9), 25522565 (2023) [60] Chen, R.J., Lu, M.Y., Wang, J., Williamson, D.F., Rodig, S.J., Lindeman, N.I., Mahmood, F.: Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis. IEEE Transactions on Medical Imaging 41(4), 757770 (2020) [61] Nakhli, R., Moghadam, P.A., Mi, H., Farahani, H., Baras, A., Gilks, B., Bashashati, A.: Sparse multi-modal graph transformer with shared-context processing for representation learning of giga-pixel images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11547 11557 (2023) [62] Song, B., Leroy, A., Yang, K., Dam, T., Wang, X., Maurya, H., Pathak, T., Lee, J., Stock, S., Li, X.T., et al.: Deep learnfusion of radioling informed multimodal ogy and pathology to predict outcomes in hpv-associated oropharyngeal squamous cell carcinoma. EBioMedicine 114 (2025) [66] Chen, Y.-C., Lu, C.-S.: Rankmix: Data augmentation for weakly supervised learning of classifying whole slide images with diverse sizes and imbalanced categories. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2393623945 (2023) [67] Yang, R., Liu, P., Ji, L.: Protodiv: Prototypeguided division of consistent pseudo-bags for whole-slide image classification. arXiv preprint arXiv:2304.06652 (2023) [68] Zhang, R., Zhang, Q., Liu, Y., Xin, H., Liu, Y., Wang, X.: Multi-level multiple instance learning with transformer for whole slide image classification. arXiv preprint arXiv:2306.05029 (2023) [69] Zhang, X., Huang, S., Zhang, Y., Zhang, X., Gao, M., Chen, L.: Dual space multiple instance representative learning for medical image classification. In: 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press, ??? (2022). https://bmvc2022.mpi-inf. mpg.de/0768.pdf [70] Zhou, H., Chen, H., Yu, B., Pang, S., Cong, X., Cong, L.: An end-to-end weakly supervised learning framework for cancer 25 subtype classification using histopathological slides. Expert Systems with Applications 237, 121379 (2024) [71] Zheng, T., Jiang, K., Yao, H.: Dynamic policy-driven adaptive multi-instance learning for whole slide image classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 80288037 (2024) [72] Lee, B., Ko, K., Hong, J., Ko, H.: Hard sample-aware consistency for low-resolution facial expression recognition. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 199 208 (2024) [73] Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 761769 (2016) [74] Wang, K., Yan, X., Zhang, D., Zhang, L., Lin, L.: Towards human-machine cooperation: Self-supervised sample mining for object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 16051613 (2018) [75] Rao, H., Leung, C., Miao, C.: Hierarchical skeleton meta-prototype contrastive learning with hard skeleton mining for unsupervised person re-identification. International Journal of Computer Vision 132(1), 238260 (2024) [76] Rao, H., Leung, C., Miao, C.: Hierarchical skeleton meta-prototype contrastive learning with hard skeleton mining for unsupervised person re-identification. International Journal of Computer Vision 132(1), 238260 (2024) [77] Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: Online learning of social representations. In: Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 701710 (2014) [78] Tang, J., Qu, M., Wang, M., Zhang, M., In: Proceedings Yan, J., Mei, Q.: Line. of the 24th International Conference on World Wide Web (2015). https://doi.org/10. 1145/2736277.2741093 . http://dx.doi.org/ 10.1145/2736277.2741093 [79] Huang, T., Dong, Y., Ding, M., Yang, Z., Feng, W., Wang, X., Tang, J.: Mixgcf: An improved training method for graph neural network-based recommender systems. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 665674 (2021) [80] Niu, C., Pang, G., Chen, L.: Affinity uncertainty-based hard negative mining in graph contrastive learning. IEEE Transactions on Neural Networks and Learning Systems (2024) [81] Suh, Y., Han, B., Kim, W., Lee, K.M.: Stochastic class-based hard example mining for deep metric learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7251 7259 (2019) [82] Yan, J., Deng, C., Huang, H., Liu, W.: Causality-invariant interactive mining for cross-modal similarity learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) [83] Wang, S., Yu, J., Li, W., Liu, W., Liu, X., Chen, J., Zhu, J.: Not all voxels are equal: Hardness-aware semantic scene completion with self-distillation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1479214801 (2024) [84] Kheradmand, S., Rebain, D., Sharma, G., Isack, H., Kar, A., Tagliasacchi, A., Yi, K.M.: Accelerating neural field training via soft mining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2007120080 (2024) [85] Sahin, U., Li, H., Khan, Q., Cremers, D., Tresp, V.: Enhancing multimodal compositional reasoning of visual language models 26 [93] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: Imagenet: large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255 (2009). Ieee [94] Chen, R.J., Lu, M.Y., Weng, W.-H., Chen, T.Y., Williamson, D.F., Manz, T., Shady, M., Mahmood, F.: Multimodal co-attention in transformer gigapixel whole slide images. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154025 (2021) prediction survival for [95] Harrell Jr, F.E., Lee, K.L., Mark, D.B.: Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Statistics in medicine 15(4), 361387 (1996) [96] Kingma, D.P., Ba, J.: Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) [97] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 96509660 (2021) with generative negative mining. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 5563 5573 (2024) [86] Yang, Z., Ding, M., Huang, T., Cen, Y., Song, J., Xu, B., Dong, Y., Tang, J.: Does negative sampling matter? review with insights into its theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) [87] Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737 (2017) [88] Chen, W., Chen, X., Zhang, J., Huang, K.: Beyond triplet loss: deep quadruplet network for person re-identification. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 403412 (2017) [89] Xu, L., Sun, H., Liu, Y.: Learning with batch-wise optimal transport loss for 3d shape recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 33333342 (2019) [90] Li, M., Wu, L., Wiliem, A., Zhao, K., Zhang, T., Lovell, B.: Deep instance-level hard negative mining model for histopathology images. In: Medical Image Computing and Computer Assisted InterventionMICCAI 2019: 22nd International Conference, Shenzhen, China, October 1317, 2019, Proceedings, Part 22, pp. 514522 (2019). Springer [91] Dong, Q., Gong, S., Zhu, X.: Class rectification hard mining for imbalanced deep learning. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 18511860 (2017) [92] Tang, W., Zhou, F., Huang, S., Zhu, X., Zhang, Y., Liu, B.: Feature re-embedding: Towards foundation model-level performance in computational pathology. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1134311352 (2024)"
        }
    ],
    "affiliations": [
        "CS, Hefei University of Technology, Hefei, China",
        "CS, Hong Kong University of Science and Technology, Hong Kong, China",
        "CS, Nanjing University of Posts and Telecommunications, Nanjing, China",
        "School of Big Data & Software Engineering, Chongqing University, Chongqing, China"
    ]
}