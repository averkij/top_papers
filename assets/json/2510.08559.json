{
    "paper_title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models",
    "authors": [
        "Andong Deng",
        "Taojiannan Yang",
        "Shoubin Yu",
        "Lincoln Spencer",
        "Mohit Bansal",
        "Chen Chen",
        "Serena Yeung-Levy",
        "Xiaohan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 5 5 8 0 . 0 1 5 2 : r SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models Andong Deng1, Taojiannan Yang, Shoubin Yu2, Lincoln Spencer1, Mohit Bansal2, Chen Chen1, Serena Yeung-Levy3, Xiaohan Wang3 1University of Central Florida 2University of North Carolina at Chapel Hill 3Stanford University Link: Project Page Code HuggingFace Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by semiautomatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science. 1. Introduction Large Multimodal Models (LMMs) [1, 2, 25, 30, 34, 41] have demonstrated rapid advancements across diverse range of capabilities, including conversational interaction [17, 52], code generation [12, 20], mathematical reasoning [10, 60], and image understanding [19, 55]. Among the various input modalities, video presents particularly rich and complex form of multimodal data, uniquely integrating temporal dynamics, spatial perception, and high-level semantic reasoning. Consequently, video understanding has emerged as critical frontier, pivotal for advancing LMMs towards next-generation applications in fields such as robotics, interactive education, and scientific discovery. To evaluate the performance of LMMs, numerous video benchmarks have been developed [13, 15, 18, 26, 33, 35, 39, 4446, 62]. However, the majority of existing benchmarks concentrate on relatively general domainssuch as movies [15, 39], daily activities [32, 45], and instructional content [62]and primarily emphasize tasks like temporal grounding, common-sense reasoning, and event understanding. While these benchmarks once posed significant challenges, current state-of-the-art LMMs, Gemini 2.5 Pro [14], now exhibit saturated performance, achieving accuracies exceeding 85% on popular benchmarks like VideoMME [15] and Neptune [33]. 1 Figure 1: SciVideoBench features research-level experimental videos accompanied by challenging questions that rigorously evaluate advanced video understanding. It emphasizes the synergistic interaction among accurate visual perception, expert knowledge, and sophisticated logical reasoning. More recent initiatives [18, 40, 61] have begun to incorporate scientific and educational videos to assess deeper domain knowledge and reasoning. Nevertheless, most of this content remains at college level, allowing LMMs to achieve success either largely through pure visual recognition with memorized domain knowledge (e.g., asking the virus type given the video) or the visual information does not dominate the reasoning process, thus lacking complex multimodal reasoning. For instance, part of the questions in MMVU [61] leverage visually-irrelevant hypotheses which could enable models to answer without watching the video, e.g., in video depicting blowing air into flask of limewater, which reacts with carbon dioxide to form cloudy calcium carbonate precipitate. The associated question asks:Assume that 2.24 liters of gas fully participates in the reaction shown in the video under the standard temperature and pressure conditions, how many grams of precipitate are produced approximately?. The assumption about gas volume is actually the most important information to complete the calculation in order to answer the question. Consequently, these benchmarks pose limited challenge to current proprietary LMMs (Gemini 2.5 Pro achieves 83.6 overall performance on Video-MMMU, and OpenAI o1 achieves 76.1 overall performance on MMVU), and can also be solved via simple language-based reasoning framework [53, 56], which achieves 82.7 and 83.1 on Video-MMMU and Video MMLU, respectively. Furthermore, current benchmarks inadequately evaluate sophisticated cognitive skills that necessitate expert-level knowledge, intricate logical reasoning, and precise visual perception. Moreover, the videos from these benchmarks are usually from daily life or simple college-level scenarios, while the advanced research-level scenarios, where complex scientific experiments happen, are ignored. To address this critical gap, we introduce SciVideoBench, an innovative benchmark specifically designed to rigorously assess advanced video reasoning capabilities. SciVideoBench consists of 1,000 meticulously crafted multiple-choice questions derived from research-level experimental videos in physics, chemistry, 2 biology, and medicine that are published with the corresponding journal publications. These videos are sourced from more than 25 distinct academic subjects, including Fluid Mechanics, Analytical Chemistry, Neuroscience, and Oncology. Each question is categorized into one of three reasoning types: conceptual, hypothetical, or quantitative. As illustrated in Figure. 1, SciVideoBench demands that models demonstrate accurate spatio-temporal grounding, possess profound domain knowledge, and execute sophisticated logical reasoning. We developed SciVideoBench using multi-stage, agent-human collaborative pipeline. This process involved mining associated experimental manuscripts, leveraging LMMs for initial question generation, and engaging domain experts to validate the question-answer pairs and filter out unanswerable or videoirrelevant questions. Our evaluation of both proprietary and open-source LMMs on SciVideoBench reveals consistently low accuracy, underscoring the significant challenge this new benchmark presents and, consequently, the substantial opportunity for advancing research-level video reasoning capabilities. In-depth analysis of the results reveals that factors such as model architecture, reasoning capacity, and perceptual grounding play critical role in shaping video reasoning performance. These insights not only offer clear guidance for future research efforts aimed at developing more sophisticated LMMs but also emphasize the broader potential of SciVideoBench. As the first comprehensive research-level video reasoning benchmark, SciVideoBench not only provides rigorous testbed for evaluating current video reasoning abilities, but also serves as catalyst for innovation, fostering the development of highly capable AI co-scientists that can accelerate future scientific discovery. 2. Dataset Construction Table 1: Benchmark comparison for multi-modal video understanding and reasoning tasks."
        },
        {
            "title": "Dataset",
            "content": "Video Domain Difficulty Level Knowledge-Driven Reasoning-Intensive # Ave. Duration (s) Question Type MovieChat-1K [39] MLVU [46] MVBench [26] LongVideoBench [44] TempCompass [29] Video-MME [15] VSI-Bench [50] MMWorld [16] MMVU [61] Video-MMMU [18] Video-MMLU [40]"
        },
        {
            "title": "Elementary\nElementary\nElementary\nElementary\nElementary\nElementary\nElementary\nElementary\nCollege\nCollege\nCollege",
            "content": "SciVideoBench (ours)"
        },
        {
            "title": "Research",
            "content": "564 930 16 473 < 30 1018 122 107 51 506 109 Open-ended Multi-choice Multi-choice Multi-choice Multi-choice Multi-choice Multi-choice Multi-choice Multi-choice Multi-choice Open-ended Multi-choice 2.1. Video Collection and Processing To build high-quality benchmark that rigorously evaluates advanced scientific reasoning in videos, we collect 241 research-grade experimental videos from the Journal of Visualized Experiments (JoVE)1, peer-reviewed platform dedicated to publishing methodological videos across broad spectrum of scientific disciplines. These videos are professionally produced and narratively structured, clearly demonstrating laboratory protocols, scientific phenomena, and technical instrumentation. Their high visual and instructional 1https://www.jove.com quality makes them an ideal foundation for constructing benchmark grounded in authentic scientific practice. Crucially, each video is accompanied by both peer-reviewed manuscript and synchronized audio narration. The manuscript describes the experimental protocol and results, while the narration provides temporally aligned explanations of each experimental step as it appears in the video. This tri-modal alignmentvideo, audio, and textenables principled and rigorous approach to question generation and answer verification. The accompanying materials offer detailed procedural descriptions, theoretical context, and experimental outcomes, which are instrumental in formulating meaningful, answerable, and visually grounded questions. This synergy ensures that each question in our benchmark is firmly grounded in both the visual content and its underlying scientific rationale. We focus our selection on four foundational domainsphysics, chemistry, biology, and medicinewhich together span diverse range of procedural complexity and reasoning challenges. Within these domains, videos are carefully selected to ensure the presence of clearly measurable variables (such as reaction time, temperature, or applied force), observable causal relationships and experimental outcomes, as well as logical sequences of actions or instrument usage that facilitate conceptual, quantitative, and hypothetical reasoning. This targeted curation ensures that each video in SciVideoBench presents rich multimodal cues necessary for rigorous scientific reasoning, making it an ideal testbed for evaluating the capabilities of large multimodal models. After downloading each video and its corresponding research paper, we use the Whisper [37] automatic speech recognition (ASR) model to extract the audio track and generate transcript. This transcript is temporally aligned with the visual content to capture step-by-step experimental narration. For the associated paper, we parse and extract the full textual content from the PDF. These multimodal inputsvideo, aligned transcript, and paper textform the basis for downstream question generation. Further details on the preprocessing steps are provided in the Appendix. Figure 2: Overview of our annotation pipeline. We manually annotate QA pairs for three example videos using both the video and the associated paper. multi-agent LLM system generates and refines QA pairs: the QA Generator produces initial questions, the Evaluator answers them with reasoning, the Visual Comparer checks for visual grounding and timestamps cues, and the Refiner ensures questions rely on video content and improves option quality. Human experts verify and refine the final QA pairs. Audio transcripts are omitted for simplicity. 2.2. Annotation Pipeline To generate high-quality, research-level question-answer pairs, we develop semi-automatic annotation pipeline, as shown in Figure 2, that integrates video, aligned transcripts, and paired research papers to provide rich scientific and procedural context. This setup enables accurate and visually grounded question generation while reducing manual workload through multi-agent system. To ensure quality and guide the semi-automatic annotation process, we first manually annotate small set of exemplar cases. Four human experts (PhD students from biology, chemistry, medicine, and physics, respectively) review four selected papers (one for each subject) to extract key scientific concepts and procedural steps, then identify corresponding visual cues in the associated videos to construct sophisticated questions. These exemplar QA pairs are further refined and expanded via GPT-4o, resulting in 12 highquality examples per question type. These are used as prompts to guide the human-in-the-loop multi-agent annotation system. In the second stage, we assign distinct roles to set of large language model agents, each responsible for specific subtask in the annotation pipeline: QA Generator produces initial questionanswer (QA) pairs from multimodal inputsvideo, transcript, and associated paper textguided by curated exemplar questions. Specifically, we design prompts for Gemini 2.5 Pro to first identify the core scientific question along with key aspects related to theory, experimental operations, and relevant numerical calculations for each video. Using these extracted elements and humandesigned exemplars as references, Gemini 2.5 Pro then generates open-ended questions. To expand these into multi-choice format, the model is prompted to create nine scientifically plausible but incorrect distractors for conceptual and hypothetical reasoning, while for quantitative reasoning, distractors are automatically generated by adding appropriate Gaussian noise to the correct answer. Evaluator attempts to answer the generated question using the video, the transcript, and paper content, while also producing detailed rationale that outlines the reasoning process. This step is to ensure all of the questions are answerable given sufficient information. Visual Comparer verifies whether the rationale is grounded in the video content by checking that all necessary visual cues are present. It also provides precise timestamps corresponding to each visual reference. Refiner refines the QA pairs by replacing generic or descriptive terms in the question with explicit references to visual segments, ensuring that the question cannot be answered without the video content. It also refines the options for each question to ensure the difficulty of the distractors. Human Verifier reviews the refined QA pairs and the outputs from the Visual Comparer to confirm that the question is fully grounded in the visual evidence. Specifically, these verifiers first check the timestamps annotation in questions to make sure the question is answerable based on the video content. If critical details (e.g., mass or temperature) are mentioned in the transcript but not visible in the video, the expert triggers script to overlay this information directly onto the relevant video frames. Furthermore, the answer will be scrutinized to ensure that it matches the video content and the reasoning path. In the final stage, the human experts perform final review to manually correct any errors and improve clarity. The expert also ensures that each question is closely aligned with the core scientific contribution or focus of the experiment. 2.3. Statistics We collected total of 241 experimental videos spanning four major domains: Physics, Chemistry, Biology, and Medicine. These videos cover more than 25 distinct scientific subjects, as illustrated in Figure 3. 5 Figure 3: Discipline and subject distribution in SciVideoBench. Our benchmark covers four major scientific disciplinesBiology, Chemistry, Medicine, and Physicsencompassing more than 25 specialized subjects. This diverse coverage ensures comprehensive evaluation across wide range of scientific domains. The videos in SciVideoBench have competitive average duration of 484 seconds, with the duration distribution shown in Figure 4(a). This relatively long temporal scale ensures that the benchmark reflects the complexity and extended reasoning often required in real-world scientific experiments. (a) Video duration distribution. (b) Question length distribution. (c) Option length distribution. Figure 4: Dataset statistics in SciVideoBench: (a) video duration, (b) question length, and (c) option length distributions. These statistics provide comprehensive overview of the datasets temporal scale and linguistic diversity. Building upon these videos, we annotated total of 1,000 challenging questions that demand research-level knowledge for both perception and reasoning. As shown in Figure 4(b), the average question length demonstrates that the questions are more linguistically complex than those in existing benchmarks, reinforcing the emphasis on detailed experimental understanding. Furthermore, the option length distribution in Figure 4(c) highlights the balanced design of ground-truth answers and distractors, with comparable average lengths, thereby reducing annotation bias and ensuring fair evaluation. To further capture the nature of academic research and experimental analysis, we carefully designed three distinct question types that reflect common reasoning scenarios observed across the videos, as illustrated in Figure 5. Collectively, these design choices ensure that the questionanswer pairs in SciVideoBench require deeper domain expertise and multi-step reasoning, distinguishing it from prior benchmarks that mainly target elementary 6 or undergraduate-level understanding. 245 Quantitative Reasoning involves numeric perception, reasoning, and calculation. For specific quantitative question, we require that all the numeric information must come from the video, which naturally ask the model to perceive informative values from the video before perform complicated calculations and reasoning. 385 Hypothetical Reasoning focuses on specific experimental operations that play critical role for the motivation or outcome of the whole experiment. This question type usually involves hypothesized error, what-if analysis, or experimental control logic, which requires both expert-level knowledge on specific domain and accurate visual perception to capture important details in the videos. 370 Conceptual Reasoning explore the mechanisms, protocols, and scientific principles behind the operations in the experiment that are inferred via visual/operational cues. Figure 5: Examples of SciVideoBench, including videos from 4 disciplines (Physics, Biology, Chemistry, and Medicine), which involve 19 different subjects. The research-level QAs challenge LMMs in three different aspects (Conceptual, Hypothetical, and Quantitative) that are of vital importance in scientific experiment video understanding. 7 3. Experiments 3.1. Models Vision-Blind Baselines To investigate the importance of visual information of our benchmark, we first conduct the vision-blind evaluation using GPT-4o [34] and Qwen2.5 series [49]. Proprietary Models We evaluate five proprietary LLMs that have superior performance over other popular video benchmarks, including Gemini-1.5-Pro [1], Gemini-2.0-Flash [1], Gemini-2.5-Pro [14], and GPT4o [34]. Open-Source Models We comprehensively evaluate 30 open-sourced LMMs with the parameter volume ranging from as tiny as 0.5B to 78B, including Qwen-VL series [3, 41], InternVL series [7, 8, 63], InternVide2.5Chat-8B [42], LLaVA-OneVision Series [24], LLaVA-NeXT-Video-32B series [25] and LongVA [58]. 3.2. Evaluation Setting We adopt the default frame sampling strategy for all open-source models. Specifically, we sample 768 frames for Qwen-2.5-VL [3], 512 frames for InternVideo2.5-Chat-8B [42], 128 frames for LongVA [58], 16 frames for InternVL2 [8], and 32 frames for InternVL3 [63], LLaVA-OneVision [24], and LLaVA-NeXTVideo-32B [25]. For Gemini [1, 11, 14], we set the frame rate to 1 FPS, and for GPT-4o [34] we use 256 frames. The temperature is fixed to 0 to ensure stable and reproducible evaluation results. All evaluations are using the LMM-Eval toolkit [57] and are conducted on 8 NVIDIA H100 GPUs. 3.3. Human Evaluation We also conduct human evaluation by recruiting three graduate students from each discipline to answer questions in closed-book setting. Their overall accuracy is only 17.4%  (Table 2)  , specifically, the quantitative reasoning is only 14.29%, showing that even advanced students cannot handle the benchmark, which requires research-level expertise. 3.4. Blind Baseline Results To assess the role of visual information in SciVideoBench, we evaluate blind baselines by removing the video input and feeding only textual content to the models. As shown in Table 2, GPT-4o achieves only 15.80% overall accuracy, with particularly poor results on Quantitative Reasoning (11.84%), barely above random guess. Qwen2.5 exhibits similar trends across scales: small variants (0.5B3B) remain near chance, while larger models (32B and 72B) reach at most 18.90%. Despite moderate gains from scaling, both GPT-4o and Qwen2.5 fail to exceed 20% accuracy without visual input. Together with the time-specific, observation-heavy expressions, e.g., \"starting at 02:31\", in the questions shown in Figure 5, these results underscore that visual information is indispensable for solving SciVideoBench, especially for reasoning types requiring precise observation and measurement from the video content. 3.5. Proprietary Models vs. Open-Source Models As shown in Table 2, proprietary models substantially outperform open-source counterparts on our SciVideoBench. The strongest proprietary system, Gemini-2.5-Pro, achieves 64.30% overall accuracy, whereas the best open-source model, InternVL-3-78B-Instruct, reaches only 38.80%. The gap is especially pronounced in Quantitative Reasoning, where Gemini-2.5-Pro attains 50.61%more than double the best open-source score (22.04% by InternVL2-Llama3-76B). Nevertheless, top open-source models demonstrate Table 2: Evaluation Results of Proprietary Models and Open-Source Models on SciVideoBench. Models LLM Size Overall - - GPT-4o [34] Qwen2.5 [49] Gemini-2.5-Pro [14] Gemini-2.5-Flash [11] Gemini-1.5-Pro [1] Gemini-2.0-Flash [1] GPT-4o [34] Gemini-1.5-Pro [1] Gemini-2.0-Flash [1] GPT-4o [34] - - - - - - - - - - - - - - - - - - - - 10.00 17.40 15.80 0.5B 12.40 1.5B 13.40 16.40 3B 16.70 7B 17.10 32B 18.90 72B 64.30 46.40 27.50 25.70 24. - - - - - - - - Question Type Discipline Conceptual Hypothetical Quantitative Biology Chemistry Medicine Physics 10.00 Random Guess 10. 10.00 10.00 10.00 10.00 10.00 Human Evaluation (Graduate Students) 18.11 18.70 14.29 15.88 16.06 21. 18.88 Vision-Blind Baselines 14.05 11.35 11.62 17.03 18.65 18.11 21.89 69.73 50.81 27.84 28.38 30.27 20. 13.51 19.22 20.52 19.74 21.30 18.44 Proprietary Models 67.79 44.16 28.31 24.94 28.05 11.84 12.24 6.94 8.98 8.98 8.98 15.10 50.61 43.27 25.71 22.86 11. 17.53 13.20 14.43 15.89 18.34 19.32 19.07 64.79 44.01 27.38 24.69 21.52 13.95 13.94 12.73 16.36 10.91 16.97 18.79 61.82 49.70 26.06 26.06 29. 16.13 11.21 14.95 15.89 14.95 13.08 20.56 74.77 55.14 27.10 22.43 31.78 14.33 10.97 11.91 17.24 18.18 15.67 18.18 61.44 44.83 28.53 27.90 24. 48.60(+21.10) 39.70(+14.00) 35.00(+10.10) Proprietary Models w/ Chain-of-Thought Reasoning 51.02(+25.31) 40.41(+17.55) 34.29(+22.45) 47.03(+19.19) 39.19(+10.81) 37.84(+7.57) 48.57(+20.26) 39.74(+14.80) 32.73(+4.68) 72B 14B 14B 7B 70B InternVL-3-78B [63] Qwen2.5 Qwen2.5 InternVL-3-14B [63] InternVL-3-14B-Instruct [63] Qwen2.5 Qwen2.5 InternVL-3-8B [63] Hermes2 InternVL2-Llama3-76B [8] InternLM3 8B InternVL-3-9B-Instruct [63] InternVL-3-2B [63] Qwen2.5 Qwen2.5-VL-32B-Instruct [3] Qwen2.5 Qwen2.5 LLaVA-OneVision-7B [24] Qwen2.5 Qwen2.5-VL-3B-Instruct [3] Qwen2.5 InternVL-3-1B [63] Qwen2.5 LLaVA-OneVision-0.5B [24] 37.90 34.20 31.50 25.50 24.90 24.00 1.5B 22.20 20.80 32B 19.90 7B 3B 18.10 0.5B 14.00 0.5B 10.40 Open-Source Models w/ Chain-of-Thought Reasoning 51.62 46.49 42.43 34.32 27.30 29.19 30.00 20.54 24.32 19.19 16.76 11.08 35.58 30.65 27.79 24.16 22.34 22.08 21.82 22.86 20.52 18.96 13.51 10.65 20.82 21.22 20.82 14.29 25.31 19.18 11.02 17.96 12.24 15.10 10.61 8.98 InternVL-3-2B-Instruct [63] InternVL-3-2B [63] InternVL2-4B [8] InternVL-3-1B-Instruct [63] InternVL-3-1B [63] Qwen2.5-VL-3B-Instruct [3] InternVL2-1B [8] InternVL2-2B [8] LLaVA-OneVision-0.5B [24] 1.5B 24.00 Qwen2.5 1.5B 22.90 Qwen2.5 4B 21.30 Phi-3-mini 0.5B 18.90 Qwen2.5 0.5B 18.50 Qwen2.5 Qwen2.5 16.10 3B InternLM2 0.5B 14.40 InternLM2 2B 13.10 0.5B 12.10 Qwen2.5 14B InternVL-3-14B [63] Qwen2.5 14B InternVL-3-14B-Instruct [63] Qwen2.5 7B Qwen2.5 InternVL-3-8B [63] Qwen2.5 InternVL-3-8B-Instruct [63] 7B InternLM3 8B InternVL-3-9B-Instruct [63] InternLM3 8B InternVL-3-9B [63] InternVideo2.5-Chat-8B [42] Qwen2.5 7B InternLM2 7B InternVL2-8B [8] 7B Qwen2.5 LLaVA-OneVision-7B [24] 7B Qwen2.5 Qwen2.5-VL-7B-Instruct [3] 7B Qwen2 LongVA [58] InternVL-3-38B [63] Qwen2.5 InternVL-3-38B-Instruct [63] Qwen2.5 Hermes2 InternVL2-40B [8] Qwen2.5-VL-32B-Instruct [3] Qwen2.5 LLaVA-NeXT-Video-32B [25] InternVL2-26B [8] 32B 32B 34B 32B Qwen2 32B InternLM2 20B InternVL-3-78B-Instruct [63] Qwen2.5 Qwen2.5 InternVL-3-78B [63] InternVL2-Llama3-76B [8] Hermes2 Qwen2.5-VL-72B-Instruct [3] Qwen2.5 72B 72B 70B 72B 35.70 35.70 30.50 29.40 29.20 27.20 25.30 19.40 18.80 16.40 14.30 38.30 37.30 23.80 21.50 21.10 19.50 38.80 38.50 26.30 20.30 23.90 22.60 24.16 18.44 18.44 17.92 14.03 15.06 12.99 Open-Source Models (0.5B - 4B) 31.08 31.08 26.22 26.76 25.95 20.00 17.57 14.59 15.41 Open-Source Models (7B - 14B) 53.51 53.24 44.59 43.78 40.27 38.65 37.84 24.86 23.51 18.92 16.15 Open-Source Models (26B - 40B) 53.78 52.43 28.38 24.86 26.22 21.89 35.32 36.36 30.39 29.35 30.91 27.79 23.12 18.96 19.22 17.14 14. 38.44 37.14 23.64 24.68 22.86 20.26 13.47 11.02 9.39 7.76 7.35 7.35 10.20 7.76 5.71 9.39 8.16 9.39 7.76 9.80 8.98 9.80 11.84 11.02 11.43 10.31 14.69 14.69 17.14 11.43 10.61 14.69 Open-Source Models (> 70B) 57.30 56.76 28.38 21. 39.74 39.22 27.01 20.78 9.39 9.80 22.04 17.55 52.60(+25.22) 41.56(+16.87) 31.78(+10.26) 44.19(+18.13) 40.70(+14.64) 37.58(+7.88) 54.84(+27.74) 35.48(+13.05) 36.45(+4.67) 44.78(+16.25) 37.01(+9.11) 37.30(+12.85) 38.39 35.70 30.07 25.18 25.18 25.92 21.76 20.78 16.87 17.60 15.40 8.56 24.21 21.52 18.09 18.34 17.60 15.16 12.96 11.98 11.74 35.94 35.70 29.10 26.16 29.10 25.67 20.29 17.85 15.56 16.87 13.69 36.67 35.94 22.74 22.49 19.80 18.83 37.90 37.65 24.94 21.27 36.36 30.30 32.73 24.85 24.24 27.88 16.36 18.18 21.21 18.79 13.94 12. 21.82 21.21 24.85 18.18 16.36 13.94 13.94 12.12 13.94 33.94 34.55 31.52 31.52 29.70 26.06 23.64 21.21 23.03 15.15 15.00 40.00 39.39 21.82 16.36 22.42 18.18 39.39 37.58 29.70 16.97 38.32 37.38 37.38 28.97 28.97 17.76 25.23 16.82 28.04 19.63 14.02 13.08 23.36 24.30 30.84 22.43 25.23 18.69 16.82 12.15 13. 38.32 38.32 35.51 34.58 33.64 31.78 31.78 19.63 26.17 16.82 15.65 42.06 40.19 30.84 20.56 23.36 22.43 46.73 46.73 29.91 24.30 37.93 33.23 30.72 25.08 23.51 21.63 24.76 23.51 20.38 17.87 12.23 10.97 25.08 25.08 20.38 18.81 18.50 17.55 15.67 15.36 11.29 35.42 35.42 30.09 30.72 27.69 28.21 30.41 20.38 18.18 16.30 14. 38.24 36.99 23.82 23.20 21.32 20.06 36.99 37.30 25.08 19.44 9 competitiveness against earlier proprietary models: for example, most InternVL-3 variants outperform Gemini-1.5-Pro, and even InternVL-3-2B surpasses it on Conceptual Reasoning. At the lower end, performance drops sharply. The weakest open-source model, LLaVA-OneVision-0.5B, achieves just 12.10% overall, with its Quantitative score (5.71%) falling below even the blind GPT-4o baseline (11.84%). On average, proprietary models achieve nearly double the accuracy of open-source models (38.83% vs. 18.14%), highlighting the considerable advantage of proprietary systems in handling the multimodal and reasoning-intensive demands of SciVideoBench. Takeaway: Proprietary vs. Open-Source Models Gemini-2.5-Pro leads overall. Proprietary models dominate quantitative reasoning, while top opensource models remain competitive on conceptual/hypothetical tasks. 3.6. Results of Different Question Types As shown in the right portion of Table 2, Quantitative Reasoning consistently emerges as the most challenging category across models, regardless of architecture or scale. This difficulty is evident not only in proprietary models but also in open-source models, where Quantitative scores are systematically lower than those for Conceptual or Hypothetical reasoning. In fact, several open-source models perform close to or even below the random-guess baseline, for example, InternVL2-4B achieves only 9.39%, and InternVL-3-1B-Instruct reaches 7.76% in Quantitative Reasoning. By contrast, Conceptual Reasoning generally yields the highest scores, slightly ahead of Hypothetical Reasoning, although the gap is modest. These patterns indicate that SciVideoBench places particularly high demands on precise numerical reasoning and multi-step calculation abilities, while Conceptual and Hypothetical questionsthough still challengingtend to be more approachable for current multimodal LLMs. 3.7. Results of Different Disciplines Across different disciplines, the performance trends remain relatively consistent, indicating that no single discipline presents disproportionately higher level of difficulty in SciVideoBench. For Gemini-2.5-Pro, Chemistry emerges as the lowest-performing discipline, whereas Medicine achieves the highest accuracy; while Gemini-1.5-Pro displays the opposite pattern, with its best results in Chemistry and comparatively lower performance in other disciplines. This contrast suggests that the impact of model architecture and scale can vary across domains, and that strengths in one discipline do not necessarily translate directly to others. 3.8. The Impact of Chain-of-Thought Prompting To highlight the reasoning-intensive nature of our SciVideoBench benchmark and to examine the performance gains attributable to deeper reasoning chains, we further evaluate models using chain-ofthought (CoT) prompts as shown in Figure 6, following the methodology in [61]. As shown in Table 2, models with CoT prompting consistently and substantially outperform their vanilla counterparts. Gemini-1.5-Pro shows the most pronounced improvement, with overall accuracy rising by +21.10%, and Quantitative Reasoning achieving remarkable +25.31% boostsurpassing even the stronger Gemini-2.5-Pro. Gemini2.0-Flash and GPT-4o also exhibit substantial overall gains (+14.00% and +10.10%, respectively). Notably, GPT-4os Quantitative Reasoning accuracy increases from near-random 11.84% to 34.29%, underscoring the transformative impact of explicit reasoning steps. Across reasoning categories, the magnitude of improvement is not uniform. On average, Quantitative Reasoning benefits the most from the thinking 10 mode (+21.77%), while Conceptual Reasoning shows the smallest relative gain (+12.52%). This disparity suggests that the benefits of CoT are particularly strong in scenarios requiring precise multi-step numerical reasoning, whereas tasks relying on conceptual or high-level understanding may saturate earlier. Figure 6: Chain-of-thought prompt used in SciVideoBench Interestingly, for most of the open-source models, leveraging the chain-of-thought prompt hurt the overall performance; however, the Quantitative Reasoning ability improves obviously. For instance, the overall performance of Qwen2.5-VL-32B-Instruct drops 0.7% but the Quantitative Reasoning increases from 11.43% to 17.96%, which is 57.13% relative boost. This demonstrates again the intricate reasoning cost in our quantitative questions, which requires multi-step calculation and can be obviously improved by the chainof-thought prompt. Figure 7: Chain-of-Thought (CoT) performance gains across proprietary models and open-source models. An obvious difference between proprietary models across all the reasoning aspects can be observed; while for open-source models, quantitative reasoning has notable performance boost, while the other two reasoning aspects have negative impacts. This phenomenon again demonstrates that the quantitative settings in SciVideoBench require sophisticated multi-step reasoning that can benefit lot from chain-of-thought prompts. Best viewed in color. To further investigate the possible reason that CoT prompting has such different impact for different reasoning type for open-source models, we analyze the predictions of three representative models: InternVL3-78B, InternVL2-Llama3-76B, and Qwen2.5-VL-32B-Instruct. From Table 3, we observe that chain-ofthought (CoT) prompting does not consistently improve evaluation performance across reasoning types. Globally, the number of instances where the direct strategy is correct but CoT is wrong slightly exceeds the reverse, indicating that CoT often introduces additional errors. more detailed breakdown by reasoning type reveals that the negative impact is concentrated in Conceptual and Hypothetical reasoning. In these categories, CoT tends to amplify hallucinations or over-elaborate on causal relations, leading the model away from the correct choice, whereas direct answering can rely more on memorized factual knowledge or surface-level pattern recognition. In contrast, Quantitative reasoning benefits substantially from CoT: across the three models, we see more cases where CoT corrects errors that the direct approach would miss. This suggests that explicit reasoning chains help models externalize intermediate computational steps (e.g., arithmetic or logical comparisons), which are otherwise challenging when only single final answer is required. plausible reason why CoT hurts open-source models on conceptual and hypothetical reasoning is that open-source models are less robust in producing faithful long-form causal explanations, often hallucinating or over-elaborating when reasoning about abstract scenarios, but CoT provides clear scaffold for step-by-step numerical comparisons, which directly reduces errors in quantitative tasks. Compared to the CoT gains reported on other video reasoning benchmarks (using Gemini-2.0-Flash as an example) MMVU [61] (+3.0%), Video-Holmes [9] (+12.5%), VideoMathQA [38] (+6.6%), and MMR-V [64] (+2.4%)the +14.00% improvement on SciVideoBench is striking. This significant gap strongly indicates that SciVideoBench inherently demands more complex, multi-step reasoning than existing benchmarks, making it more challenging and discriminative testbed for evaluating reasoning capabilities in multimodal models. Takeaway: The Impact of Chain-of-Thought Prompt Chain-of-thought prompting consistently boosts quantitative reasoning for both proprietary and opensource models, while improvements in conceptual and hypothetical reasoning are observed almost only in proprietary models. 3.9. The Impact of LMM Model Size We investigate the relationship between model size and performance across four representative opensource model families: InternVL-3, InternVL-2, and Qwen2.5-VL. In general, positive correlation between parameter count and overall performance can be observed within each series; however, larger size does not universally guarantee higher performance, and diminishing or even negative returns occur in some cases. For InternVL-3, the trend from smaller to larger configurations shows substantial gains in overall accuracy (14.0% 35.7%). The largest jump occurs when scaling from 1B to 7B, where overall performance increases by over 11 percentage points, with notable improvements in Conceptual (16.76% 43.78%) and Medicine (14.02% 34.58%). However, gains are not strictly monotonic. For instance, the 9B variant (27.20%) underperforms compared to the 8B-Instruct variant (29.40%) in Overall, despite having more parameters, partly because of different language backbones. For InternVL-2, performance improvements with scale are modest and less consistent. The overall accuracy rises from 14.4% (0.5B) to 21.3% (4B) but then plateaus, with the 20B model achieving only 19.5%. The 4B model outperforms the 7B variant in all question types except Quantitative, suggesting that architectural or training differences outweigh the size advantage. For Qwen2.5-VL, the scaling law appears weaker. The 3B, 7B, and 32B models show modest overall performance increases (16.1%, 16.4%, 21.5%), but the 72B variant drops slightly to 20.3%. This 12 Table 3: Comparison of agreement and disagreement between CoT and Direct answers across three models."
        },
        {
            "title": "Model",
            "content": "Both Correct Both Wrong Agree Both Wrong Disagree CoT Correct, Direct Wrong Direct Correct, CoT Wrong InternVL-3-78B InternVL2-Llama3-76B Qwen2.5-VL-32B-Instruct InternVL-3-78B InternVL2-Llama3-76B Qwen2.5-VL-32B-Instruct InternVL-3-78B InternVL2-Llama3-76B Qwen2.5-VL-32B-Instruct InternVL-3-78B InternVL2-Llama3-76B Qwen2.5-VL-32B-Instruct 259 139 149 58 37 93 48 49 17"
        },
        {
            "title": "Overall",
            "content": "242"
        },
        {
            "title": "Conceptual Reasoning",
            "content": "41"
        },
        {
            "title": "Hypothetical Reasoning",
            "content": "88"
        },
        {
            "title": "Quantitative Reasoning",
            "content": "113 114 140 253 287 335 77 125 147 102 114 145 74 48 43 120 110 42 43 39 44 38 39 34 29 34 126 124 119 61 47 55 58 56 7 21 18 plateauing, and in some cases regression, is particularly evident in Conceptual and Chemistry performance, where the 72B model lags behind the 32B variant. However, the 72B model shows strong Quantitative Reasoning, which requires advanced multi-step reasoning. However, between different model series, larger model does not always guarantee better performance. For instance, Qwen2.5-VL-72B-Instruct is not even as good as InternVL2-4B in overall performance. Overall, while model size generally correlates with improved performance, especially in Conceptual and domainspecific disciplines (e.g., Medicine, Biology), the relationship is far from linear or absolute. Variations between close sizessuch as InternVL-3 7B vs. 8B, or Qwen2.5-VL 32B vs. 72Bunderscore that scaling must be accompanied by corresponding improvements in data quality, training objectives, and architectural choices to realize consistent gains in our SciVideoBench. Figure 8: The impact of LMM backbones on the performance. 13 3.10. The Impact of LLM Backbone Size We comprehensively compare the performance across LMMs with different LLM backbones as shown in Figure 8. In general, we can observe clear positive correlation between the size of the LLM backbone and the overall performance of the corresponding LMMs. Larger backbones such as Qwen2.5-72B and Qwen2.532B consistently achieve the highest scores, while smaller backbones (e.g., Qwen2.5-0.5B, Qwen2.5-1.5B) exhibit significantly weaker performance. This trend highlights the critical role of language backbone capacity in driving the multimodal reasoning ability of LMMs. Interestingly, the correlation is strong in the conceptual and hypothetical reasoning (ùúå = 0.86 and ùúå = 0.88, respectively), where scaling the backbone size almost monotonically improves performance for most of the VLM series (e.g., InternVL-3 series). In contrast, the quantitative reasoning exhibits much weaker correlation (ùúå = 0.64): simply increasing the backbone size does not yield proportional gains. This indicates that the difficulty of quantitative reasoning cannot be resolved by scaling language ability alone, but likely requires more accurate visual perception and advanced numerical reasoning capability. Takeaway: The Impact of Model Scaling Larger models reliably boost conceptual/hypothetical reasoning, but quantitative gains remain weak and non-monotonic across series. 3.11. The Impact of Audio To investigate how audio, which provides additional information about the experiments shown in the video, will affect the model performance, we evaluate Gemini-2.5-Pro [14] and Qwen2.5-Omni-7B [48] due to their capability of supporting full modality input. From Table 4, we can observe consistent improvement brought by audio input for both Gemini-2.5-Pro and Qwen2.5-Omni-7B in overall performance (2.70% and 2.80%, respectively). This limited improvement also demonstrates that the visual content dominates the model performance. Table 4: Model Performance Improvement by Audio on SciVideoBench."
        },
        {
            "title": "Conceptual Hypothetical Quantitative Biology Chemistry Medicine Physics",
            "content": "Gemini-2.5-Pro [14] Gemini-2.5-Pro [14] Qwen2.5-Omni-7B [48] Qwen2.5-Omni-7B [48]"
        },
        {
            "title": "N\nY\nN\nY",
            "content": "64.30 67.00 14.70 17.50 69.73 74.32 16.69 20.29 67.79 67.53 14.97 18.51 50.61 55.10 9.74 12.80 64.79 68.70 14.39 18.24 61.82 61.21 14.36 16. 74.77 71.96 17.85 18.19 61.44 66.14 14.25 17.67 3.12. Error Analysis To gain an in-depth understanding of how far the current models are from real human experts, we comprehensively analyze the results of Gemini-1.5-Pro-Thinking and Gemini-2.0-Flash-Thinking. Comparing these reasoning steps with the real rationale from the annotation process, which was carefully verified by human experts, we found three noticeable errors: Incorrect Visual Perception, Inaccurate Reasoning, and Lack of Domain Knowledge. Importantly, most of the wrong responses stem from complex error combination, e.g., both incorrect visual perception and inaccurate reasoning progress. Incorrect Visual Perception (70.68%) is the most common factor that leads to an incorrect conclusion. It leads to misinterpret of what is visually shown in the video and identify the wrong moment in the temporal span. For instance, in Figure 15, Gemini-2.0-Flash overlooks the on-screen textual information indicating the humidity, which leads to the consequence that it obtains incorrect option analysis for option J, maintaining standardized high humidity. Inaccurate Reasoning Progress (63.25%) also constitutes main portion of the error cause. It usually involves logical flaws despite correct or partially correct observations. In Figure 15, Gemini-2.0-Flash fails to establish the connection between the operation of using potassium sulfate and the high-humidity consequence. Lack of Domain Knowledge (49.40%) happens when models fail to connect the visual evidence with specific expert knowledge that helps answer the question. For example, in Figure 15, the function of potassium sulfate should be well-known expert knowledge for researchers; however, Gemini-2.0-Flash is not able to analyze even perceive the existence of the sulfate in the video, which is strong evidence of domain knowledge lacking. 3.13. Failure Case Analysis From Table 2, we observe that even the best-performing proprietary model, Gemini-2.5-Pro, achieves limited accuracy on SciVideoBench, revealing the capped reasoning capabilities of current MLLMs in complex scientific experimental scenarios. Chain-of-thought (CoT) evaluation provides clearer insight into these reasoning limitations when facing highly specialized, visually grounded questions. For example, in the case shown in Figure 15, the question asks: Why is the setup shown during the cooling step at 03:1203:17 used? The correct rationale is as follows: the setup is sealed desiccator containing saturated potassium sulfate solution, which maintains constant relative humidity of approximately 98%. This high-humidity environment is crucial because the luminescent properties of the Ag-zeolite composite are highly sensitive to hydration. Cooling the sample in this controlled atmosphere ensures it equilibrates to well-defined hydration state, avoiding batch-to-batch variation caused by fluctuating laboratory humidity. The correct answer is (Maintains standardized high humidity). However, Gemini-2.0-Flash (CoT) fails despite partially recognizing that the desiccator controls humidity. It incorrectly rules out standardized high humidity and instead selects (Keeps the sample dry to avoid hydrolysis). This mistake stems from two major reasoning errors: (1) Ignored on-screen evidence The model completely overlooked the clear text overlay 98% relative humidity in the video, which directly confirms the correct answer. (2) Misinterpreted the chemical role of potassium sulfate The model failed to associate potassium sulfates well-established function with generating stable high-humidity environment, instead generalizing it as generic moisture control. This example illustrates that solving such questions requires combination of precise temporal localization (identifying the cooling step), fine-grained spatial perception (reading on-screen labels), and expert-level domain knowledge (understanding the chemicals function in environmental control). Current models, even with CoT prompting, often break down when any one of these components is missing. 4. Related Work 4.1. Video Large Language Models Recent advances in large multimodal models have led to the rapid development of LMMs that empower temporal perception and reasoning ability. VideoChatGPT [31] leverages simple spatiotemporal pooling mechanism for the visual encoding, while the LLaVA [25, 28] series leverage anyres technique to unify both image and video encoding. Meanwhile, LongVA [58] and LongVILA [6] aim at processing long-term video by extending the context window of LLMs in multi-stage training pipeline. However, their ability to perform scientific reasoning, such as interpreting experimental procedures, analyzing quantitative measurements, remains underexplored. To address this gap, we propose SciVideoBench, domain-specific benchmark 15 Figure 9: Failure case example of Gemini-2.0-Flash-Thinking. designed to evaluate video reasoning in real-world scientific scenarios. 4.2. Video Reasoning Benchmarks Recent benchmarks have increasingly emphasized complex temporal reasoning and multimodal understanding to evaluate LMMs beyond the traditional video QA benchmark paradigm [32, 43, 45, 47, 54]. Perception Test [35] examines multiple reasoning modes such as descriptive, predictive, and counterfactual reasoning in real-world scenarios. MVBench [26] proposes 20 temporally grounded challenges that require understanding actions, object interactions, and motion cues across multiple frames. Video-MME [15] incorporates human annotations across videos ranging from 11 seconds to 1 hour, including subtitle and audio streams to promote multi-source comprehension. MMBench-Video targets long-context temporal reasoning by leveraging hour-long videos with open-ended QA. Domain-specific benchmarks have also emerged. WorldQA [59] focuses on long-chain reasoning with multimodal world knowledge, Video-MMMU [18] collects expert-level instructional videos across disciplines for multi-stage knowledge acquisition, and VideoMMLU [40] centers on lecture-level understanding in math, physics, and chemistry. Our work extends this line of research by introducing scientific video reasoning benchmark grounded in real experimental settings, emphasizing measurement, calculation, and conceptual reasoning involved in specific experiments. 4.3. AI for Science In recent years, artificial intelligence has made tremendous progress across variety of scientific disciplines, including biology [21, 27, 51], chemistry [4, 36], materials science [5, 22], and mathematics [23]. These breakthroughs have rapidly pushed the boundaries of scientific research and demonstrated performance that rivals, and in some cases surpasses, traditional methods. For instance, AlphaFold [21, 51] revolutionized protein structure prediction by achieving near-experimental accuracy through deep learning, enabling the release of structures for nearly all known proteins. In materials science, GNoME [22] employed graph neural networks to discover over 700,000 stable crystal structures, dramatically expanding the known materials space. In chemistry, models like OrbNet [36] and ChemCrow [4] integrate quantum chemistry and symbolic tools with language models to enable efficient simulation and synthesis planning. Despite these advances, current AI systems remain far from achieving the capabilities of versatile researcher who can perform complex scientific tasks across domains with expert-level depth and accuracy. In this work, we introduce SciVideoBench, challenging benchmark that focuses on scientific video reasoning, requiring visual perception, domain-specific knowledge, and intricate reasoning. Our benchmark aims to assess how far current LMMs are from expert-level scientific reasoning and to foster the development of next-generation AI systems for science. 5. Conclusion We present SciVideoBench, the first scientific video reasoning benchmark that demands PhD-level knowledge to perform complex reasoning grounded in real-world experimental scenarios. SciVideoBench bridges the gap between general-purpose video understanding benchmarks and advanced scientific reasoning. It spans four major scientific disciplinesPhysics, Chemistry, Biology, and Medicineand covers over 20 distinct subjects, encompassing broad spectrum of scientific inquiry. To enable rigorous evaluation, we design three complementary question typesquantitative, hypothetical, and conceptualthat reflect common reasoning challenges in scientific research. We evaluate total of 21 models, including 6 proprietary and 15 open-source LMMs, to assess their current capabilities relative to expert human reasoning. Our results show that even the most advanced model (Gemini-2.5-Pro) still struggles with these challenging 17 tasks. To further explore the impact of reasoning augmentation, we evaluate model performance under chain-of-thought prompting. The results demonstrate clear performance gains, highlighting the importance of explicit reasoning in complex scientific domains. We hope SciVideoBench will serve as milestone benchmark for advancing LMMs and inspire future research in the AI for Science community."
        },
        {
            "title": "References",
            "content": "[1] Rohan Anil, Orhan Firat, Hyung Won Chung, Yanping Huang, Patrick Lewis, et al. Gemini: family of highly capable multimodal models. https://arxiv.org/abs/2312.11805, 2023. arXiv:2312.11805. [2] Anthropic. Claude 3 models. https://www.anthropic.com/news/claude-3-family, 2024. Accessed: 2025-05-09. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Amanda Bran, Thomas Black, Alex Lee, et al. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023. [5] Chi Chen and Shyue Ping Ong. Graph networks as universal machine learning framework for molecules and crystals. Nature Communications, 14(1):799, 2023. [6] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [9] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. [10] Karl Cobbe, Vedant Misra, Moritz Aschbacher, Michael Jiang, Emma Brunskill, Christopher Manning, and Mark Chen. Training verifiers to solve math word problems, 2021. [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [12] Qingxiu Dong, Tianyi Zhang, Lingfeng Gao, Yuxiao Dong, Zhilin Yang, and Jie Tang. LiveCodeBench: Realistic evaluation of code generation with online interaction, 2024. [13] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. [14] Google AI for Developers. Gemini models: Gemini 2.5 pro, 2025. URL https://developers.googleblog. com/en/gemini-2-5-video-understanding/. Accessed May 15, 2025. [15] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 18 [16] Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, et al. Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos. arXiv preprint arXiv:2406.08407, 2024. [17] Dan Hendrycks, Collin Burns, Steven Basart, Aakanksha Mazeika, Dan Lin, Tejas Paranjape, Tagyoung Song, Jacob Mazeika, Michael Tang, Nikolas Saunders, et al. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. [18] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [19] Drew A. Hudson and C. Lawrence Zitnick. GQA: new dataset for compositional question answering over real-world images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 63986407, 2019. [20] Carlos Eduardo Jim√©nez, Or Rubin, Kevin Sanik, Derrick Xin, Neelansh Mittal, Kyle Rector, Daniel Tarlow, Owolabi Legunsen, and Graham Neubig. SWE-bench: Can language models resolve real-world github issues?, 2023. [21] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [22] Hyeonhu Kim, Daniel Uzan, Tian Xie, et al. Accelerated discovery of stable materials using deep learning. Nature, 620:10181024, 2023. [23] Aitor Lewkowycz, Nicholas Schiefer, Clayton Freeman, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [25] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [27] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022:500902, 2022. [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [29] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [30] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [31] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 19 [32] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [33] Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, et al. Neptune: The long orbit to benchmarking long video understanding. arXiv preprint arXiv:2412.09582, 2024. [34] OpenAI. Gpt-4o: Openais newest multimodal model. https://openai.com/index/gpt-4o, 2024. Accessed: 2025-05-09. [35] Viorica PƒÉtrƒÉucean, Lucas Smaira, Ankush Gupta, Adri√† Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Jo√£o Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HYEGXFnPoq. [36] Zhenxing Qiao, Matthew Welborn, Anima Anandkumar, Frederick Manby, and Thomas Miller III. Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features. Journal of Chemical Physics, 153(12):124111, 2020. [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [38] Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, and Fahad Shahbaz Khan. Videomathqa: Benchmarking mathematical reasoning via multimodal understanding in videos. arXiv preprint arXiv:2506.05349, 2025. [39] Enxin Song, Wenhao Chai, Gaoang Wang, Yifan Zhang, Haoyu Zhou, Feng Wu, Hang Chi, Xudong Guo, Tong Ye, and Yan Lu. MovieChat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 17141725, 2024. [40] Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, and Gaoang Wang. Video-mmlu: massive multi-discipline lecture understanding benchmark. arXiv preprint arXiv:2504.14693, 2025. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [43] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. [44] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. URL https://arxiv.org/abs/2407.15754. [45] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. [46] Shitao Xiao, Peifei Wu, Yifan Zhang, Tianrui Li, Jinze Yu, Hua Wu, and Haifeng Wang. MLVU: Benchmarking multi-task long video understanding, 2024. 20 [47] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question In Proceedings of the 25th ACM answering via gradually refined attention over appearance and motion. international conference on Multimedia, pages 16451653, 2017. [48] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [50] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. [51] Zhenyu Yang, Xiaoxi Zeng, Yi Zhao, and Runsheng Chen. Alphafold2 and its applications in the fields of biology and medicine. Signal Transduction and Targeted Therapy, 8(1):115, 2023. [52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. [53] Shoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon, and Mohit Bansal. Mexa: Towards general multimodal reasoning with dynamic multi-expert aggregation. arXiv preprint arXiv:2506.17113, 2025. [54] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134, 2019. [55] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yan, Jun Zhu, Hongsheng Li, Yu Qiao, Hang Su, Liwei Wang, Yuxuan Sun, Wenhu Chen, and Yu Su. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2431024321, 2024. [56] Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, and Gedas Bertasius. Silvr: simple language-based video reasoning framework. arXiv preprint arXiv:2505.24869, 2025. [57] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772. [58] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [59] Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. Worldqa: Multimodal world knowledge in videos through long-chain reasoning. arXiv preprint arXiv:2405.03272, 2024. [60] Shulin Zhao, Tianyu Gao, Liangchen Luo, Renze Lou, Boxin Wang, Weiyang Liu, Hai Zhao, and Yue Zhang. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models, 2025. 21 [61] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, and Arman Cohan. Mmvu: Measuring expert-level multi-discipline video understanding, 2025. URL https://arxiv.org/abs/2501.12380. [62] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, pages 75907598, 2018. [63] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. [64] Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Mmr-v: Whats left unsaid? benchmark for multimodal deep reasoning in videos. arXiv preprint arXiv:2506.04141, 2025. 22 A. Appendix overview This document provides more details of our approach and additional experimental results, organized as follows: Discilpines and Subjects in SciVideoBench Detailed configuration of evaluated models. Gemini-assisted Metadata Generation More Failure Cases. B. Disciplines and Subjects Table 5: List of subjects grouped by their corresponding disciplines."
        },
        {
            "title": "Biology",
            "content": "Biochemistry, Bioengineering, Biogeotechnology, Bioinformatics, Genetic Engineering, Lipidomics, Mechanobiology, Microfluidics, Mycology, Neurohistology, Neuroscience, Phycology, Proteomics, Regenerative Biology, Structural Biology, Cell Biology, Molecular Biology Chemistry Analytical Chemistry, Electrochemistry, Green Chemistry, Nanomaterials, Organic Chemistry, Photocatalysis, Photovoltaics, Physical Chemistry, Radiochemistry, Materials Chemistry"
        },
        {
            "title": "Physics",
            "content": "Biomaterials, Cardiovascular Research, Dentistry, Drug Delivery, Immunology, Molecular Imaging, Nanomedicine, Oncology, Ophthalmology, Pharmacology, Radiopharmaceuticals, Regenerative Medicine, Surgical Devices, Therapeutics Acoustofluidics, Additive Manufacturing, Aerosol Science, Applied Physics, Ceramic Engineering, Ceramics, Civil Engineering, Condensed Matter Physics, Ecological Engineering, Electronics, Fluid Dynamics, Materials Science, Mechanical Engineering, Microfluidics, Nanofabrication, Plasma Physics, Semiconductor, Sensors, Soft Robotics, Thermal Engineering 1 C. Configuration of Evaluated Models All of the evaluated models in this paper were released after May 2024. The detailed configuration of the evaluated models are shown in Table 6. We follow the default frame sampling settings of the official implementation of each model. The temperature is set to 0 to ensure stable predictions. Table 6: Models used in our evaluation with confirmed release dates where available."
        },
        {
            "title": "Google\nGoogle\nGoogle\nGoogle\nOpenAI",
            "content": "Gemini-2.5-Pro Gemini-2.5-Flash Gemini-2.0-Flash Gemini-1.5-Pro GPT-4o 1fps June 17, 2025 1fps June 17, 2025 February 5, 2025 1fps September 24, 2024 1fps 256 May 13, 2024 Open-source Multimodal Models OpenGVLab Alibaba OpenGVLab LMMS-Lab OpenGVLab LMMS-Lab UCSD/CMU April 11, 2025 InternVL-3 series January 28, 2025 Qwen2.5-VL January 21, 2025 InternVideo2.5 August, 2024 LLaVA-OneVision July 4, 2024 InternVL-2 series LongVA (7B) June 24, 2024 LLaVA-NeXT-Video May 10, 2024 32 768 512 32 16 128 2 D. Metadata Generation Using Gemini 2.5 Pro Figure 10: The prompt for Gemini 2.5 Pro to generate metadata to support initial annotation. 3 Figure 11: The prompt for Gemini 2.5 Pro to generate distractors. Figure 12: The prompt for Gemini 2.5 Pro to update distractors. 5 Listing 1: Example metadata generated from Gemini 2.5 Pro 1 { 2 4 5 6 7 8 10 11 12 13 14 15 } \" 66497 \" : { \" title \" : \" Synthesis and Characterization of Self - Assembled Metal - Organic Framework Monolayers Using Polymer - Coated Particles \" , \" discipline \" : \" Chemistry \" , \" subject \" : \" Materials Chemistry \" , \" core_equation \" : \" UiO -66 - DDMAT + ( CH2 = CHCOOCH3 ) - -( Photocatalyst , Light ) --> UiO -66 - ( CH2 - CH ( COOCH3 ) ) \" , \" tim ta _breakdown \" : [ \" 00 : 00 - 01 : 58 : An introduction to the research ... \" , \" 01 : 59 - 02 : 05 : 10 mg of catechol - DDMAT is weighed ... \" , \" 02 : 05 - 02 : 10 : 5 mL of chloroform is added ... \" , \" ... \" , \" 06 : 13 - 06 : 48 : After the toluene evaporates , freestanding monolayer forms ... \" ] } 6 E. More Failure Case Studies In this section, we showcase more failure case among Conceptual, Hypothetical, and Quantitative Reasoning. InternVL-3-14B misread the electroluminescence Case 1: Misinterpretation of Spectral Evidence. spectrum by hallucinating multiple peaks and voltage-dependent shifts, concluding that multiple emissive species contributed to the emission. In reality, the spectrum displayed single stable peak, indicating well-confined recombination. The model failed to recognize confinement as the key property and instead introduced instability, leading to the wrong answer. This reflects tendency to over-generalize from prior knowledge of spectroscopy rather than grounding reasoning in the observed evidence. Case 2: Misunderstanding Experimental Procedure. Qwen2.5-VL-32B erred in interpreting the role of 20-minute incubation step. It concluded that failure of incubation would prevent substrate hydrolysis, even though the substrate was not yet present during this period. The correct rationale emphasized pre-incubation of inhibitors with elastase, ensuring binding equilibrium before substrate addition. The model ignored this inhibitor-binding context, conflating incubation with general reaction progress, which resulted in selecting an irrelevant option. InternVL-3-9B miscalculated the total solvent volume in Case 3: Incorrect Solvent Accounting. polymerization setup. It fabricated multiple additions of 1,4-dioxane and neglected the actual solvents present in the protocol (DMSO and DMF). Furthermore, it mishandled the inclusion of solvent-containing stock solutions and confused monomer additions with solvents. This misidentification led to both an incorrect summation and mismatch between its numerical reasoning and the final selected option. The correct calculation, grounded in protocol details, yielded total of 4.462 mL, which the model overlooked. Conclusion. Across these cases, consistent pattern emerges: models often fail due to misalignment between the experimental context and the reasoning they generate. Errors stem from three main sources: (1) hallucination of experimental features not present in the input (spectral peaks, solvent additions), (2) failure to account for the temporal sequence of steps (substrate not present during incubation), and (3) confusion between categories of experimental components (monomers vs. solvents). These findings highlight the importance of grounding reasoning strictly in the provided experimental evidence rather than relying on generic domain priors. Improved training on step-by-step alignment between protocol details and answer derivation may reduce these systematic failures. 7 Figure 13: Failure case example of InternVL-3-14B with chain-of-thought prompt compared with the correct reasoning of Gemini 1.5 Pro. InternVL-3-14B makes the wrong predictions because of the incorrect visual perception of the experiment results, while Gemini 1.5 Pro correctly captures the unchanged state of the peak at different wavelength. 8 Figure 14: Failure case example of Qwen2.5-VL-32B-Instruct. The model makes the wrong prediction because of lacking the domain knowledge about the incubation process. 9 Figure 15: Failure case example of InternVL-3-9B. The model makes the wrong predictions because of the wrong calculating logic and wrong solvent identification."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Central Florida",
        "University of North Carolina at Chapel Hill"
    ]
}