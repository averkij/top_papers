{
    "paper_title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding",
    "authors": [
        "Jie Feng",
        "Shengyuan Wang",
        "Tianhui Liu",
        "Yanxin Xi",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 1 2 3 2 . 6 0 5 2 : r UrbanLLaVA: Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China Department of Computer Science and Technology, Tsinghua University, Beijing, China School of Electronic and Information Engineering, Beijing Jiaotong University, China University of Helsinki, Finland {fengjie, liyong07}@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Urban research involves wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate diverse urban instruction dataset encompassing both singlemodal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both singlemodal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghuafiblab/UrbanLLaVA. 1. Introduction Urban science [53, 57] and geographic science research[35] highlight that urban data spans multiple modalities, including urban visual data [14], geo-text [45], structured geospa1 Figure 1. Existing works vs. our UrbanLLaVA in urban research. tial data [1, 2], and spatiotemporal series data [20, 28]. Together, these data types capture the multi-faceted nature of urban environments, representing wide range of spatial information and urban knowledge [35, 46, 57]. Integrating these multi-modal data into cohesive framework is essential for developing systematic understanding of urban spaces and advancing complex modeling architectures in urban research. However, the inherent heterogeneity of these diverse urban data presents substantial challenges for the integration. While numerous deep learning based methods have been proposed to fuse various cross-domain urban data [57], they are often designed for specific urban tasks, limiting their ability to achieve comprehensive understanding of urban environment and advanced reasoning for real-world urban applications [46, 53]. Recently, multi-modal (MLLMs) leveraging large language models (LLMs) language models [49] have made notable advancements by [38] with large Figure 2. The framework of UrbanLLaVA, including UData, UTrain and UBench. build-in common sense and reasoning abilities as central component for unifying the processing data across various modalities, such as images [31], speech [19], and time series [25]. For example, Ma et al. [34] develop vision-language model as conversational assistant for autonomous driving, Brohan et al. [4] introduce RT-2, vision language model based end-to-end model for flexible robotics control, Li et al. [27] train LLaVA-Med for answering open-ended questions related to biomedicine images. Building on this trend, researchers have begun to explore the potential of MLLMs in urban studies [53]. As shown in the left part of Figure 1, notable examples include GeoChat [26], an early effort in creating MLLMs for remote sensing tasks; Mobility-LLM [20], which extends LLM with capabilities for trajectory modeling; and CityGPT [16], designed to process structured geospatial In contrast to earlier urban data fusion data with LLMs. methods developing in the deep learning era [53, 57], these recent studies incorporate various unimodal urban data into LLMs to create obtain MLLMs that maintain the powerful reasoning abilities and address diverse urban tasks within single modality. However, these recent works focus solely on processing unimodal urban data and fall short of achieving comprehensive understanding and modeling of urban system across diverse tasks involving multi-modal urban data. Unified modeling of multi-modal urban data poses significant challenges. The first challenge is the scarcity of highquality data for cross-modality alignment. While previous works [16, 26] propose various methods for constructing instruction tuning dataset for different types of unimodal urban data integrated with language, these efforts are insufficient for unified modeling across multiple modalities. second challenge lies in the potential conflicts among diverse urban tasks across different modalities, which can lead to unstable training and inconsistent performance. In this paper, we introduce UrbanLLaVA, multi-modal large language model designed to build comprehensive urban cognition and addressing wide range of urban tasks, In Urwhich is shown in the right part of Figure 1. banLLaVA, we first design UData, systematic urban instruction data pipeline that enables the generation of highquality synthetic data. In UData, data generation is meticulously structured to span multiple perspectivesfrom localized view for single modality data to trajectory and global view for cross-modality datacapturing the inherently multi-faceted nature of urban system. To improve the training stability and model performance, we conduct extensive experiments to identify key factors impacting the training process and develop an effective three-stage training pipeline UTrain, based on these insights. In fact, the proposed multi-stage training framework can be viewed as promising practice that explicitly decouples the learning of reasoning capabilities from domain-specific knowledge 2 2.1. UData: Constructing Urban Instruction Data from Multi-View Perspective of Urban Space Over the past decade, effectively integrating multi-modal urban data has emerged as key research question in urban studies [57]. Building on the successes of MLLMs in various fields [49, 53], we extend the modelling of four types of urban data into unified model, UrbanLLaVA, by constructing diverse urban instruction data from systematic view on the urban environment. Specifically, we organize the urban instruction data in sequence that move from location view to trajectory view, and finally to global view. This approach ensures both broad spatial coverage and the integrity of relationships between different modalities in the final data. UData builds upon four kinds of original urban data: 1) the structured geospatial data from OpenStreetMap 1; 2) public trajectory data, e.g., Foursquare-checkins [48] and OpenStreetMap traces 2; 3) satellite images from GoogleEarth 3; 4) street view images from GoogleMap 4 and BaiduMap 5. Before experiments, we collect original data from above platforms and using the following data pipeline to build instruction data. We follow the license of these platforms and ensure that the data is used only for academic research. 2.1.1. Location View Data In the location view data construction stage, we focus on structured geospatial data and single street view images. Following the recent practices [1, 16] for structured geospatial knowledge learning, we create geospatial instruction data by designing question templates that transform basic geospatial data into natural language question and answers. For single street view image, we synthesize three types of questions: 1) two types based on predefined templates populated with information from structured geospatial data, such as, location addresses and landmark details; 2) one general MLLM generated detailed description of the image content, following the common practice for image captioning [6]. Throughout the data construction, we maintain core principle of integrating street view image content with structured geographical knowledge, such as consistency in location addresses and landmark descriptions. 2.1.2. Trajectory View Data Here, we construct the trajectory view data, which includes the geospatial data, trajectory data, and street view images. We start by creating two types of text-based trajectory data. The first type is generated by randomly sampling origin and destination points for routing, while the second type uses the real-world trajectory data collected from the public web 1https://www.openstreetmap.org 2https://www.openstreetmap.org/traces 3https://earth.google.com/ 4https://www.google.com/maps 5https://map.baidu.com/ Figure 3. The thorough composition of UData in Beijing. in MLLMs. Finally, we extend existing urban benchmarks to build systematic urban benchmark UBench for evaluating the capabilities of MLLMs in tackling diverse urban tasks. In summary, our contributions are as follows, UrbanLLaVA is the first MLLM designed for the unified modeling of four major types of urban data, with the goal of fostering comprehensive understanding and effective task-solving for urban environments, to the best of our knowledge. We conduct extensive experiments to identify the key factors influencing training and propose three-stage training pipeline that ensures stable performance of UrbanLLaVA across wide range of urban tasks involving multiple data modalities. UrbanLLaVA demonstrates effective integration of multi-modal data, establishing comprehensive spatial cognition and outperforming general MLLMs across various urban tasks based on results from an enhanced urban task benchmark. 2. Methods As illustrated in Figure 2, UrbanLLaVA comprises three key components: 1) the data pipeline, UData, designed for generating diverse and high-quality urban instruction data across various urban scenarios; 2) the training pipeline, UTrain, which facilitates efficient and stable training across wide range of urban tasks; 3) the evaluation benchmark, UBench, for evaluating the capabilities of MLLMs in multimodal urban tasks. source, including Foursquare-checkins and OpenStreetMap traces. To enhance geospatial context of trajectory data, we align the GPS coordinates from these original data sources with the structured geospatial data, using the textual addresses to represent locations within the trajectory. Additionally, we integrate street view images to enrich trajectory data, resulting two types of vision-augmented trajectory data. The first data extends the text-based trajectory data by incorporating street view images captured along the route (excluding intersections). We organize this data with the similar interleaved image-text format in VILA [29]. The second data builds on the navigation instruction format akin to the classic vision-language navigation task [5]. In this data, multiple street view images are presented at intersection during the trajectory, and the correct image is selected to guide the continuation of the journey. 2.1.3. Global View Data Here, we present the construction of global view data designed to capture relationships among diverse data types over long distances, with street view images and satellite images as primary components and geospatial data serving as auxiliary support. Initially, we create basic form of global view data by generating captions for single satellite image data enriched with structured geospatial data. Specifically, we define three types of data: 1) prompting general MLLM to produce detailed content description for individual satellite image; 2) sampling location address within satellite image and using general LLM to summarize the spatial coverage of it based on these location address; 3) prompting general MLLM with land use ground-truth label to generate land use inference results with reason. Furthermore, we introduce the multiple satellite images for more complex instruction data. The first task is to compare the building densities across multiple satellite images. The second task focuses on identifying functional point of interest within these images. For these tasks, we provide manually crafted reasoning steps in chain-of-thoughts format, supported by structured geospatial data, to improve the alignment between satellite images and geospatial data. Finally, we design two tasks to strengthen the alignment between the street view images and satellite images. The first task is to select the correct satellite image from set when given street view image, requiring the model to understand and match content or address across both image types. The second, more challenging task involves pinpointing the location of the street view image within specific satellite image, such as identifying it as located in the top-left region of satellite image. Based on the data generation steps described before, we perform data quality checks and filtering on the synthesized data to ensure its quality. 2.2. UTrain: Multi-Stage Training Pipeline for Decoupling Reasoning and Knowledge Learning Figure 4. UTrain: three-stage training pipeline. Training UrbanLLaVA presents significant challenges due to the heterogeneity of multi-modal urban instruction data and the diversity of urban tasks. Achieving stable training and balancing performance across various tasks is notably difficult. We chose VILA[29] as the base model for our experiments and conduct extensive studies to identify key factors affecting training. We examine the impact of the training order of multi-modal data and trained components, observing minimal effects. However, we find that learning rate has significant effects on training stability and performance. Detailed results of them are provided in the section 3.3. Additionally, inspired by Dong et al. [12], we explore and propose an effective multi-stage training pipeline which is shown in the Figure 4. We first introduce three kinds of learning procedures: knowledge learning, task alignment and mixture learning. The knowledge learning procedure refers to the training process which UrbanLLaVA acquires foundational urban knowledge from various urban data, such as the information of geospatial data, pure textual trajectory, and detailed description of street view and satellite images. The task alignment learning focuses on equipping UrbanLLaVA with task-specific skills for urban applications, including vision-language navigation, trajectory prediction, chain-of-thoughts reasoning across multiple satellite and street view images. Finally, mixture learning represents the standard training method used by most MLLMs, which training by directly mixing all types of instruction data. During our experiments, we observe that different combination of various learning procedures significantly impact the training. Based on the observations, we propose threestage tuning pipeline to improve the training stability and performance on diverse urban tasks. This pipeline consists of three sequential stages:task alignment, knowledge learning, and finally mixture learning. Starting with welltrained general MLLM as our base model, we first introduce the task alignment learning procedure, fine-tuning the model with diverse urban task related instructions to prepare it for various urban tasks. Through this phase, the model become familiar with variety of urban tasks, leveraging its pre-existing general knowledge to complete them. However, familiarity with general knowledge alone is insufficient for effectively addressing diverse urban tasks, so we incorporate the second stage, knowledge learning procedure. This stage imparts specialized urban knowledge from multi-modal urban data that is essential for task resolution. Finally, we introduce the mixture learning stage to enhance the models awareness of combining knowledge and skills for solving diverse urban tasks. Here, we resample 1/3 domain specific data from the first two stages and 1/3 general textual instruction data, e.g. ShareGPT 6 and UltralChat [11], for final tuning. 2.3. UBench: An Enhanced Multimodal Benchmark for Urban Intelligence Tasks To assess the potential of MLLMs in urban studies, CityBench [18] and Urbench [56] have been recently introduced. Drawing from the diverse evaluation tasks in these two benchmarks, we reorganize and expand them to create the urban evaluation benchmark UBench, which includes 12 tasks for our experiments. All the evaluation tasks are presented in Table1. We select 6 of these tasks based on the utility of the evaluation data and their relevance to urban scenarios involving UrbanLLaVAs urban data. For structured geospatial data and trajectory modelling, we incorporate the GeoQA, trajectory prediction and navigation task from CityBench. For cross-view urban tasks involving both street view and satellite images, we adopt the image retrieval, camera localization, and scene comparison task In addition, we introduce 6 new tasks in from UrBench. UBench. Four of these tasks are designed for single street view and satellite images, including address inference for both image types, landmark recognition for street view images, and land use inference for satellite images. These single-image tasks are aligned with the urban instruction data, and we partition the original dataset into training and validation sets to prevent potential data leakage. Moreover, we build 2 additional tasks involving multiple images: 1) STV-Outlier, is spatial consistency task for street view image, where multiple images from single trajectory are compared to identify an outlier image not part of the trajectory; 2) SceneFunc, extends the scene comparison task from UrBench, challenging model to select the correct satellite image that fulfills specific functional requirements. 3. Experiments 3.1. Settings We select Beijing, London and New York to conduct experiments. Due to the large volume of data, we select region Table 1. Detailed information about UBench for Beijing, STV refers to street view image, and SAT refers to satellite image."
        },
        {
            "title": "Samples Source",
            "content": "Geospatial Data GeoQA Avg. Accuracy Trajectory Data Geo+Traj GeoQA TrajPredict Navigation Geo+Traj Success Rate SceneComp Geo+SAT ImgRetrieval Multi STV & SAT Geo+SS CameraLoc Multi STV & SAT Geo+SS"
        },
        {
            "title": "Single STV\nMulti SAT",
            "content": "Top-1 STV-Address STV-Landmark SAT-Address SAT-Landuse STV-Outlier SceneFunc"
        },
        {
            "title": "Single STV\nSingle STV\nSingle SAT\nSingle SAT\nMulti STV\nMulti SAT",
            "content": "Geo+STV Geo+STV Geo+SAT Geo+SAT Geo+STV Geo+SAT"
        },
        {
            "title": "1450 CityBench\nCityBench\n500\nCityBench\n50\nUrBench\n200\nUrBench\n200\nUrBench\n200",
            "content": "200 200 200 200"
        },
        {
            "title": "UBench\nUBench\nUBench\nUBench\nUBench\nUBench",
            "content": "from each cities to conduct experiments. The spatial coverage of each region is shown in supplementary material. MLLMs We consider the following MLLMs as baselines: Qwen2VL-7B/72B [41], InternVL2-8B/26B [7, 8], VILA1.5-3B/8B/13B [29], LLama3.2-11B/90B [36], and GPT4o and GPT4o-mini [40]. For open source MLLMs, we deploy them through VLMEvalKit [13]. The max output tokens are set to 1000, and the temperature is set as 0. Metrics Table 1 contains all the metrics for UBench. For general evaluation tasks including LLaVA-Bench(Inthe-Wild) [30], RealWorldQA [44], and MM-Vet [50], RealWorldQA uses accuracy as the metric, while LLaVABench(In-the-Wild) and MM-Vet use rating score form GPT4o as the judgement. Implementation We use codes from official repository 7 of VILA [29] for fine-tuning on single 8xA100 node. The training parameters are set as follows: learning rate of 1e5, maximum sequence length of 2048, batch size of 8 per GPU, and one training epoch. Training UrbanLLaVA for Beijing on 4xA100 took total of 10.7 hours. 3.2. Main Results The main results of UrbanLLaVA on three cities are presented in Table 2, more detailed information can be accessed in 9. We use VILA1.5-8B as the default base model in most experiments and use UData with UTrain methods to fine-tune it to obtain the final model UrbanLLaVA. We analyze the results in Beijing first. One point to note is that, since LLama3.2 series models currently do not support multi-image input, the results for evaluation tasks involving multiple images in the UBench are left blank. For models within the same series, the general trend is that larger parameter models tend to perform better, e.g., VILA1.5-13b significantly outperforms VILA1.53b on 5 out of 6 tasks, including both single modal and cross modal tasks. Additionally, we observe that the latest released Qwen2VL series models outperform the GPT4o se6https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English90k 7https://github.com/NVlabs/VILA 5 Table 2. Results on UBench at Beijing, London, and New York. UrbanLLaVA significantly outperforms other baselines in most task across cities. Here, STV denotes street view images related tasks, Geo denotes geospatial data, Traj denotes trajectory related task, SAT denotes satellite images related tasks, and SS denotes street view + satellite images. Detailed subtask and metrics can refer to Table 1."
        },
        {
            "title": "City\nTask Group",
            "content": "VILA1.5-3B VILA1.5-8B VILA1.5-13B InternVL2-8B InternVL2-26B Qwen2VL-7B Qwen2VL-72B LLaMA3.2-11B LLaMA3.2-90B GPT4o-mini GPT4o London GeoQA Geo+Traj Geo+STV Geo+SAT Geo+SS GeoQA Geo+Traj Geo+STV Geo+SAT Geo+SS GeoQA Geo+Traj Geo+STV Geo+SAT Geo+SS"
        },
        {
            "title": "Beijing",
            "content": "0.3873 0.4322 0.4410 0.4709 0.4877 0.4950 0.5491 0.4229 0.4502 0.4542 0.5479 0.0200 0.0589 0. 0.1578 0.1478 0.1389 0.1611 0.0756 0.1056 0.1622 0.1522 0.3967 0.4300 0.5167 0.4667 0. 0.4383 0.5817 0.4375 0.5325 0.4350 0.4300 0.3200 0.3488 0.3638 0.3313 0.3825 0.3638 0. 0.3075 0.2925 0.3800 0.4125 0.2575 0.2425 0.2400 0.2325 0.2275 0.2675 0.2975 / / 0.2475 0.3025 0.4362 0.4841 0.4592 0.4973 0.5168 0.4991 0.5802 0.4804 0.5659 0.5357 0. 0.0400 0.0884 0.1298 0.1347 0.1288 0.1560 0.2322 0.1180 0.2010 0.1278 0.1300 0.2557 0.4495 0. 0.4477 0.4923 0.4381 0.6375 0.4000 0.5450 0.4752 0.5469 0.2850 0.4575 0.4538 0.4763 0. 0.4863 0.4375 0.3800 0.4700 0.5388 0.6050 0.2725 0.2575 0.2625 0.2400 0.2425 0.2775 0. / / 0.2675 0.2850 0.3954 0.4575 0.4501 0.4632 0.4766 0.4567 0.5273 0.4127 0. 0.5075 0.6232 0.0400 0.1200 0.2350 0.1830 0.2240 0.1700 0.2540 0.1100 0.1570 0.2320 0. 0.4400 0.4983 0.5583 0.4917 0.5217 0.5117 0.6333 0.5200 0.6825 0.5633 0.5767 0.2713 0.3763 0. 0.4175 0.4738 0.5100 0.3788 0.2225 0.3400 0.4775 0.5400 0.2425 0.2525 0.2825 0.2400 0. 0.2950 0.3275 / / 0.2350 0.2900 UrbanLLaVA-VILA1.5-8B 0.5682 vs. VILA1.5-8B vs. Best Baseline 0.7500 +31.47% +375.38% +101.16% +91.03% +189.69% +32.18% +203.17% +66.85% +55.19% +67.96% +26.19% +155.00% +70.57% +105.32% +130.69% +3.48% +72.63% +48.70% +61.53% +132.23% -0.73% +15.42% +17.65% +17.36% +33.08% -7.37% +20.47% +24.54% +43.06% +77.86% 0. 0.6663 0.2800 0.8650 0.7025 0.6399 0. 0.7100 0.4325 0.5773 0.3060 0.8500 0. ries models on 2 tasks. These results demonstrate the validity and usability of our UBench. Our UrbanLLaVA shows marked improvements over all baselines across all tasks in UBench. Against the best baselines, UrbanLLaVA achieves performance gains ranging from 3.48% to 132.23% for each task. When compared to the base model VILA1.58B, the minimum increase is 31.47% on the GeoQA task, while the maximum reaches an impressive 375.38% on the Geo+Traj task. These results highlight the effectiveness of the proposed multi modal dataset, UData, which successfully equips smaller MLLMs with variety of capabilities within urban space, achieving superior performance over all advanced general MLLMs. The results in New York and London are similar to those in Beijing. Out of 5 tasks, UrbanLLaVA@London and UrbanLLaVA@NewYork both perform best in 4 tasks. However, in GeoQA task, their performance is slightly inferior to GPT4o, with reductions of -0.73% and -7.37%, respectively. For UrbanLLaVAs performance falling short of expectations on certain task, we speculate two possible reasons: first, the quality of relevant data in the two cities may be lower than that in Beijing, preventing the model from acquiring urban capabilities through learning in the training stage; second, the base model VILA1.58B may have comparatively weaker capabilities than commercial API GPT4o, e.g., for the GeoQA task, UrbanLLaVA@London outperforms VILA1.5-8B by 32.18% but falls short of GPT4o by 0.73%. Overall, the proposed UrbanLLaVA sucessfully enhance the performance of small MLLMs on diverse urban tasks. stable and well-performing training strategies. Due to the limitation of space, we only report the multi-stage results here, more results on learning rate, modality and trained components can refer the supplementary material. We divide the training dataset into two categories: basic knowledge data and task format aligned data, aiming to develop training pipeline that enables the model to perform stably and effectively on diverse urban tasks. As Figure 5a shows, Three stage: TAKMix performs best in most tasks and maintains reliable performance, surpassing the default tuning method for MLLMs. We also probe the effects of the order between knowledge learning and task alignment in Figure 5b and Figure 5c. We find that in twostage training, KTA slightly outperforms TAK. However, when the third mixture learning is added in twostage training model, having task alignment first achieves better results, surpassing the models in two-stage training. We hypothesize that this is because in the two-stage training, the model first learns the foundational knowledge and then learns how to solve specific tasks. In the three-stage training, the two-stage model that knowledge learning first and then task alignment already possesses considerable capabilities, so the impact of mixture training is not significant. However, for two-stage model that completes task alignment before knowledge learning, mixture learning can enhance its abilities, allowing it to recall how to solve urban tasks learned previously. On the whole, the proposed three-stage training pipeline UTrain integrates cross-modal data to achieve stable training and balanced performance across various urban tasks. 3.3. Effects of Training Strategies 3.4. Model Generalization Study Since UrbanLLaVA is trained with multi-modal urban instruction data, we conduct various experiments to explore Here, we show that UrbanLLaVA can be generalized to different data distributions and tasks, which are crucial for (a) The performance of three-stage tuning, gray part is the default tuning method for MLLMs. (b) The effects of the order between knowledge learning and task alignment in two-stage tuning. (c) The effects of the order between knowledge learning and task alignment in three-stage tuning. Figure 5. Performance of different training strategies. refers to knowledge learning, TA refers to task alignment, and Mix refers to mixture learning. One stage: + TA means knowledge learning and task alignment are merged in the same stage. Two stage: TAK means task alignment first then knowledge learning in the second stage. Three stage: TAKMix adds step in the third stage: mixture learning. The tasks detailed in the table are those with significant differences across different training strategies, while Others refers to other tasks in UBench with smaller differences. general urban intelligence. As Table 3 shows, while our UrbanLLaVA performs well in diverse urban tasks, it also maintains the original stability in general scenarios, including LLaVA-Bench [30], RealWorldQA [44], and MMVet [50]. The results demonstrate that UrbanLLaVA is competitive in the dimension of various daily-life visual tasks, real-world spatial understanding and integrated capabilities which is the base for general urban intelligence. Table 3. General benchmark results. Rating Score refers to result from the LLM-as-a-judge method with GPT4o. For LLaVABench, scores range from 0 to 100, for MM-Vet, scores range from 0.0 to 1.0. Higher scores indicate better performance. Test@General LLaVA-Bench (In-the-Wild)"
        },
        {
            "title": "Rating Score",
            "content": "VILA1.5-8B Ours-8B 60.75 58."
        },
        {
            "title": "RealWorldQA",
            "content": "MM-Vet"
        },
        {
            "title": "ACC",
            "content": "0.3765 0."
        },
        {
            "title": "Rating Score",
            "content": "0.3518 0.3239 Different cities exhibit various natural and artificial features. Thus, the transferability of urban model is important for its application. As Figure 6 shows, apart from in-domain capabilities empowering and performance improvement after learning, UrbanLLaVA can generalize to out-of-domain tasks in various cities. Here, we examined our model trained in the Beijing training set and it exhibits competitive capabilities when tested on London and New York benchmarks. We can see from Figure 6, performance improvements are observed across all tasks in London and New York. Notably, for challenging aspects such as trajectory and regional tasks, the enhancements are significant, indicating the presence of similarity structures across cities that go beyond simple, naive differences. 3.5. Data Ablation Study Here, we investigate the influences of different data compositions, with results shown in Table 4. As outlined in Section 2.1, the urban instruction data is divided into three different subsets: local view, trajectory view, and global view. We remove each subset individually and observe the resulting performance differences. Local view: It consists of textual urban geography denoted as CityQA and street view related data denoted as STV. Local view data is important for different tasks requiring intelligence about local part of cities. Noticeable deterioration is observed in both singlemodal and multi-modal tasks, indicating the importance of locality knowledge for overall urban understanding. Trajectory view: It describes the knowledge about continuous spaces in urban areas. It contains text-trajectory (random walk routing and real-world trajectories) and visualtrajectory( visual-language navigation instructions and random walk with visual input). Both text and multi-modal trajectory view datasets are essential for navigation task. It is also shown that trajectory view data is helpful for different tasks like SceneFunc and GeoQA. Global view: It includes subset of single satellite images that focus on urban knowledge from specific area, as well as subset of multiple satellite images that highlight the correlations between different regions and cross alignment between satellite and street view images. Results show that they are essential to empower MLLM to handle urban tasks from global view, e.g, ImgRetrieval and CameraLoc, while local capabilities are already competitive. 3.6. Case Study Here, we show two typical examples of urban task instances to demonstrate that UrbanLLaVA can handle challenging urban tasks. Due to space limits, other cases can refer to supplementary materials. SceneFunc. This task challenges the model to identify which satellite image contains the highest concentration of Involva specified category of places of interest (POIs). ing multiple image inputs and text prompts, SceneFunc task demands the model to understand and compare the differ7 Figure 6. Learning from one city (Beijing) can be directly generalized to other cities (London and New York). In this figure, Baseline is VILA1.5-8b, and our UrbanLLaVA is only trained with the urban instruction data from Beijing. Table 4. Ablation results on different urban instruction data compositions. The arrows indicate corresponding comparison with ours. Only significant differences are denoted. For TrajPredict task, the threshold is 1%, for other tasks, the threshold is 5%. All models are trained using the one-stage strategy to optimize experimental efficiency."
        },
        {
            "title": "Metric",
            "content": "Data View GeoQA TrajPredict Address Landmark Address LandUse Navigation STV-Outlier SceneComp SceneFunc ImgRetrieval CameraLoc Avg. Acc Acc@"
        },
        {
            "title": "Acc",
            "content": "- Local Local Ours w/o CityQA w/o STV 0.5741 0.5409 0.5192 w/o Traj-Text&Nav Trajectory 0.4769 0.5590 0.5345 0.5420 w/o Traj-Vision w/o SAT-Single w/o SAT-Multi"
        },
        {
            "title": "Trajectory\nGlobal\nGlobal",
            "content": "0.0711 0.0822 0.0622 0.0644 0.0690 0.0778 0.0778 0.8750 0.8900 0.7450 0.7850 0.8550 0.7150 0.6950 0.8700 0.4300 0.7300 0.4700 0.7200 0.6350 0.7050 0.8100 0.7300 0.7100 0.8350 0.5550 0.4550 0.8600 0.6200 0.6800 0.8500 0.8800 0.9050 0.9100 0.8700 0.3600 0.4000 0.4200 0.0000 0.3000 0.3800 0.3400 0.7800 0.8050 0.6700 0.7600 0.8000 0.7800 0.6450 0.5500 0.5400 0.4900 0.4950 0.5150 0.5150 0.3500 0.5050 0.5200 0.4550 0.4300 0.4650 0.4100 0.3400 0.7300 0.7750 0.6250 0.6800 0.7150 0.7200 0.3950 0.5100 0.5200 0.4250 0.4600 0.4950 0.4800 0.2600 Figure 7. An example of the SceneFunc task, where correct answers are in green, wrong ones in red. Figure 8. An example of the STV-Outlier task. ences between different images. As shown in Figure 7, while VILA1.5-8B fails to answer the question, our UrbanLLaVA succeeds in giving the correct answer. Our model exhibits strong capabilities like multiple image understanding and comparison in this example, and is competitive with the successful closed-source model. STV-Outlier. This task requires model to compare between multiple street views and point out the closest one to reference. Figure 8 shows an instance of this task, where VILA1.5-8B does not successfully identify the scene of the reference image. GPT-4o-mini is closer, but it is still confused by another wrong option. Our model shows its capabilities of understanding multiple images and conducting high-level implicit logical reasoning in an urban context, outperforming these general MLLMs. 4. Related Work 4.1. Multi-modal Large Language Model Since the success of GPT4-V [39], MLLM [49] have become major area of focus in research community, exemplified by the development of models like the LLaVA [30, 31], VILA [29], QwenVL [41] and InternVL [7, 8]. One of the most promising solution to develop advanced MLLM is constructing diverse and high-quality instruction dataset. For example, LLaVA [31] use GPT4-V to create visual instruction tuning data, leading to the training of the first open source MLLM. Following LLaVA, VILA [29] explore the effects of training pipelines and data formats during the pretraining stage. ShareGPT4v [6] further expand data scale by developing superb caption model trained on high-quality caption data from GPT4-V. While general MLLM demonstrate strong visual understanding and reasoning capabilities [9, 21, 43, 47] in common scenarios, they often face challenges in many specialized fields such as medical applications and remote sensing tasks. Thus, domain-specific multi-modal large language models [42] are proposed, such as, Dolphins [34] for autonomous driving, GeoChat [26] for remote sensing tasks, and various models for medical application [23]. In this paper, we propose the first MLLM for urban intelligence which can handle various data and diverse tasks in urban field. 4.2. Multi-modal Model for Urban Study Urban research is an interdisciplinary field that exists multimodal data sources [10, 17, 35, 53, 57], including structured geospatial data [2], spatiotemporal series data [57], remote sensing data [35, 55] and street view data [3, 14, 54]. Inspired by the recent advances of MLLMs, researchers explore their potential in urban studies. For structured geospatial data, Balsebre et al. [1] and Feng et al. [16] propose various methods to convert structured geospatial data into language-compatible format to enhance the geospatial knowledge of large language models. For remote sensing data [24, 33, 37, 51], Kuckreja et al. [26] and Zhang et al. [52] design various remote sensing instruction data to fine-tune general MLLMs for various downstream remote sensing tasks. For street view data, Hao et al. [22] finetune CLIP model for improved urban indicator prediction by integrating street view data and remote sensing data. Liu et al. [32] evaluate the potential of multi-modal language model for urban socioeconomic sensing. For spatiotemporal series data, Li et al. [28] and Gong et al. [20] introduce domain-specific encoders to enhance LLM capabilities for spatiotemporal series modeling. Feng et al. [15] propose agentic framework to unleash the power of LLM for zeroshot mobility prediction. Unlike these works that focus on limited data types and specific tasks, our method is designed to process all these data types and address wide range of urban tasks. 5. Conclusion In this paper, we propose UrbanLLaVA, MLLM with enhanced urban spatial cognition by integrating four types of urban data and supporting wide range of urban tasks. Our approach begin with the development of diverse and highquality urban instruction data, spanning from local view to global view of urban environment. We then design threestage training pipeline to ensure the stable training and improved performance of model on diverse urban tasks. Experimental results from three cities on an extended urban benchmark highlight the effectiveness of UrbanLLaVA for integrating multi-modal urban data and solving urban tasks. In summary, UrbanLLaVA sheds lights for building the unified foundation model with powerful perception and reasoning abilities for general urban intelligence. 6. Limitation and Future Work to explore UrbanWhile we have made every effort LLaVA and present our findings clearly, several limitations remain. Our experiments have focused on the 8B model; the full potential of UData and UTrain on larger models In addition, UBench can still be has yet to be realized. improved by refining the design of tasks, testing MLLMs overall multi-modal capabilities from more fine-grained perspective. Lastly, more modalities could be included like video, time series data, etc., which are also important in urban intelligence. In the future, we plan to extend UrbanLLaVA to incorporate more diverse data types in urban research and tackle more advanced urban tasks from various interdisciplinary fields."
        },
        {
            "title": "References",
            "content": "[1] Pasquale Balsebre, Weiming Huang, and Gao Cong. arXiv preprint Lamp: language model on the map. arXiv:2403.09059, 2024. 1, 3, 9 [2] Pasquale Balsebre, Weiming Huang, Gao Cong, and Yi Li. City foundation models for learning general purpose representations from openstreetmap. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 8797, 2024. 1, 9 [3] Filip Biljecki and Koichi Ito. Street view imagery in urban analytics and gis: review. Landscape and Urban Planning, 215:104217, 2021. 9 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2 [5] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1253812547, 2019. 4 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 3, [7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5, 8 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 5, 8 9 [9] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024. 9 [10] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. Understanding world or predicting future? comprehensive survey of world models. arXiv preprint arXiv:2411.14499, 2024. 9 [11] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. [12] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023. 4 [13] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An opensource toolkit for evaluating large multi-modality models, 2024. 5 [14] Zhuangyuan Fan, Fan Zhang, Becky PY Loo, and Carlo Ratti. Urban visual intelligence: Uncovering hidden city profiles with street view images. Proceedings of the National Academy of Sciences, 120(27):e2220417120, 2023. 1, 9 [15] Jie Feng, Yuwei Du, Jie Zhao, and Yong Li. Agentmove: large language model based agentic framework for zero-shot next location prediction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 13221338, 2025. 9 [16] Jie Feng, Tianhui Liu, Yuwei Du, Siqi Guo, Yuming Lin, and Yong Li. Citygpt: Empowering urban spatial cognition of large language models. In Proceedings of the 31th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2025. 2, 3, 9 [17] Jie Feng, Jinwei Zeng, Qingyue Long, Hongyi Chen, Jie Zhao, Yanxin Xi, Zhilun Zhou, Yuan Yuan, Shengyuan Wang, Qingbin Zeng, et al. survey of large language model-powered spatial intelligence across scales: Advances in embodied agents, smart cities, and earth science. arXiv preprint arXiv:2504.09848, 2025. 9 [18] Jie Feng, Jun Zhang, Tianhui Liu, Xin Zhang, Tianjian Ouyang, Junbo Yan, Yuwei Du, Siqi Guo, and Yong Li. Citybench: Evaluating the capabilities of large language models for urban tasks. In Proceedings of the 31th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2025. 5 [19] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. [20] Letian Gong, Yan Lin, Xinyue Zhang, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, and Huaiyu Wan. Mobility-llm: Learning visiting intentions and travel preferences from human mobility data with large language models. arXiv preprint arXiv:2411.00823, 2024. 1, 2, 9 [21] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13796 13806, 2024. 9 [22] Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, and Yuxuan Liang. Urbanvlp: multigranularity vision-language pre-trained foundation model for urban indicator prediction. arXiv preprint arXiv:2403.16831, 2024. 9 [23] Iryna Hartsock and Ghulam Rasool. Vision-language models for medical report generation and visual question answering: review, 2024. 9 [24] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li. Rsgpt: remote sensing vision language model and benchmark. arXiv preprint arXiv:2307.15266, 2023. 9 [25] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, YuanFang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023. [26] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2783127840, 2024. 2, 9 [27] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 2 [28] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. Urbangpt: Spatiotemporal large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 53515362, 2024. 1, 9 [29] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for In Proceedings of the IEEE/CVF visual language models. Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 4, 5, 8 [30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 5, 7, 8 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 8 [32] Tianhui Liu, Jie Feng, Hetian Pang, Xin Zhang, Tianjian Ouyang, Zhiyuan Zhang, and Yong Li. Citylens: Bench10 [47] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. 9 Participatory cultural mapping based on collective behavior data in location-based social networks. ACM Transactions on Intelligent Systems and Technology (TIST), 7(3):123, 2016. 3 [48] Dingqi Yang, Daqing Zhang, and Bingqing Qu. [49] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 1, 3, 8 [50] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024. 5, 7 [51] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Skyeyegpt: Unifying remote sensing vision-language tasks via instruction tuning with large language model. arXiv preprint arXiv:2401.09712, 2024. [52] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and Xuerui Mao. Earthgpt: universal multi-modal large language model for multi-sensor image comprehension in remote sensing domain. IEEE Transactions on Geoscience and Remote Sensing, 2024. 9 [53] Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, and Hui Xiong. Urban foundation models: survey. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 66336643, 2024. 1, 2, 3, 9 [54] Xin Zhang, Tianjian Ouyang, Yu Shang, Qingmin Liao, and Yong Li. UrbanMLLM: Joint learning of cross-view imagery for urban understanding, 2025. 9 [55] Yunke Zhang, Ruolong Ma, Xin Zhang, and Yong Li. Perceiving urban inequality from imagery using visual language models with chain-of-thought reasoning. In Proceedings of the ACM on Web Conference 2025, pages 53425351, 2025. 9 [56] Baichuan Zhou, Haote Yang, Dairong Chen, Junyan Ye, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, Conghui He, and Weijia Li. Urbench: comprehensive benchmark for evaluating large multimodal models in multi-view urban scenarios. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1070710715, 2025. 5 [57] Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, et al. Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook. Information Fusion, 113:102606, 2025. 1, 2, 3, marking large language-vision models for urban socioeconomic sensing. arXiv preprint arXiv:2506.00530, 2025. 9 [33] Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, et al. Skysensegpt: fine-grained instruction tuning dataset and model for remote sensing visionlanguage understanding. arXiv preprint arXiv:2406.10100, 2024. 9 [34] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. arXiv preprint arXiv:2312.00438, 2023. 2, 9 [35] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. On the opportunities and challenges of foundation models for geoai (vision paper). ACM Transactions on Spatial Algorithms and Systems, 2024. 1, 9 [36] Meta AI. LLaMA 3.2: Advancing Vision, Edge, and Mobile Devices. https://ai.meta.com/blog/llama3 - 2 - connect - 2024 - vision - edge - mobile - devices/, 2024. Accessed: 2024-11-01. 5 [37] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model. arXiv preprint arXiv:2402.02544, 2024. 9 [38] OpenAI. Introducing chatgpt. https://openai.com/ blog/chatgpt/, 2022. [39] OpenAI. Gpt-4v(ision) system card. 2023. 8 [40] OpenAI. Hello GPT-4. https : / / openai . com / index/hello-gpt-4o/, 2024. 5 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5, 8 [42] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of visionlanguage tasks. arXiv preprint arXiv:2406.08394, 2024. 9 [43] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. [44] XAI Organization. Dataset. https : / / huggingface . co / datasets / xai - org/RealworldQA, 2024. Accessed: 2024-10-01. 5,"
        },
        {
            "title": "RealworldQA",
            "content": "[45] Zhaomin Xiao, Eduardo Blanco, and Yan Huang. Analyzing large language models capability in location prediction. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 951958, 2024. 1 [46] Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, and Yong Li. Urban generative intelligence (ugi): foundational platform arXiv preprint for agents in embodied city environment. arXiv:2312.11813, 2023. 1 11 UrbanLLaVA: Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Data Example for Three Stages Below, we provide examples for each stage. Figure 9. Input data examples for three-stage training. 8. Comparing with models for single-modality urban tasks We focus on comparing our approach with these modalityspecific models to better showcase its effectiveness, which are presented in following Table 5. Table 5. Comparing with models for single-modality urban tasks. well, with improvements over the best baseline spanning from 6.25% to 122.78%. 10. Additional Results for Training Strategies As supplement to results in Section 3.3, we report results on learning rate, modality and trained components here. We first adjusted some experimental parameters to explore their effects, and ultimately found that the learning rate is the key parameter influencing training stability and model performance. As shown in Figure 10a, we conduct experiments on the same data with different training parameters, and compared to learning rate of 1e-4 (the default choice of VILA), the curve is smoother and lower with learning rate of 1e-5. We think training with mixed domainspecific structured instruction data, lower learning rate (1e-5) enables the model to handle features from different modalities more robustly. Then we consider whether to separate text data and vision data during training to explore the influence of text data and vision data on training. But as shown in Figure 10b, training with text and vision data in one stage yields better results compared to the other two experiments and base model VILA1.5-8B. We also investigated the impact of training components. As Figure 10c presents, using different components to train the same data shows little difference. 11. Effects of Training Data Size Fig. 11 presents training results with different amounts, exhibiting the high quality of UData."
        },
        {
            "title": "GeoChat\nCityGPT\nUrbanCLIP\nOurs",
            "content": "GeoQA STV-Address 0.3746 0.5238 - 0.5741 0.3100 - - 0.8550 STV-Landmark 0.2050 - - 0.8750 SAT-Address 0.2872 - - 0.7450 SAT-Landuse 0.4650 - 0.3750 0.7850 12. Effects of Base Model Our method is model-agnostic and can be generalized to different MLLMs, e.g., Qwen2.5-VL-7B in Table 9. 9. Additional Detailed Results of Three Cities The detailed results of UBench on three cities are presented in Table 6, Table 7 and Table 8. Table 2 in Section 3.2 is the aggregated results of these three tables. UrbanLLaVA@Beijing surpasses the baselines in all tasks, showcasing exceptional performance. UrbanLLaVA@London delivers top results in 9 out of 12 tasks, achieving gains over the best baseline ranging from 5.17% to 49.38%. Meanwhile, UrbanLLaVA@NewYork performs best in 9 tasks as 13. Effects of Model Size As Figure 12 shows, performance generally improves with increasing parameter size for VILA 1.5 (from 3B to 13B). However, for certain tasks, models of different sizes exhibit similar capabilities. This occurs either because the tasks are inherently challenging (e.g., trajectory prediction) or relatively easy (e.g., SAT-Landuse). Compared to VILA1.58B, the performance improvement of VILA1.5-13B is minimal, likely due to the capabilities of the LLaMA3-8B and LLaMA2-13B models utilized by VILA1.5. However, due (a) The changes in training loss under the settings of learning rates 1e 4 and 1e 5. (b) Results on UBench when UData is divided into text data and vision data. (c) Results on UBench from experiments using different training components. Figure 10. (a) illustrates that the training loss is smoother and lower when the learning rate is 1e 5 (ours) compared to 1e 4(VILA). (b) One stage: text means training with text data, Two stage: textvision means training with text data in the first stage then vision data in the second stage, One stage: text+vision means training with text and visual data in one stage. Others refers to other tasks in UBench. (c) refers to Text data, refers to Vision data and T-LLM-ProjV-LLM-Proj means training text data with LLM and Projector in the first stage, later vision data with LLM and Projector. Table 6. Main results on UBench at Beijing. UrbanLLaVA significantly outperforms other baselines in every task. Tasks@Beijing Qwen2VL-7B Qwen2VL-72B InternVL2-8B InternVL2-26B VILA1.5-3B VILA1.5-8B VILA1.5-13B LLaMA3.2-11B LLaMA3.2-90B GPT4o-mini GPT4o GeoQA GeoQA TrajPredict Navigation STV-Address STV-Landmark STV-Outlier SAT-Address SAT-Landuse SceneComp SceneFunc ImgRetrieval CameraLoc Geo+STV Geo+Traj Geo+SAT Geo+SS 0.4950 0.5491 0.4709 0. 0.3873 0.4322 0.4410 0.4229 0.4502 0.4542 0.5479 0.0978 0.0822 0.0957 0.0756 0.0000 0.0578 0. 0.0711 0.0711 0.0844 0.0844 0.1000 0.18 0.24 0.22 0.22 0.04 0.06 0. 0.08 0.14 0.24 0.22 0.46 0.440 0.410 0.420 0.440 0.270 0.270 0. 0.280 0.295 0.280 0.405 0.91 0.755 0.785 0.755 0.755 0.655 0.650 0. 0.595 0.770 0.765 0.775 0.1200 0.5500 0.2250 0.1700 0.2650 0.3700 0.5300 / / 0.2600 0.1100 0.295 0.395 0.295 0.360 0.275 0.225 0.320 0.290 0.295 0.350 0. 0.405 0.395 0.300 0.375 0.475 0.405 0.320 0.325 0.290 0.360 0.420 0.400 0. 0.390 0.440 0.295 0.420 0.425 / / 0.465 0.450 0.355 0.310 0.340 0. 0.235 0.345 0.390 / / 0.345 0.390 0.275 0.290 0.210 0.230 0.250 0.195 0. / / 0.205 0.315 0.260 0.305 0.255 0.225 0.265 0.290 0.210 / / 0.290 0.290 0.870 +33.85% +10.83% 0.8150 +120.27% +48.18% 0.780 +246.67% +97.47% 0.72 +77.78% +51.58% 0.585 +39.29% +25.81% 0.58 +68.12% +48.72% 0.785 +302.56% +149.21% 0.62 +113.79% +103.28% UrbanLLaVA-VILA1.5-8B 0.5682 vs. VILA1.5-8B vs. Best Baseline +31.47% +73.10% +666.67% +237.04% +106.82% +3.48% +2.28% +91.67% Figure 11. Scaling law from training data size to performance. to limited computing resources, we were unable to provide results for VILA1.5-40B, which could potentially achieve significantly better performance than VILA1.5-8B. 14. Additional Case Study SAT-LandUse. This task needs model to speculate the land use type (commercial, residential, agricultural, etc.) based on satellite image. One example is shown in Figure 13. Our UrbanLLaVA can respond to this task precisely, showing the capability of correctly perceiving the given imFigure 12. Results on UrbanLLaVA with different model sizes. age, satisfactory instruction following, and urban knowledge mastering. STV-Landmark. representative example is in Figure 14. In this task, models are required to find out the closest landmark feature to given street view, which needs implicit logical reasoning capability to answer. By correctly answering STV-Landmark question, UrbanLLaVA presents its ability to conduct logical reasoning in multi-modal context. SAT-Address. This task needs model to speculate the most probable address description based on satellite image. One example is shown in Figure 15 2 Tasks@Beijing Qwen2VL-7B Qwen2VL-72B InternVL2-8B InternVL2-26B VILA1.5-3B VILA1.5-8B VILA1.5-13B LLama3.2-11B LLama3.2-90B GPT4o-mini GPT4o Tasks@Beijing Qwen2VL-7B Qwen2VL-72B InternVL2-8B InternVL2-26B VILA1.5-3B VILA1.5-8B VILA1.5-13B LLama3.2-11B LLama3.2-90B GPT4o-mini GPT4o Table 7. Main results on UBench at London. UrbanLLaVA achieves better performance than other baselines in the majority of tasks. GeoQA GeoQA TrajPredict Navigation STV-Address STV-Landmark STV-Outlier SAT-Address SAT-Landuse SceneComp SceneFunc ImgRetrieval CameraLoc Geo+STV Geo+Traj Geo+SAT Geo+SS 0.4991 0.5802 0.4973 0.5168 0.4362 0.4841 0. 0.4804 0.5659 0.5357 0.6446 0.1920 0.2245 0.1694 0.1776 0.0000 0.1367 0.1796 0.1959 0. 0.1755 0.2000 0.12 0.24 0.10 0.08 0.08 0.04 0.08 0.04 0.20 0.08 0. 0.405 0.485 0.290 0.380 0.230 0.330 0.430 0.360 0.375 0.375 0.580 0.760 0. 0.810 0.865 0.305 0.560 0.570 0.440 0.715 0.835 0.895 0.1492 0.5525 0.2431 0. 0.2320 0.4586 0.4972 / / 0.2155 0.1657 0.6851 +49.40% +24.00% 0.305 0.530 0.315 0. 0.200 0.305 0.275 0.260 0.385 0.390 0.480 0.550 0.535 0.490 0.490 0.445 0.485 0. 0.500 0.555 0.570 0.610 0.870 0.420 0.785 0.905 0.295 0.705 0.800 / / 0.855 0.900 0.220 0.265 0.315 0.305 0.200 0.335 0.390 / / 0.340 0. 0.270 0.405 0.215 0.215 0.290 0.250 0.275 / / 0.290 0.320 0.575 +88.52% +8.49% 0.750 +54.64% +22.95% 0.955 +35.46% +5.52% 0.560 +67.16% +30.23% 0.605 +142.00% +49.38% 0.285 0.245 0.265 0. 0.255 0.265 0.250 / / 0.245 0.250 0.260 -1.89% -8.77% UrbanLLaVA-VILA1.5-8B 0.6399 vs. VILA1.5-8B vs. Best Baseline 0.1959 +32.20% +43.28% +750.00% +41.67% -0.72% -12.73% 0.34 0.610 +84.85% +5.17% 0.955 +70.54% +6.70% Table 8. Main results on UBench at NewYork. UrbanLLaVA achieves better performance than other models in most tasks. GeoQA GeoQA TrajPredict Navigation STV-Address STV-Landmark STV-Outlier SAT-Address SAT-Landuse SceneComp SceneFunc ImgRetrieval CameraLoc Geo+STV Geo+Traj Geo+SAT Geo+SS 0.4567 0.5273 0.4632 0.4766 0.3954 0.4575 0. 0.4127 0.5234 0.5075 0.6232 0.1200 0.1480 0.1260 0.1080 0.0000 0.1000 0.1100 0.1000 0. 0.1240 0.1080 0.1120 0.22 0.36 0.24 0.34 0.08 0.14 0.36 0.12 0. 0.34 0.36 0.50 0.585 0.550 0.440 0.490 0.330 0.345 0.375 0.395 0. 0.550 0.740 0.920 0.805 0.795 0.780 0.805 0.745 0.680 0.765 0.645 0. 0.880 0.830 0.935 +37.50% +6.25% 0.1450 0.5550 0.2550 0.2700 0.2450 0.4700 0.5350 / / 0.2600 0.1600 0.6950 +47.87% +25.23% 0.455 0.520 0.395 0.495 0.310 0.235 0.325 0.295 0. 0.415 0.610 0.395 0.235 0.135 0.225 0.250 0.160 0.175 0.150 0.220 0.265 0. 0.885 +276.60% +45.08% 0.880 +450.00% +122.78% 0.875 0.470 0.835 0.885 0.280 0.795 0.820 / / 0.880 0.930 0.835 +5.03% -10.22% 0.315 0.290 0.305 0.290 0.245 0.315 0.290 / / 0.350 0.405 0.275 0.335 0.245 0.230 0.255 0.260 0.285 / / 0.255 0. 0.315 0.320 0.235 0.245 0.230 0.245 0.280 / / 0.215 0.275 0.490 +55.56% +20.99% 0.645 +148.08% +92.54% 0.520 +112.24% +62.50% CityGPT-V-VILA1.5-8B 0.5773 vs. VILA1.5-8B vs. Best Baseline +26.19% +12.00% +257.14% +166.67% +24.32% -7.36% -24.32% +38.89% Table 9. Evaluating generalizability of methods on Qwen2.5VL. Task Group @ Beijing GeoQA Geo+Traj Geo+STV Geo+SAT Geo+SS Qwen2.5-VL-7B-Instruct + Finetuned with UData 0.4324 0.5720 0.2192 0.1876 0.4467 0.6833 0.2850 0.4800 0.2225 0.3800 STV-Address. This task provides street view image and needs model to speculate the most probable address this image was taken. Figure 16 is an example. SceneComp. This task provides four satellite remote sensing images and prompts the model to choose the one with the most number of buildings. An instance is shown in Figure 17. ImgRetrieval. It evaluates capability to map given street view image to the corresponding satellite image. An example is displayed in Figure 18. CameraLoc. It requires the model to infer which quadrant of satellite image corresponds to the location where given street view image was captured. An example is shown in Figure 19. Figure 13. An example of the SAT-LandUse task. The correct answers from model are denoted with green color. The response from ours is in bold. Explanation is written by human for this question and answer. 15. Urban Instruction Data Table 10 provides detailed statistics of UData across three cities, while Table 11 presents the detailed statistics of the raw data used to construct UData. Additionally, we present 3 Figure 14. An example of the STV-Landmark task. The correct answers from model are denoted with green color. The response from ours is in bold. Explanation is written by human for this question and answer. Figure 17. An example of SceneComp task. Figure 15. Example of SAT-Address task. Figure 18. An example of an ImagRetrieval task. Figure 16. Example of STV-Address task. representative examples of our urban instruction data, as shown in Figure 20 to Figure 35. Figure 19. An example of CameraLoc task. 4 User: <image >n Could you provide description of the content shown in the arial image. Assistant: In the 256x256 pixel area, the road network is primarily composed of two main roads: the trunk road North Roud of Landianchang, which runs from [179, 249] to [178, 247] in southward direction, and the service road Road of Kunminghu, extending from [127, 124] to [84, 16], which travels in generally southwest direction. Additionally, there is tertiary road Road of Kunminghu that connects [131, 123] to [127, 124], running parallel to the service road, and secondary road North Roud of Landianchang that stretches from [176, 250] to [172, 248], also heading southward. The area features notable points of interest (POIs), including tickets located at [137, 175] and attractions at [88, 140]. The distribution of roads indicates concentration of infrastructure towards the southern part of the region, particularly along North Roud of Landianchang, which is flanked by various POIs, while Road of Kunminghu serves as key access route with notable presence of attractions nearby, suggesting vibrant area for visitors. Figure 20. An example of global view training instances of Image Content. 5 User: <image >n You are provided 256*256 satellite image. What is the landuse type in region [[32, 136], [33, 167], [39, 167], [101, 133], [99, 127], [90, 131], [86, 121], [88, 114], [74, 111], [32, 136]]? Assistant: Residential Figure 21. An example of global view training instances of Landuse Inference. User: <image >n What is the detailed address of the provided street view image? Assistant: Tsinghuayuan, Xueyuan Road, Dongsheng, Haidian District, Beijing, 100190, China Figure 22. An example of local view training instances of Location Address. User: <image >n What is shown in this street view image? And what related pois could be estimated nearby Assistant: The image depicts building partially obscured by lush greenery. The structure appears to be single-story building with flat roof, characterized by light-colored exterior that suggests concrete or stucco material. The roof is lined with blue trim, which adds subtle contrast to the overall appearance. Surrounding the building, dense foliage and various plants create vibrant green backdrop, indicating well-established natural environment. The vegetation is thick, with mix of taller trees and lower shrubs, suggesting sense of seclusion. The building is positioned towards the center of the image, with trees flanking its sides, while pathway or road runs along the left side, leading towards the viewer. The spatial relationship between the building and the surrounding greenery is notable; the building is somewhat hidden by the dense plants, which extend close to its walls, creating harmonious blend of man-made and natural elements. The sunlight filters through the leaves, casting dappled shadows on the ground, enhancing the tranquil atmosphere of the scene. Overall, the image conveys sense of quietude and integration with nature. Figure 23. An example of local view training instances of Image Description. User: <image >n What is the landmark in this image? If there is no landmark, please say None. Assistant: In the street view image, notable landmark is visible in the background: pagoda, which is likely part of the nearby Changchun Garden. The pagoda stands tall among the trees, providing distinctive silhouette against the sky, and serves as navigational reference point for the surrounding area. The road curves gently around the landmark, with vehicles traveling in both directions, while the nearby buildings, including educational institutions like the Medical College and Pharmacy College, create backdrop that emphasizes the pagodas prominence in the landscape. This combination of natural and architectural features highlights the cultural significance of the pagoda within the urban environment Figure 24. An example of local view training instances of Landmark Details. 7 User: You are given satellite image <image>and street view image <image>, can you predict which quadrant the street view image lies in the satellite image? A. Top left B. Top right C. Bottom left D. Bottom right Please provide detailed reasoning for your answer. And give me one letter as your answer in the last sentence. Assistant: Step 1: Identify the satellite view images address: The satellite image is taken in the Chaoyang District of Beijing, featuring well-organized 5x5 grid of notable locations. To the north is the Olympic Village Science Park, central site surrounded by various establishments. Directly east along Datun Road is the National Zoological Museum of China, enhancing the areas scientific theme. Moving south on Datun Road, youll find China Unicom, key telecommunications provider, and the vibrant The Red Parrot Image Agency, adding cultural flair. Datun Road connects these points, facilitating easy navigation. This area blends educational, cultural, and commercial establishments, all within close proximity. Step 2: Identify the street view images address: The street view image is taken at The Red Parrot Image Agency, # 50, Datun Road, Chaoyang District, Beijing, 100101, China. Step 3: Determine the quadrant where the street view image lies in the satellite view image: The street view image is located in quadrant of the satellite view image. Figure 25. An example of local view training instances of Cross Modality Reasoning. 8 Assistant: You are given one street view image <image>and four satellite images. Which satellite image shows the surroundings of the street view image? A. The first image <image> B. The second image <image> C. The third image <image> D. The fourth image <image> Please provide detailed reasoning for your answer. And give me one letter as your answer in the last sentence. Assistant: Step 1: Describe the street view image: The image shows parking area with various vehicles parked in semi-organized manner. In the foreground, cars of different colors, including silver, black, and white, are parked closely on paved surface made of large rectangular tiles, indicating moderate occupancy. To the right, prominent blue fence encloses structure, likely building or series of buildings, contrasting with nearby lush trees. The background features multi-storied buildings made of brick and concrete, with visible windows, suggesting residential or commercial use. The parking area is in front of these buildings, with cars facing the blue enclosure. The short distance between the cars and the fence creates compact environment. The clear sky suggests bright day, enhancing visibility. Step 2: Recall the address of the street view image: The image is taken at the School of Medicine, Jinchun Road, Qinghuayuan, Xiwacun, Haidian District, Beijing, 100084, China. Step 3: Recall the addresses of each possible satellite view image: ... Step 4: Reason the corresponding satellite view image: The satellite view image corresponding to the street view image is Figure 26. An example of global view training instances of Cross Modality Reasoning. 9 User: <image >n What is the address of this location? Please think step by step and provide detailed reasoning. Assistant: Step 1: Identify the city name. According to the satellite view image, this image was taken in Beijing. Step 2: Extract the locations features. From the image, can observe the following features: In the 256x256 pixel area, the roads are primarily oriented in north-south and east-west direction, with significant intersections facilitating access to various points of interest (POIs). Notably, the coordinates [116, 40] host both research institutes and universities, indicating concentration of educational and research facilities in this region. These POIs are strategically located alongside the main north-south road that runs through the area, enhancing their accessibility for students and researchers alike. The distribution of roads in this region is fairly balanced, with few major thoroughfares connecting key locations. The concentration of specific POIs, particularly educational institutions, suggests that this area serves as hub for academic and research activities. Overall, the layout of roads and the placement of POIs create vibrant environment conducive to learning and innovation. Step 3: Identify the locations address. Based on my observation and knowledge of this region, the area is situated in Haidian District, Beijing, characterized by 5x5 layout that features several prominent institutions. To the north, the Institute of Semiconductors, Chinese Academy of Sciences, occupies multiple locations, creating significant presence in the region. Directly to the south, Beijing Forestry University is also located, providing an educational backdrop to the area. The main thoroughfare, Beilin Ginkgo Avenue, runs through the region, connecting these institutions and facilitating easy access. The Xueyuanlu Subdistrict, where this area is found, enhances the accessibility of the institutions, with Bajia Village nearby, further enriching the local community. The proximity of these educational and research institutions fosters collaborative environment, making this area hub for academic and scientific endeavors. Figure 27. An example of global view training instances of Image Content. 10 User: <image >n Please count the number of buildings in this satellite image and provide detailed reasoning for your answer. Assistant: Step 1: Identify the city name: According to the satellite view image, this image was likely taken in Beijing. Step 2: Describe the locations features: From the image, can observe the following features: In the 256x256 pixel area, the roads are arranged to facilitate access to various points of interest (POIs) and ensure smooth traffic flow. The tertiary road, Tianchen West Road, runs vertically from [137, 188] to [137, 171], providing north-south route that connects to nearby POIs, including cars located at [47, 155]. The secondary road, Datun Road, extends horizontally from [174, 211] to [221, 209], serving as major thoroughfare likely supporting higher traffic volumes and is near cafes located at [55, 148] and [61, 142]. Additionally, another segment of the tertiary road, Datun Road, stretches from [162, 187] to [0, 193], indicating more extensive network that enhances connectivity across the region. The concentration of cafes near the secondary road suggests vibrant social scene, while the presence of cars indicates active transportation throughout the area, particularly along these key roads. Overall, the layout of the roads and the distribution of POIs create dynamic environment that caters to both vehicular movement and local amenities. Step 3: Provide the locations address: Based on my observation and knowledge of this region, the area described is located in the Chaoyang District of Beijing, characterized by well-organized 5x5 layout. To the north, you will find Beichen West Road, prominent thoroughfare, while to the south, Datun North Road runs parallel, providing easy access to the surrounding locations. The eastern section is defined by Tianchen West Road, which intersects with various points of interest, including the Beichen Century Center, situated at the northwest corner. This center is flanked by additional buildings along Beichen West Road, creating bustling environment. Adjacent to these landmarks, Datun Road runs through the area, connecting several establishments and enhancing the regions accessibility. The overall layout ensures that each location is conveniently reachable, fostering vibrant community atmosphere. Step 4: Count the number of buildings or POIs: There are 5 buildings in this image. Figure 28. An example of global view training instances of Cross Modality Reasoning. 11 User: <image >n What is the address of this location? Please think step by step and provide detailed reasoning. Assistant: Step 1: Describe the street view image: This is street view image depicting city street scene with wide, well-maintained road extending into the distance. On the left side, prominent orange guardrail runs parallel to the road, suggesting separation from lower area, possibly sidewalk or bus stop. The road is dark asphalt, reflecting the bright sunlight, indicating clear day. To the right, series of buildings are visible, showcasing mix of architectural styles. The building closest to the viewer is painted in muted red hue and features several air conditioning units mounted on its exterior walls. This building appears to be several stories tall, with large windows that allow for natural light. Further along, taller, modern glass building stands out, its reflective surface capturing the sunlight and contrasting with the surrounding structures. This building is flanked by additional high-rise buildings in the background, creating skyline that suggests bustling urban environment. In the distance, road signs are visible, indicating directions or information for drivers, while the road continues to stretch ahead, leading towards the horizon. The overall scene conveys sense of an active city, with combination of modern architecture and well-maintained infrastructure. The clear blue sky above enhances the vibrant atmosphere of the setting. Step 2: Tell the city name: According to the street view image, this is probably in Beijing. Step 3: Extract the locations features around: The street view image is taken in region with the following features: Beijing Haidian District Vocational School, Haidian District Vocational Technical School East Campus, Hongyun Laobeiping, Khan Baba, Mi Da Wan, and the Affiliated High School of China University of Geosciences. Step 4: Tell the locations address: Based on my observation and knowledge about this region, the address is Xueyuanlu Subdistrict, Dongsheng, Haidian District, Beijing, 100083, China. Figure 29. An example of local view training instances of Cross Modality Reasoning. 12 User: <image >n The following is multiple-choice question about selecting the most appropriate address for satellite image. A. The area is characterized by well-organized layout, featuring 5x5 grid that offers variety of attractions and roadways. To the north, the prominent Yuan Dynasty Capital City Wall Ruins Park serves as significant landmark, providing historical backdrop and recreational space. Adjacent to this park, Beitou City West Road runs east to west, connecting various points of interest within the vicinity. To the east ... B. The area is situated in the Huayuanlu Subdistrict of Haidian District, Beijing, characterized by well-defined square layout divided into 5x5 arrangement. To the north, the region is bordered by series of residential complexes, including the prominent Beiji Temple Residential Area, which occupies several positions within the grid. To the south ... C. The area is characterized by well-organized 5x5 layout, with the northern section primarily featuring Wanquanhe Road, which runs through the heart of the neighborhood, providing easy access to various points of interest. To the east, ... D. The area is located in the Huayuanlu Subdistrict of Haidian District, Beijing, characterized by well-defined square layout that is predominantly residential. To the north, you will find cluster of residential buildings known as Beiji Temple Residential Area, which extends down the western side of the area. Moving eastward, ... Please choose the most suitable one among A, B, and as the answer to this question. Please output the option directly. No need for explaination Assistant: Figure 30. An example of global view training instances of Location Address. 13 User: <image >n The following is multiple-choice question about selecting the most possible landuse type in the region of satellite image. A. Industrial B. Residential C. Meadow D. Forest Please choose the most suitable one among A, B, and as the answer to this question. Please output the option directly. No need for explaination. Assistant: Figure 31. An example of global view training instances of Landuse Inference. 14 User: <image >n The following is multiple-choice question about selecting the most appropriate address for street view image. A. G6 Auxiliary Road, Chaoyang District, Beijing, 100000, China B. Old Summer Palace Park, 28, Qinghua West Road, Yanyuan, Haidian Town, Haidian District, Beijing, 100084, China C. Piaoliang Sunshine Plaza, Datun, Chaoyang District, Beijing, 100012, China D. G6 Auxiliary Road, Xueyuanlu Subdistrict, Tayuan Village, Haidian District, Beijing, 100191, China Please choose the most suitable one among A, B, and as the answer to this question. Please output the option directly. No need for explaination. Assistant: Figure 32. An example of local view training instances of Location Address. User: <image >n The following is multiple-choice question about selecting the most possible nearby POIs(Place of Interests) or landmarks description in the region of street view image. A. Unpaved rural road. B. Metal fence or wall. C. Not identifiable. D. Residential apartment complex. Please choose the most suitable one among A, B, and as the answer to this question. Please output the option directly. No need for explaination. Assistant: Figure 33. An example of local view training instances of Landmark Details. User: You are given satellite image <image>and street view image <image>, and please predict which quadrant the street view image lies in the satellite image. A. Top left B. Top right C. Bottom left D. Bottom right Only provide one letter as the answer and please select your answer from A, B, C, or D. ASSITANT: Figure 34. An example of global view training instances of Cross View Data. 16 User: You are given one street view image <image>and four satellite images. Which satellite image shows the surroundings of the street view image? A. The first image <image> B. The second image <image> C. The third image <image> D. The fourth image <image> Only provide one letter as the answer and please select your answer from A, B, C, or D. Assistant: Figure 35. An example of global view training instances of Multiple SAT Comparison. (a) Beijing (b) London (c) New York Figure 36. Maps for Beijing, London and New York. Table 10. Basic information of UData on three cities."
        },
        {
            "title": "City",
            "content": "/"
        },
        {
            "title": "General",
            "content": "ShareGPT,UltraChat,Open-Platypus 19866 3."
        },
        {
            "title": "CityQA\nLocation Address\nLandmark Details\nImage Description\nCross Modality Reasoning",
            "content": "Random Walk Real-World Trajectory Visual Random Walk Vision-Language Navigation Image Content Location Address Landuse Inference Multiple SAT Comparison Cross-View Data Cross Modality Reasoning"
        },
        {
            "title": "CityQA\nLocation Address\nLandmark Details\nImage Description\nCross Modality Reasoning",
            "content": "Random Walk Real-World Trajectory Visual Random Walk Vision-Language Navigation Image Content Location Address Landuse Inference Multiple SAT Comparison Cross-View Data Cross Modality Reasoning"
        },
        {
            "title": "CityQA\nLocation Address\nLandmark Details\nImage Description\nCross Modality Reasoning",
            "content": "Random Walk Real-World Trajectory Visual Random Walk Vision-Language Navigation Image Content Location Address Landuse Inference Multiple SAT Comparison Cross-View Data Cross Modality Reasoning 19271 93246 51130 28798 2000 9001 98 8936 3000 9315 2777 3642 10114 77204 14977 28934 2172 2372 716 16524 98 13412 3000 3853 882 4332 4500 2172 5758 25413 94886 50404 24529 2012 12277 98 12229 3000 18368 5113 17899 22020 94886 23603 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Table 11. The raw data of the selected region in three cities."
        },
        {
            "title": "AoIs PoIs Roads Trajectory Street View Image Satellite Image",
            "content": "Beijing London New York 19541 11112 4647 1882 2320 13705 11715 1322 522 21015 173268 390934 28798 3125 24444 1533 556"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China",
        "School of Electronic and Information Engineering, Beijing Jiaotong University, China",
        "University of Helsinki, Finland"
    ]
}