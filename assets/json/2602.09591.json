{
    "paper_title": "On the Optimal Reasoning Length for RL-Trained Language Models",
    "authors": [
        "Daisuke Nohara",
        "Taishi Nakamura",
        "Rio Yokota"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 1 9 5 9 0 . 2 0 6 2 : r On the Optimal Reasoning Length for RL-Trained Language Models ON THE OPTIMAL REASONING LENGTH FOR RLTRAINED LANGUAGE MODELS Daisuke Nohara1, Taishi Nakamura1, Rio Yokota1 1Institute of Science Tokyo {nohara,rioyokota}@rio.scrc.iir.isct.ac.jp"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks through reinforcement learning (RL) applied during post-training, which enables extended chain-ofthought (CoT) reasoning (OpenAI, 2024; DeepSeek-AI, 2025; Wei et al., 2022). However, longer outputs increase computational costs during both training and inference, motivating length control methods that penalize verbose generations (Arora & Zanette, 2025; Xiang et al., 2025; Li et al., 2025a; Shrivastava et al., 2025). Yet the relationship between output length and the efficiencyperformance tradeoff remains unclear. In this work, we investigate whether the optimal length control strategy depends on the models pre-existing reasoning capabilities. We compare multiple length control methods (RLOO-LP, ALP, and DRPO) across Qwen3-1.7B-Base and DeepSeek-R1-DistillQwen-1.5B, which differ in their initial reasoning abilities. Our main findings are: The relationship between output length and performance differs qualitatively across models: Qwen3-1.7B-Base shows monotonically increasing trend, while DeepSeek-R1-Distill exhibits non-monotonic relationship with an optimal intermediate length. Extending the framework of Ghosal et al. (2025) to RL-trained policies, we find that long outputs degrade performance through increased dispersion, while short outputs suffer from under-thinking. These results suggest that length penalties can be harmful during reasoning acquisition, whereas models with pre-existing capabilities benefit from appropriate length control."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent reasoning models have demonstrated that reinforcement learning can substantially improve reasoning capabilities in LLMs (OpenAI, 2024; DeepSeek-AI, 2025), with methods such as GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025). While extended chain-of-thought reasoning improves performance on challenging tasks, it also incurs substantial computational costs. Recent studies have observed that RL-trained models tend to 1 On the Optimal Reasoning Length for RL-Trained Language Models produce increasingly longer outputs, phenomenon often referred to as overthinking (Chen et al., 2025). This has motivated the development of various length control methods. Several approaches apply length-based reward shaping, including RLOO-LP (Arora & Zanette, 2025) and ALP (Xiang et al., 2025), which penalize longer correct responses with different normalization strategies. Alternative approaches avoid explicit reward shaping: DRPO (Li et al., 2025a) uses length-based weighting within the DisCO framework (Li et al., 2025b), while GFPO (Shrivastava et al., 2025) filters samples based on length. However, how output length affects the efficiency-performance tradeoff, and whether an optimal length regime exists, remains underexplored. Ghosal et al. (2025) provide theoretical analysis showing that output length and performance can exhibit non-monotonic relationship. Their analysis focuses on test-time interventions, and how length control methods affect RL training dynamics remains an open question."
        },
        {
            "title": "3.1 EXPERIMENTAL SETTING",
            "content": "Training setup We conduct experiments on two models: Qwen3-1.7B-Base (Qwen Team, 2025) and DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025). Our training configuration follows DAPO (Yu et al., 2025) with two modifications. First, we disable overlong reward shaping, as length penalty methods are explored separately in our experiments. Second, we set the PPO mini batch size equal to the generation batch size, ensuring fully on-policy updates. We set the maximum response length to 8K tokens for Qwen3-1.7B-Base and 16K tokens for DeepSeek-R1-Distill-Qwen-1.5B, chosen such that fewer than 5% of rollouts exceed the cap at the start of training. Training is performed for approximately 576 GPU-hours (8 GPUs 72 hours). We report results at 640 steps for Qwen3-1.7B-Base and 480 steps for DeepSeek-R1-Distill-Qwen-1.5B. For numerical precision, we use BF16 with truncated importance sampling (TIS) (Yao et al., 2025) for Qwen3-1.7B-Base and FP16 without TIS for DeepSeek-R1-Distill-Qwen-1.5B. Evaluation setup We evaluate on four mathematical reasoning benchmarks: AIME 2024, AIME 2025, AMC, and MATH-500. Following the evaluation protocol of DeepSeek-R1 (DeepSeek-AI, 2025), we sample multiple responses per problem and report the mean accuracy. Further details of the training and evaluation configurations, along with the rationale for these choices, are provided in Appendix A. Length Control Methods We evaluate several methods for length control in RL training. As baselines, we consider Sample Avg (the original GRPO objective (Shao et al., 2024)) and Token Avg (the DAPO objective (Yu et al., 2025)), which differ in their loss normalization strategies. For explicit length penalties, we evaluate RLOO-LP (Arora & Zanette, 2025), ALP (Xiang et al., 2025), and DRPO (Li et al., 2025a). We also attempted to evaluate GFPO (Shrivastava et al., 2025) but were unable to reproduce the length reduction effect. Further details of each method, including our GFPO reproduction attempt, are provided in Appendix B. 3.2 RESULTS 3.2.1 LENGTH-PERFORMANCE RELATIONSHIP We present the relationship between output length and accuracy in Figure 1. Our key finding is that the two models exhibit qualitatively different patterns. While Qwen3-1.7B-Base shows monotonically increasing trend where longer outputs generally lead to higher accuracy, DeepSeek-R1-Distill exhibits non-monotonic relationship with an optimal output length that maximizes performance. For Qwen3-1.7B-Base, stronger length penalties consistently lead to performance degradation. For DeepSeek-R1-Distill, the best performance is achieved with moderate length penalties, indicating that both under-thinking and over-thinking hurt performance (Figure 1, bottom). We verify that this non-monotonicity is not an artifact of training compute or evaluation truncation (Appendix D.1). 2 On the Optimal Reasoning Length for RL-Trained Language Models Figure 1: Score vs. average output length. Qwen3-1.7B-Base (top) shows monotonically increasing trend, while DeepSeek-R1-Distill-Qwen-1.5B (bottom) exhibits non-monotonic relationship with optimal performance at intermediate lengths. We also observe that Sample Avg (GRPO) and DRPO perform poorly on Qwen3-1.7B-Base compared to other methods, while achieving comparable performance on DeepSeek-R1-Distill (Appendix C). We hypothesize that this qualitative difference stems from their reasoning capabilities at the start of RL training: Qwen3-1.7B-Base must acquire reasoning patterns through longer exploratory outputs, whereas DeepSeek-R1-Distill already possesses these capabilities from distillation. 3.2.2 UNDERSTANDING THE NON-MONOTONIC TREND Recent work by Ghosal et al. (2025) provides theoretical framework for understanding nonmonotonic length-performance relationships in reasoning models. They analyze simplified onedimensional model where the policy follows Gaussian distribution π(yx) = (µπ, σ2 π) and the reward function is r(y) = (µr, σ2 ). Under this stylized model, the expected reward exhibits non-monotonic relationship with policy variance σ2 π: small increases in variance improve reward through better coverage of the reward peak, while excessive variance dilutes probability mass away from it. They empirically validate this by measuring the entropy of output distributions under testtime interventions such as appending Wait tokens, finding that entropy increases with extended thinking and correlates with performance degradation beyond an optimal point. We investigate whether similar phenomenon explains the non-monotonic relationship observed in our post-RL policies. However, our setting differs from theirs in crucial way. While Ghosal et al. (2025) analyze fixed policy under test-time intervention, we compare different policies trained with varying length penalties. Since RL training updates the policy parameters, different training configurations can shift not only the variance but also the central tendency of the output distribution. We therefore extend their analysis, which uses entropy as the primary measure of dispersion, to separately quantify both aspects. Specifically, we distinguish two sources of error: the distribution may be centered on incorrect answers, or it may be spread across multiple answers. To quantify these, we compute three metrics from the sampled responses for each problem. Mode accuracy measures whether the most frequent answer is correct, capturing how far the distributions center is from the correct answer. Answer entropy and mode share (the fraction of responses matching the mode) capture dispersion, measuring how spread out or concentrated the distribution is. Figure 2 shows these metrics alongside overall accuracy as function of output length for DeepSeekR1-Distill-Qwen-1.5B on AMC and MATH-500. Results for all benchmarks are provided in ApOn the Optimal Reasoning Length for RL-Trained Language Models Figure 2: Decomposition of accuracy into mode accuracy and dispersion metrics for DeepSeekR1-Distill-Qwen-1.5B on AMC and MATH-500. In the long-output regime, degradation is driven by increased dispersion; in the short-output regime, both central tendency and dispersion are affected. pendix D.2. In the long-output regime beyond the performance peak, we observe that mode accuracy remains stable or even improves, yet overall accuracy declines. This decline coincides with increased entropy and decreased mode share, indicating that the distributions center moves closer to the correct answer while simultaneously becoming more dispersed. This pattern is consistent with the variance-based explanation of Ghosal et al. (2025). While their analysis focuses on testtime interventions that artificially extend generation through Wait tokens, our results show that the same dispersion-driven degradation occurs in standard generation from RL-trained policies. The short-output regime presents different pattern. Here, mode accuracy and mode share are low while entropy is high, suggesting that the distribution is both centered away from correct answers and highly dispersed. We hypothesize that overly short generations lead to under-thinking, where insufficient reasoning steps prevent the model from consistently arriving at correct answers. These observations suggest that length control involves trade-off. Longer outputs risk increased dispersion, while shorter outputs risk shifting the distribution away from correct answers. In our experiments, methods without length penalties such as DAPO and GRPO produce outputs in the long-output regime where dispersion-driven degradation is observed, suggesting that RL training without length control often results in policies that fall into this suboptimal region. On the other hand, we observe that mode accuracy generally increases with output length across different length penalty configurations. This is consistent with the view that stronger length penalties alter the optimization direction, prioritizing brevity over correctness and thereby shifting the distributions center away from correct answers. The goal should therefore be to find length regime that maintains the distributions center on correct answers while keeping dispersion low."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We investigated the effect of length control methods on reasoning RL and found that the relationship between output length and performance can be either monotonic or non-monotonic depending on the model. We identified two failure modes. Long outputs increase dispersion, while short outputs lead to under-thinking. Notably, the dispersion-driven degradation, previously observed in test-time interventions, also occurs in RL-trained policies. These findings suggest that length control should be tuned to avoid both under-thinking from overly short outputs and dispersion from overly long outputs. Our study is limited to two models on mathematical reasoning tasks. Additionally, optimal hyperparameters vary across methods, requiring careful tuning. promising direction for future work is 4 On the Optimal Reasoning Length for RL-Trained Language Models developing methods that automatically find the optimal length regime without manual hyperparameter search. ACKNOWLEDGEMENTS This work was supported by JST Program Japan Grant Number JPMJKP24C3. This work used computational resources TSUBAME4.0 supercomputer provided by Institute of Science Tokyo through the HPCI System Research Project (Project ID: hp250181). We thank Masaki Kawamura for helpful discussions on the experiments and feedback on the manuscript. 5 On the Optimal Reasoning Length for RL-Trained Language Models"
        },
        {
            "title": "REFERENCES",
            "content": "Daman Arora and Andrea Zanette. Training language models to reason efficiently. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do NOT think that much for 2+3=? on the overthinking of long reasoning models. In Forty-second International Conference on Machine Learning, 2025. DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, and Amrit Singh Bedi. Does thinking more always help? mirage of test-time scaling in reasoning models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview. net/forum?id=tKPqbamNb9. Gang Li, Yan Chen, Ming Lin, and Tianbao Yang. Drpo: Efficient reasoning via decoupled reward policy optimization, 2025a. URL https://arxiv.org/abs/2510.04474. Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, and Tianbao Yang. DisCO: Reinforcing large reasoning models with discriminative constrained optimization. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. URL https://openreview. net/forum?id=zzUXS4f91r. OpenAI. Openai o1 system card. arXiv:2412.16720, 2024. Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Defeating the training-inference mismatch via fp16, 2025. URL https://arxiv.org/abs/ 2510.26788. Qwen Team. Qwen3 technical report. arXiv:2505.09388, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. Vaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Behl, and Dimitris Papailiopoulos. Sample more to think less: Group filtered policy optimization for concise reasoning, 2025. URL https://arxiv.org/abs/2508.09726. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Admodels. vances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J. Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber. Just enough thinking: Efficient reasoning with adaptive length penalties reinforcement learning, 2025. URL https://arxiv.org/abs/2506.05256. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https: //fengyao.notion.site/off-policy-rl. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, YuYue, Weinan Dai, Tiantian Fan, Gaohong Liu, Juncai Liu, LingJun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Ru Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source LLM reinforcement learning system at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview. net/forum?id=2a36EMSSTp. 6 On the Optimal Reasoning Length for RL-Trained Language Models"
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 TRAINING All models are trained on the DAPO-Math-17K dataset using the verl framework. Our configuration adopts GRPO with the KL penalty removed (β = 0), computing group-relative advantages by normalizing rewards across sampled responses per prompt (subtracting mean and dividing by standard deviation). DAPO-style dynamic sampling is enabled to filter out prompts with zero gradient signal. For optimization, we use AdamW with learning rate of 1 106, weight decay of 0.1, and 10 warmup steps. We use prompt batch size of 64, sampling 16 responses per prompt, with generation temperature 1.0. The maximum response length limits (8K for Qwen3-1.7B-Base, 16K for DeepSeek-R1-DistillQwen-1.5B) are chosen such that fewer than 5% of rollouts exceed the cap at the start of training. The reported step counts (640 and 480, respectively) correspond to common wall-clock cutoff: under our 8-GPU setup, the slowest run reaches these steps within the allotted 72-hour training time. We use FSDP for distributed training and vLLM for rollout generation. A.2 EVALUATION We use the same prompt template as the training dataset to ensure consistency between training and evaluation. We sample 64 responses per problem for AIME 2024, AIME 2025, and AMC, which contain fewer problems, and 16 responses per problem for MATH-500. Following DeepSeekR1 (DeepSeek-AI, 2025), we set the temperature to 0.6 and top-p to 0.95, with context size of 32K tokens. Evaluation is conducted using the EvalScope framework with vLLM for efficient batch inference. A.3 BATCH SIZE CONFIGURATION We explain our choice of setting the mini-batch size equal to the prompt batch size, which eliminates off-policy updates during training. Initially, we followed DAPOs default configuration with generation prompt batch size of 512 and PPO mini-batch size of 32 (using BF16 with truncated importance sampling). However, under this 512/32 setting, we observed that the response length began to decrease during training, accompanied by decline in validation scores on MATH-500 and AIME 2024 (Figure 3). To address this issue, we set the generation prompt batch size equal to the PPO mini-batch size, thereby eliminating off-policy updates. With this configuration, the response length consistently increased throughout training, and the validation performance remained stable. We also measured the absolute difference between token probabilities computed by the rollout engine and the training engine. As shown in Figure 3 (top right), this difference increased substantially under the 512/32 setting, suggesting that the mismatch between rollout and training policies contributes to the instability. It is known that numerical differences between the rollout engine and training engine can cause training instability. Our results suggest that the probability difference increases substantially when multiple gradient updates are performed per rollout batch. We chose batch size of 64 rather than 32 to maximize rollout efficiency while maintaining training stability. We confirmed empirically that this setting does not exhibit the instability observed with the 512/32 configuration. A.4 PRECISION AND TRUNCATED IMPORTANCE SAMPLING We describe our choice of numerical precision and truncated importance sampling (TIS) for each model. For Qwen3-1.7B-Base, training in BF16 with TIS successfully mitigates the mismatch between the rollout and training engines, allowing stable training throughout. All experiments on this model completed without instability, so we did not explore FP16 configurations due to computational resource constraints. 7 On the Optimal Reasoning Length for RL-Trained Language Models Figure 3: Effect of batch size configuration on training dynamics. Top left: response length during training. Top right: absolute difference between rollout and training token probabilities. Bottom: validation scores on MATH-500 and AIME 2024. The 512/32 setting (generation batch size 512, mini-batch size 32) leads to decreasing response length and validation performance, while the 64/64 setting maintains stable training. For DeepSeek-R1-Distill-Qwen-1.5B, we initially attempted to train in BF16 with TIS, but encountered training instability despite using TIS. Following Qi et al. (2025), we switched to FP16, which substantially reduced the probability difference between the rollout and training engines (Figure 4) and enabled stable training. We conducted an ablation comparing FP16 with and without TIS, and found no noticeable difference in probability difference. For simplicity, we chose to train without TIS in FP16. In our However, we note that training instability can still occur even with these configurations. length penalty experiments, ALP with β = 1e4 in FP16 without TIS exhibited instability: the probability difference temporarily spiked to large value, causing training to fail. Although this difference is smaller than that observed between BF16 and FP16, it was sufficient to destabilize training. We restarted training from scratch with the same configuration, and the second run completed successfully, which we report as our result. We observed that BF16 with TIS can be unstable depending on the model; for DeepSeek-R1-DistillQwen-1.5B, training diverged in both of two independent runs. FP16 substantially reduces the probability difference, but divergence can still occur in rare cases. The instability observed with ALP does not occur consistently across runs."
        },
        {
            "title": "B DETAILS OF LENGTH CONTROL METHODS",
            "content": "B.1 RLOO-LP RLOO-LP (Arora & Zanette, 2025) applies length-based reward shaping to correct responses with per-prompt normalization with RLOO (REINFORCE Leave-One-Out) advantage estimation. RLOO samples responses {y1, . . . , yn} for prompt and estimates the advantage for each response yi as: ˆA(yi, x) = R(yi, x) 1 1 (cid:88) j=i R(yj, x) (1) 8 On the Optimal Reasoning Length for RL-Trained Language Models Figure 4: Effect of precision and TIS on training dynamics for DeepSeek-R1-Distill-Qwen1.5B. Comparison of BF16 with TIS, FP16 with TIS, FP16 without TIS, and ALP (β = 1e4) in FP16 without TIS. Note that the maximum response length differs between the ALP experiment and the precision ablation experiments, so response lengths should not be directly compared across these settings. Arora & Zanette (2025) define reward function that applies length penalty to correct responses: R(x, y) = 1{y = y(x)} (1 α (len(y))) (2) where α [0, 1) is hyperparameter controlling the penalty strength. The normalization function () is defined as: (len(y)) = σ (cid:18) len(y) mean(x) std(x) (cid:19) (3) where mean(x) and std(x) are the mean and standard deviation of correct response lengths for prompt x, estimated online during training. This per-prompt normalization prevents excessively penalizing longer reasoning required for difficult problems. When α = 0, the reward function reduces to the standard RLVR setting. Larger α values more strongly favor shorter responses. B.2 ALP Adaptive Length Penalty (ALP) Xiang et al. (2025) adjusts the length penalty strength based on per-prompt accuracy. The reward function is defined as: R(x, y) = 1{y = y(x)} β len(y) max (cid:18) acc(x), (cid:19) 1 (4) where acc(x) is the online accuracy for prompt estimated during training, is the group size (number of samples per prompt), and β is hyperparameter controlling the overall penalty strength. For high-accuracy (easy) problems, the length penalty is stronger, while for low-accuracy (difficult) problems, the penalty is weaker. The max(acc(x), 1/G) term ensures minimum penalty is applied even when accuracy is zero. 9 On the Optimal Reasoning Length for RL-Trained Language Models B.3 DRPO DRPO (Decoupled Reward Policy Optimization) (Li et al., 2025a) assigns length-based weights to correct responses within the DisCO framework (Li et al., 2025b), ensuring that learning signals remain positive regardless of response length. DisCO Objective. DisCO defines score function as the average log-likelihood of response: sθ(o, q) = 1 o (cid:88) t=1 log πθ(otq, o<t) The DisCO objective is: EoCq sθ(o, q) τ log Eq max θ (cid:18) sθ(o, q) τ (cid:19) (cid:88) exp oWq (5) (6) where Cq is the set of correct responses for prompt q, Wq is the set of incorrect responses, and τ is temperature parameter. DRPO Objective. DRPO introduces length-based weights for correct responses: ω(o) = exp (cid:19) (cid:18) 1 o/C λ (7) where is the maximum response length and λ > 0 is regularization parameter. Shorter responses receive higher weights. The DRPO objective is: Eq max θ (cid:80) oCq (cid:80) ω(o) sθ(o, q) ω(o) oCq τ log (cid:88) exp oWq (cid:18) sθ(o, q) τ (cid:19) (8) Since the weights are normalized only among correct responses, the learning signal for correct responses remains positive regardless of length. Smaller λ more strongly favors shorter responses, while λ recovers the DisCO objective. B.4 GFPO GFPO (Group Filtered Policy Optimization) (Shrivastava et al., 2025) curbs length inflation by sampling larger groups per problem and filtering responses before training. For each prompt x, GFPO samples responses and retains only the shortest correct responses for policy updates. Let Cx = {yi : yi = y(x)} denote the set of correct responses for prompt x. The filtered set Fx Cx is constructed by selecting responses with the smallest length. The policy is then updated using only responses in Fx, following the standard GRPO objective. B.4.1 GFPO REPRODUCTION ATTEMPT We attempted to evaluate the length-based filtering variant of GFPO (Shrivastava et al., 2025) in our experimental setup. Specifically, we implemented the simplest configuration: sampling = 16 responses per prompt and selecting only the = 8 shortest outputs for training. We applied this method to both Qwen3-1.7B-Base and DeepSeek-R1-Distill-Qwen-1.5B. However, we were unable to reproduce the length reduction effect reported in the original work. As shown in Figure 5, the average output length on the training dataset increased throughout training for both models, particularly in the later stages, compared to the DAPO baseline without any length penalty. This effect was even more pronounced on the validation dataset. We note that GFPO appears to produce shorter outputs than DAPO in early training in Figure 5. However, this is an artifact of our logging implementation, which computes the average length after filtering for the shortest outputs rather than across all sampled responses. 10 On the Optimal Reasoning Length for RL-Trained Language Models We hypothesize that GFPOs filtering mechanism, which excludes longer outputs from training, also prevents the model from learning from long incorrect responses. In settings where explicitly penalizing verbose failures is necessary to control output length, GFPO may not be effective. Due to this inability to reproduce the expected behavior, we exclude GFPO from our main experimental comparisons. Figure 5: GFPO reproduction attempt on two base models. Average output length during training for GFPO (G = 16, = 8) compared to the DAPO baseline. Both Qwen3-1.7B-Base and DeepSeek-R1-Distill-Qwen-1.5B show increasing output length under GFPO, particularly in later training stages."
        },
        {
            "title": "C GRPO AND DAPO OBJECTIVES",
            "content": "Group Relative Policy Optimization (GRPO) (Shao et al., 2024) eliminates the need for separate value model by computing group-relative advantages from multiple sampled responses per prompt. GRPO normalizes the loss by response length for each sample: JGRPO(θ) = 1 (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:16) min ρi,t ˆAi,t, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) (9) where is the number of responses per prompt and yi is the length of response i. DAPO (Yu et al., 2025) modifies this by normalizing by the total token count: JDAPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) (cid:16) min i=1 t= ρi,t ˆAi,t, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) (10) These normalization schemes can affect training stability depending on the variance of output lengths, as analyzed in the next section. C.1 ANALYSIS OF OUTPUT LENGTH VARIANCE We analyze why Sample Avg (GRPO) and DRPO exhibit unstable training on Qwen3-1.7B-Base compared to DeepSeek-R1-Distill-Qwen-1.5B. Figure 6 shows the evolution of average output length and length bias during training. Length bias is defined as the difference between mean lengths of correct and incorrect responses, normalized by the overall mean length. negative length bias indicates that incorrect responses tend to be longer than correct ones. On Qwen3-1.7B-Base, Sample Avg (GRPO) and DRPO exhibit large negative length bias, suggesting insufficient suppression of long incorrect responses. 11 On the Optimal Reasoning Length for RL-Trained Language Models Figure 6: Evolution of output length during training (20-step moving average). Left: average output length. Right: length bias (normalized difference between correct and incorrect response lengths). On Qwen3-1.7B-Base, Sample Avg (GRPO) and DRPO exhibit large negative length bias, indicating that incorrect responses tend to be longer than correct ones. Table 1 shows the coefficient of variation (CV) of output lengths during the first 10 training steps. We report the overall CV computed from all samples, the within-prompt CV averaged across prompts, and the between-prompt CV computed from per-prompt mean lengths. Table 1: Coefficient of variation of output lengths (Token Avg, first 10 steps). Model Overall Within-prompt Between-prompt Qwen3-1.7B-Base DeepSeek-R1-Distill-1.5B 1.53 0. 1.10 0.45 0.45 0.50 Qwen3-1.7B-Base exhibits substantially higher within-prompt CV compared to DeepSeek-R1Distill-Qwen-1.5B. In Sample Avg, each sample is weighted by the inverse of its output length 1/o; when within-prompt length variance is high, the weight differences become pronounced, leading to unstable gradient updates. DRPO exhibits similar instability because its underlying framework, DisCO, uses the same per-sample length normalization in its score function (Eq. 5). In contrast, DeepSeek-R1-Distill-Qwen-1.5B, which has been distilled from reasoning model and already generates long chain-of-thought outputs, exhibits lower within-prompt variance in output lengths. This explains why both Sample Avg and DRPO achieve stable training on this model. 12 On the Optimal Reasoning Length for RL-Trained Language Models"
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "D.1 ROBUSTNESS OF THE NON-MONOTONIC RELATIONSHIP We verify that the non-monotonic relationship observed for DeepSeek-R1-Distill-Qwen-1.5B is not explained by confounding factors. D.1.1 WALL-CLOCK TIME COMPARISON One might hypothesize that shorter outputs lead to lower performance simply because fewer tokens are processed during training, resulting in less learning signal. To test this, we compare all models at our fixed training budget of 576 GPU-hours rather than equal training steps. Figure 7 shows that the non-monotonic relationship persists: methods with strong length penalties still underperform despite having more training steps within the same time budget. This confirms that the performance degradation in the short-output regime reflects genuine under-thinking rather than insufficient training compute. Figure 7: Score vs. average output length for DeepSeek-R1-Distill-Qwen-1.5B at equal wallclock training time The non-monotonic relationship persists, indicating that performance degradation with short outputs is not due to reduced training tokens. D.1.2 EXTENDED CONTEXT LENGTH EVALUATION Another potential explanation for performance degradation at long output lengths is that responses may exceed the evaluation context limit (32K tokens), causing truncation and incorrect answers. To rule this out, we evaluate with 64K context length. Figure 8 shows the score vs. output length relationship under 64K evaluation. The non-monotonic pattern persists, confirming that the performance degradation in the long-output regime is not due to truncation. Furthermore, Figure 9 compares the truncation rates between 32K and 64K evaluation contexts across all benchmarks. The truncation rates remain similar even with the extended context length, indicating that most responses that exceed 32K tokens also exceed 64K tokens. This suggests that truncation is not the primary cause of performance degradation for long outputs. These results support our interpretation that the non-monotonic relationship reflects genuine tradeoffs between under-thinking and dispersion, rather than artifacts of the experimental setup. D.2 FULL DISPERSION ANALYSIS Figure 10 shows the mode accuracy, answer entropy, and mode share analysis for DeepSeek-R1Distill-Qwen-1.5B across all four benchmarks (AIME 2024, AIME 2025, AMC, and MATH-500). The patterns described in Section 3.2.2 are consistent across benchmarks: in the long-output regime, mode accuracy remains stable while entropy increases and mode share decreases; in the short-output regime, all metrics are degraded. 13 On the Optimal Reasoning Length for RL-Trained Language Models Figure 8: Score vs. average output length for DeepSeek-R1-Distill-Qwen-1.5B with 64K evaluation context length. The non-monotonic pattern is consistent with the 32K evaluation (Figure 1), confirming that performance degradation at long output lengths is not an artifact of response truncation. Figure 9: Truncation rate at 32K vs. 64K evaluation context length for DeepSeek-R1-DistillQwen-1.5B. Each point represents method with specific hyperparameter configuration. Points close to the diagonal indicate that extending the context length does not substantially reduce truncation. 14 On the Optimal Reasoning Length for RL-Trained Language Models Figure 10: Mode accuracy, answer entropy, and mode share vs. average output length for DeepSeek-R1-Distill-Qwen-1.5B across all benchmarks after 480 training steps. This figure extends Figure 2 to include AIME 2024 and AIME 2025."
        }
    ],
    "affiliations": [
        "Institute of Science Tokyo"
    ]
}