{
    "paper_title": "ObjectMover: Generative Object Movement with Video Prior",
    "authors": [
        "Xin Yu",
        "Tianyu Wang",
        "Soo Ye Kim",
        "Paul Guerrero",
        "Xi Chen",
        "Qing Liu",
        "Zhe Lin",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios."
        },
        {
            "title": "Start",
            "content": "ObjectMover: Generative Object Movement with Video Prior Xin Yu1, Tianyu Wang2 Soo Ye Kim2 Paul Guerrero2 Xi Chen1 Qing Liu2 Zhe Lin2 Xiaojuan Qi1, 1The University of Hong Kong 2Adobe Research https://xinyu-andy.github.io/ObjMover 5 2 0 M 1 1 ] . [ 1 7 3 0 8 0 . 3 0 5 2 : r Figure 1. Results of object movement. We demonstrate the object movement capability of ObjectMover in variety of complex and challenging scenarios. ObjectMover can well keep object identity, synchronously edit lighting and shadow effects, complete occluded parts, understand materials, adjust object perspective, and comprehend occlusion relationships to generate realistic images where the object has been moved, maintaining all other elements of the scene unchanged."
        },
        {
            "title": "Abstract",
            "content": "* Work done during an internship at Adobe. Corresponding authors. Simple as it seems, moving an object to another location within an image is, in fact, challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose 1 based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as sequence-to-sequence problem and fine-tune video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct data generation pipeline using modern game engine to synthesize high-quality data pairs. We further propose multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios. 1. Introduction Moving an object within an image is fundamental task in the field of image editing, with applications spanning photography enhancement, content creation, and entertainment. This seemingly simple task is inherently complex as it needs to preserve the objects identity, seamlessly fill occluded regions, and update effects like shadows, reflections, and lighting to be consistent to the new object location (see Fig. 1). Additionally, to realistically render the object movement in 3D scene as 2D image, the perspective view of the object often needs to change or more generally, the pose of the object and its new surroundings may need to adapt to each other (see the last three examples in Fig. 1). Despite significant advancements in image editing [6, 13, 14, 20, 34, 35, 42, 44, 49], few methods can directly accomplish the task of object movement. natural approach to solve this problem is to formulate it as two sequential image editing tasks: object removal [37, 49] at the source location and object insertion [6, 34, 35, 44] at the target location. However, for complete removal of the object at its source location, first defining an appropriate region covering the object with its associated effects such as shadows and reflections can already be highly challenging. Moreover, as object insertion models are not explicitly trained for object movement within the same image, they will often noticeably modify the identity of the object and have inconsistent lighting or shadows compared to the object in the original location (see top row in Fig. 2 for artifacts resulting from this two-step approach). Another two-step approach involves first manually copy-and-pasting the object to new location, followed by model to harmonize it with the surrounding environment [1, 42]. However, this fails to account for natural Figure 2. Limitations of existing methods for object movement. (Top): The approach of removing an object [49] and then re-inserting [6] it for repositioning leads to issues with identity preservation and ineffective synchronization of lighting effects. (Bottom): The copy-paste-based method [1] is unable to modify the perspective of the object. perspective changes for the object at the target location due to the naive copy-and-pasting and often fails to harmonize shadows (see Fig. 2, bottom). To address these limitations, we present ObjectMover, single-stage approach for object movement based on diffusion transformer model. Unlike conventional methods that rely on pre-trained text-to-image models for image editing [1, 6, 34, 35, 42], our approach reformulates the task as sequence-to-sequence prediction problem and repurposes pre-trained video diffusion model to solve the problem. By treating the input scene image, object of interest, user instructions, and target frame as sequence of frames, our method can leverage learned video priors to capture the consistent evolution of lighting, object identity, and scene context across frames. This enables natural, consistent results, which image generative models lacking such priors often fail to achieve (see Fig. 3). One main challenge in training is the lack of paired data. To tackle this, we propose to use two complementary sources of data for training. Firstly, we propose to utilize game engines like Unreal Engine [8] to generate high-quality, aligned synthetic training data specifically designed for movement, which is crucial for transferring and adapting video prior to the target task. Another source of data is from incorporating natural videos to enhance the generalization and improve the performance in real-world scenarios. Despite not being perfectly aligned with our task due to issues like changing backgrounds and hence not directly usable for object movement, we adopt multi-task learning strategy and train with videos on an auxiliary task of mask-based insertion, which enriches the model with real-world domain knowledge, leading to high-quality results, strong generalization ability (see Tab. 3 and Fig. 10). Meanwhile, we fully utilize the synthetic data by training on mask-free object insertion and removal, which equips our model with the ability to address multiple tasks within single framework (see Fig. 8 and Fig. 9). Through extensive experiments, we demonstrate that Ob2 inate other effects like reflections or indirect lighting, and they rely on an accurate shadow mask, which is challenging to obtain reliably. Recently, ObjectDrop [42] has been proposed as mask-free object insertion and removal method, allowing one to address object movement in two-step manner (i.e., first remove and then re-insertion). Nevertheless, this method primarily involves copying and pasting, treating movement as two separate editing tasks. This approach may lead to inconsistencies, unnatural perspectives, poses, or changes in lighting. Another recent insertion method [38] allows changes across the whole image when inserting objects, yet it does not support removal or movement. Generative Movement. Several image editing methods have recently been proposed to move an object shown in an image in semantically plausible way. Typically, they generate an edited image by adding identity preservation and some form of control mechanism to diffusion model. popular control mechanism is drag-based interface [3, 21, 22, 32], where the user defines where specific points in the image should end up. Other forms of control include motion fields [9], coarse version of the edit [1, 42], or 3D-aware controls [4, 20, 24, 45] that also allow for userdriven changes to the 3D pose and perspective of an object. However, none of the methods have demonstrated strong context-driven changes of the object being moved, such as changes in perspective or lighting. Motion-Controlled Video Generators. Motion control for video generation shares the goal of moving an object in scene. Recent methods allow for specifying the motion directions of full trajectories of objects in an image [31, 40]. However, trained on video data, these methods do not have mechanism to ensure that unrelated parts of the scene do not change, such as lighting or secondary objects. The recent DragAnything [43] addresses this issue to some extent, but is still hard to achieve background preservation and contextdriven changes compared to our approach (see Fig. 7). 3. Method 3.1. Overview There are two main goals for the object movement problem: (i) the complete removal of the object at the source location including its associated effects such as shadows or reflections, and (ii) the consistent insertion of the object at the target location. More specifically, the generated object should retain an identity that aligns with the source object while ensuring that its interactions with the surrounding environment are accurately updated. This often involves the appearance of new shadow in the target area and its disappearance from the original location due to movement. For instance, when an object moves from sunlight to shade, it should blend with the darker environment; similarly, persons shadow should be cast on the wall when they are closer Figure 3. Results trained with different priors. The model finetuned from video prior outperforms the one from an image prior. jectMover achieves state-of-the-art results and outperforms existing methods by large margin (see Fig. 7 and Tab. 1). In summary, our contributions are: We introduce novel framework that models object movement as sequence-to-sequence problem and repurpose video model for single-image editing tasks. To address data scarcity, we develop synthetic data construction pipeline using modern game engines which produces high-quality, high-resolution dataset suitable for object movement, removal, and insertion. We propose multi-task learning pipeline that fully leverages different data sources across relevant tasks, enhancing target task performance with real-world video data and equipping the model with versatile capabilities in object removal and insertion. Through extensive experiments, we demonstrate that ObjectMover achieves superior results and outperforms existing methods. 2. Related work Most related to our approach are image editing methods for generative removal, insertion, and movement. Additionally, motion-controlled video generators could implement generative movement by generating video of the moving object. Generative Removal & Insertion. Inpainting methods based on diffusion models [2, 18, 19, 30, 36] are currently state-of-the-art for generative removal, where the object is removed by inpainting image region covered by the object. Similarly, generative insertion [6, 17, 34, 35, 44, 48] is currently dominated by diffusion-based inpainting that additionally guide the generated content to match given object through identity preservation approaches. Most of these techniques maintain identity only roughly, which is particularly problematic for generative movement, where even minor discrepancies are noticeable. Furthermore, all inpainting-based insertion and removal methods fail to create or eliminate effects outside the inpainting mask, such as long shadows, reflections, or indirect lighting. Expanding the inpainting mask results in the loss of the original images identity, as it is not maintained within the inpainting mask. To tackle this, certain methods focus on removing only shadows while preserving other scene attributes [10, 41], but they cannot elim3 Figure 4. Model architecture. (Left): Our overall sequence-to-sequence framework by leveraging an image-to-video prior for training our single-frame image editing task (Sec. 3.2). (Right): Frame formulation on different tasks for multi-task learning (Sec. 3.4). to it. It is important to note that when there are several plausible ways of harmonizing an object at new location to make the image look natural, unlike with object insertion, the harmonization in object movement should match the object at the original location to maintain consistent physical interaction between the static scene and moving objects. Our key insight is that object movement can be considered special case of two-frame video, where the object consistency and dynamic effect evolution between different frames have already been inherently learned in video generation models, as it was trained by watching multi-frame realworld events. Thus, instead of following conventional imageediting approaches that rely on an image diffusion model, we propose to leverage video model as our foundation. Our approach consists of three primary components: First, we reformulate the task of object movement as sequence-tosequence prediction problem, enabling us to repurpose and fine-tune pre-trained image-to-video model (Sec. 3.2). Second, to fine-tune the pre-trained video diffusion model for the target task, we use game engine to construct task-aligned synthetic dataset that is sufficiently diverse and capable of simulating instructive data pairs not typically found in video datasets (Sec. 3.3). Lastly, to fully leverage the potential of synthetic data and also real-world video data, we employ multi-task learning, incorporating related tasks that can be trained on both types of data in unified manner, to enhance the generalization capability of our model (Sec. 3.4). 3.2. Repurposing Video Diffusion Models We utilize pre-trained image-to-video model based on diffusion transformer architecture [25, 39] similar to Sora [23] and devise sequence-to-sequence framework that is consistent with the original video model input, even though we only target at single-frame image generation. As depicted in Fig. 4, we formulate all conditional information, 4 including editing instructions, as image frames: (1) an input image I1; (2) foreground object image I2; and (3) an instruction map I3. The instruction map I3 is formed by layering two bounding boxes across separate channels such that I3 = [Msrc, Mtar, Mtar] with Msrc signifying the initial position and Mtar the target position. Since we use latent-diffusion [29] model, all image frames are converted into tokens within the latent space via VAE encoder. Notationally, unless specified otherwise, the symbols I1, I2, I3, and related terms will refer to the VAE-encoded tokens rather than the original images. During inference, starting from pure noise 0 (0, 1), noise frame is introduced to combine with condition images to create sequence St = [I1, I2, I3, t] and our model vθ iteratively denoises this sequence and finally predicts the clean image 1. At each denoising step, the model also produces an output sequence of equal length, St+t := [I t+t , t+t , t+t]. Before each sub3 sequent denoising step, t+t , t+t are replaced 2 back with I1, I2, I3. , t+t 2 , t+t 3 1 1 The training employs flow-matching loss [15, 16]. Specifically, we use linear interpolation to infuse noise into the ground-truth target, generating the noisy input as follows: = tX 1 + (1 t)X 0, (1) where 1 is the ground-truth image and 0 (0, 1) represents Gaussian noise. The model is designed to predict the velocity = dX dt = 0, which can be converted to an estimated t+t during inference. The loss function is formulated as: = Et,X 0,X 1 (cid:13) (cid:13)v (cid:0)St, t; θ(cid:1) (cid:13) where v[4] is the fourth frame of the output sequence, focusing the loss calculation on the target frame. [4] t(cid:13) 2 (cid:13) (cid:13) (2) , key challenge, however, is the lack of access to the ground-truth image 1. In Sec. 3.3, we present synthetic data simulation pipeline to support training, and in Sec. 3.4, we explore multi-task training on unpaired video data. 3.3. Data Generation with Game Engine As large-scale paired data for object movement are unavailable, we propose to construct the training data with game engine, Unreal Engine [8]. As shown in Fig. 6, our data generation pipeline consists of three steps: (1) background scene generation; (2) movement template pre-configuration; and (3) object movement sequence generation. Step 1: Background Scene Generation. For the background scene, we use collection of maps intended for gaming purposes to enhance realism, different from earlier simpler configurations [20] that depended on manually constructed scenes featuring limited number of scene objects and simplistic environmental maps, as shown in Fig. 5. Using these maps has the following advantages: (1) scene particulars including lighting, background meshes, and assets are meticulously orchestrated by specialists to ensure high fidelity, resulting in realistic shadows, reflections, and background scenes; (2) expansive maps that allow versatile object placement for rendering across various locations within the same environment where diverse background images can be created even from single map; (3) adjustable lighting parameters, such as intensity, temperature, and direction, for diverse lighting conditions. Step 2: Movement Template Pre-configuration. Manually placing objects at multiple locations within background scene can be extremely time-consuming. To address this, we develop variety of trajectory and camera position templates to automate object placement in intermediate locations along the trajectory, capturing intermediate frames in the process. Notably, direct interpolation and placement of objects can lead to clipping artifacts when objects penetrate the ground, violating physical placement principles. To prevent this, after determining each location, we perform ray casting to detect any intersections between the object and the ground (or other objects). We then automatically adjust the position and orientation to ensure the objects bottom surface aligns accurately with the ground. This adjustment is dynamically influenced by the maps local terrain, providing more variation and enhancing data diversity. This step is fully automated process to facilitate large-scale data generation. Step 3: Object Movement Sequence Generation. Finally, with different scene maps, objects and movement trajectories, we generate 18,783 distinct sequences with frame lengths of = 10 or = 15, including one frame with clean background devoid of objects for mask-free object insertion and removal training (see Sec. 3.3). This results in total of 1,348,248 training pairs, 13 greater than the previous Figure 5. Dataset comparison. Row (1-2) shows our synthetic data using game engine; Row (3) shows existing synthetic data [20]. Our data is more realistic and has complex lighting effects. Figure 6. Data generation pipeline. The pipeline consists of three steps: (1) background scene generation; (2) movement template pre-configuration; and (3) object movement sequence generation. work [20]. Furthermore, the images are captured in high resolutions (1024 1024 and 1080 1920), unlike earlier datasets [20] (256 256), and thus supporting our highresolution training for more practical usage. 3.4. Training with Video via Multi-Task Learning The benefit of the synthetic data generation pipeline described in Section 3.3 is clear: it enables the collection of diverse paired data dedicated to object movement under various lighting scenarios through intentional control of lighting parameters. Nevertheless, limitation of the synthetic data is that it may still exhibit domain gap from the natural image distribution and lack the real-world diversity and complexities of dynamic changes across frames. Hence, good source of data for object movement can be natural videos with moving objects. For video data to serve our task, ideally, everything would remain static except for the moving object; however, this is not the case in most videos. Thus, we devise multi-task learning (MTL) strategy that allows us to use both real video data and synthetic data. MTL on Synthetic Data. Our synthetic data includes full background images without the object, which not only supports object movement but also facilitates object removal and insertion tasks. It should be noted that existing compositing methods [6, 34] require masking out the background image, leading to significant limitations: restricted generation scope, where the model can only generate within the masked area, thus preventing the creation of more realistic effects such as shadows and reflections; and unintended background alterations, where regenerating within the mask inadvertently alters original background regions. In contrast, our approach 5 Figure 7. Qualitative comparisons on object movement. Our method consistently outperforms state-of-the-art methods in maintaining object identity, lighting consistency, and overall quality. Notably, in the last example, only our method successfully harmonizes with the target region, achieving realistic effect with partial shadows and light. does not require pre-masking the target location because full background images are available in our synthetic dataset, thereby avoiding the aforementioned limitations. On the synthetic data, our model is jointly trained for all three tasks of object removal, insertion, and movement. MTL on Video Data. As we lack full background frames for the video data, we incorporate an auxiliary task of maskbased insertion (i.e., masking out the background image as in conventional compositing models, wherein we replace the pixels within the mask region with black pixels) to integrate the video data into our training regimen. This enables our model to adapt to real-world content by training in natural videos and helps improve synthesize complex lighting effects. As illustrated on the right side of Fig. 4, our sequenceto-sequence framework consolidates the training of various tasks. In this framework, the edited image consistently corresponds to I1, the segmented object to I2, and the instruction map to I3. Specifically, for removal tasks, only the source location mask is employed in I3, while for insertion tasks, only the target location mask is utilized. For the insertion task, we use the object image from frame different from the target image to compel the model to adapt to the new environment. Across all tasks, the noisy target input is positioned in the final frame. Besides, unique task embedding is developed for each distinct task. 4. Experiments Training Details. We train ObjectMover using an imageto-video model that is based on transformer architecture and has 5B parameters. Note that the method can be applied 6 Method PSNR DINO-Score CLIP-Score DreamSim User-Study Drag-Anything [43] 3DiT [20] Paint-by-Example [44] Anydoor [6] MagicFixup [1] Ours 16.36 19.72 20.83 21.86 23.82 25.27 55.56 45.30 55.46 69.32 78.49 85.07 84.44 81.69 85.23 88.95 91.06 93.16 0.411 0.514 0.420 0.289 0.198 0.142 0.19% 0.19% 0.75% 3.56% 31.14% 64.17% Table 1. Quantitative comparisons on object movement. Our method consistently outperforms state-of-the-art methods. For the evaluation metrics, only the user-study is conducted on ObjMoveB; all other evaluations are performed using ObjMove-A, as only ObjMove-A has ground-truth data. Figure 9. Qualitative comparisons on object insertion. Our method consistently outperforms state-of-the-art methods. mark for assessment. To evaluate the result, we conduct user study in which participants select the best result from different methods based on identity preservation, consistent lighting effects editing, and overall image quality. 4.1. Comparison to Existing Methods Comparison on Object Movement. We first evaluate our model on the core task of object movement against the most relevant approaches [1, 6, 20, 43, 44]. Specifically, Anydoor [6] and Paint-by-Example [44] are limited to object insertion and cannot relocate existing objects. Therefore, we employ the state-of-the-art inpainting model PowerPaint [49] to remove the object from its original position and subsequently utilize these methods to insert the object into the target location. For Drag-Anything [43], we generate video following linear trajectory from the source location to the target location and save the last frame as the edited result (see Supplementary for implementation details). Tab. 1 shows quantitative analysis, where our approach consistently surpasses existing methods. Qualitative comparisons in Fig. 7 highlight our models significant superiority. Specifically, in the first three examples, competing methods fail to effectively adjust object shadows or reflections to the target location. Furthermore, Anydoor and Paint-byExample struggle with preserving object identity. In the challenging fourth example, involving completing an incomplete object and understanding glass material transparency requiring background synchronization, our method excels. Other methods either leave the object incomplete or distort the background. For the fifth image, where changing the 7 Figure 8. Qualitative comparisons on object removal. Our method consistently outperforms state-of-the-art methods. to any video diffusion model. We employ the AdamW [11] optimizer with weight decay of 0.01 and set the initial learning rate to 1 104. The model is trained at resolution of 512 512 pixels with batch size of 256. EMA with decay rate of 0.9999 is applied starting after the first 1,000 iterations to stabilize training. Training involves both synthetic and video datasets, with 1:1 ratio. Tasks in the synthetic dataset are divided into object movement, removal, and insertion at 6:1:1 ratio. We leave the data processing of video data in the supplementary file. Box augmentation is employed by maintaining the bounding boxs center and adjusting its dimensions randomly within set ranges (i.e., 0.8-1.2 on synthetic data; 1.0-1.5 on video data for full maskout), which improves the models robustness. Evaluation Details. For evaluation, we introduce two new datasets: ObjMove-A and ObjMove-B. ObjMove-A comprises 200 image sets captured by experienced photographers. Each set includes an object placed at two distinct locations within the same scene, alongside pure background image without the object. Object masks are subsequently generated using SAM [12]. Thus, the model performance can be quantitatively assessed between the generated image and the referenced target image on this dataset. Following [6, 35, 38], we utilize object-level metrics such as DINO-Score [5], CLIPScore [27], and DreamSim [7] to evaluate the cropped target region. Since our task involves removing the object from the source region, we also evaluate these metrics on the cropped source region, and we show the average score on these two regions. We also assess the PSNR for the full generated image to measure overall image similarity. Given that our task is inherently ill-posed and referencebased evaluations cannot fully capture the realism of the generated images, we conduct human evaluations using our ObjMove-B dataset. This dataset consists of 150 image sets sourced from Pexels [26], consisting of manually annotated object masks and bounding box masks for plausible target regions. Compared with ObjMove-A, this dataset presents more diverse scenes and intricate challenges, such as occlusions, complex shadows and reflections, and complex background, thereby providing more challenging benchMethod Object Removal 3DiT [20] PowerPaint [49] SD-Inpaint [29] LaMa [37] Ours PSNR DINO-Score CLIP-Score DreamSim 20.52 25.15 24.16 27.04 28.90 41.83 62.71 50.80 63.82 83.94 85.02 89.40 83.91 89.91 94.84 0.499 0.347 0.492 0.322 0. Object Insertion PSNR DINO-Score CLIP-Score DreamSim Paint-by-Example [44] Anydoor [6] Ours 22.64 24.10 24.99 54.80 77.49 85.69 81.81 88.33 91. 0.457 0.223 0.152 Table 2. Quantitative comparisons on auxiliary tasks. Our method consistently outperforms state-of-the-art methods. Method PSNR DINO-Score CLIP-Score DreamSim Model Model Model Model 22.86 23.83 23.89 24. 75.01 80.02 79.97 82.09 92.19 93.16 93.22 93.62 0.242 0.196 0.197 0.180 Table 3. Ablation of different models. Model indicates training from the T2I checkpoint with synthetic data only. Model indicates training from the I2V checkpoint with synthetic data only and only on the movement task. Model indicates training from the I2V checkpoint with synthetic data only but adding the insertion and removal tasks. Model indicates adding video data. cars perspective angle to suit the new location is crucial, MagicFixup fails, and Anydoor and Paint-by-Example alter the objects identity. In the final example, only our method successfully integrates into the target region using lighting information, achieving realistic effect of partial shadow and light. We also perform self-evaluation game (details in the supplementary file), presenting four images: one is the input image, and the other three are generated by our model. Users are asked to identify the input image and the results reveal that users incorrectly identify the input image 70% of the time, demonstrating our methods ability to generate images that obscure artifacts. Comparison on Auxiliary Tasks. Our model is capable of object removal and insertion, and we compare these capabilities with other methods [6, 20, 29, 37, 44, 49]. For object insertion, we use the segmented object from the source frame as reference image and prompt the model to place the object into the clean background at the target frames location. We evaluate object-centric metrics in the cropped target region for insertion and in the source region for removal. Tab. 2 shows quantitative comparisons, while Fig. 8 and Fig. 9 provide visual comparisons. Our results indicate that our model outperforms other approaches. Specifically, in object removal, our method effectively removes the object without introducing artifacts, eliminating shadows and reflections more successfully than others, and show stronger background completion ability. In object insertion, other methods struggle with consistent reflections, identity, and background preservation, while our approach maintains consistent reflections, preserves identity and background integrity, and blends seamlessly with the environment. 8 Figure 10. Qualitative comparison among different models. Model performs the worst, failing to move shadows and leaving obvious artifacts. Models and reduce artifacts but still struggle to move shadows effectively. Model performs the best. 4.2. Ablation Studies We perform ablation studies to illustrate the impact of our primary design choices. Every model is trained at resolution of 256 256 for an equal number of iterations. As shown in Tab. 3 and Fig. 10, our designs are proven to be effective. We begin by highlighting the importance of utilizing pretrained image-to-video model. To this end, we train an alternative model based on pretrained text-to-image model (Model A) using our synthetic data across three tasks. Notably, the architecture and parameters of the text-to-image model are the same as those of the image-to-video model, with the exception that the former is pre-trained on single-frame images. Model performs significantly worse compared to Model C, which is trained using pretrained image-to-video model with the identical dataset and tasks, as evidenced by both quantitative and qualitative evaluations. Furthermore, incorporating object removal and insertion tasks into the synthetic dataset maintains similar performance for our primary task while enabling inference over three tasks (Model vs Model C). Finally, training with real-world video data improves performance (Model vs Model D), particularly enhancing background occlusion filling (e.g., the second example) and the consistency of lighting and shadow adjustments. 5. Conclusion In this paper, we presented ObjectMover, novel framework for object movement that integrates sequence-to-sequence modeling using video diffusion model. Our method offers unified, single-stage solution that addresses the challenges of object identity preservation, lighting consistency, and realistic harmonization into new environments. For task-specific model finetuning, we developed data generation pipeline utilizing game engine to produce high-quality data pairs. Additionally, we propose multi-task learning approach that allows for training on real-world video data, enhancing model generalization. The trained model can also be used for object removal and insertion tasks. Extensive experimental results demonstrate that our model not only attains but surpasses the effectiveness of current leading methods."
        },
        {
            "title": "References",
            "content": "[1] Hadi Alzayer, Zhihao Xia, Xuaner Zhang, Eli Shechtman, JiaBin Huang, and Michael Gharbi. Magic fixup: Streamlining photo editing by watching dynamic videos. arXiv preprint arXiv:2403.13044, 2024. 2, 3, 7 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 3 [3] Omri Avrahami, Rinon Gal, Gal Chechik, Ohad Fried, Dani Lischinski, Arash Vahdat, and Weili Nie. Diffuhaul: training-free method for object dragging in images. arXiv preprint arXiv:2406.01594, 2024. 3 [4] Shariq Farooq Bhat, Niloy J. Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In SIGGRAPH, 2024. 3 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 7 [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level In 2024 IEEE/CVF Conference on image customization. Computer Vision and Pattern Recognition (CVPR), page 65936602. IEEE, 2024. 2, 3, 5, 7, [7] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 7 [8] Epic Games. Unreal engine 5.3, 2024. 2, 5 [9] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. International Conference on Learning Representations, 2024. 3 [10] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, and Bihan Wen. Shadowdiffusion: When degradation prior meets diffusion model for shadow removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1404914058, 2023. 3 [11] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7 [12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 7 [13] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [14] Wenbo Li, Xin Yu, Kun Zhou, Yibing Song, Zhe Lin, and Jiaya Jia. Image inpainting via iteratively decoupled probabilistic modeling. arXiv preprint arXiv:2212.02963, 2022. 2 [15] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [16] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 4 [17] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3 [18] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 3 [19] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. ICLR, 2022. [20] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 5, 7, 8, 11 [21] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. ICLR, 2024. 3 [22] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Diffeditor: Boosting accuracy and flexibilIn Proceedings of ity on diffusion-based image editing. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84888497, 2024. 3 [23] OpenAI. Video generation models as world simulators. https://openai.com/research/videogeneration - models - as - world - simulators, 2023. Accessed: yyyy-mm-dd. 4 [24] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J. Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 76957704. IEEE, 2024. [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 4 [26] Pexels. Pexels - free high-quality images. https://www. pexels.com/, 2024. 7 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [28] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman [39] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4 [40] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. In ICML, pages 5227452289, 2024. 3 [41] Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu. Instance shadow detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18801889, 2020. 3 [42] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion, 2024. 2, 3 [43] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2025. 3, 7, [44] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2023. 2, 3, 7, 8 [45] Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Image sculpting: Precise obPanozzo, and Saining Xie. In Proceedings of ject editing with 3d geometry control. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42414251, 2024. 3 [46] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 13 [47] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 13 [48] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv preprint arXiv:2308.10040, 2023. 3 [49] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023. 2, 7, Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 11 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4, 8 [30] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 3 [31] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion In ACM SIGGRAPH 2024 Conference Papers, modeling. pages 111, 2024. 3 [32] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. 3 [33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [34] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Object compositing with diffusion model. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2023. 2, 3, 5 [35] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and Daniel Aliaga. Imprint: Generative object compositing by learning identity-preserving representation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 80488058. IEEE, 2024. 2, 3, 7 [36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021. 3 [37] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 21492159, 2022. 2, 8 [38] Gemma Canet Tarres, Zhe Lin, Zhifei Zhang, Jianming Zhang, Yizhi Song, Dan Ruta, Andrew Gilbert, John Collomosse, and Soo Ye Kim. Thinking outside the bbox: Unconstrained generative object compositing, 2024. 3,"
        },
        {
            "title": "Appendix",
            "content": "A. Details on Compared Methods We provide more implementation details of two comparison methods, i.e., DragAnything and 3DiT. DragAnything [43] DragAnything is trajectory-based video generation model that allows specifying one or more objects, enabling the corresponding objects to move according to the trajectory in the generated video. However, while this method can control the positions of the generated objects to match the coordinates in the trajectory, it cannot control other elements, such as keeping the background stationary. To maintain the background as much as possible, we need to select point in the background region and assign it stationary trajectory to control the relative stability of the background. In our implementation, we strive to keep the background stationary. Specifically, we design two trajectories. The first is the foreground trajectory, which controls the movement of the target object. The start point is the original center position of the object, and the end point is the center of the target position of the object. The key points of the trajectory are obtained through linear interpolation. The second is the background trajectory, where we set the trajectories of background point in the background region to be stationary, thereby maintaining the stability of the background. To automatically select background point, we identify the location within the background mask that is farthest from both the foreground object and the image boundaries. This is achieved by computing the Euclidean distance of each background pixel to the nearest foreground pixel and the image borders, then selecting the point with the maximum minimum distance. Nonetheless, despite our effort, this can still result in detail-level jitter or control failures, as shown in the last example of Fig. 20. 3DiT [20] 3DiT is text-conditioned image editing method that cannot use bounding boxes to precisely control the objects to be moved and their target positions. Instead, it requires textual prompt to describe the objects and the coordinates of the target positions. To address this, we employ an image caption model to generate text labels for each cropped object in our evaluation set, which are then used as prompt instructions. For the target position coordinates, we use the center points of the target bounding boxes. B. Video Dataset Pipeline We use an internal video dataset as the real-world video source. Note that the dataset pipeline can be applied to any video dataset. For processing, we utilize SAM2 [28] to segment the videos and obtain consistent object labels across frames. Then, we filter out objects with masks that are too small and those that do not appear simultaneously in both frames. Finally, we obtain approximately 800,000 image groups, each containing two frames and the corresponding mask image for one object. Figure 11. Video dataset samples. Frames and corresponding masks from our video dataset, processed with the SAM2 [28] to ensure consistent object labeling across frames. Figure 12. Mask-based insertion on video data. #1: Input image with the object masked out. #2: Isolated object image. #3: Instruction map indicating where to place the object. #4: Ground-truth image for prediction. Fig. 11 shows some sample data from the video dataset. As mentioned, in the video dataset, other objects or backgrounds, except for the main subject, mostly change, making it difficult to directly use them for training movement tasks. However, we use the video data to train on mask-based object insertion task. As demonstrated in Fig. 12, during the training process, the object from frame #1 is extracted using mask #1 and serves as the object image, coupled with foreground-masked frame #2 as the input, to predict complete frame #2. Note that our model is trained on video and CG data, while it is evaluated on image data. Hence the training and evaluation data are completely different with no overlap or data leakage issues. ObjMove-A is manually captured using DSLR while ObjMove-B is web data without ground-truth, Figure 13. Interface of the additional user study. Samples from our find-the-real-image game designed to assess the realism of synthesized images. Each participant is shown set of images and asked to identify the original. which theoretically prevents data leakage. C. Samples of Synthetic Data Fig. 15 shows some rendered images of different objects in background scenes with varying camera views. We also display their corresponding clean background images where no object is located in the region of interest. These clean background images support our mask-free object removal and insertion training. Fig. 16 presents examples of the full sequence rendering. The first four rows illustrate sequences of the same scene and object in different positions, albeit under different lighting conditions. The last two rows display the corresponding object masks, which are directly obtained through rendering. Notably, these masks represent the amodal extent of the objects without considering occlusion relationships. This approach aids the model in learning to determine whether an object should be occluded when mask overlaps with another object. D. Additional User Study: Find the Real Image We conduct an additional user study where users are asked to find the real image among four images, of which three are generated by our method by moving the object of interest to different locations. The results reveal that users incorrectly identify the real (input) image 70% of the time, demonstrating our methods ability to generate realistic images that effectively obscure artifacts. Samples of this game are illustrated in Fig. 13, and we also provide web link here to play the game interactively. Figure 14. Illustration of representative failure cases. Rows 1 and 2 show unintended pose alterations when moving non-rigid objects (e.g., humans), where the generated pose significantly deviates from the original. Row 3 illustrates the disappearance of nearby objects when one object is moved closely past another. Row 4 shows text distortion after object movement, common limitation inherent in latent diffusion models. movement, removal, and insertion. For each result, we annotate key aspects above the images to better demonstrate the capabilities of our model. E. More Results F. More Comparisons We provide additional qualitative results of our model on in-the-wild internet images. Fig. 17, Fig. 18, and Fig. 19 respectively showcase more results of our model on object We present additional comparison results between our method and other approaches. Fig. 20 and Fig. 21 respectively show the comparison results for the movement and 12 Figure 15. Image samples of synthetic data. Display of synthetic scenes with object placements across varying camera angles. Figure 14), moving the object often results in distorted text. This is common limitation in latent diffusion models caused by insufficient reconstruction capabilities of the VAE. Moreover, our method exhibits relatively slow inference speed. On single NVIDIA A100 GPU, inferring an image with resolution of 512512 requires approximately 20 seconds, which is slower than other U-Net-based approaches. However, in future work, we aim to reduce the inference cost by employing model distillation and diffusion distillation techniques [33, 46, 47], thereby enhancing the practical applicability of our approach in real-world scenarios. removal tasks on ObjMove-B. Fig. 22 displays the insertion results on in-the-wild image pairs. Additionally, Fig. 23, Fig. 24, and Fig. 25 illustrate the movement, removal, and insertion results on ObjMove-A, where reference groundtruth image is also provided. G. Limitations and Future Work While our method achieves excellent results across three tasksobject movement, removal, and insertionit still possesses certain limitations. Figure 14 illustrates several failure cases, categorized into three main scenarios: 1. Unintended Pose Alterations. Our design philosophy emphasizes maintaining the objects original pose as consistently as possible, only automatically adjusting the pose when necessary for harmonious integration into the new environment. This strategy generally ensures stable and robust performance. However, for non-rigid objects (e.g., humans), generated results sometimes exhibit significant and unintended pose alterations, occasionally introducing new content (rows 1 and 2 in Figure 14). We suspect this primarily arises from the abundance of human-motion examples in real video datasets, which bias the model towards pose variability. To address this, we plan to incorporate meta-information regarding relative object-camera poses into our synthetic dataset and conditionally train the model based on this information. This enhancement will enable explicit 3D control, allowing precise, user-directed pose manipulation. 2. Disappearance of Nearby Objects. When an object is moved closely past another object (row 3 in Figure 14), the nearby object occasionally disappears. We attribute this to lack of examples where one object explicitly crosses over another within our synthetic training data. This limitation can easily be resolved by augmenting the dataset with relevant scenarios. 3. Text Distortion. For objects containing text (row 4 in 13 Figure 16. Full sequence examples of synthetic data. We show two sequences from our synthetic dataset with an object placed in different locations with two lighting conditions. The last two rows present the object masks. 14 Figure 17. Qualitative results on object movement. Key aspects to focus on are annotated above each image to highlight the models ability. 15 Figure 18. Qualitative results on object removal. Key aspects to focus on are annotated above each image to highlight the models ability. 16 Figure 19. Qualitative results on object insertion. Key aspects to focus on are annotated above each image to highlight the models ability. 17 Figure 20. Qualitative comparisons on object movement. Our method consistently outperforms state-of-the-art methods. 18 Figure 21. Qualitative comparisons on object removal. Our method consistently outperforms state-of-the-art methods. 19 Figure 22. Qualitative comparisons on object insertion. Our method consistently outperforms state-of-the-art methods. 20 Figure 23. Qualitative comparisons on object movement. Our method consistently outperforms state-of-the-art methods. 21 Figure 24. Qualitative comparisons on object removal. Our method consistently outperforms state-of-the-art methods. 22 Figure 25. Qualitative comparisons on object insertion. Our method consistently outperforms state-of-the-art methods."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "The University of Hong Kong"
    ]
}