{
    "paper_title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
    "authors": [
        "Tilemachos Aravanis",
        "Vladan Stojnić",
        "Bill Psomas",
        "Nikos Komodakis",
        "Giorgos Tolias"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 9 3 3 3 2 . 2 0 6 2 : r Retrieve and Segment: Are Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation? Tilemachos Aravanis1 Vladan Stojnic1 Bill Psomas1 Nikos Komodakis2 3 4 Giorgos Tolias1 1VRG, FEE, Czech Technical University in Prague 2University of Crete 3Archimedes, Athena RC 4IACM-FORTH Figure 1. Open-vocabulary segmentation (OVS) results. We compare three settings: (i) textual-only support (zero-shot OVS), (ii) simplified version of RNS using visual-only support, and (iii) the full RNS combining textual+visual support. Textual support: class name or description. Visual support: small set of pixel-annotated images for some classes. Initially, visual support includes images in and is later expanded to B, with B. Text-only support often yields ambiguous predictions (rider as motorcycle, background hallucinations). Visual-only support struggles when some classes lack support (person, car) and can confuse similar objects (motorcycle, bicycle) even when all classes have support. By retrieving information from images relevant to the test image and combining with the textual support, RNS is robust under missing visual support for some classes and achieves accurate segmentation."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of visionlanguage models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing fewshot setting that augments textual prompts with support set of pixel-annotated images. Building on this, we propose retrieval-augmented test-time adapter that learns lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, perquery fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability. Code Semantic segmentation traditionally relies on fully supervised models trained on dense pixel-level annotations within fixed set of categories [9, 45, 93]. While this approach yields accurate, well-localized masks, it does not scale; collecting pixel-level annotations is costly, and models cannot recognize categories unseen during training. Open-vocabulary segmentation (OVS) builds on the zero-shot recognition capabilities of contrastively trained visionlanguage models (VLMs) [25, 54, 89]. Trained on large imagetext datasets [8, 60], VLMs learn shared embedding space where images and text are directly comparable, enabling recognition of arbitrary categories specified at test time via text prompts or class names. Extending this ability from image-level to pixel-level prediction has driven rapid progress in OVS [34, 35, 67, 79]. However, substantial gap remains to fully supervised models [9]; recent improvements show signs of plateauing [59]. Two key challenges underlie this gap: (i) the mismatch between image-level supervision used to train VLMs and the fine-grained predictions required for segmentation, and (ii) the semantic ambiguity of natural language supervision. While language enables open-vocabulary recognition, it often lacks the precision needed for pixel-level tasks. We address these challenges by introducing fewshot setting that supplements the textual support of class names with small support set of visual examples, i.e. images with pixel-level annotation. We aim to bridge the gap between zero-shot OVS and supervised segmentation, while preserving the open-vocabulary predictions. We propose retrieval-augmented test-time adapter that trains lightweight classifier per test image. Inspired by retrievalbased methods [30, 77], our approach retrieves relevant visual support examples and fuses them with textual support to construct test-time training data. Unlike previous work [2, 19] relying on hand-crafted fusion, our method performs learned per-image fusion of textual and visual prototypes, enabling strong synergy between modalities, as shown in Figure 1. Importantly, we store only compact set of visual prototypes from the support images, keeping the memory footprint minimal, while our test-time training requires less than second on an NVIDIA A100 GPU1. This design is enabled by the strong, generalizable features of modern VLMs trained at large-scale [8, 60]. These rich features allow us to avoid retraining the backbone and instead steer predictions with lightweight classifier trained on only few visual examples. By leveraging these robust embeddings, our approach achieves efficient adaptation while preserving the open-vocabulary nature of the task. Our method, called Retrieve and Segment (RNS), can handle diverse real world settings, from textual and visual support for all classes, to partial support for some classes, where either textual description is not straightforward to obtain or visual examples are not yet available. Moreover, it is compatible with dynamic continually changing setting where new visual examples can be added to the support set at any time, without sacrificing the open-vocabulary nature. Due to its simplicity and its dynamic adaptability, our approach is easily applicable to segmentation of particular objects, i.e. the so called personalized segmentation [68, 91]. In summary, our contributions are: We investigate multiple few-shot settings for openvocabulary segmentation, enriching textual prompts with pixel-annotated visual examples. We introduce RNS, retrieval-augmented, test-time adapter that learns to fuse textual and visual support more effectively than prior approaches. RNS significantly reduces the performance gap between zero-shot and fully supervised segmentation, while maintaining open-vocabulary generalization. RNS supports dynamic support expansion in continually evolving environments, and adapts seamlessly to finegrained tasks such as personalized segmentation. 1See supplementary material for runtime details. 2. Related work Open-vocabulary segmentation is performed by leveraging visionlanguage models (VLMs) that align images and text in shared space [25, 54, 89]. The fixed classifier is replaced by text encoder and matches patch features to text features, but vanilla VLMs pre-trained with image-level supervision struggle with dense localization [4, 97]. There are three lines of work: (i) Training VLMs for segmentation: either weakly supervised with masks without labels [14, 18] or imagecaption pairs [7, 46, 49, 56, 82, 83], or fully supervised with pixel annotations [12, 26, 37, 39, 40, 81, 86, 94]. However, this approach hinders the open-vocabulary performance outside of the training domain [12, 67], as shown in datasets like MESS [3]. (ii) Training-free VLM tweaks: modified inference processes to boost spatial sensitivity, e.g. removing the final attention layer [97] or residual connections and feedforward networks [34], with further (iii) VLM+VM hybrids: combinvariants [4, 34, 72]. ing VLM semantics with the localization of vision models (VMs) [28, 31, 35, 62, 67, 78, 79, 90]. Methods typically localize objects with DINO [6, 51] or SAM [33, 57], then classify the localized regions in an open-vocabulary manner using VLMs. Despite progress, OVS still trails taskspecific, fully supervised models. Few-shot segmentation learns on base classes and adapts to novel classes from few (support) labeled examples. Meta-learning and prototypical methods perform episodic 1 or way training, create per-class prototypes and classify via similarity [47, 61, 64, 69, 73], typically assuming closed world. recent generalized setting evaluates on both base and novel classes [20, 21, 43, 70], but still requires abundant pixel-level annotations for base classes and does not leverage VLMs. Close to our setting, concurrent work Power-of-One [22] introduces one-shot per class fine-tuning of text embeddings and specific backbone layers. They require access to raw images, while we operate on pre-extracted features, and fine-tune internal VLM [38] layers for each new class set, which is not as light-weight as our test-time adapter. Beyond VLMs, CAT-SAM [80] explores few-shot adaptation of SAM [33] via conditional tuning with lightweight adapters, but does not combine visual and textual support. COSINE [44] unifies open-vocabulary (text-prompted) and in-context (image-prompted) segmentation by training decoder on top of frozen foundation models to handle multi-modal prompts. For open vocabulary semantic segmentation with many categories the model is only evaluated with unimodal prompts though. Retrieval augmentation in segmentation. Retrievalaugmented models for prediction and generation have shown strong performance by dynamically expanding their knowledge base [23, 42, 55, 87]. Following this paradigm in semantic segmentation, recent work enhances in-context scene understanding of vision encoders [1, 52, 2 66]. FREEDA [2] is conceptually related to RNS, but relies on generated visual examples. At test time, textual class features are expanded into visual counterparts via retrieval, forming non-parametric visual classifier, which is then combined with standard zero-shot textual classifier. For fair comparison, we adapt this method to use real support images instead of generated ones. Closely related to our work is kNN-CLIP [19], which enhances open-vocabulary semantic segmentation by leveraging memory-efficient support set of class vectors derived from pixel-annotated images. At test time, it assigns labels to image regions based on the similarity to their nearest neighbors in the support set. This approach outperforms continual learning baselines, demonstrating that dynamic support set can incorporate visual examples of new classes without forgetting previously learned ones. However, while kNN-CLIP claims to expand the models vocabulary to arbitrary class sets, its performance remains limited to classes for which annotated examples are available. Test-time adaptation (TTA) for VLMs has grown rapidly, but many methods operate in batch/transductive or streaming modes [16, 29, 36] and carry unrealistic assumptions, such as class-complete batches or i.i.d. streams, which compromises zero-shot robustness [88]. In contrast, single-image TTA adapts per sample: TPT [63] and ZERO [17] perform optimization or predictions over augmentations. For segmentation, single-image TTA is explored with self-supervised objectives [24], and OVSspecific TTA is just emerging, proposing adaptation layers for VLM-based segmenters at test time [50]. 3. Method 3.1. Task formulation Given image RHW 3 and set of semantic classes, we aim to assign each pixel of to one of the classes. The class set is arbitrary and defined at test time. Therefore, the task is referred to as open-vocabulary segmentation (OVS). Each class is specified either by textual example, e.g. class name, or by small set of visual examples in the form of raw images with pixel-level annotations, like in few-shot setting. We refer to these as the textual support set and visual support set, respectively. Typically, textual example is available for every class. However, visual examples may be missing initially, but their number and diversity can increase over inference time, reflecting continually expanding open-world scenario. In rare cases, textual examples may also be absent, e.g. for novel categories with unknown names or in specialized domains such as medical imaging or remote sensing, where naming is non-trivial. We consider the following settings: (i) full-support: every class in has class name and at least one annotated (ii) partial-visual-support: some classes support image. lack visual examples, but all have class names. (iii) partialtextual-support: some classes lack class names, but all have visual examples. (iv) only-textual-support: class names are available for all classes in C, but no visual support images are provided, i.e. the commonly studied zero-shot segmentation setting. In contrast to zero-shot segmentation, all other settings have visual support examples available, and our goal is to leverage them to improve segmentation, while preserving the ability to operate with an open-vocabulary. We denote support image by i, test (query) image by q, while we drop the superscript when referring to images in general and we adopt consistent notation for other variables as introduced in the following sections. 3.2. Zero-shot segmentation with VLMs Image RHW 3, processed by the vision encoder of VLM, is mapped to patch-level feature matrix Rnd. Here = corresponds to the flattened patch grid produced by vision transformer (ViT), and denotes the feature dimension. The patch feature at position 1, . . . , is denoted by xj Rd. Each class name C, processed by the VLMs text encoder, is mapped to textual class feature tc Rd. Both textual and visual features are normalized to unit length. The patch-level prediction map is denoted by ˆP [0, 1]nC with elements ˆPjc denoting the probability of patch for class and computed by the dot product similarity between patch feature and textual class feature as ˆPjc = sC(x tc), (1) where sC() is the softmax over class set C. Then, lowresolution prediction ˆP is reshaped and upsampled to the full-resolution prediction ˆY [0, 1]HW C, and segmentation is obtained by the argmax of ˆY over classes. 3.3. Full textual and visual support In the following, we describe how to process examples in the textual and visual support sets to construct two support features sets, namely the visual support feature set and the fused support feature set combining visual and textual information. During inference, the elements of the support feature sets that are the most relevant to test image are used to train lightweight patch-level linear classifier, which is then applied to the features of the same test image. Overview of this process is shown in Figure 2. Visual support features. Given support image and its ground-truth segmentation labels {0, 1}HW at full resolution, its patch-level feature matrix Rnd is extracted. Then, the ground-truth pixel-level labels are down-sampled and reshaped to obtain patch-level labels, which are not binary anymore due to interpolation and are subsequently L1-normalized per class (column) to obtain 3 Figure 2. Overview of RNS when full textual and visual support is available. Having access to set of pixel-level annotated images, per-image visual class features vi are extracted. These features are then aggregated by class to form visual class features vc, which are combined with textual class features tc, through mixing coefficient λ, to produce fused class features fcλ. During test-time training, test-image-relevant subset of visual support features and fused class features, along with their class labels, are used to train lightweight linear classifier gθ using cross-entropy loss. Each training sample is weighted with class relevance weight wc (e.g. for bg). At inference, this classifier, trained per test image, is applied to patch-level features xq to generate segmentation predictions. When SAM is available, patch-level features are replaced by region-level features for improved accuracy. [0, 1]nC. These labels are used to pool the patch features into per-image visual class features vi for classes Ci present in image by vi = (cid:88) j=1 jc i . (2) Assuming available support images, the union of perimage visual class features is given by (cid:91) = {v : Ci}, i=1 (3) and referred to as visual support feature set, which is used to support the test-time adaptation process. Fused support features. We aim to leverage the semantic information in the textual class features. However, due to the modality gap between visual and textual features in VLMs [41], and because our goal is to classify image patchlevel features, combining them directly with visual support features does not perform well, as confirmed by our experiments. Thus, for class c, we create fused class feature class features per class. We denote the fused support feature set by = {fcλc C, λ Λ}, which is used to support the test-time adaptation process. Support feature set maintenance. For each support image, we extract its feature matrix and pool patch features into per-image visual class features (2). If new support image arrives, we update the visual support feature set (3), the visual class features (5), the fused class features (4), and consequently the fused support feature set F. Therefore, our support sets are dynamically expandable in straightforward and efficient manner, allowing to operate in continually evolving open-world scenario. Test-time adaptation. For test image with feature matrix Rnd, and the feature of patch denoted by xq , we train linear classifier gθ : Rd RC, specifically for q, to project features to class probabilities. We leverage the relevant elements of the two support feature sets. To this end, we retrieve the nearest neighbors of each test image patch features from the visual support feature set and unite them into the retrieved visual support feature set Vr = (cid:91) j=1 kNN(cid:0)V, xq (cid:1). fcλ = λ tc + (1 λ) vc, λ [0, 1], (4)"
        },
        {
            "title": "We define the visual support loss by",
            "content": "by combining textual class feature tc and visual class feature vc obtained by aggregating all per-image visual class features for class across the visual support set by vc = , (cid:88) iIc (5) where Ic is the set of support images that contain class c. The fusion process is performed for set of mixing coefficients Λ [0, 1] to capture diverse and complementary information from both modalities, yielding multiple fused Lv = (cid:88) Vr l(v) CE(cid:0)gθ(v), 1l(v) (cid:1), where CE is the cross entropy loss, l(v) provides the label of feature (a per-image visual class feature), 1c is the onehot encoding of class c, and wc is class relevance weight for class c. This loss encourages the classifier to assign high probability to the correct class for each retrieved support feature. The retrieved set is expected to include features corresponding to classes present in the test image, particularly those originating from visually similar images. 4 (6) (7) The class relevance weights are used to suppress the impact of retrieved features irrelevant to the test image. Weight wc is estimated via the similarity between the image-level feature xq Rd and the corresponding textual class feature, followed by softmax: wc = sC (cid:16) (xq)tc (cid:17) , with xq given by global average pooling xq = 1 n (cid:88) j=1 xq . (8) (9) We utilize textual support, via the fused support set, by training the classifier on the fused class features (denoted by Fr and referred to as retrieved fused support feature set) of the classes (denoted by Cr) that appear in retrieved visual support feature set Vr. This is performed via the fused support loss: Lf = (cid:88) wc (cid:88) CE(cid:0)gθ(fcλ), 1c (cid:1). (10) cCr λΛ We observe that using multiple mixing coefficients Λ improves performance compared to using single coefficient. The total loss is given by = Lv + βf Lf . After training, classifier gθ is applied to test image patch features to obtain patch-level predictions. Then, this low resolution prediction map is upsampled to the original image resolution to obtain the final segmentation map. 3.4. Partial visual support We present an additional loss, while other components remain as in Section 3.3. Classes without visual support. The set of classes supported by their class name but with no image examples are denoted as Cd. Given the absence of visual support for these classes, we cannot compute their visual class feature vc (5), and consequently, their fused class feature fcλ (4). To circumvent this, we exploit the test image to identify whether any of the classes in Cd are present in it via zeroshot prediction ˆP (1). Predictions in ˆP are converted to one-hot vectors by assigning each patch to its most probable class, and the result is L1-normalized per class (column) to obtain [0, 1]nC. We denote by Cq the set of classes assigned to at least one patch according to q. Then, we define the visual class feature by pooling the patch features according to these pseudo-labels: vc = (cid:88) j=1 jc xq , Cd Cq. (11) visual class feature cannot be obtained by (5). This allows us to perform the modality fusion by (4) for all classes. Fused feature pseudo-labeling. Fused class feature fcλ is derived through visual class features that use pseudolabeling. Therefore, its association with class is uncertain. To circumvent this, we pseudo-label those fused class features. The predicted probability distribution for fcλ denoted by ˆpcλ [0, 1]C has its element, associated with class c, estimated by cλtc followed by softmax over all classes. Extended test-time adaptation loss. We introduce pseudo-label loss term exploiting such pseudo-labels: Lp = (cid:88) (cid:88) wc KL(cid:0)ˆpcλ (cid:13) (cid:13) gθ(fcλ)(cid:1), (12) cCdCq λΛ where KL is the Kullback-Leibler divergence. The total loss is given by = Lv + βf Lf + βp Lp. Note that classes with visual support are handled in the second loss term, while the rest are either handled in the third loss term (Cr Cd = ) or ignored if they are not predicted to be present in the test image. 3.5. Partial textual support We modify fused support features of classes without textual support, while other components remain as in Section 3.3. Classes without textual support. When class names are absent, we cannot compute textual class features tc or their fused counterparts fcλ (4). Excluding these classes from the loss introduces bias toward classes with both supports. Instead, we replace missing textual class features with the average textual class feature across classes with an available class name. This provides neutral semantic prior, ensuring that all classes equivalently participate in the loss. If textual support is missing for all classes, no textual class features can be formed. In this case, we simply set Λ = {0} and class relevance weights wc are set equal to 1 for all classes, effectively reducing to our w/o text baseline. 3.6. Region-proposal predictions Our method so far assumes patch-level feature extraction and predictions, i.e. xq for patch j. When region proposals {0, 1}HW for binary masks are available for the test image, e.g. from SAM [33, 57], we proceed as follows. We downsample each mask to patch resolution and apply L1 normalization per region, yielding [0, 1]nR. We then pool patch features into region-level features by r = (cid:88) j=1 Sjr , = 1, . . . , R, (13) Note2 that we perform this only for classes in Cq Cd whose 2Notation vc used for both types of visual class features; (5) used for classes with visual-textual support, while (11) for those with only textual. where denotes the region index. Finally, we assign labels at the region level and map them to the corresponding mask regions at the full image resolution to obtain the final segmentation map. 5 50 30 ) % ( m 40 30 RNS kNN-CLIP FREEDA Zero-shot with text w/o text SAM Patch 1 2 3 10 20 1 2 3 5 10 Support images per class Support images per class Figure 3. Full textual and visual support. We compare zero-shot, RNS, kNN-CLIP and FREEDA and their variants without class name information (w/o text) for increasing number of support images per class. SAM 2.1 is used for region proposals. Left: OpenCLIP (ViTB/16) for region-level predictions. Right: DINOv3.txt (ViT-L/16) for patch-level and region-level predictions. ) % ( I 40 20 0 RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo-label loss w/o text 35 30 25 RNS kNN-CLIP FREEDA Zero-shot with text w/o text 0 0. 0.3 0.5 0.7 0.9 0 0. 0.3 0.5 0.7 0.9 Fraction of classes w/o visual support Fraction of classes w/o textual support Figure 4. Partial visual (left) and textual (right) support settings. Results of zero-shot, RNS, kNN-CLIP, FREEDA and their variants without class name information (w/o text). RNS evaluated w/o the pseudo-label loss in (12). OpenCLIP ViT-B/16 and SAM 2.1 are used. Left: fraction of classes lack visual examples, while = 3 for the rest. Right: fraction of classes lack textual class names, and = 1. 4. Experiments 4.1. Experimental setup Datasets and evaluation. We evaluate on the validation split of six OVS benchmarks: PASCAL VOC [15] (VOC), PASCAL Context [48] (Context), COCO Object (Object), COCO-Stuff [5] (Stuff), Cityscapes [13] (City), and ADE20K [96] (ADE). We report the average mIoU over all datasets unless otherwise noted. Per dataset results are presented in the supplementary. We also evaluate on PASCAL Context-59 (C-59) [48], FoodSeg103 (Food) [76], and CUB [75] to compare OVS and fully supervised methods. Details about sampling the support images per class for the visual support set, and the construction of the fewshot benchmark are shown in the supplementary. Implementation details. We use OpenCLIP ViTB/16 [11] trained on LAION [60] and apply the MaskCLIP trick [97]. For DINOv3 [65], we use the public ViT-L/16 checkpoint adapted with dino.txt [27] to derive text-aligned patch features, denoted by DINOv3.txt. For region proposals, we run SAM 2.1 [57] Hiera-L on each query image with 3232 grid of points (one mask per point) and nonmaximum suppression to ensure non-overlap. More implementation details are shown in the supplementary. Competitors. We compare to zero-shot prediction (Section 3.2), and to two retrieval-based OVS methods that leverage both visual and textual support sets: kNN-CLIP [19] , which we re-implement, and FREEDA [2], where we adapt the official code to use real support set, i.e. pixel-annotated images, rather than synthetic-only prototypes. Please refer to the supplementary material for details in the implementation of the aforementioned competitors. We also report performance for training linear classifier or the full network offline on the entire support set in closed-set manner. These variants lose the open-vocabulary ability as they cannot provide predictions for unseen classes, but provide useful reference baselines. Specifically, we consider the following three offline training methods: (i) Linear classifier on per-image visual class features vi and their corresponding labels. (ii) Offline linear classifier on patch-level features xi and pixellevel annotations. Predictions are upsampled to apply the loss at the full image resolution. (iii) Offline finetuning on images and pixel-level annotations. This configuration builds upon the previous setup but, in addition to training the linear classifier, it also finetunes the parameters of the vision encoder. We also compare to SOTA fully supervised and different kinds of OVS methods in Table 2. 6 50 40 30 ) % ( m RNS Vr VCr random from VCr random from furthest from VCr 1 2 3 5 20 ) % ( m 30 20 10 RNS RNS w/o text Offline linear clf on visual class features Offline linear clf on pixel-level annotations Offline finetuning on pixel-level annotations Offline finetuning on pixel-level annotations + RNS Zero-shot 1 2 3 10 20 Support images per class Support images per class Figure 5. Impact of retrieval on RNS. We replace the retrieved visual support feature set Vr of RNS with random subset of the visual support feature set V, or different variants of visual support features from the retrieved classes VCr ."
        },
        {
            "title": "Method",
            "content": "B = 1 = 5 = 10 RNS RNS w/o wc RNS w/o wc Λ = {0.8} RNS w/o text 41.59 41.20 -0.39 36.40 -5.19 34.11 -7.48 47.87 47.43 -0.44 46.55 -1.32 45.71 -2. 49.02 48.54 -0.48 48.38 -0.64 48.00 -1.02 Table 1. Ablations of RNS. We report average mIoU across the considered datasets for three different numbers of available support images per class (B = 1, = 5, = 10). Blue numbers denote difference to the number in the same column but first row. 4.2. Experimental results Full textual and visual support. Figure 3 reports mIoU as we vary the number of support images per class. RNS consistently outperforms all competitors on every B, both backbones, and input feature granularity, with large gains. Moreover, it yields significant improvement over zero-shot segmentation: +7.3% on OpenCLIP and +18.4% on DINOv3.txt with just one image per class. RNS effectively leverages text, achieving performance gain at = 1 over the w/o-text variant while performing on par in the = 20 case, indicating that textual priors are valuable when support is sparse, while visuals dominate as support densifies. Interestingly, kNN-CLIP fusion heuristics help at = 1, but hinder after = 5, suggesting sensitivity to handtuned fusion as support grows. The variant of kNN-CLIP that discards text is competitive when enough support images are available but scores low in the few-shot regime, where the diversity and quality of the support are minimal. FREEDA does not benefit enough from combining textual with visual support; low gains with respect to w/o-text baseline. Moreover, on newer backbones with stronger localization, e.g., DINOv3, the gap between RNS and kNN-CLIP widens, indicating that RNS scales better with better visual representations. Region-proposals consistently boost performance compared to patch-level predictions, by leveraging the large-scale training of SAM, at the expense of higher computational cost at inference time. Figure 6. Comparison in closed vocabulary setting. We compare RNS to the offline baseline competitors. To ensure fair comparison we tune the learning rate, batch size, and number of iterations using train-validation split from the available support images. No mask proposals are used. We report average performance on VOC, ADE, and Stuff. Partial visual support. In Figure 4 (left), we vary the fraction of classes without visual examples, while keeping text descriptions for all. RNS degrades smoothly, and manages to benefit from the available visual support even for small fractions of supported classes. Removing pseudolabel loss (12) leads to steep drop in performance; this is our mechanism to compensate for the missing visual support. Further removing textual support (w/o text) degrades performance more, as the model cannot leverage neither fusion nor class relevance weights. kNN-CLIP and FREEDA drop significantly, and soon perform below zero-shot, as they do not account for missing classes in their support set. Note that missing visual support for some classes is inevitable in an open-world and dynamically expanding environment. Partial textual support. In Figure 4 (right), we vary the fraction of classes without text descriptions, while keeping the visual support set intact. RNS remains the best across the entire range, dropping only mildly as text becomes scarce. Results suggest that training jointly on visual and textual support is superior to heuristic late fusion of independent predictions; kNN-CLIP degrades steeply, while FREEDA barely benefits even when text is available. RNS uses text as an auxiliary signal, used for fusion and relevance weighting, yielding consistent boost when available but without making the model fragile when it disappears. Ablations are presented in Table 1. Removing class relevance weights (wc) yields reduction across all shots, confirming their role in suppressing irrelevant retrieved classes. Additionally, creating fused features (4) with single λ = 0.8 harms especially the low-shot regime, where effective use of text fusion is more important. Overall, components of RNS provide complementary benefits, with lower but considerable impact as visual support increases. In Figure 5, we analyze the impact of retrieval mechanism on the performance of RNS. When replacing retrieved 7 Method SAN [85] CAT-Seg [12] LPOSS+ [67] CorrCLIP [90] DINOv3.txt [65] + SAM + RNS = 1 + RNS = 20 Fully Supervised Training set Annot. VOC City ADE C-59 Food CUB Avg. COCO COCO Domain Domain Domain 118k 118k 66 964 20k 82.5 62.4 76.4 31.3 73.2 82.1 47.0 37.9 49.9 39.3 59.1 61.7 32.1 37.9 22.3 28.8 27.7 37.3 47. 57.7 63.3 38.6 47.9 36.3 52.7 62.5 24.5 33.3 26.1 27.2 42.8 52.2 19.3 22.9 12.0 5.8 34.0 65. 47.8 33.2 27.9 49.9 61.9 90.4 87.0 63.0 70. 45.1 84.6 73.4 Table 2. OVS vs. fully supervised segmentation. Fully supervised: best method picked per dataset. : self-evaluated, : from CAT-Seg. Domain: using annotations from each training set. Annot: the number of pixel-level semantic segmentation annotations that each method uses. For domain we report the ADE annotations. Full table and details in the supplementary. obtained when combining the pretrained backbone weights from the latter experiment with RNS by adapting the linear classifier during test-time. This suggests again: (i) the complementary to the visual support gains from the use of textual supervision with our method (ii) the efficacy of our test-time training on test-image relevant support compared to offline training on the entire support. Personalized segmentation. In Figure 7, we qualitatively demonstrate that the dynamic expandability of the support set in RNS enables personalized segmentation of particular object instances within class. Starting from support set that allows RNS to segment pixels of class, e.g., plate, appending only few examples of particular instance, e.g., plate with kingfisher / my plate, enables the model to distinguish that instance from the broader class. We observe that RNS accurately segments partially occluded personalized objects, such as the skirt with tropical pattern. Failure cases remain; e.g., RNS incorrectly segments part of an orange towel as swimsuit, likely due to insufficient contextual information and reliance on color cues. Nevertheless, results illustrate that RNS is effectively employed for personalized segmentation without any modifications. Bridging the gap. In Table 2, we compare different OVS methods, including RNS, and fully supervised ones (closed vocabulary). We consider methods from four categories (i) SAN and CAT-Seg as OVS methods that train on COCOStuff, (ii) LPOSS and CorrCLIP as training free OVS methods, (iii) RNS as an OVS method that uses support sets constructed from annotated images, (iv) fully supervised methods pre-trained offline on closed set of categories. RNS with = 20, narrows down the gap to fully supervised segmentation to 11.5 on average, improving the zero-shot baseline by 34. RNS also surpasses the second best OVS method, CAT-Seg, by 14.1, even though it uses much less pixel-level annotations. The improvement is more evident on fine-grained datasets like CUB and Food that are far from the domain of COCO-Stuff, which serves as training set for many OVS methods. Figure 7. RNS for personalized segmentation. We append examples of specific instance to the support. Green: personalized instance, red: generic class, and black: background. Support sets shown in the supplementary. visual support feature set Vr (6) with random subset of performance degrades lot, confirming that using similar image regions is crucial. Then, we perform the default retrieval (6) but instead of keeping Vr, we focus on retrieved classes Cr, and form set of visual support features from the retrieved classes VCr = (cid:83)M i=1{v : Cr}. We perform the test-time adaptation using different subsets of this set. Using the full set VCr yields slightly lower performance than the default approach, suggesting that VCr includes irrelevant examples that are filtered out by RNS. Using random subset of VCr performs quite better than random subset of V, indicating that restricting adaptation to semantically relevant classes is beneficial. In contrast, selecting the furthest per query feature examples from VCr results in the worst performance, even below random sampling from V. These demonstrate that retrieval in RNS identifies relevant support features, which improves performance significantly. Closed-set comparisons. To further evaluate the effectiveness of RNS, we compare it against fully supervised offline baselines and present results in Figure 6. The offline classifier trained on visual class features performs comparably to RNS w/o text in the = 1 and = 2 scenarios, but its performance deteriorates as the number of images per class increases. This highlights the advantage of test-time retrieval, which dynamically selects the most relevant visual class features from the support set for each test image. Offline methods trained on pixel-level annotations perform poorly in low-shot settings, indicating that such methods require larger amounts of data to avoid overfitting. Freezing the backbone and training only linear classifier with pixellevel cross entropy loss underperforms RNS in all shots, suggesting that our method and objective are more effective in few-shot support utilization. On the other hand, training the backbone as well results in stronger performance when sufficient support is available, at the cost of additional training computational complexity. The best performance is 5. Conclusion We introduce RNS, retrievalaugmented test-time adapter for open-vocabulary segmentation that learns lightweight per-image linear classifier on frozen VLM features by fusing textual class features with retrieved visual support features. The method operates on patches or region proposals, handles full and partial support with single objective, and avoids any type of hand-crafted solutions. Across six benchmarks and two backbones, RNS consistently outperforms all baselines and competitors. Finally, the dynamic support mechanism makes RNS naturally suited for openworld fine-grained tasks, i.e. personalized segmentation."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the Czech Technical UniSGS23/173/OHK3/3T/13, versity in Prague grant No. the EU Horizon Europe programme MSCA PF RAVIOLI (No. 101205297), and the Junior Star GACR GM 21-28830M. We acknowledge VSBTechnical University of Ostrava, IT4Innovations National Supercomputing Center, Czech Republic, for awarding this project (OPEN33-67) access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium, through the Ministry of Education, Youth and Sports of the Czech Republic via the e-INFRA CZ project (ID: 90254). The access to the computational infrastructure of the OP VVV funded project CZ.02.1.01/0.0/0.0/16 019/0000765 Research Center for Informatics is also gratefully acknowledged."
        },
        {
            "title": "References",
            "content": "[1] Ivana Balaˇzevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, and Olivier J. Henaff. Towards in-context scene understanding. In NeurIPS, 2023. 2 [2] Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Training-free openvocabulary segmentation with offline diffusion-augmented prototype generation. In CVPR, 2024. 2, 3, 6 [3] Benedikt Blumenstiel, Johannes Jakubik, Hilde Kuhne, and Michael Vossing. What mess: Multi-domain evaluation of zero-shot semantic segmentation. In NeurIPS, 2023. 2 [4] Walid Bousselham, Felix Petersen, Vittorio Ferrari, and Hilde Kuehne. Grounding everything: Emerging localizaIn CVPR, tion properties in vision-language transformers. 2024. 2 [5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCOIn CVPR, 2018. Stuff: Thing and stuff classes in context. 6 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. [7] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs. In CVPR, 2023. 2 [8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. 1, 2 [9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 1 [10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask In CVPR, transformer for universal image segmentation. 2022. 4 [11] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 6, 1 [12] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In CVPR, 2024. 2, 8, 4 [13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 6, [14] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In CVPR, 2022. 2 [15] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 6 [16] Xinqi Fan, Xueli Chen, Luoxiao Yang, Chuin Hong Yap, Rizwan Qureshi, Qi Dou, Moi Hoon Yap, and Mubarak Shah. Test-time retrieval-augmented adaptation for visionlanguage models. In ICCV, 2025. 3 [17] Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano Mancini, and Elisa Ricci. Frustratingly easy test-time adaptation of vision-language models. In NeurIPS, 2024. 3 [18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, 2022. 2 [19] Zhongrui Gui, Shuyang Sun, Runjia Li, Jianhao Yuan, Zhaochong An, Karsten Roth, Ameya Prabhu, and Philip Torr. knn-clip: Retrieval enables training-free segmentation on continually expanding large vocabularies. TMLR, 2024. 2, 3, 6 [20] Sina Hajimiri, Malik Boudiaf, Ismail Ben Ayed, and Jose Dolz. strong baseline for generalized few-shot semantic segmentation. In CVPR, 2023. [21] Mir Rayat Imtiaz Hossain, Mennatullah Siam, Leonid Sigal, and James Little. Visual prompting for generalized fewshot segmentation: multi-scale approach. In CVPR, 2024. 2 9 [22] Mir Rayat Imtiaz Hossain, Mennatullah Siam, Leonid Sigal, and James Little. The power of one: single example is all it takes for segmentation in vlms. arXiv preprint arXiv:2503.10779, 2025. 2 [23] Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. Retrieval-enhanced contrastive vision-text models. In ICLR, 2024. 2 [24] Klara Janouskova, Tamir Shor, Chaim Baskin, and Jiri Matas. Single image test-time adaptation for segmentation. TMLR, 2024. 3 [25] Hao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 1, [26] Siyu Jiao, Hongguang Zhu, Jiannan Huang, Yao Zhao, Yunchao Wei, and Humphrey Shi. Collaborative vision-text representation optimizing for open-vocabulary segmentation. In ECCV, 2024. 2 [27] Cijo Jose, Theo Moutakanni, Dahyun Kang, Federico Baldassarre, Timothee Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Michael Ramamonjisoa, Maxime Oquab, Oriane Simeoni, Huy V. Vo, Patrick Labatut, and Piotr Bojanowski. Dinov2 meets text: unified framework for imageand pixel-level vision-language alignment. In CVPR, 2025. 6, 1 [28] Dahyun Kang and Minsu Cho. grounding for open-vocabulary semantic segmentation. ECCV, 2024."
        },
        {
            "title": "In defense of lazy visual\nIn",
            "content": "[29] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. In CVPR, 2024. 3 [30] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In ICLR, 2020. 2 [31] Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, and Seong Jae Hwang. Distilling spectral graph for object-context aware open-vocabulary semantic segmentation. In CVPR, 2025. 2 [32] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 2 [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2, [34] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. In ECCV, 2024. 1, 2 [35] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Proxyclip: Proxy attention improves clip for open-vocabulary segmentation. In ECCV, 2024. 1, 2 [36] Youngjun Lee, Doyoung Kim, Junhyeok Kang, Jihwan Bang, Hwanjun Song, and Jae-Gil Lee. RA-TTA: Retrievalaugmented test-time adaptation for vision-language models. In ICLR, 2025. 3 10 [37] Fan Li, Xuanbin Wang, Xuan Wang, Zhaoxiang Zhang, and Yuelei Xu. Images as noisy labels: Unleashing the potential of the diffusion model for open-vocabulary semantic segmentation. In CVPR, 2025. 2 [38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. [39] Yongkang Li, Tianheng Cheng, Bin Feng, Wenyu Liu, and Xinggang Wang. Mask-adapter: The devil is in the masks for open-vocabulary segmentation. In CVPR, 2025. 2 [40] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In CVPR, 2023. 2, 4 [41] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In NeurIPS, 2022. 4 [42] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning customized visual models with retrieval-augmented knowledge. In CVPR, 2023. 2 [43] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Learning orthogonal prototypes for generalized few-shot semantic segmentation. In CVPR, 2023. 2 [44] Yang Liu, Yufei Yin, Chenchen Jing, Muzhi Zhu, Hao Chen, Yuling Xi, Bo Feng, Hao Wang, Shiyu Li, and Chunhua Shen. Unified open-world segmentation with multi-modal prompts. In ICCV, 2025. 2 [45] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. [46] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. In ICML, 2023. 2 [47] Juhong Min, Dahyun Kang, and Minsu Cho. HypercorreIn ICCV, 2021. lation squeeze for few-shot segmentation. 2 [48] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 6 [49] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and Ser-Nam Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In CVPR, 2023. 2 [50] Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers. Test-time adaptation of vision-language models for openvocabulary semantic segmentation. In NeurIPS, 2025. 3 [51] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 2 [52] Valentinos Pariza, Mohammadreza Salehi, Gertjan J. Burghouts, Francesco Locatello, and Yuki M. Asano. Near, far: Patch-ordering enhances vision foundation models scene understanding. In ICLR, 2025. 2 [53] Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, and Giorgos Tolias. Instance-level composed image retrieval. In NeurIPS, 2025. 8 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2 [55] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Incontext retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 2023. 2 [56] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual grouping in contrastive vision-language models. In ICCV, 2023. [57] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. In ICLR, 2025. 2, 5, 6, 1, 7 [58] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding. In ICCV, 2021. 3 [59] Josip ˇSaric, Ivan Martinovic, Matej Kristan, and Siniˇsa ˇSegvic. What holds back open-vocabulary segmentation? In ICCVW, 2025. 1 [60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 1, 2, 6 [61] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. In BMVC, 2017. 2 [62] Yuheng Shi, Minjing Dong, and Chang Xu. Harnessing vision foundation models for high-performance, training-free open vocabulary segmentation. In ICCV, 2025. 2 [63] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Testtime prompt tuning for zero-shot generalization in visionlanguage models. In NeurIPS, 2022. [64] Mennatullah Siam, Boris Oreshkin, and Martin Jagersand. Amp: Adaptive masked proxies for few-shot segmentation. In ICCV, 2019. 2 [65] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 6, 8, 1, 4, 7 [66] Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Dip: Vobecky, Andrei Bursuc, and Nicolas Thome. Unsupervised dense in-context post-training of visual representations. In ICCV, 2025. 3 [67] Vladan Stojnic, Yannis Kalantidis, Jiˇrı Matas, and Giorgos Tolias. LPOSS: Label propagation over patches and pixels for open-vocabulary semantic segmentation. In CVPR, 2025. 1, 2, 8, 4 [68] Shobhita Sundaram, Julia Chae, Yonglong Tian, Sara Beery, and Phillip Isola. Personalized representation from personalized generation. In ICLR, 2025. 2, 8 [69] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrichment network for few-shot segmentation. IEEE TPAMI, 2020. [70] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalized few-shot semantic segmentation. In CVPR, 2022. 2 [71] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understandarXiv preprint ing, arXiv:2502.14786, 2025. 1 localization, and dense features. [72] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense vision-language inference. In ECCV, 2024. 2 [73] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic segmentation with prototype alignment. In ICCV, 2019. 2 [74] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhen Huang, Zijian Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongyang Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In CVPR, 2023. 4 [75] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. [76] Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C. H. Hoi, and Qianru Sun. large-scale benchmark for food image segmentation. In ACM MM, 2021. 6 [77] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and In ICLR, Christian Szegedy. Memorizing transformers. 2022. 2 [78] Monika Wysoczanska, Michael Ramamonjisoa, Tomasz Trzcinski, and Oriane Simeoni. Clip-diy: Clip dense inference yields open-vocabulary semantic segmentation for-free. In WACV, 2024. 2 11 [94] Ziyu Zhao, Xiaoguang Li, Lingjia Shi, Nasrin Imanpour, and Song Wang. Dpseg: Dual-prompt cost volume learning for open-vocabulary semantic segmentation. In CVPR, 2025. 2 [95] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from sequence-to-sequence perspective with transformers. In CVPR, 2021. [96] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, 2017. 6 [97] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In ECCV, 2022. 2, 6, 1, 4 [79] Monika Wysoczanska, Oriane Simeoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzcinski, and Patrick Perez. Clip-dinoiser: Teaching clip few dino tricks for openIn ECCV, 2024. 1, 2, vocabulary semantic segmentation. 4 [80] Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Ling Shao, and Shijian Lu. Cat-sam: Conditional tuning for few-shot adaptation of segment anything model. In ECCV, 2024. 2 [81] Bin Xie, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, and Yanwei Pang. SED: simple encoder-decoder for openvocabulary semantic segmentation. In CVPR, 2024. 2 [82] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In CVPR, 2022. [83] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, and Weidi Xie. Learning open-vocabulary semantic segmentation models from natural language supervision. In CVPR, 2023. 2 [84] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panopIn tic segmentation with text-to-image diffusion models. CVPR, 2023. 4 [85] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabulary semantic segmentation. In CVPR, 2023. 8, 4 [86] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional CLIP. In NeurIPS, 2023. 2 [87] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented incontext learning in multi-modal large language model. In Proceedings of the Robotics: Science and Systems (RSS) 2024, 2024. 2 [88] Maxime Zanella, Benoˆıt Gerin, and Ismail Ben Ayed. Boosting vision-language models with transduction. NeurIPS, 2024. [89] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 1, 2 [90] Dengke Zhang, Fagui Liu, and Quan Tang. Corrclip: Reconstructing patch correlations in clip for open-vocabulary semantic segmentation. In ICCV, 2025. 2, 8, 4 [91] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Personalize segment anything model with one shot. In ICLR, 2024. 2 [92] X. Zhang, W. Zhao, W. Zhang, J. Peng, and J. Fan. Guided filter network for semantic image segmentation. IEEE Transactions on Image Processing, 2022. 4 [93] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 1 Retrieve and Segment: Are Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?"
        },
        {
            "title": "Supplementary Overview",
            "content": "Supplementary material contains: Experimental setup details (Section 6) evaluation protocol (Section 6.1), implementation specifics (Section 6.2), competitor configurations (Section 6.3), and details on offline baseline comparisons (Section 6.4). Additional experimental results (Section 7) outof-domain support (Section 7.1), partial visual support results on finegrained datasets (Section 7.2), backbone comparison (Section 7.3), seen/unseen class analysis (Section 7.4), complete version of Table 2 in the main paper (Section 7.5), runtime comparisons (Section 7.6), and an additional ablation study (Section 7.7). Qualitative results (Section 8) SAM Mask vs. patchlevel prediction comparisons (Section 8.1), qualitative comparisons on considered baselines (Section 8.2 and Section 8.3), and support sets for personalized segmentation results reported in the main paper (Section 8.4). Per dataset results of Figures 3-4 of the main paper are reported in Figure 17, Figure 18, Figure 19 and Figure 20. 6. Experimental setup details 6.1. Evaluation protocol To construct our few-shot benchmark, we sample support images from the training split of each dataset. We create random permutation of the classes. Then we iterate over classes and randomly sample images per class. If an encountered class already appears in previously sampled images times, we skip it. This procedure intentionally over-represents frequent classes, e.g. road in driving scenes, preserving realistic long tail distribution. We create the support set with four random seeds, except for VOC and City that we use eight random seeds, and report average performance. For partial support experiments, we randomly drop visual or textual support for fraction of classes. For partial textual support this corresponds to fraction of test class names, while for partial visual support this corresponds to the annotated visual features of visually supported classes. 6.2. Implementation details We use OpenCLIP ViT-B/16 [11] trained on LAION [60] and apply the MaskCLIP trick [97]. For DINOv3 [65], we use the public ViT-L/16 checkpoint adapted with dino.txt [27] to derive text-aligned patch features, denoted by DINOv3.txt. For SigLIP2 [71], we use the ViT-L/16 Figure 8. Open-vocabulary segmentation (OVS) results. We compare three settings: (i) textual-only support (zero-shot OVS), (ii) simplified version of RNS using visual-only support, and (iii) the full RNS combining textual+visual support. Text-only support leads to ambiguous predictions (tree branch as bird, and various background hallucinations). Visual-only support helps disambiguate some classes but still confuses contextually similar objects (sofachair, tree branchpotted plant). RNS effectively combines both modalities to achieve accurate segmentation. variant trained on images of resolution 512512. For optional region proposals, we use SAM 2.1 [57] Hiera-L with 3232 grid of points (one mask per point) and non-maximum suppression to ensure non-overlap. Pixels not belonging to any mask form an additional separate mask. For support images we extract dense patch features Rnd using sliding window of fixed crop size and stride, down-sample its mask to patch labels [0, 1]nC via label aggregation within each patch region. At inference, we resize images to fixed shorter"
        },
        {
            "title": "Cityscapes",
            "content": "45 40 35 30 ) % ( m 50 40 ) % ( m RNS: Cityscapes ACDC RNS w/o text: Cityscapes ACDC RNS: ACDC ACDC RNS w/o text: ACDC ACDC ACDC Zeroshot RNS: Cityscapes Cityscapes RNS w/o text: Cityscapes Cityscapes RNS: ACDC Cityscapes RNS w/o text: ACDC Cityscapes Cityscapes Zeroshot 1 2 3 10 20 1 2 3 5 10 Images per class Images per class Figure 9. Out-of-domain vs. in-domain visual support between Cityscapes and ACDC. The left plot reports performance on ACDC as we vary the number of visual support images per class from either Cityscapes (out-of-domain, Cityscapes ACDC) or ACDC (in-domain, ACDC ACDC). The right plot reports the analogous results when evaluating on Cityscapes, with support drawn from either ACDC (out-of-domain, ACDC Cityscapes) or Cityscapes (in-domain, Cityscapes Cityscapes). In both cases we compare RNS with and without text, and include the corresponding zero-shot baseline. side size, preserving aspect ratio, and extract dense features using sliding window with fixed crop size and stride. Textual class features tc are obtained with standard CLIP ImageNet-1k templates, e.g., photo of {class} [54]. We average the text features across templates for each class. We do not expand class names beyond this. Regarding RNS, we fix hyperparameters across all experiments to k=4, τ =0.1, βf =1.5, βp=0.2, and Λ = {0.9 , 0.8 , 0.6 , 0.4 , 0.2 , 0.0}. Our model is linear classifier gθ trained per test image. We train for 700 steps with learning rate 0.02, optimizing using Adam [32] in full-batch mode, i.e. no mini-batch stochasticity is involved. 6.3. Details on competitors For kNN-CLIP [19], we follow the official implementation. To ensure fair comparison we fix the hyperparameters across datasets and few-shot settings. The values are chosen based on the average test set performance on the considered benchmarks. The w/o text variant of the method arises by applying =1.0 to the confidence threshold they introduce on zero-shot predictions. Please refer to the original paper for additional information. FREEDA [2] generates images and pseudo-labels using diffusion model, to construct set of weakly labeled synthetic prototypes similar to our per-image visual class features. In their setup, these synthetic features are indexed via descriptive text features. At test time, the text feature of each test class is used to retrieve the most similar text indexes and thus match that class with similar synthetic prototypes. In our case, each per-image visual class feature already has ground-truth label, so the correspondence to test classes is known. We therefore remove the retrieval step and assume oracle access to this matching when adapting their method to real visual support."
        },
        {
            "title": "We fix the hyperparameters of the method in the same",
            "content": "way that we do for kNN-CLIP. The w/o text variant is obtained by applying β=1.0 which is the weight used to linearly combine local visual similarities with global textual similarities. Please refer to the original paper for additional information. In both methods, in the partial text support setup, we replace missing textual class features with the average textual class feature across classes with an available class name. 6.4. Details on comparison to offline baselines In Figure 6 of the main paper we compare RNS with baselines trained in an offline, closed-vocabulary way. To ensure fair comparison we use OpenCLIP ViT-B/16 features and no mask proposer across all methods. For each method we tune learning rate, batch size, and number of training iterations on an 8515% trainvalidation split of each support set. We use the Optuna library to get per-support-set optimal hyperparameter set, based on validation performance, for all methods. To motivate this tuning, Figure 12 compares RNS and the linear offline classifier on per-image visual class features under two fixed hyperparameter settings. The linear classifier is highly sensitive to this choice: performance can differ by more than 30 mIoU, and even collapses under mismatched settings. In contrast, RNS changes only modestly across the two configurations and stays close to its tuned optimal curve. This shows that offline baselines need careful hyperparameter tuning to stay competitive, largely because the effective training set size changes drastically between low-shot settings. In contrast, RNS is much more robust to fixed training settings: by retrieving only support features relevant to each test image, it keeps the effective training set size more stable."
        },
        {
            "title": "Offline Baselines",
            "content": ") % ( m 50 30 RNS: OpenCLIP ViT-B/16 RNS: DINOv3.txt ViT-L/16 RNS: SigLIP2 ViT-L/16 Zeroshot: OpenCLIP ViT-B/16 Zeroshot: DINOv3.txt ViT-L/16 Zeroshot: SigLIP2 ViT-L/16 ) % ( m 60 50 40 30 20 RNS, hyperparameter set 1 RNS, hyperparameter set 2 RNS, optimal linear clf on visual class features, hyperparameter set 1 linear clf on visual class features, hyperparameter set 2 linear clf on visual class features, optimal 1 2 3 5 10 20 1 2 3 10 20 Images per class Images per class Figure 10. Backbone comparison. We compare the performance of RNS using three different visionlanguage backbones (OpenCLIP ViT-B/16, DINOV3.txt ViT-L/16, and SigLIP2 ViT-L/16), along with the corresponding zero-shot baselines for each one of them. 20 ) % ( m RNS RNS w/o wc Zero-shot 100 80 60 40 ) % ( r n t Food CUB 0 0.1 0.3 0.5 0.7 0. 0 0.1 0.3 0.5 0.7 0.9 Fraction w/o visual support Fraction w/o visual support Figure 11. Partial visual (left) and Retrieval quality (right). OpenCLIP (ViTB/16) + SAM 2.1 for region proposals are used. 7. Additional experimental results 7.1. Out-of-domain visual support In Figure 9, we analyze how RNS behaves when the visual support comes from out-of-domain images of the test classes. We use ACDC [58], which provides annotations for the Cityscapes [13] class set but is captured under adverse conditions (fog, snow, rain, night). Out-of-domain support is consistently weaker than in-domain support, yet it still yields substantial gains over zero-shot segmentation and continues to improve as we add more visual examples. Remarkably, when evaluating on Cityscapes, RNS improves over the zero-shot baseline even when using ACDC as visual support, despite the fact that many ACDC scenes have ambiguous semantics (e.g., sidewalks completely covered by snow). 7.2. Partial visual support on fine-grained datasets. In Figure 11 left we present partial visual support performance in Food and CUB. On the right plot we present the retrieval error (percentage of retrieved instances from classes that are not present in each test image). Even for high retrieval errors, RNS effectively takes advantage of support examples and outperforms zero-shot. Moreover, Figure 12. Comparison to offline baseline with and w/o hyperparameter tuning. We compare RNS against the offline linear classifier trained on per image visual class features on VOC. We include the curves presented in Figure 6 that use hyperparameter tuning per and support seed (noted as optimal). We report the performance of both methods on two hyperparameter configurations: hyperparameter set 1 which corresponds to an optimal set of hyperparameters of the linear classifier for = 1, and hyperparameter set 2 which corresponds to an optimal set of hyperparameters of the linear classifier for = 20. Method 10% unseen 50% unseen 90% unseen S S Zero-shot RNS (Partial) RNS (Full) 52.13 72.55 71.83 59.61 62.80 77.89 52.85 71.63 72.41 56.24 76.39 77.42 49.12 50.01 66.89 52.85 63.83 72. 69.08 73.73 88.98 50.14 49.54 69.65 52.85 52.04 72.41 (a) VOC Method 10% unseen 50% unseen 90% unseen S S Zero-shot RNS (Partial) RNS (Full) 21.87 34.30 34.95 31.49 30.19 36.05 22.83 33.89 35.06 21.82 32.09 35.47 23.85 22.49 34.65 22.83 27.29 35. 22.06 27.10 36.18 22.92 22.56 34.94 22.83 23.01 35.06 (b) ADE20K Table 3. Partial seen/unseen classes. We report the performance of RNS under varying fractions of unseen classes (classes w/o visual support) on VOC (a) and ADE20K (b). Columns show mean IoU on seen (S), i.e. classes with visual support, unseen (U), i.e. class that lack visual support, and all (A), i.e. the entire class set. We report the zero-shot (no class visually supported) and RNS with full visual support (all classes visually supported) on the same class sets as reference. the class relevance weights defined in Eq. 8 of the main manuscript suppress the loss of retrieved instances irrelevant to the test image, resulting in significant improvement in such scenarios (Figure 11 left - curves w/o wc). 7.3. Backbone comparison Figure 10 compares RNS across three vision-language backbones. DINOv3.txt ViT-L/16 achieves the highest mIoU and benefits the most from additional visual support, while SigLIP2 ViT-L/16 follows closely. OpenCLIP ViTB/16 performs consistently lower but still improves steadily"
        },
        {
            "title": "Method",
            "content": "Training set Annot."
        },
        {
            "title": "Backbones",
            "content": "VOC City ADE C-59 Food CUB Avg. ODISE [84] OVSeg [40] SAN [85] CAT-Seg [12] LPOSS+ [67] CLIP-DINOiser [79] CorrCLIP [90] MaskCLIP [97] + SAM + RNS = 1 + RNS = 20 DINOv3.txt [65] + SAM + RNS = 1 + RNS = 20 InternImage [74] SETR [95] GFN [92] Mask2Former [10] Fully Supervised COCO COCO COCO COCO Domain Domain"
        },
        {
            "title": "Domain\nDomain\nDomain\nDomain\nDomain",
            "content": "118k 118k 118k 118k 66 964 66 964 20k Stable Diffusion v1.3, CLIP (ViT-L/14) CLIP (ViT-L/14), Swin-B CLIP (ViT-L/14) CLIP (ViT-L/14) OpenCLIP (ViT-B/16), DINO (ViT-B/16) OpenCLIP (ViT-B/16), DINO (ViT-B/16) OpenCLIP (ViT-H/14), DINO (ViT-B/8), SAM2.1 (Hiera-L) OpenCLIP (ViT-B/16), SAM2.1 (Hiera-L) OpenCLIP (ViT-B/16), SAM2.1 (Hiera-L) OpenCLIP (ViT-B/16), SAM2.1 (Hiera-L) DINOv3 (ViT-L/16), SAM2.1 (Hiera-L) DINOv3 (ViT-L/16), SAM2.1 (Hiera-L) DINOv3 (ViT-L/16), SAM2.1 (Hiera-L) InternImage-H SeTR-MLA ResNet-101 DINOv3 (7B) Best of Above 84.6 82.5 62.4 62.1 76.4 54.4 68.6 76.2 31.3 73.2 82.1 90.4 90.4 47.0 37.9 31.7 49. 37.1 46.1 52.5 39.3 59.1 61.7 87.0 76.7 86.7 87.0 29.9 29.6 32.1 37.9 22.3 20.0 28.8 22.9 28.6 38.5 27.7 37.3 47.8 62.9 48.6 63.0 63.0 57.3 55.7 57.7 63.3 38.6 35.9 47. 38.0 45.9 54.4 36.3 52.7 62.5 70.3 70.3 16.4 24.5 33.3 26.1 23.0 28.1 37.2 27.2 42.8 52.2 45.1 45.1 14.0 19.3 22.9 12.0 15.0 24.9 50.6 5.8 34.0 65.2 84.6 84.6 47.8 33.2 31.7 40.4 51.6 27.9 49.9 61.9 73.4 Table 4. OVS vs. fully supervised segmentation. Fully Supervised: best method picked per dataset. : self-evaluated, : from CAT-Seg. Mask2Former numbers from DINOv3 [65] paper. Domain: using annotations from each training set. Annot: the number of pixel-level annotated images that each method uses. For Domain we report the ADE annotations. with more support images. In all cases, RNS significantly surpasses the respective zero-shot baselines, showing that the method is effective across backbones of different capacity and training regimes. 50 40 30 100 iter. 30 iter. 300 iter. 500 iter. 700 iter. RNS kNN-CLIP Zero-shot 0.1 0. 0.3 0.4 0.5 0.6 0.7 0. 0.9 Figure 13. Average performance (mIoU) vs. inference time (s). DINOv3.txt, patch-level; B=5. Avg. on VOC, ADE, Stuff. single NVIDIA A100 GPU is used. 7.4. Performance on seen/unseen classes In Table 3 we report the performance of our partial visual support setup by separately measuring accuracy on seen classes (those with visual support) and unseen classes (those without support). The tables show that, when only small fraction of classes is unseen, our partially supported variant performs on seen classes almost identically to RNS with full support. As the proportion of unseen classes increases, widening gap appears. This is expected: as the number of unseen classes increases so does the number of falsepositives for unseen classes, which impacts the performance on seen ones too. At the same time, performance on unseen classes remains close to the zero-shot baseline across all settings. This confirms that RNS does not degrade zero-shot behavior"
        },
        {
            "title": "Method",
            "content": "B = 1 = 5 = 10 RNS (K = 1) RNS (K = 4) RNS (K = 16) RNS (K = 32) 40.02 -1.57 41.59 41.97 +0.38 42.02 +0.43 46.83 -1.04 47.87 47.78 -0.09 47.59 -0. 48.06 -0.96 49.02 48.96 -0.06 48.84 -0.18 Table 5. Ablations of the k-NN retrieval hyperparameter in RNS. We report average mIoU across the considered datasets for three different numbers of available support images per class (B = 1, = 5, = 10). The row with = 4 (highlighted) corresponds to the configuration used in RNS, and blue numbers denote the difference to this row in the same column. for classes without visual supportwhen support is absent, the model naturally falls back to its zero-shot capabilities. 7.5. Full OVS vs fully supervised table Table 4 is complete version of Table 2, presented in the main paper. We augment the table with column that mentions the backbones used within each method. We include additional OVS methods from the three categories mentioned in the main manuscript. We also include the individual fully supervised methods reported on each dataset. 7.6. Efficiency vs Runtime comparison. In Figure 13 we compare the performance and inference time of RNS, using different number of training iterations, with feedforward methods like kNN-CLIP and zero-shot baseline. We observe that the performance of RNS is robust under less iterations, while efficiency becomes comparable to feedforward kNN-CLIP. Thus, the overhead of RNS can be reduced while still achieving quite performance boost. 4 rigid fusion often struggles when the modalities conflict or when one is unreliable. In contrast, RNS learns how to fuse text and visual support, adapting to each image and yielding cleaner boundaries and fewer semantic confusions. 8.4. Personalized segmentation In Figure 16 we show the visual support sets used to perform personalized segmentation in Figure 7 of the main paper. 7.7. Ablation on in kNN retrieval In Table 5, we study the effect of the kNN retrieval hyperparameter K. Using single neighbor (K = 1) per patch/region leads to clearly lower performance across all budgets B, confirming that aggregating information from multiple retrieved exemplars is beneficial. For 4, performance varies only marginally, with = 416 achieving very similar mIoU, indicating that our method is robust to the exact choice of once it is larger than 1. We therefore use = 4 as our default setting. 8. Qualitative results 8.1. SAM mask vs patch-level predictions In Figure 14 we qualitatively compare two variants of RNS: one where the test-time linear layer is applied directly to patch-level features (RNS + Patch), and one where it operates on mask embeddings obtained by pooling features within SAM2.1 mask proposals (RNS + SAM). As expected, SAM2.1 proposals follow object boundaries much more closely than fixed patches, which yields noticeably sharper segmentation masks. Aggregating features within each mask also provides form of spatial denoising, suppressing spurious patch-level predictions. SAM further exhibits stronger notion of objectness, grouping together parts of the same object even under challenging appearance changes (e.g., shadows in the second row). At the same time, SAM does not always respect the semantic granularity required by the task and can overor under-segment regions. In the bottom three rows, this leads to masks that merge distinct semantic regions or split single objects into multiple segments, introducing ambiguity despite the improved alignment with image structure. 8.2. Unimodal vs. multimodal support In Figure 8 and Figure 15 we qualitatively compare methods that rely only on text (Zero-shot), only on visual support (RNS w/o text), or on both modalities (RNS). Using only class names often produces semantically ambiguous labels when several categories share similar appearance or context (e.g., house vs. building in the first row, wall vs. building in the third row of Fig. 15). Conversely, relying only on visual exemplars can confuse contextually similar objects (e.g., train vs. bus in the fourth row). RNS leverages both textual and visual support: text provides semantic prior that separates related categories, while visual examples anchor this prior to the image appearance, leading to more accurate segmentations across diverse scenes. 8.3. Comparisons with visually supported methods In Figure 15 we additionally compare RNS to FREEDA and kNN-CLIP, which also use visual exemplars but rely on fixed, handcrafted fusion of text and visual cues. Such"
        },
        {
            "title": "Image",
            "content": "GT"
        },
        {
            "title": "Patch feats",
            "content": "RNS + Patch"
        },
        {
            "title": "SAM mask feats",
            "content": "RNS + SAM Figure 14. Qualitative comparison of patch-level vs. mask-level segmentation. Each row shows the input image, ground-truth mask, the PCA projection of either patch features or SAM mask features (average VLM patch features on top of SAM mask), and the corresponding predictions of RNS when applied on patches or on region proposals. Both variants use DINOv3.txt features [65], and SAM 2.1 is used as the mask proposal generator [57]."
        },
        {
            "title": "Image",
            "content": "GT Zero-shot"
        },
        {
            "title": "FREEDA",
            "content": "kNN-CLIP RNS w/o text"
        },
        {
            "title": "RNS",
            "content": "Figure 15. Qualitative comparisons between zero-shot baseline, FREEDA, kNN-CLIP and RNS with and without class name information. All visually supported methods use one support image per class (B=1). All methods use OpenCLIP ViT-B/16 as VLM features [65] and SAM 2.1 as region proposal generator [57]. 7 Figure 16. Visual support sets for personalized segmentation. RNS used for personalized segmentation using OpenCLIP ViT-B/16 features and SAM 2.1 as region proposer. Initially, visual support includeds images in support before and is later expanded to support after, with support before support after. Green: personalized instance, red: generic class, and black: background. Images from PODS [68], i-CIR [53], and self-collected. 8 ) % ( m ) % ( m 70 60 50 40 50 40 40 30 ) % ( I m"
        },
        {
            "title": "ADE",
            "content": ") % ( m 35 25 RNS kNN-CLIP FREEDA Zero-shot with text w/o text RNS kNN-CLIP FREEDA Zero-shot with text w/o text 1 2 3 5 20 1 2 3 5 10 20 Support images per class"
        },
        {
            "title": "City",
            "content": "Support images per class"
        },
        {
            "title": "Stuff",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o text 1 2 3 5 10 20 Support images per class"
        },
        {
            "title": "Object",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o text 35 30 25 20 40 30 ) % ( m ) % ( m RNS kNN-CLIP FREEDA Zero-shot with text w/o text 1 2 5 10 20 Support images per class"
        },
        {
            "title": "Context",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o text 1 2 3 5 10 20 1 2 5"
        },
        {
            "title": "Support images per class",
            "content": "Figure 17. Full textual and visual support per dataset (OpenCLIP, ViT-B/16). We compare zero-shot, RNS, kNN-CLIP and FREEDA and their variants without class name information (w/o text) for increasing number of support images per class. SAM 2.1 is used for region proposals."
        },
        {
            "title": "ADE",
            "content": ") % ( m 40 RNS kNN-CLIP Zero-shot with text w/o text SAM Patch RNS kNN-CLIP Zero-shot with text w/o text SAM Patch 1 2 3 5 10 1 2 3 5 10 20 Support images per class"
        },
        {
            "title": "City",
            "content": "Support images per class"
        },
        {
            "title": "Stuff",
            "content": "40 30 ) % ( m RNS kNN-CLIP Zero-shot with text w/o text SAM Patch RNS kNN-CLIP Zero-shot with text w/o text SAM Patch 1 2 3 5 10 1 2 3 5 10 20 Support images per class"
        },
        {
            "title": "Object",
            "content": "Support images per class"
        },
        {
            "title": "Context",
            "content": ") % ( m 50 30 RNS kNN-CLIP Zero-shot with text w/o text SAM Patch RNS kNN-CLIP Zero-shot with text w/o text SAM Patch 80 60 60 50 40 50 40 ) % ( m ) % ( m ) % ( m 1 2 3 5 10 20 1 2 3"
        },
        {
            "title": "Support images per class",
            "content": "Figure 18. Full textual and visual support per dataset (DINOv3.txt, ViT-L/16). We compare zero-shot, RNS, kNN-CLIP and RNS without class name information (w/o text) for increasing number of support images per class. Prediction at the patch level or SAM 2.1 is used for region proposals."
        },
        {
            "title": "ADE",
            "content": ") % ( m 60 20 ) % ( m 20 0 40 ) % ( I 20 0 RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo class vectors w/o text 0 0. 0.3 0.5 0.7 0.9 Fraction of classes w/o visual support"
        },
        {
            "title": "City",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo-label loss w/o text 30 20 10 0 20 10 ) % ( m ) % ( m RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo-label loss w/o text 0.1 0.3 0.5 0.7 0.9 Fraction of classes w/o visual support"
        },
        {
            "title": "Stuff",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo class vectors w/o text 0 0.1 0.3 0.5 0. 0.9 0 0.1 0.3 0.5 0. 0.9 Fraction of classes w/o visual support Fraction of classes w/o visual support"
        },
        {
            "title": "Context",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo-label loss w/o text 40 ) % ( m 20 RNS kNN-CLIP FREEDA Zero-shot with text w/o pseudo class vectors w/o text 0 0.1 0.3 0. 0.7 0.9 0 0.1 0.3 0. 0.7 0.9 Fraction of classes w/o visual support Fraction of classes w/o visual support Figure 19. Partial visual support setting per dataset. Results of zero-shot, RNS, kNN-CLIP, and FREEDA, along with ablations of RNS without text and without the pseudo-label loss. OpenCLIP ViT-B/16 and SAM 2.1 are used. fraction of classes lack visual examples, while = 3 for the remaining classes. 70 60 50 40 46 42 40 38 35 30 ) % ( m ) % ( m ) % ( m"
        },
        {
            "title": "ADE",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o text ) % ( m 26 24 22 RNS kNN-CLIP FREEDA Zero-shot with text w/o text 0 0. 0.3 0.5 0.7 0.9 0 0. 0.3 0.5 0.7 0.9 Fraction of classes w/o textual support Fraction of classes w/o textual support"
        },
        {
            "title": "Stuff",
            "content": "RNS kNN-CLIP FREEDA Zero-shot with text w/o text 25 ) % ( m 20 RNS kNN-CLIP FREEDA Zero-shot with text w/o text 0 0.1 0.3 0. 0.7 0.9 0 0.1 0.3 0. 0.7 0.9 Fraction of classes w/o textual support Fraction of classes w/o textual support"
        },
        {
            "title": "Context",
            "content": "40 35 30 25 ) % ( m RNS kNN-CLIP FREEDA Zero-shot with text w/o text RNS kNN-CLIP FREEDA Zero-shot with text w/o text 0 0. 0.3 0.5 0.7 0.9 0 0. 0.3 0.5 0.7 0.9 Fraction of classes w/o textual support Fraction of classes w/o textual support Figure 20. Partial textual support setting per dataset. Results of Zero-shot, RNS, kNN-CLIP, and FREEDA, together with their variants without class name information (w/o text). OpenCLIP ViT-B/16 and SAM 2.1 are used. fraction of classes lack textual class names, and = 1 for all classes."
        }
    ],
    "affiliations": [
        "Archimedes, Athena RC",
        "IACM-FORTH",
        "University of Crete",
        "VRG, FEE, Czech Technical University in Prague"
    ]
}