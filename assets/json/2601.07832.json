{
    "paper_title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "authors": [
        "Kewei Zhang",
        "Ye Huang",
        "Yufan Deng",
        "Jincheng Yu",
        "Junsong Chen",
        "Huan Ling",
        "Enze Xie",
        "Daquan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 2 3 8 7 0 . 1 0 6 2 : r MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head Kewei Zhang1 Ye Huang1 Yufan Deng1 Jincheng Yu2 Junsong Chen2 Huan Ling2 Enze Xie2 Daquan Zhou1 1Peking University, 2NVIDIA Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving 3.6% improvement on ImageNet classification, 6.3% gain on NLP, 12.6% improvement on image generation, and 41% enhancement on video generation under the same time complexity. Project Page: https://dagroup-pku.github.io/MHLA GitHub Repo: https://github.com/DAGroup-PKU/MHLA Huggingface Repo: https://huggingface.co/DAGroup-PKU/MHLA"
        },
        {
            "title": "Introduction",
            "content": "Self-attention is the core module for the recent dominant model architecture, Transformer, for both computer vision [18], natural language processing [49], and generative tasks [41]. However, its quadratic time and memory complexity severely limit its scalability to long sequence tasks such as high-resolution image generative and video generation tasks [31, 59, 60]. To address the efficiency issue, growing line of research [9, 30] has developed linear attention mechanisms that replace the softmax kernel with associative feature maps. These approaches reduce the computational and memory complexity of attention from quadratic to linear by compressing all keys and values into global summary. Although this improves efficiency, it eliminates one of the key advantages of softmax attentionits ability to adapt to each query individually. Consequently, linear attention often experiences notable accuracy degradation, particularly in long-sequence modeling tasks. Recent works [22, 24, 25] have sought to mitigate the performance degradation of linear attention by integrating components such as depthwise convolutions and gating modules. However, this reliance on external modules introduces additional computational overhead and continues to suffer from performance degradation as 1 Model Attn Type FID DiT-S/2 DiT-XL/2 Linear-Attn Self-Attn MHLA(Ours) Linear-Attn Self-Attn MHLA(Ours) 89.7 68.4 59. 28.63 19.47 19.17 Figure 1 (a) Generation results from our fine-tuned SANA model using MHLA. (b) Performance and efficiency comparison between the proposed MHLA and baselines. The throughput was tested on the NVIDIA H100 Tensor Core GPU. Following the previous method, we report the FID in the table at resolution of 256 256. (c) Multi-domain performance of MHLA. We evaluate MHLA across diverse domains, demonstrating its strong and universal performance. (d) Throughput of DiT-S/2 at 4096 resolution across different devices. All improvements are solely due to MHLA, and can be further combined with orthogonal techniques for even greater speedups. sequence length increases. In this paper, we present solution to the performance bottleneck in linear attention that requires no additional depthwise convolution or self-attention modules. Our key insight is that, in conventional linear attention design, all tokens are compressed into single global keyvalue summary (KV summary) that is shared by every query. This design could have reduced the models representation capacity, as illustrated in Figs. 1b and 2. To evaluate diversity, we compare the rank of the attention weight matrices across different models. We find that using shared global KV summary limits the models capacity to represent rich interactions, effectively capping it at fixed rank. As sequences grow longer, this constraint tends to push the attention weights toward more uniform distribution. In practice, this reduces diversity and degrades performance on tasks where queries must concentrate on small subset of relevant tokens. Our design goal is therefore simple: restore query-dependent diversity, the ability for different queries to retrieve different contexts, without sacrificing linear-time behavior or introducing heavy auxiliary modules. Thus, we introduce Multi-head Linear Attention (MHLA) to achieve the aforementioned characteristics. Specifically, MHLA partitions tokens into non-overlapping blocks (heads in the spatial dimension), computes local key-value summaries, and lets each query block compute query-conditioned mixture over these summaries to retrieve tailored context; within the selected blocks, token contributions are further refined by query-dependent reweighting module. Thanks to the simplicity of MHLA, the implementation only relies on standard GEMMs, keeping the overall computational overhead negligible with O(N ) complexity, retaining compatibility with streaming/stateful execution. It was clearly observed that adding MHLA raise the rank Figure 2 Comparison between the proposed MHLA and other linear attentions. MHLA divides multiple heads on the token dimension. Through Multi-Head Mixing, MHLA restores query-conditioned selectivity by mixing KV summaries with query-specific weight, improving token-level diversity while keeping linear complexity. of the attention weights matrix significantly, as shown in Fig. 3b. The difference between previous linear attentions and MHLA is briefly illustrated in Fig. 2. We validate MHLA on image classification, image generation, natural language processing, and video generation tasks. Experiments show that MHLA consistently outperforms existing linear attention baselines with negligible computational overhead. Our main contributions are summarized as follows: We conduct an in-depth analysis of linear attention and identify one of the root causes of its performance degradation: the absence of grouping along the token dimension during similarity calculation. This limitation can be quantified by examining the rank of the attention matrix. We propose new formulation of linear attention that achieves state-of-the-art performance on both discriminative and generative tasks, while maintaining O(N ) computational complexity and avoiding reliance on additional modules. We conduct extensive experiments across various tasks, achieving state-of-the-art performance. On ImageNet, MHLA delivers 3.6% accuracy gain over self-attention, while on image generation tasks it improves the performance of the DiT architecture by 12.6%. MHLA also achieves 6.3% improvement on natural language processing tasks and provides substantial 41% improvement compared to vanilla linear attention in video generation tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Transformers [49] have advanced various fields [17, 18, 42], but their quadratic time and memory complexity due to self-attention limit scalability, especially for long sequences. To overcome this, linear attention mechanisms [9, 30] have been proposed, which replace softmax with kernel-based methods to achieve linear time complexity. While these mechanisms improve the efficiency, they often lose expressiveness, making them suffer from performance drop in capturing complex token interactions. Several solutions [22, 24], including adding convolutional layers or gating mechanisms, have attempted to recover performance but tend to introduce additional computational costs. See the detailed related works in the Appendix A."
        },
        {
            "title": "3.1 Preliminary",
            "content": "We first formulate the calculation of the attention weights for both self-attention and linear attention mechanism. Given an input token sequence RN d, we first compute queries, keys, and values via = XWQ, = XWK, = XWV , where WQ, WK, WV Rdd are learnable projections. The attention output of the token can be expressed as: Yi = (cid:80)N j=1 Sim(Qi, Kj)Vj m=1 Sim(Qi, Km) (cid:80)N , (1) where Sim(, ) calculates the similarity between the input matrix. In softmax attention [49], Sim(Qi, Kj) = d), all pairwise similarities need to be calculated and normalized per query, resulting in O(N 2) exp(QiK / complexity. Linear attention replaces the exponential kernel with positive feature map ϕ() such that Sim(Qi, Kj) ϕ(Qi)ϕ(Kj), Yi = ϕ(Qi)(cid:0)(cid:80)N ϕ(Qi)(cid:0)(cid:80)N j=1 ϕ(Kj)Vj m=1 ϕ(Km)(cid:1) , (cid:1) (2) where the numerator and denominator can be precomputed as global keyvalue summary = (cid:80) and normalizer = (cid:80) linear-time scaling with sequence length. ϕ(Kj)Vj ϕ(Km), respectively. This reduces the complexity from O(N 2) to O(N dϕ), enabling"
        },
        {
            "title": "3.2 Global Context Collapse",
            "content": "Linear attention achieves linear-time complexity by reusing global keyvalue summary across all queries, which can be formulated as = (cid:80)N j=1 ϕ(Kj)Vj Rdd . But this fixed-size design introduces an intrinsic information bottleneck: Observation As the sequence length increases, the information requiring representation exceeds the capacity of the fixed-size matrix, leading to performance saturation. We term this phenomenon global context collapse. This observation can be quantified using two complementary metrics, which are the rank and the sparsity of the attention matrix: Rank limitation. The rank of the attention matrix has been widely studied as key indicator of feature diversity and representational capacity in attention mechanisms [3, 22, 24]. Specifically, with (cid:101)Q = ϕ(Q) and (cid:101)K = ϕ(K), global linear attention produces Alin = (cid:101)Q (cid:101)K Rnn, rank(Alin) min{rank( (cid:101)Q), rank( (cid:101)K)} d. Conclusion 1 Regardless of , the representational capacity of Alin is strictly bounded by d. Although several prior studies have attempted to increase the rank of KeyValue summaries [5, 22], this bound results in severely rank-deficient approximation of the full attention matrix when d, constraining the models ability to capture diverse, query-conditioned attention patterns. We empirically verify this effect in Fig. 3b, which shows that the rank of attention scores in linear-attentionbased models is consistently capped by the head dimension (typically dh 72), and the relative expressivity of the attention map degrades as the sequence length increases. Loss of sparsity. The sparsity of the attention matrix is critical factor influencing the performance of attention mechanisms. Sparse distributions generally exhibit lower entropy, concentrating probability mass on smaller set of informative tokens [15, 57], which benefits model optimization. Linear attention, however, computes scores by first compressing all keyvalue pairs into single global summary, and each query interacts with this shared representation only once. In contrast, softmax attention leverages the exponential function to enable each query qi to produce distinct distribution over tokens (see Appendix B). Because linear attention relies on the same aggregated representation for all queries, it cannot reweight individual keys according to query-specific relevance. Conclusion 2 As the sequence length increases, the contribution of each token becomes negligible. Consequently, the attention weight distribution approaches uniformity, reducing the sparsity and impairing the models ability to selectively emphasize informative tokens. To quantify this effect, we compute the average entropy of the attention scores over 500 random samples for each attention variant. For each row of the attention score matrix, lower entropy indicates that the distribution is closer to one-hot vector, reflecting stronger concentration on single token. As shown in Fig. 3a and Fig. 3b, linear attention exhibits significantly higher entropy, confirming its lack of focus compared to softmax-based attention. Figure 3 (a) Visualization of attention score and attention maps of MHLA and baselines. (b) Average rank and entropy of attention scores for DeiT-T, showing MHLA yields richer and more focused attention. (a) (b) Taken together, these findings reveal that the reliance on single global keyvalue summary in linear attention leads to severe collapse in representational capacity, manifested as both rank deficiency and elevated entropy in the attention map. We refer to this phenomenon as global context collapse. Fig. 3a visualizes attention scores and maps, clearly illustrating the inability of linear attention to capture fine-grained information. This observation motivates the development of methods that restore query-conditioned token-level diversity while preserving the linear-time complexity of the attention mechanism, which was detailed in the next section."
        },
        {
            "title": "4.1 Overview\nHere we formalize the proposed Multi-Head Linear Attention (MHLA). As shown in Fig. 4a. MHLA operates\nby splitting the sequence along the token dimension into multiple \"heads\" and running linear attention\nin parallel across these \"heads\". Let the input sequence be X ∈ RN ×d, projected to queries, keys, and\nvalues: Q = XWQ, K = XWK, V = XWV , with Q, K, V ∈ RN ×d. For efficiency, we adopt a kernelized\nformulation, denoting (cid:101)Q = ϕ(Q), (cid:101)K = ϕ(K) for a chosen feature map ϕ(·).\nStandard linear attention aggregates all tokens into a single global d × d summary shared by every query,\nwhich reduces expressivity by collapsing token-level diversity. To mitigate this, we split the sequence into\nM non-overlapping blocks (the MHLA “heads”), with block b containing Nb tokens and (cid:80)M\nb=1 Nb = N . In\npractice on vision models, blocks are defined on spatial (2D) or spatiotemporal (3D) grids rather than by\nflattening to 1D. For each block b we compute a local key–value summary and its normalizer:",
            "content": "Sb = (cid:88) jb (cid:101)KjV Rdd, zb = (cid:101)Kj Rd. (cid:88) jb (3) To restore query adaptivity, MHLA constructs distinct mixture of all keyvalue summaries for each query block through Multi-Head Mixing. Queries in block can then attend to this mixture, where different keyvalue summaries are weighted according to the attention preferences of the current query block. Let mi RM denote the nonnegative, learnable mixing coefficients for block i, which are optimized during training. The mixed summaries are then defined as (cid:101)Si = (cid:80)M b=1 mi,b Sb, and the corresponding normalizer is (cid:101)zi = (cid:80)M b=1 mi,b zb. 5 Figure 4 (a) Overview of the proposed Multi-Head Linear Attention. (b) We visualize two rows of the initialized Learnable Coefficient Matrix corresponding to Block 1 and Block 14 separately when is 25. We reshape the two rows and the dimension in 2D for better understanding. (a) (b) The process can be done with highly hardware-efficient GEMM operation between keyvalue summaries and coefficient matrix Mc RM consisting of mi. Given query vector (cid:101)q Rd from block i, the output is = (cid:101)q (cid:101)Si (cid:101)q (cid:101)zi = (cid:80)M (cid:80)M b=1 mi,b (cid:101)qSb b=1 mi,b (cid:101)qzb . (4) Each output element can thus be interpreted as query-specific, block-dependent recombination of the entire value sequence. In tasks like language modeling and video generation, the normalizer term can be omitted for better training stability [39] when the sequence is getting longer."
        },
        {
            "title": "4.2 Multi-Head Mixing\nThe core of MHLA’s adaptivity is a learned coefficient matrix Mc ∈ RM ×M . The element at position (i, j)\ndenotes the affinity between query-block i and the local key–value summary of block j. Equivalently, the\ni-th row of Mc, denoted mi, specifies how query-block i linearly combines the M local summaries into a\nquery-specific global summary.",
            "content": "Each row mi is produced and learned end-to-end; in practice we enforce nonnegativity and normalization. Because blocks are defined along spatial or spatiotemporal axes, we initialize Mc to favor locality: for row we set initial coefficients as m(0) i,j 1 dist(i, j)/ maxk(dist(i, k)), where dist(i, j) measures the Euclidean distance and maxk dist(i, k) is the maximum distance from to any position k. The coefficients are then normalized such that (cid:80) i,j = 1. visualization of this initialization can be found in Fig. 4b. This locality-biased initialization produces more stable and faster convergence while leaving Mc free to adapt during training. To further ensure stability, we clip the coefficients to the interval (0, 1) on every update. m(0) The token-level effect of the Multi-Head Mixing is transparent. Let b(t) denote the block index of token t. Writing each local summary as sum over its tokens, Gj = (cid:80) , the mixture for query-block expands to tblock (cid:101)KtV (cid:101)Si = (cid:88) j=1 mi,jSj = (cid:88) t=1 mi,b(t) (cid:101)KtV Rdd. 6 For query vector (cid:101)q = ϕ(q) (from block i), the numerator of the kernelized update becomes (cid:101)q (cid:101)Si = (cid:88) t= mi,b(t) (cid:0) (cid:101)q (cid:101)Kt (cid:1)V Rd. (5) Eq. 5 makes the mechanism transparent: each query-block rescales the contribution of entire blocks via mi, and within each block the usual kernel inner product (cid:101)q (cid:101)Kt differentiates tokens. Thus, MHLA restores query-conditioned, token-level weighting in two-stage manner (block selection intra-block reweighting). Importantly, all operations reduce to blockwise summary computation and linear combinations of matrices of size d, so asymptotic complexity remains linear in while expressive capacity is substantially increased. Chunkwise parallel form of MHLA. Linear attention commonly employs chunkwise parallel training [29, 46] to maintain linear-time complexity under causal masking, by partitioning the sequence into blocks and updating running summary per block. MHLA naturally fits this setting: each head can be directly mapped to chunk, and we maintain one local summary Sb per chunk. At training time, we aggregate these local summaries using the learned mixture coefficients mi,b to form the mixed prefix summary (cid:101)Si = (cid:80) bi mi,bSb, which is then used for block-level attention. Because mixture computation is performed once per block and reused for all queries in that block, the overall complexity remains identical to chunkwise linear attention. For detailed derivation and the corresponding inference procedure, see Appendix C."
        },
        {
            "title": "4.3 Analysis of Multi-Head Linear Attention\nRank analysis. Partition the sequence into M non-overlapping blocks of size Nb. Let the query matrix\nM ]⊤ with (cid:101)Qb ∈ Rnb×d. From Eq. 5, in the calculation of attention score, the mixed key\nbe (cid:101)Q = [ (cid:101)Q⊤\nsequence seen by query-block i can be expressed as",
            "content": "1 , . . . , (cid:101)Q Yi = (cid:2) mi,b(1)k1, mi,b(2)k2, . . . , mi,b(n)kn (cid:3) Rdn, where mi,b(t) is the mixing coefficient selecting the block of token t. The attention submatrix contributed by query-block is Ai = (cid:101)QiYi RNbN , and the full attention matrix is AMHLA = (cid:2)A1 A2 AM Rnn. Then applying standard rank inequalities gives (cid:3) n, (cid:80)M (cid:17) . b=1 min(nb, d) rank(Ab) min(cid:8)rank( (cid:101)Qb), rank(Yb)(cid:9) min(nb, d), (cid:16) which yields the global bound rank(AMHLA) min This upper bound is attainable under mild, generic conditions: if each block product (cid:101)QbYb has full row rank rb = min(nb, d) and the row spaces of { (cid:101)QbYb}M are linearly independent, then we get rank(AMHLA) = b=1 min(n, (cid:80)M b=1 rb). Even when the independence assumption is not fully satisfied, the blockwise mixture still expands the diversity of the row spaces, causing rank(AMHLA) to grow roughly additively with . We empirically validate this behavior in Fig. 3b, where MHLA consistently achieves substantially higher attention-score rank than other linear attention variants and does so without relying on auxiliary components such as depth-wise convolutions. This confirms that MHLA natively restores much of the representational capacity lost in global linear attention, whose rank remains strictly limited by regardless of the sequence length . Sparsity analysis. The learned coefficient matrix Mc allows each query-block to assign higher weights to subset of blocks that are more relevant, effectively pruning irrelevant tokens at the block level. Within each selected block, the kernel inner products (cid:101)q (cid:101)Kt further differentiate token contributions, leading to sharper and more concentrated attention distributions. We validate this effect empirically in Fig. 3b, where MHLA consistently yields lower attention entropy compared to other linear-attention baselines and even the softmax attention. This confirms that MHLA preserves query-conditioned selectivity and achieves substantially higher sparsity, enabling the model to attend to small, semantically relevant subset of tokens rather than spreading attention uniformly. 7 Table 1 Comparison between Self Attention, Linear Attention, and MHLA. We report computation complexity, maximum achievable rank, memory complexity and query-conditioned selectivity. Method Time Complexity Rank Bound Memory Complexity Query-Conditioned Self Attention Linear Attention MHLA (ours) O(N 2d) O(N d2) O(N d2 + 2d2) (cid:80)M b=1min(nb, d) O(N 2) O(d2) O(M d2) Table 2 Comparison on Image Classification task. MHLA achieves the best accuracy with minimal parameter overhead on DeiT models, and outperforms Transformer-, LA-, and Mamba-based SOTAs. Results marked with an * are reproduced under the same training setup as MHLA-VLT. (a) Comparison of different attentions on DeiT. (b) Comparison with SOTA models on ImageNet-1K. Attention Type Params FLOPs Top1-ACC Cost Model Params FLOPs Top1-ACC Comparison on Deit-T Setting Self Attn Linear Attn Focused LA [24] Inline Attn [25] MALA [21] MHLA (Ours) 5.7M 5.7M 6.1M 6.5M 6.3M 5.7M 1.1G 1.1G 1.1G 1.1G 1.1G 1.1G Comparison on Deit-S Setting Self Attn Linear Attn RALA [22] MALA [21] MHLA (Ours) 22M 22M 24M 24M 22M 4.2G 4.2G 4.6G 4.6G 4.2G 72.2 69.8 74.1 74.5 75.1 75.8 79.8 77.6 80.4 80. 81.0 5 . 2 5 . 4 FL-PVT-T [24] FL-PVTv2-B1 [24] MSVMamba-M [45] NAT-M [26] RAVLT-T [22] MAViT-T [21] MHLA-VLT-T FAT-B3 [20] Vmamba-T [32] MV-T [27] MSVMamba-T [45] MAViT-S [21] MHLA-VLT-S 12M 13M 12M 20M 15M 16M 16M 29M 30M 32M 32M 27M 27M 2.0G 2.2G 1.5G 2.7G 2.4G 2.5G 2.4G 4.4G 4.9G 4.4G 5.1G 4.6G 4.6G 77.8 79.5 79.8 81.8 82.3 82.4 82.6 83.6 82.6 82.3 83.0 84.3 84.6 Efficiency analysis. The computation of MHLA consists of local Keyvalue summary computation, MultiHead Mixing, and output computation, with time complexity of O(cid:0)M Nbd2 + 2d2 + Nbd2(cid:1) = O(N d2 + 2d2). To better capture local information while ensuring efficiency, the number of blocks is usually set to satisfy 2 . Therefore, d2 becomes the leading term and the time complexity of MHLA is O(N d2). The comparison of self attention, linear attention, and MHLA is summarized in Tab. 1. We also provide an empirical analysis of the scaling relationship between and in Appendix F.4 that verifies the induced complexity."
        },
        {
            "title": "5.1\nSettings. We adopt the training configurations from prior work [21, 22, 47]. The proposed MHLA is integrated\ninto two representative architectures, DeiT [47] and VLT [22], across multiple model scales. The models are\ntrained on ImageNet-1K [14]. For VLT, we strictly follow the setup in [22]. All models are trained for 300\nepochs with a batch size of 1024 and a peak learning rate of 1e-3. For models with an input size of 224, we\npad the input size to 256 for better splitting of heads. The head number M is set to 16 if there is no extra\ndescription. See Appendix E for more details.",
            "content": "Results. We evaluate the pretrained DeiT models described above and report the result in Tab. 2a, which clearly shows the superior performance of the proposed MHLA. We reach the best accuracy in linear attention across all model sizes, while introducing the fewest extra parameters compared with baselines. We then port the proposed MHLA to VLT [22] and evaluate the performance under the same settings. The results are shown in Tab. 2b, illustrating the proposed MHLAs state-of-the-art performance with consistent improvements compared with baseline models."
        },
        {
            "title": "5.2 Image Generation\nSettings. 1) For Class-to-Image(C2I) generation, we train DiT [34] and DiG [61] from scratch for 400k steps\non ImageNet-1K [14] with batch size 256 and learning rate 1e-4, following their original settings. We evaluate\nfive variants in DiT and DiG, where the original self-attention (DiT) or GLA [54] (DiG) is replaced by our\nMHLA while keeping other components unchanged. The head number is set to 16 for both 256 and 512\nresolutions. We try extra CPE [10] and the output gating module [54]. Their effects are analyzed in Appendix\nF.2. 2) For Text-to-Image(T2I) generation, we finetune a Sana-0.6B [52] model from official checkpoint. Both\nthe original linear attention and our MHLA variant are trained for 40k steps with a batch size of 256.",
            "content": "C2I results. The main quantitative results are summarized in Tab. 3a, where our method consistently achieves state-of-the-art performance across all DiT model sizes. In addition, Fig. 1b compares the throughput of our MHLA with baseline attention mechanisms on DiT-S as the input resolution increases. Notably, MHLA maintains throughput nearly identical to linear attention while delivering performance on par with, or even surpassing, self-attention. At 512 resolution, MHLA achieves better FID scores while doubling the throughput of self-attention. To further demonstrate the fast adaptation ability of our approach to existing models, we fine-tune the pretrained DiT-XL/2 model for 400k steps under the same settings. As shown in Tab. 3b, our model achieves lower FID score than DiT-XL/2 without classifier-free guidance (CFG), and delivers comparable performance when CFG is applied. Full results of the experiments on C2I generation can be found in Appendix F. Analysis. Although we add modules such as DWConv (CPE) [22] to smaller DiT models, it is worth noting that their benefits diminish as model size increases (CPE even degrades performance on DiT-XL). As shown in Tab. 3a, plain MHLA already matches the performance of self-attention on XL models, while adding CPE leads to regression. These results highlight the intrinsic advantage of MHLA and suggest that, although modules like DWConv may offer gains at small scales, their benefits do not scale with model size or sequence length. Fast adaptation to SANA. As shown in Tab. 4, replacing linear attention with MHLA consistently improves multiple evaluation metrics, surpassing not only the baseline Sana model but also the PixArt [6] series. Fig. 5 further visualizes the training loss curves. The MHLA-based model rapidly adapts, matching the pretrained checkpoint within the first 2k steps and subsequently converging to lower loss. This demonstrates MHLAs fast adaptation capability and promising performance at larger model scale. Table 3 Class-to-Image Generation. Across all model sizes, MHLA achieves the best performance. Notably, at and XL scales, it matches self-attention performance without relying on any extra modules. (a) Comparison of attention types across models. Model Attention Type Resolution DiT-S/2 DiG-S/2 DiT-B/ DiT-L/2 DiT-XL/2 Self Attention Linear Attention MHLA (Ours) Self Attention Linear Attention MHLA (Ours) GLA [54] GLA MHLA (Ours) Self Attention Linear Attention MHLA (Ours) Self Attention Linear Attention MHLA (Ours, w/None) MHLA (Ours, w/ CPE) MHLA (Ours, w/ CPE+Gating) Self Attention Linear Attention MHLA (Ours, w/ None) MHLA (Ours, w/ CPE) MHLA (Ours, w/ CPE+Gating) 256 256 256 512 512 512 256 512 256 256 256 256 256 256 256 256 256 256 256 256 256 FID 68.40 89.72 59.80 84.54 125.33 78.63 62.06 99.04 59.49 43.47 60. 37.47 23.33 32.35 25.37 24.21 21.37 19.47 28.63 20.32 22.79 19.17 (b) Fast adaptation results on DiT-XL/2. Model DiT-XL/ Attention Type FID 9.62 Self Attention MHLA (Ours) DiT-XL/2(G) Self Attention MHLA (Ours) IS 121.50 121.27 278.24 252.07 sFID 6. 5.52 4.60 4.67 8.34 2.27 2."
        },
        {
            "title": "5.3 Video Generation\nVideo generation involves extremely long sequence lengths, where quadratic attention becomes prohibitively\nslow. To evaluate MHLA under such ultra-long contexts, we fine-tune a pretrained Wan2.1-1.3B model by\nreplacing its FlashAttention modules with MHLA. For comparison, we also fine-tune a version where all",
            "content": "9 attention layers are replaced with vanilla linear attention (LA). The training uses 81-frame videos at 480800 resolution, corresponding to sequence length of 31,500 tokens, with the mixing-head number = 105. In addition, we train hybrid model where only 2/3 of the layers are replaced by MHLA. We evaluate all models on VBench, and the results are reported in Tab. 5. MHLA delivers substantially stronger performance than vanilla LA while maintaining the same latency. At this extreme sequence length, vanilla LA suffers severe degradation due to global context collapse, whereas MHLA preserves linear-time complexity and recovers performance comparable to the original FlashAttention-based Wan2.1-1.3B, achieving 2.1 inference speedup. The hybrid model provides an excellent trade-off, achieving 1.6 speedup with even better overall performance. We further visualize the training loss curves in Fig. 6. MHLA rapidly adapts during fine-tuning and quickly approaches the pretrained models loss trajectory. In contrast, vanilla LA effectively fails to train under such long sequences, with its loss plateauing at high level. This validates our analysis of global context collapse and demonstrates that conventional linear attention breaks down entirely in ultra-long visual sequence settings. Table 4 Comparison on T2I models."
        },
        {
            "title": "Model",
            "content": "FID CLIP GenEval 6.14 PixArt-α [6] 6.34 PixArt-Σ [7] SANA* [52] 6.10 SANA-MHLA 5.90 27.55 27.62 28.15 0.48 0.52 0.64 28.26 0.68 Table 5 MHLA in Video Generation. Wan-FA indicates pretrained Wan2.1-1.3B. Wan-MHLA and Wan-LA replace all layers with MHLA and Linear Attention, respectively. Wan-MHLA-H only replaces 2/3 layers. Model Wan-FA Wan-LA Wan-MHLA Wan-MHLA-H Quality 85.23 69.96 84.26 84.87 Semantic 75.65 11.38 76.16 79.59 Total 83.31 58.24 82. 83.82 Latency (s) 166 82"
        },
        {
            "title": "5.4 Natural Language Processing",
            "content": "Figure 5 Loss comparison. Figure 6 Loss comparison on Wan-2.1-1.3B. MHLA shows much stronger convergence capability. To evaluate MHLA under autoregressive modeling, we test its performance in language modeling. Following GLA [54], we train 0.3B model from scratch on 10B tokens from FineWeb-Edu [35] with batch size of 0.25M tokens, using cosine learning rate schedule (max LR 3e-4), weight decay of 0.01, and gradient clipping of 1.0. The head number is set to 32 for MHLA with training context length of 2048. In Tab. 6, we present the language modeling perplexity, zero-shot Common-sense reasoning and MMLU. accuracy on commonsense reasoning benchmarks, and MMLU. The proposed MHLA shows comparable performance with Transformer++ [48] and the state-of-the-art linear models, including Gated DeltaNet (GDN) [55] and Mamba2 [12]. Additionally, MHLA outperforms all the baselines on the aggregated benchmark MMLU. Long context understanding. As presented in Tab. 8, we evalute the models performance on LongBench [1]. The proposed MHLA shows explicit advantages over other SOTA recurrent models, especillly in Mulit-Doc QA, Summarization, and Code tasks, and achieves the highest average score. The result demonstrates the superior long context understanding capability of the proposed MHLA."
        },
        {
            "title": "5.5 Ablation Study",
            "content": "10 Table 6 MHLA in NLP. We report results evaluated on models trained with 10B tokens. We highlight the best and second best entries. Model MMLU acc CSR Wino. acc avg. PIQA acc ARC-c acc_n OBQA acc_n ARC-e acc_n BoolQ Wiki. ppl acc GLA (340M) Transformer++(340M) Mamba (390M) Mamba2 (340M) GDN (360M) MHLA (340M) 22.9 22.9 23.5 23.0 23.0 23. 46.0 46.8 46.4 47.0 46.9 47.1 50.0 49.6 50.5 49.8 51.3 51.3 62.9 64.4 64.1 64.6 64.5 64.4 25.5 25.7 24.9 25.5 25.4 25.9 31.0 32.8 32.4 32.0 31.4 33.4 45.8 48.1 48.3 49.2 47.3 46. 60.8 60.5 58.2 61.2 62.0 61.3 41.47 34.57 38.32 35.40 35.01 38.31 LMB. ppl 86.98 60.46 62.43 58.51 60.16 71.64 Table 8 MHLA on LongBench. We report results evaluated on 340M models trained with 10B tokens. We highlight the best and second best entries Model Multi-Doc QA Single-Doc QA Few-shot Synthetic Summarization 2WM HQA Mus QQA SSM TQA PEN PZH QMS GvR MNs RBP LCC AVG Code NQA 3.37 Mamba(360M) 3.23 GLA(325M) GDN(346M) 2.86 Transformer++(325M) 4.97 3.56 Mamba2(330M) MHLA(325M) 3.58 4.57 1.60 2.36 1.67 4.53 2.31 1.54 4.73 2.24 2.13 2.22 4.45 4.70 2.38 1.69 2.97 1.87 4. 2.28 2.13 2.48 2.35 2.20 2.38 5.49 5.16 0.70 3.94 6.85 7.61 7.47 6.24 7.03 4.97 6.44 6.41 1.10 0.10 1.98 0.27 0.41 0.53 1.18 0.76 1.51 0.72 1.49 1.69 14.96 13.63 12.33 6.97 18.36 12.23 6.53 17.72 13.59 12.55 15.34 11.42 17.91 15.98 10.42 9.98 6.86 12.46 9.92 6.92 11.56 15.11 16.81 11.75 10.15 12.57 9.49 6.62 14.00 17.65 13.37 12.72 7.41 12.58 18.59 15.01 Multi-Head Mixing. To evaluate the impact of our initialization strategy and learnable design in Multi-Head Mixing, we consider two variants: (1) uniform initialization without locality bias and (2) locality-biased initialization with frozen coefficients. We train and evaluate these variants on DeiT-T, with results shown in Tab. 7a. The results show that our locality-biased initialization provides strong prior, achieving competitive performance even without learning. Allowing the coefficients to be learnable further adapts them to the dataset distribution, yielding additional performance gains. Table 7 Ablation study of the proposed MHLA. (a) Ablation of init strategy on DeiT-T. LB-init denotes Localitybiased Initialization. LB-init Learnable Top1-acc(%) 75.4 75. 75.8 (b) Token-level head number ablation on DiT-S/2, 512px. Head number. We also analyze the choice of head number . For DiTS/2 at 512 resolution, the input sequence length is 1024. As discussed in 1024 = 32. We Sec. 4.3, MHLA retains linear complexity when evaluate {4, 16, 64}, with results summarized in Tab. 7b. MHLA achieves excellent FID already at M=16 while maintaining the highest throughput, implying that MHLA can reach best performance with relatively small and thus leading to almost no overhead. Head number FID Throughput 79.56 78.63 79.50 4 16 435 408"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce novel linear attention mechanism, termed Multi-Head Linear Attention (MHLA). By partitioning tokens into multiple groups, MHLA effectively preserves token-wise diversity. Without relying on additional modules such as depthwise convolutions or hybrid self-attention layers, MHLA achieves performance comparable to or even surpassing that of self-attention-based models. We envision this work as establishing fundamental attention mechanism that can benefit wide range of downstream applications, such as high-quality image generation, long-horizon video synthesis, and large-scale language modeling."
        },
        {
            "title": "References",
            "content": "[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [2] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. In arXiv preprint arXiv:2004.05150, 2020. [3] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models, 2020. URL https://arxiv.org/abs/2002.07028. [4] Tom Brown et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 2020. [5] Yuan Cao and Dong Wang. Saga: Selective adaptive gating for efficient and expressive linear attention, 2025. URL https://arxiv.org/abs/2509.12817. [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. [8] Rewon Child et al. Generating long sequences with sparse transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [9] Krzysztof Choromanski et al. Rethinking attention with performers. In International Conference on Learning Representations (ICLR), 2021. [10] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. [11] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [12] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. [13] Tri Dao et al. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [15] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. [18] Alexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. [19] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [20] Qihang Fan, Huaibo Huang, Xiaoqiang Zhou, and Ran He. Lightweight vision transformer with bidirectional interaction. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [21] Qihang Fan, Huaibo Huang, Yuang Ai, and Ran He. Rectifying magnitude neglect in linear attention. In ICCV, 2025. [22] Qihang Fan, Huaibo Huang, and Ran He. Breaking the low-rank dilemma of linear attention. In CVPR, 2025. [23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [24] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 59615971, October 2023. [25] Dongchen Han, Yifan Pu, Zhuofan Xia, Yizeng Han, Xuran Pan, Xiu Li, Jiwen Lu, Shiji Song, and Gao Huang. Bridging the divide: Reconsidering softmax and linear attention. In NeurIPS, 2024. [26] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61856194, June 2023. [27] Ali Hatamizadeh and Jan Kautz. Mambavision: hybrid mamba-transformer vision backbone. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2526125270, 2025. [28] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate attention for efficient mobile network design. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1371313722, 2021. [29] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. [30] Angelos Katharopoulos et al. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning (ICML), 2020. [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [32] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024. [33] Ze Liu et al. Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on Computer Vision (ICCV), 2021. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [35] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. [36] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [37] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. [38] Hao Peng et al. Random feature attention. In International Conference on Learning Representations (ICLR), 2021. [39] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022. [40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 13 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [42] Chitwan Saharia et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [43] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training, 2024. URL https://arxiv.org/abs/2309.10818. [44] Zhuoran Shen et al. Efficient attention: Attention with linear complexities. In WACV, 2021. [45] Yuheng Shi, Minjing Dong, and Chang Xu. Multi-scale vmamba: Hierarchy in hierarchy visual state space model. Advances in Neural Information Processing Systems, 37:2568725708, 2024. [46] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to Transformer for large language models. ArXiv, abs/2307.08621, 2023. [47] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, volume 139, pages 1034710357, July 2021. [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [49] Ashish Vaswani et al. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [50] Jiahao Wang, Ning Kang, Lewei Yao, Mengzhao Chen, Chengyue Wu, Songyang Zhang, Shuchen Xue, Yong Liu, Taiqiang Wu, Xihui Liu, Kaipeng Zhang, Shifeng Zhang, Wenqi Shao, Zhenguo Li, and Ping Luo. Lit: Delving into simple linear diffusion transformer for image generation, 2025. URL https://arxiv.org/abs/2501.12976. [51] Sinong Wang et al. Linformer: Self-attention with linear complexity. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [52] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers, 2024. URL https://arxiv.org/abs/2410.10629. [53] Yunyang Xiong et al. Nyströmformer: nyström-based algorithm for approximating self-attention. In AAAI Conference on Artificial Intelligence, 2021. [54] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2024. URL https://arxiv.org/abs/2312.06635. [55] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=r8H7xhYPwz. [56] Manzil Zaheer et al. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [57] Zhisong Zhang, Yan Wang, Xinting Huang, Tianqing Fang, Hongming Zhang, Chenlong Deng, Shuaiyi Li, and Dong Yu. Attention entropy is key factor: An analysis of parallel context encoding with full-attention-based pre-trained language models, 2025. URL https://arxiv.org/abs/2412.16545. [58] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. [59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [60] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. 14 [61] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 76647674, June 2025."
        },
        {
            "title": "Appendix",
            "content": "A Full Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Query-Conditioned Selectivity in Softmax Attention . . . . . . . . . . . . . . . . . . . . . . MHLA for Autoregressive Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Dataset",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Extra Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Complete Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.1 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Ablation of CPE and output gating. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Classification results on Higher Resolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Scaling Anaylsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Clarification on Terminology and Computational Concepts . . . . . . . . . . . . . . . . . . G.1 Concept 1: query-conditioned . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Concept 2: KV Summary vs. Hidden States . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM Usage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 3 4 4 4 4 4 5 5"
        },
        {
            "title": "A Full Related Works",
            "content": "Transformer. Since the introduction of the Transformer architecture [49], self-attention has become the dominant mechanism across wide range of domains, including natural language processing [4, 17], computer vision [18, 28, 33, 58], and generative modeling [19, 42]. The expressive power of self-attention stems from its ability to model pairwise interactions among all tokens, but this comes at quadratic cost in both computation and memory. This limitation becomes particularly pronounced in large-scale or real-time applications, motivating the exploration of more efficient attention mechanisms. broad spectrum of strategies has been proposed, such as sparse attention [2, 8, 56], low-rank approximations [51, 53], and hardware-optimized variants such as FlashAttention [11, 13]. Despite these advances, designing efficient attention mechanisms that maintain both scalability and accuracy remains an open challenge. Linear Attention. Linear attention has emerged as prominent direction for addressing the quadratic complexity of standard self-attention. Early works reformulated the softmax operation with kernel-based feature mappings, enabling linear-time complexity in both training and inference [9, 30, 36, 37, 54]. While these approaches make Transformers scalable to long sequences, they often suffer from reduced representational power compared to full softmax attention, leading to accuracy drops in challenging tasks such as vision and generative modeling. To bridge this gap, subsequent research has incorporated additional modules to enrich the expressiveness of linear attention. For example, convolutional layers have been introduced to capture local 1 context [22, 24, 38, 44], gating mechanisms have been proposed to better control information flow. More recently, state space models such as Mamba [12, 23] and its variants [32, 45] have also been explored as efficient alternatives to linear attention, showing strong scalability on long sequences and competitive accuracy. However, these methods still face two fundamental limitations: (1) when applied in unidirectional form to tasks requiring bidirectional attention, they exhibit substantial performance degradation; and (2) when augmented with extra modules (e.g., convolutional layers or additional self-attention blocks), they inevitably incur higher computational overhead and remain vulnerable to global context collapse (see Sec. 3.2), where the global summary loses representational diversity In addition to linear attention, sparse attention mechanisms have been another major Sparse Attention. approach to addressing the computational bottleneck in Transformers. Methods such as Longformer [2] and BigBird [56] introduce sparse attention patterns, where each token only attends to subset of the other tokens, reducing the overall number of attention operations. These methods exploit structural sparsity (e.g., local or global attention patterns) to maintain efficiency while still capturing global context in long sequences. Other techniques, such as the Performer [9], propose using kernel approximations to achieve sparse attention while preserving the models expressive power. Although sparse attention mechanisms improve scalability, they often introduce trade-offs in terms of accuracy, especially in tasks requiring full token interactions. Applications of Linear and Sparse Attention. Linear and sparse attention mechanisms have been successfully applied across various domains, including NLP, CV, and generative modeling. In NLP, linear attention has been used to scale models like BERT [16] and GPT [40] to longer sequences, enabling better handling of long documents and improving efficiency in language models [4, 17]. In computer vision, linear attention methods have been applied to vision transformers to improve efficiency when processing large images, as seen in works like Swin Transformer [33] and DeiT [47]. These applications demonstrate the broad utility of linear and sparse attention mechanisms, but also highlight the need for continued development to balance efficiency with the expressive power required by complex tasks like image generation and video understanding. Query-Conditioned Selectivity in Softmax Attention key advantage of softmax self-attention is its query-conditioned selectivity. Recall the standard attention formulation: Attn(Q, K, )i = (cid:88) j=1 αijvj, αij = exp(q kj) t=1 exp(q kt) (cid:80)N . j= Two properties are crucial: (i) Query-conditioned weighting: each query qi produces its own distribution , so the relative importance of token kj is fully dependent on qi; (ii) Per-token weighting: the weights {αij}N act directly on each vj, without collapsing into global summary. Together, these properties give softmax attention the ability to produce highly adaptive, sharply concentrated context vectors. By contrast, global linear attention aggregates all tokens into single summary matrix Sglobal = (cid:80)N shared by all queries, yielding j=1 (cid:101)KjV Attnlin(Q, K, )i = (cid:101)q Sglobal (cid:0)(cid:80)N j=1 (cid:101)Kj (cid:1) , (cid:101)q where the per-token contributions are no longer explicitly separable by i. As result, different queries obtain nearly identical context vectors, losing query-conditioned selectivity. MHLA restores query-conditioned selectivity. MHLA bridges this gap by introducing learnable coefficient matrix Mc that forms query-block-specific mixtures of local summaries: (cid:101)Si = (cid:88) b= mi,bSb AttnMHLA(Q, K, )i = (cid:101)q (cid:101)Si. 2 Because mi,b varies with the query block i, MHLA assigns different effective weights to the same token depending on the querying block. Expanding Sb into its token-level definition gives (cid:101)q (cid:101)Si = (cid:88) t=1 mi,b(t) (cid:0) (cid:101)q (cid:101)Kt (cid:1)V , revealing two-stage weighting mechanism: (i) block-level selection mi,b(t) that is query-conditioned, followed by (ii) within-block token reweighting via the kernel inner product (cid:101)Kt. This design reintroduces query- (cid:101)q conditioned selectivity and per-token weighting while preserving the linear-time complexity of kernelized attention."
        },
        {
            "title": "C MHLA for Autoregressive Modeling",
            "content": "In autoregressive modeling, the causal mask prevents each token from attending to future tokens. While linear attention normally achieves O(N d2) complexity by reusing global keyvalue summary, under causal masking, the summary must be recomputed or updated for each prefix, which naively results in O(N 2d) cost over the full sequence. To avoid this quadratic overhead, widely adopted solution for linear attention is chunkwise parallel training [46], which splits the sequence into blocks of size and processes them in parallel to avoid the quadratic cost of recomputing attention over all past tokens. For block b, local keyvalue summary is computed as Sb = (cid:80) Rdd, and the global summary is updated recursively: jb (cid:101)KjV Sglobal = Sglobal i1 + Si, Hi = QiSglobal i1 + (Qi (cid:101)K )Vi. Here, the first term propagates context from preceding blocks via the prefix summary Sglobal , while the second term captures intra-block attention. This chunkwise scheme preserves causality and allows block-parallel (Cd2 + 2d)(cid:1) for sequence training with per-block complexity O(Cd2 + 2d), leading to an overall cost O(cid:0) of length L. i1 MHLA with chunkwise parallel training. MHLA extends this scheme by replacing the single global summary with query-conditioned mixtures of local summaries. Specifically, for block we form mixed summary (cid:101)Si = (cid:88) bi mi,bSb, Hi = Qi (cid:101)Si1 + mi,b(Qi (cid:101)K )Vi. where mi,b are the learnable mixing coefficients from the causal coefficient matrix Mcausal (upper-triangular entries masked to enforce causality). Queries in block then interact only with (cid:101)Si, yielding block-specific, query-adaptive context representations rather than shared global one. Because the mixing is performed once per block and reused for all tokens in that block, the asymptotic complexity matches that of chunkwise linear attention. Causal inference. At inference time, we maintain the set of past local summaries {S1, . . . , Si1} and incrementally update the current block summary Si as new tokens arrive. When block is complete, its contribution to future mixtures is fixed and cached. For new token in block i, we simply update and recompute the blocks mixed summary (cid:101)Si by applying mi,i to the incremental update. Si Si + (cid:101)KtV This avoids recomputation over previous blocks and keeps per-token complexity O(d2)."
        },
        {
            "title": "D Dataset",
            "content": "To assess the effectiveness of our approach, we conduct extensive experiments on four tasks: image classification, class-to-image (C2I) generation, text-to-image (T2I) generation, and natural language processing. Following prior works [21, 22, 24], we train classification and C2I models on ImageNet-1K [14] and evaluate them on the standard validation set. For T2I generation, we finetune pretrained model using relative small collection of 31,292k images gotten from the internet. For natural language processing, we train models with subset of SlimPajama [43] with 5B tokens."
        },
        {
            "title": "E Extra Implementation Details",
            "content": "Image Classification. For training of DeiT, we replace the class token with average pooling and train all baselines under identical settings to ensure fair comparison. We additionally add CPE [10] with kernel size of 3, following previous works for fair comparison. For VLT, we strictly follow the setup in [22]. All models are trained for 300 epochs with batch size of 1024 and peak learning rate of 1e-3. For models with an input size of 224, we pad the input size to 256 for better splitting of heads. The head number is set to 16 for DeiT modes. For VLT models, the sequence length for the two linear attention layers is {3136, 784}. So we set the head number to {49, 16} for the two layers respectively."
        },
        {
            "title": "F Complete Experimental Results",
            "content": "F."
        },
        {
            "title": "Image Generation",
            "content": "We illustrate the complete results on DiT and DiG models in Tab. 10 and Tab. 9. We provide more generation results of SANA-MHLA in Fig. 7. We additionally provide more comprehensive comparisons against other recent linear attention methods on image generation tasks [50], and report the mean and standard deviation of MHLA over three independent runs to demonstrate the stability of our results. The corresponding results are summarized in Tab. 11. F.2 Ablation of CPE and output gating. We conducted detailed analysis of the effects of CPE and Output Gating when combined with MHLA in the DiT-S model, as shown in Tab. 12. Our findings show that, in smaller models, CPE and Output Gating serve as orthogonal optimizations of MHLA, effectively enhancing the expressive ability when the model size is insufficient. However, our experiments in Tab. 3a indicate that the performance gains from CPE and Output Gating diminish as the model size increases. In the DiTXL model, adding CPE alone actually leads to performance decrease. In contrast, MHLA consistently provides significant improvements in expressivity, regardless of model size. Table 12 Ablation study of MHLA with CPE and output gating. Setting Linear Attention MHLA w/ None MHLA w/ CPE MHLA w/ Gating MHLA w/ CPE+Gating FID 89.7 76.4 64.0 68.5 59.8 F.3 Classification results on Higher Resolutions We further conducted additional experiments at resolutions of 384384 and 512512, using the DeiT-T model to verify the effectiveness of MHLA on high-resolution classification tasks. Results are shown in Tab. 13. Table 13 High-resolution classification accuracy of DeiT-T with and without MHLA. Model Resolution ACC F.4 Scaling Anaylsis In this section, we conduct empirical studies to evaluate the throughput of MHLA across different tasks under varying sequence lengths and token-level head numbers M. The results in Tab. 14 show that when 2 < is satisfied, MHLA introduces only negligible overhead, whereas larger leads to more noticeable overhead. DeiT-T DeiT-T + MHLA DeiT-T DeiT-T + MHLA 384384 384384 512512 512512 74.4 77.5 75.3 78. Table 9 Fast adaptation results on DiT-XL/2 with MHLA, with and without guidance. Model DiT-XL/2 Attention Type Resolution FID 9.62 Self Attention 8.34 MHLA (Ours) 256 256 DiT-XL/2(G) Self Attention MHLA (Ours) 256 256 2.27 2.54 IS 121.50 121.27 278.24 252.07 sFID Precision Recall 0.67 0.69 0.67 0.65 6.85 5.52 4.60 4.67 0.83 0.83 0.57 0. Table 10 Comparison of different attention types across models. Model Attention Type Resolution DiT-S/2 DiG-S/ DiT-B/2 DiT-L/2 DiT-XL/2 Self Attention Linear Attention MHLA (Ours) Self Attention Linear Attention MHLA (Ours) GLA [54] GLA MHLA (Ours) Self Attention Linear Attention MHLA (Ours) Self Attention Linear Attention MHLA (Ours, w/None) MHLA (Ours, w/ CPE) MHLA (Ours, w/ CPE+Gating) Self Attention Linear Attention MHLA (Ours, w/ None) MHLA (Ours, w/ CPE) MHLA (Ours, w/ CPE+Gating) 256 256 256 512 512 512 256 512 256 256 256 256 256 256 256 256 256 256 256 256 256 FID 68.40 89.72 59.80 84.54 125.33 78.63 62.06 99. 59.49 43.47 60.47 37.47 23.33 32.35 25.37 24.21 21.37 19.47 28.63 20.32 22.79 19.17 IS 15.24 23.49 15.53 33.11 13. 24.04 24.27 38.79 45.57 54.38 57.62 63.47 51.15 65.95 61.80 68.97 sFID Precision Recall 0.28 0.39 21.87 10. 0.41 0.56 17.02 11.64 18.50 11.51 13.69 7.35 8.55 6.06 6.12 5.80 8.23 6.01 5.53 5. 0.36 0.22 0.40 0.40 0.39 0.51 0.54 0.59 0.59 0.61 0.57 0.61 0.60 0.63 0.49 0.29 0. 0.57 0.57 0.63 0.62 0.61 0.62 0.62 0.62 0.62 0.62 0.62 However, our ablation studies in Tab. 7b have already demonstrated that choosing such that 2 < is sufficient to achieve strong performance."
        },
        {
            "title": "G Clarification on Terminology and Computational Concepts",
            "content": "In this section, we provide formal definitions for the terminology used in our method. These terms describe novel computational behaviors in MHLA that lack direct analogues in prior linear attention formulations. G.1 Concept 1: query-conditioned The phrase query-conditioned describes mechanism where the aggregation of contextual information is dynamic and specific to each query instance, distinct from the fixed recurrence found in standard linear attention. Specifically, the process operates as follows: Each query token is associated with unique vector of mixing coefficients. These coefficients are used to weight and aggregate all local KV summaries independently for every query position. Consequently, the adaptation occurs per query, rather than globally or via shared recursive rule. G.2 Concept 2: KV Summary vs. Hidden States We introduce the term KV Summary tos strictly distinguish our approach from the Hidden State found in traditional linear attention papers. While the KV summary may seemingly resemble Hidden States in notation, the underlying computation and dependency graphs are structurally different in two key aspects: 5 Table 11 Comparison with LiT. We report the FID scores (mean std) over three independent runs for MHLA to demonstrate result stability. Model LiT-S/2 DiT-S/2 with MHLA LiT-B/2 DiT-B/2 with MHLA LiT-L/2 DiT-L/2 with MHLA LiT-XL/2 DiT-XL/2 with MHLA FID (mean std) 63.21 59.744 0.100 40.86 37.519 0.039 24.04 21.426 0.051 20.66 19.164 0.031 Table 14 Profiling results of MHLA under varying sequence length and token-level head number . Left: DiT-S/2. Right: DeiT-S/16. MN 256 4096 MN 256 1024 4 16 64 256 1024 42ms 3.7G 52ms 40ms 3.9G 51ms 39ms 4.8G 52ms 7.1G 147ms 20.8G 7.2G 145ms 21.0G 8.0G 148ms 21.7G 61ms 12.0G 157ms 25.4G 219ms 40.0G 4 16 64 256 8.9G 129 imgs/s 3.4G 124 imgs/s 118 imgs/s 3.8G 118 imgs/s 9.4G 150 imgs/s 5.7G 104 imgs/s 11.0G 89 imgs/s 18.0G Unlike the strict recursive chain in traditional linear attention where ht relies on ht1, MHLA computes each Global KV Summary (Sg) independently, eliminating state propagation across positions. While traditional states are derived via one-to-one update from the previous step, MHLA follows many-to-one aggregation pattern, where each Sg is computed from all local summaries using specific mixing coefficients. By avoiding the rigid inheritance of history inherent to hidden states, MHLAs KV summaries achieve greater expressivity and flexibility. LLM Usage. We used large language models (LLMs) solely as writing aid to polish the clarity and readability of the manuscript. Specifically, we employed LLM-based tools to (i) refine grammar and phrasing for academic style consistency, (ii) improve logical flow between sections, and (iii) condense overly verbose passages. No new research ideas, experimental designs, or results were produced by the LLM; all scientific contributions, methodology development, and experimental analyses were conceived and executed by the authors. 6 Figure 7 More generation results from our fine-tuned SANA-MHLA model."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Peking University"
    ]
}