{
    "paper_title": "Tongyi DeepResearch Technical Report",
    "authors": [
        "Tongyi DeepResearch Team",
        "Baixuan Li",
        "Bo Zhang",
        "Dingchu Zhang",
        "Fei Huang",
        "Guangyu Li",
        "Guoxin Chen",
        "Huifeng Yin",
        "Jialong Wu",
        "Jingren Zhou",
        "Kuan Li",
        "Liangcai Su",
        "Litu Ou",
        "Liwen Zhang",
        "Pengjun Xie",
        "Rui Ye",
        "Wenbiao Yin",
        "Xinmiao Yu",
        "Xinyu Wang",
        "Xixi Wu",
        "Xuanzhong Chen",
        "Yida Zhao",
        "Zhen Zhang",
        "Zhengwei Tao",
        "Zhongwang Zhang",
        "Zile Qiao",
        "Chenxi Wang",
        "Donglei Yu",
        "Gang Fu",
        "Haiyang Shen",
        "Jiayin Yang",
        "Jun Lin",
        "Junkai Zhang",
        "Kui Zeng",
        "Li Yang",
        "Hailong Yin",
        "Maojia Song",
        "Ming Yan",
        "Peng Xia",
        "Qian Xiao",
        "Rui Min",
        "Ruixue Ding",
        "Runnan Fang",
        "Shaowei Chen",
        "Shen Huang",
        "Shihang Wang",
        "Shihao Cai",
        "Weizhou Shen",
        "Xiaobin Wang",
        "Xin Guan",
        "Xinyu Geng",
        "Yingcheng Shi",
        "Yuning Wu",
        "Zhuo Chen",
        "Zijian Li",
        "Yong Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community."
        },
        {
            "title": "Start",
            "content": "2025-10-"
        },
        {
            "title": "Tongyi DeepResearch Technical Report",
            "content": "Tongyi DeepResearch Team Tongyi Lab , Alibaba Group https://tongyi-agent.github.io/blog https://github.com/Alibaba-NLP/DeepResearch https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B https://www.modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"
        },
        {
            "title": "Abstract",
            "content": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across range of agentic deep research benchmarks, including Humanitys Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community. 5 2 0 2 8 2 ] . [ 1 1 0 7 4 2 . 0 1 5 2 : r Figure 1: Benchmark performance of Tongyi DeepResearch. Full author list available in the Contributions section."
        },
        {
            "title": "Introduction",
            "content": "As we advance toward Artificial General Intelligence (AGI), the emergence of Deep Research agents offers promising paradigm for augmenting and potentially liberating human intellectual productivity. Deep research is new agentic capability that autonomously conducts multi-step reasoning and information seeking on the internet for complex research tasks. It can be completed in tens of minutes, which would otherwise require several hours for human (OpenAI, 2025a; Claude Team, 2025; Grok Team, 2025; Gemini Team, 2025). However, most deep research systems remain closed-source, and their intermediate research processes are inaccessible. While the community has made preliminary explorations in this area (Wu et al., 2025a; Li et al., 2025c; Tao et al., 2025), there is still lack of systematic methodology and publicly available models that can be fully open-sourced and shared across the community. In this work, we introduce Tongyi DeepResearch, opening the era of open-source AI researchers. Our goal is to endow large language models (LLMs) with autonomous research capabilities agency, the ability to plan, search, reason, and synthesize knowledge across extended sequences of actions and diverse information sources. Tongyi DeepResearch delivers several key advancements: We propose an end-to-end agentic training paradigm that unifies agentic mid-training and agentic post-training, forming scalable foundation for deep reasoning and information-seeking behaviors. Agentic mid-training cultivates inherent agentic biases by exposing the model to large-scale, highquality agentic data, serving as progressive transition from pre-training to post-training stages. Agentic post-training further unlocks the models potential via scalable multi-turn reinforcement learning on strong base model. Together, they enable the model to gradually develop from basic interaction skills to advanced autonomous research behaviors. We design fully automated, highly scalable data synthesis pipeline that eliminates human annotation while generating diverse, high-quality agent trajectories. We design stage-specific data synthesis strategies tailored to the objectives of each training phase, ensuring that every stage is supported by appropriately structured and targeted data. Synthetic data is highly scalable, fast to validate, and enables the construction of super-human-level datasets with stable distributions. It serves as an indispensable engine for agent training. We construct stage-specific, customized environments that rely on robust infrastructure to deliver consistent interactions for data synthesis across training stages. These environments allow the agent to engage in rich, specialized interactions that are tightly aligned with its developmental stage. They can take various forms, from prior world models to simulated environments and real-world interactive contexts. Tongyi DeepResearch establishes new state-of-the-art with substantially fewer parameters, comprising total of 30.5 billion parameters while activating only 3.3 billion per token, building upon the Qwen330B-A3B-Base model (Yang et al., 2025). Empirical evaluations on deep research benchmarks demonstrate the effectiveness of our agent. Tongyi DeepResearch reaches 32.9 on Humanitys Last Exam, 43.4 on BrowseComp, 46.7 on BrowseComp-ZH, 72.2 on WebWalkerQA, 70.9 on GAIA, 75.0 on xbenchDeepSearch, 90.6 on FRAMES and 55.0 on xbench-DeepSearch-2510, outperforming strong baselines such as OpenAI-o3 (OpenAI, 2025b) and Deepseek-V3.1 (DeepSeek Team, 2025). We also provide systematic analysis covering agentic reinforcement learning, synthetic data, offering key insights into the development of deep research agent. In addition, we present the performance of Tongyi DeepResearch on general benchmarks, including AIME25, HMMT25 and SimpleQA. We believe that agentic models represent an emerging trend for the future, as models increasingly internalize agent-like capabilities and can autonomously invoke the appropriate tools to solve wide range of problems. In the following sections, we first outline the design principles underlying Tongyi DeepResearch. We then describe the training pipeline, followed by comprehensive evaluation of its performance. We 2 release the model, framework, and end-to-end solutions to support and accelerate community research. This technical report summarizes our main insights and aims to inspire further progress toward scalable and capable agentic systems."
        },
        {
            "title": "2 Design Principle",
            "content": "Agent Training Pipeline. Agent training is inherently more complex and challenging than conventional LLM training. We introduce two stages in our agent training pipeline: mid-training and post-training. We integrate mid-training directly into the deep research training process, and co-design the end-to-end on-policy reinforcement learning algorithm and its underlying infrastructure for seamless scalability and stability. While most work only applies post-training phase for DeepResearch agents, we novelly introduce mid-training for agentic learning. General foundation models usually lack agentic inductive bias. Most general foundation models are typically pretrained on plain text crawled from the internet and then post-trained on instruction-following data. These datasets lack research-level questions and agentic behaviors, resulting in the model learns agentic capabilities and alignment simultaneously during the post-training phase. Agentic post-training on these general foundation models can result in sub-optimal outcomes and inherent optimization conflicts. Mid-training endows the pre-trained base model with substantial agentic prior knowledge, thereby bridging the gap between pretraining and agentic posttraining. Mid-training phase provides powerful agentic foundation model to support effective agentic post-training. During post-training, the model further internalizes deep research capabilities through reinforcement learning with supervised fine-tuning (SFT) for cold start. SFT teaches the model to reliably imitate curated demonstrations, establishing stable behavioral baseline for research workflows and tool use. However, behavior cloning alone tends to produce mimicry without exploration. RL closes the loop with the environment, using reward signals to refine policies and to internalize agentic planning and execution. In particular, reinforcement learning (1) explores optimal strategies through active interaction with the environment; (2) internalizes goal-directed planning and execution capabilities; and 3) achieves superior sample efficiency by prioritizing high-reward behaviors. The agent first acquires general agentic pattern during supervised fine-tuning phase, while reinforcement learning phase effectively pushes the limits of its agentic performance. Synthetic Data Centric Scaling. Data serves as the foundation of training, while collecting data for DeepResearch problems is extremely hard. Deep research problems require agents capability of connecting information, reasoning across sources and validating conclusions. Unlike pre-training data, which is naturally abundant, and conventional LLM post-training data, which is relatively easy to annotate, agentic data is inherently scarce. Research-level problems are difficult to obtain through natural texts from the web. Manually annotating these problems and agentic trajectories is extremely time-consuming and costly (Wei et al., 2025). Building on the aforementioned agent training pipeline, agentic mid-training requires large-scale, diverse trajectories to align subsequent agent behaviors, while agentic post-training depends on high-quality, verifiable data to provide reliable reward signals. As result, it is hard to rely on natural data to scale DeepResearch capability. Therefore, we focus on synthetic data with large language models. Synthetic data contains several advantages over human annotations below: Synthesizing research-level questions is easy to scale. We can use LLMs to synthesize questionanswer pair efficiently compared to manually annotating. The pattern and diversity are easy to generalize. LLMs are easy to understand the structure of hard problems and usually have rare insight into diverse patterns, while training annotators to understand the structure and patterns for research-level problems is time-consuming. Synthesized data enables targeted meta-capability enhancement. By decomposing complex agent tasks into fundamental meta-capabilities (e.g., planning, information synthesis, memory management), we can generate synthetic data that specifically targets and strengthens individual agent skills. Synthesized data can be verified easily. It is much easier than finding the solution to the question, 3 which is essential in human annotating. Synthesized data can provide data flywheels in training stages. After one round of the agentic training pipeline, the trained agentic model can generate synthesized data with stronger reasoning and planning patterns. Data flywheel makes the agentic model evolve iteratively. Based on these insights, we believe synthetic agentic data becomes the key to scaling deep-research agents. The synthetic data in all phases of the agentic training pipeline are designed in three steps: (1) synthesizing research-level questions; (2) Generating agentic behavior data; (3) Utilizing agentic data in training pipeline. Learning Through Environmental Interaction. Environmental interaction plays crucial role in agent intelligence emergence (Silver & Sutton, 2025). However, relying solely on real-world environments for the whole agent training stage faces fundamental challenges: (1) Non-stationarity. The dynamic nature of environments causes continuous distribution shift in training data, undermining learning stability; (2) Interaction cost. The tangible expense of each API call makes large-scale exploration economically prohibitive. These barriers render agent capability acquisition from the real world alone formidable endeavor. In Tongyi DeepResearch, we propose fundamental reframing: environments should not be passively viewed as external reality, but actively designed as systems deeply coupled with the training process. Specifically, we model environments into three forms, each striking distinct balance between stability, fidelity, and cost: Prior World Environment. This environment provides task elements, tools, and state definitions, allowing agents to autonomously mine interaction trajectories based on pretrained knowledge without receiving actual environmental responses. It offers perfect stability, zero interaction cost, and unlimited scalability, but lacks real-world feedback signals. Simulated Environment. This environment constructs controlled, reproducible replicas of real-world interactions locally. It provides stability, rapid response, and low cost, enabling fast iteration and causal attribution analysis. However, its data coverage is inherently limited, exhibiting notable sim-to-real gap. Real-world Environment. This environment delivers the most authentic data distribution and feedback signals, serving as the ultimate proving ground for agent capabilities. Its advantage lies in absolute distributional fidelity; the cost is expensive interactions, significant non-stationarity, and exploration risks. Building on this environmental insight, we adopt adaptive strategies for synthetic data generation and training. Specifically, (1) During agentic mid-training, we primarily leverage the Prior World Environment and Simulated Environment to generate large-scale synthetic data at minimal cost, ensuring efficient agentic ability bootstrapping; (2) During agentic post-training, we validate training strategies and algorithmic techniques in the simulated environment, then deploy verified optimal policies to the real environment for final training. The choice of environments plays crucial role, agentic intelligence emerges not from single wolrd, but from carefully chosen environments. Agent training fundamentally depends on synthetic data and environment interaction. Based on these design principles, we then introduce Tongyi DeepResearch in detail below."
        },
        {
            "title": "3.1 Formulation",
            "content": "We formally define the Tongyi DeepResearchs rollout at each timestep through three fundamental components: Thought (τt): The internal cognitive process of the agent. This includes analyzing the current context, 4 recalling information from memory, planning subsequent steps, and engaging in self-reflection to adjust its strategy. Action (at): An external operation executed by the agent to interact with its environment. Tongyi DeepResearch is equipped with versatile set of tools that define its action space, enabling it to interact with wide range of information sources: Search, Visit, Python Interpreter, Google Scholar and File Parser. Actions encompass all intermediate tool calls and the final response to the user. In given trajectory, intermediate actions (at where < T) are tool calls, while the final action, aT, constitutes the generation of an in-depth report for the user. Observation (ot): The feedback received from the environment after an action is performed. This new information is used to update the agents internal state and inform its next thought. Based on the fundamental components above, we define two different rollout types as follows: ReAct. Tongyi DeepResearchs architecture is fundamentally based on the vanilla ReAct (Yao et al., 2023) framework, which synergizes reasoning and acting. In this paradigm, the agent generates both reasoning trace (Thought) and subsequent Action in an interleaved manner. This process forms trajectory, HT, which is sequence of thought-action-observation triplets: HT = (τ0, a0, o0, . . . , τi, ai, oi, . . . , τT, aT), (1) where aT represents the final answer to the given task. At any given step T, the agents policy, π, generates the current thought τt and action at based on the history of all previous interactions, Ht1: τt, at π(Ht1). While more complex single and multi-agent paradigms have emerged, our choice of ReAct is deliberate one, rooted in its simplicity and alignment with fundamental principles. This decision is informed by \"The Bitter Lesson\" (Sutton, 2019), which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human-engineered knowledge and intricate designs. Frameworks that require extensive, specialized prompt engineering or possess rigid operational structures risk becoming obsolete as the intrinsic capabilities of models scale (Li et al., 2025a). (2) Context Management. The execution of long-horizon tasks is fundamentally constrained by the finite length of the agents context window. To mitigate the risk of context overflow and ensure task focus, we propose the context management paradigm (Qiao et al., 2025), which employs dynamic context management mechanism based on Markovian state reconstruction. Within this framework, the agent is not conditioned on the complete history. Instead, at each step t, it is conditioned on strategically reconstructed workspace containing only essential elements: the question q, an evolving report St serving as compressed memory, and the immediate context from the last interaction (at and ot). This Markovian structure enables the agent to maintain consistent reasoning capacity across arbitrary exploration depths while naturally circumventing the degradation. For every step 0 < < T, this core update process can be formalized as: St, τt+1, at+1 π(St1, at, ot). This context management paradigm is particularly crucial, it not only prevents context suffocation but also enforces structured reasoning by requiring the agent to explicitly synthesize and prioritize information at each step. This design naturally aligns with human research patterns, where periodic synthesis and reflection are essential for maintaining coherent long-term investigation. (3)"
        },
        {
            "title": "3.2 Overall Training Recipe",
            "content": "The system is initialized from the pretrained base model Qwen3-30B-A3B-Base1. Tongyi DeepResearch is developed through an end-to-end training framework that integrates agentic mid-training and posttraining, enabling scalable reasoning and information seeking across complex research tasks. This 1https://huggingface.co/Qwen/Qwen3-30B-A3B-Base 5 establishes new paradigm for training agentic models. We first present the mid-training process in Section 3.3, followed by the post-training stage in Section 3.4. Figure 2: Training pipeline of Tongyi DeepResearch."
        },
        {
            "title": "3.3 Agentic Mid-training",
            "content": "3.3.1 Training Configuration Tongyi DeepResearch employs two-stage Agentic Continual Pre-training (Agentic CPT) (Su et al., 2025) as its core mid-training phase. This phase functions as critical bridge connecting pre-trained models and agentic post-training. Its primary objective is to provide base model endowed with strong inductive bias for agentic behavior, while simultaneously preserving broad linguistic competence. To achieve this, the optimization process is driven by the standard Next-Token Prediction loss function. The design of this phase is strategically optimized for both efficiency and progressive capability scaling. We initiate with 32K context length in the first stage, before expanding to 128K in the second. This expanded context window is specifically leveraged in the second stage, where we introduce substantial corpus of long-sequence (64K-128K) agentic behavior data. This approach is critical for enhancing the models capacity for coherent, long-horizon reasoning and action. Throughout both stages, small proportion of general pre-training data is interleaved, ensuring the model acquires specialized agentic competence without sacrificing its foundational generalization capabilities."
        },
        {
            "title": "3.3.2 Large-scale Agent Behavior Data Synthesis",
            "content": "Figure 3: Large-scale agent behavior data synthesis for agentic continual pre-training. In Agentic CPT, we synthesize data across the complete lifecycle of agent workflows as shown in Figure 3. typical agent workflow begins with problem, iteratively cycles through reflection and action, and ultimately converges on final solution. To comprehensively capture this process, we synthesize data for the critical steps that constitute the agents operational cycle: Question Synthesis, Planning Action, Reasoning Action, and Decision-Making Action. Note that while decision-making is often implicit within agent cycles, we explicitly model it as distinct action type in our synthesis framework. Large-scale Multi-style Question Synthesis. Grounded in continuously updated open-world knowledge, we construct an entity-anchored open-world memory. This memory consolidates diverse real-world knowledge sources, such as web-crawled data and agent interaction trajectories, into structured representations of entities and their associated knowledge. Building upon this foundation, we sample entities along with their related knowledge to generate diverse questions that embed specific behavioral pattern requirements, such as multi-hop reasoning questions and numerical computation questions. 6 Planning Action. Planning refers to problem decomposition and first-step action prediction. key insight is that planning accuracy is highly correlated with whether an agent can successfully complete task. Thus, we employ open-source models to analyze, decompose, and predict initial actions for the synthesized questions. Furthermore, we leverage the entities and associated knowledge used in question construction as the basis for rejection sampling, thereby ensuring high-quality planning outputs. Reasoning Action. Logical reasoning and knowledge integration over heterogeneous data is foundational for agents solving complex tasks. When external tools return massive unstructured responses, whether models can distill critical knowledge from noise and construct coherent reasoning paths directly determines task outcomes. To this end, given question and its dependent knowledge, we guide large models through two-stage process to generate complete reasoning chains, with dual filtering mechanism based on reasoning length and answer consistency to ensure quality. Decision-Making Action. Each step of an agents thinking and action is essentially an implicit decisionmaking process. Specifically, each decision point encompasses multiple potential reasoning and action paths, from which the agent must select the most promising solution. To capture this critical mechanism, we explicitly model this decision-making process. First, based on existing demonstration trajectories, we thoroughly explore the feasible action space at each step. Second, we reconstruct the original trajectories into multi-step decision sequences while preserving the original decision choices. General Function-calling Data Synthesis via Environment Scaling. To enhance our models general agentic capability, we systematically scale the function-calling data through environment scaling. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained (Fang et al., 2025). We also scale up environments as step towards advancing general agentic intelligence. In designing environment construction and scaling, we follow the principle that the core of an agent lies in its capacity for environment interaction, with each environment instantiated as readwrite database. We design scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. The produced data are incorporated into the models mid-training phase."
        },
        {
            "title": "3.4 Agentic Post-training",
            "content": "The post-training pipeline comprises three stages: data synthesis, supervised fine-tuning for cold start, and agentic reinforcement learning."
        },
        {
            "title": "3.4.1 High-quality Data Synthesis",
            "content": "Figure 4: High-quality data synthesis pipeline. We develop an end-to-end solution for synthetic data generation to generate complex, high-uncertainty and super-human level question and answer pairs (Li et al., 2025c;b), as shown in Figure 4. This fully automated process requires no human intervention to construct super-human quality datasets, designed to push the boundaries of agent performance. The process begins by constructing highly interconnected knowledge graph via random walks, leveraging web search to acquire relevant knowledge, and isomorphic tables from real-world websites, ensuring realistic information structure. We then 7 sample subgraphs and subtables to generate initial questions and answers. The pivotal step involves strategically increasing the uncertainty within the question to enhance its difficulty (Wu et al., 2025a). This practical approach is grounded in complete theoretical framework, where we formally model QA difficulty as series of controllable \"atomic operations\" (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity. To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed formal modeling of the information-seeking problem based on set theory (Tao et al., 2025). With this formalization, we develop agents that expands the problem in controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information-seeking data for post-training. We also develop an automated data engine to scale the generation of PhD-level research questions (Qiao et al., 2025). Starting from multi-disciplinary knowledge base, it creates seed QA pairs requiring multi-source reasoning. These seeds undergo iterative complexity upgrades, where question-crafting agent, equipped with the corresponding tool, progressively expands scope and abstraction. Each iteration refines and compounds prior outputs, enabling systematic and controllable escalation of task difficulty. 3.4.2 Supervised Fine-tuning for Cold Start The initial phase of our agentic post-training pipeline is supervised fine-tuning (SFT) stage, designed to equip the base model with robust initial policy prior to reinforcement learning. Starting from our synthesized high-quality QA data, we obtain training trajectories that cover the complete thought process and tool responses generated by high-performing open-source models, which are then subjected to rigorous rejection sampling protocol. This comprehensive filtering process guarantees that only high-quality trajectories exhibiting diverse problem-solving patterns are retained. Mixed Training Paradigm. The cold stage training leverages data from two different formulations to enhance model robustness and generalization. For the React Mode, the training samples take the historical state Ht1 as input, and output the corresponding thought τi and tool call ai for the current step. For our Context Management Mode , the training samples take as input the previous steps trajectory summary St1, tool call ai1, and tool response oi1, and output the current steps trajectory summary, thought τi, and tool call ai. The Context Management Mode data particularly strengthens the agents capabilities in state analysis and strategic decision-making, as it requires the model to synthesize complex observations into coherent summaries while maintaining task focus across extended trajectories. This synthesis-oriented training enables more deliberate reasoning patterns compared to purely ReAct. We adopt two-stage training strategy based on context length. In the first stage, the context length is set to 40K, and the training data consist of ReAct Mode samples with context lengths shorter than 40K, along with all Context Management Mode samples (as they are all within 40k). In the second stage, the context length is extended to 128K, and the training data include ReAct Mode samples with context lengths between 40K and 128K, as well as small portion of 40K data for stability."
        },
        {
            "title": "3.4.3 Agentic Reinforcement Learning",
            "content": "To advance the models capabilities toward more robust and reliable planning and searching in complex web environment, we apply an agentic RL framework, which is illustrated in Figure 5. In this framework, the model generates complete task attempt (a \"rollout\") and receives reward if its final answer matches the ground truth (RLVR) (Guo et al., 2025). Throughout this agentic RL procedure, the model continuously interacts with the environment (simulated or real-world), iteratively refining its policy with each iteration, and, in turn, using that improved policy to curate new, higher-quality set of training data. 8 Figure 5: An overview of our agentic reinforcement learning framework. Real-world Environment. Our agents toolkit is complex system that integrates several specialized tools2: (1) Search, (2) Visit, (3) Python Interpreter, (4) Google Scholar, (5) File Parser. The end-to-end reliability of this system is paramount. The inherent volatility of external APIs, encompassing high latency, outright failures, and inconsistent returns, threatens to corrupt our training trajectories. This data contamination makes it nearly impossible to diagnose performance issues, obscuring whether poor outcome is caused by weakness in the agents policy or by the instability of the environment itself. To ensure reliable tool use during agent training and evaluation, we developed unified sandbox. This interface is built around central scheduling and management layer that orchestrates every tool call. For each tool, we implement robust concurrency controls and fault-tolerance mechanisms, such as proactive QPS rate constraints, result caching, automatic timeout-and-retry protocols, graceful service degradation for non-critical failures, and seamless failover to backup data sources (e.g., backup search API). This design abstracts the tool invocation into deterministic and stable interface for the agent and thereby insulates the training loop from real-world stochasticity while also significantly reducing operational costs. This design abstracts tool invocation into deterministic interface, providing stable and fast experience that is crucial for preventing tool errors from corrupting the agents learning trajectory. Simulated Environment. Directly utilizing real-world web environment APIs presents numerous practical problems3. We first build an offline environment based on the 2024 Wikipedia database and develop suite of local RAG tools to simulate the web environment. We then reuse the data synthesis pipeline to create high-quality, structurally complex QA specifically for this offline environment. This provides us with low-cost, high-efficiency, and fully controllable platform that enables high-frequency, rapid experimentation, thereby greatly accelerating our development and iteration process. On-Policy Asynchronous Rollout Framework. The iterative nature of agentic rollouts, which require numerous interactions with the environment, creates significant bottleneck that slows down the entire RL training process. To overcome this, we implement custom, step-level asynchronous RL training loop built on the rLLM framework (Tan et al., 2025). Our solution utilizes two separate asynchronous online servers, with one for model inference and another for tool invocation. centralized interaction handler then processes the outputs from both, formatting the feedback into unified message list. This architecture allows multiple agent instances to interact with the environment in parallel, each completing its rollout independently. 2The details for each tool are shown in Appendix D. 3Queries per second (QPS) impact significantly degrade our development efficiency and compromise the reliability during our early-stage ablation studies. 9 RL Training Algorithm. Our RL algorithm is tailored adaptation of GRPO (Shao et al., 2024): (θ) = (context) (q,y)D,{Hi}G i=1 (cid:34) 1 i=1 Hi i= πθold Hi j=1 min (cid:16) ri,j(θ) ˆAi,j, clip (cid:16) ri,j(θ), 1 εlow, 1 + εhigh (cid:35) (cid:17) , (cid:17) ˆAi,j (4) where (q, y) is the question-answer pair, ri,j(θ) is the importance sampling ratio (remains 1.0 for strictly on-policy training), and ˆAi,j is an estimator of the advantage at token j: ri,j(θ) = πθ(Hi,j context) (Hi,j context) πθold , ˆAi,j = Ri mean({Ri}G i=1). (5) We employ strict on-policy regimen, where trajectories are consistently sampled using the most upto-date policy, ensuring that the learning signal is always relevant to the models current capabilities. The reward is pure 0 or 1 signal of answer correctness. We do not include format reward (e.g., 0.1 for format correctness) because the preceding cold start stage ensures the model is already familiar with the required output format. Following DAPO (Yu et al., 2025), we apply the token-level policy gradient loss in the training objective and clip-higher strategy to encourage more exploration. To further reduce variance in the advantage estimation, we adopt leave-one-out strategy (Chen et al., 2025). Furthermore, we observed in preliminary experiments that directly optimizing on an unfiltered set of negative rollouts significantly degrade training stability and can lead to policy collapse after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield final answer because they exceed length limit. The primary motivation for these modifications is not algorithmic novelty but the pragmatic pursuit of more efficient and stable training paradigm. Automatic Data Curation. We optimize data in real time, guided by training dynamics to generalize to out-of-distribution scenarios through self-exploration. This optimization is achieved through fully automated data filtering pipeline that dynamically adjusts the training set based on the improved policy model. Specifically, our process begins with large dataset, D. We use the initial SFT model as baseline policy to sample multiple solution attempts, or rollouts, for each problem. We then create an initial training set, D, by filtering out problems where the model either always fails or always succeeds, as these will offer no learning signal for RL training. This leaves us with focused subset of problems of moderate difficulty. During RL training, we continuously monitor the problems in by their latest rollouts to see if they have become too easy for the improved policy model. In parallel, separate process uses intermediate checkpoints of the policy model to sample from the entire original dataset, D. This background process identifies and collects backup pool of new problems that have become moderately difficult for the now-stronger model. When the training reaches certain step count or the reward plateaus, we refresh the active training set by removing the mastered problems and incorporating new, challenging ones from the backup pool. The entire data filtering and refreshment pipeline runs independently, never interrupting the main RL training loop. This design allows us to automatically evolve both the policy model and its training data, ensuring consistently high training efficiency and stability. Through our experiments, we arrive at critical insight: the success of agentic RL depends more on the quality of the data and the stability of the training environment than on the specific algorithm being used. Consequently, we concentrate our efforts on designing stable environment and curating high-quality data, making only few essential modifications to the algorithm itself, mainly for the purpose of stabilizing the training process. 10 3.4.4 Model Merging We employ model merging at the last stage of the pipeline. This approach is built on the key insight that when different model variants are derived from the same pre-trained model, their parameters can be effectively combined through averaging or interpolation (Wang et al., 2025). Specifically, our process involves selecting several model variants that originate from the same base model but exhibit different capability preferences. We then create the final merged model by computing weighted average of their parameters: θmerged = αk θ(k), s.t. αk = 1, αk 0. (6) where θ(k) represents the parameters of the k-th model variant, and αk is its corresponding merge weight. Empirically, this interpolation strategy not only preserves the core strengths of each contributing model but also equips the merged model with robust generalization abilities. In complex scenarios requiring synthesis of these varied capabilities, the merged model performs comparably to the best-performing source model in its respective area of strength, all without incurring additional optimization costs."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Backbones. We evaluate Tongyi DeepResearch on seven public information-seeking benchmarks spanning long-term reasoning and long-horizon tool use. The model is compared against two families of systems: 1) LLM-based ReAct agents: GLM-4.5 (Zeng et al., 2025), Kimi-K2 (Team et al., 2025), DeepSeek-V3.1 (DeepSeek Team, 2025), Claude-4-Sonnet (anthropic, 2025), OpenAI o3/o4-mini (OpenAI, 2025b)) and 2) end-to-end deep-research agents: OpenAI DeepResearch (OpenAI, 2025a), Gemini DeepResearch (Gemini Team, 2025), Kimi Researcher (Kimi, 2025). Benchmarks. We follow each benchmarks official evaluation protocol. The benchmarks cover: (1) Humanitys Last Exam (Phan et al., 2025); (2) BrowseComp (Wei et al., 2025) and BrowseComp-ZH (Zhou et al., 2025); (3) GAIA (Mialon et al., 2023); (4) xBench-DeepSearch (Xbench Team, 2025); (5) WebWalkerQA (Wu et al., 2025b); (6) FRAMES (Krishna et al., 2025); and (7) xbench-DeepSearch-2510. All scores are computed with the official scripts released by each benchmark. The details of evaluation are presented in Appendix B. Evaluation. We adopt fixed inference parameters to ensure stability and reproducibility across evaluations: temperature = 0.85, repetition penalty = 1.1, and top-p = 0.95. maximum of 128 tool invocations is allowed per task, and the context length is constrained to 128K tokens. Each benchmark is evaluated three times independently, and we report the average performance (Avg@3) as the main metric. For completeness, we also report the best Pass@1 (best result over 3 runs) and Pass@3 results in the subsequent analysis. All results are obtained on September 16, 2025, except for xbench-DeepSearch-2510, which is evaluated on October 28, 2025. Reproduce. Tongyi DeepResearch operates utilizing an action space that includes the Search, Visit, Python, Scholar, and File Parser tools. We release official reproduction scripts on GitHub4, along with the complete tool implementations and prompt configurations."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 presents the performance of Tongyi DeepResearch compared with broad range of state-of-theart LLM-based agents and proprietary deep research systems across multiple benchmarks, including Humanitys Last Exam, BrowseComp, BrowseComp-ZH, GAIA, xbench DeepSearch, WebWalker QA, and FRAMES. Tongyi DeepResearch achieves the highest scores on nearly all evaluated benchmarks, 4https://github.com/Alibaba-NLP/DeepResearch 11 Table 1: Performance comparison on various benchmarks. Benchmarks LLM-based ReAct Agent GLM 4.5 Kimi K2 DeepSeek-V3.1 Claude-4-Sonnet OpenAI o3 OpenAI o4-mini DeepResearch Agent OpenAI DeepResearch Gemini DeepResearch Kimi Researcher Tongyi DeepResearch (30B-A3B) Humanitys Browse Comp Last Exam Browse Comp-ZH GAIA xbench DeepSearch WebWalker QA FRAMES 21.2 18.1 29.8 20.3 24.9 17.7 26.6 26.9 26. 32.9 26.4 14.1 30.0 12.2 49.7 28.3 51.5 43.4 37.5 28.8 49.2 29.1 58.1 42.9 46.7 66.0 57.7 63.1 68.3 60.0 67.4 70.9 70.0 50.0 71.0 65.0 67.0 69. 75.0 65.6 63.0 61.2 61.7 71.7 72.2 78.9 72.0 83.7 80.7 84.0 78. 90.6 demonstrating strong generalization across both English and Chinese tasks. It consistently surpasses both open and closed commercial systems, including OpenAI o3, DeepSeek-V3.1, and Gemini DeepResearch. On the newly released xbench-DeepSearch-2510, Tongyi DeepResearch ranks just below ChatGPT-5-Pro, demonstrating competitive performance at the forefront of the field. Notably, these gains are achieved with only 3.3 billion activated parameters per token, underscoring the models efficiency and scalability. In aggregate, Tongyi DeepResearch sets new state of the art among open-source deep research agents, narrowing and in some cases even surpassing the performance of frontier proprietary systems while maintaining superior interpretability and computational efficiency."
        },
        {
            "title": "4.3 Heavy Mode",
            "content": "Figure 6: Performance comparison between Tongyi DeepResearch Heavy Mode and state-of-the-art models. To further unlock the potential of deep research agents, we introduce the Heavy Mode, which leverages test-time scaling through Research-Synthesis framework built upon the context management paradigm. Given that DeepResearch involves multi-round tool calls and intensive reasoning, directly aggregating contexts from multiple trajectories is computationally prohibitive. Our Heavy Mode addresses this challenge through strategic parallelization and synthesis. Parallel Research Phase. We deploy parallel agents, each following the context management paradigm but exploring diverse solution paths through different tool usage and reasoning strategies. Each agent independently processes the question and produces final report and answer: (Su T, answeru) = Agentu(q), [1, n] (7) where Su reasoning trajectory in compressed form. represents the final report summary from agent after iterations, encapsulating the complete 12 Integrative Synthesis Phase. synthesis model consolidates all parallel findings to produce the final answer: answerfinal = Synthesis ({(Su T, answeru)}n u=1) , (8) The key advantage of this approach lies in the compressed nature of context management reports Su T. Unlike traditional methods that would require aggregating full trajectories (potentially exceeding context limits with just 2-3 agents), our approach enables the synthesis model to assess diverse solution strategies within manageable context window. Each report Su preserves the essential reasoning logic and findings while discarding redundant intermediate steps, enabling effective test-time scaling. As shown in Figure 6, our Heavy Mode achieves state-of-the-art performance on Humanitys Last Exam (38.3%) and BrowseComp-ZH (58.1%), while remaining highly competitive on BrowseComp (58.3%). These substantial improvements validate the effectiveness of our heavy mode based on context management in leveraging test-time compute through parallel exploration and intelligent aggregation."
        },
        {
            "title": "4.4 Detailed Analysis",
            "content": "Pass@1 and Pass@3 Performance. We report the Avg@3 performance in Table 1. Given the dynamic and complex nature of agent environments, we further conduct fine-grained analysis of Pass@1 (over three runs) and Pass@3 in Figure 7. Despite the unstable evaluation environment, our final Avg@3 results are consistent with the Pass@1 (best result over 3 runs) results, demonstrating the robustness of our deep research approach. Our Pass@3 performance demonstrates the strong potential of our agent. In particular, it achieves 59.64 on BrowseComp, 63.67 on BrowseComp-ZH, and 45.9 on Humanitys Last Exam. Figure 7: Detailed evaluation results using Avg@3, Pass@1 and Pass@3 metric. Training Rewards and Entropy. As shown in Figure 8, the agents performance exhibits clear and significant upward trend with training, confirming effective policy learning. The sustained nature of this improvement underscores the success of our dynamic data curation, which prevents learning from stagnating by consistently providing challenging material. Concurrently, the policy entropy exhibits exceptional stability, converging to consistent value after brief initial increase and thereby avoiding both collapse and explosion. This outcome serves as strong evidence for our methodological contributions in environment design and algorithm modification, which together create the necessary conditions for remarkably stable and effective RL training paradigm. Context Length of RL. In Figure 10, we analyze the impact of the models context length on the agentic RL training process, comparing models with 32k, 48k, and 64k context limits. It is important to note 13 Figure 8: Reward and entropy loss of agentic RL training. that the dynamic data curation for all three experimental variants was performed using the same model with 64k context. Focusing first on the reward dynamics in the left panel, we observe that all three models demonstrate effective and stable policy learning, evidenced by monotonically increasing reward. This confirms the robustness of our training framework. However, their performance ceilings diverge significantly, which is an expected consequence of our data curation method. Because the curriculum is populated with problems deemed moderately difficult by the highly capable 64k context model, many of these problems inherently require long and complex reasoning to solve. Consequently, clear hierarchy emerges: the 64k model, perfectly matched to its own data, achieves the highest reward. The 48k and 32k models, being increasingly constrained, are unable to solve the most complex problems in the curriculum, thus capping their maximum potential reward. The training dynamics in the right panel reveal more interesting story. The model with 64k context exhibits steady increase in average response length, learning to leverage its expansive context to build more elaborate solutions. In contrast, the model with 48k context maintains consistent equilibrium, improving its policy within stable complexity budget. Most surprisingly, the model with 32k context displays clear downward trend in response length. This observation provides key insight: for model with limited context, RL training on curriculum designed for more capable model can force it to discover more efficient solutions. This effect arises because our dynamic data curriculum is continuously updated using the 64k context model, process that populates the training set with problems whose optimal solutions can be longer than 32k tokens. For the model with 32k context, attempting these problems is likely to yield zero-reward signal. This creates powerful implicit incentive to discover more concise, potent action sequences that fit within its limit, thus becoming more efficient over time. Figure 9: Comparison of different context length limits for RL training. Interaction Test-time Scaling. Unlike conventional models, the DeepResearch agent primarily relies on interactions with the environment to acquire information and accomplish tasks. Therefore, the number of 14 (a) Interaction turns scaling for BrowseComp. (b) Reward in the simulated environment. Figure 10: Detailed analysis on interaction scaling and simulated environments. interaction turns with the environment is crucial. While reasoning models can be scaled by increasing the number of output tokens, our approach scales along different dimension, the number of environment interactions. Naturally, as the number of interactions increases, the agent obtains more observations from environment, resulting in longer context. Figure 10a illustrates our scaling curve: as the context length and number of interactions grow, the models performance on the BrowseComp dataset improves consistently. Super-human Level Synthetic Data. To validate the effectiveness of our synthetic data, we conducted statistical analysis of the SFT dataset. Over 20% of the samples exceed 32k tokens and involve more than 10 tool invocations. This demonstrates the high complexity and richness of our synthetic data. Such high-quality, cold-start data provides the model with strong foundation for deep reasoning and research capabilities, serving as an excellent initialization for the RL phase. During reinforcement learning, we leverage automated data curation to make more effective use of the synthetic data. From Simulation to Reality. To rapidly validate our algorithm, we built simulated Wiki environment that mirrors real-world conditions. We test our adapted GRPO algorithm in this environment, and the resulting reward curve, shown in Figure 10b, closely matches the one observed in the real environment, as shown in Figure 8. This Wiki simulation environment provides functionality analogous to \"wind tunnel laboratory\", enabling fast algorithm iteration and significantly improved our development efficiency. Performance on General Benchmark. We evaluate three general benchmarks, AIME25, HMMT25 and SimpleQA (OpenAI, 2025c). The results are shown in Figure 11. Experimental results demonstrate that Tongyi DeepResearch achieves substantial improvements over the base model, which relies solely on reasoning without any tool use. On one hand, the system can retrieve external information via search, which proves particularly effective for knowledgeintensive benchmarks, and on the other, Python Interpreter enables it to enhance performance on mathematical reasoning tasks through native computational support. Looking ahead, model training increasingly converges with agent training, solving paradigms evolve toward agentic architectures that integrate tool invocation and environment interaction, reflecting more human-like problem-solving process. Figure 11: Performance on general benchmarks."
        },
        {
            "title": "5 Discussion",
            "content": ""
        },
        {
            "title": "5.1 Limitations",
            "content": "We acknowledge several limitations in our current work: First, the current 128K context length remains insufficient for handling the most complex long-horizon tasks, motivating further exploration of extended context windows or more advanced context management mechanisms (Qiao et al., 2025; Wu et al., 2025c). Second, we have not yet released larger-scale model. Although the smaller-sized model already demonstrates strong performance, larger model is currently in progress. Third, we are continuously improving report generation fidelity and optimizing for user preferences to ensure more faithful, useful, and preference-aligned outputs (Li et al., 2025e). Fourth, we aim to improve the efficiency of our reinforcement learning framework by exploring techniques such as partial rollouts, which will require addressing off-policy training challenges, including distributional shift. Finally, our current Deep Research training focuses on specific prompt instructions and predefined tool sets. We plan to enhance its robustness and extend the framework from Deep Research to broader agentic tool use scenarios."
        },
        {
            "title": "5.2 Model Scale",
            "content": "We believe that training agentic capabilities on relatively small models is highly valuable (Belcak et al., 2025). Smaller models are inherently more efficient to deploy on edge devices, broaden accessibility across diverse real-world scenarios, and deliver faster, more responsive interactions. This direction aligns with the broader goal of making autonomous research agents both powerful and practically deployable."
        },
        {
            "title": "5.3 What’s Next",
            "content": "We have long-standing commitment to advancing research and development in deep research agents. The Tongyi DeepResearch represents significant step toward AI systems capable of autonomously transforming information into insight. We advocate for open-source models with emergent agency, which are essential for democratizing agentic intelligence and deepening our fundamental understanding of how agency can emerge and scale in open systems. Looking ahead, we aim to evolve from domain-specific agents to general-purpose agents, which are capable of reasoning, planning, and acting autonomously across diverse domains with minimal human supervision. To achieve this, we are developing the nextgeneration agent foundation model, unified model designed to endow AI systems with scalable reasoning, memory, and autonomy, enabling them to operate as truly general agents. We believe it will empower individuals and organizations to reach new heights of productivity and innovation."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Tongyi DeepResearch, an open-source deep research agent that unifies agentic midtraining and post-training into scalable, end-to-end paradigm. Through automated data synthesis and stage-specific environments, the model learns to plan, search, reason, and synthesize information autonomously. Despite its efficiency, activating only 3.3B parameters, Tongyi DeepResearch achieves state-of-the-art results on multiple deep research benchmarks, surpassing strong proprietary systems. This work establishes foundation for open, reproducible research into autonomous AI agents and marks step toward more general, self-improving intelligence."
        },
        {
            "title": "Contributions",
            "content": "The names are listed in alphabetical order by first name. Project Leader Yong Jiang Core Contributors Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao Contributors Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li"
        },
        {
            "title": "References",
            "content": "anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025. Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Yuzhi Zhang, Linfeng Zhang, Siheng Chen, et al. Scimaster: Towards general-purpose scientific ai agents, part i. x-master as foundation: Can we lead on humanitys last exam? arXiv preprint arXiv:2507.05241, 2025. Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Krähenbühl. Reinforcement learning for long-horizon interactive llm agents. arXiv preprint arXiv:2502.01600, 2025. Claude Team. Claude research, 2025. URL https://www.anthropic.com/news/research. DeepSeek Team. Introducing deepseek-v3.1: our first step toward the agent era!, 2025. URL https: //api-docs.deepseek.com/news/news250821. Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, et al. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311, 2025. Gemini Team. Gemini deep research, 2025. URL https://gemini.google/overview/deep-research/. Grok Team. Grok-3 deeper search, 2025. URL https://x.ai/news/grok-3. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jina.ai. Jina, 2025. URL https://jina.ai/. Kimi. Kimi-researcher: End-to-end rl training for emerging agentic, 2025. URL https://moonshotai.g ithub.io/Kimi-Researcher/. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 47454759, 2025. Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, and Minhao Cheng. Lara: Benchmarking retrieval-augmented generation and long-context llmsno silver bullet for lc or rag routing. arXiv preprint arXiv:2502.09977, 2025a. Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, et al. Websailor-v2: Bridging the chasm to proprietary agents via synthetic data and scalable reinforcement learning. arXiv preprint arXiv:2509.13305, 2025b. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025c. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025d. doi: 10.48550/ARXIV.2504.21776. URL https://doi.org/10.48550/a rXiv.2504.21776. 18 Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025e. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. OpenAI. Deep research system card, 2025a. URL https://cdn.openai.com/deep-research-system-c ard.pdf. OpenAI. Introducing openai o3 and o4-mini, 2025b. URL https://openai.com/index/introducing-o 3-and-o4-mini/. OpenAI. Introducing simpleqa, 2025c. URL https://openai.com/index/introducing-simpleqa/. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. arXiv preprint arXiv:2509.13309, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, et al. Scaling agents via continual pre-training, 2025. Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. Sijun Tan, Michael Luo, Colin Cai, Tarun Venkat, Kyle Montgomery, Aaron Hao, Tianhao Wu, Arnav Balyan, Manan Roongta, Chenguang Wang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. rllm: framework for post-training language agents. https://pretty-radio-b75.notion.site/rLLM-A -Framework-for-Post-Training-Language-Agents-21b81902c146819db63cd98a54ba5f31, 2025. Notion Blog. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. 19 Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025b. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, et al. Resum: Unlocking long-horizon search intelligence via context summarization. arXiv preprint arXiv:2509.13313, 2025c. Xbench Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025."
        },
        {
            "title": "A Rollout Details",
            "content": "System Prompt You are deep research assistant. Your core function is to conduct thorough, multi-source investigations into any topic. You must handle both broad, open-domain inquiries and queries within specialized academic fields. For every request, synthesize information from credible, diverse sources to deliver comprehensive, accurate, and objective response. When you have gathered sufficient information and are ready to provide the definitive response, you must enclose the entire final answer within <answer></answer> tags. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\": \"function\", \"function\": {\"name\": \"search\", \"description\": \"Perform Google web searches then returns string of the top search results. Accepts multiple queries.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"array\", \"items\": {\"type\": \"string\", \"description\": \"The search query.\"}, \"minItems\": 1, \"description\": \"The list of search queries.\"}}, \"required\": [\"query\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"visit\", \"description\": \"Visit webpage(s) and return the summary of the content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"url\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The URL(s) of the webpage(s) to visit. Can be single URL or an array of URLs.\"}, \"goal\": {\"type\": \"string\", \"description\": \"The specific information goal for visiting webpage(s).\"}}, \"required\": [\"url\", \"goal\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"PythonInterpreter\", \"description\": \"Executes Python code in sandboxed environment. To use this tool, you must follow this format: 1. The arguments JSON object must be empty: {}. 2. The Python code to be executed must be placed immediately after the JSON block, enclosed within <code> and </code> tags. IMPORTANT: Any output you want to see MUST be printed to standard output using the print() function. Example of correct call: <tool_call> {\"name\": \"PythonInterpreter\", \"arguments\": {}} <code> import numpy as np # Your code here print(f\"The result is: np.mean([1,2,3])\") </code> </tool_call>\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}} {\"type\": \"function\", \"function\": {\"name\": \"google_scholar\", \"description\": \"Leverage Google Scholar to retrieve relevant information from academic publications. Accepts multiple queries. This tool will also return results from google search\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"array\", \"items\": {\"type\": \"string\", \"description\": \"The search query.\"}, \"minItems\": 1, \"description\": \"The list of search queries for Google Scholar.\"}}, \"required\": [\"query\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"parse_file\", \"description\": \"This is tool that can be used to parse multiple user uploaded local files such as PDF, DOCX, PPTX, TXT, CSV, XLSX, DOC, ZIP, MP4, MP3.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"files\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The file name of the user uploaded local files to be parsed.\"}}, \"required\": [\"files\"]}}} </tools> For each function call, return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <argsjson-object>} </tool_call> Current date: 21 The above constitutes the system prompt of our ReAct rollout."
        },
        {
            "title": "B Evaluation Details",
            "content": "For GAIA and WebWalkerQA, following the evaluation protocol of Li et al. (2025d), we adopt Qwen2. 5-72B-Instruct as the judging model. The evaluation prompt is kept identical to that used in their work to ensure consistency and comparability. For xbench-DeepSearch and xbench-DeepSearch-2510, we adopt Gemini-2.0-Flash-001 as the judge model. For BrowseComp and BrowseComp-ZH, we employ GPT-4o-2024-08-06 as the judge model. For Humanitys Last Exam, we evaluate the 2,154 text-only questions following Chai et al. (2025). The evaluation prompt follows the official protocol, with the o3-mini serving as the evaluator. The evaluation prompt for these benchmarks is kept consistent with that described in the original paper to ensure alignment and reproducibility. The evaluation prompts used for each benchmark is provided in detail on our GitHub repository5. For general benchmarks, we adopt different evaluation strategies based on task type. For mathematical problems, since our system outputs detailed report and datasets such as AIME25 and HMMT25 are relatively small in scale, we employ manual evaluation to ensure accuracy and fairness. For knowledgebased problems, we utilize the official evaluation script of SimpleQA to maintain consistency with established benchmarks. Post-training Synthetic Data Case Question: military officer, who also served as governor in western North American territory, commanded mounted infantry unit during period of significant mineral discovery in the region. His official report on the discovery prompted the minting of special commemorative coin in certain year in the mid-19th century. the unit he commanded was involved in military conflict against neighboring country. Just over decade later, this unit was officially redesignated and would be assigned to new division in the early 1920s. In the 1930s, this redesignated regiment was involved in an organizational swap. Answer: 12th Cavalry Regiment Which other regiment was it exchanged for? During that same year, Question: An 18th-century travelogue, later adapted for radio series, describes port town in southeastern England as notable for its rampant illicit trade. This town was also the home of 16th-century gentleman whose murder led to his wifes execution. Centuries later, another resident of the same town was granted letters patent providing special commercial privileges in particular year of the early 19th century. year, collector, whose large collection of manuscript poems was later auctioned, secured patent for method of grinding inks. German family; what is the German term for the princely status it conferred? Answer: Fürstenstand In that year, patent of nobility was issued to During that same Question: In trisilylamine (N(SiH3)3), the Si-N bond length is 1.736 Å. Substituting one silyl group with methyl to form (CH3)N(SiH3)2 elongates the Si-N bond to 1.752 Å. Calculate the percentage increase in bond length due to diminished hyperconjugation, and identify which specific orbital interaction weakens most significantly. Si=1.11 Å, N=0.70 Å, C=0.77 Å. Answer: σ Use covalent radii: SiC The first two cases above are synthetically generated high-quality, high-uncertainty, superhuman questionanswer pairs, examples of caliber that is exceptionally difficult to produce via human annotation. The third case represents PhD-level research question, demanding deep domain expertise, multi-step reasoning. 5https://github.com/Alibaba-NLP/DeepResearch/tree/main/evaluation"
        },
        {
            "title": "D Environment Details",
            "content": "We utilize five tools for Tongyi DeepResearch, namely Search, Visit, Python Interpreter, Google Scholar, and File Parser6: Search leverages the Google search engine for information retrieval. The tool accepts list of one or more search queries to be executed concurrently. For each query, it returns the top-10 ranked results, with each result comprising title, descriptive snippet, and its corresponding URL. Visit is designed for targeted information extraction from web pages. The tool takes as input set of web pages, where each page is paired with dedicated information-seeking goal. The process begins by employing Jina (Jina.ai, 2025) to parse the full content of given web page. Subsequently, summary model processes this content to extract only the information pertinent to that pages specific goal. Python Interpreter is used to execute Python code within sandboxed environment. The input is string of Python code, which must be enclosed within <code> tags for proper execution. The tool runs the provided code and captures its standard output; therefore, any results or values intended to be seen must be explicitly passed to the print() function. This capability enables dynamic computation, data manipulation, and the use of various Python libraries in secure and isolated manner. Google Scholar is used to retrieve information from academic publications. The input consists of list of one or more search queries, allowing for multiple, distinct searches within single tool call. The tool leverages the Google Scholar search engine to execute each query and gather relevant scholarly literature, such as articles, papers, and citations. File Parser answers user queries by analyzing mix of documents, web pages, and multimedia files (e.g., PDF, DOCX, MP4) from local or URL sources. It works in two steps: first, it converts all input into plain text, transcribing audio/video content when necessary. Second, summary model reads this unified text to generate direct answer to the users question 6Since our system relies on several internal APIs and fallback strategies (as described in Section 3.4.3), we provide alternative open implementations in our open-source GitHub repository to facilitate public use. We have verified through extensive testing that these substitutions can faithfully reproduce our results."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}