{
    "paper_title": "Unifying Specialized Visual Encoders for Video Language Models",
    "authors": [
        "Jihoon Chung",
        "Tyler Zhu",
        "Max Gonzalez Saez-Diez",
        "Juan Carlos Niebles",
        "Honglu Zhou",
        "Olga Russakovsky"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 2 4 1 0 . 1 0 5 2 : r a"
        },
        {
            "title": "UNIFYING SPECIALIZED VISUAL ENCODERS FOR\nVIDEO LANGUAGE MODELS",
            "content": "Jihoon Chung Tyler Zhu Max Gonzalez Saez-Diez Juan Carlos Niebles Honglu Zhou Olga Russakovsky Princeton University {jc5933, tylerzhu}@princeton.edu Salesforce Research"
        },
        {
            "title": "ABSTRACT",
            "content": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create unified representation of video, providing the VideoLLM with comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle wider range of open-ended and multiplechoice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Inspired by the sophisticated reasoning abilities of recent Large Language Models (LLMs) (Chiang et al., 2023; Chowdhery et al., 2023; OpenAI, 2023), researchers have focused on using them in many other domains to great success. The video counterparts, known as Video Large Language Models (VideoLLMs) (Bain et al., 2021; Li et al., 2023c; Lin et al., 2023a; Luo et al., 2023; Maaz et al., 2023; Yu et al., 2024), connect pretrained vision encoders to LLMs by training modality bridge from the vision space to the language space, allowing for reasoning to happen in the highly expressive language domain. Most multimodal LLMs, such as LLaVA (Liu et al., 2023) for images and Video-LLaVA (Lin et al., 2023a) for videos, opt for contrastively pretrained encoders like CLIP (Radford et al., 2021) and LanguageBind (Zhu et al., 2023). Their vision-language pretraining naturally lends itself as bridge between the vision input and the LLM, circumventing the need to train heavy vision-language alignment modules like QFormer (Li et al., 2022). These encoders are almost always pretrained separately and vary in architecture, training data, and optimization strategy. Consequently, the features extracted by these encoders exhibit unique characteristics, each with inherent strengths and limitations. Contrastive encoders like CLIP (Radford et al., 2021) may be better suited with their multimodal semantic alignment, but are inferior to models such as DINOv2 (Oquab et al., 2023) at fine-grained object level understanding. They also fail to take advantage of models trained specifically on videos, such as ViViT (Arnab et al., 2021). Despite this clear tension between vision backbones, previous research in VideoLLMs has relied on only one vision encoder for visual processing as one was thought to be sufficient for visual understanding, and already difficult enough to achieve vision-language alignment with. Any more encoders was unnecessary and not an effective tradeoff of runtime for compute. Equal contribution."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 1: Different visual experts exhibit individual strengths. We show some examples where one single encoder model is the only model to correctly answer Perception Test questions (Patraucean et al., 2023). In this paper, we argue that this choice to not use multiple encoders in existing VideoLLMs unnecessarily restricts their capabilities. For example, in Figure 1 we can see cases where only one of four different single-encoder models answers given question correctly. While simple scene descriptions can be answered by image-level models, other questions require temporal and actionlevel comprehension, benefiting from features encoded with video models like ViViT (Arnab et al., 2021). Consequently, the reasoning capabilities of these VideoLLMs are directly limited by the inherent weaknesses of their respective pretrained encoders. Therefore, employing multiple encoders could allow us to complement one encoders weaknesses with another encoders strengths. The wide adoption of the LLaVA paradigm is also indication that vision-language alignment is simple to achieve, even without language-aware vision models. We propose MERV, Multi-Encoder Representation of Videos, as new method for integrating multiple visual encoders into single VideoLLM using cross-attentive encoder mixer for fusing representations. We introduce spatio-temporally aligned representation for mixing the information from multiple types of visual encoders. Given the computational complexity of video tasks, we carefully experiment with optimization strategies and parallelizing the visual experts, allowing us to combine four distinct visual encoders with minimal computational overhead. Our frozen method outperforms all individual encoder methods, is up to 3.7% better than prior works (Lin et al., 2023a) on video reasoning benchmarks, i.e., from 47.1% to 50.8% on ActivityNet-QA (Yu et al., 2019), and on par with the state-of-the-art (Yu et al., 2024) on Perception Test (Patraucean et al., 2023), challenging perception and reasoning diagnostic for video models. Finetuning the full model improves MERV past SeViLA (Yu et al., 2024) by 2.2%, from 46.2% to 48.4%. Finally, we do detailed qualitative study of our models capabilities on the Something-Something v2 dataset (Goyal et al., 2017). We show that MERV can accurately capture both contrastive encoders (Zhai et al., 2023; Zhu et al., 2023) strengths on general vision-language understanding, as well as ViViTs (Arnab et al., 2021) specialty on temporally-sensitive tasks (e.g. distinguishing pushing left vs. right), without trading off performance between these specializations as single encoder models do. Our code and pretrained weights are available at https://github.com/princetonvisualai/merv."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "VideoLLMs build upon the powerful reasoning capabilities of LLMs by utilizing them as language decoders to enable instruction-followed video understanding. Key advancements include VideoChat (Li et al., 2023c) and Video-LLaMA (Zhang et al., 2023a) for chat-based video understanding, LLaMA-Adapter (Zhang et al., 2023b) for pre-alignment, Valley (Luo et al., 2023) with multilingual LLMs, InternVideo (Wang et al., 2022) with dedicated video encoder training phase, and VideoChatGPT (Maaz et al., 2023) combining video-adapted encoders with LLMs. GPT4Video (Wang et al., 2023b) supports video understanding and generation, while MovieChat (Song et al., 2023) focuses"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "on long video comprehension. Models like Chat-UniVi (Jin et al., 2023) and LLaMA-VID (Li et al., 2023e) optimize token usage for video representation. Other notable models include Vamos (Wang et al., 2023a), which flexibly uses visual embeddings, action labels, and video captions as input; VideoChat2 (Li et al., 2023d), developed through three-stage progressive training; Video-LLaVA (Lin et al., 2023a), which aligns image and video representations before projecting them to the LLM space; and VideoPrism (Zhao et al., 2024), which also further trains video encoder through masked distillation. Specialized models like VTimeLLM (Huang et al., 2023) focus on fine-grained video moment understanding and time-bound reasoning, while models like Elysium (Wang et al., 2024) and Merlin (Yu et al., 2023) can predict object trajectories. SeViLA (Yu et al., 2024) uses LLM for frame localizer of the video for multiple-choice tasks. Finally, recently LLaVA-Hound-DPO (Zhang et al., 2024) explored using DPO and higher quality training set for better instruction following. Distinct from these aforementioned works, our approach centers on utilizing diverse array of visual and especially video encoders, each with its own unique strengths, to significantly enhance the capabilities of the VideoLLM framework. By strategically using these specialized encoders, we aim to capture broader spectrum of visual information, enriching VideoLLMs understanding of videos. Combining multiple encoders for multimodal LLMs is gaining attention. Eyes Wide Shut (Tong et al., 2024) explored mixing DINOv2 and CLIP features for LLaVA, but their results signal that mixing features effectively requires investigation. Both Mipha (Zhu et al., 2024) and PrismaticVLMs (Karamcheti et al., 2024) found that image encoders like CLIP and SigLIP, which are trained using vision-language contrastive loss, surpass other image encoders such as ViT and DINOv2, with SigLIP showing further improvements over CLIP. SPHINX-X (Gao et al., 2024) and SPHINX (Lin et al., 2023b) combines multiple image encoders by concatenating features along the channel dimension, while BRAVE (Kar et al., 2024) concatenates features from multiple encoders sequence-wise, followed by QFormer with masked modeling. There is also the popular body of research on multimodal LLMs using many modalities including image, video, audio and/or 3D (Chen et al., 2023; Han et al., 2024; 2023; Li et al., 2023a; Liu et al., 2024a;b; Lyu et al., 2023; Panagopoulou et al., 2023; Su et al., 2023; Sun et al., 2023a; Zhang et al., 2023a). In contrast, this paper dives into the video-language domain, exploring combining multiple image and video encoders and exploiting their structural similarities. Our feature fusion is both performant and efficient in FLOPs, and results in an all-encompassing additive mixture of features which previous works could not achieve."
        },
        {
            "title": "3 MERV: MULTI-ENCODER REPRESENTATION OF VIDEOS",
            "content": "Our goal for MERV is to systematically build video model that leverages multiple encoders with an LLM to process video following the LLaVA/PrefixLM (Liu et al., 2023; 2018) paradigm (see Figure 2). Unlike previous works, our focus is not on combining multiple modalities (Bachmann et al., 2022; Zhu et al., 2023), but instead on combining multiple image and video encoders trained on different datasets and objectives. We extensively ablate three key aspects to make this possible: our selection of multiple encoders, i.e., which visual encoders and how many to use (Sec 3.1); how we align the spatio-temporal representations of each encoder to mix the information together, especially in an efficient manner (Sec 3.2); and our implementation efficiencies, from the parallel visual processing to the training recipes (Sec 3.3). 3.1 MULTI-ENCODER FEATURE EXTRACTION Our final architecture uses four distinct types of models: spatial experts, fine-grained temporal experts, image-language experts, and video-language experts. We found experimentally that our choice of four performed the best across all types of questions, and ablate our choices in Section 4.2. More details about these four encoders and other encoders we considered are in Appendix Table 4. Spatial expert: DINOv2 (Oquab et al., 2023) is trained using unsupervised learning on local-toglobal correspondences in image data. The resulting features have robust object part understanding, as well as semantic and low-level image understanding, but can suffer from poor language grounding. Temporal expert: ViViT (Arnab et al., 2021) is trained using supervised learning on short videos. The architecture is designed for modeling the interactions between frames using spatial and temporal attention, which lets it capture longer temporal dependencies than pure image models can."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 2: Overview of MERV, Multi-Encoder Representation of Videos. MERV proceeds in three main stages. First, we feed in our input video into each of visual encoders to get different representations. They are then spatio-temporally aligned before being fused by cross-attentive mixer. The output is visual embedding with an additive mix of information from all of the encoders, which is combined with the text query to produce our final generation. Image-Language contrastive expert: SigLIP (Zhai et al., 2023) is trained using sigmoid contrastive learning on image-text pairs. The model is designed to learn joint embedding space for images and text, which makes it good at understanding vision-language associations. However, it can overlook the finer details of an image which are not well described by text in its training data. Video-Language contrastive expert: Finally, our video-language expert is LanguageBind (Zhu et al., 2023). Used by Video-LLaVA (Lin et al., 2023a), LanguageBind is trained through joint multimodal learning between text and multiple modalities, including videos, infrared, and audio, and understands the relationship between video and text and their high-level semantics. We only use the video encoder of LanguageBind. 3.2 SPATIO-TEMPORALLY ALIGNED REPRESENTATIONS FOR FEATURE FUSION Our input is batch of text, image-text, or video-text queries. The visual part of the input, either images or videos V, is passed through each of the visual encoders to extract the respective features. Here we describe the detailed care we took in pre-processing to prepare the features for alignment. First, images are treated as videos with repeated frames, so assume all inputs are videos from here on out. video is of shape , where is the number of frames and H, are the height and width of the frames, and produce an output of shape te he we for an encoder e. One obstacle with using different visual encoders is that each model outputs features with different structure. For example, given an input of shape 16 224 224, ViViT outputs feature of shape 8 14 14 whereas LanguageBinds features are of shape 16 16 16. Image-based encoders will not change the temporal dimension, whereas ViViT downsamples the frames by factor of 2. For temporal alignment, as each encoder is flexible enough to handle varying input frames, we simply choose our input for each encoder so that each output te is the same across all encoders, i.e. t. Pre-fusion projection. Now we need to achieve spatial alignment among the features. Naïvely combining them is not possible as they all have different spatial shapes, and would also be prohibitively expensive at full resolution. We design pre-fusion projector to both align and compress them. Suppose our feature from encoder is ve Rthewede , where de is the dimension of encoder e, and assume the output spatial representations are square (i.e. he = we, but we keep notation for clarity). Our pre-fusion projector uses an adaptive 2D average pool for each encoder to resize the spatial dimensions to the same for all encoders, where < he and < we. As is the same across each ve, this spatio-temporally aligns the representations. Finally, we need to connect the varying embedding dimensions de to same dimensional space. We add linear layer to project the features from dimension de to d, the LLMs dimension. In total, our"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "pre-fusion projection is xe := P(ve)We Rℓd (1) where We Rded is each encoders output linear layer, and ℓ = w. This projector is lightweight, having only (cid:80) de trainable parameters for dimension matching, making it easy to scale to an arbitrary number of visual encoders. For detailed ablations, see Section 4.2.1. for Encoders Feature fusion strategies. The final part of our pipeline is fusing the multi-encoder information together using cross-attention with learnable queries to additively mix the different representations together. The visual features determine the weights of the linear mixture, which we find sufficient for our task. We use single randomly initialized query R1d, keys as = [x1 . . . xN ] RN dL, where xe Rd is each encoders features averaged over the sequence dimension ℓ for faster computation, and the number of encoders, and values as = [x1 . . . xN ] RN ℓd. We calculate our final unified feature as := Softmax (cid:33) (cid:32) QX Rℓd. (2) The final step is to concatenate the visual embedding and tokenized text together into the LLM. We use the base LLaMA-2 7B model (Touvron et al., 2023b), which we found performs better than the chat model. We test multiple alternate feature fusion strategies and their tradeoffs in Section 4.2.2. 3.3 IMPLEMENTATION EFFICIENCIES Parallelized visual encoding. At first glance, using multiple encoders seems to be large cost to pay when comparing the raw FLOPs and parameters. However, key benefit of the LLaVA style architecture is that the entire feature extraction and projection pipeline can happen in parallel. To make this possible, we build on top of recent powerful advances in parallel processing for LLMs and use PyTorchs Fully Sharded Data Parallel (Zhao et al., 2023). As the video encoders themselves are much smaller than the LLM blocks and complete in around the same time, most of the overhead in running four encoders is already covered by having one encoder. We provide some timing numbers in Section 4.2.3 and find that our step time is similar to that of the single-encoder methods. Our code is built on top of the Prismatic VLM codebase (Karamcheti et al., 2024), which efficiently implements vision-language model (VLM) training. We add the ability to handle videos and an arbitrary number of visual encoders, along with many useful features for training. Our training is efficient for using multiple visual models, completing in under 24 hours using 8 L40-48GB GPUs, and down to 8 hours using 8 H100s. The Video-LLaVA codebase runs Stage 2 in around 38 hours on the same L40 setup and could not easily support multiple encoders in our initial attempts. MERV frozen and full. Many different recommendations for training LLaVA style models have been made since its inception. This is only made more complicated by the introduction of new datasets with every new VideoLLM architecture, making it difficult to properly determine the best recipe for ones own setup. We intentionally fix our dataset to be the same as Video-LLaVAs so we can isolate the impacts of the training setup, from which we find two viable settings. MERV (frozen), which performs only Stage 2 instruction tuning and achieves similar results to the original Video-LLaVA recipe in only 43% of the time, and MERV (full), which unfreezes the LLM during Stage 1 as well for slight improvement on few benchmarks. As MERV (frozen) is faster to train with similar performance, we adopt that recipe by default for analysis, and interchangeably use MERV to refer to it for simplicity from here on out. Detailed analysis is provided in Section 4.2.3."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "In this section, we show that our method outperforms prior works across standard video benchmarks before moving onto an in-depth analysis of our method and its specializations in the next section. Datasets and training procedure. Our data mix is the same as Video-LLaVA (Lin et al., 2023a). The Stage 1 data is single-turn concise captioning, with 558k (image, text) pairs from LAION filtered by LLaVA (Liu et al., 2023) and 702k (video, text) pairs from Valley (Luo et al., 2023). The Stage"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Methods Alternative data mixes MSVD-QA MSRVTT-QA Score Acc Score Acc TGIF-QA Acc Score Video-Chat (Li et al., 2023c) LLaMA-Adapter (Zhang et al., 2023b) Video-LLaMA (Zhang et al., 2023a) Video-ChatGPT (Maaz et al., 2023) SeViLA (Yu et al., 2024) LLaMA-VID-7B* (Li et al., 2023e) LLaMA-VID-13B* (Li et al., 2023e) 56.3 54.9 51.6 64.9 - 69.30 70.25 2.8 3.1 2.5 3.3 - 3.74 3.77 45.0 43.8 29.6 49.3 - 57.84 58.58 Same data mixes Video-LLaVA* (Lin et al., 2023a) MERV (frozen) Gains to Video-LLaVA* MERV (full) Gains to Video-LLaVA* 56.90 3.69 67.74 59.03 70.97 3.76 +3.23 +.07 +2.13 3.79 70.48 57.25 +2.74 +.10 +0.35 2.5 2.7 1.8 2.8 - 3.24 3.26 3.18 3.25 +.07 3.24 +.06 - - - - - 51.31 51.26 - - - - - 3.26 3.26 3.17 47.99 3.26 51.1 +3.11 +.09 51.39 3.28 +3.40 +. Perception ActivityNet-QA NExT-QA VLEP TVQA Acc Acc Score - - - - 46.2 41.64 41.54 44.22 46.21 +1.99 48.41 +4. 26.5 34.2 12.4 35.2 - 46.45 46.79 47.08 50.87 +3.79 49.93 +2.85 2.2 2.7 1.1 2.7 - 3.22 3.23 3.27 3.34 +.07 3.33 +.06 Acc - - - - - 63.6 60.61 60.03 - 59.61 63.09 +3.48 61.36 +1.75 Acc Acc - - - - 64.4 57.65 61.98 61.21 58.66 -2.55 60.07 -1.14 - - - - 38.2 37.43 41.33 37.66 42.28 +4.62 39.42 +1.76 Table 1: Comparison of different multimodal LLMs on video reasoning benchmarks. We employ ChatGPT to evaluate performance following Video-ChatGPT where applicable1(version gpt-3.5-turbo-0613). * denotes our evaluation of using the author provided checkpoint. The first five datasets were used as development sets; the last three were held-out for our final evaluation. 2 data is multi-turn conversations, detailed captioning and reasoning, with 665k (image, text) pairs from LLaVA and 100k (video, text) instructions from Video-ChatGPT (Maaz et al., 2023). All the preprocessing, including frame extraction, adheres to the original method that each encoder is trained with. We extract 16 uniformly sampled frames from each video, except for ViViT which extracts 32 frames by default but produces 16-frame output feature. For MERV (frozen), we train on only Stage 2 data for 1 epoch with learning rate of 2 105 and batch size of 128 with gradient accumulation. For MERV (full), we first train on Stage 1 data with learning rate of 1 104 and the projectors, feature fusion, and LLM unfrozen with similar settings. Both recipes use an initial warmup ratio of 0.03 and cosine schedule. Evaluation. We evaluate our model on comprehensive suite of video understanding benchmarks, including the open-ended MSVD-QA (Xu et al., 2017), MSRVTT-QA (Xu et al., 2017), TGIF (Jang et al., 2017), and ActivityNet-QA (Yu et al., 2019), as well as the multiple-choice benchmarks NExT-QA (Xiao et al., 2021), VLEP (Lei et al., 2020), TVQA (Lei et al., 2018), and Perception Test (Patraucean et al., 2023). We emphasize that NExT-QA, VLEP, and TVQA datasets are held-out datasets that we did not use during our experiments, and only evaluated once after all the design is completed. We report both accuracy and score following the Video-ChatGPT evaluation protocol where applicable, and all evaluations are done zero-shot without any dataset-specific fine-tuning. Results using GPT-3.5-turbo for evaluation are done with the June 13th, 2023 cutoff date. 4.1 COMPARISON TO STATE OF THE ART Table 1 tabulates the performance of MERV (frozen) and (full). We compare our model to the existing state-of-the-art works, including Video-LLaVA (Lin et al., 2023a) that share our training data mixture, and other VideoLLMs (Li et al., 2023c;e; Maaz et al., 2023; Yu et al., 2024; Zhang et al., 2023a;b). We find that our method, generating video representations using multiple visual encoders that specialize in different skills of video understanding, outperforms Video-LLaVA across nearly all of the benchmarks, with 3.2% gain on MSVD and 3.7% gain on ActivityNet. Both of our methods perform better overall than Video-LLaVA, even when using less data with just Stage 2 as shown by the MERV numbers. While MERV (full) is not strict improvement to MERV, it still improves on some difficult benchmarks with its additional video-language alignment. We believe that outside of these testing sets, MERV (full) is better model overall and recommend using this recipe when possible. Compared to LLaMA-VID (Li et al., 2023e), which uses different training mix, we also better in nearly all benchmarks, up to around 4.5% across Perception Test, ActivityNet, and TVQA. MERV (full) outperforms the previous state-of-the-art on the Perception Test zero-shot with 48.4%, compared to SeViLa (Yu et al., 2024) with 46.2% accuracy. Overall, our design shows significant improvement over Video-LLaVA and prior methods as whole. 1Video-ChatGPTs and Video-LLaVAs author-reported numbers on TGIF are incomparable as they were on subset of the dataset. See https://github.com/PKU-YuanGroup/Video-LLaVA/issues/37."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Projector Avg Acc Params FLOPs - 257 tok - class tok 2.1M 2D Avg 2D Avg* 4.2M 12.7M 9.7G 2D Attn 237M 241G 2D Conv 3D Avg* 4.2M 113M 232G 3D Conv 54.76 52.05 54.96 55.86 52.12 54.23 55.09 55.42 - - 0 0 0 Tkns MSVD MSRVTT TGIF 54.64 41.41 1 55.72 45.32 4 56.44 47.75 16 58.00 50.01 64 57.47 48.78 100 57.73 48.81 144 57.72 48.66 256 61.94 64.47 67.23 69.08 68.38 68.65 68. Strategy Cross-Attn Concat (Seq.) Concat (Ch.) Learnable 25% - Mixed Avg Acc FLOPs 56.83 17.19 54.45 43.09 56.64 16.29 55.01 16.24 54.19 16.39 (c) Feature fusion strategy. Cross-Attn (a) Pre-fusion projectors. * is additive mixing is best overall among all 16 frames instead of 8. Top two the strategies on accuracy, for its FLOPs. rows are projector-free baselines. Table 2: Ablating design choices. We highlight our defaults in orange and bold the best results. Average accuracy is on MSVD, MSRVTT, TGIF, and Perception Test. Full results are in the Appendix. (b) Pre-fusion output token. We ablate the optimal token size per frame for the pre-fusion projector."
        },
        {
            "title": "4.2 ABLATIONS",
            "content": "In this section, we justify the design choices for our architecture, covering our projectors, feature fusion strategies, and training recipes. Our ablations are done with the MERV (frozen) recipe. 4.2.1 PRE-FUSION PROJECTORS The first module we investigate is our projectors, which serve to connect each encoder from its pretrained embedding space to common embedding space. We test two types of projectors: imagelevel, which operate on frames independently, and video-level, which aggregate information across frames. Projector details are provided in Section B.1 in the Appendix. We report average performance across our development sets of MSVD, MSRVTT, TGIF, and Perception Test. Pre-fusion projector. Table 2a tabulates our projector ablation on average accuracy, parameter count, and FLOPs, with the default settings of LanguageBind as the single vision encoder and an 8 frame 64 token projection output. We find that 2D average pooling is the best, with 55.86 average accuracy, even better than using no projector on the full 257 token representation (as in Video-LLaVA (Lin et al., 2023a)), with 54.76 average accuracy. It also has no trainable parameters and the fewest FLOPs. The projection serves as form of feature selection, allowing the LLM to efficiently reason only about the most relevant information. One result worth noting is the poor performance (52.12, 3.7% lower) of the attentive resamplers, popular projector choice. They are agnostic to structure, which leads them to being weaker projectors for us. However, it would likely be better with higher quality data from which the transformer can learn. Increasing the frame resolution from 8 to 16 was also large improvement, showing that increasing temporal resolution is still important. This highlights the importance of aligning representations with their spatial and temporal structure, especially for video models, which extract many more frames of visual information. Projector token length. Similarly, we ablate the output token length of the projector. Table 2b tabulates the performance of producing 1 to 256 tokens for 2D average pooling projector with 16 frames as input. We see that the performance peaks at 64 tokens with 69.08 MSVD accuracy, with worse performance for longer token lengths (max 68.65). This balances the number of tokens used for condensing the visual embedding while also minimizing the extra processing needed by the LLM. 4.2.2 FEATURE FUSION STRATEGIES Next, we test different strategies for fusing the information from all of the features, with detailed breakdowns in Table 2c. First, we evaluate two popular concatenation methods, in either the token sequence dimension, or the channel dimension followed by an MLP projector for matching the LLM dimension. While sequence-wise concatenation is widely used in multimodal LLMs (Tong et al., 2024), our method outperforms it while using significantly less computation, with 56.8% average accuracy compared to 54.4%, while also using 2.5 fewer FLOPs. Concatenation channel-wise reaches similar performance of 56.6% and lightweight cost. However, our cross-attention shows slightly better performance, with the additional benefit of having accessible encoder weightings for analysis, so we stick with cross-attention for our final design. We also try different methods of additive mixing as an ablation. The last two rows of Table 2c show the performance when either learning the additive weights directly as learnable scalar or by fixing the weights to be 0.25 for each"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "of 4 encoders. We see that using cross attention outperforms both methods by 1.8% and 2.6%, as our feature fusion module can dynamically generate better fused embeddings given the visual input."
        },
        {
            "title": "4.2.3 TRAINING RECIPES",
            "content": "Finally, we also compare different training recipes based on the literature and our own expertise. Traditional rule-of-thumb follows that of the original LLaVA recipe: Stage 1 pre-training on captioning data to align the projectors only, and Stage 2 instruction tuning on multi-turn complex reasoning data for both projectors and the LLM. Many recent works have attempted some combination of other strategies, such as unfreezing the vision encoders in Diao et al. (2024) or skipping the Stage 1 in Karamcheti et al. (2024). We systematically map out this landscape, fixing our dataset to be the same as Video-LLaVAs. Unlike them, we found that the Stage 1 phase did not help much when training only the projectors and feature fusion (Table 5 in the Appendix), with roughly the same average accuracy. Stage 2 instruction tuning alone leads to similar results in 43% of the total time, so we adopt this recipe for efficiency and refer to it as MERV (frozen). This recipe is still unsatisfying as it leaves large amount of data, approximately 1.3M vision-text pairs, unused for training. In our empirical observations, we found that video-language alignment was not very strong. The distributions of language used in video datasets and benchmarks sparsely overlap based on their sentence embeddings, which could be impacting our ability to generalize zero-shot on downstream benchmarks. We address this by unfreezing the LLM during Stage 1 to better learn this alignment, improving performance on few key benchmarks, especially Perception Test, by up to 2.2%. We call this recipe MERV (full). As another ablation, we train MERV on single stage comprised of the Stage 1 and Stage 2 data mixed together (bottom of Appendix Table 5). Surprisingly, this does worse than the explicit two stage training recipe. We attribute this to the explicit types of data in each stage being form curriculum learning, showing that these stages are still important for optimal performance. Finally, we provide evidence for the efficiency of our method (see Figure 3). We use the default FSDP sharding strategy PyTorch provides; it is not currently possible to specify explicit plans for which modules go where (but may be possible as FSDP matures). However, even with this basic strategy, our method is dominated by the slowest single encoder present, incurring very little additional overhead from extra encoders due to this parallelization, making it cost-efficient to scale up in the number of encoders. Figure 3: Extra encoders incur minimal step time overhead. Here we add encoders in the order of DINOv2, LanguageBind, SigLIP, ViViT, plotted alongside the slowest single encoder in each group. 4.3 ENSEMBLE COMPOSITION The original motivation of our work was to choose encoders with complementary visual knowledge to form comprehensive representation for our final model. The key questions are 1) do we benefit by using more than one encoder, and 2) do we need all four encoders, i.e. does each one meaningfully contribute to the final performance? Can we make use of more encoders? The conventional wisdom is to use single encoder, typically contrastively trained vision-language model like CLIP, SigLIP, or LanguageBind (Radford et al., 2021; Zhai et al., 2023; Zhu et al., 2023), in VideoLLM. In Figure 4a, we show the four single encoder models corresponding to each of our chosen encoders using their full embeddings. They not only all perform worse than MERV but also use more FLOPs, as without our pre-fusion projectors, their sequence lengths are at least 4 ours. Are each of the encoders contributing? To affirm that this set of four encoders is actually beneficial for improving understanding, we train three-encoder VideoLLMs under the same strategy, but removing different encoder each time. Each of these models does worse based on the strength of the"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "(a) Visual Encoder Subsets. MERV outperforms single-encoder VideoLLMs, with our feature projectors unlocking more computational effiency. Removing any encoder also reduces MERV performance. Figure 4: Analysis plots supporting our design of multiple encoders, from their accuracy to their skill specializations. Average accuracy is across MSVD, MSRVTT, TGIF, and Perception Test. Full results are in the Appendix Tables 7, 8, 10. (b) SSv2-MCQ and Temporal. Temporal denotes performance on 12 selected classes where actions are indistinguishable if played in reverse. encoder removed, meaning that MERV is using their knowledge (Fig. 4a). The minor drop in FLOPs illustrates how most of the computation is still dominated by the LLM, not the vision encoders."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 CROSS ATTENTION WEIGHTS ACTIVATE ON CORRESPONDING VIDEOS We first understand our method by looking at the cross attention weights on our 4 benchmark datasets (MSRVTT, TGIF, MSVD, and Perception Test), and visualizing the videos which have the highest attention weight for each of the encoders on Figure 12 in the Appendix. This lets us see what types of videos activate each encoder the most. As expected, ViViT attention weights are highest on videos with large motion, as ViViT has strong temporal understanding. Meanwhile, SigLIP is utilized for videos that have textual data in the video, likely due to being vision-language contrastively trained, especially with textual data during training. DINOv2 and LanguageBind are both preferred by videos with static scenes, but LanguageBind is preferred for videos with foreground motion. 5.2 MERV CAN CAPTURE VISUAL SKILLS OF DIFFERENT ENCODERS Next, we ask if our model effectively captures knowledge from its encoders. We first answer through our previous open-ended QA benchmarks. To assess the performance across different visual tasks, we create pseudo-skill categories by looking at the first word of the question sentence, which are often WH-words. They can be viewed as proxy of skills required to solve the task. For example, Where requires spatial understanding and When requires temporal understanding. Figure 5 shows the relative performance of different visual encoders. While the contrastive models generally dominate each category, no single encoder performs best in all tasks. LanguageBind, for example, performs the best in TGIF-What with 46.23%, while DINOv2 performs on par with the best in MSVD-Who with 82.12%. Our method combines different encoders into an unified representation and consistently matches or improves the best-performing encoder. Raw numbers are in Table 9 in the Appendix. 5.3 MERV CAN INTUIT MOTION AND GENERAL UNDERSTANDING SIMULTANEOUSLY We take another angle to quantifying how well our model learns from each of its individual encoders by looking back to classic video action recognition datasets. They are commonly used for general action understanding, but they can also be used to evaluate finer grained capabilities. We are most interested in both general understanding and distinguishing actions which are temporally ambiguous,"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 5: Single encoder vs. MERV on different types of video tasks. We plot the relative performance of VideoLLMs with different visual encoders. While each single encoder has its strength in different tasks, our method shows better performance than all the other single encoders in almost every task. We only plot tasks with more than 500 samples. See Appendix for details. Figure 6: Single-Encoder Performance Difference in Something-Something v2 - MCQ. ViViT shows better performance on tasks where temporal understanding is crucial, while LanguageBind and SigLIP show better performance where task can be solved from single-frame understanding. i.e. indistinguishable when reversed in time, such as Pulling [something] from left to right and Pulling [something] from right to left. This offers fair analysis of both general and video understanding. We turn to the Something-Something v2 (Goyal et al., 2017) (SSv2) dataset, where the goal is to classify an input video into one of 174 classes, e.g., Pulling [something] from left to right. This allows us to analyze our models understanding of temporal-spatial interaction with minimal distractions from scene understanding and real-world semantics. However, evaluating SSv2 as zero-shot openended task is difficult with long tail of specific categories. Thus we reformat the dataset into 5-way multiple-choice questions (MCQ) and fix the prompt to be How is the object in the video being interacted with?. Incorrect choices were randomly sampled from the other 173 classes. We call this benchmark SSv2 - MCQ to distinguish it from the original classification task. To form the temporal subset, we selected 12 classes priori from SSv2, where the action is indistinguishable if reversed in time, e.g., Pulling [something] from left to right and Pulling [something] from right to left. Figure 4b plots the performance of MERV and single-encoder models on this temporal subset (x-axis) against the full dataset (y-axis). We see that ViViT, which often falls short in other Video QA benchmarks, surprisingly performs better than other encoders at 39.77%, which is 9.19% higher than the next closest model LanguageBind. However for the full dataset, ViViT suffers with worse performance of 26.78%, as ViViTs strength is on temporal understanding despite lacking in vision-language understanding. Contrastive encoders have the upper-hand in most other classes. We believe that the architecture, datasets, and objective of each model causes these difference. ViViT processes spatial-temporal tubelets for embeddings, leading to better temporal understanding despite only being pre-trained on Kinetics-400 classification. SigLIP uses image-based ViT with no temporal layer has limited temporal understanding, but has greater knowledge due to its larger training set and contrastive objective. MERV, at 42%, shows better performance compared to all these single-encoder models via leveraging strength of all the individual encoders. MERV (full) performs better than both VideoLLaVA (Lin et al., 2023a) and the 7B and 13B variants of LLaMA-Vid (Li et al., 2023e). Finally, we also plot the performances of 10 SSv2 classes where the performance difference between ViViT and SigLIP is largest in Figure 6. We see that actions which cannot be inferred from single frame are the ones that ViViT performs better, e.g., Moving [something] down is indistinguishable from Moving [something] up if temporal information is omitted. Meanwhile, SigLIP performs better for classes where understanding the semantics of the scene can hint the action that is happening, e.g., if the video contains cup and bottle of water, one can easily expect Showing [something] is empty without watching the full video. See Appendix Figure 9 for sample videos of the 10 classes."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Previous VideoLLMs have been limited to relying on single visual model for feature extraction, which leads to limited understanding capabilities of vastly different video tasks. In our work, we break this paradigm and explore various fusion strategies for combining information from multiple visual experts to generate representation that can leverage the capabilities of different video encoders. We find that our multi-encoder feature fusion is able to outperform comparable methods by up to 3.79% on video reasoning benchmarks. We show that the method can obtain better performance than the best-performing single-encoder model with minimal computational overhead. Finally, we quantitatively and qualitatively observe the skill specializations our model learns on an MCQ format of Something-Something v2, which confirms both that encoders can be specialized and that our model captures both axes of knowledge. Our paper proposes some initial steps in rethinking how we approach the use of multiple encoders. We hope that this inspires others to also consider this problem as potentially another direction for scaling and improving their VideoLLMs. Acknowledgements: This work is supported by the National Science Foundation under Grant No. 2107048 and the Princeton First Year Fellowship to TZ. We also thank Allison Chen and William Yang for detailed comments and feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 18 Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In ICCV, 2021. 1, 2, 3, 17 Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task masked autoencoders. In ECCV, 2022. 3 Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 1 Adrien Bardes, Quentin Garrido, Jean Ponce, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv:2404.08471, 2024. 18 Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In CVPR, 2024. 18 Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv, 2023. 3 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. 1, Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023. 1 Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 17 StableLM contributors. Stablelm: Stability ai language models, 2023. URL https://github. com/Stability-AI/StableLM. 17 Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv, 2024."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR, 2023. 17 Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv, 2024. 3 Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The \"something something\" video database for learning and evaluating visual common sense. In arXiv, 2017. 2, 10, 17 Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv, 2023. 3 Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In CVPR, 2024. 3 Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. arXiv, 2023. 3 Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In CVPR, pp. 27582766, 2017. 6 Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv, 2023. 3 Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave: Broadening the visual encoding of vision-language models. arXiv, 2024. 3 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In ICML, 2024. 3, 5, Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. In EMNLP, 2018. 6 Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. What is more likely to happen next? video-and-language future event prediction. arXiv, 2020. 6 Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv, 2023a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 1288812900. PMLR, 2022. 1 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv, 2023b. 17 KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv, 2023c. 1, 2, 6, 17 Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. arXiv, 2023d. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv, 2023e. 3, 6, 10, 17 Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv, 2023a. 1, 2, 3, 4, 5, 6, 7, 10,"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv, 2023b. 3 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 3, 5 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. 3 Peter Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv, 2018. Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: vision-language model with multi-task experts. Transactions on Machine Learning Research, 2024b. 3 Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv, 2023. 1, 2, 5 Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv, 2023. 3 Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv, 2023. 1, 2, 6, 17, 20 Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. Mm1: Methods, analysis & insights from multimodal llm pre-training, 2024. 18 OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023. Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 1, 3, 17 Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. X-instructblip: framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning. arXiv, 2023. 3 Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. URL https://openreview.net/forum?id=HYEGXFnPoq. 2, 6 Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 8, 17,"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph Feichtenhofer. Hiera: hierarchical vision transformer without the bells-andwhistles. In ICML, volume 202 of ICML23, pp. 2944129454, Honolulu, Hawaii, USA, July 2023. JMLR.org. 18 Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv, 2023. 2 Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv, 2023. 3 Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Fine-grained audio-visual joint representations for multimodal large language models. arXiv, 2023a. 3 Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023b. 17 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 3, 7 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models, February 2023a. 17 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv, 2023b. 5, 16 Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level perception in videos via mllm, 2024. URL https://arxiv.org/abs/2403.16558. Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. Vamos: Versatile action models for video understanding. arXiv, 2023a. 3 Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv, 2022. 2 Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, and Zhaopeng Tu. Gpt4video: unified multimodal large language model for lnstruction-followed understanding and safety-aware generation. arXiv, 2023b. 2 Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In CVPR, 2021. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia, 2017. 6, 17 Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv, 2022. 18 En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. arXiv, 2023. 3 Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. NeurIPS, 2024. 1, 2, 3, 6,"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, 2019. 2, 6, 17 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 2, 4, 8, 17 Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv, 2023a. 2, 3, 6, 17 Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv, 2023b. 2, 6, Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward, 2024. 3 Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. arXiv, 2024. 3 Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. 5, 16 Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to nmodality by language-based semantic alignment. arXiv, 2023. 1, 2, 3, 4, 8, 17 Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. comprehensive overhaul of multimodal assistant with small language models. arXiv, 2024."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "A APPENDIX / SUPPLEMENTAL MATERIAL A.1 LIMITATIONS AND BROADER IMPACT. Our works are based on LLaMA-2 7B model (Touvron et al., 2023b), and as with many other VideoLLM models, the performance of our method is hugely dependent on the capabilities of the LLM model, and better-performing models often demand significantly more computation. MERV requires running multiple encoders, which can be computationally intensive and can lead to out-of-memory errors in limited resource settings. While FSDP (Zhao et al., 2023) allows us to easily and effectively train larger models across multiple GPUs than would otherwise be possible, its generality also makes it difficult for us to design tailored sharding strategies that would maximize the performance of our model. However, with future improvements to data parallelism, our model can still benefit greatly and run even more efficiently. Also, despite the improved speeds, there is still an upper bound for what constitutes reasonable training time that still allows us to test many of our design assumptions, which limits the scale and number of experiments we can run. While we show that our method can successfully leverage information from different visual encoders, nevertheless if the encoders themselves are limited in video understanding capability, MERV cannot fully compensate for that. Our work aims to facilitate video understanding, which can lead to positive social impacts such as video captioning model for low-vision users, automatic detection of medical emergencies, or better self-driving cars. It can also lead to negative social impacts like easy surveillance by the authorities, and human-like internet bots being used for scamming purposes. We follow the same safeguards implemented by the original authors of the datasets, the visual models, and the LLM models. We have not put any additional safeguards ourselves. A.2 QUALITATIVE RESULTS Figure 7: Samples of MERV in video understanding"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 8: MERV examples. MERV tend to show improved understanding in temporal-heavy videos as in Something-Something v2 dataset (Goyal et al., 2017) (Top Row), while retaining the performance on scenic understanding, seen from popular video benchmarks (Xu et al., 2017; Yu et al., 2019) (Bottom Row)."
        },
        {
            "title": "B TRAINING DETAIL",
            "content": "B.0.1 BASELINE ENCODER AND LLM DETAILS Model Visual Encoder LLM Video-Chat (Li et al., 2023c) LLaMA-Adapter (Zhang et al., 2023b) Video-LLaMA (Zhang et al., 2023a) Video-ChatGPT (Maaz et al., 2023) SeViLA (Yu et al., 2024) LLaMA-VID-7B (Li et al., 2023e) LLaMA-VID-13B (Li et al., 2023e) Video-LLaVA* (Lin et al., 2023a) ViT-G (EVA-CLIP) (Sun et al., 2023b) CLIP (Radford et al., 2021) ViT-G (EVA-CLIP) (Sun et al., 2023b) + BLIP-2 Q-Former (Li et al., 2023b) CLIP (Radford et al., 2021) ViT-G (EVA-CLIP) (Sun et al., 2023b) + BLIP-2 Q-Former (Li et al., 2023b) EVA-G (Fang et al., 2023) EVA-G (Fang et al., 2023) LanguageBind (Zhu et al., 2023) StableVicuna (contributors, 2023) LLaMA-1 7B (Touvron et al., 2023a) Vicuna-7B v0 (Chiang et al., 2023) Vicuna-7B v1.1 (Chiang et al., 2023) FlanT5-XL (3B) (Chung et al., 2024) Vicuna-7B v1.5 (Chiang et al., 2023) Vicuna-13B v1.5 (Chiang et al., 2023) Vicuna-7B v1.5 (Chiang et al., 2023) Table 3: Visual Encoder and LLM Information. B.0.2 MERV ENCODER DETAILS Model Architecture Expertise Training Datasets Training Objective LanguageBind (Zhu et al., 2023) DINOv2 (Oquab et al., 2023) ViViT (Arnab et al., 2021) SigLIP (Zhai et al., 2023) ViT-L/14 ViT-L/14 Spatial ViViT-B/162 Actions/Temporal Image+Language ViT-B/16 LVD-142M Kinetics-400/600, short videos 4B curated image/text pairs Video+Language VIDAL-10M, five-modal video examples Contrastive Self-Supervised Supervised Contrastive Table 4: Encoder Information. Detailed information about the four encoders used in our experiments. They represent broad coverage of visual information and training objectives. Here, we detail the visual encoder details, LLM, and the training objectives. We plan to release the code for the camera-ready version of the paper. LanguageBind We use the code from the original author, using the pre-trained weight LanguageBind/LanguageBind_Video_merge uploaded on huggingface. DINOv2 As DINOv2 is an image-model, we get embedding per frame, and concatenate them to be video embedding. We use ViTLarge model, pre-trained on LVD-142M Specifically, we use timms dataset, and take the penultimate layer for the embeddings. vit_large_patch14_reg4_dinov2lvd142m ViViT We use ViTbase as our backbone, pre-trained on Kinetics-400 dataset. Specifically, we use google/vivit-b-16x2-kinetics400 uploaded on huggingface. We use featurizer output as the video embedding."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "SigLIP As SigLIP is an image-model, we get embedding per frame, and concatenate them to be video embedding. We use ViTbase as our backbone, and take the penultimate layer for the embeddings. Specifically, we use timms vit_base_patch16_siglip_224 We also considered multiple other options for encoders, such as CLIP-ViP (Xue et al., 2022) for our video-language contrastive expert, V-JEPA (Bardes et al., 2024) and Hiera (Ryali et al., 2023) for our pure video model, and CLIP (Radford et al., 2021) for our image-language contrastive expert, but found that our choices performed better overall. B.1 DETAILED EXPERIMENTAL RESULTS. Here we tabulate the full experimental results that was abbreviated from the main paper. The first table  (Table 5)  ablates the different training recipes we tried for MERV, with extended discussion in Section 4.2.3. Methods MERV (frozen) MERV, Video-LLaVA recipe MERV (full) MERV, mixed Stage 1+ MSVD-QA MSRVTT-QA Score Acc 3.25 70.97 3.25 70.92 3.24 70.48 3.08 69.9 Score Acc 59.03 3.76 58.74 3.78 3.79 57.25 55.14 3.73 TGIF-QA Acc 51.1 51.67 51.39 51.53 Score 3.26 3.27 3.28 3.26 Perception ActivityNet-QA Acc 46.21 47.48 48.41 45. Acc 50.87 50.42 49.93 39.98 Score 3.34 3.33 3.33 2.95 Table 5: Ablation of training stage recipes. We explore different training recipe strategies, starting with the standard LLaVA recipe which Video-LLaVA adopted, along with some other variations. Pre-fusion projector details. The image-level projectors are similar to those described in MM1 (McKinzie et al., 2024): 2D adaptive average pooling, shallow attention resampler similar to Perceiver Resampler (Alayrac et al., 2022), and convolutional pooling with 3 RegNet blocks on both sides of an average pool layer such as the C-Abstractor in Honeybee (Cha et al., 2024). For video-level projectors, we use 3D average pool, where we pool to the same spatial dimension but furthermore pool the frame dimension by 2, and 3D convolution where we add single 2 3 3 convolution before the same average pooling. For all projectors, we project to the same number of tokens w, using an adaptive average pool or latent tokens for the attention resampler. We also provide the full numbers for our ablations in Sections 4.2.1 and 4.2.2. Projector MSVD MSRVTT TGIF Perc. Params FLOPs - 257 tok - class tok 2.1M 2D Avg 2D Avg* 4.2M 12.7M 9.7G 2D Attn 237M 241G 2D Conv 4.2M 3D Avg* 113M 232G 3D Conv 48.62 46.14 43.7 43.51 48.99 45.69 50.01 46.34 43.35 44.14 47.6 45.04 49.59 44.95 49.28 46.81 68.47 65.98 68.23 69.08 65.76 67.48 68.62 68.56 55.81 55 56.92 58 55.23 56.78 57.2 57.03 - - 0 0 0 (a) Pre-fusion projectors. * is 16 frames instead of 8. Top two rows are projector-free baselines. Tkns MSVD MSRVTT TGIF Perc. 54.64 41.41 42.85 1 55.72 45.32 43.31 4 56.44 47.75 43.18 16 58.00 50.01 46.34 64 57.47 48.78 45.56 100 57.73 48.81 43.94 144 57.72 48.66 43.51 256 61.94 64.47 67.23 69.08 68.38 68.65 68.46 (b) Pre-fusion output token. We ablate the optimal token size per frame for the pre-fusion projector. Strategy 70.97 Cross-Attn 66.99 Concat (Seq.) Concat (Ch.) 70.02 Learnable 68.06 68.38 25% - Mixed FLOPs MSVD MSRVTT TGIF Perc. 59.03 51.1 46.21 17.19 56.95 48.20 45.67 43.09 51.1 47.36 16.29 58.08 46.6 16.24 56.54 48.82 56.99 47.71 43.66 16.39 (c) Feature fusion strategy. We compare our feature fusion strategy with concatenating the visual embeddings in either token sequence dimension or the channel dimension, learning an optimal embedding mixture weights, and training with equal 25% mixture of visual embeddings. Table 6: Full design choice ablation numbers. Detailed experimental results of Tables 2a, 2b, 2c. We highlight our defaults in orange and bold the best results."
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Methods MSVD-QA MSRVTT-QA Score Acc Score Acc TGIF-QA Acc Score All 4 encoders w/o LanguageBind w/o DINOv2 w/o ViViT w/o SigLIP 70.97 68.52 69.75 70.12 69.85 3.76 3.69 3.74 3.75 3.74 59. 57.10 57.70 58.26 57.55 3.25 3.19 3.23 3.23 3.22 51.10 50.20 49.94 50.45 50.27 3. 3.23 3.23 3.22 3.22 Perception ActivityNet-QA Acc 46.21 45.23 46.57 46.94 46.20 Acc Score 50.87 49.78 51.43 51.36 50.06 3.34 3.31 3.34 3.33 3.32 Avg Acc 55.64 54.17 55.08 55.43 54.79 Table 7: Effect of Each Encoder. Detailed results of Figure 4a Methods MERV LangBind DINOv2 ViViT SigLIP MSVD-QA MSRVTT-QA Score Acc Score Acc TGIF-QA Acc Score 70.97 68.47 65.44 59.95 66. 3.76 3.71 3.62 3.43 3.64 59.03 55.81 53.46 51.81 56.41 3.25 3.16 3.09 3.05 3. 51.10 48.62 41.53 38.1 48.22 3.26 3.19 2.96 2.84 3.16 Perception ActivityNet-QA Acc 46.21 46.14 42.73 40.2 45.09 Acc Score 50.87 44.72 43.39 43.98 49. 3.34 3.17 3.09 3.16 3.31 Avg Acc FLOPs Params Overall 55. 17.19 7686.0 52.75 49.31 46.81 53.16 41.3 7147.0 40.88 7046.0 27.12 6830.0 31.08 6834.0 Table 8: MERV Captures Single Encoder Performances. Detailed experimental results of Figure 4a. MSRVTT-what MSRVTT-who MSRVTT-how MSRVTT-when MSVD-what MSVD-who TGIF-what TGIF-how TGIF-where MERV ViViT DINOv2 LanguageBind SigLIP 50.62 43.06 44.54 46.89 47.96 77.17 70.43 73.00 74.86 74.41 83.96 78.90 76.95 83.41 84.21 72.23 68.54 65.73 72.53 71.20 62.68 50.10 55.71 59.66 57.17 84.62 75.90 82.12 82.95 82. 49.44 32.90 37.14 46.23 45.65 53.33 50.10 50.69 53.24 53.25 65.34 50.62 58.40 59.92 60.21 Table 9: Performance on WH-words. Detailed experimental results of Figure 5 B.2 SOMETHING SOMETHING V2 DETAILS Figure 9: Example video of Something-Something V2. We see that ViViT show better performance in classes where temporal movement is critical for solving the task (Top row), while SigLIP performs better when the action can be inferred from the image without temporal information (Bottom row). MERV MERV-Full LanguageBind DinoV2 ViViT SigLIP LLaMA-Vid-7B LLaMA-Vid-13B VideoLLaVA Smth-Smth V2-OE-Temporal Smth-Smth V2-OE Smth-Smth V2-MCQ-Temporal Smth-Smth V2-MCQ 6.82 17.70 36.84 42.01 9.13 20. 40.65 39.76 3.63 13.83 30.58 36.82 3.88 11.03 28.08 33.06 5.50 10. 39.77 26.78 4.25 13.84 25.39 34.86 6.07 16.47 27.14 36.63 3.94 15. 17.89 39.43 5.57 19.18 22.47 23.14 Table 10: Performance on Something-Something V2 - OpenEnded. These are the performance in shown Figure 4b"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 10: Samples of MERV in SSv2. Due to our design, our method shows better temporal action understanding than other VideoLLMs. (Top two rows) However, due to the difficulty of the task, we see failure cases for VideoLLMs. (Bottom two rows) B.2.1 SOMETHING-SOMETHING V2 - OPENENDED. Additionally, we evaluate Something-Something V2 as an open ended QA task, where the question is \"How is the object in the video being interacted with?\", and the answer is expected to be similar to the class label. We use Video-ChatGPT (Maaz et al., 2023)s LLM evaluation for validating the VideoLLMs output. Table 10 tabulates the results. B.2.2 SOMETHING-SOMETHING V2 - TEMPORAL. The 12 selected classes are as following: Approaching [something] with your camera Turning the camera downwards while filming [something] Turning the camera left while filming [something] Turning the camera right while filming [something] Turning the camera upwards while filming [something] Moving away from [something] with your camera Moving [something] away from the camera"
        },
        {
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "content": "Figure 11: Example VideoLLM output on Something-Something v2. While SigLIP performs better on object and scene recognition, it fails to understand temporal actions. ViViT fails on the details of object recognition, but has better understanding in temporal movements. Moving [something] towards the camera Pulling [something] from left to right Pulling [something] from right to left Pushing [something] from left to right Pushing [something] from right to left"
        },
        {
            "title": "C ADDITIONAL ANALYSIS",
            "content": "C.1 ATTENTION WEIGHTS Our analysis in Section 5.1 looks at how different cross-attention weights activate most on different videos, illustrated in Figure 12."
        },
        {
            "title": "LanguageBind",
            "content": "DINOv2 ViViT SigLIP Figure 12: Videos that give the highest attention weight for each of the encoders. The right-most column shows the average frame of the video."
        }
    ],
    "affiliations": [
        "Princeton University",
        "Salesforce Research"
    ]
}