{
    "paper_title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
    "authors": [
        "Zhengfeng Lai",
        "Vasileios Saveris",
        "Chen Chen",
        "Hong-You Chen",
        "Haotian Zhang",
        "Bowen Zhang",
        "Juan Lao Tebar",
        "Wenze Hu",
        "Zhe Gan",
        "Peter Grasch",
        "Meng Cao",
        "Yinfei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 0 4 7 2 0 . 0 1 4 2 : r Preprint REVISIT LARGE-SCALE IMAGE-CAPTION DATA IN PRETRAINING MULTIMODAL FOUNDATION MODELS Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch Meng Cao, Yinfei Yang Apple AI/ML {jeff_lai,v_saveris,pgrasch,mengcao,yinfeiy}@apple.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large-scale image-text datasets have been crucial in advancing multimodal foundation models. For instance, CLIP (Radford et al., 2021) is pre-trained on 400 million image-text pairs collected from the Web. However, web-crawled data, particularly AltText, often suffer from insufficient visual details and noisy content, as illustrated in Fig. 1. Recent studies highlight the benefits of synthetic captions, which provide better image-text alignment and improved data quality. Research on LaCLIP (Fan et al., 2024) and ShareGPT4V (Chen et al., 2024a) demonstrates that synthetic captions can improve the performance of CLIP and multimodal large language models (MLLMs), respectively. This raises key question: if higher-quality synthetic captions can be generated, could they fully replace web-crawled AltText? Should we consider disregarding AltText altogether? To investigate this question, we first adopt the approach of VeCLIP (Lai et al., 2024) and train CLIP using synthetic captions generated by LLaVA (Liu et al., 2023b). Similar to the results discussed in Li et al. (2024b), training CLIP fully on synthetic captions of higher quality degrades CLIPs performance significantly: as shown in Fig. 2, when compared to using only AltText, the use of LLaVA captions results in substantial drop on zero-shot ImageNet classification tasks. However, after combining original noisy AltText and LLaVA captions, we achieve the best results in both classification and retrieval tasks. This observation raises critical question: what constitutes the optimal image-text data for multimodal foundation models? Despite its importance, research on the interplay between synthetic captions and AltText remains limited. Our findings suggest that while rewriting techniques can enhance image-text alignment, they may reduce data diversity due to Equal contribution. Corresponding author. 1 Preprint Figure 1: The role of image-text data in multimodal foundation models: key component in training CLIP and Diffusion Model, and essential for multimodal LLM (MLLM) pre-training alongside text and interleaved image-text data. We propose controllable captioning pipeline to synthesize different types of captions and explore optimal image-text data recipes for training these foundation models. dependence on limited set of LLMs or MLLMs for caption generation. Specifically, since CLIP is foundational vision model that benefits from learning diverse concepts, relying on synthetic captions can potentially hinder CLIPs training due to lack of diversity in vocabulary and mentioned concepts (Fan et al., 2024). In addition to the role of AltText, another open question concerns the optimal formats for synthetic captions. For instance, advanced multimodal models like LLaVA-NeXT (Li et al., 2024a) indicate that recaptioned datasets are advantageous during stages focused on high-quality knowledge acquisition. DALL-E 3 (Betker et al., 2023) demonstrates that using 95% synthetic captions can yield superior results, particularly when the captions are highly descriptive. Similarly, MM1 (McKinzie et al., 2024) shows that even small fraction (7%) of high-quality caption data can significantly boost few-shot performance. Given these insights, our work focuses on two key unresolved questions: 1) What is the role and value of synthetic captions, and how do they interact with the original AltText? 2) What types of synthetic captions are most effective for different foundation models? To address the first question, we revisit why prior works (Betker et al., 2023; Lai et al., 2024; McKinzie et al., 2024) continue using noisy web-crawled AltText, even when rewritten captions are available during training. Intuitively, since CLIP is straightforward model pre-trained on image-text pairs, highly aligned captions should be advantageous. However, relying solely on synthetic captions may actually degrade CLIPs performance, as shown in Fig. 2(a). For the second question, we investigate the effects of Short Synthetic Captions (SSC) and Descriptive Synthetic Captions (DSC) on CLIP. As depicted in Fig. 2(b), surprisingly, more descriptive captions yield inferior results compared to shorter captions for CLIP training, despite their greater detail. To explore these insights further to address the two questions, we introduce novel, controllable, and scalable captioning pipeline that enables the generation of diverse caption formats at scale, tailored to the specific needs of different multimodal foundation models. Our pipeline is designed to build large-scale image-text data for the pre-training stage in scalable way. With this pipeline, we use SSC and DSC as two main examples on how to customize the captioning format. Our pipeline can serve as cost-effective alternative to GPT-4V for generating high-quality captions. To solve the second question, we conduct comprehensive study on the effectiveness of different types of synthetic captions across range of foundational models and downstream tasks. Our approach involves systematic evaluation of various captioning strategies, including SSC, DSC, and mixed training methods that combine original AltText with synthetic data. We seek to determine the optimal captioning techniques for specific models, such as CLIP, diffusion models, and multimodal LLMs, and to assess their impact on both model performance and data diversity. Furthermore, we investigate the interaction between synthetic captions and original AltText, analyzing whether hybrid approach can balance the need for diverse data with the benefits of enhanced image-text alignment. 2 Preprint (a) (b) Figure 2: Zero-shot retrieval and classification performance of CLIP models. (a) The effect of synthetic captions (LLaVA recaptioned) and AltText: solely using LLaVA captions can improve retrieval tasks but significantly deteriorate the zero-shot classification performance. (b) The effect of different formats of synthetic captions on CLIP: Short Synthetic Captions (SSC) show superior results to Descriptive Synthetic Captions (DSC) and the combination of them achieves the best results. Overall, our contributions are summarized as follows. We explore the MLLM as the image describer and present controllable and human-aligned captioning pipeline to convert MLLM into an image captioner. We synthesize several formats of captions including Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+), then conduct extensive pre-training experiments to systematically study the role of synthetic captions and their intersection with original AltText across three multimodal foundation models. We verify the image-caption training recipe that 1) AltText provide data variety and synthetic captions provide better image-text alignment, 2) different foundation models have their own preferred formats, which highlights the necessity and importance of the controllable captioning pipeline in building multimodal foundation models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal Foundation Models. CLIP (Radford et al., 2021) is one of the pioneering multimodal foundation models connecting images and text. By training on 400 million image-text pairs, CLIP shows strong zero-shot image classification and retrieval capabilities. It lays the groundwork for the development of more advanced multimodal foundation models, such as multimodal large language models (MLLMs) (Liu et al., 2023b; Wang et al., 2023; Chen et al., 2024b; Tong et al., 2024; Zhang et al., 2024b) for vision-language understanding and diffusion models (Rombach et al., 2022; Podell et al., 2023) for text-to-image generation. These advanced models often utilize CLIPs vision tower as their vision encoder. Improving Image-Text Data. Web-crawled image-text data often suffer from issues like image-text misalignment and poor-quality textual descriptions (Lai et al., 2024; Li et al., 2024b). There are two common ways for improving image-text data: 1) data filtering based methods remove low-quality data such as misaligned image-text pairs by human-assisted systems (Yu et al., 2024a; Sun et al., 2023) or pre-trained models (Li et al., 2022b; Schuhmann et al., 2021; Gadre et al., 2024; Fang et al., 2023); 2) data recaptioning based methods usually leverage LLM to rewrite the original caption or MLLM to rewrite caption for the image. For example, ShareGPT4V (Chen et al., 2024a) uses GPT-4V to write highly descriptive captions for their images. LaCLIP (Fan et al., 2024) leverages several LLMs to rewrite captions with different writing styles for data diversity. Recap-DataComp-1B (Li et al., 2024b) uses LLaMA-3 based model to scale the captions. Different from the aforementioned works, we mainly focus on generating different types of captions and exploring 1) the format of ideal captions needed for each multimodal foundation model and 2) systematic analysis of the intersection between AltText and synthetic captions. 3 Preprint Figure 3: Examples of controllable captions of diverse formats generated by our captioner: we can generate from brief to dense descriptions and fuse AltText into the caption (AFC)."
        },
        {
            "title": "3 CUSTOMIZED RE-CAPTIONING FOR MULTIMODAL FOUNDATION MODELS",
            "content": "Image-text data are fundamental to the success of multimodal foundation models, serving as bridge between visual and textual representations. For example, CLIP (Radford et al., 2021) is pre-trained on 400M web-crawled image-text pairs, enabling it to learn rich, transferable representations that can be applied to various downstream tasks. The importance of precise and detailed captioning is further highlighted in LLaVA-NeXT (Li et al., 2024a), where re-captioned detailed descriptions are utilized for model training at Stage-1.5, enhancing the models ability to understand and generate nuanced content. Similarly, DALL-E 3 (Betker et al., 2023) shows that the prompt-following capabilities of text-to-image models can be significantly improved by training on highly-descriptive generated image captions. This shows the critical role of captions in shaping models capacity to align visual and textual information, ultimately improving performance across wide range of multimodal tasks. However, the optimal captioning strategy for different foundational models remains under-explored. To address this gap, we introduce novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats at scale, supported by evaluation metrics that ensure high CLIP 4 Preprint Figure 4: Directly using MLLMs as image captioners may result in hallucinations and generate captions that do not align with specific instructions: both LLaVA (Liu et al., 2023b) and ShareGPT4V (Chen et al., 2024a) generate over three sentences and obvious hallucination. scores and minimal hallucination. We summarize the capability of our captioning model by generating the following formats of captions as shown in Fig. 3: Short Synthetic Caption (SSC): concise sentence that describes the primary subject of the image. Descriptive Synthetic Caption (DSC): description limited to 78 tokens, emphasizing the central subject and key visual elements. Dense Synthetic Caption (DSC+): more comprehensive description detailing the main subject along with the background, setting, and any significant objects or actions. AltText Fusion Caption (AFC): caption similar to DSC, but integrated with AltText where appropriate. This type of caption removes unnecessary details often found in AltText, offering cleaner, more cohesive description than simple concatenation of AltText and synthetic caption. 3.1 MLLM AS AN IMAGE DESCRIBER VeCLIP (Lai et al., 2024) employs LLaVA (Liu et al., 2023b) for image captioning, while ShareGPT4V (Chen et al., 2024a) utilizes GPT-4V for this task. Compared to traditional image captioners like BLIP (Li et al., 2022a), MLLMs offer several advantages and are natually good image describers. MLLMs can generate longer and more detailed captions, as demonstrated by LLaVA-NeXT (Li et al., 2024a), where 34B model was used to produce highly descriptive captions. This ability to generate more context-aware descriptions stems from the integration of large language model (LLM) with vision encoder. This combination allows MLLMs to capture fine-grained visual details and complex inter-object relationships. Additionally, MLLMs benefit from their multi-stage training process, which combines pre-training on large-scale datasets and supervised fine-tuning for downstream tasks. These characteristics make MLLMs powerful tool for generating more descriptive captions. However, directly using MLLM as an image describer may have two major limitations: 1) MLLM may not strictly follow the instruction to generate specific format of caption (Liu et al., 2023a); 2) these instruction fine-tuned MLLMs tend to hallucinate. As shown in Fig. 4, both LLaVA (Liu et al., 2023b) and ShareGPT4V (Chen et al., 2024a) fail to describe the image using only three sentences but generate hallucinated contents (highlighted in red). Although GPT-4V shows stronger capability and many works use it for captioning, the scalability remains limited due to the cost. Therefore, in this work, we focus on building cost-effective captioner instead of using GPT-4V. To alleviate the above two limitations, we first investigate the origins of hallucination in MLLMs, hypothesizing two primary sources: 1) inherent limitations of the LLM, and 2) the quality of supervised fine-tuning (SFT) datasets, which are often themselves synthetically derived or processed. We focus on the latter, proposing that mitigating hallucinations at the dataset level is essential for converting an MLLM into an effective captioner. We also address the format-following issue by fine-tuning the MLLM on curated captioning-specific dataset, transforming it into purpose-built captioning model to generate diverse captions. 3.2 TWO-STAGE HUMAN-ALIGNED CAPTIONING Stage 1: transforming MLLM into customized captioner. To minimize hallucinations from MLLMs, we begin by constructing clean and precise fine-tuning dataset. Instead of relying on Preprint Figure 5: Overview of the controllable and human-aligned captioning pipeline. In Stage 1, we convert pre-trained MLLM into customized captioner that strictly follows the captioning instructions. In Stage 2, we leverage human-aligned captions to further fine-tune the captioner. GPT-4 generated data, we curate high-quality dataset of human-annotated image-text pairs, named Stage-1-1M. This dataset contains short, human-curated captions, with five captions per image. Additionally, we integrate an OCR detection model to extract in-image text, which, alongside the captions, is fed into an LLM for summarization and controlled rewriting. By employing strict prompt, we prevent the LLM from introducing extraneous information, while few-shot prompts guide the model in generating various caption formats, including both concise and descriptive styles. We further enhance the dataset through post-processing, using heuristic and model-based quality checks to improve its overall quality. This comprehensive fine-tuning on the Stage-1-1M dataset transforms the MLLM, specifically the 3B version of MM1 (McKinzie et al., 2024), into customized captioner that aligns closely with the intended output characteristics. Stage 2: Human-aligned further fine-tuning. While the Stage-1-1M dataset effectively establishes foundation, it lacks the depth required for more descriptive captioning tasks. In Stage 2, we address this by incorporating descriptive human-annotated data to enhance caption diversity and quality. We curate new dataset, named Stage-2-HA, specifically designed for detailed captioning. This dataset is meticulously annotated by human experts to capture nuanced visual elements and complex scene descriptions. Post-annotation, we leverage an LLM to reformat these captions into multiple stylistic variations, including both concise and richly descriptive formats. By imposing strict constraints during the LLM processing, we ensure alignment with human-generated content, avoiding the pitfalls of hallucination. The captioner is then fine-tuned on the Stage-2-HA dataset, resulting in highly refined and human-aligned captioning model capable of generating captions tailored to specific use cases. This dual-stage fine-tuning process not only enhances the models adaptability but also ensures balance between brevity and descriptiveness, making it versatile tool in controllable captioning. The overview of the complete captioning pipeline is shown in Fig. 5. 3.3 CAPTION ANALYSIS Richness assessment: token length and average number of assertions. Fig. 6 illustrates the distribution of the number of tokens of various caption types generated in this study. Specifically, SSC mainly ranges from 10 to 15 tokens, while DSC spans from 40 to 60 tokens, both fitting within the text encoders capacity in CLIP. In contrast, most DSC+ captions exceed 100 tokens. Besides that, we propose Average Number of Assertions (ANA) to quantify the richness of captions. We prompt an LLM to generate different assertions of caption to analyze our different formats of captions in terms of the richness. More details of this approach is in Appendix. Note that the ANA for SSC is 2.49, DSC as 8.13 and DSC+ as 12.20, showing more visual contents. Diversity assessment: number of unique entities in captions. We hypothesize that the original, albeit noisy, AltText may carry broader range of diverse information and knowledge, offering potential advantages for CLIPs pre-training. To assess this diversity, we quantify the number of 6 Preprint Figure 6: Distribution of token lengths of our generated captions in four formats: we caption COCO 2017 images and visualize their distributions. Figure 7: The number of unique entities in different synthetic captions (randomly sample 17.5k images) compared to AltText: AltText provides more unique entities as wider knowledge. unique entities present in the captions and present visualization in Fig. 7. Our analysis shows that AltText contains higher number of unique entities, which could be beneficial in providing wider knowledge base. 4 IMAGE-CAPTION DATA FOR MULTIMODAL FOUNDATION MODELS In this section, we mainly discuss three foundation models: CLIP, multimodal LLM, and diffusion models. For both CLIP and diffusion models, since the text encoder is limited to 77 tokens (Zhang et al., 2024a), we focus primarily on SSC and DSC. For multimodal LLM, we explore more detailed versions, including DSC+ and AFC. We summarize our key findings below: The tradeoff between the richness of captions and their accuracy needs to be balanced based on the multimodal tasks. Both AltText and synthetic captions are important for CLIP training, with shorter captions yielding better performance. Linear probing is an additional effective way to evaluate the representations. Pre-training and SFT benchmarks can behave differently in multimodal LLMs. On the SFT benchmarks, MM1 shows preference for DSC+ alone. For diffusion models, DSC emerges as the most effective captioning strategy. 7 Preprint Table 1: Effect of different synthetic captions on CLIP with ViT-B/16 as the backbone. IN: ImageNet. Table 2: Evaluation with linear probing (LP) on ImageNet for CLIP. Pre-train Caption COCO (R@1) I-T T-I Flickr30k (R@1) I-T T-I IN INV2 AltText DSC SSC AFC DSC + AltText SSC + AltText AFC + AltText All Synthetic + AltText 54.24 52.28 57.00 54.82 65.84 66.67 63.98 70.12 36.98 29.00 35.15 34.84 46.08 48.13 43.76 50.21 81.30 80.90 84.67 84.00 90.26 91.81 89.10 93. 65.80 54.75 63.35 62.18 73.94 76.54 73.32 77.72 65.70 27.30 49.10 38.98 66.18 66.63 66.47 64.91 58.58 21.91 43.31 35.11 58.74 59.57 58.84 57.92 Pre-train Caption Zero-shot LP Gain AltText DSC SSC AFC DSC + AltText SSC + AltText AFC + AltText DSC + SSC + AltText All Synthetic + AltText 65.70 27.30 66.63 38.98 66.18 66.63 66.47 65.16 64.91 78.34 75.72 79.94 75.53 79.96 79.94 78.70 80.01 79.55 +12.64 +48.42 +13.31 +36.55 +13.78 +13.31 +12.23 +14.85 +14.64 4. IMAGE-CAPTION DATA FOR CLIP We use VeCap-300M (Lai et al., 2024), web-crawled dataset with raw AltText as our main pretraining dataset for CLIP. Besides AltText, we generate several synthetic caption datasets for the study. Then, we use ViT-B/16 as the vision encoder. The training details can be found in Appendix. Effect of synthetic captions. We first study the effect of Short Synthetic Captions (SSC) and Descriptive Synthetic Captions (DSC). Results are summarized in Table 1. Interestingly, while SSC enhances retrieval performance, it leads to substantial drop in zero-shot ImageNet accuracy. Moreover, despite demonstrating that synthetic captions have superior image-text alignment and reduced noise, the more descriptive captions (DSC) perform worse across all benchmarks. Based on these results, DSC appears suboptimal for CLIP training, leading to inferior performance. For example, despite DSC capturing more visual concepts within the captions, its zero-shot performance shows significant degradation of 21.8% compared to SSC. We hypothesize this performance drop may be partially due to distribution mismatch as the prompts in COCO/Flickr30k/ImageNet datasets are short (e.g., photo of {}). To further explore the performance gap between DSC and SSC on CLIP, we also use linear probing, which provides direct measure of the quality and generalization capability of the representations learned by CLIP. Strong performance from linear classifier on specific tasks indicates that the pre-trained model has effectively captured relevant and discriminative features, underscoring the robustness of its embeddings. We summarize the results on linear probing in Table 2. Even though DSC and SSC show lower zero-shot performance, they achieve comparable results to AltText after linear probing, indicating similar pre-trained representations. This indicates that the relatively poor zero-shot results of DSC can be significantly improved with linear probing, implying that representations learned from DSC are richer than initially presumed. Intersection of synthetic captions and AltText. From Table 1, we find training CLIP solely on large-scale synthetic captions may get inferior results compared to the original AltText, even though synthetic captions have better image-text alignment. We hypothesize that synthetic captions rewritten from MLLM may hurt the diversity and knowledge coverage of the original AltText. Therefore, we blend our synthetic captions and AltText, such as DSC + AltText and SSC + AltText: there is significant boost in retrieval performancee.g., over 10% improvement on COCO. Additionally, SSC + AltText also leads to gains in ImageNet accuracy. Utilizing mixture of all synthetic caption formats yields the best performance in retrieval tasks, likely due to the wider entity-based knowledge, as visualized in Fig. 7. Optimal mixture ratio between synthetic captions and AltText. Considering the wider knowledge of AltText  (Fig. 7)  and the better alignment of synthetic captions, we explore the optimal mixture ratio. Specficially, we use SSC and AltText from VeCap-300M (Lai et al., 2024) as an example, with results shown in Fig. 8. ratio of 0 corresponds to using only SSC, while 100 corresponds to using only AltText. We observe that CLIP achieves optimal performance across both retrieval and classification tasks when the ratio is tuned to around 40-50%. lower proportion of AltText leads to drop in retrieval performance, whereas lower proportion of SSC results in decreased accuracy in ImageNet zero-shot classification. This observation aligns with findings in Li et al. (2024b). From this, we verify that AltText provides broader knowledge coverage and greater diversity, which benefits CLIPs pre-training by enabling the model to grasp wider range of concepts. This diversity may serve as foundation for generalization, allowing CLIP to better represent varied contexts and domains in zero-shot classification. 8 Preprint Figure 8: The intersection of synthetic captions and AltText for CLIP. We gradually increase the proportion of SSC mixed with AltText during training. All experiments use ViT-B/16 as the backbone and VeCap-300M (Lai et al., 2024) as the pre-training dataset. Exploration of the optimal way of using AltText: AFC vs. simple mixture. As shown in Fig. 8 and Table 1, we verify that simple mixture of our synthetic captions and AltText can achieve superior results. In addition to this straightforward combination, we investigate alternative methods of incorporating AltText into the training process. One promising approach is to fuse the knowledge from AltText directly into the synthetic captions. To enable this, we fine-tune our captioner with instruction-following capabilities, generating AltText Fusion Captions (AFC). While AFC shows improvement over DSC due to the enriched AltText information, it underperforms when compared to the simple mixture of DSC and AltText. 4.2 IMAGE-CAPTION DATA FOR MULTIMODAL LLM Table 3: The effect of synthetic captions on MM1 (1.2B model) pre-training. Table 4: The ratio ablation on MM1 pre-training between synthetic captions and AltText. Pre-train Caption TextCore 0-Shot 4-Shot 8-Shot Mixing Ratio TextCore 0-Shot 4-Shot 8-Shot AltText DSC DSC + AltText SSC SSC + AltText 53.48 53.76 53.71 53.46 53.20 34.80 37.05 37.09 37.35 37.16 55.81 59.36 60.31 59.19 58.22 59.72 63.68 63.96 63.56 62.22 33/66 50/50 66/33 80/20 100/ 53.86 53.86 54.24 53.96 54.19 35.16 36.40 37.02 35.74 15.54 59.69 60.17 60.26 60.09 60.24 63.97 64.13 64.71 64.69 64.02 As we study large-scale image-caption data for MLLMs, we use MM1 (McKinzie et al., 2024) as one example and focus on the pre-training stage. MM1 (McKinzie et al., 2024) claims that captioning data lift the zero-shot performance and synthetic captions are helpful for few-shot learning. Based on this insight, we further study the captioning data recipe on how to balance the use of original AltText and synthetic captions. We follow the pre-training setup and the evaluation benchmark in MM1 (McKinzie et al., 2024) to report TextCore and 0/4/8-shot performance. All of the experiments are conducted on the 1.2B model. We pre-train the model with 50K steps and the batch size is 512. More details are in Appendix. Effect of synthetic captions for pre-training benchmarks. We generate DSC and SSC captions for VeCap-300M (Lai et al., 2024), as used in MM1 (McKinzie et al., 2024), and replace the captions during MM1 pre-training. The results, summarized in Table 3, show that our synthetic captions yield improved performance in image-text benchmarks across 0-shot to 8-shot settings. For example, SSC achieves +1.3% performance gain in 0-shot evaluation compared to the original MM1. Unlike CLIP experiments, DSC outperforms SSC, with the combination of DSC and original AltText delivering the best results in this context. We also conduct an ablation study of data mixing ratios on MM1 pre-training to explore the optimal balance between synthetic captions and AltText, as summarized in Table 4. The results indicate that 66/33 mixing ratio yields the best overall performance across all 9 Preprint Table 5: SFT evaluation of models pre-trained with different types of captions. We use the same SFT evaluation benchmarks as in MM1 (McKinzie et al., 2024). (*): Concatenation of two captions. Pre-Trained Data VQAv2 VQAT MMMU MathV MMEP MMEC SEED POPE LLaVAW Average AltText LLaVA Caption SSC DSC DSC+ AltText + SSC (*) AltText + DSC (*) AltText + DSC+ (*) AFC SSC + DSC + DSC+ AFC + SSC + DSC + DSC+ 77.1 78.0 78.8 77.1 79.0 77.7 78.2 79.2 78.0 79.4 77.3 66.1 65.4 65.8 61.8 65.4 65.4 66.9 66.9 65.6 66.5 62.6 31.4 30.0 33.6 30.8 32.6 32.3 31.9 31.1 32.3 30.2 31.6 28.3 27.6 28.2 27.9 29.4 27.6 30.7 29.6 29.4 30.3 27. 781.7 773.4 760.2 596.6 727.2 781.9 686.4 740.2 713.2 689.3 661.5 225.4 200.4 216.4 213.6 224.3 236.1 216.4 229.3 214.3 198.9 198.2 61.7 63.5 63.1 64.6 66.8 62.0 63.8 65.5 66.0 65.5 64.1 84.6 83.7 84.4 83.9 85.1 84.2 84.3 85.1 84.4 83.8 83.5 69.8 63.6 69.0 70.8 71.5 70.3 72.0 68.1 70.0 67.5 64.2 57.6 56.5 57.9 56.3 58.7 57.7 58.2 58.2 58.0 57.5 55. evaluation settings. Specifically, this ratio achieves the highest scores for TextCore (54.24), 0-shot (37.02), 4-shot (60.26), and 8-shot (64.71) performance. While increasing the proportion of synthetic captions generally improves performance, there is significant drop in 0-shot performance when using only synthetic captions (100/0 ratio). Effect of synthetic captions for SFT benchmarks. Besides pre-training benchmarks, we also conduct SFT and then evaluate the model to analyze the profound effect of image-caption data in MLLMs. We use the same SFT recipe to have fair comparison. As shown in Table 5, DSC+ and the concatenation of AltText with DSC+ deliver the best performance. This strongly suggests the importance of detailed captions, despite them containing potentially the highest number of hallucinations among all the caption types we test. On the other hand, concatenating AltText with synthetic captions does not yield significant improvement in the SFT benchmarks, contrasting with the gains observed in pre-training benchmarks. We hypothesize that the primary role of image-caption data during the pre-training phase of MLLMs is to enhance image-text alignment. Consequently, more detailed captions, such as DSC+, deliver superior results after the SFT stage. DSC+ alone can outperform diverse synthetic captions. Unlike CLIP, where mixing diverse synthetic captions leads to superior results, for MM1, detailed captions (DSC+) alone yield the best performance after the SFT stage. As shown in Table 5, DSC+ achieves 58.7% score, outperforming LLaVA captions (56.5%) by 2.2%. This suggests that providing richer and more specific information in captions helps multimodal LLMs like MM1 generalize better after the SFT stage. The combination of SSD, DSC, DSC+, and AFC does not lead to better results, suggesting that multimodal LLMs may benefit more from detailed captions during pre-training. These findings suggest that while combining synthetic captions proves beneficial in some contexts (e.g., CLIP), for multimodal LLMs like MM1, single, detailed caption offers more effective guidance during pre-training. 4.3 IMAGE-CAPTION DATA FOR DIFFUSION MODEL Inspired by DALLE-3 (Betker et al., 2023), detailed and short captions can improve the prompt following ability. In this work, our DSC not only covers the main objects within the scene, but also their relationships, attributes, and the broader context in which they are situated. We hypothesize that this level of detail allows the model to generate images that are not only visually accurate but also semantically aligned with the textual input. We implement Stable Diffusion 3 (Esser et al., 2024) and use this diffusion model as our studying example on text-to-image generation tasks. The backbone is based on the DiT architecture (Peebles & Xie, 2023) that focuses exclusively on class-conditional image generation and incorporates modulation mechanism to condition the network based on both the diffusion process timestep and the class label. Different from DALLE-3 (Betker et al., 2023), we report results on more comprehensive benchmarks instead of only CLIP score, such as GenEval (Ghosh et al., 2024) and DSG (Cho et al., 2024). Effect of synthetic captions. Synthetic captions lead to significant improvements on the GenEval benchmark (Ghosh et al., 2024), as shown in Table 6, highlighting the advantage of enhanced prompt-following capabilities. Notably, incorporating SSC or DSC with AltText boosts the GenEval average score from 58.8 to 65.5. Additionally, synthetic captions yield over 3.5% improvement on the DSG benchmark (Cho et al., 2024). However, SSC achieves better performance on the FID score (Jayasumana et al., 2024). Overall, the descriptive captions show better results among these benchmarks. 10 Preprint Table 6: The effect of synthetic captions on diffusion models. Single Obj Two Obj GenEval (Ghosh et al., 2024) Counting Colors Position Attribution Average FID COCO30k DSG Average AltText SSC SSC + AltText DSC DSC + AltText 99.1 98.5 98.2 93.9 99. 80.4 80.1 84.4 60.2 84.2 58.3 68.9 65.1 67.7 68.9 78.5 80.4 77.3 79.8 79.0 17.4 18.6 18.6 13.8 21.4 40.2 50.2 50.2 39.2 52.0 62.3 66.1 65.5 59.1 67. 13.6 13.1 13.1 14.8 14.0 72.4 73.1 74.2 73.8 74.2 Figure 9: The intersection of synthetic captions and AltText for diffusion models. We gradually increase the proportion of DSC mixed with AltText during training. Ablation study on mixing ratio of synthetic captions and AltText. We examine the impact of varying the ratio between DSC and AltText in diffusion model training, evaluating performance across FID@COCO30k (Jayasumana et al., 2024), CLIP@COCO30k, DSG Average (Cho et al., 2024), GenEval Overall (Ghosh et al., 2024). Results are summarized in Fig. 9. The FID@COCO30k metric shows gradual increase, suggesting that higher DSC ratios lead to improvements in generation quality. The DSG Average score exhibits improvements with higher DSC ratio, indicating that DSC can enhance the models ability to handle complex tasks. However, the performance on the GenEval related metric peaks at 50% DSC, after which it begins to decline, highlighting the necessity of balancing synthetic and original captions to achieve optimal results across diverse evaluation merics. Besides DSC, we further conduct experiments using SSC as well. Results are summarized in Table 6. Overall, the use of SSC alsone also achieves competitive performance, but the use of DSC and AltText together appears to be better captioning strategy."
        },
        {
            "title": "5 DISCUSSION",
            "content": "In this study, we examine the role and value of image-text data in multimodal foundation models, including CLIP, multimodal LLMs, and diffusion models. Our research focuses on the intersection between synthetic image-aligned captions and the original web-crawled AltText. To identify the most effective captions for each foundation model, we develop controllable and human-aligned captioning pipeline designed to minimize hallucinations and generate various types of captions as needed. Through extensive pre-training experiments, we derive the following key insights. 1) Both AltText and synthetic captions play crucial rolesAltText contributes to more diverse information, while synthetic captions offer improved image-text alignment. 2) CLIP tends to favor short synthetic captions, whereas MLLMs benefit from more descriptive captions. We also observe that the benchmarks in the pre-training and SFT stage of MLLMs may have different preferences of captions. 3) We verify the observation from DALLE-3 on text-to-image generation with more comprehensive benchmarks and show the benefits of synthetic captions. For future work, we aim to further refine our captioning pipeline, enhancing its ability to generate task-specific captions across wider range of multimodal applications. 11 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 15331544, 2013. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. Github, 2018. URL http://github.com/google/jax. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. ECCV, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Jaemin Cho, Yushi Hu, Jason Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In ICLR, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In The Twelfth International Conference on Learning Representations, 2023. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024. 12 Preprint Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and In Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 93079315, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao. Veclip: Improving clip training via visual-enriched captions, 2024. URL https://arxiv.org/abs/2310.07699. Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. instruction tuning beyond data?, May 2024a. URL https://llava-vl.github.io/blog/ 2024-05-25-llava-next-ablations/. Llava-next: What else influences visual Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022b. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023b. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 13 Preprint Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. EMNLP, 2018. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019a. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019b. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1380713816, 2024a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In Forty-first International Conference on Machine Learning, 2024b. 14 Preprint Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1812318133, 2022. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. ECCV, 2024a. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024b. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Preprint"
        },
        {
            "title": "Appendices",
            "content": "We provide additional details for datasets, experimental settings, results, and analysis in the supplementary material."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 CLIP We summarize the training details in Table A1. For the pre-training stage, we pre-train models on up to 512 TPUs with JAX (Bradbury et al., 2018). Table A1: Pre-training hyper-parameters and settings for the in-house CLIP. Batch size Image size Image pre-processing Text tokenizer Text maximum length Steps Optimizer Peak learning rate (LR) LR schedule Weight decay Dropout rate 32768 224 224 (ViT-B/16) long-side resizing with padding (i.e., tf.image.resize_with_pad) T5 (Raffel et al., 2020), lowercase 77 435, 000 (i.e., 14B examples seen) AdamW (β1 = 0.9, β2 = 0.98) 0.0005 cosine decays with linear warm-up (first 2k steps) 0.2 0.0 A.1.1 ADDITIONAL EXPERIMENTS To further explore the performance gap between DSC and SSC on CLIP, we present two additional benchmarks to enhance the representativeness of CLIPs existing evaluations: 1) linear probing and 2) transferability between CLIP pre-trained with different captions and LLaVA-style MLLMs. Linear probing provides direct measure of the quality and generalization capability of the representations learned by CLIP. Strong performance from linear classifier on specific tasks indicates that the pre-trained model has effectively captured relevant and discriminative features, underscoring the robustness of its embeddings. Additionally, we assess CLIPs representation quality using LLaVA (Liu et al., 2023b) as case study, where the vision encoder remains frozen during both pre-training and SFT stages. This makes LLaVA an ideal benchmark for evaluating the transferability and integrity of CLIPs learned representations. We summarize the results on linear probing in Fig. 2: even though DSC and SSC shows lower zero-shot performance, they achieve comparable results to AltText after linear probing, indicating similar pre-trained representations. Furthermore, combining synthetic captions with AltText yields the best overall performance. Then we use these pre-trained vision encoders and insert them into LLaVA and complete the default pre-training and SFT stages in LLaVA (Liu et al., 2023b). All of our pre-trained CLIPs use ViT-B/16 as the backbone. We use Vicuna-1.3 as the LLM for LLaVA training and report recent benchmarks in Table A2: POPE (Li et al., 2023b), TextVQA (Singh et al., 2019b), GQA (Hudson & Manning, 2019), SciQA (Lu et al., 2022), LLaVA-Bench (Liu et al., 2023b), MME (Fu et al., 2024), and MM-Vet (Yu et al., 2024b). In this case, the combination of SSC and AltText achieves the highest overall performance, leading in 5 out of 9 columns. This highlights the critical role of both synthetic captions and original AltTexts in CLIPs pre-training: synthetic captions enhance image-text alignment, while AltTexts introduce valuable data diversity. Compatibility of rewritten-based methods and filtering-based methods. Besides rewritten-based datasets like web-crawled VeCap-300M (Lai et al., 2024), which leverage recaptioning techniques to improve image-text alignment, it is essential to evaluate the compatibility between rewriting-based 16 Preprint Table A2: LLaVA as the benchmark for evaluating CLIP vision encoder pre-trained on different captions. Pre-trained Caption AltText DSC DSC + AltText SSC SSC + AltText POPE Avg. 84.4 83.4 84.6 84.5 84.7 TextVQA GQA SciQA IMG Accuracy LLaVA-Bench COCO MME Perception Cognition MM-Vet 50.2 47.6 49.8 48.2 49.4 59.1 59.1 59.9 59.6 60.1 64.3 63.2 63.6 59.0 65. 64.8 64.8 65.3 62.7 65.8 76.4 75.6 77.2 76.7 77.9 1332.6 1307.8 1405.3 1343.6 1370.7 271.1 312.9 273.2 248.6 270.7 25.0 23.9 25.5 22.4 25.1 Table A3: Compatibility between rewritten-based and filteringbased methods. We use DFN-2B (Fang et al., 2023) as the example and train CLIP with ViT/B-16 on different captions. Pre-train Caption COCO (R@1) I-T T-I Flickr30k (R@1) I-T T-I ImageNet ImageNetV2 AltText DSC SSC DSC+SSC 61.22 51.47 59.06 60.68 AltText+DSC+SSC 70.56 49.27 30.99 31.91 38.46 50.74 86.20 77.28 87.41 89.60 92.40 68.94 55.68 62.49 68.68 76. 76.12 27.97 53.96 56.10 72.45 68.49 23.67 46.39 49.15 64.98 Figure A1: Effect of LiT (Zhai et al., 2022) on different captions after pretraining. and filtering-based methods that remove mismatched image-text pairs. We consider DFN-2B (Fang et al., 2023) as representative example, where pre-trained CLIP model filters the dataset to retain pairs that align well with CLIPs capabilities. key research direction is to explore whether synthetic captions can effectively replace the original CLIP-selected AltText. To investigate this, we apply our captioning pipeline to DFN-2B (Fang et al., 2023) to generate synthetic captions and pre-train CLIP models on these captions. The results are summarized in Table A3. In this CLIP-filtered dataset, neither rewritten DSC nor SSC, nor their combination, outperforms the original AltText. However, when combining all our synthetic captions with the original AltText (using uniform sampling during training), we observe significant improvements in retrieval tasks, such as +9.34% on COCO I-T and +7.98% on Flickr T-I tasks, demonstrating the enhanced image-text alignment provided by our synthetic captions. Nevertheless, incorporating synthetic captions results in performance drop of around 4% on ImageNet, highlighting the crucial role of the diverse information contained in the original AltText for CLIPs learning. Effect of Locked-image text Tuning (LiT) (Zhai et al., 2022) after pre-training with different captions. LiT (Zhai et al., 2022) trains text model to derive meaningful representations from pre-trained image model by freezing the image encoder and fine-tuning only the text encoder. We investigate the impact of LiT after pre-training CLIP on different captions, with the results summarized in Fig. A1. During the LiT stage, we continue to train the text encoder using AltText. Our findings reveal that LiT with AltText consistently benefits both SSC and TSC across all evaluation sets, highlighting once again the critical role of AltText in CLIP training. A.2 MULTIMODAL LLM We summarize the training details in Table A4. For the pre-training stage, we pre-train models on up to 512 TPUs with JAX (Bradbury et al., 2018). Table A4: Pre-training hyper-parameters and settings for the Multimodal LLM experiments. We use the same configuration as the 1.2B model in MM1 (McKinzie et al., 2024). General Batch size Image encoder Visual-language connector Language model Steps 512 336 336 ViT-L/14 C-Abstractor with 144 image tokens 1.2B transformer decoder-only language model 50000 17 Preprint For the SFT experiments, we follow the same datasets and configuration as in MM1 (McKinzie et al., 2024). TextCore. For the pre-training benchmarks, TextCore is an average number of 8 benchmarks: ARC (Clark et al., 2018), PIQA (Bisk et al., 2020), LAMBADA (Paperno et al., 2016), WinoGrande (Sakaguchi et al., 2021), HellaSWAG (Zellers et al., 2019), SciQ (Welbl et al., 2017), TriviaQA (Joshi et al., 2017), and WebQS (Berant et al., 2013). SFT dataset. For the SFT benchmarks, we summarize the details in Table A5: we mainly use MME (Fu et al., 2024), SEED (Li et al., 2023a), POPE (Li et al., 2023b), LLaVA-Bench (Wild) (Liu et al., 2023b), MM-Vet (Yu et al., 2024b), TextVQA (Singh et al., 2019a), MMMU (Yue et al., 2023), MathVista (Lu et al., 2023), ScienceQA (Lu et al., 2022). Benchmark Metric MME (Fu et al., 2024) SEED (Li et al., 2023a) POPE (Li et al., 2023b) LLaVA-Bench (Wild) (Liu et al., 2023b) MM-Vet (Yu et al., 2024b) TextVQA (Singh et al., 2019a) MMMU (Yue et al., 2023) MathVista (Lu et al., 2023) ScienceQA (Lu et al., 2022) Normalized Accuracy Seed-IMG Average of random, popular and adversarial GPT-assisted score GPT-assisted score VQA Open Flamingo Accuracy Accuracy GPT-assisted score Accuracy-IMG Table A5: Details of benchmarks and their metrics used in MM1. A.3 DIFFUSION MODEL We summarize the training details of our self-implemented diffusion model based on Stable Diffusion 3 (Esser et al., 2024) in Table A6. Table A6: Pre-training hyper-parameters for our diffusion model based on Stable Diffusion 3. Batch size Image size Step Text condition Text maximum token length Optimizer Learning rate (LR) Ema decay Classifier free guidance 4096 256 256 500, 000 in-house CLIPs G/14 text encoder with T5 tokenizer (Raffel et al., 2020) 77 Adafactor (β1 = 0.9, β2 = 0.999) 0.0001 (constant with linear warm-up for 1k steps) 0. 7."
        },
        {
            "title": "B A DEEPER ANALYSIS OF GENERATED CAPTIONS",
            "content": "Fig. 5 is an overview of our two-stage fine-tuning process: we first convert MLLM into an image captioner, then we further fine-tune it to convert it into human-aligned captioner. Our smaller image captioning model (3B) efficiently generates large volumes of synthetic data for our experiments. Using this model, we re-captioned dataset of 7 billion images across multiple iterations. Furthermore, our larger model, with 7 billion parameters, is designed to produce more detailed captions, surpassing the level of detail offered by our long caption format. AFC fine-tuning dataset. To generate AltText Fusion Captions (AFC), we also prepare fine-tuning dataset in this format. Specifically, given AltText and DSC caption generated by our captioner, we ask LLM to fuse AltText information to the DSC. By this way, we construct 20K training dataset for our captioner. 18 Preprint Less hallucinations in our DSC. The Caption Hallucination Assessment with Image Relevance (CHAIR) metric (Rohrbach et al., 2018) is custom-designed evaluation tool developed to identify and measure the extent of object hallucination in image captioning tasks. The metric determines the proportion of generated words that accurately correspond to objects present in the image, as verified by ground truth sentences and object segmentations. It includes two scores: one that measures the fraction of hallucinated object instances (referred to as CHAIRi), and the other that calculates the fraction of sentences containing at least one hallucinated object (referred to as CHAIRs): CHAIRi = {hallucinated objects} {all objects mentioned} , CHAIRs = {sentences with hallucinated object} {all sentences} . Table A7: Hallucination detection across different MLLMs and our captioner. As shown in Table A7, our model achieves CHAIRi score of 5.9 and CHAIRs score of 19.6, outperforming leading models such as LLaVA-1.5 (Liu et al., 2023b), Shikra (Chen et al., 2023), and MiniGPT-4 (Zhu et al., 2023). The lower CHAIR scores indicate that our captioner produces fewer hallucinated objects per instance and fewer sentences containing hallucinated objects. This improvement shows the effectiveness of our two-stage fine-tuning process, which strategically reduces objects hallucination by leveraging human-aligned data and strict prompt constraints. Consequently, our model offers more reliable and accurate objects recognition capabilities, making it robust tool for generating high-quality captions across various applications. InstructBLIP (Dai et al., 2023) MiniGPT-4 (Zhu et al., 2023) Shikra (Chen et al., 2023) LLaVA-1.5 (Liu et al., 2023b) CHAIRi() CHAIRs() 30.0 24.2 22.0 20.6 14.5 8.2 7.0 6.2 Model Our 19.6 5.9 ANA: Average Number of Assertions, metric for evaluating richness of captions. Besides hallucination, the richness of captions are also an important index to control the generated captions. We propose ANA to quantify the richness of captions. Inspired by GenEval (Ghosh et al., 2024) for evaluating text-to-image generation models, we reverse the process of text-to-image to image-to-text. As shown in Fig. A2, we prompt an LLM to generate different assertions of caption. After that, we can also leverage VQA model to check if these details are aligned with the visual contents. CapScore: metric for evaluating hallucinations in synthetic captions. Although the CHAIR metric is widely used, we find it insufficient for detecting hallucinations in object attributes, especially in highly descriptive captions. To overcome this limitation, we propose new metric to evaluate both the hallucination and richness of captions. Inspired by GenEval (Ghosh et al., 2024) for evaluating text-to-image generation models, we reverse the process of text-to-image to image-to-text and propose CapScore. CapScore measures the correctness of synthetic captions by evaluating the alignment between generated textual assertions and the actual content of the image. As shown in Fig. A2, CapScore has two key steps: 1) use an LLM to extract structured assertions from the captions; 2) use MLLM to serve as VQA (Visual Question Answering) model to verify the truthfulness of these assertions. Specifically, each assertion represents distinct factual claim made within the caption. Then the VQA model determines whether the image supports each claim by answering questions based on the assertions. CapScore is then defined as the percentage of assertions validated as correct by the VQA model. higher CapScore indicates fewer hallucinations and greater factual accuracy in the generated captions. As shown in Table A8, there is notable trade-off between the richness of captions and their accuracy. As captions become longer, the Average Number of Assertions (ANA) increases, reflecting the growing richness and complexity of the generated captions. However, CapScore drops with longer captions, which suggests that while the captions provide more content, they are more prone to hallucinationswhere the captioning model introduces incorrect or irrelevant information not present in the image. For instance, while DSC+ produces the highest ANA, it also demonstrates the lowest CapScore, highlighting this balance. By contrast, SSC maintains higher CapScore with fewer assertions, demonstrating better alignment with the image content but at the cost of less detailed descriptions. This behavior highlights the importance of balancing richness and accuracy in multimodal tasks. Models aiming for high-precision applications (e.g., zero-shot classification with CLIP) may benefit from shorter captions (e.g., SSC), whereas scenarios requiring more detailed scene descriptions (e.g., 19 Preprint Figure A2: An overview of CapScore to evaluate the quality of captions: we use LLM to generate assertions based on the caption and then use VQA model to check these assertions. mutlimodal LLMs) may prioritize longer captions like DSC+, accepting some decrease in factual accuracy. Table A8: CapScore and Average Number of Assertions (ANA) to evaluate the richness and accuracy of different captions. Caption CapScore() ANA() LLaVA-1.5 (Liu et al., 2023b) SSC DSC DSC+ 88.76 91.56 87.30 75.74 7.30 2.49 8.13 12.20 More detailed but noisy captions are helpful for MLLM pre-training. We examine the impact of hallucinations in image-text data used for MLLM pre-training, focusing on the type of hallucinations detected in our captions. For our primary comparison, we select the best-performing model, LLaVA1.5 (Liu et al., 2023b), as shown in Table A7. We first utilize LLaVA-1.5 to generate captions on the VeCap-300M dataset (Lai et al., 2024). Next, we apply our captioner to generate DSC+ for the same set of images. Our captions are more detailed but contain more hallucinations on object. We use MM1s pre-training under small scale setting as case study to examine whether image-caption data with fewer hallucinations or with more details can offer advantages for MLLM pre-training. We evaluate the two models after applying fine-tuning using the same data recipe. As shown in Table A9, the MLLM pre-trained with more detailed captions performs better, even though these data contain more hallucinations. When examining task-specific results, we observe that our DSC+ pre-trained model outperforms the LLaVA captions model on 7 out of 9 benchmarks. Specifically, DSC+ improves performance on VQAv2, MMMU, MathV, and SEED, among others, indicating that the added detail from DSC+ benefits these tasks. However, despite the slight degradation in MMEP results (727.2 vs. 773.4), the overall advantage of detailed captions with minor hallucinations demonstrates that balance between information richness and accuracy can positively impact MLLMs performance. The larger gains in multimodal understanding tasks, such as MMEC and LLaVAW, suggest that hallucination-tolerant MLLM pre-training may help with complex vision-language reasoning. Table A9: SFT evaluation of models pre-trained with captions generated by LLaVA-1.5 and our DSC+. We use the same SFT recipe and evaluation benchmarks as in MM1 (McKinzie et al., 2024). Pre-Trained Data VQAv2 VQAT MMMU MathV MMEP MMEC SEED POPE LLaVAW LLaVA Captions DSC+ 78.0 79. 65.4 65.4 30.0 32.6 27.6 29.4 773.4 727.2 200.4 224.3 63.5 66. 83.7 85.1 63.6 71."
        }
    ],
    "affiliations": [
        "Apple AI/ML"
    ]
}