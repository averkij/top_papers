{
    "paper_title": "ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model",
    "authors": [
        "Jinwei Qi",
        "Chaonan Ji",
        "Sheng Xu",
        "Peng Zhang",
        "Bang Zhang",
        "Liefeng Bo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements."
        },
        {
            "title": "Start",
            "content": "ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model"
        },
        {
            "title": "Jinwei Qi Chaonan Ji",
            "content": "Sheng Xu Tongyi Lab, Alibaba Group https://humanaigc.github.io/chat-anyone/"
        },
        {
            "title": "Liefeng Bo",
            "content": "5 2 0 2 7 2 ] . [ 1 4 4 1 1 2 . 3 0 5 2 : r Figure 1. Illustration of real-time portrait video generation. Given portrait image and audio sequence as input, our model can generate high-fidelity animation results from full head to upper-body interaction with diverse facial expressions and style control."
        },
        {
            "title": "Abstract",
            "content": "Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains challenge. To address these limitations, we introduce novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512768 resolution at up to 30fps on 4090 GPU, supporting interactive videochat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements. 1. Introduction In recent years, the rapid advancement of LLMs and diffusion models has significantly enhanced the capabilities of text and voice chat [2, 5], which achieves astonishing results in interactive AI conversation. Looking ahead, realtime interactive digital humans in video formats represent the future trend, promising even more immersive and truly lifelike interactions. Consequently, as shown in Figure 1, there is growing demand for highly expressive portrait videos that not only exhibit natural head poses and facial expressions, but also fluid and contextually appropriate body movements, creating an authentic and engaging user experience. Achieving highly expressive real-time interactive portrait video generation has always been hot research topic. Early works [21, 34] leverage the advancements in Generative Adversarial Networks (GANs) [8] to focus on lip sync, predicting lip motion in mouth region. Further research [41] extends the scope to generate overall head movements from single image, including not only facial expressions but also head poses. Additionally, some studies go step forward to introduce basic emotion control [6] or expand the approach to generate simple body movements [36]. Despite these advancements, there is still long way to go for achieving fine-grained emotionally controllable and photorealistic portrait video generation. With the rise of diffusion models recently, many efforts have explored the powerful generative capabilities of diffusion models to achieve higher expressiveness in both face and body dynamics [4, 26, 27]. However, these methods often struggle with real-time inference due to their computational complexity and the iterative nature of denoising process, which makes them fail to meet the latency requirements for real-time interactions. In addition to the aforementioned approaches, some recent works [35, 37] design lightweight diffusion models, which learn audio to facial motion representation mappings, by taking advantage of diverse generative capabilities of diffusion models for expressive face motion generation. Subsequently, they employ efficient GANs to generate high-quality head images. Although these methods make step forward to balance the trade-off between efficiency and diversity in talking head contents, these methods still fall short in finegrained expression control, for example, achieving subtle variations in expression intensity or replicating specific emotional style to particular individuals. Besides, generating highly realistic textures and detailed hand gestures also remains challenging. To address these limitations and achieve more realistic real-time portrait video interactions, we need to focus on three key areas: 1) Fine-grained expression control and style transfer: Achieve precise control over facial expression and head pose, while enabling style transfer to enhance expressiveness and personalization. 2) Coordinated body dynamics including hand gestures: Generate natural body movements including detailed hand gestures, which also need to be synchronized with facial expressions for realistic interaction. 3) High extensibility and real-time inference efficiency: Support extensible real-time generation from head-only to upper-body animations, ensuring high performance and efficiency for interaction applications. In response to the above challenges, we introduce novel framework for stylized real-time portrait video generation including two stages, namely upper-body motion representation generation from audio inputs and high-quality portrait video generation from single image in real-time. Our approach enables expressive and flexible video chat which can extend from talking head to upper-body interaction, significantly enhancing the realism and engagement of digital human interactions. The key contributions of our work are as follows: Efficient Hierarchical motion diffusion model is proposed for the first stage to generate face and body control signals hierarchically based on input audio, considering both explicit and implicit motion signals for precise facial expressions. Furthermore, fine-grained expression control is introduced to realize different variations in expression intensity, as well as stylistic expression transfer from reference videos, which aims to produce controllable and personalized expressions. Hybrid control fusion generative model is designed for the second stage, which utilizes explicit landmarks for direct and editable facial expression generation, while implicit offsets based on explicit signals are introduced to capture facial variations on diverse avatar styles. We also inject explicit hand controls for more accurate and realistic hand textures and movements. Additionally, facial refinement module is employed to enhance facial realism ensuring highly expressive and lifelike portrait videos. Extensible and real-time generation framework is constructed for interactive video chat application, which can adapt to various scenarios through flexible sub-module combinations, supporting tasks ranging from head-driven animation to upper-body generation with hand gestures. Besides, we establish an efficient streaming inference pipeline that achieves 30fps at resolution of maximum 512 768 on 4090 GPU, ensuring smooth and immersive experiences in real-time video chat. Our experimental results highlight the effectiveness of our proposed approach in generating portrait videos that exhibit both rich facial expressiveness and upper-body movements. This capability is particularly valuable for creating engaging and lifelike digital human interactions. 2. Related Work In this section, we focus on reviewing existing methods that utilize two-stage framework for audio-driven portrait video synthesis. The first stage involves generating motion representations from audio inputs, while the second stage aims at transforming motion representation into highquality digital human videos. 2.1. Audio to motion representation The first stage converts audio inputs into detailed facial motion representations, and various techniques have been proposed to achieve this. Some early methods [22, 41, 42] take the parameters of three-dimensional morphable face models (3DMMs) to represent facial motions. In these methods, identity, facial expressions and head pose are decomposed into separate coefficients, supporting independent control over each aspect of the face. However, 3DMM-based methods have limitations in capturing fine-grained facial details due to the parameterized facial model. Subsequent works [33, 44] introduce explicit facial landmark or mesh as control signals, which offer more flexible and expresFigure 2. Pipeline of upper-body video generation with hybrid control fusion, which takes both explicit facial keypoints and implicit body keypoints to conduct feature warping, while rendered hand image further inject into generator for improving the quality of hand generation. sive representation, and have strong generalization capabilities, enabling the facial control of non-human characters. Despite these improvements, landmark-based methods still face challenges in capturing subtle variations and nuances in facial expressions. The latest advancements propose the use of implicit control signals, such as implicit keypoints or motion latent variables to achieve better facial expression control. EAT [6] and Ditto [14] employ 3D implicit keypoints for facial motion control and further apply emotion label as additional condition. While AniTalker [16] and VASA-1 [35] extract implicit motion latent variables from video sequences to capture more detailed movements, and construct diffusion motion generator to perform emotionally rich facial expressions. However, these fully implicit motion representations still have the challenge of completely decoupling identity and expression, leading to potential artifacts where identity might inadvertently change along with expression. To address these limitations, we explore hybrid multimodal model to combine explicit and implicit motion representations for fully utilizing their complementary advantages. Beyond facial expressions, some recent works [4, 27, 36] have extended their focus to predict control signals for upper-body movements, though further exploration is still needed to develop effective coordination between facial and body movements. 2.2. Portrait video generation The second stage focuses on the generation of high-quality digital human video frames, conditioned on the motion representations predicted from the first stage by audio inputs. Early works [18, 44] in this domain primarily rely on Generative Adversarial Networks (GANs) [8] for generating portrait video frames. Furthermore, some researchers explore warp-based methods [6, 16, 35, 36] that learn dense motion flow from source image to target, achieving better results in terms of preserving image quality and detail. Despite their generative power, GANs still struggle with high-quality detail generation, especially in facial expressions and body movements as well as their complex interactions. With the advent of neural rendering techniques, some methods adopt Neural Radiance Fields (NeRF) [10, 17, 38] or Gaussian Splatting [39] for portrait video generation, which offer superior rendering quality. These neural rendering based methods are capable of generating highly photorealistic portrait videos, supporting real-time generation at the same time. However, notable drawback of these methods is the need for retraining on each specific character, which limits their scalability. More recently, diffusion models have shown remarkable progress in generating high-quality digital human videos. Methods like AniPortrait [33], Vlogger [4], EMO2 [27] inject face or body control signals into denoising UNet along with reference net to generate talking portrait video. However, diffusion methods face challenges in generating longduration videos stably and efficiently. Their inference process is computationally intensive and slow, making realtime generation difficult. Given the strengths and weaknesses of the above approaches, we opt to employ GANs as our primary generator aiming to balance the inference efficiency and performance for both facial and upper-body generation. 3. Method Our proposed approach generates high-quality, stylized portrait videos driven by audio inputs. It takes character image (either head shot or upper-body shot) and an audio clip as primary inputs, with an optional reference video for style transfer. The process consists of two main stages: first, we convert audio input into motion representations that capture facial expressions and body movements; second, we synthesize the portrait video using these motion representations and the character image. If provided, the reference video guides the style transfer to ensure the generated video reflects the desired stylistic elements. In the following subsections, we first give preliminaries of our approach, then motion representation learning is introduced. Next we present the module that generates motion representations from audio inputs. Finally, we describe the video generation based on motion representations. 3.1. Preliminaries To balance the quality of generated videos and model inference performance, we employ warping-based GAN framework inspired by existing works [9, 30]. The general framework consists of four key components as shown in Figure 2: 1) Appearance feature extraction to get visual features from source image; 2) Motion representation extraction that extracts motion to represent facial expressions and body movements, described in Section 3.2; 3) Warping field estimation to calculate the transformation from source to target; and 4) Generator to synthesize final images from warped appearance features, introduced in Section 3.4. To generate high expressive motion representations from audio inputs, our approach is based on the diffusion model [12]. In the forward process, noise ϵ (0, 1) is gradually added to the original data x0 over series of time steps t. The reverse process aims to remove the noise and recover the original data. This is achieved using learned neural network ϵθ(xt, c, t), where indicates that control signals in our approach discussed in Section 3.3, e.g. audio and reference motions. The training objective is defined as follows: = Et,c,xt,ϵ (cid:104) ϵ ϵθ(xt, t, c)2(cid:105) (1) which minimizes the difference between predicted noise ϵθ(xt, c, t) and actual noise. 3.2. Motion representation To balance the expressiveness and controllability of face and upper-body movements, we consider three types of motion representations commonly used in existing methods. Explicit keypoints [13] and body templates [45] provide clear and editable control over movements, while implicit keypoints [9] offer richer detail information. To leverage the strengths of all three representations, we adopt hybrid representation that combines their advantages. Specifically, head motion is represented by 3D keypoints projected from 3DMM head template [29], upper body motion is derived from implicit keypoints, while hand motion is captured using rendered images based on the MANO template [23]. 4 ori For head motion, 3D keypoints head on key face regions are projected from the head template, including eyes, mouth, eyebrows and face contours, driven by 3DMM coefficients, which are capable of representing diverse range of facial expressions. However, restricted by the head templates expressive capabilities, these keypoints cannot represent extreme facial features, such as large eyes or twisted mouth. To address this issue, we propose keypoints displacement module that aligns the keypoints with actual positions of facial features from image by adding an implicit offset head to the original explicit keypoints head , as shown in the right of Figure 4, which can capture more detailed facial expressions. The final head keypoints head are represented as: ori head = head ori + head (2) For upper body motion, we adopt 3D implict keypoints similar to LivePortrait [9]. Specifically, given the canonical Rk3 extracted from the image, along keypoints body with body movement δ Rk3, rotation R33, scale factors R1, and translations R3. The rotation is set as the identity matrix that simplifies body movements into planar motion. The final driving 3D implicit keypoints body can be represented as: body = s(X body + δ) + (3) For hand motion, we adopt hand coefficients to render hand images from MANO template as control signals, using textures derived from our collected high-resolution data. The pixel-aligned hand control signals are particularly beneficial for preserving the hand shape and ensuring highquality hand gesture generation. 3.3. Audio to motion representation To effectively generate motion representations from audio inputs, especially in the context of upper-body driving, we design hierarchical model, which is inspired by the observation that human upper-body movements are often driven by head motions, while hand movements mainly follow the rhythm of audio signal. Thus, as shown in Figure 3, our hierarchical audio driving model first predicts facial motion from audio inputs, and then generates upper-body (including hands) motions driven by the previously predicted facial motion. Facial motion prediction with style control. In the first sub-stage, facial motion is predicted directly from the audio inputs. Specifically, we use the expression blendshapes and 6-dimensional pose coefficients including rotation and translation defined on the head template [29] as facial motion representation, with the goal of simple, effective and efficient model learning. For the input audio clip, we extract audio features from transformer-based speech recognition The model predicts facial keypoint offsets head, as mentioned in Section 3.2 to refine facial expression details and generates upper-body keypoints along with hand control coefficients. This ensures that the upper-body motions naturally follow the heads dynamics while incorporating hand gestures synchronized with the audio rhythm, which can generate natural and coherent body motions as well as expressive and realistic gestures that match the spoken content for lifelike portrait video. 3.4. Video generation We employ warping-based GAN framework similar to [9] for generating upper-body image from motion representation. The entire process is divided into two stages: upper body image generation and face refinement, which share similar network architecture. Upper body image generation. The framework shown in Figure 2 includes an appearance feature extractor F, keypoint detector L, warping field estimator and generator G. Given source image, detects canonical body keypoints body c,s , body movement δ and translation t. The source 3D implicit body keypoints body can be obtained according to Eq. 3. Notably, we select explicit 3D face keypoints head from the head template as additional control signals, and incorporate them with implicit body keypoints. Ultimately, the control signals received by can be represented as: f ull = head body (4) to head The source feature volume derived from is warped to the target feature volume based on source ull and driving ull through W. Subsequently, generator processes the warped features and generates target image. Besides, we also train face animation version with the same architecture driving from head , realizing talking head generation. High quality hand generation. The structure of hand is complex and prone to self-occlusion, making it challenging to generate high-quality hand images based solely on 3D implicit keypoints. To address this issue, we first utilized Image Quality Assessment (IQA) [1] to filter out lowquality hand data. Then, we employ the MANO template to render hand images and inject them into the generator, thereby providing stronger prior information to facilitate hand generation. Specifically, given that certain layer of the generator has feature values denoted as , we introduce an additional AdaIN module to incorporate the supplementary information from rendered hand image Ihand, which is represented as: = + AdaIN (Ihand) (5) The rendered hand image Ihand is injected across multiple scales within the generator, significantly improving the 5 Figure 3. Illustration of hierarchical audio2motion diffusion model, including facial motion prediction with style control at bottom, and upper-body motion prediction with hands at top. model [7], containing rich semantic information that guides the generation of lip movements and facial expressions. The core face driving model is based on diffusion transformer [20]. We inject audio features into the network using cross-attention mechanisms. To ensure temporal coherence, we also incorporate historical motion information via cross-attention for smooth transitions between consecutive frames. Besides, for style control, we use Adaptive Layer Normalization (AdaLN) to inject coarse-grained expression and pose range information into the network, enabling precise control over the amplitude of facial movements. Additionally, facial expression sequence extracted from reference video by 3D face reconstruction, can be optionally provided for style transfer. The reference sequence is concatenated along the temporal dimension with the noise latent, which can effectively transfer the style from the reference video to the final output, enabling high controllable facial expression generation. Upper-body motion prediction with hands. In the second sub-stage, we extend the facial motion predictions to drive upper-body movements. Considering the discrepancies between facial coefficients and body keypoints, as illustrated in the top of Figure 3, we first project the facial parameters into 3D keypoints through head template. These projected keypoints are served as conditions injected into transformer network. Simultaneously, the aforementioned audio features as well as appearance feature from source image are also injected through cross-attention. 4. Experiments In this section, we introduce our experimental setup and results. We begin with the implementation details, followed by presenting results under both video-driven and audiodriven setup compared with existing methods. Finally, we conduct ablation studies to validate the effectiveness of different modules in our approach. 4.1. Implementation details We independently train two-stage models for audio-driven and video generation tasks. For audio to motion representation part, we use 6-layer diffusion transformer to predict facial coefficients, while the following upper-body motion prediction model adopts 2-layer transformer. Both modules are trained on an A100 GPU with batch size of 16 for 100K steps. For video generation part, we have different resolutions depending on whether we generate upper-body or head-only images. Upper-body generation outputs at 512 768, while head-only generation has 512 512 output resolution. All models utilize Adam optimizer with learning rate of 1e-4, trained on 8 A100 GPUs for 7 days with batch size of 32. During inference, the video generation modules run serially and achieve real-time inference speed of 30fps on 4090 GPU. More details of our implementation as well as inference pipeline are presented in supplementary materials. For training and testing data, we collect and clean about 20,000 talkshow video clips from YouTube, totaling approximately 30 hours at 30 fps, and we randomly select 500 clips for testing during comparison with existing methods as well as our ablation studies. 4.2. Video generation results Given the limited number of GAN-based upper-body audiodriven works, we first independently validate the effectiveness of our video generation model in producing upperbody videos through self-driven reenactment setup. This validation is crucial as it directly determines the quality of the generated portrait video in upper-body audio-driven scenarios. We compare our approach against several existing GAN-based video-driven methods, including FOMM [24], MRAA [25], LIA [31], and TPSMM [43], using multiple metrics to assess the quality of the generated frames: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) [32], Learned Perceptual Image Patch Similarity (LPIPS) [40], Frechet Inception Distance (FID) [11], and Frechet Video Distance (FVD) [28]. We also compute the cosine similarity (CSIM) between the features of reference image and generated video frames. Additionally, we calculate Hand Keypoint Confidence (HKC) to evaluate the quality of hand representation in generated frames. We present quantitative comparison results in Table 1, and we also provide qualitative comparisons through visual Figure 4. Illustration of face refine network, the left of figure shows the architecture, while the right demonstrates that more precise facial keypoints are located by adding implicit offset. quality of hand generation with negligible additional computational cost. Cascaded loss terms. We follow LivePortrait [9] to use implicit keypoints equivariance loss LE, keypoint prior loss LL, deformation prior loss LD, perceptual loss LP er, GAN loss LGAN , reconstruction loss LRecon. To enhance the quality of hand region, we obtain the hand mask using the MANO model and introduce hand region perceptual loss LHand er . For more precise representation of limb movements through implicit keypoints, we adopt 2D landmark loss Llms to optimize the positions of implicit keypoints similar to [9]. The overall training objective is formulated as follows: = LE + LL + LD + LP er + LGAN + LRecon + Llms + LHand er (6) Face refine network. Restricted by the generative capabilities of GANs and the relatively small proportion of the face in images, the aforementioned network fails to produce satisfactory facial details, which significantly diminishes overall realism. To overcome this, we introduce lightweight face refine network, driven by 3D explicit head keypoints with implicit offset as described in Equation 2, as shown in Figure 4, which shares similar structure with the body generation network but with the following distinctions. Specifically, the input for the generators AdaIN module is the cropped background image of the head, where the facial region is masked out, ensuring that the restored face can seamlessly blend with the generated half-body image. The training loss is similar to that used in the upper-body training, with hand region perceptual loss LHand replaced by facial region loss Lf ace er . Additionally, we omit the 2D landmark loss Llms. er Finally, our generative network produces upper-body portrait images with hand motion, and the facial region is specially refined and reintegrated into the original image to obtain the final result. This strategy ensures that the generated image retains rich details of facial expressions while maintaining the visual quality of body movements."
        },
        {
            "title": "Method",
            "content": "FOMM [24] MRAA [25] LIA [31] TPSMM [43] w/o hand injection w/o face refine Ours w/o facial hybrid control* Ours* PSNR SSIM LPIPS FID FVD CSIM HKC FPS (Resolution) 18.92 19.12 18.96 19.64 24.59 24.87 24.88 22.85 23. 0.677 0.696 0.681 0.707 0.829 0.829 0.831 0.799 0.807 0.269 0.253 0.258 0.237 0.132 0.126 0.126 0.170 0. 42.690 35.546 44.747 34.509 6.825 5.799 5.505 6.355 6.297 569.893 419.293 387.924 384.663 38.401 34.124 33.349 64.249 47. 0.525 0.536 0.590 0.597 0.605 0.613 0.654 0.627 0.632 0.494 0.534 0.548 0.567 0.607 0.652 0.652 - - 87 (256*256) 77 (384*384) 30 (256*256) 48 (384*384) 34 (512*768) 37 (512*768) 33 (512*768) 40 (512*512) 41 (512*512) Table 1. Quantitative comparisons of upper-body video generation under self-driven reenactment mode. The inference speed, measured in frames per second (FPS), and the resolution of the generated output are also presented in the table. The * symbol denotes that the evaluations are conducted on talking head video reenactment to verify the effectiveness of implicit facial keypoint offset. Method HKC CSIM Method FID CSIM Sync Diversity CyberHost* [15] Ours* EchoMimicV2* [19] Ours* 0.723 0. 0.618 0.642 0.706 0.657 0.621 0.683 Table 2. Quantitative comparisons of video generation with diffusion based method. We reenact the demo videos provided by the compared methods and evaluate the results of hand generation. examples as shown in Figure 5. The results clearly show that our proposed approach significantly outperforms existing methods in terms of upper-body generation quality, achieved by hybrid control fusion of both explicit and implicit motion representation for video generation. Specifically, our approach demonstrates superior performance in generating hand movements, as evidenced by qualitative comparisons where the quality of hand generation is notable better than that of existing methods, which is thanks to our proposed high-quality hand generation by incorporating the rendered hand images as supplementary information. Besides, we further compared with latest diffusion based methods as shown in Table 2, including CyberHost [15] and EchoMimicV2 [19] to evaluate the upper-body generation with hand, and we can observe that we still achieve comparable results that indicate the effectiveness of our approach. Additionally, the inference speeds are also presented in Table 1. Although the compared methods achieve higher FPS, their model output resolutions are relatively low. In contrast, our approach achieves real-time inference at 30+FPS with higher output resolution as well as superior quantitative performance, which highlights the advantages of our solution, balancing both quality and efficiency, and making it suitable for practical applications. SadTalker [41] AniTalker [16] Ours 52.32 19.74 9.49 0.595 0.578 0.668 4.120 4.066 5.668 0.112 0.099 0.137 Table 3. Quantitative comparisons of audio-driven results under head-only talking animation generation. 4.3. Audio driven results In this subsection, we focus on comparing the results of audio-driven portrait videos, with an emphasis on lip-sync accuracy and motion diversity. Given the limited number of GAN-based works directly addressing audio-driven upperbody motion generation, we conduct targeted comparison with existing GAN-based talking head methods, specifically evaluating the quality of facial generation under audiodriven conditions. The primary compared methods include SadTalker [41] ad AniTalker [16]. Our evaluation metrics include not only the image quality metrics mentioned earlier, but also the Sync metric, proposed by SyncNet [3] to assess the synchronization between lip movements and audio signals, and face keypoint variance indicating the diversity of facial movements. We present the quantitative experimental results in Table 3, while visual comparisons of talking head generation results from different methods are provided in supplementary materials. These quantitative comparisons and visual examples highlight the differences in lip-sync accuracy and motion diversity among the compared methods. The results demonstrate that our proposed approach outperforms existing methods, indicating better alignment between mouth movements and corresponding audio signals and more varied facial expressions. These enhancements can be attributed to our hierarchical motion diffusion model, which 7 Figure 5. Qualitative comparisons of upper-body video generation under self-driven reenactment setup. Our approach significantly outperforms the GAN-based comparison methods, and achieves comparable quality with the diffusion-based method EchoMimicV2. Method MAE SSIM w/o style transfer Ours 0.074 0.049 0.373 0.709 Table 4. Comparison on reference style transfer, calculated on the face coefficients predicted from audio2motion model. combines explicit and implicit control signals for more precise and expressive facial and limb movements. 4.4. Ablation studies To evaluate the effectiveness of various sub-modules in our proposed approach, we conduct comprehensive ablation studies, including facial style control and transfer capability, the impact of explicit and implicit hybrid control fusion, the effect of hand signal injection, and the performance of face refinement. Visual comparisons of these ablation studies are shown in supplementary materials. Facial style control and transfer. We evaluate the ability to control and transfer facial expression styles. First, we compare the impact of injecting expression motion sequences from reference video on the final generated facial coefficients. The similarity between the generated facial coefficients and the ground-truth is measured by MAE and SSIM. The results presented in Table 4 show that injecting expression information from reference video can improve the similarity to ground-truth, indicating that the expression style in reference video is effectively transferred into the generated results. Besides, qualitative comparisons shown in supplementary materials further validate the influence of explicitly controlling the magnitude of expressions. Explicit and implicit hybrid control fusion. We examine the effect of combining explicit and implicit keypoints on facial expression image generation. Specifically, we compare the results with and without the implicit keypoint offsets, as described in Equation 2. The comparison results, shown in Table 1, indicate that incorporating implicit keypoint offsets improves the quality and accuracy of generated facial expression images. This enhancement suggests that introducing hybrid control helps to increase the flexibility and diversity of audio-driven facial expressions, while maintaining high controllability at the same time. Hand signal injection. We also investigate the impact of injecting hand control signals on the quality of upper-body image generation. The results in Table 1 reveal significant degradation in image quality when hand control signals are not injected. This finding indicates the critical role of hand signal injection, enabling the network to produce accurate and clear hand movements. The addition of hand control signals ensures that the generated upper-body images are more realistic and coherent. Face refinement. We finally assess the effectiveness of the face refinement module. The results are presented in Table 1, which show notable improvement in the quality of generated facial images when face refine is applied. This indicates that face refine enhances the overall realism and expressiveness of the portrait video by improving the quality of facial region. The enhanced facial details contribute to more lifelike upper-body portrait video. 5. Conclusion In this paper, we introduce novel stylized portrait video generation approach using hierarchical motion diffusion audio-driven model and hybrid control fusion video generation model. Our two-stage extensible framework supports tasks from head-driven animations to upper-body videos with hand movements, achieving expressive lifelike portrait video generation, and ensuring inference at 30fps on 4090 GPU, which can provide robust solution for real-time and high-quality video-chat interactions with various applications such as virtual avatars, live streaming, and augmented reality."
        },
        {
            "title": "References",
            "content": "[1] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch toolbox for image quality assessment. [Online]. Available: https : / / github . com / chaofengc / IQA - PyTorch, 2022. 5 [2] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding arXiv via unified large-scale audio-language models. preprint:2311.07919, 2023. 1 [3] Joon Son Chung and Andrew Zisserman. Out of time: Automated lip sync in the wild. In ACCV, pages 251263, 2016. 7 [4] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, and Cristian Sminchisescu. VLOGGER: multimodal diffusion for embodied avatar synthesis. arXiv preprint:2403.08764, 2024. 2, 3 [5] Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for realtime dialogue. arXiv preprint:2410.00037, 2024. [6] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, and Yi Yang. Efficient emotional adaptation for audio-driven In ICCV, pages 2257722588, talking-head generation. 2023. 2, 3 [7] Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition. In Interspeech, pages 20632067, 2022. 5 [8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint:1406.2661, 2014. 1, 3 [9] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint:2407.03168, 2024. 4, 5, 6 [10] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In ICCV, pages 5764 5774, 2021. 3 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, pages 66266637, 2017. 6 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 4 [13] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, pages 81538163, 2024. 4 [14] Tianqi Li, Ruobing Zheng, Minghui Yang, Jingdong Chen, and Ming Yang. Ditto: Motion-space diffusion arXiv for controllable realtime talking head synthesis. preprint:2411.19509, 2024. 3 [15] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, and Yanbo Zheng. Cyberhost: Taming audiodriven avatar diffusion model with region codebook attention. In ICLR, 2025. [16] Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu. Anitalker: Animate vivid and diverse talking faces through identity-decoupled facial motion encoding. In ACM MM, pages 66966705, 2024. 3, 7 [17] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou. Semantic-aware implicit neural audiodriven video portrait generation. In ECCV, pages 106125, 2022. 3 [18] Yuanxun Lu, Jinxiang Chai, and Xun Cao. Live speech portraits: real-time photorealistic talking-head animation. ACM TOG, 40(6):220:1220:17, 2021. 3 [19] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Echomimicv2: Towards striking, simplified, and Ma. semi-body human animation. arXiv preprint:2411.10061, abs/2411.10061, 2024. 7 [20] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41724182, 2023. 5 [21] K. R. Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C. V. Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In ACM MM, pages 484492, 2020. 1 [22] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, and Shan Liu. Pirenderer: Controllable portrait image generation via In ICCV, pages 1373913748, semantic neural rendering. 2021. 2 [23] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: modeling and capturing hands and bodies together. ACM TOG, 36(6):245:1245:17, 2017. [24] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In NeurIPS, pages 71357145, 2019. 6, 7 [25] Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In CVPR, pages 1365313662, 2021. 6, 7 [26] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. EMO: emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In ECCV, pages 244260, 2024. 2 [27] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. EMO2: end-effector guided audio-driven avatar video generation. arXiv preprint:2501.10687, 2025. 2, 3 [28] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. In ICLR, 2019. FVD: new metric for video generation. 6 [29] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. Faceverse: fine-grained and detailcontrollable 3d face morphable model from hybrid dataset. In CVPR, pages 2030120310, 2022. 4 [30] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In CVPR, pages 1003910049, 2021. [31] Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. In ICLR, 2022. 6, 7 9 [32] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600612, 2004. 6 [33] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint:2403.17694, 2024. 2, 3 [34] Xiuzhe Wu, Pengfei Hu, Yang Wu, Xiaoyang Lyu, Yan-Pei Cao, Ying Shan, Wenming Yang, Zhongqian Sun, and Xiaojuan Qi. Speech2lip: High-fidelity speech to lip generation by learning from short video. In ICCV, pages 22111 22120, 2023. 1 [35] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. VASA-1: lifelike audio-driven talking faces generated in real time. In NeurIPS, 2024. 2, [36] Huan Yang, Jiahui Chen, Chaofan Ding, Runhua Shi, Siyu Xiong, Qingqi Hong, Xiaoqi Mo, and Xinhan Di. Self-supervised learning of deviation in latent represenarXiv tation for co-speech gesture video generation. preprint:2409.17674, 2024. 2, 3 [37] Sejong Yang, Seoung Wug Oh, Yang Zhou, and Seon Joo implicit face motion diffusion model arXiv Kim. for high-fidelity realtime talking head generation. preprint:2412.04000, 2024. 2 IF-MDM: [38] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, and Zhou Zhao. Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis. In ICLR, 2023. 3 [39] Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, and Gang Yu. Gaussiantalker: Speaker-specific talking head synthesis via 3d gaussian splatting. In ACM MM, pages 35483557, 2024. 3 [40] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pages 586 595, 2018. [41] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In CVPR, pages 86528661, 2023. 1, 2, 7 [42] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with highresolution audio-visual dataset. In CVPR, pages 36613670, 2021. 2 [43] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In CVPR, 2022. 6, 7 [44] Yang Zhou, Dingzeyu Li, Xintong Han, Evangelos Kalogerakis, Eli Shechtman, Jose Echevarria. Makeittalk: Speaker-aware talking head animation. arXiv preprint:2004.12992, 2020. 2, 3 and [45] Yang Zhou, Jimei Yang, Dingzeyu Li, Jun Saito, Deepali Aneja, and Evangelos Kalogerakis. Audio-driven neural gesture reenactment with video motion graphs. In CVPR, pages 34083418, 2022."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}