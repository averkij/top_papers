{
    "paper_title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
    "authors": [
        "Chunwei Wang",
        "Guansong Lu",
        "Junwei Yang",
        "Runhui Huang",
        "Jianhua Han",
        "Lu Hou",
        "Wei Zhang",
        "Hang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing."
        },
        {
            "title": "Start",
            "content": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, Hang Xu Huawei Noahs Ark Lab 4 2 0 2 9 ] . [ 1 3 7 6 6 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce ILLUME, unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within single large language model through unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of vision tokenizer that incorporates semantic information and progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining over four times fewer than what is typically needed while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing. 1. Introduction Recent research efforts [1, 9, 27, 29] have equipped Large Language Models (LLMs) with the capability to see images by utilizing vision adapters to map features from CLIPlike encoders into LLMs input spaces. Works like the LLaVA series [27, 28] have demonstrated exceptional results on visual comprehension tasks. Meanwhile, the field of text-to-image generation has achieved remarkable progresses in developing both diffusion-based [37, 38] and more recent autoregressive models [17, 44]. These techEqual contribution, Corresponding author Figure 1. Performance on various visual understanding (blue for General and green for Document-oriented), generation (red), and editing (gray) benchmarks. ILLUME achieves competitive results with state-of-the-art works. nological strides are propelling the community towards the creation of versatile Multimodal Large Language Models (MLLMs) that can seamlessly integrate visual understanding and generation capabilities. This integration not only extends LLMs across wide range of multimodal applications, but also unlock new possibilities for improving the synergy between vision and language tasks. Previous works have explored several methods that empower MLLMs to draw images. In [24, 50], image generation models are deployed as tools, with text commands as input to create images. This decoupled architecture inherently limits the models potential. In contrast, Emu [45], Emu2 [46], and X-VILA [55] introduce unified autoregressive model that alternatively predicts the next element by regressing visual embeddings or classifying text tokens. While these models have shown promising results, joint training of LLMs and diffusion models requires substantial engineering costs and well-designed training strategy to ensure stability. To avoid these complexities, recent inno1 Figure 2. ILLUME can handle various multimodal tasks, including understanding for images and charts; text-to-image generation; and mixed-modal generation task such as object modification and style transfer. vations like Chameleon [47], AnyGPT [59] and Emu3 [49] employ Vector-Quantized (VQ) tokenizer to transform images into discrete tokens and extends LLMs with an additional vision vocabulary. In this paradigm, the MLLM is optimized via unified next-token prediction formulation. Along with the consistency of the discrete design with text, these approaches open up substantial potential for multimodal models. However, we observe that extending the vision vocabulary in an LLM necessitates extensive data for image-text alignment in existing methods, as indicated in Table 1. This observation prompts us to ask: Can we develop unified MLLM more efficiently? In response, we propose ILLUME, unified MLLM that requires only 15M data for image-text alignment during MLLM pretraining four times fewer than Janus [51] yet delivers competitive performance compared to state-of-the-art models. This increased efficiency is primarily attributed to two designs. First, we employ semantic vision tokenizer tailored for MLLMs. Unlike traditional VQ tokenizers that rely on image reconstruction loss for training (e.g., VQGAN [12]), our approach quantizes images into discrete tokens within seMethod LLM Num. of image-text pairs Num. of interleaved data Chameleon [47] LWM [30] Unified IO 2 [33] SEED-LLaMA [15] AnyGPT [59] Janus [51] 7B from scratch LLaMA-2-7B 6.8B from scratch Vicuna-7B LLaMA-2 7B DeepSeek-LLM-1.3B ILLUME (Ours) Vicuna-7B 1.4B 1B 970M 600M 300M 65M 15 400B tokens - 157M 150M 7.3M - - Table 1. Statistics on the data volumes required for image-text alignment in previous next-token prediction-based works. Notably, ILLUME utilizes only 15M image-text pairs, which is 4 times fewer than Janus, yet achieves superior performance. mantic feature space. This method significantly accelerates the image-text alignment process in MLLMs. Moreover, ILLUME is implemented with three-stage training procedure. It innovatively introduces an image reconstruction task to facilitate rapid initialization of the newly integrated weights in LLMs due to the extension of vision vocabulary, promoting the model to learn pixel dependencies for image generation. With diverse range of vision-language data types utilized during training, ILLUME is ultimately capable of handling various multimodal tasks, as illustrated in Figure 2. 2 Figure 3. Overall architecture of ILLUME. (a) We enhance LLMs with the capability to see images by employing vision adapter that maps features from vision encoder into LLMs input spaces. To expand the models abilities to generate images, the LLM is extended with an additional vision vocabulary to produce discrete vision tokens. (b) In the vision tokenizer, we utilize pretrained vision encoder to extract semantic features and supervise quantization process through feature reconstruction loss. The reconstructed features are then processed by Stable Diffusion model to recover the original images. When we obtained our unified model, new question arises: Can the discriminative and generative capabilities of an MLLM enhance each other? Previous research on LLMs [5] suggests that self-generated contents can serve as valuable feedback for the model to self-improve. The ability of an MLLM to generate images allows it to pinpoint and address its weaknesses by learning from its own imperfect outputs, thereby enhancing its ability to interpret images more accurately. Furthermore, the MLLM can utilize its discriminative skills to evaluate whether self-generated images align with user instructions, guiding it to avoid potential mistakes in generating images. To harness this potential, we propose novel self-enhancing multimodal alignment scheme that teaches MLLMs to assess the consistency between self-generated images and text descriptions, as well as to understand the underlying reasons for any discrepancies. With this alignment scheme, we observe improvements in both discriminative and generative capabilities within unified infrastructure. We evaluate our model on popular visual understanding, generation and editing benchmarks, where ILLUME achieves competitive results with existing unified MLLMs and specialized models. In brief, our contributions are summarized as follows. We introduce ILLUME, unified MLLM that seamlessly integrates visual understanding and generation within single LLM, which is efficiently trained with the aid of semantic vision tokenizer and three-stage procedure. To promote synergistic enhancement between understanding and generation capabilities, we introduce novel self-enhancing multimodal alignment scheme that trains MLLMs to self-assess the consistency between text descriptions and self-generated images. ILLUME excels among existing unified MLLMs and exhibits competitive performance compared to specialized models across diverse range of benchmarks in multimodal understanding, generation, and editing. 2. Related Work Multimodal Understanding. The significant advancements in LLMs have spurred the development of Large Vision-Language Models (LVLMs). To bridge the gap between vision and text modalities, early approaches such as LLaVA [29] and MiniGPT-4 [61] utilize vision adapters to align vision features from vision encoders to the input space of LLMs. Further improvements have been observed in models such as the LLaVA series [27, 28], Qwen series [1, 48], and InternVL series [8, 9], which are achieved through the use of higher-quality datasets, increased input image resolution, and enhanced training strategies. Despite their strong understanding capabilities, these models support only visual perception and comprehension tasks. Visual Generation. Diffusion-based methods [37, 38, 40, 41] have shown exceptional capabilities and have become dominant in the image generation domain in recent years. These models operate by predicting Gaussian noise in forward process, and then generating high-quality images through an inverse denoising process. Another line of research [11, 39, 56] converts images into discrete tokens using VQGAN-like vision tokenizers [12, 20], and generates images by predicting the next token in sequence autoregressively. In this paper, we extend the capabilities of MLLMs to image generation tasks using unified autoregressive form, and further adopt diffusion model to reconstruct high-quality images from the predicted tokens. 3 Figure 4. Overview of the three-stage training procedure and its corresponding data composition of different stages in MLLM training. Unified Visual Understanding and Generation. An increasing number of studies are making efforts to unify visual understanding and generation tasks with LLMs. Pioneering works such as Emu [45], Emu2 [46], and XVILA [55] develop unified autoregressive model that predicts the next multimodal element, either by regressing visual embeddings or classifying text tokens. Yet, the nonunified design of optimization goals for different modalities may limit the feature integration across modalities, while the joint training with an extra diffusion decoder further complicates the infrastructure design with overall lower efficiency. On the other hand, methods like LWM [30], AnyGPT [59], Show-o [54], and VILA-U [52] utilize VQ tokenizer to transform images into vision tokens, enabling LLMs to be optimized via unified loss to predict the next token in both text and vision contexts. In this work, we explore methods for both the efficient text-image alignment in MLLMs and the synergy of discriminative and generation capabilities, which are under exploration in previous works. 3. ILLUME This section presents our proposed framework ILLUME, unified model for visual understanding and generation. More specifically, details of the design of vision tokenizer, MLLM, and training procedures are discussed. 3.1. Vision Tokenizer To process input images in LLMs, previous VLMs such as LLaVA [29], have demonstrated efficient text-image alignment by utilizing vision adapter to map semantic features from vision encoder to text space, utilizing only dataset of 558K samples for pretraining. However, in the domain of image generation, most existing autoregressive-based unified models [47, 59] are struggling with extensive training data required for LLM pretraining, as detailed in Table 1. We hypothesize that this issue stems from the inadequate semantic information provided by current vision tokenizers, such as VQGAN [12], which are not optimally suited for LLMs. These tokenizers are trained primarily on image reconstruction loss, with visual representation focusing on low-level textures for quantization, which in turn hampers text-image alignment in MLLMs. To this end, we resort to quantizing images into discrete tokens within semantic feature space. Specifically, as illustrated in Figure 3(b), we utilize UNIT [62], pretrained vision encoder, to extract semantic features and supervise the quantization process along with codebook learning through feature reconstruction loss. This approach significantly accelerates the image-text alignment process in comparison with those tokenizers with image reconstruction loss, as demonstrated by the observations in Figure 6. Moreover, since quantization occurs within semantic feature space, we further utilize the Stable Diffusion (SD) model [38] to reconstruct these semantic features back into images with high compression ratio of 32. The robust SD model effectively compensates for the low-level details that are not preserved during the quantization process. This allows for the generation of higher-resolution images from fixed number of discrete tokens output by the MLLMs. 3.2. MLLM Architecture. As shown in Figure 3, ILLUME inherits the architecture of existing Visual Language Models (VLMs) [27, 28] by extending LLMs with an additional vision vocabulary to generate discrete vision tokens. For visual understanding, we utilize the UNIT encoder [62], which is utilized in our proposed vision tokenizer, to extract semantic features from input images. These features are further aligned to the input space of the LLM via vision adaptor. This design mitigates the information loss caused by vector quantization, which is vital for tackling fine-grained multimodal understanding tasks. For visual generation, we use our vision tokenizer to convert images into discrete indices, and supervise the token prediction at each location for both modalities with shared prediction head in LLMs. With this architecture, ILLUME adopts the general Language Modeling (LM) objective to directly maximize the likelihood of each multi-modal sequence in an auto-regressive manner: = (cid:88) i=1 log Pθ(yiyi), (1) 4 Figure 5. Procedure of self-enhancing multimodal alignment scheme, which contains three steps: corpus self-generation, assessment generation and SFT for multimodal alignment. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, enabling the model to more accurately interpret images and avoid potential mistakes in image generation. where yi represents the text or visual token, and θ denotes the parameters of the LLM. Notably, given our models capability to handle images in both the input and output, our proposed framework is compatible with interleaved imagetext data to support any-to-any multimodal tasks. Training Procedure. The training procedure and data composition of MLLM is illustrated in Figure 4. The training procedure consists of three stages as below. Stage-1: Visual Embedding Initialization. The primary goal of this stage is to initialize good visual representation for subsequent training steps. The vision adaptor is trained by leveraging image-to-text pairs from LLaVAPretrain [29] to transform the visual features from vision encoder into LLMs text space. Meanwhile, this stage also involves the learning of new learnable embeddings, where only the vision embedding and the vision part of final classifier head of the LLM are updated. We introduce image reconstruction task, i.e., supervising the LLM to generate the original images, to facilitate rapid initialization of the introduced integrated weights in LLMs. Stage-2: Unified Image-Text Alignment. This stage focus on image-text alignment to learn both the understanding and generation tasks on multimodal data. We unfreeze the LLM and vision adaptor, utilizing 15M training data for training, including text data, image caption data for both natural images and documents, image data for reconstruction, and text-to-image generation data. Stage-3: Supervised Fine-tuning. After pretraining, we train the whole model with task-specific data to handle various multimodal understanding and generation tasks. To receive high-resolution images for fine-grained multimodal understanding like OCR and document-oriented tasks, we employ image patchfy strategy following Tasks GenAI-Bench GenEval POPE MME-P MMBench SEED MMVet Gen. only Und. only Gen. and Und. 0.63 - 0.63 0.58 - 0.56 - 84.6 86. - 1339.0 1358.6 - 60.9 61.6 - 64.0 65.0 28.0 27.4 Table 2. Comparison between the specialist model and unified model. Joint training presents no significant negative impact on the two tasks, but it also does not obviously promote each other. LLaVA-NeXT [28]. This stage utilizes instruction tuning data following [6] for visual understanding, high-quality image-text pairs for text-to-image generation and various mixed-modal generation data. Inference. During inference, our model adopts the nexttoken prediction approach. For visual understanding, we follow the standard practice of sampling tokens sequentially from the predicted distribution. For image generation, we utilize classifier-free guidance (CFG) as used in prior works [26, 54]. 4. Self-Enhancing Multimodal Alignment The primary goal of our community in developing unified MLLM is twofold: first, it can be easily extended to various vision-language tasks; second, the complete unification of representation spaces facilitates more efficient learning process through better multimodal interaction and alignment. Therefore, once we build our ILLUME, our priority was to investigate whether jointly improving these capabilities could benefit from the commonalities across the knowledge required for each one. However, according to our experimental results as shown in Table 2, while there was no significant negative impact from joint training, the anticipated mutual enhancement between understanding and generation was not presented on existing benchmarks. This out5 Method LLM. POPE MMBench SEED MME-P MM-Vet MMMU AI2D VQA-text ChartQA DocVQA InfoVQA OCRBench General Doc InstructBLIP [10] Vicuna-7B Qwen-VL-Chat [1] Qwen-7B Vicuna-7B LLaVA-1.5 [27] ShareGPT4V [7] Vicuna-7B LLaVA-NeXT [28] Vicuna-7B Emu3-Chat [49] 8B from scratch - - 85.9 - 86.5 85. 6.8B from scratch 7B from scratch LLaMA-2-7B Phi-1.5B Unified-IO 2 [33] Chameleon [47] LWM [30] Show-o [54] VILA-U (256) [52] LLaMA-2-7B VILA-U (384) [52] LLaMA-2-7B Janus [51] 87.7 - 75.2 73.8 83.9 85.8 DeepSeek-LLM-1.3B 87.0 ILLUME (Ours) Vicuna-7B 88. Understanding Only 53.4 58.2 58.6 69.7 64.7 68.2 - 1487.5 1510.7 1567.4 - - 26.2 - 31.1 37.6 43.9 37.2 30.6 35.9 35.4 37.2 35.1 31.6 33.8 45.9 54.8 58 66.6 70. Unify Understanding and Generation 61.8 - - - 56.3 59 63.7 72.9 - - - 948.4 1336.2 1401.8 1338.0 1445.3 - 8.3 9.6 - 27.7 33.5 34. 37.0 - 22.4 - 25.1 - - 30.5 38.2 - - - - - - - 71.4 36.0 60.6 64.3 68.8 67.4 58. - - - - - - 69.4 75.1 50.1 61.5 58.2 60.4 64.9 64.7 - - 18.8 - 48.3 60.8 - 72.1 12.5 66.3 18.2 21.3 54.8 68. - - - - - - - 13.9 62.6 28.1 - 74.4 76.3 - - - - - - - - - 25.8 - 37.1 43.8 - - - - - - - 276 488 318 371 532 - - - - - - - 66.7 76.0 45.5 669 Table 3. Quantitative results on visual understanding benchmarks. Our performance is close to and even outperforms both understanding only and unified models. The performance with top-1 and top-2 value are denoted in bold and underline respectively. come underscores that while these capabilities can coexist without detrimental effects, their synergistic potential may require further exploration and more refined approaches. In this work, we introduce novel self-enhancing multimodal alignment scheme, as depicted in Figure 5, which employs self-assessment process as bridge to synergistically enhance the discriminative and generative capabilities. We posit that if an MLLM can learn to assess the quality of its self-generated images during training, it can benefit in two aspects: Generation Aids Discrimination: By analyzing selfgenerated negative samples, the MLLM learns to identify and understand its failures, thereby enhancing its ability to interpret images more accurately. This introspective process allows the model to pinpoint and address its weaknesses through self-assessment, leading to improved understanding and fewer misinterpretations. Discrimination Aids Generation: The MLLM could utilize its discriminative skills to assess whether its selfgenerated images align with texts, making necessary adjustments based on this analysis. This capability ensures that during inference, the model is more cautious and precise, avoiding potential mistakes in generating images. Inspired by the above motivation, we design selfenhancing multimodal alignment scheme, which comprises three steps: Step 1: Corpus self-generation. The model selfgenerates images from subset of text-to-image data within the training set. Step 2: Assessment generation. We assess the inconsistencies between image and text against specific criteria such as object accuracy, count, color, and spatial relations. During generation, not only the assessment score (i.e., good or bad), but also the corresponding analysis are included. To obtain high-quality data, we resort to GPT4-o for assessment data generation with the template in Figure 5(b). Step 3: SFT for multimodal alignment. we reformat the assessment data as depicted in Figure 5(c). Specifically, for instances identified as good generation cases, we structure the data to only undergo the first round for assessment. As for bad generation cases, we reconstruct the data to two rounds of conversations, where the first round for assessment and the second round for refinement. In total, 50K assessment data are created with this scheme and we incorporate it into the Stage-3 of our training process. 5. Experiments We evaluate the proposed ILLUME on various multimodal understanding and generation benchmarks, and conduct ablation studies to verify our design choices. 5.1. Implementation Details In our experiments, we utilize Vicuna-7B as the base language model. For the vision encoder used in understanding tasks, we select UNIT [62]. The input image resolution is set as 224 in Stage-1 and Stage-2, with 256 token per image for LLMs. In Stage-3, we employ the image patchfy strategy following [28] to support high resolution images as input for fine-grained understanding, with maximum slice number of 9 and the base resolution of 448. Each sliced image is downsampled to 256 tokens. For image generation, the vision tokenizer has codebook of size of 16384, where the generated image has the resolution of 512 512 with 256 discrete tokens. The training hyperparameters are illustrated in Table 5. The whole training process took 3 days on cluster of 32 nodes, each equipped with 8 Ascend NPUs. 6 Method Params. Type MJHQ30k GenAI-bench GenEval FID Basic Advanced Overall Single Obj Two Obj. Counting Colors Position Color Attri. SDv1.5 [40] PixArt-α [4] SDXL [38] Emu3-Gen [49] 0.9B 0.6B 2.6B 8B Diffusion Diffusion Diffusion Autoregressive Chameleon [47] LWM [30] Show-o [54] VILA-U(256) [52] VILA-U(384) [52] Janus [51] 7B Autoregressive Autoregressive 7B 1.5B Autoregressive Autoregressive 7B 7B Autoregressive 1.3B Autoregressive - 6.14 9.55 - - 17.77 15.18 12.81 7.69 10.10 ILLUME (Ours) 7B Autoregressive 7.76 Generation Only - - 0.83 - - - 0.63 - 0.43 0.48 0.55 0.54 0.97 0.98 0.98 0. Unify Understanding and Generation - 0.63 0.70 0.76 0.73 - 0.75 - 0.53 0.60 0.64 0.61 - 0.60 0.39 0.47 0.53 - - 0. 0.61 - 0.93 0.95 - - 0.97 0.99 0.38 0.50 0.74 0.71 - 0.41 0.52 - - 0.68 0. 0.35 0.44 0.39 0.34 - 0.46 0.49 - - 0.30 0.45 0.76 0.80 0.85 0.81 - 0.79 0.82 - - 0.84 0. 0.04 0.08 0.15 0.17 - 0.09 0.11 - - 0.46 0.39 0.06 0.07 0.23 0.21 - 0.15 0.28 - - 0.42 0. Table 4. Quantitative results on text-to-image generation benchmarks. ILLUME achieves comparable results with specialist models and unified MLLMs. The performance with top-1 and top-2 value are denoted in bold and underline respectively. Setting LR. Batch size Training Step Stage-1 Vision adapter 1.0 103 Vision Embed. & Head 2.0 104 256 5000 Stage-2 Vision adapter 5.0 105 LLM 5.0 105 1024 Stage-3 Vision encoder 2.0 106 LLM & Vision adapter 2.0 105 1024 8000 Table 5. Detailed hyperparameters of our ILLUME. LR denotes learning rate for training. Vision Embed. & Head refers to the vision embedding and LM head of vision part. Method Type Tasks Emu Edit DINO CLIP-I CLIP-T InstructPix2Pix [2] Diffusion MagicBrush [60] Diffusion Diffusion OmniGen [53] Diffusion Emu Edit [42] Edit only Edit only Edit only Edit only 0.762 0.834 0.776 0.838 0.804 0.836 0.819 0.859 PUMA [13] ILLUME (Ours) AR AR Edit only 0.785 0.846 Und, Gen, Edit 0.791 0.879 0.219 0.222 0.233 0.231 0.270 0.260 Table 6. Quantitative results on image editing benchmarks. The performance with top-1 and top-2 value are denoted in bold and underline respectively. 5.2. Evaluation Setup Multimodal Understanding. To evaluate the multimodal understanding capabilities, we conduct evaluation on two types of widely-used benchmarks: (1) General, including POPE [23], MMBench [32], SEED [21], MME-P [14], MM-Vet [57], MMMU [58] and AI2D [19]; (2) Documentoriented, [43], ChartQA [34], DocVQA [35], InfoVQA [36] and OCRBench [31]. including VQA-text to reflect the comprehensive generative abilities. Multimodal Image Editing. To assess the multimodal image editing capability of our method, we evaluate it on the Emu Edit [42] benchmark and report the CLIP-I, CLIPT, and DINO [3] scores. The CLIP-I and DINO scores measure the models ability to preserve elements from the source image, while the CLIP-T score measures the consistency between the output image and the target caption. 5.3. Comparison with State-of-the-arts Multimodal Understanding. We report the performances on various multimodal understanding benchmarks of ILLUME and previous state-of-the-art multimodal understanding-only models, including InstructBLIP [10], Qwen-VL-Chat [1], LLaVA-1.5 [27], ShareGPT4V [7], LLaVA-NeXT [28] and Emu3-Chat [49], and unified models, including Unified-IO 2 [33], Chameleon [47], LWM [30], Show-o [54], VILA-U [52], and Janus [51], in Table 3. As we can see, ILLUME wins the first or second places on 10 out of 12 benchmarks. Specifically, ILLUME achieves 25% and 14% improvements on the MMMU and SEED benchmarks against the previous best unified multimodal model, Janus. Compared with the Emu3 model, ILLUME achieves comparable performance on documentoriented benchmarks and better performance on almost all general benchmarks, indicating the superiority of ILLUME. Multimodal Image Generation. To evaluate the multimodal visual generation capability of ILLUME, we use the MJHQ-30K [22], GenAI-bench [25] and GenEval [16] benchmarks. For MJHQ-30K, we adopt the Frechet Inception Distance (FID [18]) metric on 30K generated images compared to 30K high-quality images, measuring the generation quality and diversity. GenAI-bench and GenEval are challenging text-to-image generation benchmarks designed Multimodal Image Generation. We benchmark the multimodal image generation capability of ILLUME on MJHQ30K, GenAI-bench and GenEval benchmarks in Table 4. We compare ILLUME against previous stateof-the-art multimodal generation-only models, including SDv1.5 [40], PixArt-α [4], SDXL [38] and Emu3-Gen [49], and unified models stated above. As we can see, ILLUME achieves 7.76 FID scores on the MJHQ30K bench7 Understanding POPE MME-P MMBench 86.4 86.1 1358.6 1446.7 61.7 63. SEED GQA MM-Vet MMMU 65.0 66.0 27.4 29.0 31.2 32.0 60.0 60.7 Generation Overall Single Obj Two Obj. Counting Colors Position Color Attri. 0.35 0. 0.22 0.24 0.34 0.33 0.98 0.99 0.8 0.84 0.56 0.59 0.69 0. baseline + assessment baseline + assessment Figure 6. Comparison of different tokenizers for MLLM training. We compare two types of tokenizers: 1) Reconstruction tokenizer: supervised by image reconstruction loss. 2) Semantic tokenizer: supervised by feature reconstruction loss. The results manifest that vision tokenizer with semantics significantly accelerates the convergence of MLLM pretraining. mark, which is better than previous high-performance unified models such as Show-o and Janus, indicating better generation quality and diversity of ILLUME. We also achieve comparable results on the GenAI-bench benchmark against baseline methods and achieves the best overall accuracy (0.61) on the GenEval benchmark, surpassing previous generation-only and unified models, demonstrating superior comprehensive generation ability of ILLUME. Multimodal Image Editing. We compare ILLUME with previous state-of-the-art multimodal image editing models including InstructPix2Pix [2], MagicBrush [60], OmniGen [53], Emu Edit [42] and PUMA [13] in Table 6. As we can see, in comparison with those baseline models only supporting image editing task, ILLUME achieves competitive results even though it is unified model, indicating the effectiveness of our framework. 5.4. Ablation Studies Design Choice of Vision Tokenizer. To investigate whether semantic information is pivotal factor in designing an effective vision tokenizer, we conduct comparative analysis of vision tokenizers under two types of supervision: 1) Reconstruction tokenizer: We use VQGAN as implemented in [20], which is supervised using image reconstruction loss. 2) Semantic tokenizer: The quantization process is supervised with the objective of reconstructing semantic features extracted by UNIT [62]. We trained the MLLM with these two different tokenizers on 20M text-toimage generation dataset under the same setting. As depicted in Figure 6 (Left), the training-loss curves manifest that the vision tokenizer with semantics significantly hastens the MLLM training convergence. For image reconstruction and detail compensation in our vision tokenizer, we employed diffusion model. To ensure fair comparison, we substituted the original decoder in VQGAN with diffusion model to reconstruct images at 512 512 resolution. As shown in Figure 6 (Right), with only 20M 8 Table 7. Ablation of self-enhancing multimodal alignment. generation data, the performance of the reconstruction tokenizer was unsatisfactory, whereas the semantic tokenizer achieved commendable performance. These findings confirm that semantic information is indeed pivotal factor for vision tokenizer suited to MLLMs. Effectiveness of Self-Enhancing Multimodal Alignment Scheme. In our study, we conducted an ablation analysis to validate the effectiveness of our approach. The baseline involves sampling subset of 1.3M data points during Stage-3 training for efficiency, whereas our method also integrates assessment data generated by our scheme. As shown in Table 7, despite incorporating only 50K additional data, we observe improvements in performance across both understanding and generation benchmarks. This enhancement underscores that teaching the MLLM to self-assess not only enables the model to interpret images more accurately but also helps prevent potential errors in image generation. We hope this finding will inspire further exploration into the synergistic and generalization potentials between discriminative and generative capabilities. 6. Conclusion In this paper, we introduce ILLUME, unified MLLM which is efficiently pretrained and further improved by novel self-enhancing multimodal alignment scheme, exhibiting competitive or even superior performance compared to existing unified MLLMs across various multimodal benchmarks. Looking ahead, we plan to further develop ILLUME in several key areas: 1) We aim to extend its capabilities to accommodate more modalities, such as video, audio and 3D data, for broader applicability across various fields. 2) We intend to design more versatile vision tokenizer that can support both images and videos. Moreover, our findings in this study suggest that incorporating semantic information into traditionally well-designed vision tokenizers holds great potential for making them more suitable for MLLMs. 3) We plan to further explore our self-enhancing strategy by incorporating more recognized criteria, such as aesthetic quality, allowing for better data utilization and generation that more closely align with human preferences. These future directions will significantly broaden the applicability and effectiveness of ILLUME, paving the way for unified, highly effective, and efficient any-task, any-modality MLLM."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 3, 6, 7 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 7, 8 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 7 [4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 7 [5] Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, et al. Gaining wisdom from setbacks: Aligning large language models via mistake analysis. arXiv preprint arXiv:2310.10477, 2023. 3 [6] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 6, 7 [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 3 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 3 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 6, 7 [11] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 3 [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3, 4 [13] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Puma: Empowering unified mllm Li, and Xihui Liu. arXiv preprint with multi-granular visual generation. arXiv:2410.13861, 2024. 7, [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 7 [15] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. 2 [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 7 [17] Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. 1 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. [20] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 3, 8 [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 7 [22] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 7 [23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 7 [24] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 1 [25] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2025. 7 [26] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 5 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 3, 4, 6, 7 [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 3, 4, 5, 6, 7 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 3, 4, [30] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 2, 4, 6, 7 [31] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 7 [32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 7 [33] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. 2, 6, 7 [34] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 7 [35] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 7 [36] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 3 [38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 3, 4, 7 [39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 3 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 7 [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [42] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 7, 8 [43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1 [45] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 1, 4 [46] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 1, 4 [47] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 4, 6, 7 [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 10 guided image editing. Advances in Neural Information Processing Systems, 36, 2024. 7, [61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 [62] Yi Zhu, Yanpeng Zhou, Chunwei Wang, Yang Cao, Jianhua Han, Lu Hou, and Hang Xu. Unit: Unifying image and text recognition in one vision encoder. arXiv preprint arXiv:2409.04095, 2024. 4, 6, 8 [49] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 6, 7 [50] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 1 [51] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2, 6, 7 [52] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 4, 6, [53] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 7, 8 [54] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 4, 5, 6, 7 [55] Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. 1, 4 [56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 3 [57] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 7 [58] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 7 [59] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 2, [60] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction11 Figure A. Comparison of different hyper-parameters in inference. Inference hyper-parameters. Figure presents comparison of different inference decoding hyperparameters for text-to-image generation. It can be observed that increasing temperature, top-k, and guidance scale all lead to improved visual details. Data examples of assessment data. Figure illustrates an example of assessment data for self-enhancing multimodal alignment scheme. This example showcases how the data identifies specific reasons for inconsistencies between self-generated images and text descriptions, which aids the model in interpreting images more accurately and helps prevent mistakes during image generation. A. Implementation Details of Vision Tokenizer We supervise the quantization process within semantic feature space, which is promising to facilitate the imagetext alignment in MLLM training. Given an image x, it is fed into UNIT encoder to extract semantic features = {x1, ..., xN }. The semantic features then pass into quantizer, which tokenizes to sequence of discrete tokens = {v1, ..., vN } by looking up learnable codebook = {c1, ...cK}, where is the codebook size. The discrete token vi is calculated by assigning xi to its closest neighbourhood code in according to the L2 norm: vi = arg min xi cj, vi [0, 1] (2) Based on the discrete tokens, we can obtain its quantized embeddings, which is then fed into decoder to obtain reconstructed semantic features rec = {xrec }. The quantization process is supervised by the feature reconstruction loss using cosine loss and smoothl1 loss: 1 , ..., xrec (cid:88) (smoothl1(xi, xrec = ) + (1 cosine(xi, xrec ))) i=1 (3) During training, the vision encoder is kept frozen and only the parameters of quantizer and decoder are updated. It is trained for 80K steps on 80M images with the batch size of 2048, epochs of 2 and learning rate of 5e-5. To further recover the original pixel space, the reconstructed semantic features are set as conditions and injected to each block of conditional diffusion model through cross-attention layers. The conditional U-Net is initialized from SDXL and finetuned 80K steps with the batch size of 128 and learning rate of 2e-5. Only the attention layer of UNet is updated for efficient training. Note that the whole tokenizer training only requires pure image data without corresponding text descriptions. B. More Results of ILLUME More qualitative results. Figure showcases additional qualitative results for comprehension tasks, demonstrating that our ILLUME model can adeptly handle various comprehension tasks and images with significant differences in aspect ratio. Figures and Figures provide further visualizations in text-to-image generation and mixed-modal generation tasks, respectively. In the future, we plan to enhance MLLMs to produce higher resolution images and to support wider range of mixed-modal generation tasks. Detailed performance results on GenAI-bench. We details per-category performance on GenAI-bench in Table A, where our ILLUME achieves competitive results with current autoregressive-based unified MLLMs. 12 Figure B. More qualitative results on understanding tasks. Regions that related to the QAs are marked with red ellipses. 13 Figure C. More qualitative results on text-to-image generation tasks."
        },
        {
            "title": "Method",
            "content": "Params."
        },
        {
            "title": "Advanced",
            "content": "SDXL LWM Show-o VILA-U(256) VILA-U(384) Diffusion 2.6B 7B Autoregressive 1.5B Autoregressive Autoregressive 7B Autoregressive 7B ILLUME (Ours) 7B"
        },
        {
            "title": "Autoregressive",
            "content": "0.84 0.63 0.72 0.78 0.75 0.75 0.84 0.62 0.72 0.78 0.76 0.79 0.82 0.65 0.70 0.77 0.75 0. 0.83 0.89 0.63 0.70 0.70 0.75 0.78 0.79 0.73 0.75 0.77 0.73 0.83 0.63 0.70 0.76 0.73 0.75 0.71 0.59 0.70 0.70 0.68 0. 0.73 0.58 0.62 0.71 0.67 0.68 0.69 0.54 0.71 0.74 0.71 0.67 0.50 0.49 0.51 0.53 0.51 0. 0.66 0.52 0.65 0.66 0.64 0.63 0.63 0.53 0.60 0.64 0.61 0.60 Table A. Detailed quantitative results on GenAI-bench. Figure D. More qualitative results on mixed-modal generation tasks. 15 Figure E. Data example of assessment data for self-enhancing multimodal alignment."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab"
    ]
}