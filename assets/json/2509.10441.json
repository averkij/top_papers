{
    "paper_title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
    "authors": [
        "Tao Han",
        "Wanghan Xu",
        "Junchao Gong",
        "Xiaoyu Yue",
        "Song Guo",
        "Luping Zhou",
        "Lei Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds."
        },
        {
            "title": "Start",
            "content": "InfGen: Resolution-Agnostic Paradigm for Scalable Image Synthesis Tao Han1,2, Wanghan Xu2, Song Guo1(cid:12), Junchao Gong2, Xiaoyu Yue2,3, Lei Bai2(cid:12) Luping Zhou3, 5 2 0 S 2 1 ] . [ 1 1 4 4 0 1 . 9 0 5 2 : r 1Hong Kong University of Science and Technology 2Shanghai Artificial Intelligence Laboratory 3The University of Sydney hantao10200@gmail.com, {xuwanghan, gongjunchao, yuexiaoyu}@pjlab.org.cn songguo@ust.hk, luping.zhou@sydney.edu.au, bailei@pjlab.org.cn Figure 1. The proposed InfGen creates highly photo-realistic and detail-rich images at various resolutions when it is applied on SDXL [24]. Best viewed zoomed in. For more image generation, please visit our demo website to experience it."
        },
        {
            "title": "Abstract",
            "content": "Arbitrary resolution image generation provides consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with compact generated latent using one-step generator. Thus, we present the InfGen, replacing the VAE decoder with the new generator, for generating images at any resolution from fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary highresolution era while cutting 4K image generation time to under 10 seconds1. 1. Introduction In recent years, image generation has been significantly advanced by diffusion models [1, 23, 24, 29] and autoregres- (cid:12) Corresponding Authors. 1https://github.com/taohan10200/InfGen 64 64) latent feature maps to high-resolution images (e.g. 4K) remains challenging task. To this end, we propose generating high-resolution images by conditioning on generated latent maps, resulting in simple yet effective framework, InfGen. Since the image content has already been obtained by the diffusion model, we can employ lightweight generator to capture fine-grained textures and details. InfGen is decoder-based generator, which embeds vision transformer decoder to transfer generated latent into any resolution through crossattention operation conditioned on the latent. Notably, although VAE employs perceptual loss and GAN loss during training, it still lacks generative capability like InfGen. The reason is that the reconstruction target in the VAE is in the same pixel space as the input, meaning the required information for restoring the target is complete. However, the size of the reconstructed target in InfGen is greater or much bigger than the input image; thus, it is required to have generative ability to complete information loss. InfGen is trained on widely used VAE model, which is adopted by Stable Diffusion [24, 29], DiT [23], and SiT [20]. Therefore, our InfGen acts as program patch, upgrading these methods to high-resolution image generation with low computational cost. Fig. 1 shows images generated by off-the-shelf generative models, which show InfGen can improve the resolution of SD-1.5 [29] into arbitrary resolution diffusion model while keeping photo-realistic generation ability. Notably, our proposed paradigm for generating arbitrary image resolutions offers significant advantages in inference latency, improving the generation speed for images larger than 4K by 10 compared to the previously fastest UltraPixel [27] as shown in Fig. 2. In summary, our main contributions are: New Paradigm. Introducing novel paradigm for generating images at arbitrary resolutions. The secondary generation based on generated latent is an unexplored area. Plug and Play. The generator can serve as plugin to upgrade all models based on VAE without further training, showing significant improvements in empowering the existing generative models for arbitrary resolutions. High Quality and Fast. Compared to existing state-ofthe-art methods, the proposed approach not only achieves top-tier generation quality but also improves generation speed by over ten times. 2. Related Work 2.1. Latent diffusion image generation. Diffusion models [3, 10, 22, 33, 34] are type of generative model commonly used for producing high-quality images. They function by establishing Markov diffusion chain, which gradually adds Gaussian noise during the diffusion process and learns to remove this noise using neural Figure 2. Inference time (seconds per image) for high-resolution image generation methods. The vertical axis is logarithmic scale. sive models [1, 35]. Due to the high training costs and slow multi-step inference, most generative methods follow twostage paradigm: First, the generative model produces an intermediate representation of the image, and then the decoder from pre-trained tokenizer maps the intermediate representation to pixels. The first stage typically employs generative models, such as LDMs [29] and DiT [23], to generate image representations, while the second stage uses autoencoder models such as VAE [15, 28] and VQGAN [40] to decode the representations. The research community has long focused on the generative models of the first stage, neglecting the second stage of mapping latents to pixels. However, modern generative models have their own challenges. For tasks such as arbitrary high-resolution image generation, diffusion models typically require thousands of sampling steps, leading to prohibitively high computational costs. For instance, generating single 4K image with diffusion model can take over 100 seconds [8, 11, 39]. In contrast, the decoder in the second stage typically requires only single forward pass. Therefore, enhancing the capabilities of the decoder to achieve efficient high-resolution image generation is more effective approach. In this work, we shift our attention to the mapping from intermediate representations to image pixel values. tokenizer is essentially an autoencoder, where the encoder compresses the input image into more compact latent space to facilitate the design of the generator, while the decoder performs the inverse operation, reconstructing images from generated samples in the latent space. It is evident that by mapping latent vectors to higher-resolution images during the decoding process, the arbitrary high-resolution image generation task can be simplified into the superresolution task, which has already been well addressed. However, reconstructing low-resolution (e.g. 32 32 or networks. In the generation phase, the model samples random Gaussian noise and iteratively generates clear image through denoising process. The latent diffusion models [19, 24, 25, 29, 30, 44] utilizes an autoencoder, such as VAE [15, 28, 38] or VQVAE [26, 36], to encode images from pixel space to latent space, conducting diffusion training and inference within this latent space. Finally, the corresponding decoder restores the latents to pixel space, generating the final image. This approach leverages the encoders compression, enabling efficient training and inference. 2.2. High-resolution image generation. High-resolution image generation has gained popularity in recent years. common approach is to first generate lowresolution images and then use super-resolution methods [4, 17, 37, 41] to upscale them. However, superresolution in pixel space is labor-intensive and underutilizes the decoders generation capacity. Some methods modify the diffusion inference process to achieve superresolution. For instance, ScaleCrafter [8] adjusts image resolution during inference by dilating convolutions, while FouriScale [11] employs dilated convolution in the frequency domain to enhance resolution. Additionally, another method [13] modifies attention entropy in the denoising networks attention layer based on feature resolutions. However, these training-free methods are often closely tied to specific network architectures and inference processes, limiting their applicability to particular generative models and reducing their generalizability to others. In addition to training-free methods, some approaches achieve high-resolution generation by redesigning network structures. For example, Inf-DiT [39] employs unidirectional block attention mechanism to adaptively manage memory overhead during inference. UltraPixel [27] incorporates Guidance Fusion and Scale-Aware Norm between the blocks of the diffusion model, enabling high-resolution image generation through fine-tuning. 3. Method ϵ-prediction Overview. Diffusion models using the paradigm typically operate in latent space. Generating arbitrary-resolution images involves handling latents of varying shapes, resulting in high computational costs and latency for ultra-high-resolution generation. This paper proposes faster approach for arbitrary-resolution generation with diffusion models. In Section 3.1, we redefine arbitrary resolution generation as two-stage task. Without modifying the diffusion model, we replace the VAE decoder with generator capable of decoding at arbitrary resolutions. Section 3.2 details the design and training of such generator. Since training high-resolution decoders is computationally expensive, Section 3.3 introduces an iterative extrapolation scheme for ultra-high-resolution generation, offering training-free resolution enhancement method. 3.1. Preliminary Background Variational Auto-Encoders. VAEs [15] are advanced generative models that combine deep learning with probabilistic frameworks to learn efficient latent representations of data. These models are commonly employed as image tokenizers, offering structured latent space that serves as the foundation for latent diffusion models [20, 24, 29]. Let R3HW represent an RGB image, where and are height and width. typical VAE consists of two main components: the encoder and decoder, which work together seamlessly to process and reconstruct data. The encoder transforms data into latent space that obeys Gaussian distribution. This mapping is crucial as it compresses the input into lower-dimensional space, qϕ(zx) = (z; µϕ(x), σ ϕ(x)I), (1) where µϕ(x) and σ2 ϕ(x) are neural networks parameterized by ϕ. This ensures essential features are captured, facilitating effective reconstruction. Once the data is encoded, the decoder takes over to reconstruct the input data from the latent space. It models the distribution of the data given the latent variables, enabling the generation of new samples, pθ(xz) = (x; µθ(z), σ2I), (2) where µθ(z) is neural network parameterized by θ. Implicit Neural Representation. INRs [32] are usually adopted in the Coordinate-Based Networks modeling continuous signals. For images, the network fits continuous function from pixel coordinates (u, v) to RGB color, fθ(u, v) = NN((u, v); θ), (3) where θ represents the network parameters. This formulation allows the network to map discrete pixels to continuous space, capturing intricate details in generalized way. 3.2. InfGen: Fixed Latent in, Arbitrary Image Out InfGen is secondary generation model, in which the diffusion model is fundamentally viewed as content generator, primarily generating fixed-compact content latent z. Furthermore, InfGen extends into images of arbitrary sizes, focusing on refining details and textures during this process. There are two notable benefits for this paradigm: 1) high inference speed: InfGen achieves very fast inference speeds by avoiding multi-step denoising on high-resolution latent spaces. This efficiency is due to the generating of arbitraryresolution images on compact latent with one-step inference, which reduces computational demands. 2) Plug Figure 3. Illustration of the training and inference processes. The generator is trained in the latent space to reconstruct images at any resolution and aspect ratio. During inference, it can be applied to improve various diffusion models, enabling them to generate images of arbitrary resolution across various tasks. and play: InfGen offers strong generalization as it can be applied to any diffusion model trained on the same latent space. This flexibility makes it adaptable to various generative models and tasks without extra training. 3.2.1. Overall Introduction of InfGen To enable InfGen to integrate seamlessly with various diffusion-based generative models and enhance their resolution flexibility, we propose pipeline grounded in the latent space of the VAE. As depicted in Fig. 3, this approach leverages the vanilla VAE architecture introduced in Section 3.1. Notably, our InfGen framework refrains from altering the VAE encoder, thereby preserving compatibility with existing diffusion models (e.g., SDXL [24], DiT [23]). Instead, we incorporate secondary generator within the decoder. Training pipeline. During the training phase, highresolution images are initially subjected to cropping and resizing operations to conform to fixed dimension, such as 512 512. These preprocessed images are subsequently encoded by the VAE encoder into fixed-size latent z, typically reduced by 8, resulting in dimensions like 4 64 64. The InfGen model is designed to achieve arbitrary resolution generation. The mapping is defined as: : Inf Gen(z, (h, w)) x(h,w), (4) where h, is dynamic parameter representing the expected image size, and x(h,w) is the desired output image. Optimization objectives. The loss functions include adversarial loss, which is defined as: loss, reconstruction loss, and perceptual LAE = ℓ1(x, ˆx) + λP LP (x, ˆx) + λGLG(ˆx), (5) where ℓ1 represents L1 loss between the reconstruction and the ground truth, LP is the perceptual loss measured by LPIPS [43], and LG is the adversarial loss derived from PatchGAN discriminator [12]. Inference pipeline. As illustrated in Fig. 3, during the inference phase, by replacing the input latent vector with one generated by compatible diffusion models including but not limited to DiT [23], SDXL [24], SiT [20], FiT [19], InfGen can generate images of any size for latent representation. Through this paradigm, InfGen can cost-effectively enhance the flexibility and resolution of diffusion models on different tasks, like class-guidance generation [20, 44], text-conditional generation [24, 29], and inpainting [2, 42]. 3.2.2. Arbitrary-Resolution Decoder Architecture As illustrated in Fig. 3 (a), we propose novel architecture that enables reconstruction at any resolution. Building upon the traditional VAE structure, we introduce transformerbased latent generator. Specifically, we treat the latent variables as prompts, utilizing them as keys and values. Guided by the latent z, we create mask token serving as the query, which is tailored to the desired image dimensions (h, w). For generating an image of size (h, w), the mask token is shaped as (h/8, w/8). This forms key-query-value triplet suitable for the Multi-Head SelfAttention (MHSA) mechanism in transformer blocks. Within the multi-layer transformer blocks, this triplet undergoes cross-attention computation. The mask token, acting as the query, continuously interacts with the latent keys to gather information through similarity calculations. Finally, we send the mask token to the decoder for upsampling, resulting in the arbitrary final image. 3.2.3. Implicit Neural Positional Embedding Positional encoding is crucial for matching the spatial information between the mask and latent tokens. The image is split into fixed-size patches [14], treating the position embedding for each patch as learnable token [5]. Positional encoding helps retain spatial information. However, this typically restricts input image size, as these encodings are designed for fixed number of tokens. To overcome this limitation, we propose an Implicit Neural Positional Embedding (INPE) method to generate when the number of mask tokens is dynamic, which allows interactions between fixed-size latent representations and differently-sized mask tokens in cross-attention, enabling the regeneration of latent tokens with varying dimensions. Standardization and Conversion. First, the coordinates of each mask token (xm, ym) and latent token (xl, yl) are standardized to map the different sizes to unified scale: (ˆxm, ˆym) = ( xm , ym ), (ˆxl, ˆyl) = ( xl , yl ), (6) where Wi and Hi denote the width and height of the latent token or the mask token. The standardized 2D coordinates are then converted to 3D Cartesian coordinates on the unit sphere: = cos(π ˆyi) cos(2πˆxi), = cos(π ˆyi) sin(2πˆxi), = sin(π ˆyi). This mapping leverages spherical geometry to capture complex spatial relationships, enhancing continuity for smooth feature learning, reducing coordinate bias with symmetric structure. Fourier Transformation and Neural Mapping. The 3D coordinates are transformed into high-frequency Fourier features to enhance the models pattern-capturing ability: γ(x, y, z) = [cos(B[x, y, z]T ), sin(B[x, y, z]T )] (7) where diagonal matrix is randomly sampled from Gaussian distribution (µ, σ). It is used to map coordinates into higher-dimensional space. These Fourier features are then fed into an implicit neural network to generate dynamic positional encodings through Equation 3. Alignment and optimization. The generated positional encodings align with latent tokens and mask tokens to enhance information interaction in cross-attention. Dynamic encodings are integrated into and to improve alignment and attention effectiveness. Finally, parameters θ are synergistically optimized by minimizing the loss function introduced in Eq. (5). 3.3. Training-free Resolution Extrapolation To produce images beyond the training resolution, we introduce an iterative generation method, which is training-free extrapolation method to scale the generated latent into arbitrary ultra-high resolution (e.g. 4K) continuously."
        },
        {
            "title": "Reliable\nextrapolation",
            "content": "Max. sw ) (sh 32 32 256 512 256 1024 64 512 1024 512 2048 512 2048 512 4096 16 16 64 Table 1. Recommended extrapolation resolution. Guaranteed base resolution. As shown in Tab. 1, InfGen receives very-low resolution latent representation of size 64 64 and is capable of generating from the latent space to an image resolution R, where = sh512sw512. An initial latent L0 of size 64 64 is used to generate an image I1 with resolution up to 2048 2048: I1 = InfGen(L0, ks 0) = InfGen(L0, (sh 0 , sw 0 )), where sh and sw are scaling factors for height and width within the range 1 sh, sw 2. Iterative extrapolated resolution. The generated image is then encoded back into latent representation for further generation: Ln = Encoder(In1), In = InfGen(Ln, ks n), (8) where each iteration involves scaling factor ks n. It is recommended to iterate within the scale range shown in Tab. 1. Achieving arbitrary resolutions. By repeating this process, the model achieves final resolution Rf , Rf = 512 (cid:89) i=1 sh 512 (cid:89) i=1 sw . The results in Tab. 3 verify this is robust extrapolation method for flexible resolution enhancement, ensuring highquality image generation. 4. Experiment Dataset. High-resolution training dataset is required to enhance the decoders ability to express image details and textures during reconstruction. We selected 10 million images with resolutions exceeding 10242 from LAIONAesthetic [31] as our training set. Further filtering resulted in subset of 5 million images with resolutions over 20482, dividing the high-resolution training data into two parts. Since the input image resolution varies dynamically during training, we divide the training into two stages and change the batch size to avoid out-of-memory errors. Implementation details. During training, we keep the encoder of the pre-trained VAE frozen. For different batches, images are randomly cropped into different sizes. For inputs, they are then resized to fixed size, such as 512 512, to obtain fixed-size latent. For targets, they keep the cropped shape, except for performing another cropping to support decoding with arbitrary ratios. Due to the expensive computational and memory demands of highresolution image reconstruction, the first training stage uses resolutions from 5122 to 10242 with batch size of 32. In the second stage, the resolution ranges from 5122 to 20482, with the batch size reduced to 8. The training iterations are 500k and 100k respectively, lasting 15 days with 8 A100 GPUs. The AdamW optimizer [18] is used with an initial learning rate of 2e 4, gradually decreasing to 1e 5 using cosine decay. The λP and λG in Eq. (5) are both set to 0.1. Metrics. For generation, we mainly focus on FID [9], sFID [21], precision, and recall [16]. Since FID requires downsampling to resolution of 229229 for testing, downsampling can degrade the details of high-resolution images, making it unsuitable for evaluating the performance of generating high-resolution images. Therefore, we adopt the approach proposed in UltraPixel [27] to crop highresolution images into different 229229 patches for testing, denoted as FIDp, sFIDp, Pre.p and Rec.p. We also follow the previous tokenizer [24] to report PSNR and SSIM as the metrics of reconstruction quality. 4.1. Comparison with Alternative Image Tokenizers We evaluate our tokenizer against the discrete image tokenizers, like VQGAN [6], and continuous image tokenizers, such as SD VAE [29] and SDXL VAE [24]. As detailed in Tab. 2, our tokenizer achieves competitive reconstruction performance compared with the commonly employed VAEs, even though InfGen is trained on more complex task. Additionally, we tested it on the LAION dataset on different input&output resolutions to assess image reconstruction quality. LAION images present more complex scenes, and our results align with those from the ImageNet"
        },
        {
            "title": "Method",
            "content": "VQGAN VQGAN SD-VAE SDXL-VAE InfGen(Ours) InfGen(Ours) SD-VAE InfGen(Ours) Input&Output Resolution 2562 2562 2562 2562 2562 2562 2562 2562 2562 2562 5122 5122 2562 5122 2562 5122 InfGen(Ours) InfGen(Ours) SD-VAE InfGen(Ours) InfGen(Ours) 2562 2562 5122 5122 2562 5122 2562 5122 5122 10242 ImageNet-50k rFID PSNR SSIM 4.99 1.19 0.74 0.68 1.07 0. 1.43 1.15 3.30 0.58 3.49 2.27 1.16 20.00 23.38 25.68 26.04 24.61 27.92 24.14 22.86 LAION-50k 26.14 28.85 25.17 23.40 27. 0.629 0.762 0.820 0.834 0.798 0.867 0.759 0.728 0.870 0.910 0.836 0.809 0.872 Table 2. Comparisons with other image tokenizers. The evaluations are conducted on 256 256 ImageNet 50k validation set and LAION-5B 50k set. All models are trained on part of the LAION5B dataset except * is trained on ImageNet. validation set, demonstrating that our tokenizer effectively handles both object-centric and scene-centric images. 4.2. Improving Performance for Diffusion Models InfGen model is expected to enhance resolution for generative models. After training, InfGen can be used as plugin to replace the VAE decoder of existing latent spacebased generative models, especially those sharing the same VAE encoder as ours. To evaluate, we tested several classic and newly published generative models, including DiTXL/2 [23], SiT-XL/2 [20], MaskDiT [44], MDTv2 [20], and FiTv2 [19]. By replacing these models VAE decoders with InfGen, the generated latent can be decoded into outputs of any resolution. The results in Tab. 3 detail the performance improvements InfGen brings to each generative model for resolution upgrades. For the original models, which cannot generate images at arbitrary resolutions, we upsample their outputs to the evaluation size. We assessed two different latent space sizes: 32 32 and 64 64. Quantitative comparison. For the 32x32 latent space, we evaluated performance when generating images at 512 512 (4 upscaling) and 10241024 (16 upscaling). For the 64x64 latent space, we assessed performance improvements at 10241024, 20482048, and 30723072 resolutions. The experimental results show that InfGen significantly enhances the generative performance of all models at high resolutions. For instance, the FID of 3072 3072, InfGen achieved up to 41% improvement on DiT [23] and 44% improvement on SD1.5 [29]. Across all five evaluated Method latent: 32 32 DiT-XL/2 [23] InfGen+DiT-XL/2 SiT-XL/2 [20] InfGen+SiT-XL/ MDTv2 [7] InfGen+MDTv2 latent: 64 64 DiT-XL/2 [23] InfGen+DiT-XL/2 FiTv2-XL/2i [19] InfGen+ FiTv2-XL/2 SD1.5 [29] InfGen+ SD1.5 latent: 64 64 DiT-XL/2 [23] InfGen+DiT FiTv2-XL/2 [19] InfGen+ FiTv2-XL/ FIDp sFIDp Pre.p Rec.p FIDp sFIDp Pre.p Rec.p image: 512 512 image: 1024 1024 44.17 39.81 (9.9%) 40.89 38.83(5.0%) 42.36 38.46(9.1%) 43.17 39.06 (9.5%) 42.04 38.77 (7.8%) 25.99 24.25 30.38 27.59 0.44 0. 0.35 0.38 28.11 25.43 image: 1024 1024 0.38 0.42 32.27 23.13 31.12 22.17 0.50 0. 0.49 0.51 0.26 0.30 0.30 0.33 0.29 0.35 0.40 0.43 0.41 0. 21.58 16.92 (21%) 33.91 24.52 0.45 0.49 extrapolation image: 3072 3072 0.44 0.49 77.84 45.94 (41%) 79.30 45.72 (42%) 47.57 32.52 48.47 29.45 0.44 0.65 0.43 0.62 0.20 0.34 0.19 0. 61.52 41.75 (32%) 67.98 46.50(36%) 60.63 40.53(33%) 64.87 56.21 (13.4%) 66.95 61.56 (8.1%) 55.30 41.12 (26%) 48.11 26.64 54.23 30.37 0.27 0.51 0.23 0.48 51.62 28.98 0.25 0.48 image: 2048 59.18 40.24 59.82 38.61 0.45 0.56 0.45 0.56 74.58 45.18 0.36 0.51 image: 3072 SD1.5 [29] InfGen+ SD1.5 0.22 0.40 0.22 0.39 0.23 0.41 0.33 0.42 0.29 0. 0.38 0.49 73.13 40.75 (44.3%) 62.83 38.97 0.34 0.57 0.24 0.44 Table 3. Improved performance at arbitrary resolution for diffusion models. Our InfGen can improve the performance of existing latent-based diffusion models on all metrics across different resolutions, especially significant improvement in high-resolutions. Method Resolution FIDP sFIDP Pre.P 1024 1024 Rec.P Latency (s) FIDP sFIDP Pre.P 2048 2048 Rec.P Latency (s) ScaleCrafter [8] Inf-DiT [39] UltraPixel [27] InfGen+SD1.5 InfGen+SDXL-B55.36 48.48 48.37 44.85 35.14 90.34 106.45 85.74 98.16 79.25 0.45 0.47 0.58 0.50 0.67 0.41 0.56 0.42 0.52 0.66 7 50 11 2.9+0.4 5.4+0.4 144.61 142.05 127.26 139.14 96. 242.76 255.07 221.14 225.27 189.68 0.45 0.53 0.61 0.53 0.59 0.59 0.55 0.55 0.53 0.62 97 255 20 2.9+1.9 5.4+1.9 Table 4. Quantitative comparison with other methods. Our InfGen+SDXL-B-1 yields the best performance on different resolutions while achieving an extremely low latency. All latencies are tested on an A100 GPU device. Underline means the second-best. resolutions, the average improvements were 8%, 34%, 13%, 16%, and 42%, respectively. These results indicate that InfGen not only enables existing diffusion models to generate at any resolution but also significantly improves generation quality at different resolutions. Visualization comparison. Fig. 4 presents visual comparison between our InfGen model and the baseline across various resolutions. Using SD1.5 for high-resolution image synthesis results in visually unappealing structures and extensive irregular textures, which greatly reduce visual qualIn contrast, our method excels in producing superior ity. semantic coherence and detailed intricacies. For example, at resolution of 3072 3072, our generated images of panda, cat, and lion display more detailed features. Even with fixed-latent approach for generating images at different resolutions, our method consistently delivers visually pleasing and semantically coherent outcomes. 4.3. Comparison to other State-of-the-Art Methods Benchmark and evaluation. We also compare the textto-image generation performance with some existing arbitrary resolution generation methods. Here we select recent methods, including training-free methods like ScaleCraft [8] and training-based methods, UltraPixel [27] and Inf-DiT [39], for comparison. We comprehensively evaluate the performance of our model at resolutions of 10241024 and 20482048. For fair comparison, we implement all methods on same A100 GPU device and sample 1,000 images and 600 images for 10242 and 20482, respectively, where the reference images and captions are selected from the LAION dataset. Additionally, we include results for SDXL-Base-1.0 [24], which achieves best performance with an FIDP of 35.14 at 10241024 resolution and 94.61 at 2048 2048 resolution, demonstrating its robustness in generating high-resolution images. These results further validate the effectiveness of our approach when Figure 4. Visualizations of arbitrary image generation. The proposed InfGen improves the generation ability for LDMs [29] across various resolutions. More visual examples are provided in the appendix. integrated with diverse base models. Quantitative comparison. As indicated in Tab. 4, our method InfGen+SD1.5 performs competitively in terms of FID, sFID, Precision, and Recall in two high resolutions. Notably, InfGen has significant disadvantage in inference efficiency, producing 20482048 image in 5 seconds, 4 faster than the UltraPixel. Training-free method [8] and the super-resolution-based method [39] take several minutes to generate 2K image. These findings underscore the effectiveness of our method in generating ultrahigh-resolution images with exceptional efficiency. 5. Conclusion InfGen offers highly efficient framework for generating images at arbitrary resolutions, addressing the limitations of existing methods that focus on generating arbitrary resolution latent spaces in diffusion models, often resulting in significant delays and computational overhead. By training secondary generative model in compact latent space, InfGen decodes low-resolution latent into images of any resolution without altering the structure or training of existing diffusion models. Our experiments demonstrate that InfGen, as an off-the-shelf enhancement, can improve diffusion models for arbitrary resolutions. Compared to other specialized methods, InfGen achieves superior quality and significantly reduces inference time, generating 4K image only takes 7.4 seconds. This advancement highlights InfGens potential to significantly enhance fast ultra-highresolution image generation capabilities."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported partially by the Shanghai Artificial Intelligence Laboratory."
        },
        {
            "title": "We also acknowledge funding support",
            "content": "from Hong including the Hong Kong RGC Kong-based sources, 152228/23E, (152169/22E, General Research Fund the Research Impact Fund (R5060-19, 162161/24E), R5011-23), the Collaborative Research Fund (C104223GF), the NSFC/RGC Collaborative Research Scheme (62461160332 & CRS HKUST602/24), the Areas of Excellence Scheme (AoE/E-601/22-R), and the InnoHK initiative (HKGAI)."
        },
        {
            "title": "References",
            "content": "[1] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 1, 2 [2] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 43344343, 2024. 4 [3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [4] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. 3 [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 5 [6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 6 [7] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 7 [8] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 7, 8 [9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [11] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024. 2, 3 [12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 4 [13] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Trainingfree diffusion model adaptation for variable-sized text-toimage synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. [14] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: survey. ACM computing surveys (CSUR), 54(10s):141, 2022. 5 [15] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. 2, 3 [16] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6 [17] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 18331844, 2021. 3 [18] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [19] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. ICML, 2024. 3, 4, 6, 7 [20] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 3, 4, 6, 7 [21] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 6 [22] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 4, 6, 7 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and national conference on computer vision, pages 19051914, 2021. 3 [38] Wanghan Xu, Xiaoyu Yue, Zidong Wang, Yao Teng, Wenlong Zhang, Xihui Liu, Luping Zhou, Wanli Ouyang, and Lei Bai. Exploring representation-aligned latent space for better generation. arXiv preprint arXiv:2502.00359, 2025. 3 [39] Zhuoyi Yang, Heyang Jiang, Wenyi Hong, Jiayan Teng, and Jie Inf-dit: Upsampling any-resolution image with arXiv preprint Wendi Zheng, Yuxiao Dong, Ming Ding, Tang. memory-efficient diffusion transformer. arXiv:2405.04312, 2024. 2, 3, 7, 8 [40] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2 [41] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind In Proceedings of the IEEE/CVF image super-resolution. International Conference on Computer Vision, pages 4791 4800, 2021. 3 [42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 4 [43] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [44] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked In Transactions on Machine Learning Retransformers. search (TMLR), 2024. 3, 4, 6 Robin Rombach. Sdxl: els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 2, 3, 4, 6, 7 Improving latent diffusion modarXiv preprint [25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [26] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 3 [27] Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks. In NeurIPs, 2024. 2, 3, 6, 7 [28] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 12781286. PMLR, 2014. 2, 3 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 4, 6, 7, 8 [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [31] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [32] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020. 3 [33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [34] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [35] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 [36] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [37] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF inter-"
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Shanghai Artificial Intelligence Laboratory",
        "The University of Sydney"
    ]
}