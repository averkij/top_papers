{
    "paper_title": "Mechanistic Permutability: Match Features Across Layers",
    "authors": [
        "Nikita Balagansky",
        "Ian Maksimov",
        "Daniil Gavrilov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 2 6 5 6 7 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MECHANISTIC PERMUTABILITY: MATCH FEATURES ACROSS LAYERS Nikita Balagansky T-Bank Moscow Institute of Physics and Technologies n.n.balaganskiy@tbank.ru Daniil Gavrilov T-Bank Ian Maksimov HSE University T-Bank"
        },
        {
            "title": "ABSTRACT",
            "content": "Understanding how features evolve across layers in deep neural networks is fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, novel, data-free method for aligning SAE features across different layers of neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides new tool for mechanistic interpretability studies."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, foundation models have become pivotal in natural language processing research (Llama Team, 2024; Team et al., 2024). As these models are applied to growing range of tasks, the need to interpret their predictions in human-understandable terms has intensified. However, this interpretability is challenged by the presence of polysemantic featuresfeatures that correspond to multiple, often unrelated conceptsespecially prevalent in large language models (LLMs). significant advancement in addressing polysemanticity is the superposition hypothesis, which posits that models represent more features than their hidden layers can uniquely encode. This leads to features being entangled in non-orthogonal basis within the hidden space (Bricken et al., 2023; Arora et al., 2018; Elhage et al., 2022), complicating interpretation because individual neurons or features do not correspond neatly to singular, human-understandable concepts. To unravel this complexity, Sparse Autoencoders (SAEs) trained on the hidden states of LLMs were employed, using feature sizes significantly larger than the models hidden dimensions (Yun et al., 2021; Bricken et al., 2023). SAEs aim to extract monosemantic featuressparse activations that occur only when processing specific, interpretable functions. For instance, Templeton et al. (2024) used this approach to interpret the Claude Sonnet 3 model, while Lieberum et al. (2024) open-sourced an SAE for the Gemma 2 model, enabling researchers to delve deeper into LLM interpretability. While SAEs have advanced the interpretability of individual model layers, crucial question remains: How do these interpretable features evolve throughout the layers of model during evaluation? Understanding this evolution is essential for comprehensive interpretation of the models internal dynamics and decision-making processes."
        },
        {
            "title": "Preprint",
            "content": "In this paper, we introduce SAE Match, data-free method for aligning SAE features across different layers of neural network. Our approach enables the analysis of feature evolution throughout the model, providing deeper insights into the internal representations and transformations that occur as data propagate through the network. By addressing the challenge of feature alignment across layers, we contribute to more comprehensive understanding of neural network behavior and advance the field of mechanistic interpretability. Our main contributions are as follows: We propose SAE Match, novel method for aligning Sparse Autoencoder features across layers without the need for input data, enabling the study of feature dynamics throughout the network. We introduce parameter folding, technique that incorporates activation thresholds into the encoder and decoder weights, improving feature matching by accounting for differences in feature scales. We validate our method through extensive experiments on the Gemma 2 language model, demonstrating improved feature matching quality and providing insights into feature persistence and transformation across layers. By advancing methods for feature alignment and interpretability, our work contributes to the broader goal of making neural networks more transparent and understandable, facilitating their responsible and effective deployment in various applications."
        },
        {
            "title": "2 BACKGROUND",
            "content": "The goal of the mechanistic interpretability field is to develop tools for understanding the behavior of neural networks. The main challenge with naïve model interpretation approaches stems from the polysemanticity of features in trained models, which states that each feature is linked to combination of unrelated inputs (Bricken et al., 2023). One explanation for polysemanticity is superposition hypothesis, which suggests that the number of features trained by model exceeds the number of neurons in that model (i.e., the size of its hidden layer) (Arora et al., 2018; Elhage et al., 2022). In this scenario, the features might be represented using non-orthogonal basis in the hidden space, making them challenging to interpret with simple methods. Assuming the superposition hypothesis is correct, Bricken et al. (2023) used Sparse Autoencoders (SAEs) (Yun et al., 2021) to uncover features in language models that are comprehensible to humans. Given the assumption that models capture more features than their hidden dimensionality allows, unveiling these features can be achieved by training an autoencoder on the hidden states of model with large representation size (Templeton et al. (2024) used representation sizes orders of magnitude larger than models hidden size to interpret the Claude 3 Sonnet model): (x) = σ (Wencx + benc) , ˆx(f ) = Wdecf + bdec, (x) RF , Rd, d. (1) Here, σ is an activation function (e.g., ReLU). Importantly, (x) is made to be sparse to help the SAE capture monosemantic features. The final loss is defined as L(x) = ˆx(f (x))2 + λf (x)0. The sparsity of the model can be increased by adjusting the λ hyperparameter, which balances the trade-off between the reconstruction term and the regularization term. However, determining the optimal value for λ is not straightforward. Building on this approach, Lieberum et al. (2024) introduced family of SAE models trained on the hidden states of the Gemma 2 model for each layer. These models, along with detailed feature descriptions, are available on Neuronpedia1. Notably, this work departs from previous methods by employing JumpReLU (Rajamanoharan et al., 2024) as the activation function: 1https://www.neuronpedia.org/gemma-2-2b"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Left: Hidden state norms and the mean θ value in trained JumpReLU activations within SAE modules. Right: Dynamics of hidden state norm changes and the differences in norms of matched decoder columns Wdec after the folding operation. These results suggest that θ captures the growth of hidden state norms. After folding θ into the weights, the decoder weights dec become dependent on the dynamics of hidden state norms, leading to lower overall MSE during matching. For more details on the folding operation and method description, see Section 3. JumpReLU(z) = H(z θ), θ RF ; (2) where H() is the Heaviside step function, and θ represents learnable thresholds for each component of z."
        },
        {
            "title": "3 MATCHING SPARSE AUTOENCODER FEATURES",
            "content": "While SAEs can extract human-interpretable features from specific layers of language model, understanding how these features evolve across layers remains an open question. To address this, we introduce SAE Match, data-free method to align SAE features across different layers. This approach enables us to analyze the evolution of features throughout the models depth, providing deeper insights into the models internal representations. Our central idea is that features from different layers might be similar but permuted differentlythat is, the i-th feature in layer might correspond to the j-th feature in layer B. Therefore, aligning features across layers involves finding the correct permutation that matches semantically similar features. Hypothesis 1: Features (A) from layer can be matched with features (B) from layer by calculating the mean squared error (MSE) between the relevant SAE weights. These weights might be either the rows of the encoder weights (A) enc or the columns of the decoder weights (A) dec (or both). Rows or columns that have low MSE between them suggest semantic similarity between the features. dec and (B) enc and (B) This matching problem resembles the task of aligning weights from two neural networks that perform the same function but have permuted weights (Ainsworth et al., 2022). In our case, we aim to find permutation matrix (AB) PF (the set of permutation matrices) that aligns the features of layer with those of layer B. Formally, we seek permutation that minimizes the MSE between the decoder weights: (AB) = arg min PF (cid:88) i=1 (A) deci,: (B) deci,: 2 = arg max PF (cid:28) (cid:16) , (cid:17) (A) dec (B) dec (cid:29) , (3)"
        },
        {
            "title": "Preprint",
            "content": "and (B) deci,: Figure 2: schematic illustration of the differences in SAE matching with and without folded parameters. When no folding is performed (top), θ encapsulates differences in hidden state norms, causing features (A) and (B) to have different scales, while the columns of decoder weights (A) have similar norms. Matching similar columns leads to differences in the actual deci,: reconstructions of the input ˆx, which we hypothesize is detrimental for matching SAE features. With θ folding (bottom), we transfer the differences in input (and thus feature) norms to the decoder weights (A) , thereby matching features while accounting for differences in input deci,: norms. As result, reconstructions of matched features are closer to each other than in the unfolded variant of the algorithm. See Section 3.1 for more details. and (B) deci,: where A, BF = (cid:80) i,j Ai,jBi,j is the Frobenius inner product. To solve this, one can utilize Linear Assignment Problem (LAP) solver (Ainsworth et al., 2022). The same logic applies to the encoder weights, provided they are transposed. 3.1 FOLDING SPARSE AUTOENCODER WEIGHTS While the above method can be directly applied, it does not utilize the information stored in the learnable thresholds θ of the JumpReLU activation function. Moreover, differences in feature scales can lead to suboptimal matching when minimizing MSE between vectors with varying norms. To address this, we propose technique called parameter folding, where we incorporate the activation thresholds into the encoder and decoder weights: enc = Wenc diag (cid:19) (cid:18) 1 θ , enc = benc 1 θ , dec = Wdec diag(θ), θ = 1, (4) where diag(θ) creates diagonal matrix from θ, and denotes element-wise multiplication. This transformation does not alter the SAEs output but adjusts the weights to account for differences in feature scales. Our hypothesis is that parameter folding helps align decoder weights in way that reflects the actual dynamics of hidden state norms during model evaluation. Specifically, we observed that the learned thresholds θ capture the growth of hidden state norms across layers (see Figure 1). We observed that θ values, in fact, encapsulate the growth of hidden state norms. By folding θ into the weights, we normalize the decoder weights to align with these dynamics. Hypothesis 2: Folding the activation thresholds into the weights aligns decoder columns with the dynamics of hidden state norms. This process allows us to match decoder weights based on their MSE in the space of actual hidden state norms. By folding parameters, we can match features across layers in data-free manner, without needing input data to capture input scales. For schematic illustration of hypothetical behavior, refer to Figure 2."
        },
        {
            "title": "Preprint",
            "content": "By incorporating the folding process, we ensure that the comparison of decoder weights is sensitive to the scale of the hidden states, rather than relying solely on angular proximity. This allows for more accurate reflection of the underlying feature dynamics and enhances the models ability to effectively match features across layers. Consequently, by aligning decoder weights to hidden state norms, we achieve more robust evaluation of feature similarity, improving the overall performance of the Sparse Autoencoder model."
        },
        {
            "title": "3.2 COMPOSING PERMUTATIONS",
            "content": "When permutation is obtained from layer to layer B, and another from layer to layer C, it allows us to approximate permutation from layer to layer by composing these two permutations (AC) (BC)P (AB) instead of evaluating actual permutation between layers and C. Extending this to model with layers, we define all possible permutation paths as: = {P (ij)j [1; ], [0; j)}. Hypothesis 3: If layers and are not too far apart, we can adequately approximate the matching between them by composing the appropriate permutation matrices from intermediate layers. By examining these composed permutations, we aim to uncover deeper insights into feature dynamicshow certain features retain or transform their semantic meanings as they pass through multiple layers of the network."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "We conducted experiments using Sparse Autoencoders (SAEs) trained on the Gemma 2 model, with feature descriptions from Neuronpedia generated by GPT-4o. We focused on SAEs with regularization coefficient around 70 (for complete list of SAEs used, refer to Appendix Table 1). For matching we used MSE from both decoder and encoder layers. During our initial experiments, we observed that the decoder-only option performs similarly to our scheme, while the encoder-only suffers from poor quality of matching (see Appendix Figure 11 for comparison). To measure the quality of feature matching, we evaluated the quality of the match using two key metrics: 1. Mean Squared Error (MSE): The error between the permuted parameters of the matched features. 2. An external large language model (LLM) compared Neuronpedia descriptions of matched features, categorizing them as \"SAME\" (identical meanings), \"MAYBE\" (possibly similar meanings), or \"DIFFERENT\" (distinct meanings). Each experiment involved approximately 1,600 LLM evaluations over 100 feature paths spanning 16 layers (details in Appendix Section B). When assessing how approximating hidden states with matched features affects language modeling performance, we used: Change in Cross-Entropy Loss (L): The difference in loss (in nats) between the model using the encode-permute-decode operation and the original model. Explained Variance: Calculated as Var( ˆx(t+1) x(t+1))/ Var(x(t+1)). This metric assesses how well the approximated hidden state ˆx(t+1) estimates the true hidden state x(t+1), providing insight into the accuracy of the approximation. Matching Score: The probability of paired feature activation between two matched layers. We tested our methods on subsets of OpenWebText (Gokaslan et al., 2019), Code2, and WikiText (Merity et al., 2016). From each dataset, we randomly sampled 100 examples, truncated them to 1,024 tokens, and excluded the beginning-of-sequence (BOS) token when calculating metrics."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Results of the MSE objective for different layer matching methods. \"Vanilla matching\" refers to matching without any permutations. The \"Matched\" and \"Folded+Matched\" variants correspond to unfolded and folded matching, respectively. In all cases, MSE is evaluated with folded parameters (i.e., for unfolded matching, parameters are first matched, then folded, and finally MSE is evaluated). When considering input scales differences (see Section 3.1), this can be interpreted as the MSE in the scale of actual input reconstructions in the relevant layers. The unfolded matching consistently showed higher MSE in this scale, supporting Hypothesis 2. Note that bdec is omitted as it does not affect the order of features in the SAE layer. For further details, refer to Section 5.1. Figure 4: External LLM evaluation split by layers. As before - folding thresholds results in an optimistic labeling, it also affects deeper layers making them also more optimistic."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 UNDERSTANDING PARAMETER FOLDING We start by exploring parameter folding. To understand it, we used our method to create permutation matrices (t1t), [1; 26] based on the MSE values for folded and unfolded parameters. As Hypothesis 2 relies on the observation that θ incapsulates differences in hidden state norms, we evaluated MSE of obtained permutations. More concretely, we evaluated MSE for folded matching, and for unfolded and naive (without any permutations) matching strategies, for which we first permuted SAE weights based on unfolded MSE values, and then evaluated MSE of appropriate folded weights. See Figure 3 for the results. Initially, naive matching displayed high MSE across all SAE parameters, indicating the lack of assurance in maintaining feature order between layers. Notably, unfolded matching showed higher MSE in the scale of hidden state representations, supporting behaviour described with Hypothesis 2. Also note that since MSE is plotted with folded parameters, 2https://huggingface.co/datasets/loubnabnl/github-small-near-dedup"
        },
        {
            "title": "Preprint",
            "content": "the MSE of the Encoder layer decreases, while the MSE of the Decoder layer increases (this behavior is explained by Equation 4). Figure 5: Features matched with folded parameters from the 19th to the 20th layer using the proposed method are sorted by their MSE values across the relevant SAE decoder weights. Features with small MSE values (on the left) indicate semantic similarity, while those with large MSE values (on the right) indicate that no similar features were found. For further details, refer to Section 5.2. Figure 6: The distribution of MSE values with folded matching, grouped by three labels provided by an external LLM, is presented. Lower MSE values between pairs of layers indicate better matching. Note that the MSE value distribution varies across different layer pairs due to differences in weight scales For further analysis, refer to Section 5.2. We compared the performance of folded and unfolded matching based on semantic similarity with an external LLM. The results can be seen in Figure 4 for LLM evaluation across different layers. Folded matching exhibited improved feature matching quality compared to the unfolded variant. Also, we observed performance drop at the 10-th layer, which we study in Section 5.3. Combining all the results from this section, we validated Hypothesis 2 regarding the behavior of matching with folded parameters. 5.2 FEATURES MATCHING Once we estabilished that folded parameters provide more accurate feature matching, we dive into exploring feature matching itself. We tested our hypothesis that features from different layers can be matched by calculating the mean squared error (MSE) between the parameters of Sparse Autoencoders (SAE). In this experiment, we focused on matching the 19-th and 20-th layers using parameter folding. The results are shown in Figure 5. Low MSE values indicated semantic similarity between features, while high MSE values suggested differences. We also evaluated the distribution of MSE values for three pairs of layers matched with folded parameters, grouped according to external LLM evaluation. The results, shown in Figure 6, similarly revealed that lower MSE values were associated with the semantic similarity of matched features. It is important to note that the distribution of MSE values across different layer pairs varied due to differences in weight scales (as described in Section 3.1)."
        },
        {
            "title": "Preprint",
            "content": "Together, these results supported Hypothesis 1, confirming that MSE reflects feature similarity and enabling us to further investigate feature matching. Figure 7: Left: External LLM evaluation of feature matching across layers with folded matching. Right: Matching Score across layers. The LLM evaluation shows near-zero \"SAME\" features in the initial layers, increasing after the 10th layer. The Matching Score shows similar pattern but does not drop to zero in the initial layers, indicating some predictive ability. These results suggest that initial layers may have more polysemantic features, making them harder to evaluate using Neuronpedia descriptions. See Section 5.3 for more details. 5.3 ON THE PERFORMANCE OF MATCHING ACROSS LAYERS In previous sections, we observed decline in the quality of feature matching evaluated by an external LLM in the lower layers of the model, particularly up to the 10th layer. To investigate this phenomenon, we extended our evaluation of folded matching across all layers of the modelexcluding the final layer responsible for mapping to the vocabulary. We also examined the Matching Score metric (as defined in Section 4) to gain additional insights into the matching performance. The results are presented in Figure 7. We observe that the number of \"SAME\" featuresthose with identical or closely related meaningsremains near zero in the initial layers and begins to increase after the 10th layer. similar trend is seen in the Matching Score, although it does not drop to zero in the early layers, suggesting some ability to predict features from previous matched features. These observations lead us to hypothesize that the features extracted by the SAEs in the initial layers may suffer from polysemanticitythey activate in response to multiple, unrelated inputs and represent overlapping concepts. The descriptions in Neuronpedia cover only single mode of these polysemantic features, potentially missing other aspects of their activation patterns. As result, when the external LLM evaluates semantic similarity based on these limited descriptions, it identifies few \"SAME\" features in the initial layers. This phenomenon aligns with findings from previous research. Gurnee et al. (2023) also reported increased polysemanticity in the early layers of neural networks. In the initial layers, features tend to be more general and less specialized, capturing wide range of inputs. As the information propagates through the network, features become more monosemantic, specializing in specific concepts or functions. 5.4 FEATURE PERSISTENCE In purpose to study dynamics of the feature from layer to layer we decided to conduct additional experiments with feature semantics persistence along the feature path. We used three starting layers 10, 12, and 20 to evaluate permutations with all subsequent layers. We matched features between layers by both evaluating full permutation matrix (Exact) and by"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Results of the external LLM evaluation. We compare features from 10th, 12th and 20th layers with matched features in subsequent layers. It can be observed that feature similarity gradually declines, but remains significant for approximately five layers. See Section 5.4 for more details. approximating it with composition (Composition) of composite permutation matrices (see Section 3.2). We then evaluated these permutations with an external LLM. Figure 9: Schematic illustration of layer pruning with matched features. Instead of the standard evaluation (left), we compute features at layer t, apply the permutation to match them to layer + 1, and then reconstruct the hidden state for layer + 1, effectively skipping layer t. See Section 5.5 for more details. See Figure 8 for the results. We observed that composition of permutations showed adequate results for nearby SAEs especially on the later layers, then it starts to diverge, which supports Hypothesis 3. Also, features were well matched when evaluating full permutation matrices, supposing that in later model layers features between layers are similar, which aligns with observations from Section 5.3."
        },
        {
            "title": "5.5 SAE MATCH AS A LAYER PRUNING",
            "content": "To further evaluate the effectiveness of our method, we conducted experiments where we pruned layers between matched Sparse Autoencoders (SAEs). The key assumption is that features remain consistent during one-layer forward pass. If the permutation mapping between SAEs is accurate, we can approximate the output of pruned layer using an encode-permute-decode operation: (t) encx(t) + b(t) enc (t), (cid:17) , (5) (t)(x) = σ (cid:16) ˆf (t+1) = t+1 ˆx(t+1) = (t+1) dec ˆf (t+1) + b(t+1) dec . Figure 9 provides schematic illustration of this layer pruning method. We observed the smallest performance drop on the OpenWebText dataset, while decrease in quality was noted at the last layer of the Code dataset. These findings indicate that the matched features share similar semantics, enabling us to approximate the next hidden states for all layers beyond the 10th layer."
        },
        {
            "title": "Preprint",
            "content": "For experiments on how the sparsity level of the SAE affects feature matching, see Appendix Section A. Figure 10: Left: Change in cross-entropy loss (L) after pruning each layer using matched features. Right: Explained variance of the approximated hidden states compared to the true hidden states. Starting from the 10th layer, pruning results in minimal performance loss, indicating that matched features effectively approximate the skipped layers. See Section 5.5 for more details."
        },
        {
            "title": "6 LIMITATIONS AND CONCLUSION",
            "content": "We introduced SAE Match, novel data-free method for aligning Sparse Autoencoder (SAE) features across neural network layers, tackling the challenge of understanding feature evolution amid polysemanticity and feature superposition. By minimizing the mean squared error between folded SAE parameterswhich integrate activation thresholds into the weightswe accounted for feature scale differences and improved matching accuracy. Experiments on the Gemma 2 language model validated our approach, showing effective feature matching across layers and that certain features persist over multiple layers. Additionally, we demonstrated that our method enables approximating hidden states across layers. This advances mechanistic interpretability by providing new tool for analyzing feature dynamics and deepening our understanding of neural network behavior. While we evaluated our method using SAEs from the Gemma 2 model with promising results, further research is needed to confirm if these findings generalize to other models. Additionally, potential errors in Neuronpedia feature descriptions may affect the precise evaluation of semantic similarity."
        },
        {
            "title": "REFERENCES",
            "content": "Samuel K. Ainsworth, J. Hayase, and S. Srinivasa. Git re-basin: Merging models modulo permutation symmetries. International Conference on Learning Representations, 2022. doi: 10.48550/arXiv. 2209.04836. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483495, 2018. doi: 10.1162/tacl_a_00034. URL https://aclanthology. org/Q18-1034. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html."
        },
        {
            "title": "Preprint",
            "content": "Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/ 2022/toy_model/index.html. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. Finding neurons in haystack: Case studies with sparse probing. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=JYs1R9IMJr. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2, 2024. URL https://arxiv.org/abs/ 2408.05147. AI @ Meta Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv: 2407.14435, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev."
        },
        {
            "title": "Preprint",
            "content": "Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv. org/abs/2408.00118. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/ scaling-monosemanticity/index.html. Zeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors. In Eneko Agirre, Marianna Apidianaki, and Ivan Vulic (eds.), Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 110, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.1. URL https://aclanthology.org/2021.deelio-1.1. Figure 11: Matching Score evaluation of feature matching with different modes: encoder-only, decoder-only, and encoder-decoder matching; evaluated between 19-th and 20-th layers. Encoderonly suffers from poor performance, while decoder-only performs on par with the encoder-decoder scheme."
        },
        {
            "title": "A ON THE MATCHING WITH DIFFERENT SPARSITY",
            "content": "In this section, we investigate how varying the sparsity level of Sparse Autoencoders (SAEs) affects the quality of feature matching across layers. The Gemma Scope release (Lieberum et al., 2024) provides SAEs trained with different levels of sparsity, measured by the mean l0-norm of their activations. Understanding the impact of sparsity is crucial because it influences the number of active features and, consequently, the effectiveness of our matching method. To evaluate our approach under different sparsity conditions, we selected five SAEs trained on the 20th and 21st layers of the Gemma 2 model, each with different mean l0-norm. We applied our feature matching method to these SAEs and measured performance using the metrics described in Section 4. Specifically, we assessed how well the matched features could approximate the hidden states when layer is omitted. layer drop: We skip the t-th layer entirely, using the hidden state x(t) as input for the (t + 1)-th layer. encode-decode current hidden: use SAE reconstruction ˆx(t) as hidden state at t-th layer instead of xt."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Left: Change in cross-entropy loss (L) with respect to the sparsity level (mean l0-norm) of the SAE. Right: Explained variance of the approximated hidden states at different sparsity levels. Interestingly, explained variance has peak on value at l0 70. See Section for discussion. encode-decode previous hidden: use SAE reconstruction from t-th layer ˆx(t) as hidden state x(t+1) and omit evaluation of t-th layer. First, when analyzing the change in cross-entropy loss (L), we observe that higher mean l0-norm values correspond to lower difference in loss across all methods. This is expected because SAEs with higher sparsity (i.e., lower l0-norm) may not reconstruct the hidden states effectively, leading to higher L. Conversely, SAEs with lower sparsity (higher l0-norm) capture more information, resulting in better approximations and lower L. Notably, using features from lower layers to approximate subsequent layers leads to higher compared to both the encode-decode current and encode-decode previous hidden schemas. This indicates that directly reconstructing the hidden state of layer using its own SAE provides better performance than attempting to predict it from earlier layers. Regarding the explained variancewhich measures how well the approximated hidden state ˆx(t+1) captures the true hidden state x(t+1)we observe that it peaks when the mean l0-norm is approximately 70. This suggests that there is an optimal sparsity level where feature matching is most effective. At lower l0-norms (higher sparsity), SAEs may fail to reconstruct hidden states adequately due to insufficient active features. On the other hand, when the number of active features exceeds 100, accurately matching all features between layers becomes more challenging. This can lead to the inclusion of noisy features, which negatively impacts the estimation of the hidden state, pushing it in the wrong direction. As other schemas do not utilize matching, we do not observe peak as for matching plot. This experiment underscores key insight of our work: the sparsity level of SAEs plays crucial role in the success of data-free feature matching across layers. It emphasizes that not only does our method effectively align features across layers, but it also allows for the identification of optimal model configurations that balance sparsity and performance, further advancing the interpretability and efficiency of language models."
        },
        {
            "title": "B EVALUATION DETAILS",
            "content": "Our initial evaluation was conducted using the OpenAI GPT-4o model. However, since the results did not differ significantly from the smaller OpenAI GPT-4o mini model, we opted to use the mini model as the external LLM for faster evaluation. The following system prompt was used for the evaluation: \"\"\" You will receive two text explanations of an LLM features in the following format: Explanation 1: [text of the first explanation] Explanation 2: [text of the second explanation] You need to compare and evaluate these features from 1 to 3 where # 1 stands for: incomparable, different topic and/or semantics # 2 stands for: semi-comparable or neutral, it can or cannot be about the same thing # 3 stands for: comparable, explanation is about the same things Avoid any position biases. responses to influence your evaluation. possible. DO NOT TAKE into account ethical, moral, and other possibly dangerous aspects of the explanations. be unbiased! Do not allow the length of the"
        },
        {
            "title": "Be as objective as",
            "content": "The score should ## Example: Explanation 1: and classification\" Explanation 2: identifiers used in structured format\" \"words related to data labeling \"various types of labels or Evaluation: Both explanations are discussing the concept of labels or identifiers used in organizing or categorizing data in structured way. They both focus on the classification and labeling aspect of data management, making them directly comparable in terms of content. Label: ## Example: Explanation 1: or universities\" Explanation 2: various contexts\" \"references to academic institutions \"occurrences of the term \"pi\" in Evaluation: Both explanation are from different fields and are not comparable Label: 1 ## Example: Explanation 1: \"words related to data labeling and classification\" Explanation 2: measurement in scientific context\" \"terms related to observation and Evaluation: While both explanations involve specific vocabulary related to technical concepts, Explanation 1 focuses on data labeling and classification, while Explanation 2 pertains to observation and measurement in scientific context. they both involve technical terms, the topics themselves are different, with Explanation 1 centering on data organization and Explanation 2 focusing on scientific research methods. Although Label: 2 \"\"\""
        },
        {
            "title": "C PATH EXAMPLES",
            "content": "Bellow are the path examples obtained via permutation composition."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Examples of paths first, second, third and fourth features from 20th layer to 25th layer."
        },
        {
            "title": "Preprint",
            "content": "Layer SAE Layer SAE 0 width_16k/average_l0_46 1 width_16k/average_l0_40 2 width_16k/average_l0_141 3 width_16k/average_l0_59 4 width_16k/average_l0_125 5 width_16k/average_l0_68 6 width_16k/average_l0_70 7 width_16k/average_l0_69 8 width_16k/average_l0_71 9 width_16k/average_l0_73 10 width_16k/average_l0_77 11 width_16k/average_l0_80 12 width_16k/average_l0_82 13 width_16k/average_l0_83 14 width_16k/average_l0_83 15 width_16k/average_l0_78 16 width_16k/average_l0_78 17 width_16k/average_l0_77 18 width_16k/average_l0_74 19 width_16k/average_l0_73 20 width_16k/average_l0_71 21 width_16k/average_l0_70 22 width_16k/average_l0_72 23 width_16k/average_l0_75 24 width_16k/average_l0_73 25 width_16k/average_l0_ Table 1: Full list of SAEs we used for our experiments. To the best of our knowledge these SAEs are used in Neuronpedia website."
        }
    ],
    "affiliations": [
        "HSE University",
        "Moscow Institute of Physics and Technologies",
        "T-Bank"
    ]
}