{
    "paper_title": "A Survey on Latent Reasoning",
    "authors": [
        "Rui-Jie Zhu",
        "Tianhao Peng",
        "Tianhao Cheng",
        "Xingwei Qu",
        "Jinfa Huang",
        "Dawei Zhu",
        "Hao Wang",
        "Kaiwen Xue",
        "Xuanliang Zhang",
        "Yong Shan",
        "Tianle Cai",
        "Taylor Kergan",
        "Assel Kembay",
        "Andrew Smith",
        "Chenghua Lin",
        "Binh Nguyen",
        "Yuqi Pan",
        "Yuhong Chou",
        "Zefan Cai",
        "Zhenhe Wu",
        "Yongchi Zhao",
        "Tianyu Liu",
        "Jian Yang",
        "Wangchunshu Zhou",
        "Chujie Zheng",
        "Chongxuan Li",
        "Yuyin Zhou",
        "Zhoujun Li",
        "Zhaoxiang Zhang",
        "Jiaheng Liu",
        "Ge Zhang",
        "Wenhao Huang",
        "Jason Eshraghian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/."
        },
        {
            "title": "Start",
            "content": "Rui-Jie Zhu,, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian UCSC, FDU, NJU, PKU, RUC, UoM, UW-Madison, PolyU, M-A-P"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the models expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the models continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: LatentCoT-Horizon. 5 2 0 2 8 ] . [ 1 3 0 2 6 0 . 7 0 5 2 : r Figure 1. Explicit reasoning transmits discrete tokens ( 15 bits each), whereas latent reasoning exchanges full 2560-dimensional FP16 hidden states ( 40, 960 bits each), revealing 2.7 103fold bandwidth gap between the two approaches. Equal Contribution. Corresponding Authors."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminary: Latent Chain-of-Thought"
        },
        {
            "title": "2.1 General Framework .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.3 Latent Reasoning Updates of Diffusion Models",
            "content": ". . . . . . . . . . . . . . . . . . . 3 Latent Reasoning"
        },
        {
            "title": "3.1.2 Activation with Explicit Hidden-State Feedback . . . . . . . . . . . . . . .",
            "content": "3.1.3 Training-induced Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.1.4 Training Strategies for Recurrent Reasoning . . . . . . . . . . . . . . . . . .",
            "content": "3.1.5 Applications and Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Horizontal Recurrent: Hidden state-based Methods . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2.1 Linear-State Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.2.2 Gradient-State Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.2 Mechanisms of Latent CoT in Layer Representation . . . . . . . . . . . . . . . . .",
            "content": "4.3 Turing Completeness of Layer-Based Latent CoT . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Towards Infinite-depth Reasoning",
            "content": "5.1 Spatial Infinite Reasoning: Text Diffusion Models . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.2.2 A Unifying View .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "2 3 4 5 6 7 8 8 10 11 13 14 14 15 17 18 19 20 22 22 25 25 26 26 27 1. Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities in performing reasoning tasks, in some cases even exceeding human-level performance [47, 62, 79, 128]. LLMs often reason more effectively when they produce Chain-of-Thought (CoT) [110], spelling out each intermediate step in natural language before arriving at final answer. Initially viewed as logical extension to prompt engineering, CoT gained traction once supervised instruction tuning exposed models to many annotated reasoning traces. It then became the norm when RL rewarded answer correctness [49], which encouraged models to generate their own effective chains of thought. As result, LLMs that think in language before answering have attained remarkable performance improvements. This principle now anchors leading reasoning models, including the Qwen3 series [113], DeepSeek-R1 [41], and Gemini 2.5 series [32]. However, just as humans do not always rely on language for their cognitive processes, LLMs spend most of their processing budget in the latent space. Enforcing CoT to operate with natural language can constrain models expressive range and can also impose redundant computation. Latent Chain-of-Thought (Latent CoT) has the potential to overcome these limits [23, 44]. Unlike its explicit counterpart that relies on discrete tokens, latent CoT carries reasoning in continuous internal representations, often via recurrent mechanisms within the model. This offers richer expressivity and access to non-linguistic reasoning paths, potentially unlocking new frontiers in model reasoning. This survey examines the emerging landscape of Latent CoT and its potential to surpass language-based reasoning constraints. While explicit CoT forces thoughts into string of tokens, Latent CoT shifts the entire reasoning process into the models continuous representational space. The aim is to expand expressiveness and raise the performance ceiling: freed from finite vocabulary, model can explore reasoning trajectories with no direct linguistic equivalent. We categorize and analyze the technical approaches that leverage these continuous representations to achieve more advanced reasoning. The structure of this survey is designed to provide comprehensive understanding of Latent CoT and its various implementations. Our taxonomy breaks this down in Figure 2. We begin by establishing general formulation that captures most Latent CoT implementations, before classing techniques into more specific categories. These categories can be broadly divided into two types: 1) vertical recurrence for expanding computational depth, and 2) horizontal recurrence for increasing sequential capacity. Vertical recurrence applies feedback loops to activation values, and can be thought of activation-based reasoning [22, 69]. Alternatively, horizontal recurrence uses hidden states to propagate context across long sequences of reasoning trajectories [84, 97]. We then explore fine-tuning strategies designed to compress or internalize explicit reasoning traces, which concludes the review of Latent CoT implementations. This sets the stage for understanding the mechanistic interpretability of latent reasoning to understand how these processes are realized within neural networks. This section examines the foundational role of network layers as the primary computational substrate for reasoning [89, 131]. We explore the theory of Layer Specialization, which posits that different layers develop distinct, hierarchical functionsfrom feature extraction in shallow layers to complex logical operations in intermediate layers and final integration in deep layerscollectively forming an implicit computational pipeline analogous to an explicit CoT. Explicit CoT comes with the benefit of intermediate tokens which offers degree of post-hoc interpretability, and we similarly aim to uncover the mechanisms that enable latent reasoning. 3 Preliminary: Latent Chain-of-Thought (2) General Framework Connections to Explicit Chain-of-Thought Latent Reasoning Updates of Diffusion Models Activation-based Recurrent Methods Architectural Recurrence Universal Transformer [22], CoTFormer [69], Recursive Transformer [2], AlgoFormer [29], Recurrent-Depth [31] Training-induced Recurrence Coconut [44], CODI [88], VO-VAE [95], CCOT [16], Decomposes Reasoning [50], Light thinker[129], Filler Tokens [76], Pause Tokens [37], Planning Token [107] Training Strategies for Recurrent Reasoning MIDAS [82], Looping-Inspired Regularization [83], Stepwise Internalization [23], Coconut [44], RELAY [122] Latent Reasoning (3) Applications and Capabilities Looped Architectures [35, 85] , Looped Transformers [21], Planning Tokens [107], Decomposes Reasoning [50], Optimization and Meta-Learning [13, 30] n e e L Mechanical Interpretability (4) Towards Infinite-depth Reasoning (5) Temporal Hiddenstate Methods Do Layer Stacks Reflect Latent CoT? Linear-State Recurrence Linear Attention [53], RetNet [98], GLA [116], RWKV-6 [74], HGRN-2 [77], Mamba-2 [20], DeltaNet [118], Gated DeltaNet [117] Gradient-State Recurrence TTT [97], Titans [3], ATLAS [4], Gated delta [117], Soft Reasoning [134] Training-induced Hidden-State Conversion Layer Depth Influence Reasoning Capabilities Layer Depth Serves As The Primary Bottleneck SUPRA [67], MOHAWK [7], Llamba [8], LoLCATs [130], Liger [56] Implicit Reasoning [123], Two-Hop Reasoning [42] Looped Transformers [83], Log-Depth Transformers [68] Clear Correspondence Between Layer Depth and Resoning MechanisticProbe [46], Back Attention [124], Iteration Head [11], Matching Operation [109] Shallow Layers Micse [54], Feed-forward Layers [34], Unveiling Induction Heads [14], Hopping [9], Loss Landscape Geometry [104], Memory Bottleneck [59], Unpacking Robustness [102], Multi-hop Reasoning [115], Layer by Layer [93], Distributional Reasoning [86] Mechanisms of Latent CoT in Layer Representation Intermediate Layers Representation [92], Group Operations [112], Interpretability [105], Emergent Abilities [6] Compositional Generalization [6], Factual Recall Behaviors [108], Test-time Compute [31] Deep Layers Interwoven Structured Knowledge [57], Layer by Layer [93], Gpt-2 [43], Inheritune [81], Step-by-step [27], Lift [125], LLM Depth [19], Void in LLM [87], Curse of Depth [96] Theory of Information Flow Causal Mediation Analysis [94], Edge of Generalization [103], Back Attention [124] Limits of Layer-Based Latent CoT Interpretation Proof of Turing completeness in model architectures RNN encoder-decoder [17], Transformers [101], Turing Completeness [75] [78] Masked Diffusion Models D3PM [1], SEDD [64], RADD [73], MD4 [90], Simple-MDM [80], MDM [71], MMaDA [114], IRED [26], LLaDA [72], dKV-Cache [65], DoT-SEDD [119], dLLM-Cache [63], d1-LLaDA [132], MGDM [120], LLaDA 1.5 [133], DCoLT [48] Spatial Infinite Reasoning: Text Diffusion Models Embedding-based Diffusion Models Diffusion-LM [60], CDCD [24], Bit Diffusion [15], Plaid [40], BFN [38], DoT-Plaid [119], TESS [66], TESS 2 [99] Hybrid AR-Diffusion Models DiffuLLaMA [36], Dream [121], L2D [12], Gemini Diffusion [33], Mercury [55] Infini-attention Transformers with infini-attention [70] Infinitely Long Optimizer Network Test-time training (TTT) and its descendants TTT [97], Titans [3], OmegaNet and Atlas [4] Implicit Fixed-Point RNNs Implicit language models are rnns [84] Figure 2. Taxonomy of Latent Reasoning. Finally, we explore advanced paradigms at the frontier of LLM cognition, focusing on the pursuit of infinite-depth reasoning. This concept refers to models ability to devote unbounded computational steps to refine solution, moving beyond fixed-depth architectures. Our discussion centers on spatial infinite reasoning as realized by text diffusion models [71, 119]. Unlike traditional autoregressive generation, these models operate on the entire output sequence in parallel, enabling global planning and iterative selfcorrection through bidirectional context. This approach facilitates globally consistent and reversible reasoning processes, offering promising path toward more powerful and flexible AI systems. 2. Preliminary: Latent Chain-of-Thought In this section, we present unified mathematical framework for understanding various Latent CoT approaches. Unlike traditional CoT reasoning that generates explicit textual intermediate steps, latent CoT methods perform reasoning through continuous representations and hidden states within the models computational graph. We categorize these approaches based on how they propagate information across layers (spatial dimension) and time steps (temporal dimension). 4 2.1. General Framework We begin by establishing general formulation for transformer-based reasoning systems. Con𝑡 R𝑑 denote sider transformer model processing information at time step 𝑡 and layer 𝑙. Let 𝑥 𝑙 the activation at layer 𝑙 and time 𝑡. We introduce S𝑙 𝑡 to represent the hidden state that captures historical information. The structure and dimensionality of S𝑙 𝑡 are architecture-dependent and define how context is maintained. This state can manifest in several forms, including: KV Cache: In standard Transformers, S𝑙 𝑡 is the Key-Value (KV) cache, comprising pair of 𝑡), where K𝑙, V𝑙 R𝑛𝑑 and 𝑛 is the sequence length of the context. Note matrices (K𝑙 𝑡, V𝑙 that as 𝑡 increases, so does 𝑛. Linear Attention State: In models with linear attention, the hidden state can be compressed 𝑡 R𝑑𝑑, which allows for efficient, recurrent-style updates. 𝑡 R𝑑, that Recurrent State: For RNN-like mechanisms, S𝑙 into fixed-size state matrix, S𝑙 𝑡 is single state vector, S𝑙 summarizes all past information into fixed-size representation. With this generalized view, the fundamental operations in latent reasoning can be decomposed into spatial and temporal transformations."
        },
        {
            "title": "The spatial transformation propagates information vertically through layers at a fixed time",
            "content": "step: 𝒙 𝑙+1 𝑡+1 = 𝑓 (𝒙 𝑙 𝑡+1, 𝑔(S 𝑙 𝑡, 𝒙 𝑙 𝑡)) (1) where 𝑓 represents the layer-wise transformation function (e.g., transformer block), which uses the historical context in S𝑙 𝑡 to compute the next layers activation; 𝑔 captures how historical information is maintained and updated. The implementation of 𝑔 depends on the form of S𝑙 𝑡 (e.g., appending to the KV cache, or performing matrix/vector update). Activation-Based Methods Activation-based methods focus on deepening the computational graph by iteratively refining activations within single time step. These approaches implement form of recursive computation where the same transformation is applied multiple times, allowing for progressive refinement of representations. Formally, activation-based methods compute: 𝑙+𝑛 𝑡 = 𝑓 𝒙 (cid:16) . . . 𝑓 (cid:16) 𝑓 (𝒙 𝑙 𝑡, 𝑔(S 𝑙 𝑡, 𝒙 𝑙 𝑡)), 𝑔(S 𝑙+1 𝑡 , 𝒙 𝑙+1 𝑡 ) (cid:17) 𝑙+𝑛1 , . . . , 𝑔(S 𝑡 , 𝒙 𝑙+𝑛1 𝑡 ) (cid:17) (2) This recursive application can be understood as creating computational loop within the forward pass. At each iteration 𝑖 {1, . . . , 𝑛}, the model refines its representation by applying the transformation function 𝑓 , potentially with access to different hidden states S𝑙+𝑖1 . Here, 𝑙 denotes the starting layer index, constrained by 1 𝑙 𝐿 𝑛, where 𝐿 is the total number of layers in the model. The key insight is that by repeatedly processing the same input with shared parameters, the model can perform iterative refinement analogous to human step-by-step reasoning. 𝑡 Hidden State-Based Methods Hidden state-based methods take fundamentally different approach by aggregating information from multiple temporal or spatial contexts simultaneously. Rather than iterative refinement, these methods leverage rich historical representations to inform current computations. 5 The core computation in hidden state-based methods is: 𝑙+1 𝑡 = 𝑓 𝒙 (cid:16) 𝑙 𝑡, 𝑔 𝒙 (cid:16)(cid:16) 𝑙 𝑡, 𝑙 𝑡1, . . . , 𝑙 𝑡𝑛 (cid:17)(cid:17) (cid:17) 𝑙 𝑡 , 𝒙 , (3) This operation allows the model to access broader context of hidden states, effectively creating memory bank that spans multiple layers or time steps. The function 𝑓 must be designed to effectively aggregate and utilize this expanded context, often through specialized attention mechanisms or learnable aggregation functions. 2.2. Connections to Explicit Chain-of-Thought Understanding how these latent methods relate to explicit Chain-of-Thought reasoning provides important insights. Traditional CoT generates sequence of tokens 𝑦1, 𝑦2, . . . , 𝑦𝑇 representing intermediate reasoning steps. In the latent framework, these explicit tokens are replaced by continuous representations that evolve according to the dynamics described above. The correspondence can be formalized by considering the generation process. In explicit CoT: 𝒚𝑡+1 = Decode(Transform(𝒙𝑡, S𝑡)), (4) where the decoding step projects continuous representations back to discrete tokens. Latent methods eliminate this decoding step, instead maintaining reasoning in the continuous space: 𝒛𝑡+1 = Transform(𝒛𝑡, S𝑡), (5) where 𝑧𝑡 represents the continuous \"thought\" at step 𝑡. This fundamental difference enables latent methods to explore reasoning pathways that may not have natural linguistic expressions, potentially discovering more efficient or powerful reasoning strategies unconstrained by the token vocabulary. However, it also introduces challenges in interpretability and training, as the intermediate states no longer correspond to human-readable explanations. 2.3. Latent Reasoning Updates of Diffusion Models Understanding how latent update methods relate to diffusion models reveals fundamental differences from autoregressive (AR) generation. Traditional diffusion models operate purely through temporal updates without explicit spatial transformations, fundamentally differing from the spatial-temporal decomposition in transformer-based reasoning systems. Temporal-Only Updates Diffusion Models Classical diffusion models perform updates exclusively in the temporal dimension through iterative denoising. The process involves two primary update mechanisms: Discrete updates (mask-based): Given sequence of tokens 𝑦1, . . . , 𝑦𝑁, the model selectively updates positions based on masking patterns: (cid:40) 𝑓 (𝒙𝑙 𝒙𝑙 𝑡 (𝑖), (𝑖) = 𝑙 𝑡+1 𝒙 𝑡 (𝑖), 𝜖𝑡), if 𝑚𝑡 (𝑖) = 1 otherwise (6) where 𝑚𝑡 (𝑖) represents the mask indicating which tokens to update at step 𝑡. 6 Continuous updates (noise-based): The model applies global noise reduction across all positions: 𝑡+1 = 𝑓 (𝒙 where 𝑓 represents the denoising function that operates uniformly across all token positions. 𝒙 (7) 𝑙 𝑡, 𝜖𝑡) 𝑙 KV-cache Integrated Diffusion Models Recent advances have begun incorporating bidirectional KV cache mechanisms [65] into diffusion models, introducing spatial-like transformations alongside temporal updates. This hybrid approach bridges the gap between traditional diffusion and transformer-based reasoning. Confidence-thresholded spatial transformation: All token activations are updated layerwise at each denoising iteration: (8) where 𝑓𝜏 denotes bidirectional Transformer block that refines every token representation while utilizing cached states. 𝒙 𝑡 = 𝑓𝜏 (cid:0)𝒙 𝑙+1 𝑙 𝑡, S𝑡𝑙 , 𝜖𝑡(cid:1) Selective temporal cache updates: Only tokens whose confidence score 𝑐𝑙 𝑡 (𝑖) = conf(cid:0)𝑥 𝑙 𝑡 (𝑖)(cid:1) meets or exceeds threshold 𝜏 refresh their KV cache: 𝑡 (𝑖)(cid:1), 𝑡 (𝑖), S𝑙 𝑙 𝑡+1 (𝑖) = (cid:40)𝑔𝜏 (cid:0)𝑥 𝑙 S𝑙 𝑡 (𝑖), 𝑐𝑙 𝑡 (𝑖) 𝜏 otherwise (9) Complete spatio-temporal evolution: The framework combines spatial refinement with selective temporal caching: 𝒙 𝑡+1 = 𝑓𝜏 (cid:0)𝒙 𝑙+1 𝑙 𝑡+1, 𝑙 𝑡+1 (cid:1) (10) This evolution represents significant departure from traditional diffusion models, incorporating transformer-style spatial processing while maintaining the iterative refinement benefits of temporal diffusion. The confidence-thresholded mechanism enables efficient cache management in bidirectional contexts, addressing the fundamental incompatibility between traditional KV caching and diffusion model architectures. Consequently, diffusion models scan the entire sequence to identify and update the highestconfidence tokens in parallelcontinuously correcting their representations across layers whereas autoregressive models must commit to single next token and cannot revisit or refine earlier outputs. As result, diffusions spatiotemporal mechanism enables ongoing, bidirectional refinement of multiple reliable latent states, while AR generation proceeds strictly forward, leaving past tokens fixed once generated. 3. Latent Reasoning The development of latent CoT reasoning follows two fundamental computational paradigms: expanding depth through activation recurrence and expanding temporal capacity through hidden state evolution. As illustrated in Figure 3, activation-based methods create deeper computational graphs by iteratively processing information through the same set of layers, akin to vertical expansion. In contrast, hidden-state-based methods expand the models memory horizontally, allowing it to access and integrate information over longer sequences. This distinction raises critical implementation and theoretical questions. For activation-based approaches, how can model with fixed number of layers be architecturally designed or 7 Figure 3. Comparison of Activation-Based and Hidden-state-Based Latent Reasoning. Activation-based methods (left) iteratively refine representations by looping through the same layers over multiple time steps (𝑇 = 1, 2, ..., 𝑁), increasing computational depth. Hidden-statebased methods (right) process information sequentially, evolving hidden state that carries information across potentially long temporal sequence (𝑇 = 1, 2, ..., 𝑁). trained to \"think\" longer about problem, effectively creating vertical computational depth on the fly? What are the principles that govern this induced recurrence, and what new capabilities does it unlock? Conversely, for hidden-state methods, as reasoning chains extend, how can model maintain coherent \"state of mind\" over vast temporal sequences without succumbing to the bottleneck of ever-expanding memory? Can this temporal evolution be reframed as form of continuous online optimization, conceptually unifying this horizontal expansion with the iterative vertical refinement seen in activation-based methods? While both approaches enhance reasoning capabilities, they differ in implementation requirements and deployment flexibility, offering distinct pathways toward more powerful latent reasoning. The following sections of this paper will describe these parts in detail. 3.1. Vertical Recurrent: Activation-based Methods Activation-based approaches achieve latent reasoning by creating recurrent computational flows, either through architectural design or training-time manipulation. These methods share common principle: iteratively refining representations without generating explicit reasoning tokens. 3.1.1. Loop/Universal Transformer Recurrence Loop-based architectures represent the foundational approach to activation-based latent CoT reasoning, implementing continuous activation propagation across Transformer layers through explicit architectural modifications. These models share core principle: enabling iterative refinement of hidden states within single forward pass through layer-wise recurrence. Starting from the Universal Transformer (UT) [22], which pioneered dynamic recurrence over layers with its Adaptive Computation Time (ACT) mechanism, this architectural paradigm has established depth-adaptive reasoning as viable alternative to traditional fixed-depth transformers. The key innovation lies in treating network depth not as static hyperparameter but as dynamic 8 Architecture Pre/Loop/Coda Per-iter input 𝑥𝑡 Hidden state 𝑆𝑡 Dynamic stop Depth-emb 𝑑𝑡 Universal Transformer [22] CoTFormer [69] Recursive Transformer [2] AlgoFormer [29] Recurrent-Depth [31] No No Optional Yes Yes 𝑥 𝑙1 𝑡 𝑆𝑙1 𝑡 + 𝑑𝑡 , 𝑥 𝑙1 𝑡 𝑥 𝑙1 𝑡 𝑥 𝑙1 𝑡 𝑡 , 𝑥 𝑙1 𝑥1 𝑡 standard unroll standard unroll share/refill ˆℎ standard roll modulo (𝑡 mod 𝑟) reuse ACT, (cid:205)𝑡 𝑝𝑡 > 𝜏 MoR router 𝑔𝑖 early-exit, max𝑡 Δℎ < 𝜀 fixed sinusoidal 𝑑𝑡 learnable 𝑑𝑡 none none fixed-point iteration tried, dropped Table 1. Comparison of activation-based latent CoT architectures and their key design characteristics, showing the evolution from early monolithic designs to structured Pre/Loop/Coda frameworks with simplified dynamic stopping mechanisms. Input tokens + 𝑑𝑡 Prelude 𝑙 = 1, 2, . . . , 𝑁 iterations Loop blocks 𝜎 stop 𝑡 , 𝑆2 𝑆 𝑡 , ..., 𝑆𝑙 𝑡 KV-cache Coda Output Legend: 𝑆𝑡: hidden state / KV 𝑑𝑡: depth embedding 𝜎: dynamic stop gate Figure 4. Conceptual diagram of Pre/Loop/Coda architecture with per-iteration input 𝑥𝑡, hidden state 𝑆𝑡 (KV-cache), depth embedding 𝑑𝑡, and dynamic-stop gate. computational resource that can be allocated based on task complexity. Extending activationreuse beyond Universal/Looped Transformers, Zeng et al. [127] introduce Pondering LM that performs 𝑘 iterative ponder cycles inside every token prediction. Each cycle converts the models softmax into continuous pondering embedding: weighted sum of all vocabulary vectors, which is fed back via residual path to refine the hidden state. Since this seminal work, the field has undergone systematic evolution along several key dimensions, revealing important design principles for latent reasoning architectures (Table 1 and Figure 4). The Rise of Pre/Loop/Coda Structure Early models like Universal Transformer and CoTFormer [69] adopted monolithic recurrent designs without explicit stage separation. However, recent architectures like Recursive Transformer [2], AlgoFormer [29], and Recurrent-Depth [31] have converged on three-stage Pre/Loop/Coda structure. This design explicitly separates input encoding (Prelude), iterative reasoning (Loop blocks), and output decoding (Coda), enabling more modular and interpretable computation flows. The modularization of the architecture improves interpretability and facilitates the injection of task-specific priors, such as fixed-point iteration constraints or algorithmic templates, into the reasoning process. Per-iteration Input and Hidden State Management Input handling strategies vary across models, reflecting different hypotheses about information flow during recurrence. Universal Transformer combines previous layer output 𝑥 𝑙1 𝑡 with depth embedding 𝑑𝑡. CoTFormer uses both hidden state S𝑙1 , while Recursive Transformer and AlgoFormer simplify to just 𝑥 𝑙1 𝑡 . Recurrent-Depth adopts hybrid approach with both 𝑥1 𝑡 and 𝑥 𝑙 and 𝑥 𝑙1 . 𝑡 𝑡 𝑡 For hidden state management, most models use standard unrolling of KV caches. Notable 9 exceptions include Recursive Transformers share/refill mechanism and Recurrent-Depths modulo-based reuse (𝑡 mod 𝑟), which improve memory efficiency through periodic cache recycling, as shown in Table 1. These innovations strike balance between preserving temporal coherence and managing computational resources. The Decline of Depth Embeddings Depth embeddings show clear deprecation trend. Universal Transformer introduced sinusoidal 𝑑𝑡, and CoTFormer experimented with learnable embeddings. However, subsequent models like Recursive Transformer and AlgoFormer completely dropped them. Recurrent-Depth tried but ultimately abandoned depth embeddings, suggesting their limited utility in recurrent architectures despite initial enthusiasm. This trend indicates that explicit positional encoding of depth may be redundant when the architecture inherently encodes iteration count through state evolution. Simplification of Dynamic Stopping Mechanisms Dynamic stopping mechanisms exhibit clear trend toward simplicity. Universal Transformers sophisticated ACT mechanism (with cumulative probability (cid:205)𝑡 𝑝𝑡 > 𝜏) gave way to CoTFormers MoR router 𝑔𝑖. Recent models adopt even simpler strategies: Recursive Transformer uses early-exit based on change magnitude (max𝑡 Δℎ < 𝜀), AlgoFormer opts for fixed iterations, and Recurrent-Depth explores fixed-point criteria. This evolution suggests that complex adaptive mechanisms may not justify their computational overhead in practice. These architectural trends reflect the fields maturing understanding: moving from complex adaptive mechanisms toward stable, modular designs while preserving the core benefit of enhanced reasoning through layer-wise recurrence. The convergence on simpler, more interpretable designs suggests that the key to latent reasoning may lie not in sophisticated control mechanisms but in providing sufficient computational depth with efficient resource management. 3.1.2. Activation with Explicit Hidden-State Feedback While loop-based architectures refine token representations by rerunning the same set of layers, distinct family of models feeds hidden states back into the input stream between iterations. In these systems the hidden activations themselves become new sequence elements, so each recurrent step simultaneously extends the effective depth and exposes internal computation to subsequent attention. Coconut Proposed by Hao et al. [44], Coconut inserts continuous thought vectorthe lastlayer hidden state of the previous decoding stepas an extra position before the current token. Pondering therefore occurs in latent space without emitting textual reasoning, enabling breadth-first exploration while reusing the same Transformer parameters. In CoTFormer [69], every forward pass first computes preliminary token emCoTFormer beddings; these activations are then interleaved back into the sequence and the shared block stack is executed again. Early-exited tokens thus attend to deeper refinements of their own representations, realizing adaptive depth with minimal parameters. Both models share three properties that distinguish them from pure activation-based recurrence: Key characteristics. Explicit state tokens re-inject hidden vectors as sequence elements, bridging vertical recurrence and horizontal memory; no architectural expansionthe model reuses the same layers so parameter count stays constant while depth grows dynamically; and latent reasoning remains internal, thereby avoiding the latency of producing explicit CoT tokens. These designs demonstrate that passing hidden states across recurrent hops can unlock stronger reasoning while preserving the efficiency of shared-weight loops, and they foreshadow later hybrids that blend activation and hidden-state paradigms. 3.1.3. Training-induced Recurrence While architectural recurrence requires explicit structural modifications, an alternative pathway achieves similar computational benefits through specialized training on standard transformer architectures. These methods fundamentally create recurrent activation flows without changing the models underlying structure, demonstrating that the key insight of iterative refinement can be induced through training alone. This approach is particularly valuable as it enables existing pretrained models to develop latent reasoning capabilities without architectural constraints. The core principle unifying these methods is the creation of implicit loops in the computation graph: whether by feeding activations back into the model (continuous recurrence), compressing multi-step reasoning into iteratively-processed representations (compressed states), or extending the effective computation depth through strategic token insertion (expanded iterations). All these approaches share the goal of enabling deeper reasoning without explicit architectural loops. Continuous Activation Recurrence The most direct form of training-induced recurrence involves creating explicit loops of continuous activations. Ref. [44] pioneers this approach with Coconut, which loops the LLMs last hidden state (the \"continuous thought\") directly back into the model as input for the next step. This mechanism creates recurrence pattern strikingly similar to architectural approaches like Universal Transformer, but implemented entirely through training. The continuous thought can encode multiple reasoning paths simultaneously, enabling breadth-first search-like exploration in latent space. Building on this foundation, subsequent work has refined the training methodology while maintaining the core recurrence principle. Shen et al. [88] propose CODI, which frames the problem as learning to align recurrent hidden states through self-distillation. By aligning the hidden activation before the final answer between teacher (with full CoT) and student (with compressed reasoning) paths, CODI effectively learns fixed-point iteration in activation space. This single-step alignment proves more stable than Coconuts curriculum learning, achieving parity with explicit CoT on GSM8K for the first time among latent methods. Cheng and Van Durme [16] take different approach with CCOT, training the model to generate variable-length sequences of continuous embeddings that approximate full reasoning traces. These embeddings function as compressed representations of recurrent computation steps, maintaining the iterative nature while reducing sequence length. The optional decoding back to text preserves interpretability while confirming that meaningful computation occurs in these latent iterations. PCCOT [111] uses Jacobi-iteration allowing parallel continuous thoughts. Building on pauseand filler-token methods that prolong hidden-state computation, System-1.5 Reasoning [106] introduces Depth and Step Shortcuts that dynamically allocate vertical layer depth and horizontal reasoning steps, delivering over 20 faster inference on GSM8K while preserving chain-of-thought accuracyall without modifying the Transformer backbone. 11 Compressed State Recurrence Rather than continuous loops, another strategy compresses reasoning steps into discrete or semi-discrete representations that the model processes recurrently. Su et al. [95] replace early CoT segments with discrete latent tokens learned via VQ-VAE, creating \"assorted\" reasoning that mixes compressed abstract steps with detailed reasoning. This approach effectively creates hierarchical recurrence where abstract tokens trigger expanded computation in subsequent layers. Zhang et al. [129] employ \"gist tokens\" as compression anchors in hidden space. Though these tokens themselves are semantically meaningless, they serve as recurrence checkpoints where the model aggregates and redistributes computational state. The attention mask manipulation enforces that subsequent reasoning depends on these compressed states, creating an implicit recurrence structure through the sequence. The key insight across these compression methods is that they transform horizontal (sequencelevel) reasoning into vertical (depth-level) computation, effectively increasing the recurrence depth available for each logical step. Iteration Expansion through Strategic Tokens third category of training-induced recurrence works by expanding the number of implicit iterations through token insertion. This approach recognizes that additional tokens, even without explicit semantic content, can provide more recurrence steps for internal computation. Pfau et al. [76] demonstrate that even meaningless filler tokens (e.g., \"......\") can improve reasoning by simply providing more attention steps, effectively increasing the number of recurrent iterations the model can perform. Goyal et al. [37] refine this with learnable <pause> tokens that explicitly signal computation steps, creating trainable recurrence points that the model learns to utilize effectively. More sophisticated approaches inject structured tokens that organize the recurrence pattern. Wang et al. [107] introduce planning tokens that create hierarchical recurrence structure, where each planning token initiates new reasoning loop with specific computational goals. Jin et al. [50] further decompose reasoning into <memory> and <reason> tokens, creating specialized recurrence patterns for different types of cognitive operations. These structured approaches demonstrate that training can induce not just recurrence, but organized, interpretable recurrence patterns. Implications and Connections These training-induced methods reveal fundamental insight: recurrence for reasoning is not solely an architectural property but can emerge from appropriate training objectives. The success of these approaches suggests that standard transformers possess latent capacity for iterative computation that training can unlock. Moreover, the convergence of continuous, compressed, and token-based methods toward similar performance outcomes indicates that the specific implementation of recurrence matters less than ensuring sufficient computational depth for reasoning tasks. The relationship between these training-induced methods and architectural recurrence is complementary rather than competitive. Future work might explore hybrid approaches that combine architectural loops with training-induced recurrence patterns, potentially achieving the benefits of both explicit structure and learned optimization. 3.1.4. Training Strategies for Recurrent Reasoning Effectively training models with recurrent activation flows presents unique challenges, as these architectures must learn to leverage iterative computation rather than relying solely on feedforward depth. Researchers have developed specialized training strategies that address both architectural and induced recurrence. For architectural recurrence, MIDAS [82] proposes progressive stacking framework to address training stability in loop-based models. It defines replication operator ( 𝑓 , 𝑏) that duplicates the middle layers of base model 𝑓 by factor 𝑏, enabling gradual depth expansion. Training proceeds through 𝑘 stages where model depth increases progressively, with each deeper model initialized from the previous stage. This curriculum approach helps models develop stable iterative reasoning patterns. Complementing this architectural focus, Saunshi et al. [83] introduce looping-inspired regularization that enables even standard Transformers to benefit from recurrence-like properties through cosine-similarity term R𝐺 (𝑘) in the loss function. This approach reveals that recurrent behavior can emerge from appropriate training objectives alone. For training-induced recurrence, Stepwise Internalization [23] pioneered curriculum-based compression of reasoning traces. This technique gradually removes CoT tokens during finetuning, allowing models to internalize reasoning patterns into their parameters. This curriculum principle has been widely adopted, notably by Coconut [44] which progressively replaces CoT tokens with continuous thoughts, achieving fully latent inference loops. RELAY [122] takes more direct approach by explicitly aligning recurrence steps with reasoning steps through two-stage process: first training looped Transformers with CoT-aligned supervision using loss = L𝑎𝑛𝑠 + 𝜆L𝑖𝑡𝑒𝑟, then fine-tuning autoregressive models on the generated reasoning chains. These diverse training strategies converge on key principles: gradual complexity increase, alignment between recurrence depth and reasoning steps, and careful balance between architectural constraints and learned behaviors. The success of both architectural and training-induced approaches suggests that effective recurrent reasoning emerges from the interplay of structure and optimization. 3.1.5. Applications and Capabilities The true test of recurrent reasoning methods lies in their ability to tackle complex tasks requiring structured, multi-step computation. Both architectural and training-induced recurrence have demonstrated remarkable capabilities across diverse domains. In algorithmic generalization, recurrent models exhibit unprecedented extrapolation abilities. Schwarzschild et al. [85] and Giannou et al. [35] demonstrate that looped architectures can generalize from small problem instances to significantly harder ones by extending recurrence steps at test timea property unavailable to static-depth Transformers. This recurrence-controlled scaling mimics human-like progressive problem-solving and has been formalized through theoretical frameworks of looped computation graphs. Similarly, training-induced methods like Coconut show that continuous thought loops can solve logical reasoning tasks (ProsQA, PrOntoQA) through latent breadth-first search, while compressed-state methods achieve parity with explicit CoT on mathematical reasoning (GSM8K). In symbolic reasoning and graph algorithms, recurrent models bridge neural and algorithmic computation. De Luca and Fountoulakis [21] show that looped Transformers with graphspecific attention heads can simulate classical algorithms (BFS, DFS, shortest-path) within bounded memory. This capability extends to training-induced recurrence: models with planning 13 tokens [107] demonstrate improved performance on multi-hop reasoning by creating hierarchical computation structures. The decomposition of reasoning into specialized tokens (<memory>, <reason>) [50] further enhances performance on tasks requiring both retrieval and logical inference. In optimization and meta-learning, works like [13, 30] prove that looped models implicitly implement multi-step gradient descent, revealing deep connections between recurrence and optimization. This theoretical insight explains why both architectural loops and training-induced continuous thoughts converge on similar computational patterns: they are fundamentally performing iterative refinement analogous to optimization algorithms. These applications demonstrate that recurrent reasoningwhether achieved through architecture or trainingprovides general framework for complex computation. The convergence of different approaches on similar capabilities suggests that the key insight is not the specific implementation but ensuring sufficient iterative depth for the task at hand. 3.2. Horizontal Recurrent: Hidden state-based Methods As previously mentioned, activation-based approaches focus on expanding layer depth in networks. However, deeper networks inevitably encounter challenges such as gradient explosion or vanishing. In contrast, the temporal dimension can be readily expanded to millions of tokens. From theoretical perspective, the temporal dimension can also be conceptualized as form of depth, which raises an important research question: How can we effectively expand the latent reasoning process along the temporal dimension? Traditional approaches like Transformers handle temporal information by storing previous token inputs as key-value pairs in the KV cache. While this preserves most information, it suffers from unbounded memory consumption that scales linearly with sequence length. To address this challenge, we can compress previous information into fixed-size vector or matrix, similar to RNNs. When working with hidden states, there are two primary approaches to enhance their expressiveness: (1) the Linear-State recurrence approach, which applies update and decay rules to the hidden states, and (2) Gradient-State recurrence approach, treating hidden states as online-learning parameters and optimizing them using online learning methods. Notably, although these methods have not yet produced evidence demonstrating enhanced reasoning capabilities, their theoretical properties suggest they may play significant role in the future, as they represent form of iterative processing that is conceptually similar to layer stacking. 3.2.1. Linear-State Recurrence For the first approach, models such as Mamba-2 [20], GLA [116], RWKV-6 [74], and HGRN2 [77] represent early attempts in this direction. matrix-valued hidden state 𝑆 is transmitted and updated along the temporal dimension. At each time step, the hidden state undergoes global decay, followed by updates incorporating information from the current time step. Remarkably, these diverse linear attention models can be unified under general framework of associative recurrent neural networks with matrix-valued hidden states [117, 118]. Given matrix-valued hidden state S𝑡 R𝑑𝑛 and current input x𝑡 R𝑑, these models follow the general form: S𝑡 = S𝑡1 + 𝒌𝒕𝒗 𝒕 , o𝑡 = S𝑡𝒒𝑡, (recurrence) (memory read-out) (11) (12) 14 where represents an associative operator (e.g., Hadamard product, matrix multiplication), and M𝑡, 𝒗𝑡, 𝒌𝑡, 𝒒𝑡 are functions of the current input 𝒙𝑡. The use of associative operators enables parallel scan calculations of S1, . . . , S𝐿, facilitating efficient training. Table 2 illustrates how various models instantiate this framework. However, more profound perspective emerges when interpreting this state evolution through the lens of online optimization gradient. key insight comes from DeltaNet [118], which perfectly exemplifies this duality. While its state update rule has closed-form algebraic expression (see Table 2 linear recurrent attention part), it is mathematically equivalent to applying single gradient descent step to an online regression objective (𝑺) = 1 2 𝑺𝒌𝑡 𝒗𝑡 2 2. This gradient-state recurrence view is conceptually transformative. It reframes the temporal evolution of the hidden state 𝑺𝑡 as form of iterative refinement, akin to training neural network layer. In this sense, the state matrix 𝑺 is effectively treated as dynamic, \"fast weight\" layer that is updated at each step based on local objective. This perspective conceptually unifies the \"temporal\" recurrence of hidden-state models with the \"depth\" recurrence of activation-based models, suggesting shared underlying principle of iterative processing for latent reasoning. 3.2.2. Gradient-State Recurrence While linear-state models rely on predetermined decayadd rules, gradient-state methods treat the hidden matrix as set of fast-adapting parameters updated by learnable optimizer. Each token triggers lightweight descent step that steers the state toward the current keyvalue target, allowing the model to internalize task-specific dynamics on the fly. This view shifts the design space from choosing fixed linear kernels to selecting optimization algorithms (SGD, Adam-like, second-order, etc.), opening rich continuum of memory behaviors governed by learning-rate schedules, momentum terms and higher-order corrections. This insight paved the way for second research trajectory that abandons closed-form descriptions entirely, in favor of direct online learning formulations [35, 52, 97]. This line of work, progressing from TTT (implementing SGD-like dynamics) [97] to Titans (incorporating Adam-like behaviors) [3] and ATLAS (utilizing Muon optimization principles) [4], formulates the state update explicitly as gradient-based optimization step. Despite their different origins, these approaches converge conceptually and can be understood through the general update rule: S𝑡 = 𝛼𝑡S𝑡1 𝜂𝑡𝑺ℓ(𝑺𝑡1; 𝒌𝑡, 𝒗𝑡) (13) While powerful, this approach introduces significant challenges for parallelization. Unlike linear recurrent models that can be parallelized efficiently with single scan operation, the gradient ℓ at step 𝑡 depends on the previous state 𝑺𝑡1. This inherent sequential dependency prevents parallel computation across the entire sequence length. Furthermore, these recurrent updates are embedded within complex architectural blocks that include standard components like LayerNorm and residual connections, making it difficult to fuse the computation into single, hardware-efficient kernel. To overcome these limitations, practical solution known as chunk-wise parallelization has been widely adopted [3, 97, 117]. This strategy balances expressiveness and efficiency: Intra-chunk Parallelism: Within small, fixed-size block (chunk) of the sequence, the gradients for all tokens are computed in parallel with respect to the same initial state (the final state of the previous chunk). This breaks the sequential dependency within the chunk, allowing for efficient, batched computation. 15 Method Unified Memory-update Rule Linear-State Recurrence Linear Attn [53] RetNet/Lightning [98] GLA [116] Mamba-2 [20] HGRN-2 [77] 𝑡 𝑺𝑡 = 𝑺𝑡1 + 𝒌𝑡𝒗 𝑺𝑡 = 𝛾𝑺𝑡1 + 𝒌𝑡𝒗 𝑺𝑡 = 𝑺𝑡1Diag(𝒂𝑡) + 𝒌𝑡𝒗 𝑺𝑡 = 𝛼𝑡 𝑺𝑡1 + 𝑏𝑡 𝒌𝑡𝒗 𝑺𝑡 = 𝑺𝑡1Diag(𝒂𝑡) + (1 𝒂𝑡)𝒗 𝑡 𝑡 𝑡 𝑡 Linear/Gradient-State duality DeltaNet [118] G-DeltaNet [117] Gradient-State Recurrence TTT [97] Titans [3]* Lattice (orth.) [52] Moneta [5] Yaad (Huber) [5] Memora [5] OmegaNet [4] State-update: 𝑺𝑡 = 𝑺𝑡1 (𝐼 𝛽𝑡 𝒌𝑡 𝒌 Optimization: 𝑺𝑡 = 𝑺𝑡1 𝛽𝑡𝑺 State-update: 𝑺𝑡 = 𝛼𝑡𝑺𝑡1 (𝐼 𝛽𝑡 𝒌𝑡 𝒌 Optimization: 𝑺𝑡 = 𝑺𝑡1 𝛽𝑡𝑺 1 2 𝑡 ) + 𝛽𝑡 𝒌𝑡𝒗 𝑡 𝑺𝒌𝑡 𝒗𝑡 2 2 𝑡 ) + 𝛽𝑡 𝒌𝑡𝒗 𝑡 + 𝜆 𝑺 𝛼𝑡𝑺𝑡12 𝐹 𝑺𝒌𝑡 𝒗𝑡 2 1 2 (cid:17) 𝑺𝑖,𝑡1𝑺 𝑖,𝑡1 𝑺𝑖,𝑡1 2 𝑺𝑡 = 𝑺𝑡1 𝜂𝑡 𝑺ℓ(𝑺𝑡1; 𝒌𝑡, 𝒗𝑡) 𝑺𝑡 = 𝛼𝑡 𝑺𝑡1 𝜂𝑡 𝑺ℓ(𝑺𝑡1; 𝒌𝑡, 𝒗𝑡) (cid:16) 𝑺𝑖,𝑡 = 𝑺𝑖,𝑡1 + 𝛼𝑖,𝑡 𝐼 𝒉𝑡 𝑺𝑡 = Norm𝑞(cid:0)𝛼𝑡𝑺𝑡1 𝜂𝑡𝑺ℓ𝑝(𝑺𝑡1; 𝒌𝑡, 𝒗𝑡)(cid:1) 𝑺ℓ2, 𝛿𝑡 𝑺ℓ1, otherwise 𝑺𝑡 = 𝑎𝑡𝑺𝑡1 𝜂𝑡 𝑺𝑡 = Softmax(cid:0)𝛼𝑡 log 𝑺𝑡1 𝜂𝑡𝑺ℓ2 (𝑺𝑡1; 𝒌𝑡, 𝒗𝑡)(cid:1) (cid:13) (cid:13)𝑺𝑡1𝜙(𝒌𝑖) 𝒗𝑖 𝑆(𝒌𝑡) 𝒗𝑡 𝛿𝑡 𝑺𝑡 = 𝛼𝑡𝑺𝑡1 𝛾𝑖𝑺 𝑡 (cid:13) (cid:13) (cid:40) 2 Atlas [4] 𝑺aux 𝑡 = 𝜃𝑡𝑺aux 𝑡1 𝜂𝑖𝑺 (cid:13) (cid:13)𝑺𝑡1𝜙(𝒌𝑖) 𝒗𝑖 (cid:13) (cid:13) 2 2 𝑖=𝑡𝑐+1 𝑡 𝑖=𝑡𝑐+1 * Titans omits momentum and norm-adaptation terms for brevity. 𝑺𝑡 = 𝛼𝑡𝑺𝑡1 + NS5(𝑺aux 𝑡 ) Table 2. Unified hidden-state and optimization-based memory updates. Each model is recurrence on matrix memory 𝑺𝑡: apply decay, projection or an optimization step to 𝑺𝑡1, then add an outer-product or gradient correction (read-out: 𝒐𝑡 = 𝑺𝑡𝒒𝑡). Symbols: For uniformity, 𝛼𝑡 generally denotes the gate controlling the retention of the previous state, while 𝜂𝑡 denotes the learning rate. An important exception is DeltaNet and Gated-DeltaNet, whose learning rate or writing strength is denoted by 𝛽𝑡. Additionally, 𝛾𝑖 is the weight for each tokens gradient in the OmegaNet context window, and 𝜃𝑡 is the momentum decay term in Atlas. All these parameters are data-/channel-dependent scalars, typically in (0, 1). 𝛿𝑡 is the Huber threshold; ℓ𝑝, ℓ1, ℓ2 are gradients w.r.t. (𝒌𝑡, 𝒗𝑡); Norm𝑞() is 𝑞-norm normalization (Moneta); 𝜙() denotes polynomial/high-order feature map; NS5() is the Muon/NewtonSchulz 2nd-order update (Atlas); 𝐼 𝒔𝒔 𝒔 2 is the orthogonal projector in Lattice. 16 Inter-chunk Recurrence: The overall sequential nature of the model is maintained between chunks. The final state of one chunk is passed recurrently to become the initial state for the next, forming chain at the chunk level. Extending the optimization perspective beyond internal state updates, Zhu et al. [134] introduce Soft Reasoning, which treats the first-token embedding as controllable latent variable. By injecting Gaussian noise and maximizing an Expected-Improvement objective via Bayesian optimization, the method dynamically searches the hidden space for reasoning trajectory. Although current research has not yet produced evidence demonstrating enhanced reasoning capabilities in these models, their theoretical properties suggest significant potential, particularly for enabling self-iteration in the absence of input tokens. 3.2.3. Training-induced Hidden-State Conversion Building on the success of training-induced recurrence for activation-based models, parallel line of work shows that fixed-architecture Transformers can be converted, rather than redesigned into hidden-state (RNN/SSM) models through targeted fine-tuning or distillation. These methods preserve most of the teachers parameters while replacing quadratic self-attention with sub-quadratic mixers that maintain single recurrent state, thereby inheriting constant-memory inference. Cross-architecture distillation. Earlier Transformer-to-RNN (T2R) conversions replaced softmax with trainable linear kernels but required heavy retraining. SUPRA [67] refines this idea: starting from strong Llama-2/Mistral checkpoints, it swaps attention for GroupNorm-stabilized linear kernels and fine-tunes on 20 tokens, reaching competitive accuracy with only 5% of the cost of pretraining recurrent model from scratch. MOHAWK [7] introduces three-phase procedure (matrixorientation hidden-state alignment knowledge distillation) that transfers pretrained Transformer into Mamba-2 state-space model using only 3B tokens, yielding Phi-Mamba which outperforms all prior open recurrent LMs of similar size. The same recipe scales to 18 models in Llamba [8], demonstrating that recurrent students can match Llama-3 teachers with 0.1% of original training compute while enabling larger batch sizes and higher. Low-rank linearization. LoLCATs [130] shows that high-fidelity conversion does not need full-model updates. It first matches every attention head with sliding-window linear mixer (attention transfer), then restores any residual loss with LoRA adapters touching just 0.2% of weights. This two-step \"low-rank linearization\" narrows the MMLU gap to 1% for 8 models and scales to 70405 parameters within single day of training. Gated conversions. Liger [56] repurposes the pretrained key matrix to build per-channel forget gates, yielding gated recurrent student that recovers 93% of teacher performance with only 0.02% of the original token budget and no extra parameters beyond LoRA. 4. Mechanistic Interpretability This section demonstrates the feasibility of Latent CoT and justifies the use of layers as indicators to facilitate the implementation of Latent CoT. As previously discussed, the majority of latent reasoning behaviors in large language models emerge through operations across layers, both 17 in temporal and spatial dimensions. This raises fundamental question: Are layers the basic computational units of reasoning? Mechanistic Interpretability providing tools like Probing and Circuit Analysis, enables us to shift from observing model behavior in reasoning to understanding its mechanism. This is crucial to unveil the role of Transformers layers in reasoning. In this section, we first summarize existing work from an interpretability perspective to address whether layer stacking represents form of Latent CoT. Next, we analyze how layers function as latent CoT by examining aspects such as layer specialization and inter-layer information flow. Finally, we illustrate the limitations of expressing CoT using layer representations. 4.1. Do Layer Stacks Reflect Latent CoT? The concept of Chain of Thought (CoT) reasoning allows models to generate sequential thought tokens, giving them more time and computational resources before arriving at an answer. This idea has been influential in shaping new paradigms for scaling inference in thinking models, such as OpenAI o1 [49] and DeepSeeks R1 [41]. In parallel, theres growing evidence suggesting that the stacking of layers in neural networks similarly impacts reasoning capabilities, indicating layer-based hidden CoT. This relationship between layer depth and latent reasoning is critical for understanding the models potential reasoning ability. At macro level, series of studies have found close correlation between layer depth and the reasoning capabilities of the model. Yu [123] found that the models Implicit CoT capabilities are strictly limited by the number of network layers. For 5-step reasoning task, although intermediate results emerge within some layers, the final reasoning outcome fails to emerge due to an insufficient number of layers. Guo et al. [42] discovered that at least 2-3 layers are required to form complete two-step reasoning chain within the model. Insufficient layers or inadequate depth in subsequent layers will hinder the ability to perform multi-hop reasoning. In addition, some studies have explored the structural advantages brought by layer depth from the perspective of representational capacity. Saunshi et al. [83] formally establish that any K-layer transformer performing m-step CoT reasoning can be simulated by an (L+O(1)) layer transformer through iterative forward passes. Merrill and Sabharwal [68] demonstrate that increasing Transformer depth significantly enhances reasoning abilities, enabling complex tasks like language recognition and graph connectivity that fixed depths cannot achieve. This theorem fundamentally establishes that layer depth serves as the primary bottleneck for latent reasoning capacity, where the achievable CoT step length scales linearly with layer count. At micro level, studies commonly reveal clear correspondence between specific layers and tasks within CoT reasoning. Just like the various steps in CoT, different layers play distinct roles in the reasoning process, while the overall reasoning depth (layer count) influences the final reasoning performance. series of interpretability studies have revealed significant functional differentiation across layers of varying depths in reasoning tasks [11, 46]. Layer depth affects the completeness of reasoning chains [42], which expand in parallel and grow exponentially [109], with intermediate information being integrated and transmitted across depths [124]. These observations at the micro-level strongly suggest structured functional differentiation across layers, each performing distinct computational roles analogous to steps in an explicit CoT. To better understand how this latent chain emerges from layer stacks, it is necessary to delve deeper into the specific mechanisms of layer specialization and inter-layer information flow. 18 4.2. Mechanisms of Latent CoT in Layer Representation Following the evidence from the micro-level analysis, we formalize the theory of Layer Specialization as foundational framework for interpreting Latent CoT. This perspective posits that individual layers within Transformer models systematically specialize to support distinct reasoning operations, collectively forming an implicit computational pipeline analogous to an explicit CoT. Next, we articulate the role each layer group (shallow, intermediate, and deep) plays in supporting this latent reasoning structure, followed by discussion of how information is propagated across these specialized layers. Theory of Layer Specilization The Transformer model consists of alternating self-attention and feed-forward network (FFN) modules. natural assumption is that different layers play distinct roles in reasoning tasks [39, 89, 131]. series of interpretability studies are focusing on uncovering how these layers work together to build and convey the underlying CoT processes. From shallow to deep layers, the model exhibits clear division of labor. The reasoning process transitions from specific, local, and syntactic information in the shallow layers to rich semantic integration and the merging of reasoning paths in the intermediate and deep layers. This differentiated structure leads us to consider each layer as the smallest functional unit in the reasoning process. Shallow Layers: Basic representational processor of Latent CoT. Transformers shallow layers perform initial text processing, laying the groundwork for higher-level semantic analysis and reasoning. Functionally, the shallow layers primarily process local information, syntactic structures [54], and surface patterns [34], perform initial data transformations [14], and form early circuit primitives [59, 102, 104]. Additionally, studies indicate that shallow layers are responsible for storing and recalling factual knowledge [93, 115] and bridging entity parsing in multi-hop reasoning tasks [9, 86, 115]. In summary, shallow layers are crucial for processing fundamental information and factual knowledge, with their ability to establish bridging variables directly influencing the models reasoning performance. Intermediate Layers: Core of Latent CoT. Intermediate layers play pivotal role in complex, multi-step reasoning tasks for the following reasons: (1) Intermediate layers form specialized sub-circuits dedicated to reasoning functions, (2) Intermediate layers exhibit superior representational capabilities, and (3) Activations in intermediate layers have decisive impact on reasoning outcomes. Intermediate layers contain specific, identifiable computational sub-circuits specialized for distinct reasoning sub-tasks. These circuits typically involve coordinated interactions between attention heads and MLP modules. Wang et al. [105] reverse-engineer the internal algorithm by which GPT-2 identifies indirect objects in sentences. They identify mid-layer attention sub-circuit responsible for entity tracking and pronoun resolution, showing that intermediate layers carry out essential structured reasoning. Similarly, series of studies have identified potential reasoning circuits within the intermediate layers [43, 92, 105, 112]. The formation of these circuits is emergent, representing efficient computational patterns spontaneously learned by the model from large-scale data [6, 100]. Intermediate layers exhibit unique characteristics in representation, not only demonstrating powerful expressive capabilities but also playing crucial role in knowledge storage and encoding. The performance of intermediate layer embeddings can exceed that of final layer embeddings by up to 16% in text embedding tasks, and show consistency across different model architectures and scales [93]. Some researchers believe that this powerful representation capabil19 ity stems from the objective function used during pretraining. The autoregressive paradigm induces an information bottleneck at intermediate depths of the model, forcing it to distill the most essential and salient information [57, 105]. Intermediate layers have causal influence on final reasoning outcomes. Correct activation of these layers is necessary for the model to produce valid inferences. series of studies identify specialized neurons in intermediate layers and perform causal interventions. They find that enhancing activations significantly improves reasoning performance, while suppressing activations leads to decline in reasoning ability [31, 108]. Intermediate layer representations, acting as bridging entities, also play causally critical role in multi-step reasoning outcomes [115]. The functional specialization of intermediate layers makes their correct activation critically decisive for the final reasoning outcomes. Deep Layers: Output Refinement and Decision-making of Latent CoT. The deep layers of Transformer models lie at the end of the information processing flow, play pivotal role in output optimization and decision-making. Deep layers receive rich representational information from intermediate layers and perform semantic transformation tailored to specific downstream tasks [57, 93], performing more complex logical integration and determine the final answer [27, 43]. However, several layer pruning studies indicate that deeper layers exhibit characteristics such as poor training performance, limited functionality, and reduced representation learning capabilities [19, 87, 125]. Existing research attributes this degradation to variance issues in Pre-Layer Normalization and the frequent degeneration of attention matrices. Sun et al. [96] suggest that the exponential growth of output variance in Pre-LN and derivatives approaching the identity matrix in deeper layers are the main causes of layer degradation. Sanyal et al. [81] found that attention matrices in deeper layers frequently degenerate, often collapsing into nearly rank-one single-column patterns. We believe that maintaining the effectiveness of each layer during pre-training is crucial. Enhancing the functionality of layers, especially deep layers, is future direction to improve the models reasoning abilities. Theory of Information Flow Given the layer specialization, the flow of information across these layers is crucial for reasoning process. Stolfo et al. [94] quantify the indirect contributions of MLP and attention modules to clarify internal information flow pathways in LLM during arithmetic tasks. The results highlight the crucial role of the attention mechanism in inter-layer information flow during reasoning, which transmits computational information from early processing layers to the final token. Wang et al. [103] discover generalizing circuit emerging during the grokking process. This circuit enables cross-layer information flow, with lower layers extracting bridge entities and higher layers conducting reasoning. Yu et al. [124] present neuron-level investigation into the logits flow of LLMs during multi-hop knowledge prediction. With \"back attention\" mechanism, hidden information can be effectively transmitted from higher layers to lower layers, enhancing models reasoning ability. 4.3. Turing Completeness of Layer-Based Latent CoT Turing completeness is fundamental concept in theoretical computer science. It describes the ability of system to perform any computation that can be performed by universal Turing machine. computational system is considered Turing complete if it can simulate the computational process of any Turing machine. In this section, we first attempt to answer whether the Vanilla Transformer is Turing complete. Next, we summarize what modifications are needed to make the Transformer achieve Turing completeness. 20 Proof of Turing completeness in model architectures Before the emergence of Transformers [101], Recurrent Neural Networks (RNN) [28, 51] were the dominant architecture for processing sequential data. Owing to their inherent recursive nature, RNNs were theoretically proven to be Turing complete as early as 1996, setting precedent for neural networks to achieve universal computational capabilities [91]. Subsequently, LSTM [45] and GRU [17] were proposed to address the vanishing gradient problem in RNNs, enabling more stable memory states over long sequences. series of research efforts have attempted to prove the Turing completeness of Transformers from an architectural perspective under certain assumed constraints. Pérez et al. [75] formally proved for the first time that the Transformer architecture is Turing complete, possessing the universal capability to execute any computable function. However, the validity of this proof relies on three crucial theoretical assumptions: Arbitrary Precision, Positional Encodings, and Hard-Max Attention. Following this idealized and groundbreaking proof, more researchers began to consider the conditions under which Transformer can achieve Turing completeness. Further, Li and Wang [58] proved for the first time that Turing completeness can be achieved under constant numerical precision. This study directly addresses the controversial assumption of infinite precision from earlier proofs, bringing the theoretical model closer to the computational constraints of the real world. Proof of Turing completeness with Chain-of-Thought Additionally, another research path focuses on achieving more universal computational capabilities through CoT reasoning. Functionally, CoT transforms the Transformer from limited context window into dynamic computational tape. The model employs an autoregressive approach, writing each steps calculation result on notepad and reusing the intermediate results in subsequent calculations. Qiu et al. [78] proposed that prompting is Turing complete. They demonstrate that single, finite-sized Transformer, as long as it is given suitably constructed prompt, can compute any computable function.This is the first time the Turing completeness of Transformers has been revealed from the perspective of prompts. Li et al. [61] discovered that Transformer with constant depth can simulate Boolean circuit of size T, provided it is allowed to perform T-step CoT reasoning. These studies on the Turing completeness of CoT indicate shift in the definition of general computation. Generality does not necessarily need to be embedded within the model architecture; it can also be achieved through interaction paradigms using fixed-depth models. Enhancing Transformer for Turing Completeness Beyond theoretical proofs, series of studies have enhanced the expressive power of Transformers through architectural modifications, aiming to approach their theoretical limit of Turing completeness. series of studies have introduced recurrent mechanisms to break through the fixed depth constraints of Transformers, as discussed in Section 3. Additionally, some studies have incorporated external memory into Transformers [10]. Unifying View of Implicit and Explicit Reasoning The reasoning process of Transformers can be viewed as thought unfolding across two dimensions. The well-known CoT unfolds along the horizontal sequence dimension, creating visible reasoning steps. Meanwhile, the networks layer-by-layer computation can be seen as implicit unfolding and refinement of each token along the vertical depth dimension. As discussed above, CoT acts as the scratchpad between questions and answers, allowing the model to perform reasoning in an auto-regression mode, theoretically possessing Turing completeness. Meanwhile, each layer of the Transformer 21 represents an implicit reasoning step, progressively optimizing the prediction of the next token. Thus, both methods represent form of computational expansion, differing fundamentally in whether they unfold across the sequence or through the networks depth. Operation Storage Standard CoT Layer-based Latent CoT"
        },
        {
            "title": "Explicit Tokens\nIn the Sequence\nHidden States",
            "content": "Resource Constraint Optimization Objective Context Window End-to-end Task"
        },
        {
            "title": "Next Token\nPrediction",
            "content": "Table 3. comparison of Standard Chain-of-Thought (Horizontal Expansion) and Layer-based Latent CoT (Vertical Expansion) across key computational dimensions. Moreover, series of studies have sought to break the boundary between implicit CoT and explicit CoT. Chowdhury and Caragea [18] propose Universal Transformers (UTs), which approach Turing completeness by implementing adaptive computation depth. The core idea of UTs is to repeatedly apply the same Transformer block across multiple layers or computational steps, thereby introducing form of recurrence into the architecture. Zelikman et al. [126] integrate the CoT between layers and CoT between tokens, allowing for the output of intermediate thought processes among tokens as well. Furthermore, they proposed Fast Quiet-Star, which retains the token-level thinking trace while reducing computational cost. Dong et al. [25] reframed next-token prediction as reasoning task trained using Reinforcement learning, where the model receives verifiable rewards for correctly predicting the next token for given context. 5. Towards Infinite-depth Reasoning Infinite-depth reasoning refers to an AIs ability to devote unbounded thinking time to refine and perfect solution irrespective of output length. In this section we first introduce spatial infinite-depth reasoning and then temporal infinite-depth reasoning. Spatial infinite-depth reasoning is realized by diffusion models that begin with fully masked or noisy draft of the entire output and iteratively denoise it in parallel: each pass has bidirectional access to the full context, enabling global planning, logical consistency across distant segments, and iterative self-correction, with the number of refinement steps adjustable at inference time to trade speed for depth of reasoning. Temporal infinite-depth reasoning, by contrast, relies on autoregressive extensions that generate tokens one at time in left-to-right stream and can in principle produce arbitrarily long sequencesbut their irreversible early decisions can accumulate errors and limit true global coherence. 5.1. Spatial Infinite Reasoning: Text Diffusion Models Text diffusion models represent paradigm shift for complex reasoning tasks, offering an alternative to traditional AR generation. Unlike sequential models that generate text token-bytoken, diffusion models enable spatial infinite reasoning through iterative global refinement. This approach allows models to engage in holistic planning and develop logical connections across the entire reasoning chain simultaneously, overcoming the limitations of sequential generation where early decisions become irreversible constraints. The connection between diffusion models and infinite depth reasoning lies in their iterative refinement capacity. While traditional models 22 Method Unified Latent-Update Formula Masked Diffusion Models (Temporal-only) D3PM [1], SEDD [64], RADD [73] MD4 [90], Simple-MDM [80], MDM [71] MMaDA [114], IRED [26] Masked Diffusion Models (With Cache) LLaDA [72], dKV-Cache [65], DoT-SEDD [119] dLLM-Cache [63], d1-LLaDA [132], DCoLT [48] LLaDA 1.5 [133], MGDM [120] Embedding-based Diffusion Models Diffusion-LM [60], CDCD [24] Plaid [40], DoT-Plaid [119] TESS [66], TESS 2 [99], Bit Diffusion [15] Hybrid AR-Diffusion Models DiffuLLaMA [36], Dream [121] L2D [12], Gemini Diffusion [33] Mercury [55] 𝒙𝑙 𝑡+ (𝑖) = 𝑓 (𝒙𝑙 𝑡 (𝑖)) 𝑡 = 𝑓𝜏(𝒙𝑙 𝒙𝑙+1 S𝑙 𝑡+1 𝑡, S𝑙 𝑡) 𝑡 (𝑖), S𝑙 (𝑖) 𝑔𝜏(𝒙𝑙 𝑡 (𝑖)) 𝒙𝑙 𝑡+ = 𝑓 (𝒙𝑙 𝑡, 𝜖𝑡) 𝒙𝑙+1 𝑡 = 𝑓𝜏(𝒙𝑙 𝑡, S𝑙 𝑡) S𝑙 𝑡 (𝑖), S𝑙 (𝑖) = 𝑔𝜏(𝒙𝑙 𝑡+1 + AR prefix caching 𝑡 (𝑖)) Table 4. Text diffusion models organized by cache integration capabilities, showing the evolution from temporal-only updates to spatial-temporal frameworks with KV cache mechanisms. are constrained by fixed computational depth, diffusion models can theoretically refine reasoning through unlimited denoising steps. Each step provides additional reasoning depth, allowing progressive elaboration from high-level plans to detailed solutions. We organize text diffusion models into three architectural families: Masked Diffusion Models that enable bidirectional context awareness, Embedding-based Diffusion Models that preserve structured reasoning while enabling global refinement, and Hybrid AR-Diffusion Models that combine diffusion and AR paradigms. 5.1.1. Masked Diffusion Models Masked Diffusion Models (MDMs) exemplify spatial reasoning in text generation. These models operate on complete text sequences where tokens are initially masked, requiring simultaneous prediction of all missing tokens based on bidirectional context. This provides full access to the entire information landscape at each denoising step. MDMs adopt latent update mechanism driven by an explicit token-level mask 𝑀𝑡 at each step 𝑡, the corresponding unified latent-update formulas are described in Table 4. For temporalonly MDMs, the latent update formula, 𝒙𝑙 𝑡 (𝑖)), describes how the representation of an individual token, 𝒙𝑙 𝑡 (𝑖), is updated from denoising step 𝑡 to 𝑡 + 1 within specific layer 𝑙. This indicates direct token-level update, where the models focus is on iteratively refining the masked parts of the sequence. For MDMs with cache, two formulas describe the process. The (𝑖) = 𝑓 (𝒙𝑙 𝑡+ 23 Figure 5. An evolutionary graph of the text diffusion models, including three architectural families: Masked Diffusion Models, Embedding-based Diffusion Models, and Hybrid ARDiffusion Models. 𝑡 = 𝑓𝜏(𝒙𝑙 𝑡 𝑡, S𝑙 first formula, 𝒙𝑙+1 𝑡), describes the temporal transformation. It shows that at denoising step 𝑡, the output 𝒙𝑙+1 for layer 𝑙 + 1 is generated by function 𝑓𝜏 that takes the current token representations 𝒙𝑙 𝑡 as input. This indicates that the Transformer blocks processing for generating token representations directly leverages the KV-cache for spatial context. 𝑡 and the current KV-cache S𝑙 The iterative unmasking process enables sophisticated reasoning capabilities impossible in sequential generation. Some pioneers provide strong foundation for masked diffusion models and supporting reasoning tasks through better input, intermediate steps, and outputs. D3PM [1] goes beyond corruption processes with uniform transition probabilities, while SEDD [64] introduces the EBLO loss that naturally extends score matching to discrete spaces. Further refinements, like RADD [73], MD4 [90], and Simple-MDM [80], have streamlined training through hybrid masked losses, facilitating conversion of encoder models like BERT into effective generative reasoning systems. Besides, MMaDA [114] adopts unified diffusion architecture for multi-modal reasoning and aligns reasoning processes between textual and visual domains. Research has shown that MDMs can be scaled effectively, achieving strong performance and efficiency [71]. IRED [26] framed reasoning as an energy minimization process implemented through diffusion models. This approach enables iterative refinement from vague reasoning paths to precise solutions, particularly effective for multi-constraint problems. Energy diffusion demonstrated significant advantages over traditional methods in complex reasoning tasks. The LLaDA model [72] uses discrete random masking, enabling sophisticated capabilities like reverse-order reasoning. To accelerate masked diffusion language models, dKV-Cache [65] introduces delayed and conditioned keyvalue caching strategy that achieving up to 2-10 inference speedup. dLLM-Cache [63] introduces an adaptive caching strategy achieves up to 9.1 speedup over the standard inference method of LLaDA [72]. DoT-SEDD [119] subsequently generalized chain-of-thought (CoT) reasoning to the MDM framework, enhancing coherence 24 and accuracy through natural self-correction, with particular strengths in mathematical reasoning. The framework has been extended through Multi-Granularity Diffusion Modeling (MGDM) [120], which prioritizes difficult subgoals and achieves state-of-the-art results on complex planning tasks. d1-LLaDA [132] introduces diffu-GRPO, lightweight policy-gradient algorithm tailored to masked diffusion models that surpasses SFT across mathematical and planning benchmarks. LLaDA 1.5 [133] advances this line with VRPO, which combines unbiased Monte-Carlo budget allocation and antithetic sampling to sharply reduce the variance of ELBO-based preference optimization. DCoLT [48] applies outcome-based reinforcement learning by using probabilistic policy or ranking-based Unmasking Policy Module to jointly optimize the entire reasoning trajectory. 5.1.2. Embedding-based Diffusion Models Embedding-based diffusion models (EDMs) extend the paradigm of spatial reasoning by first mapping discrete token sequences into continuous token embeddings and then operating on these embeddings, where they are disrupted with Gaussian noise. The models denoise every latent vector using bidirectional context, enjoying complete visibility of the information landscape at each refinement step. Although this high-level objective mirrors that of MDMs, EDMs inhabit fundamentally different design space due to their continuous embeddings formulation. EDMs achieve latent update by applying noise to all tokens uniformly and allow denoising dynamics to determine recovery, the corresponding unified latent-update formulas are described in Table 4. The formula describes how the representation of the entire sequences tokens, 𝒙𝑙 𝑡, is updated from denoising step 𝑡 to 𝑡 + 1 for given layer 𝑙, enabling iterative refinement within the continuous latent space. The function 𝑓 represents the diffusion models core denoising network (typically Transformer), taking the current noisy embeddings 𝒙𝑙 𝑡 and noise term 𝜖𝑡 to compute the denoised embeddings. Conceptually, this process operates on the entire sequences embedding representation, rather than specific parts of individual tokens or their hidden states. Early EDM research emphasized controllable generation [60] and sequence-to-sequence tasks [24, 66], as well as efficient latent encodings of discrete sequences [15, 40, 99]. Plaid [40] systematically characterizes the capacity of this model family by deriving empirical scaling laws, closing the compute-efficiency gap with autoregressive language models to 64. DoT-Plaid [119] subsequently generalized chain-of-thought (CoT) reasoning to the EDM framework, allowing entire reasoning paths to evolve through iterative latent refinement and enhancing coherence and accuracy through natural self-correction, with particular strengths in mathematical reasoning. 5.1.3. Hybrid AR-Diffusion Models The third family explores direct integration of diffusion and autoregressive paradigms, creating hybrid systems that leverage complementary strengths. These models recognize that while diffusion excels at global planning, autoregressive generation remains effective for certain sequential dependencies. Hybrid AR-Diffusion models integrate autoregressive generation with diffusion-based latent refinement, combining the strengths of sequential coherence and bidirectional reasoning. The corresponding unified latent-update formulas are described in Table 4. The formula 𝒙𝑙+1 𝑡 = 𝑓𝜏(𝒙𝑙 𝑡) details the temporal transformation. Here, Transformer block 𝑓𝜏 refines token representations 𝒙𝑙 𝑡 from layer 𝑙 to 𝑙 + 1 at denoising step 𝑡. This refinement explicitly uses the 𝑡, S𝑙 25 current KV-cache S𝑙 𝑡. Crucially, this temporal update is enhanced by AR prefix caching, which brings in forward-context alignment from the already generated text 𝑥<𝑡. The second formula governs the spatial update of the KV-cache for individual token 𝑖. This update is driven by the function 𝑔𝜏 that takes the tokens representation 𝒙𝑙 𝑡 (𝑖) as input. The explicit inclusion of AR prefix caching in this formula indicates that the KV-cache update directly incorporates AR prefix caching mechanisms, enhancing the cache with forward context. This allows the model to dynamically stabilize reliable representations, focusing refinement on uncertain tokens while leveraging the strength of pre-existing sequential information. 𝑡 (𝑖) and its old cache S𝑙 DiffuLLaMA [36] introduces continual pre-training approach that converts existing autoregressive models (like GPT-2 and LLaMA) into diffusion models, which provides powerful and scalable tool for complex reasoning tasks that demand efficient and flexible text processing. L2D [12] uses modular design integrating diffusion pipeline with pre-trained autoregressive model, creating synergy between global reasoning and sequential fluency. The Dream model [121] leverages autoregressive initialization for training stability and context-adaptive noise scheduling. By leveraging diffusion method for parallel, coarse-to-fine token generation, commercial frameworks such as Gemini Diffusion [33] and Mercury [55] significantly boost the speed and efficiency of code processing in large language models. This provides more effective solution for latency-sensitive reasoning tasks like chain-of-thought and agentic workloads. These hybrid approaches represent promising direction, acknowledging that different reasoning aspects may benefit from different computational paradigms. 5.2. The optimization-Based Perspective: Trading Time for Depth The optimization-based perspective introduced in Section 3.2.2 suggests that time itself can be traded for network depth. When the hidden state S𝑡 is updated by gradient-like rule S𝑡 = S𝑡1 𝜂𝑡Sℓ(S𝑡1; k𝑡, v𝑡), each additional token performs one extra step of (stochastic) optimizer that refines an implicit layer. Consequently, processing longer sequence is mathematically equivalent to running the same layer for more optimization iterations, thereby yielding greater reasoning depth without adding parameters. This observation converts the long-context challenge into new question: how can we instantiate network of unbounded depth that remains trainable and efficient? 5.2.1. Towards an Infinitely Long Optimizer Network Recent work pursues three complementary strategies: Infini-attention: Munkhdalai et al. [70] attach compressive memory to every Transformer block. Each incoming segment updates this memory via lineardelta rule that asymptotically approaches the fixed-point of an associative array, allowing the model to stream infinitely long inputs with (1) memory. From the optimization viewpoint, the Infini-memory applies an online regression step to match past keys to their values, while its gated aggregation with local causal attention preserves short-range precision. Test-time training (TTT) and its descendants: Sun et al. [97] pioneered the idea of performing few steps of SGD on the hidden state during inference. Follow-up models like Titans [3], OmegaNet and Atlas [4], replace first-order updates with Adamor Muon-style optimizers and introduce chunk-wise parallelism so that 106token streams can be handled on modern accelerators. Empirically, Titans-S (250 M) already matches 1.3 Transformer on 1-shot recall 26 after only 1 optimization steps, demonstrating that deeper through time can substitute for deeper via layers. Implicit Fixed-Point RNNs: An orthogonal line of work revisits classical RNNs through the lens of implicit layers. Schöne et al. [84] show that iterating state-space block until convergence yields non-linear, non-diagonal transitions that recover the expressivity of general RNNs while retaining training parallelism. Practically, one runs only small, adaptive number of self-iterations ( 16 for most natural-language tokens), giving another route to unbounded depth: the model simply halts when additional refinement becomes irrelevant. 5.2.2. Unifying View All three families embody the same principle: Depth emerges from optimization over time. The hidden state plays the role of fast-weight layer whose parameters are refined either explicitly (TTT, Titans, Atlas), implicitly (fixed-point RNNs), or through an associative cache (Infini-attention). Longer sequences therefore unlock deeper reasoning. Crucially, chunk-wise scans and parallel fixed-point solvers keep the wall-clock cost nearly linear, enabling experiments with million-token contexts on single GPU. 6. Discussion and Conclusion This survey provides comprehensive overview of Latent CoT and reasoning, an emerging paradigm in AI reasoning. While large language models have demonstrated impressive reasoning using explicit CoT that verbalizes intermediate steps, this approach is limited by the expressive bandwidth of natural language. Latent CoT addresses this by shifting the entire reasoning process into the models continuous hidden state, aiming to enhance expressive power and performance. By operating in continuous space, the model is freed from the constraints of finite token vocabulary and can explore more efficient and powerful reasoning strategies that may not have direct linguistic equivalents. Latent reasoning methodologies primarily follow two paradigms: vertical and horizontal recurrence. Vertical recurrence, or activation-based methods, expands computational depth by iteratively refining information within the same set of layers, either through explicit architectural loops or induced through specialized training. In contrast, horizontal recurrence, or hiddenstate-based methods, expands the models temporal capacity by evolving compressed hidden state over long sequences, allowing for the integration of vast amounts of information. These approaches are complemented by mechanistic interpretability research, which examines how different network layers specialize to form an implicit computational pipeline analogous to an explicit CoT. Notably, this survey does not offer direct empirical comparison across these varied models. The field is developing rapidly, with different models being created under disparate training conditionssome are pre-trained from scratch, while others are adapted from existing foundation models via continual pre-training. Furthermore, most studies compare their models to non-reasoning LLM baselines rather than to each other. This lack of consistent training methodologies and standardized benchmarks currently makes direct, apple-to-apples comparison of 27 empirical results challenging. It is our hope that unified evaluation framework will emerge in the future to enable clearer assessment of the relative strengths of these approaches. The survey culminates by exploring the frontier of infinite-depth reasoning, which aims to give models the ability to use unbounded computational steps to refine solution. Text diffusion models are key innovation in this area, as they operate on the entire output sequence in parallel. This allows for global planning, iterative self-correction, and logically consistent reasoning processes that are not constrained by sequential, irreversible decisions. By unifying these perspectives, the survey charts the conceptual landscape of latent reasoning and points toward future directions in advanced AI cognition."
        },
        {
            "title": "References",
            "content": "1 Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 2 Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, and Tal Schuster. Relaxed recursive transformers: Effective parameter sharing with layer-wise lora. arXiv preprint arXiv:2410.20672, 2024. 3 Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 4 Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, and Vahab Mirrokni. Atlas: Learning to optimally memorize the context at test time. arXiv preprint arXiv:2505.23735, 2025. 5 Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization. arXiv preprint arXiv:2504.13173, 2025."
        },
        {
            "title": "6 Leonardo Berti, Flavio Giorgi, and Gjergji Kasneci. Emergent abilities in large language",
            "content": "models: survey. arXiv preprint arXiv:2503.05788, 2025."
        },
        {
            "title": "10 Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Ad-",
            "content": "vances in Neural Information Processing Systems, 35:1107911091, 2022."
        },
        {
            "title": "12 Edoardo Cetin, Tianyu Zhao, and Yujin Tang. Large language models to diffusion finetuning.",
            "content": "arXiv preprint arXiv:2501.15781, 2025."
        },
        {
            "title": "14 Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction\nheads: Provable training dynamics and feature learning in transformers. arXiv preprint\narXiv:2409.10559, 2024.",
            "content": "29 15 Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning, 2023. URL https://arxiv.org/abs/22 08.04202. 16 Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. 17 Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17241734. Association for Computational Linguistics, 2014. 18 Jishnu Ray Chowdhury and Cornelia Caragea. Investigating recurrent transformers with dynamic halt. arXiv preprint arXiv:2402.00976, 2024. 19 Róbert Csordás, Christopher Manning, and Christopher Potts. Do language models use their depth efficiently? arXiv preprint arXiv:2505.13898, 2025."
        },
        {
            "title": "20 Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms",
            "content": "through structured state space duality. arXiv preprint arXiv:2405.21060, 2024."
        },
        {
            "title": "21 Artur Back De Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped",
            "content": "transformers. arXiv preprint arXiv:2402.01107, 2024."
        },
        {
            "title": "22 Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser.",
            "content": "Universal transformers. arXiv preprint arXiv:1807.03819, 2018."
        },
        {
            "title": "23 Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to",
            "content": "internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024."
        },
        {
            "title": "25 Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei.",
            "content": "Reinforcement pre-training. arXiv preprint arXiv:2506.08007, 2025."
        },
        {
            "title": "26 Yilun Du, Jiayuan Mao, and Joshua B. Tenenbaum. Learning iterative reasoning through",
            "content": "energy diffusion. In International Conference on Machine Learning (ICML), 2024."
        },
        {
            "title": "30 Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar.\nCan looped transformers learn to implement multi-step gradient descent for in-context\nlearning? arXiv preprint arXiv:2410.08292, 2024.",
            "content": "30 31 Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. 32 Team Gemini. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. 2025. 33 Team Gemini. Gemini diffusion is our new experimental research model. 2025. 34 Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. 35 Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 1139811442. PMLR, 2023. 36 Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?i d=j1tSLYKwg8."
        },
        {
            "title": "38 Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian",
            "content": "flow networks, 2025. URL https://arxiv.org/abs/2308.07037."
        },
        {
            "title": "39 Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A.\narXiv preprint",
            "content": "Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv:2403.17887, 2024. URL https://arxiv.org/abs/2403.17887."
        },
        {
            "title": "40 Ishaan Gulrajani and Tatsunori B. Hashimoto. Likelihood-based diffusion language models,",
            "content": "2023. URL https://arxiv.org/abs/2305.18619."
        },
        {
            "title": "44 Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuan-\ndong Tian. Training large language models to reason in a continuous latent space. arXiv\npreprint arXiv:2412.06769, 2024.",
            "content": "31 45 Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9 (8):17351780, 1997. 46 Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. Towards mechanistic interpretation of multistep reasoning capabilities of language models. arXiv preprint arXiv:2310.14491, 2023. 47 Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models. 2024. URL https://arxiv.org/pdf/ 2411.04905. 48 Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models, 2025. URL https: //arxiv.org/abs/2505.10446. 49 Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024."
        },
        {
            "title": "52 Mahdi Karami and Vahab Mirrokni. Lattice: Learning to efficiently compress the memory.",
            "content": "arXiv preprint arXiv:2504.05646, 2025."
        },
        {
            "title": "54 Tassilo Klein and Moin Nabi. micse: Mutual information contrastive learning for low-shot",
            "content": "sentence embeddings. arXiv preprint arXiv:2211.04928, 2022."
        },
        {
            "title": "56 Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. Liger: Linearizing large language",
            "content": "models to gated recurrent structures. arXiv preprint arXiv:2503.01496, 2025."
        },
        {
            "title": "58 Qian Li and Yuyi Wang. Constant bit-size transformers are turing complete. arXiv preprint",
            "content": "arXiv:2506.12027, 2025."
        },
        {
            "title": "59 Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time\nseries forecasting. Advances in neural information processing systems, 32, 2019.",
            "content": "32 60 Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation, 2022. URL https://arxiv.org/ab s/2205.14217. 61 Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 1, 2024. 62 Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025. 63 Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching, 2025. URL https://arxiv.org/abs/2506.06295. 64 Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning, pages 3281932848, 2024. 65 Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025."
        },
        {
            "title": "68 William Merrill and Ashish Sabharwal. A little depth goes a long way: The expressive",
            "content": "power of log-depth transformers. arXiv preprint arXiv:2503.03961, 2025."
        },
        {
            "title": "74 Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman,\nEugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch:\nRwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 3,\n2024.",
            "content": "33 75 Jorge Pérez, Javier Marinkovic, and Pablo Barceló. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. 76 Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=NikbrdtYvG. 77 Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. 78 Ruizhong Qiu, Zhe Xu, Wenxuan Bao, and Hanghang Tong. Ask, and it shall be given: On the turing completeness of prompting. arXiv preprint arXiv:2411.01992, 2024. 79 Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, ZhiqiBai, JiakaiWang, Yuanxing Zhang, Xu Tan, Jie Fu, Jiamang Wang, Lin Qu, Wenbo Su, and Bo Zheng. D-CPT law: Domain-specific continual pre-training scaling law for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=JzKFN5fWOk."
        },
        {
            "title": "81 Sunny Sanyal, Ravid Shwartz-Ziv, Alex Dimakis, and Sujay Sanghavi. Inheritune: Training",
            "content": "smaller yet more attentive language models. arXiv preprint arXiv:2404.08634, 2024."
        },
        {
            "title": "86 Yuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel",
            "content": "reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024."
        },
        {
            "title": "88 Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi:\nCompressing chain-of-thought into continuous space via self-distillation. arXiv preprint\narXiv:2502.21074, 2025.",
            "content": "34 89 Guangyuan Shi, Zexin Lu, Xiaoyu Dong, Wenlong Zhang, Xuanyu Zhang, Yujie Feng, and Xiao-Ming Wu. Understanding layer significance in llm alignment. arXiv preprint arXiv:2410.17875, 2024. 90 Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. 91 Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. Journal of Computer and System Sciences, 50(1):132150, 1995. 92 Oscar Skean, Md Rifat Arefin, Yann LeCun, and Ravid Shwartz-Ziv. Does representation matter? exploring intermediate layers in large language models. arXiv preprint arXiv:2412.09563, 2024. 93 Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013, 2025."
        },
        {
            "title": "96 Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, and Shiwei Liu. The",
            "content": "curse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025."
        },
        {
            "title": "99 Jaesung Tae, Hamish Ivison, Sachin Kumar, and Arman Cohan. Tess 2: A large-scale",
            "content": "generalist diffusion language model. arXiv preprint arXiv:2502.13917, 2025."
        },
        {
            "title": "100 Cheng Tang, Brenden Lake, and Mehrdad Jazayeri. An explainable transformer circuit for",
            "content": "compositional generalization. arXiv preprint arXiv:2502.15801, 2025."
        },
        {
            "title": "103 Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. Grokked transformers are implicit reasoners:\nA mechanistic journey to the edge of generalization. arXiv preprint arXiv:2405.15071, 2024.",
            "content": "35 104 George Wang, Matthew Farrugia-Roberts, Jesse Hoogland, Liam Carroll, Susan Wei, and Daniel Murfet. Loss landscape geometry reveals stagewise development of transformers. In High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning, 2024. 105 Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. 106 Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. System-1.5 reasoning: Traversal in language and latent spaces with dynamic shortcuts. arXiv preprint arXiv:2505.18962, 2025. 107 Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=wi 9IffRhVM. 108 Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. Unveiling factual recall behaviors of large language models through knowledge neurons. arXiv preprint arXiv:2408.03247, 2024."
        },
        {
            "title": "111 Haoyi Wu, Zhihao Teng, and Kewei Tu. Parallel continuous chain-of-thought with jacobi",
            "content": "iteration. arXiv preprint arXiv:2506.18582, 2025."
        },
        {
            "title": "117 Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2",
            "content": "with delta rule. arXiv preprint arXiv:2412.06464, 2024."
        },
        {
            "title": "118 Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear\ntransformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024.",
            "content": "36 119 Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models, 2024. URL https://arxiv.org/abs/2402.0 7754. 120 Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning, 2025. URL https://arxiv.org/abs/2410.14157. 121 Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dre am. 122 Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, and Di He. Enhancing auto-regressive chain-of-thought through loop-aligned reasoning. arXiv preprint arXiv:2502.08482, 2025. 123 Yijiong Yu. Do llms really think step-by-step in implicit reasoning? arXiv preprint arXiv:2411.15862, 2024."
        },
        {
            "title": "130 Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan\nSinghal, Krithik Ramesh, and Christopher Ré. Lolcats: On low-rank linearizing of large\nlanguage models. arXiv preprint arXiv:2410.10254, 2024.",
            "content": "37 131 Yang Zhang, Yanfei Dong, and Kenji Kawaguchi. Investigating layer importance in large language models. arXiv preprint arXiv:2409.14381, 2024. 132 Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning, 2025. URL https://arxiv. org/abs/2504.12216. 133 Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models, 2025. URL https://arxiv. org/abs/2505.19223. 134 Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, and Lin Gui. Soft reasoning: Navigating solution spaces in large language models through controlled embedding exploration. arXiv preprint arXiv:2505.24688, 2025."
        }
    ],
    "affiliations": [
        "FDU",
        "M-A-P",
        "NJU",
        "PKU",
        "PolyU",
        "RUC",
        "UCSC",
        "UW-Madison",
        "UoM"
    ]
}