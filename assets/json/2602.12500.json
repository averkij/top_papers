{
    "paper_title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
    "authors": [
        "André Storhaug",
        "Jiamou Sun",
        "Jingyue Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 0 0 5 2 1 . 2 0 6 2 : r Favia: Forensic Agent for Vulnerability-fix Identification and Analysis André Storhaug Norwegian University of Science and Technology, Trondheim, Norway andre.storhaug@ntnu.no"
        },
        {
            "title": "Jiamou Sun",
            "content": "CSIROs Data61, Canberra, ACT, Australia Frank.Sun@data61.csiro.au"
        },
        {
            "title": "Jingyue Li",
            "content": "Norwegian University of Science and Technology, Trondheim, Norway jingyue.li@ntnu.no Abstract Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using ReAct-based LLM agent. By providing the agent with pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude singlepass or similarity-based methods. We evaluate Favia on CVEVC, large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores."
        },
        {
            "title": "Introduction",
            "content": "Security vulnerabilities in open-source software pose persistent and growing threat to modern software ecosystems. Once vulnerability is disclosedtypically through Common Vulnerabilities and Exposures (CVE) entry, developers, security teams, and downstream users must This work was partially done during research visit at CSIRO Data61. 1 quickly identify the corresponding patch commit in the affected repository to assess exposure, apply fixes, and propagate updates. However, locating vulnerability-fixing commits remains challenging and time-consuming task. Large repositories such as the Linux kernel contain millions of commits, of which only tiny fraction correspond to security patches, making manual inspection or exhaustive analysis infeasible. Prior research has explored automated vulnerability fix detection using range of techniques, including handcrafted rules (Śliwerski, Zimmermann, and Zeller, 2005), traditional machine learning (X. Wang, K. Sun, et al., 2019), and deep learning models (Hoang, Lawall, et al., 2019). While these approaches have demonstrated promising results, they suffer from poor precisionrecall trade-offs. Conservative models miss most true patches, while aggressive models generate large numbers of false positives. Recent advances in large language models (LLMs) have enabled more semantically rich reasoning over code and natural language (John Yang et al., 2024), leading to LLM-based frameworks for vulnerability fix detection (X. Yang et al., 2025; Z. Wu et al., 2025). These systems leverage pretrained knowledge and structured prompting to reason about CVE descriptions and code changes. However, existing LLM-based approaches typically operate in single-pass manner over shallow context, making them prone to superficial associations such as keyword overlap or file-name similarity. Further, most evaluations rely on simplified or randomly sampled candidate sets which substantially underestimate the difficulty of real-world deployment scenarios. As most commits in repository are not security-relevant and not very similar to the true patch, it is easy to distinguish between patch and non-patch. As result, reported performance often fails to reflect behavior under realistic operating conditions. In this work, we propose Favia, forensic agent for vulnerability-fix identification and analysis. Favia adopts hybrid design that combines scalable candidate ranking with deep, agentbased semantic reasoning. We adopt this hybrid design to balance scalability and analytical depth. Lightweight candidate ranking efficiently narrows the vast search space of commits to manageable subset, while agent-based semantic reasoning enables fine-grained, evidence-driven analysis of each candidate. Given CVE and ranked list of candidate commits, Favia instantiates an LLM-based agent within code environment reflecting the repository state prior to each commit. Using iterative reasoning loop, the agent retrieves CVE and CWE reports, localizes affected components, navigates the codebase, and explicitly correlates code changes with the vulnerabilitys root cause, entry points, and impact. This iterative, evidence-driven process enables Favia to reason about indirect and non-trivial fixes that are difficult to capture with similarity-based or single-pass methods. To rigorously evaluate Favia, we construct CVEVC, large-scale dataset comprising over 8 million commits drawn from 3,708 real-world repositories associated with publicly disclosed CVEs. Using this dataset, we compare Favia against state-of-the-art traditional and LLM-based baselines under two evaluation settings: (i) random setting commonly used in prior work, and (ii) realistic setting where all repository commits are candidates. In addition to standard precision, recall, and F1-score metrics, we perform an in-depth analysis of agent behavior, tool usage, failure modes, and computational cost. Our results show that Favia consistently outperforms state-of-the-art traditional and LLMbased baselines. On the realistic dataset, Favia achieves the highest F1-scores across all tested models by preserving near-perfect recall (up to 0.98) while substantially improving precision over prior LLM-based approaches by up to 92%. This is achieved by perform iterative, evidencegrounded code navigation, enabling reliable causal alignment between CVE root causes and commit-level changes. We also demonstrate how randomly sampled commits dramatically inflate performance for all methods, with F1-scores increasing by up to 95%, masking real-world difficulty and narrowing apparent gaps between approaches. Failure analysis shows that over 2 85% of agent errors stem from superficial association between code changes and CVE, or misinterpretation of CVE root causes rather than limited exploration or tool access. Our contributions are threefold: We introduce Favia, an agent-based framework that performs iterative, evidence-grounded reasoning for vulnerability-fix identification. We provide high-quality dataset designed to support realistic evaluation of vulnerabilityfix detection. We present comprehensive empirical study showing that agent-based reasoning substantially improves precisionrecall trade-offs under realistic conditions, while also revealing key failure modes and efficiency trade-offs. The rest of the paper is organized as follows. Section 2 explains required background theory. Section 3 reviews related work. Section 4 details our agentic approach for patch classification. Section 3 introduces related work. We explain our experimental design in Section 5. Section 6 presents the experimental results. Section 7 discusses our results and limitations. Section 8 concludes the study and proposes future work."
        },
        {
            "title": "2.1 Patch Commit",
            "content": "A patch commit refers to specific change in software repository that addresses and resolves known vulnerability. These commits are typically associated with security advisories, such as CVEs (Common Vulnerabilities and Exposures), and are intended to mitigate or eliminate exploitable flaws in the codebase. Patch commits may involve direct modifications to the vulnerable code or indirect changes that affect related components, configurations, or control flows. As modern software is composed of deeply nested dependencies, when an upstream vulnerability is addressed, downstream maintainers must precisely identify the corresponding patch commits to assess exposure, backport fixes, and release secure updates (F. Li and Paxson, 2017). Thus, identifying patch commits is critical task in software security, as it enables downstream systems to track, verify, and propagate security fixes across dependent projects and distributions. Failing to do so has led to billion-dollar losses from malicious sabotage, such as the infamous Log4Shell (Doll et al., 2025) and SolarWinds (Jeong Yang, Lee, and McDonald, 2022) attacks."
        },
        {
            "title": "2.2 Vulnerability Fix Detection",
            "content": "The task of vulnerability fix detection, also referred to as patch detection, is the process of identifying which commits in version-controlled repository correspond to security patches. This problem is motivated by the need to automate vulnerability tracking and remediation in large-scale software ecosystems. key challenge is that all commits in repository are potential candidates, which makes the search space prohibitively large. For instance, as of January 2006, the Linux kernel contains over 1.4 million commits. Only small fraction of which are related to security fixes. As of December 30th, 2025, 9981 CVEs are reported in the linux-cve mailing list (Linux Kernel Organization, 2025). Moreover, vulnerability fixes are often not trivial. While some vulnerabilities, such as integer overflows, may be resolved with localized checks or type constraints, others, such as logic flaws, race conditions, or privilege escalation bugs, require broader contextual reasoning. These fixes 3 may span multiple files, involve indirect control paths, or rely on subtle behavioral changes that are not immediately evident in the diff. As result, distinguishing true vulnerability patches from unrelated or cosmetic changes demands deep understanding of both the vulnerability and the surrounding codebase. Listing 1: Patch commit 67f2cdd1for CVE-2014-100019. diff --git a/Pomm/Converter/PgLTree.php b/Pomm/Converter/PgLTree.php index 6377f8e..e408c7b 100644 --- a/Pomm/Converter/PgLTree.php +++ b/Pomm/Converter/PgLTree.php @@ -27,6 +27,9 @@ class PgLTree implements ConverterInterface */ public function toPg($data, $type = null) { } - + + + + } return sprintf(\"'%s'::ltree\", join('.', $data)); $data = join('.', $data); $data = str_replace(\"'\", \"''\", $data); $data = str_replace(\"\", \"\", $data); return sprintf(\"'%s'::ltree\", $data); 27 28 29 30 32 33 34 35 36 To illustrate this challenge, consider SQL injection vulnerability in the PgLTree converter of POMM (CVE-2014-100019), which allows remote attackers to execute arbitrary SQL commands due to improper handling of user-controlled input. The corresponding patch (shown in listing 1) mitigates the vulnerability by introducing proper string escaping (lines 31-34), directly addressing the root cause of the SQL injection. Such fixes are relatively easy to detect, as the mitigation mechanism clearly aligns with the vulnerability description. However, in practice, vulnerability-fixing commit may closely resemble non-patching commit, making the two difficult to distinguish. For example, listing 2 shows another commit from the same repository that appears to address similar SQL-related issue but modifies different, albeit superficially plausible, code location. Specifically, line 75 is changed into line 76, wrapping the array_shift($elts) in function call to stripcslashes. Despite its similarity (escaping slashes), this commit does not resolve the reported vulnerability. Correctly avoiding false positive in this case requires reasoning about the presence, or absence, of PgLTree-related logic. In particular, human would typically search for references to PgLTree and assess whether the changes meaningfully interact with the vulnerable PgLTree conversion path. If no such relationship exists, the commit is unlikely to constitute the true vulnerability fix, even if the change appears relevant at surface level. Listing 2: Commit d84dff22from chanmix51/Pomm. diff --git a/Pomm/Converter/PgEntity.php b/Pomm/Converter/PgEntity.php index 15eb808..f83aaf6 100644 1https://github.com/chanmix51/Pomm/commit/67f2cddd1cd79153ebb37228472d8962a541a6fd 4 --- a/Pomm/Converter/PgEntity.php +++ b/Pomm/Converter/PgEntity.php @@ -70,7 +70,7 @@ class PgEntity implements ConverterInterface 70 71 72 74 75 76 77 78 - + throw new NullPointerException(); } $fields = array(); foreach ($this->map->getFieldDefinitions() as $field_name => $pg_type) { } $fields[$field_name] = array_shift($elts); $fields[$field_name] = stripcslashes(array_shift($elts)); if (count($elts) > 0)"
        },
        {
            "title": "3.1 Vulnerability Fix Detection",
            "content": "Vulnerability fix detection has been prominent research area, encompassing static and dynamic analysis techniques as well as machine learning approaches. Early studies primarily relied on rulebased systems and manually crafted features to identify security-relevant commits (Mockus and Votta, 2000; Śliwerski, Zimmermann, and Zeller, 2005). With the growth of large-scale software projects, classical machine learning and natural language processing techniques were increasingly applied to commit messages, code diffs, and issue-tracking artifacts (X. Wang, K. Sun, et al., 2019). More recent work has leveraged deep learning (Hoang, Lawall, et al., 2019), graph-based representations (B. Wu et al., 2022), and large language models (LLMs) to capture semantic, syntactic, and structural characteristics of code changes (X. Yang et al., 2025). In the following sections, we provide structured overview of this evolution. 3.1.1 Heuristic and Metadata-Based Detection Early works primarily relied on heuristic signals derived from commit metadata and simple code change characteristics in order to identify patches. Several studies searched for keywords like fix and bug in log messages to find vulnerability patches (Mockus and Votta, 2000; Śliwerski, Zimmermann, and Zeller, 2005; Dallmeier and Zimmermann, 2007; Kim et al., 2008). Tian, Lawall, and Lo (2012) extended the analysis to use both commit messages and code changes. These approaches extracted manually designed facts from text and code, such as the number of added loops or function calls, to build discriminative models. This line of work naturally evolved toward vulnerability identification by incorporating repository-level metadata, including committer experience, contribution history, and submission timing, which were used to train Support Vector Machine (SVM) classifiers to support security auditing workflows (Perl et al., 2015). These methods are brittle to vocabulary changes and developer practices, and their reliance on shallow, manually engineered signals limits generalization and recall for non-explicit or silent vulnerability fixes. 2https://github.com/chanmix51/Pomm/commit/d84dff2165d5900296cf47d8923ff6333f5638a"
        },
        {
            "title": "3.1.2 Classical and Semi-Supervised Learning Approaches",
            "content": "Subsequent research expanded beyond kernel-specific heuristics to broader software ecosystems, leveraging classical machine learning and natural language processing to analyze developerwritten artifacts. Studies utilized commit messages and bug reports collected from issue-tracking platforms such as JIRA and Bugzilla to detect security-relevant changes (Y. Zhou and Sharma, 2017). To reduce analyst burden in industrial settings, later systems combined independent classifiers over log messages and patches, prioritizing high precision to limit false positives (Sabetta and Bezzi, 2018). In parallel, researchers began addressing the challenge of secret security patches-fixes applied without public disclosure-by applying ensemble-based classifiers to capture syntactic and semantic indicators of vulnerability fixes in code changes (X. Wang, K. Sun, et al., 2019). While more flexible than heuristics, these approaches depend heavily on labeled data and surface-level textual cues, making them less effective for semantically complex fixes and previously unseen vulnerability patterns."
        },
        {
            "title": "3.1.3 Deep Learning for Commit and Code Change Modeling",
            "content": "The adoption of deep learning marked turning point in vulnerability-fixing commit detection. Hierarchical neural architectures such as PatchNet were proposed to learn semantic representations that reflect the sequential and hierarchical structure of code diffs (Hoang, Lawall, et al., 2019). Around the same time, the notion of safe patches emerged, using symbolic execution and interpretation to identify fixes that constrain input spaces without altering intended program behavior, enabling faster downstream adoption (Machiry et al., 2020). Further advances incorporated representation learning guided by commit messages, as exemplified by CC2Vec, which employs attention mechanisms to jointly model code changes and their textual descriptions while capturing structural dependencies (Hoang, Kang, et al., 2020). Despite improved representation learning, deep neural models often require large, curated training datasets and struggle with interpretability and transferability across projects and programming languages. 3.1.4 Structural and Graph-Based Representations To better capture program structure, later work emphasized syntax-aware and graph-based representations of code changes. Approaches such as Commit2Vec (Cabrera Lozoya et al., 2021) modeled the symmetric difference between Abstract Syntax Trees (ASTs) before and after commit to encode fine-grained structural modifications. RNN-based models like PatchRNN (X. Wang, Shu Wang, et al., 2021) extracted syntax-level features from both diffs and commit messages to improve detection performance. Similarly, the SPI (Y. Zhou, Siow, et al., 2021) system employed composite neural architectures to learn distinctions at the statement level within code revisions. More recently, graph neural networks have been applied to enriched representations such as Patch Code Property Graphs (PatchCPGs), enabling systems like GraphSPD (Shu Wang et al., 2023) and E-SPI (B. Wu et al., 2022) to capture semantic relationships and control/dataflow dependencies. Other tools, including VFFINDER (S. Nguyen, Vu, and Vo, 2023), rely on annotated ASTs to represent structural changes with high precision, while transformer-based models fine-tuning CodeBERT have been used to detect silent vulnerability fixes across projects and programming languages, e.g. the popularly used VulFixMiner (J. Zhou et al., 2021). Although structurally precise, graphand syntax-based methods incur substantial preprocessing and computational overhead, which hampers scalability and adoption in large, rapidly evolving codebases."
        },
        {
            "title": "3.1.5 Multi-View, Traceability, and Ranking-Based Methods",
            "content": "A persistent challenge in the literature is the lack of explicit links between commits and external vulnerability or issue-tracking data. To address this, systems, such as HERMES by NguyenTruong et al. (2022), introduced link recovery techniques to enrich commit context and improve vulnerability-fix detection. In parallel, ranking-based methods reframed the problem as correlating vulnerabilities with commits based on identifiers, file locations, and textual similarity. Approaches like PatchScout and VCMATCH rank candidate patches by measuring alignment between vulnerability descriptions and code changes (Tan et al., 2021; Shichao Wang et al., 2022). To handle tangled commits, multi-granularity frameworks such as MiDas analyze changes at multiple levels-commit, file, hunk, and line-using dedicated neural components (T. G. Nguyen, Le-Cong, Kang, Widyasari, et al., 2023). Efforts to improve usability and interpretability include VulCurator, which produces ranked lists of likely fixes using BERT-based representations of text and code (T. G. Nguyen, Le-Cong, Kang, Le, et al., 2022), and methods that generate aspect-level explanations such as root causes and impacts alongside predictions (J. Sun, Xing, et al., 2023). These techniques rely on the availability and quality of auxiliary artifacts and recovered links, and their performance degrades when vulnerability descriptions are incomplete, ambiguous, or weakly aligned with code changes. 3.1.6 Large Language ModelBased Approaches The recent advances in Large Language Models (LLMs) has enabled reasoning about vulnerability fixes at higher semantic level. Two-phase frameworks such as PatchFinder combine hybrid retrieval mechanisms for coarse filtering with semantic re-ranking to identify relevant patches efficiently (K. Li et al., 2024), reporting recall@10 of 80.63%. Frameworks such as CompVPD (T. Chen et al., 2024), LLM4VFD (X. Yang et al., 2025) and CommitShield (Z. Wu et al., 2025) leverage pretrained LLMs to reason over code changes in natural language. CompVPD introduces iterative refinement through precise context selection and human validation signals to address generalization limitations (T. Chen et al., 2024). LLM4VFD employs pretrained language models to align vulnerability descriptions with code changes, enabling more semantically informed identification of vulnerability-fixing commits (X. Yang et al., 2025). CommitShield extends this paradigm by incorporating Chain-of-Thought reasoning and program slicing to provide structured contextual evidence for its predictions, allowing for iterative refinement and improved precision (Z. Wu et al., 2025). Although LLM-based approaches show significant improvements over traditional learningbased methods, they introduce challenges in complexity, computational cost, and scalability. Running large pretrained models with reasoning-intensive pipelines on repositories containing millions of commits, such as the Linux kernel, remains expensive. Consequently, evaluations are often limited to curated or constrained experimental settings, leaving open questions about robustness and real-world effectiveness. In contrast, our approach integrates the efficiency and scalability of classical machine learning techniques with the adaptive reasoning capabilities of an LLM-based agent system, providing human-like contextual understanding while remaining practical for large-scale software ecosystems."
        },
        {
            "title": "4 Favia Approach",
            "content": "Our vulnerability fix detection framework Favia identifies vulnerability-fixing commits through two-stage process. It first applies fast, coarse-grained ranking over all commits in an affected repository to surface small set of candidate patches. Each candidate is then examined in 7 depth by an Large Language Model (LLM) agent that performs detailed semantic reasoning over the codebase and vulnerability context to determine whether the commit constitutes true vulnerability fix. This hybrid design balances scalability with semantic depth, enabling efficient filtering of potential patch commits followed by rigorous contextual analysis."
        },
        {
            "title": "4.1 Candidate Ranking",
            "content": "Figure 1: Top-n ranking using machine learning classifier. The first stage aims to drastically reduce the search space of potential patch commits. Given that repositories such as the Linux kernel contain over 1.4 million commits, exhaustive evaluation with heavy Large Language Models (LLMs) is computationally infeasible. We therefore employ more conventional machine learning classifier to rank commits based on their likelihood of being vulnerability fixes. These classifiers may also include static and dynamic analysis tools, as summarized in section 3. Specifically, we chose to use PatchFinder by K. Li et al. (2024) (section 5.2.2) as our classifier for commit candidate ranking because it is the leading approach among efficiency-oriented methods for vulnerability-fix identification. However, any sufficiently efficient classifier may be used. The only requirement is that the classifier selects the top-k candidate commits for further inspection. Figure 1 illustrates the first stage of our Favia approach. Using the CVE description, along with commits message and code changes, these are classified with machine learning classifier, outputting classification score of the commit being the true patching commit for the CVE. This is repeated for every commit in the repository. All repository commits are then ranked according to score. Finally, only the top-k candidates are kept for the next stage - agent-based classification. The value of can be tuned based on resource constraints and desired recall. It is important to note that if the true patch is not included in the top-k, the subsequent evaluation stage cannot recover it."
        },
        {
            "title": "4.2 Agent-based Classification",
            "content": "Figure 2 illustrates the second stage of our Favia approach. This stage involves reasoning agent tasked with evaluating each of the top-k candidates. To facilitate this process, we adopt the ReAct (Yao et al., 2023) framework, which integrates reasoning and acting in unified agent architecture. ReAct enables the agent to iteratively decompose complex evaluation tasks into subgoals, alternating between chain-of-thought reasoning and tool-based interactions. This allows the agent to simulate human-like analysis, retrieve relevant context, and dynamically refine its understanding. Specifically, we implement the agent using the smolagents (Roucher et al., 2025) library from Hugging Face (2025). We keep the standard system prompt that allows for generic tool usage as default (see fig. 11). Each candidate commit is presented to the agent with the goal of determining whether it constitutes valid vulnerability patch for given CVE. We structure this task into three segments: 8 Figure 2: Agent classification for each top-n ranked commits. 1. Understand the Vulnerability: The agent begins by analyzing the associated CVE and CWE reports to comprehend the nature of the vulnerability. 2. Analyze the Commit Changes: The agent inspects the diff to identify what modifications were introduced. 3. Correlate Code Changes with the CVE: The agent evaluates whether the changes effectively mitigate the described vulnerability. We formulate these segments into the task prompt available in fig. 12. The expected output of the system is threefold. First, the agent produces an explanation of its reasoning, which encourages the agent to consolidate and justify its answer and allows for straightforward qualitative evaluation. Second, the agent reports confidence score using 5-point ordinal confidence scale, where 1 indicates no confidence and 5 indicates full confidence. Third, the agent produces final verdict, true or false, indicating whether the commit constitutes the true vulnerability patch. As described in section 2.2, vulnerability fixes are hard to distinguish from unrelated or cosmetic changes. It is therefore necessary for the agent to be able to inspect the underlying codebase. To support this, we instantiate the agent within code environment (described in section 4.2.1 that reflects the repository state prior to the commit. This enables the agent to locate the vulnerable logic and determine whether the proposed changes address it. The agent is equipped with suite of tools, detailed in section 4.2.2. These tools allow the agent to navigate the codebase, retrieve relevant context, and perform targeted inspections. By combining structured reasoning with interactive tool use, the agent achieves level of semantic depth that surpasses conventional classifiers. We allow the agent to run maximum of 20 steps. If the agent has not finished in 20 steps, the agent is exempted to produce the final answer."
        },
        {
            "title": "4.2.1 Environment",
            "content": "To enable deep semantic evaluation of candidate commits, we instantiate the agent within simulated code environment. This environment reflects the state of the repository immediately prior to the commit under investigation. By providing access to the pre-change codebase, the agent can inspect the original logic, identify potential vulnerabilities, and assess whether the proposed diff mitigates the issue. This setup is particularly important for evaluating indirect fixes, where the vulnerability may reside in transitive dependency or auxiliary function not explicitly modified in the commit. As seen from fig. 2, the environment is designed to support interactive exploration, allowing the agent to reason about the codebase in manner similar to human analyst. It is equipped with suite of tools that facilitate targeted inspection, contextual retrieval, and structured reasoning. These tools are exposed as Python functions. Through code generation, the agent can generate function calls to execute any number of tools dynamically on-demand as it sees fit, based on the identification of missing information. For example, the agent may invoke file search via file_search(query=\"FileOutputStream.java\")."
        },
        {
            "title": "4.2.2 Tools",
            "content": "In the following paragraphs we describe the set of tools available to the agent during vulnerability fix detection analysis. Each tool provides access to specific type of information, ranging from structured vulnerability metadata to repository-level code inspection, and is explicitly invoked by the agent as part of its reasoning process. CVE report The CVE Report tool retrieves structured vulnerability metadata for given CVE (Common Vulnerabilities and Exposures) identifier. The information is returned in Markdown format and includes the following key sections: CVE Details: Includes the CVE ID, source identifier, publication and last modification dates, and current vulnerability status. Known Exploited Status: Indicates whether the vulnerability is known to be actively exploited in the wild. Scores: Severity metrics such as CVSS (Common Vulnerability Scoring System) scores, which quantify the impact and exploitability of the vulnerability. Description: natural language summary of the vulnerability, filtered by language preference (e.g., English). Weaknesses: Associated CWE (Common Weakness Enumeration) entries that categorize the vulnerability type. Configurations: Affected software configurations, platforms, and version constraints. This tool enables the agent to understand the nature and scope of the vulnerability, identify related weaknesses, and contextualize the commit under evaluation. Because the output is rendered in Markdown, it is easily readable and structured for parsing or display. Additionally, this tool serves as diagnostic mechanism: if the agent bypasses this lookup and prematurely concludes patch validity, it may indicate memorization or data leakage from pretraining. By requiring explicit invocation of this tool, we ensure that the agents reasoning is grounded in retrieved evidence rather than latent knowledge. 10 CWE report The CWE Report tool retrieves structured information about CWE (Common Weakness Enumeration) entry and returns it in Markdown format. The agent uses this tool to understand the general class of vulnerability associated with given CVE, such as buffer overflows, improper input validation, or race conditions. This contextual knowledge helps the agent reason about the nature of the fix and whether the code changes align with the expected mitigation strategy. It could also find other CVEs with same vulnerability, and consecutively look them up using the CVE report tool. The CWE report includes the following key sections: Description and Extended Description: concise summary of the weakness and its broader implications. Common Consequences: Typical impacts of the weakness, categorized by scope (e.g., confidentiality, integrity) and effect (e.g., data corruption, privilege escalation). Relationships: Links to related CWEs, including parent, child, or peer relationships. Content History: Metadata about the evolution of the CWE entry, including submission and modification records. Code search The Code search tool allows searching for file contents in git repository files. It takes simple string and greps the entire codebase. An optional file argument is available to limit search to specific file. This allows the agent to efficiently locate relevant keywords, functions, files, or code patterns. The output reports all matches along with their corresponding file paths and the line of code in which they occur. File search This tool searches for files in Git repository by filename, keyword, or glob pattern. It uses git ls-files to list files tracked by the repository and supports glob-style searches. This enables the agent to to obtain high-level overview of the repositorys structure and contents, or to determine the existence of specific files. Open file file can be opened by requesting the path to the file. It may also be opened at specific line. This allows the agent to look at the contents of files. In order to avoid exceeding LLM context limits, only total of 100 lines is shown at any time. Scroll file Once file has been opened, the agent can scroll up or down through the file to view the rest of it. It scrolls 100 lines at time. Final answer Once the agent is ready to answer, it uses the Final answer tool. This is default tool of the smolagents (Roucher et al., 2025) library. Once called, it terminates the run-loop and outputs any supplied arguments."
        },
        {
            "title": "5.1 Research Questions",
            "content": "Our study addresses the following research questions: RQ1: How effective is Favia compared with existing vulnerability fix detection approaches? 11 RQ2: Why do agent-based approaches fail when they make incorrect predictions? RQ3: How efficient is Favia compared to other approaches?"
        },
        {
            "title": "5.2 Design to answer RQ1",
            "content": "To answer RQ1, we chose to evaluate diverse set of open-source Large Language Models (LLMs) that differ substantially in scale, architecture, and design goals."
        },
        {
            "title": "5.2.1 Models",
            "content": "All selected models are instruction-tuned, which is required for their use in agent-based settings, as instruction tuning improves adherence to structured prompts, tool-use conventions, and multistep task execution. The following models are used: gemma-3-27b-it (G. Team et al., 2025) is 27 billion parameter, decoder-only transformer model released by Google as part of the Gemma 3 family. It is instruction-tuned using supervised fine-tuning and alignment techniques to improve instruction following and reasoning behavior. Compared to very large frontier models, gemma-3-27b-it offers favorable trade-off between model capacity and computational cost, making it suitable for controlled experimental evaluations where inference efficiency is concern. The model supports long-context inputs of 128K and is trained with strong emphasis on reasoning, coding, and general-purpose instruction following. Llama-3.3-70B-Instruct (Meta, 2024)is 70 billion parameter dense transformer model developed by Meta. It is instruction-tuned on mixture of supervised and preferencebased data to improve helpfulness and safety. Llama-3.3-70B-Instruct is designed as highcapacity general-purpose model with strong performance on reasoning, code understanding, and long-context tasks. With its large context window of 128K tokens, its scale and dense architecture serves as strong open-source baseline for tasks that require deep semantic understanding and robust multi-step reasoning. Qwen3-235B-A22B-Instruct-2507 (Q. Team, 2025) is Mixture-of-Experts (MoE) instruction-tuned model from the Qwen3 family. The model has total of 235 billion parameters, with 22 billion parameters active per forward pass. This MoE architecture enables very high representational capacity while keeping inference costs lower than comparably sized dense model. The instruction-tuned variant is explicitly optimized for complex reasoning, code-related tasks, and agentic workflows involving tool use and decision making. The model has very large context window of 256K tokens. 5.2.2 Baselines We compare Favia against several representative works. As the most commonly used baseline for vulnerability fix detection, we select the machine learning method VulFixMiner J. Zhou et al., 2021. We also select the state-of-the-art (SOTA) machine learning classifier approach PatchFinder K. Li et al. (2024). Finally, we evaluate against the SOTA LLM-based frameworks LLM4VFD X. Yang et al. (2025) and CommitShield Z. Wu et al., 2025. VulFixMiner by J. Zhou et al., 2021 is transformer-based approach for automatically identifying silent vulnerability-fixing commits. It analyzes commit-level code changes without relying on commit messages, leveraging fine-tuned CodeBERT model to learn semantic representations 12 of added and removed code. File-level representations are aggregated into commit-level embedding, which is then used to rank commits by their likelihood of fixing vulnerabilities. We base our implementation of VulfixMiner on the implementation by VulCurator 3 . We closely follow VulFixMiners original model selection and hyperparameters. However, as our training dataset is roughly 100 times larger than that of VulFixMiner, we increase the effective batch size hundred fold. From 8 to 800 during first training phase, and from 32 to 3200. PatchFinder by K. Li et al. (2024) is two-phase framework designed to trace security patches for disclosed vulnerabilities in open-source software. PatchFinder first performs an initial retrieval, using both lexical (tf-idf) and semantic (pre-trained code model) similarity to narrow down candidate set of commits. It then applies re-ranking phase with an end-to-end learned model to capture deeper semantic correlations between CVE descriptions and commits, enabling more accurate ranking of vulnerability patches. PatchFinder is open-sourced and available at GitHub4. Following the original implementation, we use the tf-idf vectorizer from Scikit-Learn Pedregosa et al., 2011 library to calculate the tf-idf score for the training dataset split. Similarly, we use the CodeReviewer Z. Li et al., 2022 model from microsoft for calculating the semantic similarity using the CR_score5. These two similarities are fused together and keep the top 100 commit candidates for each cve in our dataset. We then fine-tune CodeReviewer to re-rank the top-100 candidate patches from the training dataset. We keep hyperparameters unchanged from the original paper. LLM4VFD by X. Yang et al. (2025) is framework that leverages Large Language Models with structured reasoning and contextual learning to identify vulnerability-fixing commits in open-source software. The framework uses commit intent analysis (CCI), related development artifacts (DA), and historical vulnerability fixes (HV). We use LLM4VFD as baseline but exclude the development artifacts to ensure fair comparison, as such artifacts are not available to our agent-based approach. According to the original papers ablation analysis, incorporating development artifacts yields maximum F1-score improvement of only 0.03, indicating that their exclusion has negligible impact on overall performance. We base the implementation of LLM4VFD on the replication package provided by the paper 6. We first construct vector database of historical vulnerability fixes (HV). We generate 3-aspect summaries for all patching commits in our CVEVC dataset using the three models described in section 5.2.1. We create embeddings of these summaries using Qwen/Qwen3-Embedding-8B (Zhang et al., 2025), and store this in ChromaDB (Chroma, 2025) vector database. CommitShield by (Z. Wu et al., 2025) is an LLM-based framework for vulnerability-fixing commit identification. CommitShield operates by enriching commit descriptions and analyzing the patches they contain. It first collects additional information related to the commit description and leverages large language model (LLM) to generate more detailed summary. The tool then evaluates the relevance of patches, retaining only those connected to the description. These patches are categorized into intra-procedural (changes within single function) and interprocedural (changes involving multiple functions). For intra-procedural patches, CommitShield prepares contextual information about the modified functions, while for inter-procedural patches, it gathers details about related function calls. To support this process, CommitShield employs Joern (Joern, 2025) to generate Code Property Graphs (CPGs), enabling structured program analysis. Finally, all of this organized information is used to support CommitShields vulnerability and fault detection analysis. While the original work by Z. Wu et al. (2025) only considers and C++, our dataset is not constrained to single programming language. We therefore extend 3https://github.com/ntgiang71096/vfdetector/blob/main/vulfixminer.py. 4https://github.com/MarkLee131/PatchFinder 5https://github.com/MarkLee131/CR_score/tree/issta24 6https://doi.org/10.5281/zenodo.13776994 13 CommitShield to support the languages C, C++, Python, PHP, Java, and Go. These languages are facilitated by the main Tree-Sitter(Tree-sitter, 2024) project, and are also supported by Joern. The languages account for more than 50% of all files in the diffs of our evaluation datasets. Patches of unsupported languages will be judged based on the description and content of the diff."
        },
        {
            "title": "5.2.3 Datasets",
            "content": "Prior work frequently evaluates vulnerability-fix detection on randomly sampled commits, which may substantially underestimate real-world difficulty. To address RQ1, we evaluate all approaches under two settings: (i) random setting, where candidate commits are uniformly sampled from the repository, and (ii) realistic setting, where all commits are potential candidates. Exhaustively evaluating every commit for each CVE is computationally expensive and not scalable. More importantly, the most challenging cases arise from commits that are highly similar to the true vulnerability fix but do not actually resolve the vulnerability. As discussed in section 2.2, random sampling used in prior studies is unlikely to include such hard negatives, leading to overly optimistic performance estimates. To approximate realistic conditions while remaining scalable, Favia first use an efficient machine-learningbased ranking step to filter out commits that are clearly unrelated to the vulnerability. We then evaluate how Favia and the baseline methods identify the true fixing commit among the remaining candidates, which are similar to the correct fix. By comparing performance across these settings, we quantify how random sampling inflates reported performance and assess whether conclusions drawn from simplified evaluations hold under realistic deployment conditions. Our raw data comes from previous work J. Sun, J. Chen, et al., 2024, which contains 17,293 CVEs with 4,682 corresponding open-source repositories and 23,303 patching commits. The including different languages like Java patching commits span over 200 different file types, (13.7%), PHP (13.6%), (13.1%), JavaScript (10.0%), Python (4.3%), Go (4.1%), and others (41.2%), which reflects the complicated and highly unbalanced code environments for vulnerability fix tracing. For every CVE in the dataset, we git clone every repository that is GitHub repository. We get all patch commits, and select up to 5,000 non-patch commits at random. If repository has fewer than or equal to 5,000 commits, we select all. In total, we identify total of 3,820 repositories, of which 3,708 are successfully downloaded, resulting in approximately 2 terabytes of data. We also try to remove binary or otherwise excessively large diffs from the non-patch set; we filter commits based on diff size. These are primarily dependency updates, vendor code imports, or asset changes that are unlikely to represent vulnerability fixes and would otherwise add noise and unnecessary computational overhead. Specifically, we retain diffs up to the 95th percentile of diff string length (measured in number of characters); any diff exceeding 153,993 characters is discarded. After filtering, total of 8,283,424 commits are left. We split the patch commits into training, validation, and test sets using an 80/10/10 split. To prevent data contamination, the split is performed at the repository level, ensuring that no repository appears in more than one split. Non-patch commits are then assigned to splits based on the repositorylevel partitioning induced by the patch data. We name the collected dataset CVEVC, short for Common Vulnerability Enumeration Vulnerability-fixing Commits. Due to the large amount of data, we normalize the dataset into three separate datasets. One dataset stores unique commit data 7, another stores CVE data 8, and the third dataset stores mapping between labeled commits to CVEs 9. From the CVEVC dataset, we construct two new datasets 10. One based on random sampling, which is based on the dataset construction strategies of baselines (J. Zhou et al., 2021; K. Li et al., 2024; X. Yang et al., 2025). Another dataset emulates realistic scenario in which all commits are considered candidates. However, to perform scalable comparisons between Favia and baselines, we focused on the most challenging aspect of identifying the correct commit in the realistic dataset. We assume that the top candidates (the top 10 in our design) are most likely the correct ones identified. We then compare how well the tools can find the correct one from the top candidates. Random dataset. We construct the random_10 evaluation dataset consisting of randomly selected commits from the test split. For each CVE, we select up to 10 commits. This includes all corresponding patch commits and randomly samples the rest with non-patch commits from the same repository. If fewer than 10 non-patch commits are available, all are included. The sampled non-patch commits may include other patching commits that are unrelated to the specific CVE under consideration. Realistic dataset. To identify the top 10 most difficult commit candidates, we use PatchFinder, trained on the training split of the CVEVC dataset (see section 5.2.2) to rank the commits for each CVE in the test split of the dataset. Figure 3 shows the distribution of recall@k, computed over the top-k ranked commits across all available patches. We select the top 10 ranked commits for each CVE, and package this as PatchFinder_top10. The selected commits may very well include other patching commits that are unrelated to the specific CVE under consideration. As we select = 10, any downstream evaluation on this dataset can at most find 48% of the total patches. Figure 3: PatchFinder effectiveness on test split of CVEVC dataset. 7https://huggingface.co/datasets/andstor/cvevc_commits 8https://huggingface.co/datasets/andstor/cvevc_cve 9https://huggingface.co/datasets/andstor/cvevc_cve_commit_mappings 10https://huggingface.co/datasets/andstor/cvevc_candidates"
        },
        {
            "title": "5.2.4 Metrics",
            "content": "We evaluate vulnerability fix detection as binary classification task at the commit level, where each candidate commit is classified as either vulnerability-fixing patch or non-patch commit for given CVE. Due to the highly imbalanced nature of the task, we report precision, recall, and F1-score. This comparison allows us to determine whether agent-based, iterative reasoning provides measurable benefits over current state-of-the-art (SOTA) methods. All methods are evaluated on the same candidate sets using identical train/validation/test splits and metrics. Precision measures the proportion of commits predicted as vulnerability fixes that are correct. High precision indicates that method produces few false positives, which is critical in large repositories where incorrectly labeling benign commits as security patches can lead to unnecessary manual inspection or incorrect downstream actions. Recall measures the proportion of true vulnerability-fixing commits that are successfully identified. High recall is essential in security settings, as missing true patch may result in an unmitigated vulnerability remaining in the codebase or downstream dependencies. F1-score is the harmonic mean of precision and recall, providing single metric that balances false positives and false negatives. It is particularly useful when comparing methods with different trade-offs between precision and recall on imbalanced datasets. All metrics are computed by aggregating predictions across all CVEs within dataset split, reflecting realistic deployment scenario where system must identify vulnerability-fixing commits among large number of unrelated changes. Accuracy is omitted, as it would be dominated by the majority non-patch class and thus provide limited insight."
        },
        {
            "title": "5.3 Design to answer RQ2",
            "content": "To understand the limitations of agent-based reasoning, RQ2 analyzes failure cases produced by Favia. We examine agent trajectories, tool usage patterns, and reasoning traces, and categorize incorrect predictions into distinct failure modes. This analysis reveals whether failures stem from insufficient exploration, misunderstanding of vulnerability semantics, overconfidence, or reliance on surface-level cues, providing insight into how agent behavior can be improved. 5.3.1 Agent trajectories From traces collected from experiments of RQ1, we analyze the amount of calls to the different tools. Because model can invoke arbitrary many tool calls at any one step, this might give misleading information. For example, opening file and scrolling in loop with 100 iterations. We therefore only count invocation of each tool type once per step. 5.3.2 Failure mode classification We categorize incorrect classifications by prompting an Large Language Model (LLM) to classify the agents run history into eight failure modes defined in table 1. In order to provide fair and unbiased evaluation, we use LLM from an independent model family, separate from the models in section 5.2.1. Specifically, we use gpt-oss-120b (OpenAI et al., 2025). gpt-oss-120b is an open-weight Mixture-of-Experts model released by OpenAI, featuring approximately 117 billion total parameters with about 5.1 billion active parameters per token, designed for general-purpose and high-reasoning tasks. It supports configurable reasoning effort and supports 128K tokens context window. We collect all Favias agent traces from the results of RQ1. We then filter out all traces that were correctly classified. We then prompt the LLM to classify Favias agent traces. In the 16 Table 1: Failure mode categories."
        },
        {
            "title": "Category",
            "content": "Superficial Association"
        },
        {
            "title": "Description",
            "content": "The model inferred match due to keywords, filenames, or conceptual similarity without real evidence. Failed to Find Relevant Context The agent were not able to find the relevant files. CVE Misinterpretation Memorized CVE Incorrect Classification Ran Out Of Budget Gave Up Prematurely Other The model misinterpreted the CVEs root cause, vulnerable component, or exploit mechanismresulting in justification that does not align with the actual CVE description. The model did not retreve the CVE report and concluded based on memory. The agent collected reasonable evidence but came to the wrong conclusion. The agent seem to be on the right track, but the episode ended prematurely. The agent decided to stop solving the problem after encountering some difficulty. There was some other problem that prevented the agent from correctly classifying the commit. classification prompt, we supply the CVE description and the agent trace. In order to focus the analysis on the agent behavior, in the agent trace, we only include the agent task, reasoning steps, and tool calls. We exclude any tool outputs. We configure gpt-oss-120b to use medium reasoning effort."
        },
        {
            "title": "5.4 Design to answer RQ3",
            "content": "Agent-based reasoning introduces additional computational cost due to multi-step interaction and context accumulation. To answer RQ3, we measure efficiency in terms of input, output, and embedding token consumption across all approaches. We report mean token usage per commit and analyze how cost scales with model size and reasoning depth. This allows us to characterize the trade-off between improved detection performance and increased computational overhead, and to assess the practicality of deploying agent-based systems in large-scale settings."
        },
        {
            "title": "6 Experimental Results",
            "content": "In this section, we present detailed results of each research question."
        },
        {
            "title": "6.1 Results of RQ1: Effectiveness",
            "content": "6.1.1 Results based on the random dataset Table 2 reports performance of Favia and the baselines on the random dataset, where candidate commits are sampled uniformly at random, using the dataset construction strategies in +++ref. 17 Considering performance ranges across models provides more robust comparison than focusing on individual modelapproach pairs. The results show that VulFixMiner and PatchFinder benefit from random sampling primarily in terms of precision. VulFixMiner reaches precision of 0.83 but continues to exhibit extremely low recall (0.01), resulting in negligible F1-score. PatchFinder achieves high precision (0.86) and moderate recall (0.23), improving its F1-score to 0.36. Despite these gains, both methods remain recall-limited, even in this simplified setting. LLM-based techniques, i.e., LLM4VFD and CommittShield, show the largest absolute gains under random evaluation. Across models, precision ranges from 0.47 to 0.82, while recall remains consistently high (0.710.94), yielding F1-scores of up to 0.87. Favia achieves the strongest overall performance across models, with precision between 0.59 and 0.82, recall between 0.86 and 0.93, and F1-scores ranging from 0.72 to 0.87. Table 2: Performance metrics across different approaches on Random 10 subset"
        },
        {
            "title": "Model",
            "content": "CodeBERT CodeReviewer Llama-3.3-70B-Instruct Qwen3-235B-A22B-Instruct-2507 gemma-3-27b-it"
        },
        {
            "title": "Recall",
            "content": "F1-score VulFixMiner PatchFinder LLM4VFD CommitShield Favia LLM4VFD CommitShield Favia LLM4VFD CommitShield Favia 0.83 0.86 0.77 0.45 0.74 0.74 0.62 0.82 0.47 0.29 0.59 0. 0.23 0.74 0.75 0.86 0.81 0.71 0.92 0.87 0.94 0.93 0.03 0. 0.75 0.56 0.79 0.77 0.66 0.87 0.61 0.44 0.72 Bold: best-performing approach per model. 6.1.2 Results based on the realistic dataset Table 3 highlights clear performance strata between traditional methods, LLM-based frameworks, and the proposed agent-based approach (Favia) on the realistic dataset. Regarding each evaluated approach, VulFixMiner exhibits high precision but extremely low recall, with precision around 0.43 and recall as low as 0.03, yielding F1-scores in the 0.06 range. This narrow operating regime reflects highly conservative detector: when it predicts vulnerability-fixing commit it is often correct, but it fails to recover almost all true patches. As result, VulFixMiner performs poorly in recall-oriented security settings and does not scale well to large, diverse candidate sets. PatchFinder shows more balanced but still limited performance profile, with precision and recall both lying in the 0.370.40 range and an F1score of approximately 0.38. Compared to VulFixMiner, PatchFinder substantially improves recall while maintaining competitive precision. However, its performance remains bounded by similarity-based retrieval and re-ranking, which limits its ability to capture deeper semantic and contextual relationships present in complex patches. For the LLM-based techniques, LLM4VFD consistently achieves very high recall, ranging from 0.89 to 0.94, but with low precision, ranging from 0.18 to 0.30. This results in F1-scores 18 Table 3: Performance metrics across different approaches on the realistic dataset"
        },
        {
            "title": "Model",
            "content": "CodeBERT CodeReviewer Llama-3.3-70B-Instruct Qwen3-235B-A22B-Instruct-2507 gemma-3-27b-it"
        },
        {
            "title": "Recall",
            "content": "F1-score VulFixMiner PatchFinder LLM4VFD CommitShield Favia LLM4VFD CommitShield Favia LLM4VFD CommitShield Favia 0.43 0.40 0.30 0.18 0.30 0.29 0.22 0.39 0.18 0.12 0.23 0. 0.37 0.89 0.87 0.94 0.93 0.85 0.98 0.94 0.99 0.98 0.06 0. 0.45 0.30 0.46 0.45 0.35 0.56 0.30 0.21 0.37 Bold: best-performing training approach per model. between 0.30 and 0.45. These results indicate that LLM4VFD is effective at identifying most vulnerability-fixing commits but tends to over-predict, leading to many false positives. Increasing model capacity improves recall slightly but does not substantially close the precision gap. CommitShield follows similar but slightly weaker trend compared to LLM4VFD. Recall ranges from 0.85 to 0.99, while precision drops to 0.120.22, producing F1-scores between 0.21 and 0.35. The method is strongly recall-oriented, benefiting from structural program analysis, but its aggressive inclusion of candidate patches results in the lowest precision among LLM-based baselines. Favia consistently delivers the strongest and most stable performance across models. Precision improves substantially compared to other LLM-based methods, ranging from 0.23 to 0.39, while recall remains extremely high at 0.940.98. This leads to F1-scores between 0.37 and 0.56, representing the best overall range among all approaches. Notably, Favia maintains high recall comparable to LLM4VFD and CommitShield, while significantly reducing false positives, especially with larger models. 6.1.3 Performance differences using random vesus realistic datasets Compared to performance using the random dataset for evaluation (as shown in 2), all approaches achieve substantially lower precision, recall, and F1-scores, indicating that random evaluation leads to markedly easier detection task, meaning random sampling significantly reduces ambiguity between vulnerability-fixing and non-fixing commits. However, an evident pattern emerges: the performance advantage of Favia over other LLM-based baselines is more pronounced when using the realistic dataset than when using the randomly generated dataset. This highlights the benefits of Favias structured, multi-step reasoning approach. In contrast, this advantage becomes less noticeable when the candidate set is constructed randomly. Randomly selected commit sets contain high proportion of trivially non-security-related changes, making vulnerability-fixing commits easier to identify. This substantially inflates precision and F1-scores for all approaches and reduces the relative difficulty of the task. As result, random evaluation does not accurately reflect real-world deployment scenarios, where all com19 mits are candidates, and most security-relevant commits will have high chance of being classified as false positive. An additional factor influencing the observed recall differences is the construction of the realistic candidate set. In the realistic setting, evaluation is conditioned on PatchFinder ranking the true vulnerability-fixing commit within the top-10 candidates; if the true patch is not retrieved at this stage, it is excluded from downstream evaluation. Consequently, only 1,081 out of 1,857 true patches are present in the realistic dataset, with the remaining 776 patches omitted. Importantly, the retained patches are not easier cases: they are typically harder, more ambiguous fixes that survive similarity-based ranking and are surrounded by highly similar, security-relevant nonpatch commits. However, because each CVE contributes at most one true patch and competing positives unrelated to the target CVE are largely absent, recall is measured over smaller and more constrained set of positives. In contrast, the random dataset may include multiple patch commits per CVE that address different vulnerabilities and are correctly rejected by Favia due to lack of causal alignment, but are nevertheless counted as false negatives. This difference in evaluation conditioning explains why recall is higher on the realistic dataset despite its greater semantic difficulty, and further underscores the importance of realistic candidate selection when assessing vulnerability-fix detection systems. 6.1.4 Analysis of Performance differences between Favia and the baselines To explain Favias superiour precision, we present comparison between the results of Favias, CommitShields, and LLM4VFDs assessments of the commit 705a427 from the VLC media player (videolan/vlc), as the patch for CVE-2014-9625. The commit 705a427 is ranked number 6 in the top 10 candidate commits selected by PatchFinder for CVE-2014-9625. This is not the correct patch, but is somewhat similar to the correct patch commit. The CVE describes an integer truncation vulnerability in the GetUpdateFile function in misc/update.c, where an incorrect cast from 64-bit integer to 32-bit integer during memory allocation for downloaded update file can result in buffer overflow, allowing remote attackers to execute arbitrary code via crafted update status file. CommitShield The commit description indicates that the patch checks the return value of vlc_memstream_open in the function vlc_xml_encode to ensure the stream handle is valid before use, preventing potential undefined behavior due to failed stream initialization. This implies that prior to the patch, there was known issue where the code did not validate the success of stream initialization, which could lead to undefined behavior - runtime error with potential security implications. The modification adds check on the return value and returns NULL on failure, thus fixing known vulnerability related to improper error handling. Since the patch addresses known runtime error that could result in undefined behavior (a recognized class of security vulnerabilities)}, and the language in the description (prevent potential undefined behavior) suggests concrete defect was corrected, this qualifies as vulnerability repair commit under the given definition. Figure 4: Reasoning output of CommitShields analysis of commit 705a427 against CVE-20149625. As shown in fig. 4, CommitShield incorrectly classifies the commit as vulnerability patch by over-interpreting generic safety language in the commit description. In particular, its conclusion hinges on the highlighted phrases potential undefined behavior and runtime error with potential security implications, which CommitShield treats as sufficient evidence of security fix. This reasoning culminates in the explicit assertion that the change qualifies as vulnerability repair commit, despite the absence of any linkage to the CVE itself. However, the highlighted emphasis on undefined behavior reflects only general robustness improvement, not remediation of the specific vulnerability described by CVE-2014-9625. The CVE concerns an integer truncation leading to buffer overflow in the GetUpdateFile function in misc/update.c, whereas the commit modifies vlc_xml_encode in src/text/strings.c. By relying on surface-level indicators, such as the presence of defensive checks and language suggesting safety improvements, CommitShield conflates generic error handling with CVE-specific vulnerability remediation, leading to false positive. LLM4VFD The current patch modifies the vlc_xml_encode function to check the return value of vlc_memstream_open, which is function that can fail due to memory allocation or system resource limitations. Previously, the return value was ignored, and the code proceeded under the assumption that the stream was successfully initialized. This could lead to the use of an invalid or uninitialized vlc_memstream_t handle in subsequent operations, potentially resulting in undefined behavior such as memory corruption, crashes, or data integrity issues. By adding check and returning NULL on failure, the patch ensures that the function fails gracefully instead of proceeding with an invalid state. Comparing this to the historical vulnerability fix in Kodi, both changes focus on improving robustness by properly handling resource initialization failures. In the Kodi case, improper handling of input streams during XML parsing led to buffer overflow and denial of service. Similarly, the current patch addresses improper handling of stream initialization, which, while not directly involving buffer overflow, mitigates path where failure to check critical function return value could lead to undefined behavior-often precursor to exploitable conditions in memory-sensitive contexts. Although the current patch does not address buffer overflow per se, it corrects missing error check in security-critical function that handles string encoding for XML, which is often part of input processing pipelines. Such functions are common targets for attackers when error paths are not properly handled. The fact that the patch prevents use of an uninitialized resource and aligns with secure coding practices (e.g., CERT ERR33-C) suggests it is addressing potential vulnerability, specifically resource initialization failure that could be leveraged in denial-of-service attacks or lead to memory-related vulnerabilities under certain conditions. 21 The commit message emphasizes that the function is not guaranteed to succeed, and using the handle without checking is explicitly acknowledged as unsafe. This indicates awareness of defect that could compromise program reliability and safety. While no direct exploit is demonstrated, the nature of the fix-adding essential error checking to prevent use of an invalid resource-aligns with vulnerability fixes seen in similar codebases. Therefore, the patch is best classified as vulnerability fix, as it mitigates potential security issue rooted in improper error handling that could lead to undefined behavior in function likely used during untrusted input processing. Figure 5: Reasoning output of LLM4VFDs analysis of commit 705a427 against CVE-2014-9625. LLM4VFD similarly misclassifies the commit as vulnerability fix, as shown in fig. 5, but does so through speculative analogy rather than explicit CVE alignment. Its reasoning repeatedly emphasizes highlighted phrases such as undefined behavior, memory corruption, and often precursor to exploitable conditions, framing the change as security-relevant despite acknowledging that it does not address buffer overflow per se. The model further reinforces this conclusion by drawing comparisons to historical vulnerability fix in Kodi and by labeling vlc_xml_encode as security-critical function, even though no such claim is made in the CVE description. These highlighted analogies substitute pattern-based reasoning for concrete evidence. Crucially, LLM4VFD never references the integer truncation, the 64-bit to 32-bit cast, or the GetUpdateFile function central to CVE-2014-9625. As result, the final highlighted conclusion the patch is best classified as vulnerability fix rests on abstract security heuristics rather than CVE-grounded causal alignment, again producing false positive. Favia The CVE-2014-9625 vulnerability is an integer truncation issue in the GetUpdateFile function in src/misc/update.c, related to 64-bit to 32-bit cast during memory allocation for downloaded update file. The commit 705a427f75915d9951f62fa86851693dc16a73a7, however, modifies vlc_xml_encode in src/text/strings.c by adding check for the return value of vlc_memstream_open. This change is unrelated to integer casting, buffer overflow from truncation, or the update mechanism. The affected component, function, and vulnerability type do not match. Therefore, this commit is not patch for CVE-2014-9625. Figure 6: Reasoning output of Favias analysis of commit 705a427 against CVE-2014-9625. Favia correctly identifies the commit as non-patch, as shown in fig. 6, by grounding its analysis in explicit alignment between the CVE description and the code changes. Favia first extracts the vulnerability semantics from the CVEnamely, an integer truncation during memory allocation in GetUpdateFile within misc/update.cand then systematically verifies whether the commit addresses the same component, function, and vulnerability type. Since commit 705a427 exclusively modifies XML string encoding logic in src/text/strings.c and introduces no changes related to integer casting, buffer sizing, or the update mechanism, Favia correctly concludes that the commit is unrelated. Upon manually analyzing the full trace of Favia11, we also see the agent tries to find the GetUpdateFile inside misc/update.c and finds it is already fixed. This flexible evidence-driven, component-aware reasoning enables Favia to avoid false positives that arise from generic security heuristics, demonstrating its superior precision compared to CommitShield and LLM4VFD. Together, these outcomes illustrate why Favia provides the most accurate assessment: by requiring semantic, structural, and causal alignment between CVE descriptions and code changes, it distinguishes true vulnerability patches from incidental robustness improvements, significantly reducing false positive classifications. Summary for RQ1: Favia is more effective than state-of-the-art techniques for vulnerabilityfixing commit identification. Compared to traditional methods, it avoids the severe recall limitations, and compared to existing LLM-based approaches, it substantially reduces false positives while preserving very high recall. LLM-based vulnerability fix detection appears significantly more effective when evaluated on randomly sampled commits than under realistic candidate selection. Random evaluation inflates precision and F1-scores across all approaches and narrows the performance gap between competing methods. In contrast, PatchFinder-based evaluation better reflects real-world conditions and more clearly reveals the strengths of agent-based reasoning."
        },
        {
            "title": "6.2 Results of RQ2: Failure analysis results",
            "content": "As explained in section 5.3, we analyzed agent trajectories, tool usage patterns, and reasoning traces, and categorized incorrect predictions into distinct failure modes. In the following sections, we characterize some of the common system behaviors, including the common patterns arising from the order and volume of tool call usage, and the summary of failure modes and their possible reasons. 6.2.1 Results of trajectory analysis The results of the trajectory analyses are shown in fig. 7 and fig. 8. The observed trends and patterns are characterized in the following paragraphs. Low degree of memorization. Nearly all successful trajectories begin with an explicit call to the CVEReportTool. The CVE description is imperative, as it provides the task definition. As described in section 4.2.2, omitting this call implies that the model proceeds without externally retrieving the CVE description, suggesting reliance on internalized knowledge from pretraining and thus potential short-circuit of the intended reasoning process. We observe substantial variation across models. Qwen consistently invokes the CVEReportTool in the first turn across both datasets, with no successful trajectories starting directly with the FinalAnswerTool. In contrast, on the realistic dataset, Llama and Gemma exhibit 2,958 and 61 rounds, respectively, where the FinalAnswerTool is called without any prior CVE retrieval. Similarly, on the random dataset, Llama and Gemma exhibit 2,421 and 46 rounds, respectively. 11https://andstor-phoenix-cvevc-candidates-patchfinder-top10.hf.space/projects/UHJvamVjdDoz/trac es/c21e04f502c97d292247a7fc63c8ba05 23 (a) Qwen3-235B-A22B-Instruct-2507 (b) Llama-3.3-70B-Instruct (c) gemma-3-27b-it Figure 7: The frequency of tool calls invoked at each turn on the realistic dataset. (a) Qwen3-235B-A22B-Instruct-2507 (b) Llama-3.3-70B-Instruct (c) gemma-3-27b-it Figure 8: The frequency of tool calls invoked at each turn on the random dataset. 24 This suggests that these models rely more heavily on memorized patterns or heuristics, bypassing explicit evidence retrieval and thereby reducing the faithfulness of the intended reasoning process. Despite this, the overall frequency of such memorized cases remains relatively low relative to the total number of successful attempts. For Qwen, Llama, and Gemma, the corresponding rates are 0%, 24%, and 0.8% on the realistic dataset, and 0%, 21%, and 0.5% on the random dataset. Upon manually inspecting the potentially memorized runs, many of the cases are credited due to changes being only documentation, or completely irrelevant code changes. Hence, the agent dismisses these as not possibly related to any vulnerability and calls FinalAnswerTool without needing to check the CVE report. Early localization through file search. After retrieving the CVE description, the second turn is dominated by localization actions. As shown in fig. 7 and fig. 8, the most frequently invoked tool is the FileSearchTool, primarily used to identify files explicitly mentioned in the CVE report or implied by the affected components. This step grounds the abstract vulnerability description in concrete locations within the repository, enabling targeted inspection of relevant code regions. Progressive narrowing through code navigation. Subsequent turns typically follow structured exploration pattern, where the agent alternates between opening files, scrolling through relevant code regions, and inspecting related functions or call sites. This repeated openread navigate loop reflects progressive narrowing of focus from repository-level context to specific code changes, allowing the agent to align the commit diff with the vulnerable logic described in the CVE. 6.2.2 Results of failure analyses We categorize incorrect classifications by prompting gpt-oss-120b to classify the agents run history into eight failure modes defined in table 1. See section 5.3 for detailed steps. The results are presented in fig. 9, showing the dominant source of error is Superficial Association, accounting for 58.7% of all failures on the realistic dataset and 55.9% on the random dataset, where the agent infers valid patch based on surface-level cues such as keyword overlap, file names, or coarse semantic similarity without establishing causal link between the code changes and the CVE. The second most common failure mode is CVE Misinterpretation, with 29.8% on realistic dataset, and 22.9% on the random dataset. This occurs when the agent retrieves the CVE report but misunderstands its root cause, affected component, or exploit mechanism, leading to justifications that do not align with the actual vulnerability description. Together, these two categories account for the vast majority of failures, indicating that incorrect decisions are primarily driven by insufficient semantic grounding rather than lack of exploration or premature termination. 6.2.3 Stable analysis approach Comparing agent trajectories across the random and realistic datasets, we observe similar consistent behavioral patterns. This indicates that Favias agent exhibits stable and reproducible reasoning behavior, largely unaffected by differences in data distribution and driven primarily by the underlying CVE characteristics. Comparing the results between fig. 7 and fig. 8, we observe similar agent trajectories and tool usage patterns across the two settings. This indicates that the agents exploration strategy is 25 (a) Realistic dataset. (b) Random dataset. Figure 9: Failure mode reasons. insensitive to the underlying dataset distribution and is instead driven by the CVE description and fixed reasoning stereotype. Consequently, there no significant performance differences in search depth or tool access. Nevertheless, the variation arises from structural properties of the candidate sets themselves. In the random dataset, negative samples are typically drawn without semantic or structural proximity to the true patch. As result, most negative candidates exhibit clear distributional separation from the CVE description, such as unrelated filenames, components, or modification types. This effectively reduces the classification task to coarse semantic filtering problem, where identifying obvious mismatches is sufficient for correct decisions. Under such conditions, ambiguities or incompleteness in the CVE description are largely masked. This behavior is illustrated by the random commit from the pytorch-lightning repository associated with CVE-2021-4118, shown in listing 3. The CVE description states only that \"pytorch-lightning is vulnerable to Deserialization of Untrusted Data.\" As shown in the listing, the corresponding diff merely corrects typo in documentation file. Despite the severe lack of detail in the CVE description, the commit can be readily rejected, as documentation updates are clearly unrelated to deserialization vulnerability. Listing 3: Commit 3b6b6c8 randomly selected for CVE-2021-4118. diff --git a/docs/source/new-project.rst b/docs/source/new-project.rst index e5ba47351..e50d17046 100644 --- a/docs/source/new-project.rst 26 +++ b/docs/source/new-project.rst @@ -339,7 +339,7 @@ You can also add forward method to do predictions however (cid:45) you want. { return image autoencoder = LitAutoencoder() - + image_sample = autoencoder(() image_sample = autoencoder() Option 3: Production -------------------- 339 340 342 343 344 345 346 In contrast, the PatchFinder dataset deliberately constructs hard negatives that are semantically plausible, whose candidates often share keywords, file paths, or conceptual similarity with the CVE description. This collapses the decision boundary from surface-level semantic cues to deep reasoning, requiring the agent to accurately interpret the CVEs root cause, affected component, and exploit mechanism, and to align them with the specific code changes in the commit. Our failure analysis in Figure fig. 9 shows that this shift exposes two dominant weaknesses. First, CVE Misinterpretation becomes significantly more frequent, reflecting the difficulty for LLM-based agents to reliably ground under-specified or ambiguous CVE descriptions in codelevel semantics. Second, Superficial Association failures increase, indicating that when causal grounding fails, the agent tends to revert to heuristic shortcuts based on keyword overlap or coarse semantic similarity. Consequently, these results suggest that random negative construction systematically underestimates task difficulty by allowing models to succeed without resolving the core semantic and causal uncertainties inherent in real-world CVEpatch linking. In contrast, PatchFinder-style candidate sets surface these uncertainties explicitly, leading to lower apparent performance but more faithful evaluation of real-world capability. Summary for RQ2: Agents fail primarily due to insufficient semantic grounding rather than limited exploration or tool access. Most incorrect classifications arise from Superficial Associations, where agents rely on surface-level cues such as keyword overlap, file names, or coarse semantic similarity without establishing causal link between the code changes and the vulnerability. secondary but significant source of error is CVE misinterpretation, in which the agent retrieves the CVE report but misunderstands the affected component, root cause, or exploit mechanism. These failures are often amplified by overconfident early termination, where agents commit to decision without seeking disconfirming evidence, indicating that confidence calibration and causal reasoningrather than deeper searchare the primary limitations."
        },
        {
            "title": "6.3 Results of RQ3: Efficiency",
            "content": "We measure efficiency using mean token consumption decomposed into input, output, and embedding tokens. The agent operates in multi-turn fashion so every previous turn and every prior output is appended and counted as new input data; this accumulation is reflected in the input-token totals. Across the realistic dataset, the agent method Favia consumes substantially more tokens on 27 (a) PatchFinder top10 dataset (b) random 10 dataset Figure 10: Mean token usage for each model using various approaches. Table 4: Mean token usage on realistic dataset across all models. Method Input tokens LLM4VFD CommitShield Favia 6,456 9,330 66,159 Output tokens 676 1,035 1, Embedding tokens 362 0 0 Total tokens 7,494 10,365 67,202 Table 5: Token usage on random dataset across all models. Method Input tokens LLM4VFD CommitShield Favia 5,368 7,328 57,851 Output tokens 1,040 889 1,002 Embedding tokens 347 0 0 Total tokens 6,755 8,217 58, 28 average than the non-agent baselines. The difference is driven almost entirely by input tokens. As can be seen in table 4, the mean input tokens for Favia equal 66,159 compared with 6,456 for LLM4VFD and 9,330 for CommitShield. Mean output tokens remain modest and comparable across methods. Favias mean output tokens equal 1,043 while LLM4VFD and CommitShield report 676 and 1,035 respectively. Embedding tokens are nonzero only for LLM4VFD, which reports mean embedding cost of 362; embedding overhead is otherwise not applicable. We see similar usage on the random dataset. From table 5, the mean input tokens for Favia, LLM4VFD, and CommitShield is respectively 57,851, 5,368, and 7,328. Favias mean output tokens equal 1,0402 while LLM4VFD and CommitShield report 1,040 and 889 respectively. Embedding tokens are nonzero only for LLM4VFD, which reports mean embedding cost of 347. The elevated input-token cost for the agent follows directly from its multi-turn interaction pattern. Typical agent trajectories begin with retrieval of the CVE description, proceed to filelevel localization, and continue with iterative code navigation. Because every prior turn and every prior output is appended and counted as new input, these steps accumulate context across turns and inflate cumulative input tokens relative to single-pass approaches that operate on fixed shallow context. Token usage also varies across models. Larger models, such as Qwen3-235B-A22B-Instruct2507, exhibit the highest token consumption, reflecting longer trajectories and greater context retention across turns. Smaller models, such as gemma-3-27b-it, show lower overall usage while preserving the same qualitative interaction structure. These differences suggest that efficiency is influenced both by the agent framework and by model-specific reasoning behavior. As shown by the results of RQ1 (see section 6.2.3), Favias reasoning framework is largely insensitive to differences in data distribution and is driven primarily by the characteristics of the underlying CVEs. Consistent with this observation, comparison of token usage between the realistic and random datasets in fig. 10 reveals the same stable behavior, further confirming that Favias token consumption patterns are governed by CVE complexity rather than dataset composition. Importantly, this increased and stable token consumption is not incidental but necessary to avoid false positives. By enforcing consistent and CVE grounded evaluation of each candidate commit, independent of superficial security signals or generic robustness improvements, Favia expends additional reasoning effort to verify alignment between the vulnerability description and the affected code. As illustrated in section 6.1.4 from RQ1, this consistency enables Favia to correctly reject commits that appear security relevant at surface level but are unrelated to the actual vulnerability, in contrast to CommitShield and LLM4VFD. This shows that higher token usage reflects deliberate analytical rigor rather than inefficiency. Additionally, this disparity in token usage between Favia and the baselines can be translated into real monetary cost under commonly used pay-per-token APIs. As of February 2026, pricing for the latest-generation OpenAI model GPT-5.2 is $1.75 per million input tokens and $14.00 per million output tokens, with text-embedding-3-large priced at $0.13 per million tokens. Using these concrete rates provides conservative upper-bound estimate, as GPT-5.2 represents one of the most capable and expensive publicly available models. Under this pricing, Favias mean usage of 66,159 input tokens and 1,043 output tokens per CVE on the realistic dataset corresponds to an absolute cost of roughly $0.13 per sample. Non-agent baselines operating in the 7k10k token range, LLM4VFD would incur approximately $0.02 per sample, and CommitShield $0.03 per sample. Although this constitutes clear relative increase, the absolute difference remains small in practical terms. Because Favia is applied only to limited set of high-ranked candidate commits (e.g. top-10 per CVE), the total end-to-end cost per vulnerability remains well below one dollar in typical evaluation or deployment scenarios. Consequently, even when instantiated with state-of-the-art model such as GPT-5.2, the agents additional token expenditure is economically 29 feasible and justified by its ability to perform iterative, evidence-driven reasoning and fine-grained code inspection, rather than being prohibitive barrier at realistic scales. This trade-off becomes particularly compelling when considering effectiveness. On the realistic dataset, LLM4VFD, CommitShield, and Favia miss true patch on average (across models) 75, 97, and 33 times, respectively. Thus, Favia identifies on average 42 additional patches compared to the strongest baseline. Since Favia is applied to the top-10 candidate commits per CVE, the cost of fully evaluating one CVE corresponds to ten samples, yielding per-CVE cost of 10 0.13$ = 1.30$. Recovering these 42 additional patches therefore incurs total cost of 42 1.30$ = 54.6$. While not negligible, this cost is modest when weighed against the potential consequences of missing vulnerability-fixing patch, which can entail severe security, financial, and operational risks. In summary, the agent pays clear efficiency price in mean input tokens in order to perform iterative, evidence-driven localization and inspection. This additional computational cost supports systematic grounding in CVE descriptions and targeted code inspection rather than increased final verbosity. Summary for RQ3: On the realistic dataset the agent Favia incurs substantially higher mean input-token cost, 66,159 tokens, and higher mean total, 67,202 tokens, because every prior turn and output is appended and counted as new input. Mean output tokens remain modest at 1,043 and embedding overhead is limited to LLM4VFD at 362. The result is clear efficiency trade-off: the agent expends more computation to enable iterative, evidence-driven localization while nonagent baselines remain far more token-efficient but operate with shallower, single-pass context."
        },
        {
            "title": "7.1 Comparison with Related Studies",
            "content": "Our findings show that Favias agent-based reasoning provides substantial advancement over existing vulnerability-fix detection approaches. Unlike traditional machine-learning models or embedding-based LLM systems discussed in section 3.1, Favia performs iterative, evidence-grounded reasoning that allows it to correlate CVE semantics with code changes rather than relying on surface-level similarity. This enables the agent to correctly identify indirect fixes, multi-file patches, and subtle logic adjustments that prior systems frequently miss. Crucially, Favia can fully automate the final selection of the correct patch, eliminating the manual expert intervention required by ranking-based systems such as PatchFinder K. Li et al., 2024, where humans must still inspect the top-k candidates to determine the true fix. Our evaluation also highlights critical gap in recent LLM-based systems such as LLM4VFD X. Yang et al., 2025 and CommitShield (Z. Wu et al., 2025). These methods report strong performance but evaluate exclusively on randomly sampled commits, where most negatives are trivially unrelated to security. Such settings dramatically inflate performance and do not reflect real-world difficulty. By contrast, we evaluate on both heterogeneous random commits (random_10) and homogeneous, security-relevant candidate sets (PatchFinder_top10), where commits are highly similar and difficult to distinguish. This dual evaluation reveals that Favia maintains highest F1 score on all accounts. We also see significant discrepancy between PatchFinders reported recall@10 of 80.63% and the 48% recall we observe on our realistic dataset. This further underscores the importance of sound, rigorous, and representative evaluation dataset which capture the true complexity of vulnerability-fix identification. Finally, our efficiency analysis shows that agent-based reasoning incurs higher computational cost due to multi-step interactions. However, because Favia evaluates only the top-k 30 ranked commits, the approach remains practical even for large repositories. Importantly, our candidate-ranking stage is not tied to any specific method. It can be powered by classical machine-learning models, lightweight heuristics, or future retrieval systems. Unlike CommitShield (Z. Wu et al., 2025), which relies on language-specific program-analysis pipelines and heavyweight static-analysis infrastructure, Favia performs deep semantic code reasoning while remaining fully language-agnostic and lightweight. This flexibility allows Favia to operate across diverse repositories without the engineering overhead required by analysis-dependent systems. 7."
        },
        {
            "title": "Implications to Academia and Industry",
            "content": "For the research community, our findings highlight two important implications. First, evaluation methodology strongly influences perceived performance. Randomly sampled candidate sets substantially overestimate the effectiveness of all approaches, including LLM-based ones. Realistic, ranked candidate evaluation is therefore essential for meaningful progress in vulnerability-fix detection research. Second, our analysis shows that agent behavior and failure modes can be systematically studied, opening new directions for understanding and improving LLM-based systems beyond aggregate metrics. For industry practitioners, Favia demonstrates the feasibility of deploying agent-based reasoning in security workflows where precision is critical. While the agent incurs higher computational cost, this cost is concentrated in input tokens used for evidence gathering rather than verbose outputs, aligning well with use cases such as security triage, patch auditing, and dependency risk assessment. Moreover, the agents step-by-step reasoning and tool usage provide natural hooks for interpretability and human-in-the-loop validation, which are essential in high-stakes security settings."
        },
        {
            "title": "7.3 Threats to Validity",
            "content": "7.3.1 External validity potential threat to external validity arises from the selection of models included in our evaluation. We mitigate this risk by evaluating diverse set of models spanning multiple families, scales, pre-training corpora, and learning objectives. Concerning our experiments, they are conducted on real-world CVEs and corresponding commits drawn from publicly available repositories and spanning multiple programming languages. This substantially improves ecological validity compared to evaluations based on synthetic vulnerabilities or narrowly scoped benchmarks. Because large repositories may contain an extremely high number of commits, processing all commits is computationally and financially infeasible. In line with earlier works K. Li et al., 2024, we collect representatively large candidate pool of up to 5,000 commits per repository. Nevertheless, several limitations remain. First, the construction of realistic candidate sets relies on PatchFinder. While PatchFinder constitutes strong and widely applicable baseline, alternative retrieval or ranking methods could yield candidate distributions with different characteristics, potentially affecting downstream agent performance. Second, although the dataset spans multiple programming languages, it is skewed toward languages that are well supported by existing tooling and prevalent in CVE reporting, which may limit generalization to lessrepresented ecosystems. Third, our evaluation is restricted to GitHub-hosted repositories and publicly disclosed CVEs; vulnerabilities in proprietary codebases or those without public advisories may exhibit different patterns that are not captured by our study. Finally, baseline methods introduce additional external validity considerations. In particular, CommitShield (Z. Wu et al., 2025) was originally designed for and C++ only. Although we extend CommitShield to support C, C++, Python, PHP, Java, and Go, its language coverage 31 remains limited relative to the full diversity of real-world repositories. To ensure fair and controlled comparison across baselines, we exclude additional auxiliary artifacts such as issue reports and linked discussions. 7.3."
        },
        {
            "title": "Internal validity",
            "content": "To mitigate bias, we apply consistent experimental protocols across all methods, including identical dataset splits, evaluation metrics, and candidate sets. Dataset splits are performed at the repository level to prevent information leakage between training, validation, and test sets. All baselines are implemented following their original specifications, with deviations explicitly documented when required for scalability or language coverage. Nonetheless, internal threats remain. First, agent behavior may be sensitive to prompt design, tool availability, and step budgets. While we fix these parameters across experiments, alternative prompts or budgets could influence absolute performance. Second, LLMs may contain latent knowledge of well-known CVEs from pretraining. We partially try to mitigate this risk by analyzing tool usage patterns and identifying memorization-related failure modes, but we cannot fully eliminate the possibility of implicit leakage. We do not add countermeasures for existing approaches, which should mitigate potential bias. Third, failure mode categorization relies on judgments produced by an auxiliary LLM, which may introduce labeling noise despite using model from an independent family. Here we do however use an model from completely separate model family in order to produce consistent results."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "We introduced Favia, forensic, agent-based framework for identifying vulnerability-fixing commits that combines scalable candidate ranking with deep, iterative semantic reasoning. Through large-scale evaluation on the CVEVC dataset, we showed that agent-based reasoning consistently outperforms traditional and existing LLM-based approaches under realistic candidate selection, achieving the strongest precisionrecall trade-offs and highest F1-scores across models. Our results further demonstrate that evaluations based on random sampling of commits substantially overestimate performance, while realistic candidate sets expose meaningful differences between methods. Most agent failures arise from superficial semantic associations and misinterpretation of CVE descriptions rather than from limited exploration or tool access. Overall, Favia demonstrates that forensic, agent-driven analysis is practical and effective paradigm for vulnerability-fix detection. These findings motivate several directions for future work. Improving causal reasoning and confidence calibration may further reduce false positives in ambiguous candidate sets, while adaptive control of reasoning depth could reduce the computational cost of agent-based analysis. More broadly, extending CVEVC and similar datasets to support additional ranking strategies, languages, and vulnerability classes would further enable realistic, scalable evaluation without the prohibitive cost of full repository analysis."
        },
        {
            "title": "Acknowledgments",
            "content": "The empirical results were supported through the computational resources of HPC facilities at the Norwegian University of Science and Technology (NTNU) (Själander et al., 2019)."
        },
        {
            "title": "Data Availability",
            "content": "The cvevc_candidates dataset is available at: https://huggingface.co/datasets/andstor/cvevc_candidates The cvevc_cve dataset is available at: https://huggingface.co/datasets/andstor/cvevc_cve The cvevc_commit dataset is available at: https://huggingface.co/datasets/andstor/cvevc_commits mapping dataset between the cvevc_cve and cvevc_commit dataset is available at: https://huggingface.co/datasets/andstor/cvevc_cve_commit_mappings Experiment code and results are available at: https://github.com/andstor/agentic-security-patch-classification-replication-p ackage Experiments trajectories are available at: https://huggingface.co/datasets/andstor/favia_trajectories Experiments traces are available at following demos: https://huggingface.co/spaces/andstor/phoenix-cvevc_candidates_PatchFinder_top1 0 https://huggingface.co/spaces/andstor/phoenix-cvevc_candidates_random_"
        },
        {
            "title": "References",
            "content": "Cabrera Lozoya, Rocío et al. (2021). Commit2Vec: Learning Distributed Representations of Code Changes. In: SN Computer Science 2.3, p. 150. issn: 2661-8907. doi: 10.1007/s4297 9-021-00566-z. url: https://doi.org/10.1007/s42979-021-00566-z. Chen, Tianyu et al. (2024). CompVPD: Iteratively Identifying Vulnerability Patches Based on Human Validation Results with Precise Context. arXiv: 2310.02530 [cs.CR]. url: https ://arxiv.org/abs/2310.02530. [SW Rel.] Chroma, Chroma version 1.0.20, 2025. url: https://github.com/chroma-core/ch roma. Dallmeier, Valentin and Thomas Zimmermann (Nov. 2007). Extraction of bug localization benchmarks from history. In: Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering. ASE07. ACM, pp. 433436. doi: 10.1145/1 321631.1321702. url: http://dx.doi.org/10.1145/1321631.1321702. Doll, John et al. (2025). Unraveling Log4Shell: Analyzing the Impact and Response to the Log4j Vulnerabil. arXiv: 2501.17760 [cs.CR]. url: https://arxiv.org/abs/2501.17760. 33 Hoang, Thong, Hong Jin Kang, et al. (2020). CC2Vec: distributed representations of code changes. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. ICSE 20. Seoul, South Korea: Association for Computing Machinery, pp. 518529. isbn: 9781450371216. doi: 10.1145/3377811.3380361. url: https://doi.org/10.1145/3 377811.3380361. Hoang, Thong, Julia Lawall, et al. (2019). PatchNet: tool for deep patch classification. In: Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings. ICSE 19. Montreal, Quebec, Canada: IEEE Press, pp. 8386. doi: 10.1109/ICSECompanion.2019.00044. url: https://doi.org/10.1109/ICSE-Companion.2019.00044. Hugging Face (2025). Hugging Face: The AI community building the future. url: https://hugg ingface.co (visited on 02/11/2026). [SW Rel.] Joern, Joern - The Bug Hunters Workbench version v4.0.426, 2025. url: https://j oern.io. Kim, Sunghun et al. (2008). Predicting faults from cached history. In: Proceedings of the 1st India Software Engineering Conference. ISEC 08. Hyderabad, India: Association for Computing Machinery, pp. 1516. isbn: 9781595939173. doi: 10.1145/1342211.1342216. url: https://doi.org/10.1145/1342211.1342216. Li, Frank and Vern Paxson (2017). Large-Scale Empirical Study of Security Patches. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. CCS 17. Dallas, Texas, USA: Association for Computing Machinery, pp. 22012215. isbn: 9781450349468. doi: 10.1145/3133956.3134072. url: https://doi.org/10.1145/313395 6.3134072. Li, Kaixuan et al. (2024). PatchFinder: Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software. In: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. ISSTA 2024. Vienna, Austria: Association for Computing Machinery, pp. 590602. isbn: 9798400706127. doi: 10 .1145/3650212.3680305. url: https://doi.org/10.1145/3650212.3680305. Li, Zhiyu et al. (2022). Automating Code Review Activities by Large-Scale Pre-training. arXiv: 2203.09095 [cs.SE]. url: https://arxiv.org/abs/2203.09095. Linux Kernel Organization (2025). Linux CVE Announce Archive. url: https://lore.kerne l.org/linux-cve-announce/2025123055-directory-hemlock-a282@gregkh/ (visited on 12/30/2025). Machiry, Aravind et al. (2020). SPIDER: Enabling Fast Patch Propagation In Related Software Repositories. In: 2020 IEEE Symposium on Security and Privacy (SP), pp. 15621579. doi: 10.1109/SP40000.2020.00038. Meta (2024). Llama 3.3 model card. url: https://github.com/meta-llama/llama-models/b lob/main/models/llama3_3/MODEL_CARD.md (visited on 01/23/2026). Mockus, Audris and Lawrence G. Votta (2000). Identifying Reasons for Software Changes Using Historic Databases. In: Proceedings of the International Conference on Software Maintenance (ICSM00). ICSM 00. USA: IEEE Computer Society, p. 120. isbn: 0769507530. Nguyen, Son, Thanh Trong Vu, and Hieu Dinh Vo (2023). VFFINDER: Graph-Based Approach for Automated Silent Vulnerability-Fix Identification. In: 2023 15th International Conference on Knowledge and Systems Engineering (KSE), pp. 16. doi: 10.1109/KSE5912 8.2023.10299438. Nguyen, Truong Giang, Thanh Le-Cong, Hong Jin Kang, Xuan-Bach D. Le, et al. (2022). VulCurator: vulnerability-fixing commit detector. In: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE 2022. Singapore, Singapore: Association for Computing Machinery, 34 pp. 17261730. isbn: 9781450394130. doi: 10.1145/3540250.3558936. url: https://doi.o rg/10.1145/3540250.3558936. Nguyen, Truong Giang, Thanh Le-Cong, Hong Jin Kang, Ratnadira Widyasari, et al. (Aug. 2023). Multi-Granularity Detector for Vulnerability Fixes. In: IEEE Transactions on Software Engineering 49.8, pp. 40354057. issn: 1939-3520. doi: 10.1109/TSE.2023.3281275. Nguyen-Truong, Giang et al. (2022). HERMES: Using Commit-Issue Linking to Detect VulnerabilityFixing Commits. In: 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 5162. doi: 10.1109/SANER53432.2022.00018. OpenAI et al. (2025). gpt-oss-120b & gpt-oss-20b Model Card. arXiv: 2508.10925 [cs.CL]. url: https://arxiv.org/abs/2508.10925. Pedregosa, F. et al. (2011). Scikit-learn: Machine Learning in Python. In: Journal of Machine Learning Research 12, pp. 28252830. Perl, Henning et al. (2015). VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits. In: Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. CCS 15. Denver, Colorado, USA: Association for Computing Machinery, pp. 426437. isbn: 9781450338325. doi: 10.1145/2810103.2813604. url: https://doi.org/10.1145/2810103.2813604. [SW Rel.] Roucher, Aymeric et al., smolagents: smol library to build great agentic systems. Version 1.21.2, 2025. url: https://github.com/huggingface/smolagents. Sabetta, Antonino and Michele Bezzi (Sept. 2018). Practical Approach to the Automatic Classification of Security-Relevant Commits . In: 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). Los Alamitos, CA, USA: IEEE Computer Society, pp. 579582. doi: 10.1109/ICSME.2018.00058. url: https://doi.ieeecomputers ociety.org/10.1109/ICSME.2018.00058. Själander, Magnus et al. (2019). EPIC: An Energy-Efficient, High-Performance GPGPU Computing Research Infrastructure. arXiv: 1912.05848 [cs.DC]. Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller (May 2005). When do changes induce fixes? In: ACM SIGSOFT Software Engineering Notes 30.4, pp. 15. issn: 0163-5948. doi: 10.1145/1082983.1083147. url: http://dx.doi.org/10.1145/1082983.1083147. Sun, Jiamou, Jieshan Chen, et al. (Apr. 2024). Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports. In: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. ICSE 24. ACM, pp. 113. doi: 10.1145/3597503.3639202. url: http://dx.doi.org/10.1145/3597503.3639202. Sun, Jiamou, Zhenchang Xing, et al. (2023). Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation. In: Proceedings of the 45th International Conference on Software Engineering. ICSE 23. Melbourne, Victoria, Australia: IEEE Press, pp. 970982. isbn: 9781665457019. doi: 10.1109/ICSE48619.2023.00089. url: https://doi.org/10.1 109/ICSE48619.2023.00089. Tan, Xin et al. (2021). Locating the Security Patches for Disclosed OSS Vulnerabilities with Vulnerability-Commit Correlation Ranking. In: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. CCS 21. Virtual Event, Republic of Korea: Association for Computing Machinery, pp. 32823299. isbn: 9781450384544. doi: 10 .1145/3460120.3484593. url: https://doi.org/10.1145/3460120.3484593. Team, Gemma et al. (2025). Gemma 3 Technical Report. arXiv: 2503 . 19786 [cs.CL]. url: Team, Qwen (2025). Qwen3 Technical Report. arXiv: 2505.09388 [cs.CL]. url: https://arxi https://arxiv.org/abs/2503.19786. v.org/abs/2505.09388. 35 Tian, Yuan, Julia Lawall, and David Lo (June 2012). Identifying Linux bug fixing patches. In: 2012 34th International Conference on Software Engineering (ICSE), pp. 386396. doi: 10.1109/ICSE.2012.6227176. [SW Rel.] Tree-sitter, Tree-sitter version v0.21.3, 2024. url: https://tree-sitter.github.i o/tree-sitter/. Wang, Shichao et al. (Mar. 2022). VCMatch: Ranking-based Approach for Automatic Security Patches Localization for OSS Vulnerabilities. In: 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 589600. doi: 10.1109/SANE R53432.2022.00076. Wang, Shu et al. (2023). GraphSPD: Graph-Based Security Patch Detection with Enriched Code Semantics. In: 2023 IEEE Symposium on Security and Privacy (SP), pp. 24092426. doi: 10.1109/SP46215.2023.10179479. Wang, Xinda, Kun Sun, et al. (June 2019). Detecting \"0-Day\" Vulnerability: An Empirical Study of Secret Security Patch in OSS. In: 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pp. 485492. doi: 10.1109/DSN.2019.00056. Wang, Xinda, Shu Wang, et al. (2021). PatchRNN: Deep Learning-Based System for Security Patch Identification. In: MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM). San Diego, CA, USA: IEEE Press, pp. 595600. doi: 10.1109/MILCOM52596.2 021.9652940. url: https://doi.org/10.1109/MILCOM52596.2021.9652940. Wu, Bozhi et al. (July 2022). Enhancing Security Patch Identification by Capturing Structures in Commits . In: IEEE Transactions on Dependable and Secure Computing 01, pp. 115. issn: 1941-0018. doi: 10.1109/TDSC.2022.3192631. url: https://doi.ieeecomputersoci ety.org/10.1109/TDSC.2022.3192631. Wu, Zhaonan et al. (Apr. 2025). COmmitSHield: Tracking Vulnerability Introduction and Fix in Version Control Systems. In: 2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). IEEE, pp. 279290. doi: 10.1109 /ICSE-Companion66252.2025.00082. url: https://doi.org/10.1109/ICSE-Companion66 252.2025.00082. Yang, Jeong, Young Lee, and Arlen P. McDonald (2022). SolarWinds Software Supply Chain Security: Better Protection with Enforced Policies and Technologies. In: Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing. Ed. by Roger Lee. Cham: Springer International Publishing, pp. 4358. isbn: 978-3-030-92317-4. doi: 10.1007 /978-3-030-92317-4_4. url: https://doi.org/10.1007/978-3-030-92317-4_4. Yang, John et al. (2024). SWE-agent: agent-computer interfaces enable automated software engineering. In: Proceedings of the 38th International Conference on Neural Information Processing Systems. NIPS 24. Vancouver, BC, Canada: Curran Associates Inc. isbn: 9798331314385. Yang, Xu et al. (2025). Code Change Intention, Development Artifact and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM. arXiv: 2501.14983 [cs.SE]. url: https://arxiv.org/abs/2501.14983. Yao, Shunyu et al. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv: 2210.03629 [cs.CL]. url: https://arxiv.org/abs/2210.03629. Zhang, Yanzhao et al. (2025). Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv: 2506.05176 [cs.CL]. url: https://arxiv.org/abs/2 506.05176. Zhou, Jiayuan et al. (Nov. 2021). Finding Needle in Haystack: Automated Mining of Silent Vulnerability Fixes. In: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 705716. doi: 10.1109/ASE51524.2021.9678720. Zhou, Yaqin and Asankhaya Sharma (2017). Automated identification of security issues from commit messages and bug reports. In: Proceedings of the 2017 11th Joint Meeting on Foun36 dations of Software Engineering. ESEC/FSE 2017. Paderborn, Germany: Association for Computing Machinery, pp. 914919. isbn: 9781450351058. doi: 10.1145/3106237.3117771. url: https://doi.org/10.1145/3106237.3117771. Zhou, Yaqin, Jing Kai Siow, et al. (Sept. 2021). SPI: Automated Identification of Security Patches via Commits. In: ACM Trans. Softw. Eng. Methodol. 31.1. issn: 1049-331X. doi: 10.1145/3468854. url: https://doi.org/10.1145/3468854."
        },
        {
            "title": "A Prompts Used",
            "content": "A.1 System prompt"
        },
        {
            "title": "Prompt Template",
            "content": "You are an expert assistant who can solve any task using code blobs. You will be given task to solve as best you can. To do so, you have been given access to list of tools: these tools are basically Python functions which you can call with code. To solve the task, you must plan forward to proceed in series of steps, in cycle of Thought, Code, and Observation sequences. At each step, in the Thought: sequence, you should first explain your reasoning towards solving the task and the tools that you want to use. Then in the Code sequence you should write the code in simple Python. The code sequence must be opened with <code>, and closed with </code>. During each intermediate step, you can use print() to save whatever important information you will then need. These print outputs will then appear in the Observation: field, which will be available as input for the next step. In the end you have to return final answer using the final_answer tool. Here are few examples using notional tools: --- Task: \"Generate an image of the oldest person in this document.\" Thought: will proceed step by step and use the following tools: document_qa to find the oldest person in the document, then image_generator to generate an image according to the answer. <code> answer = document_qa(document=document, question=\"Who is the oldest person mentioned?\") print(answer) </code> Observation: \"The oldest person in the document is John Doe, 55 year old lumberjack living in Newfoundland.\" Thought: will now generate an image showcasing the oldest person. <code> image = image_generator(\"A portrait of John Doe, 55-year-old man living in Canada.\") 37 final_answer(image) </code> --- Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\" Thought: will use python code to compute the result of the operation and then return the final answer using the final_answer tool <code> result = 5 + 3 + 1294.678 final_answer(result) </code> --- Task: \"Answer the question in the variable question about the image stored in the variable image. The question is in French. You have been provided with these additional arguments, that you can access using the keys as variables in your python code: {question: Quel est lanimal sur limage?, image: path/to/image.jpg}\" Thought: will use the following tools: translator to translate the question into English and then image_qa to answer the question on the input image. <code> translated_question = translator(question=question, src_lang=\"French\", tgt_lang=\" English\") print(f\"The translated question is {translated_question}.\") answer = image_qa(image=image, question=translated_question) final_answer(f\"The answer is {answer}\") </code> --- Task: In 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer. What does he say was the consequence of Einstein learning too much math on his creativity, in one word? Thought: need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin. <code> pages = web_search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\") print(pages) </code> Observation: No result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\". Thought: The query was maybe too restrictive and did not find any results. Lets try again with broader query. 38 <code> pages = web_search(query=\"1979 interview Stanislaus Ulam\") print(pages) </code> Observation: Found 6 pages: [Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oralhistories/stanislaus-ulams-interview-1979/) [Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattanproject/ulam-manhattan-project/) (truncated) Thought: will read the first 2 pages to know more. <code> for url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams -interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulammanhattan-project/\"]: whole_page = visit_webpage(url) print(whole_page) print(\"n\" + \"=\"*80 + \"n\") # Print separator between pages </code> Observation: Manhattan Project Locations: Los Alamos, NM Stanislaus Ulam was Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at (truncated) Thought: now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Lets answer in one word. <code> final_answer(\"diminished\") </code> --- Task: \"Which city has the highest population: Guangzhou or Shanghai?\" Thought: need to get the populations for both cities and compare them: will use the tool web_search to get the population of both cities. <code> for city in [\"Guangzhou\", \"Shanghai\"]: print(f\"Population {city}:\", web_search(f\"{city} population\") </code> Observation: Population Guangzhou: [Guangzhou has population of 15 million inhabitants as of 2021.] Population Shanghai: 26 million (2019) 39 Thought: Now know that Shanghai has the highest population. <code> final_answer(\"Shanghai\") </code> --- Task: \"What is the current age of the pope, raised to the power 0.36?\" Thought: will use the tool wikipedia_search to get the age of the pope, and confirm that with web search. <code> pope_age_wiki = wikipedia_search(query=\"current pope age\") print(\"Pope age as per wikipedia:\", pope_age_wiki) pope_age_search = web_search(query=\"current pope age\") print(\"Pope age as per google search:\", pope_age_search) </code> Observation: Pope age: \"The pope Francis is currently 88 years old.\" Thought: know that the pope is 88 years old. Lets compute the result using python code. <code> pope_current_age = 88 ** 0.36 final_answer(pope_current_age) </code> Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions: <code> def cve_report(cve_id: string) -> string: \"\"\"This is tool that fetches information about CVE (Common Vulnerabilities and Exposures) entry. The information is returned as markdown. Args: cve_id: The CVE ID to fetch information for. \"\"\" def cwe_report(cwe_id: string, view: string) -> string: \"\"\"This is tool that fetches information about CWE (Common Weakness Enumeration) entry. The information is returned as markdown. Args: cwe_id: The CWE ID to fetch information for. Example: 79 \"\"\" def file_search(query: string) -> string: \"\"\"This is tool that searches for files in Git repository by filename, keyword, or glob pattern. It uses git ls-files to list files tracked by the 40 repository and supports glob-style searches. Args: query: The filename, keyword, or glob pattern to search for in the repository. \"\"\" def code_search(query: string, file: string) -> string: \"\"\"Tool for searching for file contents in git repository files. Args: query: The string to search for. Can only be simple string. file: The file to search in. Optional. \"\"\" def open_file(path: string) -> string: \"\"\" tool to open file and initialize it for paginated viewing. The window size is set to 100 lines. Args: path: The path to the file to open. \"\"\" def scroll_file(direction: string) -> string: \"\"\" tool to scroll up or down through the currently opened file. Scrolls by one window size. Args: direction: The direction to scroll: up or down. \"\"\" def final_answer(answer: any) -> any: \"\"\"Provides final answer to the given problem. Args: answer: The final answer to the problem \"\"\" </code> Here are the rules you should always follow to solve your task: 1. Always provide Thought: sequence, and <code> sequence ending with </ code>, else you will fail. 2. Use only variables that you have defined! 3. Always use the right arguments for the tools. DO NOT pass the arguments as dict as in answer = wikipedia_search({query: \"What is the place where James Bond lives?\"}), but use the arguments directly as in answer = wikipedia_search( 41 query=\"What is the place where James Bond lives?\"). 4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, call to wikipedia_search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block. 5. Call tool only when needed, and never re-do tool call that you previously did with the exact same parameters. 6. Dont name any new variable with the same name as tool: for instance dont name variable final_answer. 7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables. 8. You can use imports in your code, but only from the following list of modules: [collections, datetime, itertools, math, queue, random, re, stat , statistics, time, unicodedata] 9. The state persists between code executions: so if in one step youve created variables or imported modules, these will all persist. 10. Dont give up! Youre in charge of solving the task, not providing directions to solve it. Now Begin! A.2 Task prompt Prompt Template Figure 11: System prompt template. ## Task Overview You are tasked with determining whether the commit {{ commit_id }} in the repository {{ repository }} is the actual patch for the vulnerability identified as {{ cve_id }}. ## Code Changes (from git show for {{ commit_id }}): diff {{ commit_diff }} You have access to the full codebase at the parent commit of {{ commit_id }}. Use all available tools at your disposal to analyze the codebase-such as searching files, reading file contents, navigating function definitions, and locating usage patterns. --- ## 1. Understand the Vulnerability ({{ cve_id }}) Begin by retrieving and then carefully reviewing the official CVE report for {{ cve_id }}. The report is your primary source of truth about the nature of the vulnerability. Take the time to read it thoroughly and ensure you understand the context and scope of the issue it describes. ### 1.1 Identify Affected Components 42 Determine which parts of the system the vulnerability impacts. This may include specific modules, source files, classes, functions, or configuration layers. Understanding what is affected will help guide your investigation into whether the commit addresses these areas. ### 1.2 Understand the Symptoms and Impact Extract clear description of what the vulnerability causes when exploited. What is the practical impact? This might include things like privilege escalation, denial of service, authentication bypass, memory corruption, or information disclosure. ### 1.3 Analyze Entry Points Identify how the vulnerability is triggered. This could be specific API call, input vector (e.g., HTTP request, user input), or usage pattern. Understanding this helps in locating where and how the system is exposed. ### 1.4 Determine the Root Cause Look for the underlying flaw in logic, data handling, or assumptions that made the vulnerability possible. Common root causes include missing input validation, unsafe memory operations, race conditions, improper access control, or insecure default configurations. ### 1.5 Optional: Use CWE Classification If available, use the CVE-to-CWE mapping to categorize the vulnerability into known class (e.g., buffer overflow, SQL injection, use-after-free). This may give you additional guidance on what kind of fixes to expect in the code. --- ## 2. Analyze the Commit Changes Next, examine the code changes introduced by the commit {{commit_id}}. This will help you form hypothesis about what the commit is doing and whether it is related to the CVE. ### 2.1 Identify Scope of Changes Go through the diff and identify what files, functions, methods, classes, constants, or configuration entries have been changed. Pay attention to whether these components overlap with those described in the CVE. ### 2.2 Understand the Purpose of Each Change Interpret each change carefully. What is the modification doing? Is it fixing logic bug, adding input validation, tightening access controls, changing types, or restructuring control flow? Make sure you understand the *intended effect* of each code modification. ### 2.3 Map Changes to the CVE Description Attempt to directly associate each code change with some aspect of the CVE you reviewed in step 1. Does new check correspond to missing validation? Does the removal of function correlate with the described insecure behavior? If change cannot be tied back to the CVE, be cautious about assuming relevance. --- ## 3. Correlate Code Changes with the CVE This step is the most critical: you must rigorously validate whether the commit actually addresses and fixes the vulnerability described in the CVE. ### 3.1 Evaluate Direct Fixes Determine whether the changes mitigate or eliminate the vulnerable behavior. Does the new code prevent the exploit from occurring via the same entry point? Does it neutralize the root cause? Look for structural corrections that align with the vulnerabilitys nature. ### 3.2 Trace Usage and Context 43 Investigate how the modified code is used across the repository. Trace function calls, variable usage, and related logic paths. Make sure that the new behavior propagates correctly and consistently throughout the codebase and that it plausibly fixes the described issue. ### 3.3 Confirm Against Symptoms and Entry Points Verify that the changes prevent the previously identified symptoms from occurring under the same conditions. Use the same entry points and see if the new logic would block or mitigate the attack path. --- ## Common Mistakes to Avoid Your task is to **verify**, not speculate. Any unjustified assumption is grounds for failure. Do not conclude that commit is related to CVE merely because it modifies the same file or function; superficial associations are not sufficient. Similarly, CVE mention in commit message does not guarantee that the commit implements valid fix-only the code changes themselves matter. Do not rely on commit dates to determine relevance, as the actual patch may precede or follow the CVE publication by significant margin. Finally, be cautious of version bump or release-tag commits: these may indicate when fix was included but often do not contain the fix themselves. Your analysis should focus strictly on code-level changes that directly and verifiably address the vulnerability. --- ## Final Answer When you have completed your analysis, please provide your answer in the following format: <code> result = { \"explanation\": \"Detailed justification including the key evidence you used.\" \"confidence\": 1 to 5, # How probable this commit is patch (see scale below) \"answer\": True or False, # Is this commit patch for the CVE? } final_answer(result) </code> - The answer should be True if the commit is valid patch for the CVE, and False otherwise. - The confidence is an integer score from 1 (lowest) to 5 (highest) based on how strongly the evidence supports your conclusion of the commit being valid CVE patch. See the confidence scale below for guidance. - The explanation should clearly explain your reasoning process, mentioning key findings such as how the changes address the CVEs root cause, symptoms, affected components, or entry points. ### Confidence Scale - 1 = No confidence (little or unclear evidence) - 2 = Low confidence (some clues, major gaps) - 3 = Moderate confidence (likely correct, but nontrivial uncertainty) - 4 = High confidence (strong supporting evidence, small doubts) - 5 = Full confidence (clear and comprehensive verification) Figure 12: Task prompt template."
        }
    ],
    "affiliations": [
        "CSIROs Data61, Canberra, ACT, Australia",
        "Norwegian University of Science and Technology, Trondheim, Norway"
    ]
}