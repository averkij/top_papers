{
    "paper_title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding",
    "authors": [
        "Jiaze Li",
        "Hao Yin",
        "Wenhui Tan",
        "Jingyang Chen",
        "Boshen Xu",
        "Yuxun Qu",
        "Yijing Chen",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench."
        },
        {
            "title": "Start",
            "content": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding Jiaze Li1*, Hao Yin2, Wenhui Tan3, Jingyang Chen1, Boshen Xu3 Yuxun Qu1, Yijing Chen3, Jianzhong Ju1, Zhenbo Luo1, Jian Luan1 1MiLM Plus, Xiaomi Inc. 2Independent Researcher 3Renmin University of China 5 2 0 2 5 ] . [ 2 6 2 0 3 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1) longform video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the models reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench. 1. Introduction Multimodal reasoning is fundamental to many real-world applications, such as interpreting scientific figures, per- *Project leader. These authors contributed equally. Corresponding author. forming geometric reasoning, and solving complex visionlanguage understanding tasks. However, when reinforcement learning (RL)-based reasoning strategies [12] are transferred from text-only models to multimodal settings [30, 36, 55], they often fail to show clear advantages over fast-thinking models and may even underperform in certain cases. This phenomenon primarily arises because current multimodal large language models (MLLMs) typically generate outputs under token-level Markov assumption [26, 58], relying on local contextual dependencies. Such locality often leads to repetitive or incorrect reasoning steps [59]. Recent studies suggest that incorporating selfreflection mechanism [16, 28] can mitigate these issues. By explicitly guiding the model to review, evaluate, and revise its reasoning trajectory, self-reflection helps prune invalid or erroneous reasoning paths, improve logical consistency, and promote deeper multimodal understanding[10, 41]. Despite significant progress in reflection mechanisms, most studies rely on text-only reconsideration processes. Such methods perform well on general multimodal tasks. However, when directly applied to long-form video understanding, their limitations become evident. As demonstrated in our experiments in Sec. 2.1, the representative text-based reflection approach, VL-Rethinker [42], leads to degraded performance in long-form video scenarios. We attribute this phenomenon to two main factors: Unlike image understanding, long-form video reasoning involves richer and more dynamic visual inputs. Purely text-based reflection is insufficient to correct reasoning errors without explicitly reconsidering visual information. Text-only reflection lack cross-modal interaction capabilities, preventing the model from integrating visual cues during reflection. This limitation restricts the reasoning improvement potential of MLLMs in video understanding tasks. To address these challenges, we propose the concept of Figure 1. Operational workflow of the proposed REVISOR framework, contrasting it with traditional reflection mechanisms. The top panel illustrates typical traditional approach, often employing text-based re-evaluation mechanism. In contrast, the bottom panel details the REVISOR framework. This process involves two distinct stages: (1) Initial Inference, which generates preliminary reasoning trace and identifies critical regions for detailed analysis; and (2) Reflective Reasoning, which integrates this initial trace with newly sampled, fine-grained visual evidence to yield refined and robust final prediction. multimodal self-reflection, which extends traditional textbased reflection into an integrated process encompassing both language and vision. This concept envisions model that not only reviews and refines its textual reasoning but also revisits key video segments during reflection. In doing so, the model achieves joint correction of both its linguistic reasoning logic and its visual attention focus. As demonstrated in Sec. 2.2, incorporating ground-truth key video segments during reflection significantly improves accuracy on long-form video understanding tasks, highlighting the effectiveness and promise of multimodal self-reflection for enhanced reasoning in long-form video comprehension. Motivated by the insights above, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), novel two-stage reasoning framework designed to enhance video understanding. REVISOR leverages toolaugmented multimodal reflection mechanism that enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, thereby enhancing reasoning capability. As illustrated in Fig. 1, REVISOR comprises an MLLM that collaborates with viIn Stage 1, the MLLM performs an initial sual toolbox. reasoning step, identifies video segments requiring further examination, and invokes the visual toolbox to resample key frames from these segments as supplementary inputs. In Stage 2, the MLLM integrates the initial reasoning trace with the reviewed visual frames to iteratively refine its reasoning, ultimately generating more accurate response. Relying solely on verification-based reinforcement from the final answer during GRPO [35] training constrains REVISORs capacity to identify question-relevant review segments in its reflection stage. To overcome this limitation, we propose Dual Attribution Decoupled Reward (DADR) mechanism in Sec. 3.2, which supplements the final-answer verification reward with Causal Segment Sufficiency Reward (CSSR). The CSSR enforces causal alignment between the models reasoning and the selected video evidence, rewarding correctness only when the answer is derived exclusively from those segments. This mechanism explicitly encourages the model to focus on informative visual cues while implicitly discouraging reliance on irrelevant or spurious content, thereby enhancing REVISORs multimodal reasoning consistency and overall robustness. The REVISOR framework significantly enhances the video understanding capabilities of MLLMs without requiring supplementary supervised fine-tuning (SFT) or external models. Across the VideoMME [9], LongVideoBench [48], MLVU [62], and LVBench [43] benchmarks, REVIFigure 2. Motivation for proposing multimodal reflection mechanism. Left: Text-only reflection mechanisms, such as VL-Rethinker, achieve significant performance improvements in image understanding tasks. Middle: However, applying the same text-based reflection strategy to long-form video understanding leads to performance degradation. Right: Incorporating revisit of key video segments during the reflection stage effectively improves performance on video understanding tasks. SOR consistently improves the base models average accuracy by about 2%. Extended analysis in Sec. 5 validates two key drivers of REVISORs success: (1) for long-form video understanding, revisiting and refining visual information is more critical than rethinking the textual reasoning process; and (2) DADR mechanism ensures the precise recall and utilization of salient visual cues. In summary, the contributions of this paper are threefold: We diagnose the root cause for the poor performance of conventional text-based self-reflection mechanisms when applied to long-form video understanding. We propose REVISOR, two-stage reasoning framework that transforms the conventional text-based self-reflection mechanism into tool-augmented multimodal one, significantly enhancing the models reasoning capability and accuracy in long-form video understanding tasks. We integrate the DADR mechanism into the GRPO method, enabling MLLMs to learn how to review the correct video content during self-reflection within the RL training process, thereby maximizing REVISORs performance boost on long-form video comprehension. 2. Motivation Sec. 2.1 shows that text-only reflection mechanisms are inadequate for long-form video understanding. The underlying reason is that, unlike static images, long-form videos present far richer and more dynamic visual information, making purely textual re-evaluation insufficient for correcting reasoning errors. In Sec. 2.2, we initially demonstrated the importance of incorporating visual re-evaluation process during the reflection stage, with preliminary experiments showing that this significantly enhances the accuracy of MLLMs in long-form video understanding tasks. 2.1. Dilemma of Text-Based Reflection Self-reflection mechanisms, pivotal strategy for enhancing model performance, were initially applied to LLMs to bolster their complex reasoning capabilities. This approach has since demonstrated promising outcomes when extended to multimodal contexts. As illustrated in the left part of Fig. 2, the representative VL-Rethinker method demonstrates substantial performance improvements across various image understanding benchmarks. However, despite their operation within multimodal frameworks, the reflection processes of such mechanisms remain purely text-based. They do not incorporate re-evaluation of visual inputs, thereby classifying them as text-driven reflection mechanisms. As shown in the middle part of Fig. 2, applying these text-based reflection mechanisms to long-form video understanding tasks paradoxically leads to decline in model performance. To eliminate the possibility that this degradation stems from VL-Rethinkers lack of video training, we developed comparable model equipped with purely textbased reflection mechanism and trained it on video data. Details are provided in Appendix Sec. 9. Our experiments revealed that this model also failed to improve performance in long-form video understanding tasks. We attribute this phenomenon to two primary factors: (1) Compared to static images, long-form videos encompass significantly richer and more dynamic visual content. Thus, re-evaluating only text-based representations is inadequate for correcting reasoning errors, necessitating visual reevaluation process. (2) Purely text-based reflection mechanisms inherently lack cross-modal interaction capabilities, preventing full integration of visual information during reflection and thereby limiting improvements in MLLMs for long-form video understanding. 2.2. Necessity of Integrating Visual Rethinking To substantiate the claim advanced in Sec. 2.1, specifically that re-evaluating visual information can improve MLLMs accuracy in long-form video comprehension, we conducted In datasets such as preliminary validation experiment. NExT-GQA [50], ReXTime [4], and CG-Bench [3], the critical video segments necessary for answering each question, representing only small fraction of the total footage, are annotated. Using these datasets, we first allowed MLLMs to perform initial reasoning based on the raw video frames and questions. Subsequently, we provided the models with the annotated key segments and instructed them to re-evaluate the problem, integrating their previous reasoning with the newly supplied visual cues to generate the final answers. As illustrated in the right part of Fig. 2, introducing visual rethinking process during the reflection phase yields an average accuracy gain of approximately 7.3% on the NExTGQA, ReXTime, and CG-Bench datasets. In contrast, reflection based solely on textual information results in negligible improvement. These results substantiate two key insights: (1) In the domain of video understanding, visual reflection plays far more crucial role than textual reflection; and (2) when MLLMs effectively revisit key visual cues during this rethinking stage, their accuracy in long-form video comprehension tasks can be substantially enhanced. 3. Methodology In this section, we present REVISOR (REflective VIsual Segment Oriented Reasoning), novel two-stage reasoning framework designed to enhance long-form video understanding. REVISOR leverages tool-augmented multimodal reflection mechanism that enables MLLMs to collaboratively construct reflection processes across textual and visual modalities, enhancing their reasoning capability. 3.1. Tool-Augmented Multimodal Reflection The core concept of REVISOR involves two-stage reasoning framework: (1) an initial inference stage that generates preliminary reasoning trace and identifies key moments for further examination, and (2) reflective reasoning stage that integrates the initial analysis with newly sampled, finegrained visual evidence to yield refined final answer. An overview of this information flow is illustrated in Fig. 1. Stage 1: Initial Inference and Visual Review Proposal. The process begins with video and user-posed question Q. To mitigate the computational cost and contextlength limitations of the MLLM, we first perform sparse, uniform sampling of frames from the video, producing an initial frame set Finit = f1, f2, . . . , fN , where denotes the number of sampled frames. The MLLM, denoted as M, takes as input question and the initial set of frames Finit. The model is prompted to engage in chain-of-thought reasoning process to generate preliminary reasoning trace. Importantly, beyond producing the reasoning content, the model is instructed to identify and output temporal segment that it considers most relevant or uncertain with respect to its conclusion. This output is represented as structured tuple containing the initial reasoning trace Tinit and the proposed review segment S. We formalize this stage as: (Tinit, S) = Minf er(Q, Finit), (1) where Minf er denotes the MLLM operating in its initial inference mode. The segment = [tstart, tend] specifies the start and end timestamps of the video interval that warrants closer examination. The models ability to propose emerges naturally from its reasoning process Tinit, through which it articulates which portions of the video were most pivotal or ambiguous. Visual Toolbox Call: Retrieve Review Segment Frames. Upon receiving the proposed segment from the MLLM, the REVISOR framework engages the Visual Toolbox, denoted as . The role of is to perform targeted, highdensity re-sampling of frames from the original video , restricted to the temporal window specified by S. For segment = [tstart, tend], the toolbox generates denser frame sequence, formalized as: Freview = SampleDense(V, [tstart, tend]), (2) where SampleDense() denotes sampling function that extracts frames at higher temporal resolution (e.g., increased FPS) compared to the initial sparse sampling. This toolassisted process offloads the computational and procedural burden of locating fine-grained visual details, enabling the MLLM to conduct focused examination of critical moments without processing the entire video at full resolution. Stage 2: Reflective Reasoning and Answer Refinement. The final stage centers on reflection and self-correction. The MLLM is re-invoked, now operating within richer and more structured contextual frame. The inputs to this reflective stage are: (1) the original question Q, (2) the initial reasoning trace Tinit, and (3) the newly acquired, densely sampled visual evidence Freview. By providing the model with access to its own initial reasoning Tinit, the REVISOR framework enables form of in-context reflection, prompting the model to reassess its preliminary conclusions in light of the enhanced visual data from Freview. This process allows the MLLM to validate its earlier hypotheses, resolve ambiguities identified in Stage 1, or correct prior misinterpretations. The generation of the refined reasoning trace Tref ine and final answer Af inal is formalized as: (Tref ine, Af inal) = Mref lect(Q, Tinit, Freview), (3) Figure 3. Overview of the Dual-Attribution Decoupled Reward Mechanism (DADR). Final Answer Verification Reward (top) is derived from verifying the correctness of the models synthesized final answer, directly targeting the accuracy objective of the reflective stage. Conversely, Causal Segment Sufficiency Reward (bottom) is granted upon verifying an attribution answer derived exclusively from reviewed video segments, thereby guiding the model to identify and utilize segments highly pertinent to the user query. where Mref lect denotes the MLLM operating in its reflection mode. This reflective mechanism mirrors human expert analysis, where an initial overview is followed by focused examination of critical evidence before forming conclusion. By structuring the MLLMs reasoning in this iterative manner, REVISOR enhances both the accuracy and reliability of responses in complex video understanding tasks. 3.2. Dual Attribution Decoupled Reward In standard practice, reward signals based on the verification of the final answer are integrated with reinforcement learning to train the aforementioned tool-augmented MLLM in an end-to-end manner. Specifically, the GRPO algorithm is commonly employed to maximize the expected total reward, thereby stabilizing the training process and encouraging exploration within the complex videotemporal action space. The policy πθ parameterized by θ generates reasoning trajectory τ = (Tinit, S, Tref ine, Af inal) based on the input (Q, ). The overall objective is to maximize the expected cumulative reward J(θ): J(θ) = E(Q,V )D,τ πθ(Q,V )[R(τ )]. (4) However, as shown in Tab. 3, training the REVISOR framework using the GRPO algorithm with final-answer verification as the sole reward function leads to decreased performance on long-form video understanding tasks compared to the base model. This limitation primarily arises from the complex nature of the REVISOR reasoning process, which consists of three components: the initial reasoning content, the video review segmentation interval, and the reflective reasoning phase. Consequently, even when the model correctly outputs the video review segments, it receives insufficient positive feedback; conversely, incorrect segment predictions fail to incur adequate penalties. This imbalance prevents the model from effectively learning to identify key video segments during reinforcement learning (see Sec. 5.2 for supporting evidence). To address this issue, we decouple the reward for video review segment localization from the general reward signal and propose Dual-Attribution Decoupled Reward Mechanism (DADR), as shown in Fig. 3. This mechanism integrates the reward for the final output with an additional causal reward, ensuring that the proposed segment is sufficient for deriving the correct conclusion. The total reward R(τ ) is defined as weighted sum of two distinct components: the Final Answer Verification Reward (Rf inal) and the Causal Segment Sufficiency Reward (Rcausal): R(τ ) = λ1Rf inal + λ2Rcausal, (5) where λ1 and λ2 are hyperparameters controlling the balance of the rewards. Final Answer Verification Reward. The term Rf inal is standard task-specific reward that validates the correctness of the final refined answer Af inal against the ground truth answer A. This reward directly targets the accuracy objective of the reflective stage (Stage 2): Rf inal = I(Af inal = A), (6) where I() is the indicator function. Table 1. Evaluation results of the four long-form video understanding benchmarks. indicates models trained with the text-based reflection mechanism using the same dataset as ours. represents our reproduction. highlights the superior performance of the REVISOR framework relative to the base model Qwen2.5-VL-7B. Model Model Size Video Tokens VideoMME Long Overall LongVideoBench MLVU LVBench Gemini-1.5-Pro[39] GPT4o[14] ShareGPT4Video[5] Video-LLaVA[22] LongVA[60] LongVU[37] Vamba[34] LLaVA-OneVision[17] Hour-LLaVA[23] VideoChat-Flash[19] NVILA[25] Open-o3-Video[31] Video-MTR[51] Video-R1[8] LongVILA-R1[7] VL-Rethinker[42] Qwen2.5-VL[2] Qwen2.5-VL[2] Ours - - 8B 7B 7B 7B 10B 7B 7B 7B 8B 7B 7B 7B 7B 7B 7B 7B 7B - - - - 224K 8K - 6K - 8K 8K 2K 4K 8K - 8K 8K 8K 8K 67.4 65.3 37.9 38.1 47.6 59.5 - - 55 55.4 54.8 54.9 51.0 - 55.2 75 71. 43.6 40.4 54.3 60.6 57.8 46.7 63.6 65.3 64.2 63.6 59.0 61.4 65.1 64 66.7 39.7 39.1 - - 55.9 56.4 60.4 - 70.1 - - - 58 - 64. 46.4 47.3 56.3 65.4 65.9 64.7 - 74.7 57.7 - 48.4 - - - - - - - - 42.1 - 45.6 48.2 - - - - - 51.9 53.2 53.4 56.2 2. 62.1 63.4 64.3 65.7 1.4 56.4 57.4 56.5 57.5 1.0 63.2 69 67.3 69.8 2.5 37.2 39.2 40.2 42 1.8 Causal Segment Sufficiency Reward. The Causal Segment Sufficiency Reward (CSSR) is designed to enforce the quality of the proposed review segment (generated in Stage 1). The CSSR provides positive signal only if the model is capable of deriving the correct answer solely based on the question and the densely sampled visual evidence Freview extracted from S. This implicitly encourages the MLLM to select segments that are truly causal and sufficient for the task, preventing the selection of irrelevant or overly long segments. To formalize this, we define an attribution-based answer ˆA using the same MLLM M, but conditioned solely on the original question and the fine-grained visual evidence Freview. This step serves as crucial sufficiency test for the proposed segment S. Let Msuf denote this sufficiency prediction mode: ˆA = Msuf (Q, Freview). (7) The Causal Segment Sufficiency Reward is then defined based on the correctness of this attribution-based prediction: Rcausal = I( ˆA = A). (8) Incorporating Rcausal into the GRPO optimization loop guides the REVISOR framework toward policies πθ that not only yield accurate final answers but also robustly identify the critical temporal evidence required for those answers in the initial inference stage. 4. Experiments We first detail our experimental setup in Sec. 8.1. The performance of the REVISOR framework for long-form video understanding tasks is then presented in Sec. 4.2. Subsequently, Sec. 4.3 conducts an ablation study to evaluate the contribution of REVISORs various components. Please refer to Appendix Sec. 10 for the Case Study of REVISOR. 4.1. Experimental Setup Training Details. We adopt Qwen2.5-VL-7B [2] as our base model. The training is conducted using the verl framework [38], which we further extend to support the training of REVISOR. Our training process consists of singlestage reinforcement learning phase following DAPO. [58]. Dataset Construction. We use total of 25K training samples collected from STAR [20], PerceptionTest [32], NExT-QA [49], CLEVRER [57], LLaVA-Video-178K [61], TimeRFT [45], CG-Bench [3], and ReXTime [4]. Detailed information regarding the dataset selection procedure is available in Appendix Sec. 8.2. Hyperparameter Selection. In our experiments, we set λ1 = 0.6 and λ2 = 0.3. We adopt AdamW [27] as the optimizer with learning rate of 1 106 and batch size of 32. The number of rollouts is set to 8. During both training and evaluation, the total number of video tokens in the input is limited to maximum of 8192. For more details, please refer to Appendix Sec. 8. 4.2. Main Results Long-Form Video Understanding Task. We evaluate our method on four widely used long-form video benchmarks: VideoMME [9], LongVideoBench [48], MLVU [62], and LVBench [43]. As shown in Tab. 1, with an input of 8K video tokens, REVISOR achieves 65.7% on VideoMME, outperforming the base model Qwen2.5-VL-7B by 1.4%. Notably, REVISOR outperforms the base model by 2.8% on the long subset of VideoMME and by 2.5% on MLVU, which contains videos up to 120 minutes. This indicates that as video duration increases, accurately reviewing the relevant video content becomes increasingly crucial. Compared with the latest text-only reasoning approach Video-R1 [8], REVISOR achieves 4.3% improvement on VideoMME, demonstrating that the integration of the DADR mechanism provides clear advantage over pure text-based reasoning. Furthermore, compared with textbased reflection methods such as VL-Rethinker and our own video-data-trained, text-only reflection method, REVISOR shows gains of 3.6% and 2.3%, respectively, highlighting the necessity of Visual Rethinking within REVISOR. Table 2. Evaluation results for the temporal video grounding task, including the Charades-STA and NExT-GQA benchmarks. Bold fonts highlight the best performance."
        },
        {
            "title": "Model",
            "content": "Charades-STA NExT-GQA R@0.7 mIoU R@0.7 mIoU Qwen2.5-VL-7B[2] VTimeLLM[13] iMOVE[18] TimeChat[33] VideoChat-TPO[53] TVG-R1[6]"
        },
        {
            "title": "Ours",
            "content": "15.5 14.7 26.1 13.4 18.4 23.9 31.8 36.9 34.6 47.3 - 38.2 46.7 51.4 7.5 9.7 - 6.2 8.2 10 11. 20.9 24.4 - 20.6 27.7 29.3 33.2 Temporal Video Grounding Task. As shown in Tab. 2, REVISOR achieves 51.4% mIoU on Charades-STA [11], surpassing the prior SFT-based SOTA method iMOVE [18] by 4.1% mIoU. Moreover, it outperforms the RL-based temporal grounding model TVG-R1 [6] by 4.7% mIoU on Charades-STA and 3.9% mIoU on NExT-GQA [50], showing its superior capability in temporal video grounding. 4.3. Ablation Study Dual Attribution Decoupled Reward. We perform an indepth investigation of the DADR mechanism. As shown in Tab. 3, setting λ2 to 0 (i.e., using only the Final Answer Verification Reward) results in drop of REVISORs Video-MME score from 65.7% to 62.2%, which is below the 64.3% achieved by the base model Qwen2.5-VL-7B. This result suggests that, without the CSSR component, the model fails to learn how to locate the correct review segment from sparse reward signals. Please refer to Appendix Sec. 8.3 for more ablation study results. Table 3. Ablation study of the Dual Attribution Decoupled Reward mechanism. V-MME, LongVB, LV, and NExT-G represent VideoMME, LongVideoBench, LVBench, and NExT-GQA, respectively. Bold fonts highlight the best performance. The row marked in gray represents our base model. λ1 λ2 V-MME LongVB LV MLVU NExT-G - 0.3 0.6 0.6 - 0.6 0.0 0.3 64.3 64.0 62.2 65.7 56. 56.0 54.0 57.5 40.2 41.1 40.8 42.0 67.3 68.7 68.3 69.8 20. 33.9 32.1 33.2 When the value of λ2 exceeds that of λ1, the models temporal video grounding ability improves, but its longform video understanding performance declines (e.g., MLVU drops from 69.8% to 68.7%). This occurs because the model becomes overly focused on locating the correct review segment S, while neglecting how to utilize to derive the correct answer. Therefore, we set the value of λ1 to be greater than that of λ2 to encourage the model not only to accurately identify the correct review segment but also to leverage it to enhance its reasoning capability. Table 4. Ablation study of the training data composition. SVQ, TRF, RXT, and CGB represent short video QA, TimeRFT, ReXTime, and CG-Bench, respectively. V-MME, LVB, LV, and NExTG represent VideoMME, LongVideoBench, LVBench, and NExTGQA, respectively. Bold fonts highlight the best performance. The row marked in gray represents our base model. SVQ TRF RXT CGB V-MME LVB LV MLVU NExT-G - - - - 64.3 62.6 63.8 64.0 65.7 56.5 40.2 67.3 52.7 38.6 56.8 40.3 57.1 40.5 57.5 42.0 66.7 67.6 68.4 69. 20.9 25.0 22.6 29.5 33.2 Data Composition. As shown in Tab. 4, REVISORs longform video understanding capability improves with the inclusion of more diverse datasets. Specifically, its performance on MLVU increases from 66.7% to 69.8%. Notably, even when trained exclusively on short video QA datasets, REVISORs mIoU on NExT-GQA rises from 20.9% to 25%. This result demonstrates that, when combined with the DADR mechanism, our framework can effectively translate its understanding into the accurate localization of the correct review segment S. 5. Extended Analysis This section details comprehensive analytical experiments validating two key factors crucial for the REVISOR frameworks success: (1) For long-form video understanding tasks, the re-examination of visual information proves significantly more critical than that of textual inference processes. (2) The DADR mechanism effectively enhances the MLLMs precision in retrieving critical visual information. 5.1. Importance of Visual Rethinking Sec. 2 posited and preliminarily demonstrated that the limitations of purely text-based reflection in long-form video understanding arise from the inherently richer and more dynamic nature of long-form video inputs compared to static images. Reflection based solely on textual information is insufficient to correct reasoning errors, necessitating the inIn this tegration of dedicated visual reflection process. section, we provide further indirect evidence supporting this claim through an analysis of REVISORs output. the model to precisely locate the minimal video segments sufficient for answering the given question. Building on these findings, we conducted further comparative experiments on the length of the text generated during the reflection process. By encouraging the model to engage in deeper deliberation through the system prompt, we made the model output more extensive textual reasoning during the reflection stage. However, as shown in the right part of Fig. 4, this intervention paradoxically led to decline in the models performance on the video understanding task. This result further substantiates that pure textual reflection is not critical for video understanding tasks; instead, generating more ineffectual reasoning content can have negative impact on performance. 5.2. Accuracy of Visual Information Retrieval In Sec. 3.2, we propose DADR mechanism to enable more accurate revisiting of key video segments during the reflection phase. To assess its efficacy, we conduct targeted experiments on the video segments REVISOR processes within the visual reflection stage. Figure 4. The superior efficacy of visual reflection over textual reflection in long-form video understanding. The left panel demonstrates that the length of the generated textual reflection consistently decreases throughout training. The right panel further indicates that forcing the model to perform longer textual reflection actually leads to degradation in model performance. Left part of Fig. 4 depicts the evolution of textual reasoning length and video review length during REVISORs reflection stage throughout training. As training progresses, the length of textual reasoning steadily decreases. This trend clearly indicates that, through interaction with the environment during reinforcement learning, the model gradually learns that textual reflection plays relatively minor role in long-form video understanding tasks. In contrast, the length of the reviewed video segments first increases and then decreases. We attribute this to the model initially expanding its search range for critical video segments to ensure the inclusion of highly relevant content, and later learning to discard redundant portions. This refinement enables Figure 5. Comparative accuracy of key moment review across different methods. Methods based on the REVISOR framework and its variants are highlighted in blue, while different Temporal Grounding baselines are represented in green. We aim to examine the effect of incorporating the DADR mechanism during training on the accuracy of retrospective video segment extraction. To this end, we conducted an experiment in which models trained with and without the DADR mechanism were required to extract key video segments necessary for answering specific questions. These extracted segments were subsequently provided to the original Qwen2.5-VL-7B model, which generated responses based solely on the given segments. As the model architecture and parameters remained identical, the correctness of the responses depended entirely on the relevance of the retrieved video segments to the corresponding questions. Thus, by comparing the answer accuracy of Qwen2.5-VL7B under both conditions, we can indirectly assess the precision of the video segment extraction process. As shown in Fig. 5, when the DADR mechanism is applied during training, the video segments revisited by the model during the reflection phase are generally more accurate. This clearly demonstrates that the DADR mechanism enables MLLMs to learn to recall key video segments during the reinforcement learning process. Furthermore, we conducted comparative analysis between the DADR mechanism and the four temporal grounding methods TimeR1 [45], TVG-R1 [6], Open-o3-Video [31], and VideoChatR1.5 [52], and found that DADR still achieves the highest accuracy in recalling relevant video segments. 6. Related Work Long-Form Video Understanding. In long-form video understanding, the visual inputs are far more complex than those in image-based tasks. Consequently, identifying and distilling only the question-relevant information from large amounts of redundant visual content becomes crucial for improving model performance. Existing approaches can be grouped into three categories: external-model augmentation, agentic methods, and model-internalized selection. External-model augmentation techniques [21, 29, 37], rely on visiontext similarity models like CLIP for keyframe selection. While efficient, these methods perlimiting their adaptabilform static, one-shot selection, ity to complex or evolving queries. Agentic approaches [24, 46] address this limitation through iterative frameworks that dynamically refine frame selection. For example, VideoAgent [44] employs an LLM as central agent in stateactionobservation loop, whereas VideoTree [47] adopts hierarchical tree structure to perform coarse-to-fine search, directing computation toward the most relevant segments. More recent model-internalized selection methods [40] embed frame selection directly into the models reasoning process. GenS [56] trains lightweight generative sampler to identify query-relevant frames end to end, though the selection and QA stages remain decoupled. Our proposed REVISOR framework, after reinforcement learning training, can autonomously identify and explore visual content that requires additional careful review during the inference stage. Unlike previous methods that are limited to statically searching for important video segments based solely on the question itself, our approach fully integrates with the reasoning capabilities of MLLMs. This allows it to engage in deeper deliberation and, through comprehensive interaction with its ongoing reasoning, more precisely pinpoint critical visual information, thereby significantly enhancing the models reasoning ability. Self-Reflection Mechanism. Incorporating self-reflection into inference has been shown to improve model performance [12]. Self-Refine [28] implements an iterative feedbackrevision loop without requiring additional training or external supervision. SCoRe [16] adopts multi-round reinforcement learning to cultivate reflective reasoning skills, yielding substantial gains. GEPA [1] further introduces Pareto-based reflective prompt-evolution framework that outperforms reinforcement learning in both sample efficiency and overall performance. Extending self-reflection to multimodal settings likewise yields consistent improvements [15, 54]. VL-Rethinker [42] incorporates self-verificationand-correction stage via reinforcement learning, substantially enhancing multimodal reasoning. SRPO [41] further strengthens selfreflection and self-correction by curating reflection-oriented supervised data and introducing reflection-aware reward under group relative policy optimization, significantly boosting complex multimodal reasoning. However, existing multimodal self-reflection mechanisms still depend on text-based re-evaluation processes. They lack explicit reflective operations over visual information and therefore remain fundamentally text-centric. The REVISOR framework proposed in this paper addresses this limitation by extending text-centric reflection into truly multimodal reflective process. 7. Conclusion Text-based reflection fails to capture the complexity of long-form video reasoning, which depends critically on rich and dynamic visual information. We propose REVISOR, tool-augmented multimodal reflection framework that revisits essential visual content to correct errors arising in the initial reasoning process. To train REVISOR to precisely localize the video segments most relevant to each query, we incorporate Dual Attribution Decoupled Reward (DADR) mechanism into GRPO. Extensive experiments across four long-form video understanding benchmarks show that REVISOR consistently delivers significant performance gains."
        },
        {
            "title": "References",
            "content": "[1] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, et al. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457, 2025. 9 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 7, 2 [3] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cgbench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075, 2024. 4, 6 [4] Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, YenChun Chen, and Frank Wang. Rextime: benchmark suite for reasoning-across-time in videos. Advances in Neural Information Processing Systems, 37:2866228673, 2024. 4, 6 [5] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. 6 [6] Ruizhe Chen, Tianze Luo, Zhiting Fan, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, and Zhang Huaijian. Datasets and recipes for video temporal grounding via reinforcement In Proceedings of the 2025 Conference on Emlearning. pirical Methods in Natural Language Processing: Industry Track, pages 983992, 2025. 7, 9, [7] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025. 6 [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 6, 7 [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 2, 7 [10] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 1 [11] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 7 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, [13] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. 7, 2 [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [15] Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, and Jiajun Zhang. Look again, think slowly: Enhancing visual reflection in vision-language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 92629281, 2025. 9 [16] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language arXiv models to self-correct via reinforcement learning. preprint arXiv:2409.12917, 2024. 1, 9 [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6 [18] Jiaze Li, Yaya Shi, Zongyang Ma, Haoran Xu, Huihui Xiao, Ruiwen Kang, Fan Yang, Tingting Gao, Di Zhang, et al. imove: In Findings of the Association for Computational Linguistics: ACL 2025, pages 2395923975, 2025. 7, Instance-motion-aware video understanding. [19] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochatflash: Hierarchical compression for long-context video modeling, 2025. 6 [20] Yansheng Li, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Youming Deng, Wenbin Wang, Xian Sun, Haifeng Li, et al. Star: first-ever dataset and large-scale benchmark for scene graph generation in large-size satellite imagery. IEEE Trans. Pattern Anal. Mach. Intell, 47(3): 18321849, 2025. 6 [21] Hao Liang, Jiapeng Li, Tianyi Bai, Xijie Huang, Linzhuang Sun, Zhengren Wang, Conghui He, Bin Cui, Chong Chen, and Wentao Zhang. Keyvideollm: Towards large-scale video keyframe selection, 2024. 9 [22] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. 6 [23] Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, et al. Unleashing hour-scale video training for long video-language understanding. arXiv preprint arXiv:2506.05332, 2025. [24] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning, 2025. 9 [25] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 41224134, 2025. 6 [26] Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291, 2023. 1 [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [28] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. 1, [29] Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, and Miao Yin. Adacm2: On understanding extremely long-term video with adaptive cross-modality memory reduction, 2025. 9 [30] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 1 [31] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, et al. Open-o3 video: Grounded video arXiv reasoning with explicit spatio-temporal evidence. preprint arXiv:2510.20579, 2025. 6, 9 [32] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 6 [33] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 7, 2 [34] Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hourlong videos with hybrid mamba-transformers. arXiv preprint arXiv:2503.11579, 2025. [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2 [36] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 1 [37] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 6, 9 [38] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 6 [39] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6 [40] Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, and Xipeng Qiu. Thinking with video: Video generation as promising multimodal reasoning paradigm, 2025. [41] Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. 1, 9 [42] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 1, 6, 9 [43] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, et al. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967, 2025. 2, 7 [44] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. Videoagent: Long-form video understanding with large language model as agent, 2024. 9 [45] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. 6, 9 [46] Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, and Yali Wang. Videochat-a1: Thinking with long videos by chain-of-shot reasoning, 2025. 9 [47] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos, 2025. 9 [48] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 2, 7 [49] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. [50] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question In Proceedings of the IEEE/CVF Conference answering. on Computer Vision and Pattern Recognition, pages 13204 13214, 2024. 4, 7 [51] Yuan Xie, Tianshui Chen, Zheng Ge, and Lionel Ni. Videomtr: Reinforced multi-turn reasoning for long video understanding. arXiv preprint arXiv:2508.20478, 2025. 6 [52] Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Videochat-r1. 5: Visual test-time scaling to reinforce multimodal reasoning by iterative perception. arXiv preprint arXiv:2509.21100, 2025. 9 [53] Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, et al. Task preference optimization: Improving multimodal large language models with vision task alignment. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2988029892, 2025. 7, 2 [54] Shuo Yang, Yuwei Niu, Yuyang Liu, Yang Ye, Bin Lin, and Li Yuan. Look-back: Implicit visual re-focusing in mllm reasoning. arXiv preprint arXiv:2507.03019, 2025. 9 [55] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [56] Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long video understanding, 2025. 9 [57] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 6 [58] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 1, 6 [59] Jinghan Zhang, Xiting Wang, Fengran Mo, Yeyang Zhou, Wanfu Gao, and Kunpeng Liu. Entropy-based exploration conduction for multi-step reasoning. arXiv preprint arXiv:2503.15848, 2025. 1 [60] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 6 [61] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [62] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13691 13701, 2025. 2, 7 REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "8. More Experimental Details on REVISOR 8.2. Detailed Information of Training Data In Sec. 8.1, we present the detailed experimental setup for experiments involving the REVISOR framework. Sec. 8.2 outlines the composition of the training data for the REVISOR framework. Sec. 8.3 then presents supplementary experimental results, including comprehensive results on the Temporal Video Grounding task and complete ablation studies on the reward scaling factors λ1 and λ2. Figure 6. REVISOR framework training dataset composition. The training dataset for the REVISOR framework consists of three tasks: Short Video QA, Temporal Grounding, and Grounded Video QA, totaling 25K training samples. The parenthetical value for each dataset denotes its specific sample contribution. 8.1. Detailed Experimental Setup Our experiments utilized Qwen2.5-VL-7B as the base model, which comprises visual encoder, merger projector, and large language model. REVISOR was trained using single stage of reinforcement learning, eliminating the need for an additional supervised fine-tuning phase. We extended the verl framework to support REVISORs training. Following the approach in DAPO, we removed the KL regularization term from GRPO. Key hyperparameters were set as follows: λ1 = 0.6, λ2 = 0.3, learning rate of 1 106, batch size of 32, and 8 rollouts. During both training and evaluation, input video tokens were limited to maximum of 8192, sampled at 1 FPS. For the Review Segment, the sampling rate was increased to 2 FPS, while still adhering to maximum of 8192 video tokens. Absolute timestamps were displayed in the lower-left corner of each image frame. The entire training phase consisted of 792 optimization steps. As detailed in Fig. 6, our training corpus spans three tasks central to video understanding: Short Video QA, Temporal Grounding, and Grounded Video QA. For Short Video QA, we aggregate data from LLaVA-Video-178K, CLEVRER, NExT-QA, PerceptionTest, and STAR, all of which provide short video clips ( few minutes). From these datasets, we sample balanced set of 20K videoquestionanswer triplets. The Temporal Grounding task is sourced directly from TimeRFT without further filtering. For Grounded Video QA, we incorporate CG-Bench and ReXTime, from which we randomly select 2K examples. 8.3. Supplementary Experimental Results Temporal Video Grounding. As summarized in Tab. 5, we conduct an extensive assessment of REVISORs temporal grounding performance across both the Charades-STA and NExT-GQA benchmarks. On Charades-STA, REVISOR attains an R@0.3 accuracy of 76.5%, surpassing the prior state-of-the-art method iMOVE by margin of 4.8%. On NExT-GQA, REVISOR reaches an R@0.5 accuracy of 25.5%, outperforming TVG-R1, designed explicitly with reinforcement learning to enhance temporal localization, by 4.3%. Together, these results highlight the robustness and effectiveness of REVISOR, equipped with DADR, in achieving precise temporal video localization. Ablation Study of DADR Mechanism. We conducted an in-depth analysis of the Dual Attribution Decoupled Reward (DADR) mechanism. As shown in Tab. 6, when λ2 = 0, that is, when only the Final Answer Verification Reward is applied, the performance of REVISOR on Video-MME decreases substantially from 65.7% to 62.2%, even falling below the base model. This demonstrates that, without the Causal Segment Sufficiency Reward (CSSR), the model struggles to reliably identify the correct review segment due to the sparsity of the reward signal. When λ2 exceeds λ1, the models temporal grounding improves; however, its long-video reasoning capability degrades. For example, MLVU performance declines from 69.8% to 68.7%, suggesting that the model overemphasizes locating the review segment while underutilizing it for answer derivation. Consequently, we set λ1 > λ2 to encourage accurate localization of while still promoting strong reasoning based on it. Nonetheless, if λ1 is too large and λ2 too small, the models ability to locate deteriorates, harming long-form video understanding. Conversely, when λ1 is only slightly Table 5. Comprehensive evaluation of REVISOR on the temporal video grounding task. Bold text indicates the best performance. Charades-STA NExT-GQA Model R@0.3 R@0. R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU Qwen2.5-VL-7B[2] VTimeLLM[13] iMOVE[18] TimeChat[33] VideoChat-TPO[53] TVG-R1[6] Ours 57.1 55.3 71.7 51.5 58.3 70.8 76.5 33.6 34.3 51.3 32.2 40.2 50.5 57.3 15.5 14.7 26.1 13.4 18.4 23.9 31.8 36.9 34.6 47.3 - 38.2 46.7 51.4 31.6 37.9 - 34.1 41.2 41.7 47. 18.1 20.2 - 17.9 23.4 20.8 25.5 7.5 9.7 - 6.2 8.2 10.0 11.9 20.9 24.4 - 20.6 27.7 29.3 33.2 Table 6. Complete ablation study of the Dual Attribution Decoupled Reward mechanism. Bold fonts highlight the best performance. The row marked in gray represents our base model. λ1(Rf inal) λ2(Rcausal) VideoMME LongVideoBench LVBench MLVU NExT-GQA - 0.3 0.6 0.45 0.5 0.6 0.8 - 0.6 0.0 0.45 0.4 0.3 0.1 64.3 64.0 62.2 64.3 64.9 65.7 64.6 56. 56.0 54.0 57.3 57.1 57.5 57.6 40.2 41.1 40.8 41.2 41.8 42.0 40.3 67.3 68.7 68.3 68.5 69.0 69.8 68.4 20. 33.9 32.1 33.7 33.4 33.2 31.6 greater than λ2, REVISOR identifies effectively but fails to fully leverage it for reasoning, resulting in strong temporal grounding but weaker long-video comprehension. Empirically, we find that λ1 = 0.6 and λ2 = 0.3 provide an effective balance, enabling REVISOR to both accurately localize and utilize it to enhance reasoning performance. 9. Training Qwen2.5-VL-7B with Textual Reflection Mechanism on Video Data To ensure fair comparison, we train Qwen2.5-VL-7B model equipped with text-based self-reflection mechanism using the datasets listed in Fig. 6. Specifically, unlike REVISOR, the text-reflection model generates only textual output during the reflection phase. Apart from this distinction, all other experimental settings remain identical to those of REVISOR. The learning rate is set to 1 106, the total batch size is 32, and the number of rollouts is set to 8. In addition, the total number of input video tokens is capped at 8192. The prompt template is shown in Fig. 11. 10. Case Study of REVISOR Framework Improved Video Reasoning Capability. REVISOR can significantly enhance the long-form video understanding capabilities of MLLMs. Fig. 7, Fig. 8, and Fig. 9 respectively demonstrate these improvements from three perspectives: more precise detail capture, more comprehensive scene understanding, and more accurate object counting. Visual Reflection is Even More Important. In this paper, we repeatedly emphasize that, for long-video understanding tasks, visual reflection is more important than textual reflection. In Sec. 5.1, we quantitatively validate this conclusion by monitoring changes in the length of textual reflections during training. Here, we further illustrate this point with concrete example. As shown in Fig. 10, during training, the textual reflection generated by the REVISOR framework for given question becomes increasingly concise, while its retrieval of key video segments becomes increasingly accurate. Ultimately, the model precisely identifies the critical 25-second segment within 300-second video, confirming that respiratory cells could potentially be used in treatment for cardiovascular disease. 11. Prompt Templates of REVISOR The complete prompt template used during the training of the REVISOR framework consists of three primary components: the system prompt, the initial reasoning stage, and the reflective reasoning stage. Fig. 12 illustrates the templates for both the initial reasoning and reflective reasoning stages, while Fig. 13 presents the system prompt template. key distinction from the plain-text reflection mechanism is that, during the initial reasoning stage, the REVISOR framework identifies the critical video segments that require further examination. Figure 7. Successful example of the REVISOR framework: achieving more accurate detail capture. Figure 8. Successful example of the REVISOR framework: achieving more accurate scene understanding. Figure 9. Successful example of the REVISOR framework: achieving more accurate object counting. Figure 10. As training progressed, REVISORs outputted reflective text paragraphs became increasingly concise, and the video segments it referenced grew more precise. Ultimately, it accurately concluded that respiratory cells could potentially be used in the treatment of cardiovascular disease. The Complete Prompt Template for Plain-Text Reflection Mechanism System Prompt: You are helpful assistant. Stage 1: Preliminary Reasoning Phase User Instruction: <User Question>. Please think carefully and then provide your answer. Output Format: Format strictly as: <think> Your reasoning steps </think> <answer> Your answer </answer> Stage 2: Reflective Reasoning Phase User Instruction: Please rethink the reasoning process above and provide your final answer. Output Format: Format strictly as: <rethink> Your rethinking reasoning steps </rethink> <answer> Your final answer </answer> Figure 11. The complete prompt template used during the training of Qwen2.5-VL-7B equipped with pure text-based reflection mechanism, covering both the initial reasoning and reflective reasoning stages."
        },
        {
            "title": "Reasoning Prompt Template for REVISOR Framework Training",
            "content": "Stage 1: Preliminary Reasoning Phase User Instruction: <User Question>. Please think first, and then use the temporal zoom in tool to find the video segment that can answer the users question. Output Format: Format strictly as: <think> Your reasoning steps </think> <time interval>[start time, end time] </time interval> Stage 2: Reflective Reasoning Phase User Instruction: Please refer to the Visual Review segment above, think carefully, and provide your final answer. Output Format: Format strictly as: <rethink> Your reasoning steps </rethink> <answer> Your final answer </answer> Figure 12. Reasoning prompt template for REVISOR framework training"
        },
        {
            "title": "System Prompt Template for REVISOR Framework Training",
            "content": "You are helpful assistant. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> 1 { \"type\": \"function\", \"function\": { \"name\": \"temporal_zoom_in_tool\", \"description\": \"Identify the precise time segment in the video that contains enough information to answer the question.\", \"parameters\": { \"properties\": { \"interval\": { \"type\": \"str\", \"description\": \"The time range to zoom in on, formatted as start_time to end_time. Timestamps are in seconds.\" } }, \"required\": [\"interval\"] } } 2 3 5 6 7 8 9 11 12 13 14 15 16 } (cid:6) (cid:5) </tools> # Explanation of temporal zoom-in When you call temporal zoom in tool with <time interval> [start time, end time] </time interval>, the tool returns new sequence of denser video frames sampled from the specified time range ([start time, end time]) in the original video. This provides higher temporal precision, helping you answer the users question more accurately. # How to call tool Return the interval directly in XML tags: <time interval> [12.3, 28.7] </time interval> Figure 13. System prompt template for training the REVISOR framework."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "MiLM Plus, Xiaomi Inc.",
        "Renmin University of China"
    ]
}