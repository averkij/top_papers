{
    "paper_title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "authors": [
        "Jiaru Zou",
        "Yikun Ban",
        "Zihao Li",
        "Yunzhe Qi",
        "Ruizhong Qiu",
        "Ling Yang",
        "Jingrui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 7 2 6 1 . 5 0 5 2 : r Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning Jiaru Zou1, Yikun Ban1, Zihao Li1, Yunzhe Qi1, Ruizhong Qiu1, Ling Yang2, Jingrui He1 1University of Illinois Urbana-Champaign, 2Princeton University jiaruz2@illinois.edu"
        },
        {
            "title": "Abstract",
            "content": "Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take deeper step by retaining and leveraging the models own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the models learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design Copilot model to refine the Pilots inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) novel Copilot model design, (ii) joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) fused inference paradigm where the Copilot rectifies the Pilots logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability1."
        },
        {
            "title": "Introduction",
            "content": "Transformers, the foundation of modern large language models (LLMs), leverage attention and feedforward layers to compute logits for sequence generation [78]. Pre-trained on general-domain corpora, these models capture rich statistical patterns and exhibit strong generation capabilities [13, 81, 35, 58]. On top of that, supervised fine-tuning (SFT) serves as critical technique for adapting pre-trained LLMs to specific domains [41, 66, 81, 97]. While SFT enables significant flexibility and task-specific optimization, the performance of fine-tuned LLMs during inference often remains suboptimal, exhibiting misalignment between training and testing stages [48, 80]. This gap arises from the models inability to fully capture task-specific nuances or from overfitting to patterns within the training data, ultimately degrading its final performance [66, 57, 94, 54]. Without data-side interventions [53, 55, 27] or receiving external feedback [59, 73, 91], this paper aims to address fundamental question: Can we enhance the inference performance by retaining and leveraging the models own learning signals in standard fine-tuning? To address this question, our core idea draws inspiration from common strategy by human learners: maintaining log to record mistakes during practice, reflecting, and using insights to improve performance in formal tests. Rather than merely memorizing these mistakes, proficient learners engage in reflective thinkinganalyzing their internal cognitive states at the moment the errors Corresponding author 1Code will be released at https://github.com/jiaruzouu/TransformerCopilot Preprint. occurred, questioning how and why the mistakes were made. The reflective practice enables learners to identify recurring error patterns and approach uncertain problems with greater caution and awareness. Motivated by this human reflection thinking mechanism [31], we propose the concept of Mistake Log tailored for LLMs fine-tuning. At training stages, standard SFT primarily focuses on optimizing model parameters by minimizing the expected loss over fine-tuning datasets [81, 96]. We take deeper step to systematically record the rich intermediate information within the model, including input data (Question), internal hidden state representations (Rationale), and token-level quantified errors (Mistakes), as Mistake Log components to track models mistakes through its training trajectory. Next, to fully exploit the Mistake Log, we propose the Transformer Copilot (abbreviated as TCopilot), novel Pilot-Copilot framework that enables error-aware refinement by learning from model-internal signals [11, 12, 34]. In addition to the original model (referred to as the Pilot), we introduce Copilot model that captures and leverages the Pilots Mistake Log throughout its learning trajectory, rectifying the Pilots logits to improve final token-by-token generation. Overall, our learning framework offers advantages from three key perspectives: (i) New Model Architecture Design: We design the Copilot as transduction neural network that learns recurring error patterns from the Mistake Log. residual flow connection is then established between the Copilot and Pilot models, allowing the Copilot to assist the Pilot via token-level error correction during generation. (ii) New Training Paradigm: We redesign the SFT procedure by jointly training the Pilot and Copilot models in each round, enabling the Copilot to continuously learn from the evolving Mistake Log and adapt alongside the Pilot model. (iii) New Inference Paradigm: During next-token generation, we fuse the output logits from the Pilot and Copilot models into unified probability distribution, enabling collaborative auto-regressive generation. In this way, T-Copilot fundamentally integrates an internalized reflection mechanism into standard SFT, enabling an adaptive and error-aware generation. To demonstrate the efficacy of the T-Copilot, we provide detailed analyses from both theoretical and empirical perspectives. We incorporate T-Copilot into representative encoder-decoder and decoderonly Pilot models, and conduct extensive experiments across 12 tasks on commonsense, arithmetic, and real-world recommendation. T-Copilot improves the performance of Pilot by up to 34.5% while surpassing strong baselines with significantly fewer parameters. For example, integrating T-Copilot with Qwen2.5-7B outperforms Qwen2.5-14B using 4B fewer parameters. We further comprehensively study the efficiency, transferability, and scalability of T-Copilot, showing that T-Copilot brings marginal computational overhead to Pilot, scales well across different model types and sizes, and effectively transfers to new Pilot models for inference without additional training costs."
        },
        {
            "title": "2 Definition of Mistake Log",
            "content": "2.1 Preliminary and Notations Let (; θP ) denote the function computed by standard Transformer model [78], parameterized by θP . In our context, we refer to as the Pilot model. Suppose there are fine-tuning rounds. For each round [T ], given an input sequence Xt = (xt,1, . . . , xt,n) where is the maximum sequence length, the input is sampled from data distribution = DX ,Y over input-output pairs. The Pilot model then generates an output sequence ˆYt = (ˆyt,1, . . . , ˆyt,n) in an auto-regressive manner to approximate the target sequence Yt = (yt,1, . . . , yt,n), where (Xt, Yt) D. During t-th fine-tuning round, let (cid:101)Xt denote the input representation of Xt, defined as either the encoder output in an encoder-decoder Transformer or the output of the token and positional embedding layer in decoder-only Transformer. In the forward pass through the residual stream of the model, let LP be the total number of decoder layers in the Pilot model. For each layer [LP ], we define ht,i,l( (cid:101)Xt; θP t1) as the (decoder) hidden representations of the i-th token. After the final decoder layer, the Pilot model outputs logits over the vocabulary , conditioned on the input Xt and shifted target sequence yt,<i. The resulting output probabilities for the i-th token are given by: (1) We denote pt,i the ground-truth distribution over for the i-th token, which places full probability mass on the correct token yt,i. The objective of training is to minimize the cross-entropy loss between the predicted and ground-truth tokens, formulated as: ˆpt,i = softmax (cid:0)f (Xt, yt,<i; θP t1)(cid:1) . LP = (cid:88) i=1 log ˆpt,i(yt,i Xt, yt,<i). (2) 2.2 The Mistake Log Next, we define the Mistake Log in fine-tuning scenarios. As shown in Figure 1, the Mistake Log concludes three key components: the input representations (Questions), internal hidden states representations (Rationales), and the token-level error made by the model (Mistakes). In each round [T ], draw the sequence pair (Xt, Yt) D. As defined in Section 2.1, we set (cid:101)Xt as the input representation component, as it provides contextual grounding for the Pilot models specific input sequence. Inspired by prior works [25, 20, 14, 46], the intermediate states hidden representations produced by Transformer blocks also encapsulate rich contextual and semantic information, reflecting the models internal rationales. Therefore, we define ht(Xt; θP t1) as the collection of these internal hidden representations for each token in round t: ht( (cid:101)Xt; θP t1) = ht,i( (cid:101)Xt; θP t1) (cid:110) (cid:111)n , with ht,i( (cid:101)Xt; θP t1) = (cid:110) ht,i,l( (cid:101)Xt; θP t1) (cid:111)LP l=1 , (3) i=1 where ht,i( (cid:101)Xt; θP t1) captures the i-th token level internal states representation at the point when the i-th token error occurs. Then, to quantify the token-level error of the Pilot model, we compute the discrepancy between the predicted distribution ˆpt,i and the ground-truth distribution pt,i for each token, with the error defined as: ℓt(pt, ˆpt) = {ℓt(pt,i, ˆpt,i)}n i=1 , with ℓt(pt,i, ˆpt,i) = pt,i ˆpt,i. (4) Consistent with standard LLM fine-tuning procedures, where the loss LP is used to compute gradients and update the Pilot models parameters across rounds, we simultaneously collect key intermediate signals described above into the Mistake Log throughout this process. Formally, we define the Mistake Log as: (cid:26) (cid:16) MT = (cid:101)Xt, ht( (cid:101)Xt; θP t1), ℓt(pt, ˆpt) (cid:17) (cid:27)T . (5) t=1 The Mistake Log systematically records contextual inputs, internal representations, and token-level prediction errors of the Pilot model throughout its entire fine-tuning trajectory. We next investigate how to leverage the Mistake Log during fine-tuning to enhance the Pilot models final inference performance. Figure 1: Illustration of the Mistake Log. We use the encoder-decoder architecture as an example here. (cid:2)LP Motivation for Transformer Copilot. Recall that the goal of SFT is to optimize θP by minimizing (cid:3). While this process adjusts model parameters using gradient the expected loss E(Xt,Yt)D descent, it treats each error as transient signal, consumed and discarded immediately after the parameter update. As result, the final model parameters θP might not retain an explicit memory of where, how, or why errors occurred during the training trajectory. This oversight leaves valuable training-time information, which we captured in the Mistake Log, untapped at inference time. To address this, we propose new Copilot model to learn from the Mistake Log. Rather than altering the Pilots optimization path, the Copilot operates as an auxiliary module that internalizes the distribution of past mistakes and corrects the Pilots output at inference time. This design enables the Copilot to assist the Pilot model by reflecting on prior missteps and adaptively revising the predictions."
        },
        {
            "title": "3 Transformer Copilot",
            "content": "We introduce our proposed framework, Transformer Copilot, which is designed for both encoderdecoder and decoder-only Transformer architectures. In the following sections, we will elaborate on the Copilot model design, the training paradigm, and the inference paradigm, respectively. 3.1 The Copilot Model Design The Copilot model is initialized from the decoder module of the corresponding Pilot model, but with several new architectural modifications. Consistent with the Pilot model , we denote the Copilot model as C, parameterized by θC. The Copilot model is also auto-regressive, generating outputs 3 Figure 2: Transformer Copilot Framework. The overall framework comprises three key components: (1) Copilot Model Design, (2) Training Paradigm, and (3) Inference Paradigm. over the vocabulary . However, the objective of the Copilot model is to learn from the Mistake Log MT and output rectified logits that correct the predictions made by the Pilot model. Below, we specify the Copilot model design for the encoder-decoder and decoder-only Pilot model separately. t1), ℓt(pt, ˆpt))}T Encoder-Decoder Copilot. As shown in Figure 2.1, the Copilot model receives its inputs from the Mistake Log, MT = {( (cid:101)Xt, ht( (cid:101)Xt; θP t=1. Specifically, the Copilot is conditioned on the sequence of token-level errors made by the Pilot model, as recorded in MT , i.e. ℓt,<i = (pt,1 ˆpt,1, . . . , pt,i1 ˆpt,i1). These discrepancy sequences are provided as labels during training from MT and are auto-regressively generated during inference. As positional information is inherently preserved through the Pilots output, we apply single linear layer to project the token-level errors from vocabulary space into the Copilots hidden dimension. Next, to incorporate additional information from the Pilots input and internal hidden representations ( (cid:101)Xt and ht from MT ), we propose modified cross-attention mechanism in each layer of the Copilot, defined as: New = New = Concat(cid:0) New = Concat(cid:0) l1 Q, for = 1, ..., LC, (cid:0)ht( (cid:101)Xt; θP (cid:0)ht( (cid:101)Xt; θP (cid:101)Xt, PoolLP (cid:101)Xt, PoolLP t1)(cid:1)(cid:1) K, t1)(cid:1)(cid:1) , (6) where PoolLP () denotes the mean pooling across LP layers of the Pilot and Concat() indicates concatenation along the sequence dimension to ensure input dimensional compatibility and computational efficiency; l1 is the Copilot models hidden state from the previous layer (or input projection layer at = 1); and Q, K, are learnable attention weights. We then apply the standard scaled dot-product attention using the new Q, K, and . This modified attention allows the Copilot to jointly attend to both the external input context and the internal processing dynamics of the Pilot. Note that all components retrieved from the Mistake Log can be directly accessed during the forward pass of the Pilot model, without incurring additional computational overhead. After the final layer LC, we add linear projection layer in the Copilot model to map the residual hidden representation into the vocabulary space, producing rectified logits as the output. Decoder-only Copilot. We slightly adapt the Copilot model to accommodate the corresponding decoder-only Transformer [77, 1], while keeping the majority of the model input and design above unchanged. Specifically, we modify the self-attention mechanism to incorporate the information from the Mistake Log: In the odd-numbered layers of LC, we retain the standard self-attention to allow the Copilot model to capture intra-sequence dependencies; In the even-numbered layers, we replace self-attention with the modified cross-attention mechanism defined in Eq. 6, enabling the Copilot to attend to the Pilots input and internal state representations stored in MT . This alternating structure is consistent with the encoder-decoder Copilot to capture its own error-correction dynamics and attend to informative signals from the Pilots behavior. We also explore several alternative designs and empirically validate the effectiveness of our proposed design against these variants in Appendix G.6. Learning Objective. Give the sequence pair (Xt, Yt), at t-th round, the objective of training the Copilot model at i-th token is defined as: LC = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 t,i ℓt(pt,i, ˆpt,i)2, with t,i = C( (cid:101)Xt, ht,<i, ℓt,<i; θC t1), (7) 4 Algorithm 1: Transformer Copilot (Training Paradigm) Input: Pilot model (; θP ), Copilot model (; θC ); Learning rates ηP , ηC ; , 0 , θC 1 Initialize θP 2 for = 1, 2, . . . , do 0 , M0 Draw (Xt, Yt) Pilot - token-level forward pass for = 1, . . . , do Compute ˆpt,i via Eq.1 t1), ℓt(pt, ˆpt)) end Collect Mistake Log (2.2) Mt Mt1 ( (cid:101)Xt, ht( (cid:101)Xt; θP Compute LP via Eq.2; Update θP /* For brevity, we reuse notation */ Draw ( (cid:101)Xt, ht( (cid:101)Xt; θP t1), ℓt(pt, ˆpt)) Mt Copilot - learn from the Mistake Log (3.1) for = 1, . . . , do t1 ηP θP θP LP t1 t,i ( (cid:101)Xt, ht,<i, ℓt,<i; θC t1) end Compute LC Update θC via Eq.7; θC t1 ηC θC LC t1 5 6 7 8 9 11 12 13 14 15 17 18 19 20 end 21 return θP , θC Algorithm 2: Inference Paradigm Input: θP ; Tuning parameter λ , θC 1 Draw new Xt DX , > 2 for = 1, . . . , do 3 ˆpt,i softmax(f (Xt, ˆyt,<i; θP Observe (cid:101)Xt, ht,<i from C t,i ( (cid:101)Xt, ht,<i, pt,i ˆpt,i + λf ˆyt,i Decoding(pt,i) t,<i; θC ) t,i (via Eq.8) 4 6 7 8 end 9 return (ˆyt,1, . . . , ˆyt,n) )) where t,i is the Copilot models prediction, ℓt(pt,i, ˆpt,i) = pt,i ˆpt,i is the corresponding label for the Copilot model, and ht,<i is the collection of Pilots hidden states for the preceding tokens. We adopt the RMSE loss to prevent the distribution error from being further diminished by the square operation, avoiding the over-smoothing effect that squaring may introduce in the gradient signal during backpropagation. Next, we show how to jointly train the Pilot model and the Copilot model during fine-tuning, and collaborate on the generation during inference. 3.2 Training Paradigm. Algorithm 1 outlines the process for jointly training the Pilot and Copilot model. In training round [T ], one sequence pair (Xt, Yt) is drawn from the data distribution D. For each token [n], we first compute the Pilot models output distribution ˆpt,i (Line 5-7). We then retrieve information directly from the forward pass of the Pilot model and update the Mistake log Mt by recording (cid:101)Xt, ht, and ℓt for each token (Line 9). Meanwhile, we compute the Pilot models cross-entropy loss LP and update its parameters (Lines 10-11). Next, we prepare the input for training the Copilot model. Given all collected previous training rounds information, we draw sample ( (cid:101)Xt, ht, ℓt) from the updated mistake log Mt (Line 13). We obtain the Copilot models output t,i for each token [n] (Line 15-17). Finally, we compute the Copilot models RMSE loss LC and update its parameters and θC (Line 18-19). After rounds of iterative training, we obtain the final θP for the Pilot and Copilot model, respectively. Note that this fine-tuning process can be readily extended to mini-batch stochastic gradient descent for scalability. 3.3 Inference Paradigm After learning from the Mistake Log, the Copilot model is deployed alongside the Pilot model to enhance inference-time generation. To avoid abuse of notation, we reuse the same symbols as in training. Given new input sequence Xt DX , > , where Xt is not part of the training data, indexes the inference-time inputs and does not correspond to training rounds. As the objective of the Copilot model is to predict the token-level probability discrepancy pt,i ˆpt,i, we directly use the Copilot models output to rectify the Pilot models prediction ˆpt,i towards the ground-truth pt,i. Formally, the rectified predicted distribution is given by: pt,i = ˆpt,i + λf t,i, 5 (8) where λ (typically set to 1) is tunable hyperparameter controlling correction strength. Introducing λ at inference allows for more flexible modulation, and as we later show in Section 4, with proper λ, the rectified pt,i theoretically provides closer approximation to the target distribution pt,i. Algorithm 2 outlines the overall inference paradigm. Given Xt, the Pilot model outputs predicted distribution ˆpt,i at each token generation step [n] (Line 3). Subsequently, the Copilot model auto-regressively computes its output t,i (Line 5). Finally, the rectified pt,i is obtained via Eq.8 and used to generate the next token via decoding function (Lines 6-7). The inference process is adaptive and can optionally terminate upon generation of the [EOS] (end-of-sequence) token."
        },
        {
            "title": "4 Analyses - Why Learn from the Mistake Log?",
            "content": "To elucidate the roles of the Mistake Log and Copilot model in enhancing the Pilot models inferencetime performance, we present both theoretical and empirical analyses in this section. Theoretical Guarantee. Recall that the Copilot model is designed to analyze the Pilot models internal cognitive states (cid:101)Xt, ht via the collected Mistake Log MT , and learns to predict errors measured by the token-level discrepancies ℓt(pt,i, ˆpt,i). During inference, we use the rectified prediction as pt,i = ˆpt,i + λf t,i. In the following analysis, we show that, under mild assumptions, the adjusted prediction pt,i yields improved inference performance over the original estimate ˆpt,i. Let AP , AC denote the distributions over the function classes of θP , θC, induced by the randomness in the fine-tuning process. Let [k] denote the k-th dimension of vector in RV . Then, we define the expected error and variance of the Pilot and Copilot model at the k-th output dimension as: ϵ2 = E(Xt,Yt)D σ2 = E(Xt,Yt)D ϵ2 = θP AP (Xt,Yt)D σ2 = θP AP (Xt,Yt)D θP AP [ˆpt,i[k] ˆyt,<i])2(cid:3) , (cid:2)(pt,i[k] (cid:2)VarθP AP [ˆpt,i[k] ˆyt,<i](cid:3), (cid:2)(cid:0)pt,i[k] ˆpt,i[k] θC AC [f t,i[k] t,<i](cid:1)2 ˆyt,<i (cid:3), (cid:2)VarθC AC [f t,i[k] t,<i] ˆyt,<i (cid:3). Theorem 4.1. For any [V ], suppose that ϵ2 . Then there exists λ0 > 0 such that for any 0 < λ < λ0, the rectified prediction pt,i = ˆpt,i + λf t,i yields strictly closer approximation to the ground-truth distribution pt,i at dimension k. Specifically, at the i-th token prediction step for Xt DX , we have: + σ2 > 0 and ϵC < (cid:112)ϵ θP AP θC AC (Xt,Yt)D (cid:104) (pt,i[k] (cid:101)pt,i[k])2 (cid:12) (cid:12) (cid:12) t,<i, ˆyt,<i (cid:105) < θP AP (Xt,Yt)D (cid:2)(pt,i[k] ˆpt,i[k])2 (cid:12) (cid:12) ˆyt,<i (cid:3) . Remark 4.2. The assumption ϵC < (cid:112)ϵ2 to have larger bias than the bias ϵP of the Pilot model (; θP ), i.e., ϵ2 + σ2 in Theorem 4.1 allows the Copilot model C(; θC) < ϵ2 < ϵ2 + σ2 . Theorem 4.1 suggests that the rectified prediction pt,i after incorporating the Copilot model achieves strictly lower expected error at k-dimension under mild assumptions and proper λ, indicating the Copilot helps improve the inference performance of the Pilot. The full proof is provided in Appendix C. In addition, Remark 4.2 implies that the Copilot model can improve inference performance without needing to match the Pilots accuracy in isolation. This insight motivates us to apply relatively smaller size of Copilot to complement the Pilot in our empirical implementation. Figure 3: Logits Correction by Copilot. We visualize the logits correction introduced by 1B Copilot model (computed as Fused logits Pilot logits) to highlight the shift by the Copilots rectification. Left: Percentage of logits correction over original Pilots output logits range for three LLaMA-3 Pilot models. Right: Distribution of logits correction magnitudes across reasoning types. 6 Figure 4: Example of Copilots Token-level Rectification on SIQA. The token-level formatting error (forgot) originates during the Pilots mid-way generation and is corrected (answer) by incorporating the Copilot. Empirical Analysis. Complementing our theoretical analysis, we empirically examine the rectification effectiveness of the Copilot model during inference. We leave the setups in Appendix D. Figure 3 illustrates the average logits correction induced by the 1B Copilot model across different Pilot models and reasoning categories. Given that the typical logits range is approximately [10, 10], the observed shifts on the logits distribution indicate clear and consistent adjustment on the final predictions by the Copilot model. We further verify that this Copilots adjustment indeed steers the token prediction toward the correct direction: We analyze representative error patterns frequently observed in the Pilot models output, particularly factual and formatting mistakes. Figure 4 shows detailed example of token-level logits rectification on Pilot model LLaMA-3.2-3B by the 1B Copilot, visualized using the layer-wise Logits Lens [8]. At mid-inference, the Pilot does not follow the correct answer format and makes mistakes (the correct token answer has high but suboptimal logit). The Copilot rectifies the prediction by decreasing the logit of the incorrect token forgot and amplifying that of the correct token, thereby correcting the token prediction error. We leave analyses on other error patterns in Appendix D."
        },
        {
            "title": "5 Empirical Evaluations",
            "content": "Tasks and Datasets. To comprehensively evaluate T-Copilot, we utilize broad suite of reasoning and generation tasks: (i) Commonsense reasoning: PIQA [10], HellaSwag [92], WinoGrande [69], BoolQ [18], SIQA [71], and OpenbookQA (OBQA) [56]. (ii) Arithmetic reasoning: AQuA [49], GSM8K [19], MAWPS [43], and SVAMP [61]. and (iii) Downstream Recommendation: Beauty [30] and LastFM [68]. Detailed dataset descriptions are provided in Appendix E. Implementation Details. For T-Copilot, we construct the Copilot model using the same type of decoder architecture as the Pilot model to ensure consistency. We use the AdamW optimizer and Cosine learning rate scheduler for both Pilot and Copilot models. We modify the generate in HuggingFace Transformers [22] to perform token-level logits fusion and rectified next-token generation during inference. All experiments are conducted on NVIDIA A100 GPUs. We leave all hyperparameter setups and training/inference details in Appendix F.1. Models and Baselines. We incorporate T-Copilot with varying backbone Pilot models. For encoderdecoder Pilots, we utilize T5 [66] and FLAN-T5 [17] across small/base/large variants. For decoderonly Pilots, we employ multiple models from LLaMA-3 [21] and Qwen2.5 [90] families. We denote T-Copilot-small/base/0.5B/1B/3B as the Copilot model on different scales. Detailed model configuration and implementation details are provided in Appendix F.2. We compare against three baseline types: (i) Pilot-only models as described above. (ii) Frontier LLMs with comparable and larger parameters, including LLaMA-3.1-8B [21], Gemma-2-9B [76], and Qwen2.5-14B. (iii) Layer/Adapter expansion methods, including MoE models [72] (Mistral-7B, Ministral-8B), LLaMA/Mistral-Pro-8B [84], Mergekit-9B [26], and TIES[89]. Detailed baseline descriptions are provided in Appendix F.3. 5.1 Incorporating T-Copilot into Pilot Models Yields Better Performance Effectiveness of Copilot in Enhancing Pilot. Table 1 presents the performance gains of incorporating T-Copilot into the Pilot models across different model scales and types. T-Copilot consistently 7 Table 1: Experiment results (%) of incorporating T-Copilot on encoder-decoder/decoder-only backbone models. Results are averaged over 3 independent runs. We report the relative improvement on the backbone Pilot models. T-Copilot boosts existing LLMs on ten reasoning tasks by 2.0%34.5%. Type Model Commonsense Reasoning (Acc. ) Impr. Arithmetic Reasoning (Acc. ) Impr. PIQA WinoG. HellaS. BoolQ SIQA OBQA Avg. AQuA GSM8K MAWPS SVAMP Avg. T5 LLaMA Qwen FLAN-T5-small + T-Copilot-small FLAN-T5-base + T-Copilot-base FLAN-T5-large + T-Copilot-small + T-Copilot-base LLaMA-3.2-1B + T-Copilot-1B LLaMA-3.2-3B + T-Copilot-1B + T-Copilot-3B LLaMA-3.1-8B + T-Copilot-1B 60.3 63.1 65.4 67.3 70.5 72.2 72.8 77.5 80.2 83.3 84.1 85. 85.4 86.2 Qwen2.5-3B 83.6 + T-Copilot-0.5B 85.4 + T-Copilot-3B 87.8 Qwen2.5-7B 87.2 + T-Copilot-0.5B 89.3 + T-Copilot-3B 92.5 52.1 54.4 54.6 56.2 60.4 61.9 63. 71.1 73.7 79.6 82.6 83.7 84.3 86.8 77.5 79.1 81.7 82.1 85.3 87.2 31.6 34. 36.8 39.7 49.5 51.3 52.3 61.8 63.3 89.4 91.1 91.3 90.9 93.5 89.8 91.3 94. 91.4 93.5 95.3 57.9 61.7 61.1 62.5 62.2 63.2 63.7 63.9 65.5 69.1 70.3 72. 69.6 71.8 63.4 66.8 68.7 71.2 73.6 74.8 47.8 52.7 48.6 54.3 58.1 59.8 60. 71.9 74.9 77.4 78.6 79.2 79.9 82.7 77.6 78.1 79.9 79.3 80.0 84.3 29.2 32. 29.6 34.7 31.7 32.6 34.2 66.8 68.9 75.6 77.2 81.3 82.6 83.2 84.6 86.0 89. 89.1 92.1 94.9 46.5 19.6 50.0 7.5% 24.8 49.4 22.8 52.5 6.3% 24.4 23.2 55.4 56.8 2.5% 24.7 57.9 4.5% 25.1 68.8 25.6 71.1 3.3% 28.3 79.1 33.1 80.7 2.0% 36.6 82.3 4.0% 40. 82.1 37.3 84.0 2.3% 38.9 79.4 55.9 81.1 2.1% 57.3 83.6 5.3% 59.4 83.4 61.0 85.6 2.6% 61.4 88.2 5.8% 64.2 5.6 7.4 7.2 9.3 9.9 11.3 11. 27.3 32.2 55.3 58.2 63.1 63.5 66.1 71.4 74.2 76.8 75.3 78.2 79.7 14.7 20. 27.1 32.4 36.7 37.2 39.8 77.1 81.5 86.1 89.1 91.2 89.1 90.8 89.6 91.8 92. 91.2 93.0 94.8 5.3 8.0 6.3 10.3 9.7 11.6 13.8 47.3 51.6 64.2 68.7 71. 73.6 75.4 81.5 82.8 83.5 84.8 86.5 88.1 11.3 15.2 34.5% 15.9 19.1 20.1% 19.9 21.2 6.5% 22.6 13.6% 44.3 48.4 9.3% 59.7 63.2 5.9% 66.5 11.4% 65.9 67.8 2.9% 74.6 76.5 2.5% 78.1 4.7% 78.1 79.8 2.2% 81.7 4.6% Table 2: Performance comparison (%) with baselines under matched parameter scales. Results are averaged over 3 runs. Adding T-Copilot consistently surpasses baselines of equal or even larger size. Model Params Commonsense Reasoning (Acc. ) Arithmetic Reasoning (Acc. ) PIQA WinoG. HellaS. BoolQ SIQA OBQA Avg. AQuA GSM8K MAWPS SVAMP Avg. LLaMA-3.1-8B LLaMA-3.2-3B + T-Copilot-3B 6B (-2B) 8B Qwen2.5-7B Qwen2.5-3B + T-Copilot-3B 7B 6B (-1B) 85.4 85.6 87.2 87.8 Qwen2.5-14B Qwen2.5-7B + T-Copilot-3B 91.8 14B 10B (-4B) 92.5 84.3 83.7 82.1 81.7 85.6 87.2 90.9 91.3 91.4 94. 94.3 95.3 69.6 72.8 71.2 68.7 75.2 74.8 79.9 79.2 79.3 79. 84.5 84.3 82.6 81.3 89.1 89.4 93.1 94.9 82.1 82.3 83.4 83. 87.4 88.2 Comparison with Layer/Adapter Expansion Baselines Mistral-Pro-8B LLaMA-Pro-8B Ministral-8B LLaMA-3.2-3B + T-Copilot-3B 6B (-2B) 8B 8B 8B MergeKit-9B 9B LLaMA-3.1-8B + T-Copilot-1B 9B 83.1 88.4 85.7 85. 86.1 86.2 81.9 81.4 84.1 83.7 84.7 86.8 86.1 86.9 91.3 91.3 91.1 93.5 70.8 73.9 70.3 72. 71.1 71.8 76.1 76.1 77.5 79.2 79.3 82.7 80.6 77.8 81.3 81.3 80.2 83.2 79.8 80.8 81.7 82. 82.1 84.0 37.3 40.1 61.0 59.4 63.5 64.2 35.5 38.2 37.4 40.1 37.0 38. 63.5 63.1 75.3 76.8 79.5 79.7 54.4 57.2 62.9 63.1 65.2 66.1 89.1 91. 91.2 92.6 92.4 94.8 88.2 92.5 90.2 91.2 90.3 90.8 73.6 71.4 84.8 83. 87.9 88.1 68.5 63.5 73.2 71.4 75.2 75.4 65.9 66.5 78.1 78.1 80.8 81. 61.7 62.9 65.9 66.5 66.9 67.8 improves performance across all T5, LLaMA, and Qwen models on 10 commonsense and arithmetic reasoning tasks. In particular, lightweight Copilot (e.g., T-Copilot-small) can deliver meaningful improvements (6.5% on arithmetic) when paired with much larger Pilot model (e.g., FLAN-T5large). Moreover, scaling up the Copilot model leads to additional improvement, underscoring its effectiveness in rectifying the Pilot models predictions during inference. Comparison with Size-Matched Baselines. As shown in Table 2, we first compare our method against stronger models with larger parameters under the same model backbones. While LLaMA-3.23B initially lags significantly behind LLaMA-3.1-8B, incorporating T-Copilot-3B enables the model to outperform LLaMA-3.1-8B, despite using 2B fewer total parameters. Similarly, for the Qwen2.5 series, incorporating T-Copilot-3B enables the smaller Qwen2.5-7B to surpass Qwen2.5-14B with 4B fewer parameters. To provide broader perspective, we also compare with strong baselines from different methods and model types. For instance, although LLaMA-3.2-3B originally trails behind models like Ministral-8B and LLaMA-Pro-8B, incorporating T-Copilot-3B enables it to outperform the strongest baseline under the 8B scale, Ministral-8B, while maintaining 2B parameter advantage. Due to page limits, full comparison results are provided in Appendix G.1. Downstream Tasks. Additional evaluation of T-Copilot and baseline comparisons on downstream recommendation tasks is provided in Appendix G.2. Figure 5: Efficiency Analysis on T-Copilot during fine-tuning and inference. (a) Inference model throughput. (b) Fine-tuning running speeds. (c) Overall training and inference time overhead. 5.2 Efficiency, Transferability, and Scalability Efficiency. To thoroughly evaluate T-Copilots running efficiency, we compare against Pilot and baseline models with the same LLaMA-3 backbone architecture under similar parameter scales. As shown in Figure 5, T-Copilot maintains comparable inference throughput (Figure 5 (a)) and training speed (Figure 5 (b)) to its corresponding Pilot models, while incurring only 4% marginal average increase in time overhead (Figure 5 (c)). In contrast, other baselines such as LLaMA-Pro-8B and MergeKit-9B suffer from significantly higher latency and computational costs relative to their base model LLaMA-3.1-8B. We provide more detailed inference latency report in Appendix G.5  (Table 15)  and discuss the architectural advantage of our model design in Appendix B.1. Transferability and Scalability. Due to space constraints, we move all experiment details here in Appendix G.3 and Appendix G.4. Our results show that T-Copilot is scalable and can be seamlessly transferred to new Pilot models with comparable effectiveness, with no additional fine-tuning needed. Ablation Studies. Detailed ablation studies on T-Copilot, including model design choices, input insertion patterns, and the effect of the hyperparameter λ, are presented in Appendix G.6."
        },
        {
            "title": "7 Conclusion\nIn this paper, we introduce Transformer Copilot, a novel learning framework that enhances\nTransformer-based Pilot models by integrating an auxiliary Copilot model during fine-tuning. By\ncapturing the Pilot model’s learning signals in a Mistake Log during fine-tuning, the Copilot model\nlearns to rectify the Pilot’s logits at inference time, enabling error-aware predictions. We provide\nboth theoretical and empirical evidence that our method improves the Pilot model’s inference pre-\ndictions. Experiments on 12 benchmarks demonstrate the effectiveness, efficiency, scalability, and\ntransferability of Transformer Copilot. Discussions on limitations are provided in Appendix A.",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level In Proceedings of the AAAI conference on language modeling with deeper self-attention. artificial intelligence, volume 33, pages 31593166, 2019. [3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. [4] Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [5] Yikun Ban, Ishika Agarwal, Ziwei Wu, Yada Zhu, Kommy Weldemariam, Hanghang Tong, and Jingrui He. Neural active learning beyond bandits. arXiv preprint arXiv:2404.12522, 2024. [6] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. Ee-net: Exploitation-exploration neural networks in contextual bandits. arXiv preprint arXiv:2110.03177, 2021. [7] Yikun Ban, Jiaru Zou, Zihao Li, Yunzhe Qi, Dongqi Fu, Jian Kang, Hanghang Tong, and Jingrui He. Pagerank bandits for link prediction. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 2134221376. Curran Associates, Inc., 2024. [8] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. [9] Candice Bentéjac, Anna Csörgo, and Gonzalo Martínez-Muñoz. comparative analysis of gradient boosting algorithms. Artificial Intelligence Review, 54:19371967, 2021. [10] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [11] Evelyn Boyd and Ann Fales. Reflective learning: Key to learning from experience. Journal of humanistic psychology, 23(2):99117, 1983. [12] Anne Brockbank, Ian McGill, and Nic Beech. Reflective learning in practice. In Reflective learning in practice, pages 1828. Routledge, 2017. [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [14] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. [15] Jianpeng Cheng. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. [16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1 113, 2023. [17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 10 [18] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [19] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [20] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021. [21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [22] Hugging Face. Transformers documentation. https://huggingface.co/docs/ transformers/main/en/index, 2024. [23] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806, 2017. [24] Yoav Freund, Robert Schapire, and Naoki Abe. short introduction to boosting. JournalJapanese Society For Artificial Intelligence, 14(771-780):1612, 1999. [25] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. [26] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. [27] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. [28] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [30] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web, pages 507517, 2016. [31] Mandy Hommel, Bärbel Fürstenau, and Regina Mulder. Reflection at worka conceptual model and the meaning of its components in the domain of vet teachers. Frontiers in Psychology, 13:923888, 2023. [32] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [33] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. [34] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. [35] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. 11 [36] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [37] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. [38] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [39] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pages 197206. IEEE, 2018. [40] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:10221035, 2021. [41] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2. Minneapolis, Minnesota, 2019. [42] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [43] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. In Proceedings of the 2016 conference of the Mawps: math word problem repository. north american chapter of the association for computational linguistics: human language technologies, pages 11521157, 2016. [44] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. [45] Mike Lewis. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [46] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024. [47] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. [48] Yanhong Li, Chenghao Yang, and Allyson Ettinger. When hindsight is not 20/20: Testing limits on reflective thinking in large language models. arXiv preprint arXiv:2404.09129, 2024. [49] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017. [50] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. [51] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology, page 100017, 2023. [52] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782, 2023. 12 [53] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [54] Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019. [55] Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023. [56] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [57] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. [58] Yuhong Mo, Hao Qin, Yushan Dong, Ziyi Zhu, and Zhenglin Li. Large language model (llm) ai text generation detection based on transformer deep learning algorithm. arXiv preprint arXiv:2405.06652, 2024. [59] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [60] Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and Arsalan Shahid. The ultimate guide to fine-tuning llms from basics to breakthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities. arXiv preprint arXiv:2408.13296, 2024. [61] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. [62] Yunzhe Qi, Yikun Ban, and Jingrui He. Graph neural bandits. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 19201931, 2023. [63] Alec Radford. Improving language understanding by generative pre-training. 2018. [64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [65] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [67] Raquel Robinson, Karin Johansson, James Collin Fey, Elena Márquez Segura, Jon Back, Annika Waern, Sarah Lynne Bowman, and Katherine Isbister. Edu-larp@ chi. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, pages 15, 2023. [68] Benedek Rozemberczki and Rik Sarkar. Characteristic functions on graphs: Birds of feather, from statistical descriptors to parametric models. In Proceedings of the 29th ACM international conference on information & knowledge management, pages 13251334, 2020. [69] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 13 [70] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. [71] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [72] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [73] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [74] Reece Shuttleworth, Jacob Andreas, Antonio Torralba, and Pratyusha Sharma. Lora vs full fine-tuning: An illusion of equivalence. arXiv preprint arXiv:2410.21228, 2024. [75] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991 13005, 2022. [76] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [77] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [78] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [79] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, pages 2296422984. PMLR, 2022. [80] Yihan Wang, Andrew Bai, Nanyun Peng, and Cho-Jui Hsieh. On the loss of context-awareness in general instruction fine-tuning. arXiv preprint arXiv:2411.02688, 2024. [81] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [82] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [83] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [84] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. Llama pro: Progressive llama with block expansion. arXiv preprint arXiv:2401.02415, 2024. [85] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher Manning, and Christopher Potts. Reft: Representation finetuning for language models. arXiv preprint arXiv:2404.03592, 2024. [86] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. 14 [87] Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep representation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020. [88] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. Openp5: An open-source platform for developing, training, and evaluating llm-based recommender systems. arXiv preprint arXiv:2306.11134, 2023. [89] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. [90] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [91] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [92] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [93] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2023. [94] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 1269712706. PMLR, 2021. [95] Jiawei Zheng, Hanghai Hong, Feiyan Liu, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai Wu. Fine-tuning large language models for domain-specific machine translation. arXiv preprint arXiv:2402.15061, 2024. [96] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. [97] Jiaru Zou, Qing Wang, Pratyush Thakur, and Nickvash Kani. STEM-pom: Evaluating language models math-symbol reasoning in document parsing. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [98] Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, and Dongmei Zhang. Promptintern: Saving inference costs by internalizing recurrent prompt during large language model fine-tuning. arXiv preprint arXiv:2407.02211, 2024."
        },
        {
            "title": "Table of Contents",
            "content": "Table of Contents Broader impact and Limitation Additional Details on Transformer Copilot B.1 Architectural Advantages of T-Copilot. . . . . . . . . . . . . . . . . . . . . . . . . B.2 Decoder-only Copilot Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem 4. Additional Empirical Analysis Datasets E.1 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Arithmetic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Downstream tasks: Recommendation . . . . . . . . . . . . . . . . . . . . . . . . E.4 Fine-tuning Dataset Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Setups F.1 Hyperparameters and Training/Inference Details . . . . . . . . . . . . . . . . . . . F.2 T-Copilot Configurations and Implementations . . . . . . . . . . . . . . . . . . . . F.3 Baseline Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experiments G.1 Full Table Report on Baseline Comparison . . . . . . . . . . . . . . . . . . . . . . G.2 Downstream Recommendation Evaluation . . . . . . . . . . . . . . . . . . . . . . G.3 Transferability . G.4 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5 Efficiency Analysis on Transformer Copilot . . . . . . . . . . . . . . . . . . . . . G.6 Ablations on Transformer Copilot . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Related Works 16 17 17 17 18 20 20 20 21 22 23 23 26 27 28 28 29 30 30"
        },
        {
            "title": "A Broader impact and Limitation",
            "content": "Broader Impact. This paper introduces Transformer Copilot, novel framework that enhances LLM fine-tuning by introducing Mistake Log and an auxiliary Copilot model that learns to rectify errors during inference. Our approach improves model reliability and efficiency with minimal overhead and promotes more transparent and interpretable behavior by grounding predictions in prior training dynamics. While our method has broad applicability across domains, we do not foresee any specific societal risks or negative impacts that require special consideration. Limitation. While Transformer Copilot demonstrates robust improvements in inference quality by leveraging model-internal training signals, one potential consideration for future work lies in the coverage and diversity of the Mistake Log itself. Since the Mistake Log is constructed from the forward pass during supervised fine-tuning, its quality is inherently dependent on the richness and representativeness of the fine-tuning data distribution. In scenarios with limited domain coverage or skewed data sources, the Mistake Log may capture narrower set of error patterns, potentially limiting the Copilots generalizability. On the other hand, in our primary experimental setup, we fine-tune on task-diverse datasets with ample coverage, which ensures that the Mistake Log remains informative and representative. Our transferability experiments on the Copilot model further validate the Mistake Logs utility across unseen Pilot models, suggesting robustness to architectural and distributional shifts. Still, exploring data augmentation strategies or adaptive logging policies to enrich Mistake Logs for low-resource or domain-shifted settings remains an interesting future direction. Overall, Transformer Copilot offers promising paradigm shift toward internal signal utilization during LLM fine-tuning. We are optimistic that future research will build upon these contributions to develop even more precise and generalizable models through the continued adoption of reflective learning mechanisms."
        },
        {
            "title": "B Additional Details on Transformer Copilot",
            "content": "B.1 Architectural Advantages of T-Copilot. In Section 3.1, we introduce the Copilot model inherited from the standard decoder module in Transformer [78]. However, our model design exhibits several key advantages compared to the standard decoder module: Specifically, our Copilot model (i) eliminates the need for positional embeddings to preprocess input sequences, (ii) does not require softmax layer to normalize highdimensional logits distributions, and (iii) avoids waiting for the computation of key-value (KV) pairs from the previous layer. These architectural design choices distinguish our method from layer adaptation methods [26, 84] that modify internal Transformer layers, which inherently introduce additional computational overhead. As result, our method minimizes the gap in efficiency between our framework and vanilla models. B.2 Decoder-only Copilot Details The decoder-only copilot model inherits its structure from the pilot model and processes three inputs from the Mistake Log: the token-level discrepancy sequence ℓt, the embedded input sequence (cid:101)Xt, and the pilot models hidden states ht. Note that, different from the encoder-decoder Pilot model, (cid:101)Xt here is derived from the input sequence Xt after the positional embedding layer. In the Decoder-only Copilot model, as stated in Section 3.1, the alternating attention mechanisms effectively mirror the encoder-decoder structure, enabling the decoder-only Copilot to leverage information inside the Mistake Log corrected from the Pilot model. The loss function (RMSE) and target values ℓt(pt,i, ˆpt,i) for the Decoder-only Copilot model remain identical to those used for the encoder-decoder Copilot version. The fine-tuning and inference paradigm are also the same as the encoder-decoder Copilot model, as stated in Algorithm 1 and 2. 17 Proof of Theorem 4. Given the model parameters θP and θC, we denote the Pilot model as (; θP ) and the Copilot model as C(; θC). Let Xt DX represent the input sequence at inference step t, and (cid:101)Xt be the input representation of the Xt; Yt be the corresponding ground-truth answer for the input sequence Xt. For the t-th token prediction during inference, recall that: pt,i = (yt,i Xt, ˆyt,<i) , ˆpt,i = softmax(f (Xt, ˆyt,<i; θP )), t,i = C( (cid:101)Xt, ht,<i, f2,t,<i; θC). Let AP , AC denote the distributions over the function classes of θP , θC, induced by the randomness in the fine-tuning process. Let [k] denote the k-th dimension of vector in RV . Then, we define the expected error and variance of the Pilot and Copilot model at the k-th output dimension as: := E(Xt,Yt)D ϵ2 := E(Xt,Yt)D σ2 := ϵ2 θP AP (Xt,Yt)D := σ θP AP (Xt,Yt)D (pt,i[k] EθP AP [ˆpt,i[k] ˆyt,<i])2(cid:105) (cid:104) (cid:2)VarθP AP [ˆpt,i[k] ˆyt,<i](cid:3) < , (cid:2)(cid:0)pt,i[k] ˆpt,i[k] EθC AC [f < , t,i[k] t,<i](cid:1)2 ˆyt,<i (cid:3) < , (cid:2)VarθC AC [f t,i[k] t,<i] ˆyt,<i (cid:3) < where we assume and have the bounded error and variance at k-th dimension. Here, pt,i[k] denotes the ground-truth probability assigned to the token at dimension [V ] of the vocabulary, for the i-th token prediction step within input sequence Xt. Then, we have the following theorem, which corresponds to Theorem 4.1 in the main body. Theorem C.1 (Restate). Given AP , AC, the Pilot model (; θP ), the Copilot model C(; θC), and data distribution D. For any [V ], suppose the Pilot model is imperfect, i.e., ϵ2 > 0, and the Copilot models error satisfies ϵC < (cid:112)ϵ2 . Then there exists constant λ0 > 0 such that for any 0 < λ < λ0, the rectified prediction pt,i = ˆpt,i + λf t,i yields strictly closer approximation to the ground-truth distribution pt,i at dimension k. Specifically, at the i-th token prediction step for Xt DX , we have: + σ2 θP AP θC AC (Xt,Yt)D (cid:104) (pt,i[k] (cid:101)pt,i[k])2 (cid:12) (cid:12) (cid:12) t,<i, ˆyt,<i (cid:105) < θP AP (Xt,Yt)D (cid:2)(pt,i[k] ˆpt,i[k])2 (cid:12) (cid:12) ˆyt,<i (cid:3) . Proof. For brevity, we omit the explicit expectation condition over the Pilot models previously generated tokens ˆyt,<i, the Copilot models preceding outputs t,<i, and the dimension index [k] in the following proof. Firstly, by the law of total expectation w.r.t.(Xt, Yt) and the bias-variance decomposition w.r.t. ˆpt,i, θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] = E(Xt,Yt)D[EθP AP [(pt,i ˆpt,i)2]] = E(Xt,Yt)D[(pt,i EθP AP [ˆpt,i])2 + VarθP AP [ˆpt,i]] = E(Xt,Yt)D[(pt,i EθP AP [ˆpt,i])2] + E(Xt,Yt)D[VarθP AP [ˆpt,i]] = ϵ2 + σ2 . 18 Secondly, by the law of total expectation w.r.t. (Xt, Yt) and ˆpt,i and the bias-variance decomposition w.r.t. t,i, θP AP θC AC (Xt,Yt)D = θP AP (Xt,Yt)D = θP AP (Xt,Yt)D [(pt,i ˆpt,i t,i)2] [EθC AC [(pt,i ˆpt,i t,i)2]] [(pt,i ˆpt,i EθC AC [f t,i])2 + VarθC AC [f t,i]] = = ϵ2 θP AP (Xt,Yt)D + σ2 C. [(pt,i ˆpt,i EθC AC [f t,i])2] + θP AP (Xt,Yt)D [VarθC AC [f t,i]] Thirdly, by the law of total expectation w.r.t. (Xt, Yt) and ˆpt,i and the CauchySchwarz inequality, θP AP θC AC (Xt,Yt)D = θP AP (Xt,Yt)D = θP AP (Xt,Yt)D [(pt,i ˆpt,i)(pt,i ˆpt,i t,i)] [EθC AC [(pt,i ˆpt,i)(pt,i ˆpt,i t,i)]] [(pt,i ˆpt,i)(pt,i ˆpt,i EθC AC [f t,i])] (cid:114) θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] θP AP (Xt,Yt)D [(pt,i ˆpt,i EθC AC [f t,i])2] (cid:113) (ϵ + σ2 ) ϵ2 = ϵC (cid:113) + σ2 ϵ2 . = Together, it follows that θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i λf t,i)2] θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] [((1 λ)(pt,i ˆpt,i) + λ(pt,i ˆpt,i t,i))2] E[(pt,i ˆpt,i)2] = θP AP θC AC (Xt,Yt)D = (1 λ)2E θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i)2] + λ2E θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i t,i)2] + 2(1 λ)λE θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i)(pt,i ˆpt,i t,i)] θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] = (1 λ)2E θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] + λ2E θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i t,i)2] + 2(1 λ)λE θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i)(pt,i ˆpt,i t,i)] θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] = ((1 λ)2 1)E θP AP (Xt,Yt)D [(pt,i ˆpt,i)2] + λ2E [(pt,i ˆpt,i t,i)2] θP AP θC AC (Xt,Yt)D t,i)] + 2(1 λ)λE θP AP θC AC (Xt,Yt)D [(pt,i ˆpt,i)(pt,i ˆpt,i ) + λ2(ϵ2 + σ2 (cid:1)2 ((1 λ)2 1)(ϵ2 = λ(cid:0)(cid:0)(cid:0)(cid:113) + σ2 ϵ2 which is strictly smaller than 0 as long as 2(cid:112)ϵ2 ϵC + σ2 0 < λ < (cid:113) + σ2 (cid:113) C) + 2(1 λ)λϵC (cid:0)(cid:113) + σ2 ϵ2 + σ2 ϵ ϵ2 + σ2 (cid:1)(cid:1), ϵC (cid:1)λ 2 + σ2 + σ2 (cid:0)(cid:112)ϵ2 ϵC + σ2 (cid:1) (cid:0)(cid:112)ϵ2 ϵC + σ2 (cid:1) =: λ0."
        },
        {
            "title": "D Additional Empirical Analysis",
            "content": "Figure 6: Example of Copilots Token-level Rectification on MAWPS. Setups. In our empirical analysis, we choose LLaMA-3.2-3B as the Pilot model and T-Copilot-1B as the Copilot model. The Copilot models implementation details are the same as stated in Appendix F.2. We evaluate on two reasoning tasks, including SIQA [71] and MAWPS [43]. The dataset details are provided later in Appendix E. Example of Copilots Token-level Rectification. Figure 6 demonstrates another representative example of Copilots token-level rectification on the factual error made by the Pilot model. The token 10\" is originally predicted wrong during the Pilot model mid-generation and is later corrected (token 9\") through the Copilot models logits rectification. To visualize the process, we present three plots showing the top-5 tokens output logits and probabilities in the current token prediction. Note that the Copilot not only increases the logits value on the groundtruth token but also decreases the logits value on the original Pilot models falsely predicted token. We further apply the Logit Lens [8], standard interpretability tool, to project hidden state embeddings from each intermediate layer onto the vocabulary space to show how the Copilot adjusts the Pilot models predictions on each state."
        },
        {
            "title": "E Datasets",
            "content": "E.1 Commonsense Reasoning For the commonsense reasoning tasks, we choose six open-ended multiple-choice QA tasks. The detailed description for each dataset is listed below: PIQA [10]: dataset for physical commonsense reasoning, requiring models to choose the more plausible solution for everyday tasks. WinoGrande (WinoG.) [69]: large-scale dataset for commonsense pronoun resolution, extending the Winograd Schema Challenge [44] with diverse and harder examples. HellaSwag (HellaS.) [92]: benchmark testing commonsense reasoning in story completion by selecting the most plausible next sentence among adversarial choices. BoolQ [18]: question-answering dataset where models answer yes/no questions based on given passage, requiring deep reading comprehension. SIQA [71]: dataset for reasoning about social and emotional situations by selecting the most appropriate response to everyday scenarios. Openbook QA (OBQA) [56]: dataset that tests knowledge-based question answering by requiring models to combine common knowledge with reasoning over multiple facts. 20 In our commonsense reasoning experiments, we follow the experimental setup from [33] and fine-tune both our models and baseline models on the combined training dataset, Commonsense170K, which is constructed by sampling and integrating the training sets of the aforementioned commonsense reasoning datasets. Each datasets individual test set is used for evaluation. Both fine-tuning and testing data instances utilize zero-shot input prompts. E.2 Arithmetic Reasoning For arithmetic reasoning tasks, we evaluate our method on four open-ended math problem-solving datasets spanning multiple mathematical domains. The detailed description of each dataset is provided below: AQuA [49]: dataset of algebraic and arithmetic word problems presented in multiple-choice format, requiring logical reasoning and numerical computation. GSM8K [19]: dataset of grade-school-level math word problems designed to evaluate step-bystep reasoning and arithmetic skills. MAWPS [43]: dataset aggregating math word problems from various sources, focusing on problem diversity and automatic equation generation. SWAMP [61]: dataset that introduces systematic variations of simple arithmetic word problems to assess model robustness against linguistic perturbations In our arithmetic reasoning experiments, we follow the experimental setup from [33] and fine-tune both our models and baseline model on the combined training dataset, Math10K. We also adopt the data preprocessing setup in [85] to avoid any potential training data leakage. Each aforementioned datasets individual test set is used for evaluation. Note that, unlike commonsense reasoning, finetuning for arithmetic reasoning involves labels with zero-shot Chain-of-Thought (CoT) [42] prompts. Consequently, the training cutoff length is longer due to the increased token count and additional information contained in the prompts. E.3 Downstream tasks: Recommendation For downstream application experiments, we utilize two sequential recommendation datasets, as LLM-based recommendation is widely adopted task to evaluate language models generation and decision-making capabilities. The detailed description for each dataset is listed below: Beauty [30]: The Beauty dataset comprises user-item interaction data from the Amazon beauty product category. It includes 22,363 users and 12,101 items, with total of 198,502 interactions. The dataset has sparsity level of 99.93%. LastFM [68]: The LastFM dataset contains 1,090 users and 3,646 items, with 52,551 interactions in total. The sparsity of the dataset is 98.68%. In our experiments, we use the training and testing datasets from [88]. To ensure fair comparison, we assign random numeric IDs to items and evaluate our method and baselines on sequential recommendation tasks. Metrics. For evaluation, we employ two commonly used metrics Hit@K and NDCG@K metrics with {5, 10, 20, 100}. We define each metric in detail below: Hit Rate measures the proportion of users for whom at least one relevant item appears within the top recommendations. H@K = 1 (cid:88) uU I(Rel(u) RK = ) (9) where is the set of users, RK is the top-K recommended items for user u, Rel(u) is the set of relevant items for user u, and I() is the indicator function which equals 1 if the condition is true, and 0 otherwise. NDCG evaluates both the relevance and position of items in the ranked list, assigning higher importance to relevant items that appear earlier, thereby reflecting the overall quality of the ranking 21 system. NDCG@K = 1 (cid:88) uU (cid:80)K i=1 (cid:80)R i=1 relu,i log2(i+1) relu,i log2(i+1) (10) where relu,i is the relevance score of the item at position in the ranked list for user u, and the ideal ranking of relevant items for user u. is E.4 Fine-tuning Dataset Template Table 3: Examples of dataset templates used in Transformer Copilot. Task Type Template Input: Please choose the correct answer to fill in the blank to complete the given sentence: \"After being diagnosed with Parkinsons, he now was taking supplements and prescriptions, but the doctor warned the ___ would be ineffective.\" Option1: supplements Option2: prescriptions Answer format: option1/option2 Answer: The correct answer is option1. Commonsense Reasoning Input: Please choose the correct ending to complete the given sentence: High jump: The boy lands on his back on to red mat. The boy gets up from the mat. The boy: Ending1: starts doing spins. Ending2: celebrates by clapping and flexing both arms. Ending3: is dancing on the mat. Ending4: does jump jacks on his stick. Answer format: ending1/ending2/ending3/ending4 Answer: ending Input: Please answer the following question with true or false, question: is elder scrolls online the same as skyrim? Answer format: true/false. Answer: false Input: Please answer the following math question step by step: Darnel sprinted 0.875 of lap and then took break by jogging 0.75 of lap. How much farther did Darnel sprint than jog? Answer: Step 1: Find the total distance Darnel covered. Total distance = distance sprinted + distance jogged Total distance = 0.875 + 0.75 = 1.625 laps Step 2: Find the distance Darnel sprinted. Distance sprinted = 0.875 laps Step 3: Find the distance Darnel jogged. Distance jogged = 0.75 laps Step 4: Find the difference between the distance Darnel sprinted and jogged. Difference = Distance sprinted - Distance jogged Difference = 0.875 - 0.75 = 0.125 laps Therefore, Darnel sprinted 0.125 laps farther than jogged. The answer in Arabic numerals is 0.125. Input: If the probability that Stock will increase in value during the next month is 0.56, and the probability that Stock will increase in value during the next month is 0.74. What is the greatest value for the probability that neither of these two events will occur? Answer: The probability that stock does not increase is 0.44, and the probability that stock does not increase is 0.26. Now, how can the probability that both do not increase be more than individual probability of not increasing for each? So the probability that both do not increase can not be more than 0.26. Basically the probability that both do not increase is between 0 and 0.26. Input: typical tournament of tennis has 4 rounds. There are 8 games in the first round, 4 in the second round, 2 in the third round and 1 during the finals. If each game requires new tennis balls, and on average each game uses 5 cans of tennis balls, how many tennis balls in total are used at the end of the tournament if each can has 3 tennis balls? Answer: Step 1: Calculate the number of tennis balls used in each game. Each can has 3 tennis balls, so 5 cans have 3 5 = 15 tennis balls. Therefore, each game uses 15 tennis balls. Step 2: Calculate the total number of games in the tournament. The first round has 8 games, the second round has 4 games, the third round has 2 games, and the finals have 1 game. Therefore, the total number of games in the tournament is 8 + 4 + 2 + 1 = 15.Step 3: Calculate the total number of tennis balls used in the tournament. Each game uses 15 tennis balls, so 15 games use 15 15 = 225 tennis balls.Therefore, the total number of tennis balls used in the tournament is 225.0. Input: Considering {dataset}, user_{user_id} has interacted with {dataset} items {history}. What is the next recommendation for the user? Arithmetic Reasoning Downstream Recommendation Answer: {dataset} {target} E.g. Beauty item_ In Table 3, we provide examples of data instances for each task mentioned above during model fine-tuning. All experiments are conducted in the zero-shot setting to better facilitate model-wise evaluation using pass@1 accuracy (i.e., based on single generation attempt)."
        },
        {
            "title": "F Experiment Setups",
            "content": "F.1 Hyperparameters and Training/Inference Details Tables 4-9 present our hyperparameter settings of each task for reproducibility. We perform hyperparameter tuning for both T-Copilot and baseline methods. Unless otherwise specified, both our method and baseline implementations use beam search decoding [23] during inference. All experiments have been run three times with random seeds, reporting average accuracy. For FLAN-T5, LLaMA-3, and Qwen2.5 models, checkpoints are saved every 1,000 steps to track parameters and monitor training to ensure robustness and avoid overfitting. Table 4: Hyperparameter configuration of Transformer Copilot for LLaMA-3 and Qwen-2.5 series models on the Commonsense Reasoning Tasks. Hyperparameters Pilot Model Copilot Model LLaMA-3.2-1B LLaMA-3.2-3B LLaMA-3.1-8B T-Copilot (1B) 3 16 4 256 3e4 Cosine AdamW 200 0.00 32 64 0.05 λ Fine-tuning Configurations Epochs Batch Size Micro Batch Size Cut Off Length Maximum Learning Rate Learning Rate Scheduler Optimizer Warmup Steps Weight Decay LoRA Configurations Rank LoRA Alpha LoRA Dropout Inference Configurations Temperature Top Top Num Beams Maximum New Tokens [0.1, 0.3, 0.5, 0.8, 1.0] 3 16 4 256 3e4 Cosine AdamW 200 0.00 32 64 0.05 3 16 4 256 3e4 Cosine AdamW 200 0.00 32 64 0. 3 16 4 256 5e4 Cosine AdamW 200 0.00 32 64 0.08 0.1 0.95 40 4 64 Table 5: Hyperparameter configuration of Transformer Copilot for LLaMA-3 and Qwen-2.5 series models on the Arithemtic Reasoning Tasks. Hyperparameters Pilot Model Copilot Model LLaMA-3.2-1B LLaMA-3.2-3B LLaMA-3.1-8B T-Copilot (1B) 3 16 4 256 2e4 Cosine AdamW 100 0.00 32 64 0.05 λ Fine-tuning Configurations Epochs Batch Size Micro Batch Size Cut Off Length Maximum Learning Rate Learning Rate Scheduler Optimizer Warmup Steps Weight Decay LoRA Configurations Rank LoRA Alpha LoRA Dropout Inference Configurations Temperature Top Top Num Beams Maximum New Tokens [0.1, 0.3, 0.5, 0.8, 1.0] 3 16 4 256 2e4 Cosine AdamW 100 0.00 32 64 0.05 3 16 4 256 1e4 Cosine AdamW 100 0. 32 64 0.05 3 16 4 256 3e4 Cosine AdamW 100 0.00 32 64 0.08 0.1 0.95 40 4 256 23 Table 6: Hyperparameter configuration of Transformer Copilot for LLaMA-3.2-1B, LLaMA-3.2-3B, and LLaMA-3.1-8B on the Downstream Recommendation Tasks. Hyperparameters Pilot Model Copilot Model LLaMA-3.2-1B LLaMA-3.2-3B LLaMA-3.1-8B T-Copilot (1B) 3 32 1 256 3e4 Cosine AdamW 100 0.00 16 16 0. λ Fine-tuning Configurations Epochs Batch Size Micro Batch Size Cut Off Length Maximum Learning Rate Learning Rate Scheduler Optimizer Warmup Steps Weight Decay LoRA Configurations Rank LoRA Alpha LoRA Dropout Inference Configurations Temperature Top Top Num Beams Maximum New Tokens [0.1, 0.3, 0.5, 0.8, 1.0] 3 32 1 256 3e4 Cosine AdamW 100 0. 16 16 0.05 3 32 1 256 3e4 Cosine AdamW 100 0.00 16 16 0.05 3 32 1 256 5e4 Cosine AdamW 100 0.00 16 16 0.08 0.1 0.95 40 4 Table 7: Hyperparameter configuration of Transformer Copilot for FLAN-T5-small/base/large on the Commonsense Reasoning Tasks. Hyperparameters Pilot Model Copilot Model FLAN-T5-small FLAN-T5-base FLAN-T5-large T-Copilot (small/base) 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0. λ Fine-tuning Configurations Epochs Batch Size Micro Batch Size Cut Off Length Maximum Learning Rate Learning Rate Scheduler Optimizer Warmup Ratio Weight Decay Drop Out Inference Configurations Temperature Top Top Num Beams Maximum New Tokens [0.1, 0.3, 0.5, 0.8, 1.0] 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0.1 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0. 12 16 1 256 3e3 Cosine AdamW 0.05 0.01 0.1 0.1 0.95 40 4 64 24 Table 8: Hyperparameter configuration of Transformer Copilot for FLAN-T5-small/base/large on the Arithmetic Reasoning Tasks. Hyperparameters Pilot Model Copilot Model FLAN-T5-small FLAN-T5-base FLAN-T5-large T-Copilot (small/base) 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0.1 λ Fine-tuning Configurations Epochs Batch Size Micro Batch Size Cut Off Length Maximum Learning Rate Learning Rate Scheduler Optimizer Warmup Ratio Weight Decay Drop Out Inference Configurations Temperature Top Top Num Beams Maximum New Tokens [0.1, 0.3, 0.5, 0.8, 1.0] 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0.1 12 16 1 256 1e3 Cosine AdamW 0.05 0.01 0.1 12 16 1 256 3e3 Cosine AdamW 0.05 0.01 0.1 0.1 0.95 40 4 256 Table 9: Hyperparameter configuration of Transformer Copilot for T5-small/base on the Downstream Recommendation Tasks. Hyperparameters Pilot Model Copilot Model T5-small T5-base T-Copilot (small/base) λ [0.1, 0.3, 0.5, 0.8, 1.0] 20 16 1 256 1e3 Cosine AdamW 0.05 0.01 0.1 20 16 1 256 1e3 Fine-tuning Configurations 20 Epochs 16 Batch Size 1 Micro Batch Size 256 Cut Off Length 1e3 Maximum Learning Rate Learning Rate Scheduler Cosine Cosine Optimizer AdamW AdamW Warmup Ratio Weight Decay Drop Out 0.05 0.01 0.1 0.05 0.01 0.1 Inference Configurations Temperature Top Top Num Beams Maximum New Tokens 0.1 0.95 40 4 64 25 Table 10: Total and Trainable Parameter Statistics. We report the total trainable parameter count for encoder-decoder models. For other model types, we present the proportion of trainable parameters under LoRA fine-tuning relative to the total model size. Type Model Size (Total) Params (Trainable) T5-small + T-Copilot-small 61M 92M T5-small12 T5-base + T-Copilot-base T5/FLAN-T T5-base24 122M 223M 349M 446M 77M FLAN-T5-small + T-Copilot-small 118M FLAN-T5-base + T-Copilot-base 248M 385M 783M FLAN-T5-large + T-Copilot-small 824M + T-Copilot-base 920M LLaMA Pro MoE Llama-Pro-8B Mistral-Pro-8B Mistral-7B Ministral-8B MergeKit MergeKit-9B Gemma Gemma-2-9B 8.9B 8.3B 7.3B 8.0B 8.9B 9.2B 1.3B 2.4B 3.2B 4.3B 8.0B 9.1B LLaMA-3.2-1B + T-Copilot-1B LLaMA-3.2-3B + T-Copilot-1B LLaMA-3.1-8B + T-Copilot-1B 3.1B Qwen2.5-3B + T-Copilot-0.5B 3.6B + T-Copilot-3B 6.1B Qwen2.5-7B 7.6B + T-Copilot-0.5B 8.0B + T-Copilot-3B 10.8B LLaMA Qwen 61M 92M 122M 223M 349M 446M 77M 118M 248M 385M 783M 824M 920M 0.832% 0.858% 0.721% 0.821% 0.710% 0.813% 1.215% 1.246% 1.018% 1.018% 0.700% 0.705% 1.244% 1.650% 1.263% 0.814% 0.819% 0.815% Qwen2.5-14B 14.8B 0.211% F.2 T-Copilot Configurations and Implementations In our implementation, we integrate the Transformer Copilot learning framework into both encoderdecoder and decoder-only LLMs mentioned above. Specifically, we introduce Copilot model as an auxiliary component to the original Transformer architecture. Below, we provide details on our models implementation and notations. T5/FLAN-T5: T-Copilot-small: This refers to our Copilot model being initialized from the decoder module of pre-trained T5-small or FLAN-T5-small model. Specifically, T-Copilot-small consists of 6 decoder layers with hidden state dimension of 512, 8-headed attention, and logit distribution dimensionality of 32,100. To adopt the model for our method, we exclude the conventional positional embedding mechanism and omit the softmax layer typically used for normalizing logits into probability distributions. Additionally, we add linear layer to map the Copilot inputs from the logits distribution dimension to the decoder hidden state dimension. If the Copilots hidden state dimension differs from the Pilot model, an additional linear layer is added for dimension alignment. T-Copilot-base: This refers to our Copilot model being initialized from the decoder module of pre-trained T5-base or FLAN-T5-base model. The overall model implementation is similar to T-Copilot-small. T-Copilot-base consists of 12 decoder blocks with hidden state dimension of 768, 12-headed attention, and logits distribution dimensionality of 32,100. LLaMA-3: T-Copilot-1B: This refers to our Copilot model being initialized from the decoder module of pre-trained LLaMA-3.2-1B model. T-Copilot-1B consists of 16 decoder blocks with hidden state dimension of 2048, 32-headed attention, and logits distribution dimensionality of 128,256. To adapt the model for our method, we exclude the conventional positional embedding mechanism and omit the softmax layer typically used for normalizing logits into probability distributions. To accelerate training, we incorporate the flash-attention mechanism. To enhance inference efficiency, we apply mean pooling to the concatenated input hidden states ht,i(Xt; θ1 t1) without compromising performance accuracy. We add linear layer to map the Copilot inputs from the logits distribution dimension to the decoder hidden state dimension. If the Copilots hidden state dimension differs from the Pilot model, an additional linear layer is added for dimension alignment. T-Copilot-3B: This refers to our Copilot model being initialized from the decoder module of pre-trained LLaMA-3.2-3B. T-Copilot-1B consists of 28 decoder blocks with hidden state dimension of 3072, 24-headed attention, and logits distribution dimensionality of 128,256. Qwen2.5: The model configurations for Qwen2.5 are similar to LLaMA-3 models as they share similar model implementation details. We provide the additional model configurations below: T-Copilot-0.5B: This refers to our Copilot model being initialized from the decoder module of pre-trained Qwen2.5-0.5B. T-Copilot-0.5B consists of 24 decoder blocks with hidden state dimension of 896, 14-headed attention, and logits distribution dimensionality of 151,936. T-Copilot-3B: This refers to our Copilot model being initialized from the decoder module of pre-trained Qwen2.5-3B. T-Copilot-3B consists of 36 decoder blocks with hidden state dimension of 2048, 16-headed attention, and logits distribution dimensionality of 151,936. Notation. In our experiments, we represent our methods using the original model name +\" the Copilot model. For example, FLAN-T5-small+T-Copilot-small denotes the integration of FLANT5-small with T-Copilot-small, and LLaMA-3.1-8B+T-Copilot-1B indicates the incorporation of LLaMA-3.1-8B with T-Copilot-1B. F.3 Baseline Details Frontier Models. Below, we detail the specific model versions of the backbone and baseline models in our experiments. (i) Encoder-Decoder Models: We use T5 and FLAN-T5 [66] with different sizes as our backbone and baseline models for the encoder-decoder Transformer architecture: T5-small, T5-base, T5-large and FLAN-T5-small, FLAN-T5-base, FLAN-T5-large. (ii) Decoder-Only Models: For the decoder-only models, we utilize the LLaMA-3 family [21] as our backbone and baseline models. Our experiments include LLaMA-3.2-1B, LLaMA-3.2-3B, LLaMA-3.1-8B, and LLaMA-2-13B. (iii) MoE Models: For the Mixture-of-Expert based models, we use Mistral-7B with version Mistral-7B-v0.3 and Ministral-8B with version Ministral-8B-Instruct-2410. Layer/Adapter Expansion Models. In our experiments, we also compare against baseline methods that utilize layer and adapter expansion approaches. Below, we provide the model configurations and implementation details for these baselines. (i) LLaMA Pro [84]: LLaMA-Pro-8B incorporates content-addressable working memory module In our implementation, we initialized with the to store and retrieve task-relevant information. 27 LLaMA-3.1-8B base model and expanded the number of blocks from 32 to 40 using an interleaved approach. Mistra-Pro-8B is an enhanced version of the original Mistral model [38], augmented with additional Transformer blocks. The model excels in integrating general language understanding with domain-specific knowledge and follows the same methodology as LLaMA-Pro-8B for block expansion. Following [84], we use the version of Mistral-Pro-8B-v0.1. (ii) MergeKit [26]: MergeKit is an open-source toolkit designed for efficiently merging LLM checkpoints to combine their strengths without additional training. In our experiments, we train and apply one MergeKit model named MergeKit-9B. MergeKit-9B is initialized from LLaMa-3.1-8B and replicates additional layers with post-merge healing. The model is merged using the Passthrough method. In our experiments, we first compare the model with the original LLaMA-3.1-8B to ensure that the merged model does not lead to performance degradation. (iii) TIES [89]: T5-small12 and T5-base24 are T5 type models merged using the TIES method. T5-small12 merges two T5-small models and extends the original T5-small to 12 encoder and decoder layers. And T5-base24 merges two T5-base models and extends the original T5-base to 24 encoder and decoder layers by duplicating existing layers. Model Parameters. In table 10, we provided the detailed model sizes and trainable parameters for both Transformer Copilot and baseline models."
        },
        {
            "title": "G Additional Experiments",
            "content": "G.1 Full Table Report on Baseline Comparison Table 11: Full performance comparison (%) with frontier baselines under matched-parameter scales. Results are averaged over 3 independent runs. Model Params Commonsense Reasoning (Acc. ) Arithmetic Reasoning (Acc. ) PIQA WinoG. HellaS. BoolQ SIQA OBQA Avg. AQuA GSM8K MAWPS SVAMP Avg. Mistral-7B LLaMA-Pro-8B LLaMA-3.1-8B Ministral-8B 7B 8B 8B 8B Qwen2.5-3B + T-Copilot-0.5B 3.5B LLaMA-3.2-3B + T-Copilot-3B 6B Qwen2.5-3B + T-Copilot-3B 6B Qwen2.5-7B + T-Copilot-0.5B 7.5B Gemma-2-9B MergeKit-9B Qwen2.5-14B 9B 9B 14B LLaMA-3.1-8B + T-Copilot-1B 9B 10B Qwen2.5-7B + T-Copilot-3B 8B-level Frontier LLMs 81.3 86.9 90.9 91.3 91.3 91.3 94.0 93.5 65.4 73.9 69.6 70.3 66.8 72.8 68.7 73. 73.1 76.1 79.9 77.5 78.1 79.2 79.9 80.0 >8B-level Frontier LLMs 93.5 91.1 94.3 93.5 95.3 70.2 71.1 75. 71.8 74.8 79.5 79.3 84.5 82.7 84.3 75.3 81.4 84.3 84.1 79.1 83.7 81.7 85.3 82.8 84.7 85. 86.8 87.2 74.5 77.8 82.6 81.3 86.0 81.3 89.4 92.1 86.1 80.2 93.1 83.2 94.9 75.4 80.8 82.1 81. 81.1 82.3 83.6 85.6 82.3 82.1 87.4 84.0 88.2 28.9 38.2 37.3 37.4 57.3 40.1 59.4 61.4 40.1 37.0 63. 38.9 64.2 83.0 88.4 85.4 85.7 85.4 85.6 87.8 89.3 81.4 86.1 91.8 86.2 92.5 50.2 57.2 63.5 62. 74.2 63.1 76.8 78.2 64.3 65.2 79.5 66.1 79.7 85.3 92.5 89.1 90.2 91.8 91.2 92.6 93.0 82.7 90.3 92. 90.8 94.8 57.4 63.5 73.6 73.2 82.8 71.4 83.5 86.5 75.0 75.2 87.9 75.4 88.1 55.5 62.9 65.9 65. 76.5 66.5 78.1 79.8 65.5 66.9 80.8 67.8 81.7 Table 11 shows the full comparison results of T-Copilot against baseline models and methods with matched and larger parameter scales. Notably, under the same model architectures and with less pre-trained knowledge, LLaMA-3.2-3B+T-Copilot-3B outperforms LLaMA-3.1-8B with 2B fewer parameters, Qwen2.5-7B+T-Copilot-3B outperforms Qwen2.5-14B with 4B fewer parameters, and Qwen2.5-3B+T-Copilot-3B outperforms Qwen2.5-7B with 1B fewer parameters. Our method also outperforms other layer/adapter expansion baselines. These results underscore the parameter efficiency and architectural strength of our learning framework. G.2 Downstream Recommendation Evaluation In Table 12 and Table 13, we report the results of T-Copilot on two downstream recommendation datasets: Beauty and LastFM. We choose T5 and LLaMA-3 series models as the backbone Pilot models. Overall, T-Copilot improves the Pilot models by an average of 16.6% across all evaluation metrics on the two datasets. Furthermore, compared to other baselines, incorporating T-Copilot 28 Table 12: Performance comparison on Beauty. All methods are evaluated using both Hit Rates (H@K) and Normalized Discounted Cumulative Gain (N@K). The performance gains are also reported relative to respective backbone methods. Models Beauty H@5 H@ H@20 H@100 N@5 N@10 N@20 N@ T5-small12 1.9 3.2 5.3 15.4 1. 1.8 3.9 6.2 T5-small + T-Copilot-small 2.4 (+0.7) 3.4 (+0.5) 6.2 (+0.8) 5.4 2. 1.7 14.6 17.8 (+3.2) 1.6 (+0.6) 2.1 (+0.7) 4.5 (+1.0) 6.4 (+0.7) 1.0 5.7 3.5 1. T5-base24 2.6 4.6 7.5 18.6 2. 2.9 4.7 6.8 T5-base + T-Copilot-base LLaMA-3.2-1B + T-Copilot-1B LLaMA-3.2-3B + T-Copilot-1B LLaMA-3.1-8B + T-Copilot-1B 2.3 3.2 (+0.9) 4.4 (+1.1) 8.2 (+2.0) 6.2 3.3 17.4 19.8 (+2.4) 2.7 (+0.6) 3.3 (+0.7) 5.2 (+0.7) 7.2 (+1.0) 2. 6.2 2.6 4.5 5.2 6.1 (+0.9) 8.1 (+0.7) 12.5 (+2.5) 24.6 (+5.8) 4.3 (+0.5) 5.1 (+0.7) 5.8 (+0.7) 7.4 (+0.7) 10.0 18. 6.7 3.8 7.4 5.1 4.4 5.1 6.7 (+1.6) 8.6 (+1.0) 13.2 (+2.4) 25.6 (+3.5) 4.3 (+0.7) 5.6 (+1.1) 5.9 (+0.6) 7.8 (+0.6) 10.8 22.1 7.2 3.6 7.6 4. 5.3 5.8 7.1 (+1.3) 9.2 (+0.9) 13.5 (+2.4) 26.4 (+4.9) 4.7 (+0.6) 6.2 (+1.3) 6.4 (+0.8) 8.1 (+0.6) 21.5 11.1 7.5 4. 8.3 4.9 5.6 Table 13: Performance comparison on LastFM. All methods are evaluated using both Hit Rates (H@K) and Normalized Discounted Cumulative Gain (N@K). The performance gains are also reported relative to respective backbone methods. Models LastFM H@5 H@10 H@20 H@100 N@5 N@ N@20 N@100 T5-small12 2.5 3.8 4. 12.4 1.8 2.2 2.8 3.9 T5-small + T-Copilot-small 3.2 (+1.1) 4.4 (+0.7) 5.7 (+1.5) 4.2 2.1 3.7 11.0 15.3 (+4.3) 1.9 (+0.3) 3.2 (+1.2) 3.8 (+1.3) 4.0 (+0.8) 3.2 1. 2.0 2.5 T5-base24 3.8 4.6 7. 17.5 2.0 3.8 3.3 4.7 T5-base + T-Copilot-base LLaMA-3.2-1B + T-Copilot-1B LLaMA-3.2-3B + T-Copilot-1B LLaMA-3.1-8B + T-Copilot-1B 2.7 4.2 (+1.5) 5.1 (+0.9) 8.1 (+2.8) 5.3 4. 14.9 19.4 (+4.5) 2.3 (+0.4) 3.5 (+1.1) 4.2 (+1.3) 5.2 (+1.8) 3.4 1.9 2.4 2.9 5.0 6.4 (+1.4) 6.8 (+1.1) 11.2 (+2.1) 24.7 (+2.8) 2.9 (+0.5) 3.5 (+0.5) 4.3 (+0.4) 6.7 (+0.5) 21.9 6.2 2.4 9.1 5.7 3. 3.9 6.1 6.8 (+0.7) 7.4 (+1.0) 12.1 (+2.9) 25.1 (+1.2) 3.1 (+0.5) 4.2 (+0.7) 5.3 (+1.1) 7.5 (+0.7) 23.9 6.8 2.6 9. 6.4 3.5 4.2 4.7 6.9 (+2.2) 8.6 (+1.3) 12.7 (+2.4) 28.0 (+2.4) 3.9 (+0.8) 4.8 (+1.1) 5.4 (+0.7) 7.9 (+0.9) 10.3 25. 7.0 3.1 7.3 3.7 4.7 achieves 16.7% and 8.6% higher performance than T5-small12 and T5-base24, respectively, on Beauty and LastFM, while using 30M and 126M fewer parameters. These results demonstrate that the error-correction capabilities of T-Copilot are not confined to reasoning tasks but also generalize effectively to other application domains, such as recommendation, where precise LLM decisionmaking is critical for downstream utility. G.3 Transferability In the T-Copilot learning framework, the Copilot model is fine-tuned alongside but separately from the Pilot model. Since the same type of models generally have similar learning trajectories under 29 Table 14: T-Copilot Transferability Results. We report the performance of T-Copilot paired with new Pilot models across four reasoning tasks. The results demonstrate that the Copilot model remains effective for the new Pilot models without being jointly trained . T-Copilot-1B HellaSwag BoolQ GSM8K SVAMP Overall Impr. with new LLaMA-3.2-1B with new LLaMA-3.3-3B with new LLaMA-3.1-8B 63.1 91.4 93.1 65.2 70.2 71.7 32.2 58.8 66.0 51.4 68.5 75.8 6.1% 4.2% 2.4% identical training settings, we further investigate: Can the Copilot model leverage the mistake log of one Pilot model and still be effective on another Pilot model of the same type? We conduct controlled experiments on LLaMA-3 series models in which we directly apply finetuned 1B Copilot model to new Pilot models during inference. The new Pilot model shares the same architecture as the original one but is trained independently. Note that the Copilot model does not \"see\" or \"learn\" any information from the new Pilot model, as they are not jointly trained during finetuning. In Table 14, transferring the Copilot model leads to slight 0.2% performance difference compared to applying the Copilot to the initial Pilot models (jointly training together). We hypothesize that the minor discrepancy is due to the hardware inference differences between the original and new Pilot models. Nonetheless, the transferred Copilot model still delivers substantial performance gains for the new Pilot and consistently outperforms competing baselines. These results demonstrate that T-Copilots error-correction capabilities are not tightly coupled to specific Pilot model and can be effectively transferred without additional rounds of fine-tuning. G.4 Scalability Figure 7: Inference Scaling Laws for T-Copilot. We evaluate the average accuracy of T-Copilot and backbone frontier LLMs across all reasoning tasks at varying model scales. The results are shown for three architectures: FLAN-T5 (left), LLaMA-3 (middle), and Qwen2.5 (right). Figure 7 illustrates the relationship between accuracy and model parameter size for T-Copilot. Overall, incorporating the Copilot model consistently demonstrates improved performance as model size increases. We analyze the relationship between performance accuracy (A) and model parameter size (N ) in billions. The derived equations for our method are as follows: for Flan-T5 backbones: 8.74 log10(N ) + 40.17; For LLaMA-3 backbones: 29.58 log10(N ) + 50.20, and for Qwen2.5 backbones: 12.40 log10(N ) + 71.80. G.5 Efficiency Analysis on Transformer Copilot Table 15 presents the inference latency evaluation across six reasoning datasets. Our learning framework achieves lower latency than baseline models with comparable parameter scales. Specifically, LLaMA-3.1-8B+T-Copilot-1B consistently achieves 22.9% lower inference latency, 3% higher training throughput, and 57% higher tokens-per-second (TPS) on average compared to methods such as LLaMA-Pro-8B and MergeKit-9B. Furthermore, we observe that incorporating T-Copilot increases the inference latency by less than 2% relative to the original Pilot models, while yielding significant performance gains. 30 Table 15: Efficiency Comparison on Inference Latency. We report the total response time (s) per instance across six commonsense reasoning datasets, along with the average result. Inference Latency () PIQA WinoG. HellaS. BoolQ SIQA OBQA Avg. 0.28 LLaMA-3.2-1B + T-Copilot-1B 0.31 0.24 0.25 0.33 0. 0.27 0.29 0.23 0.26 0.27 0.28 0.36 0.39 LLaMA-3.2-3B + T-Copilot-1B LLaMA-3.1-8B + T-Copilot-1B LLaMA-Pro-8B MergeKit-9B 0.46 0.48 0.52 0.52 0.83 0.64 0.45 0.47 0.52 0. 0.75 0.64 0.47 0.49 0.51 0.53 0.82 0.57 0.46 0.46 0.49 0. 0.76 0.54 0.46 0.48 0.49 0.50 0.75 0.63 0.55 0.56 0.62 0. 0.73 0.72 0.48 0.49 0.53 0.53 0.77 0.62 Buffer of the Mistake Log. As described in Section 3, we maintain Mistake Log to record the Pilot models internal learning signals, which serve as training data for the Copilot model. To store this information efficiently with minimal GPU and CPU memory overhead, we detach all relevant outputs from the Pilots forward pass and store them in CPU-resident buffer. By default, we use fixed-size buffer that retains the most recent 128 training rounds. The buffer is updated at each training step, and all Copilot training samples are drawn exclusively from it. This design keeps the additional memory footprint lightweight, typically under 500MB of the CPU memory and less than 200MB of the GPU memory. G.6 Ablations on Transformer Copilot In this section, we perform multiple ablation studies to evaluate the influence of key hyperparameters and alternative method design on the T-Copilots overall performance. Model Design of T-Copilot. Table 16 compares T-Copilot-1B with variant that excludes learning from the Pilot models intermediate fine-tuning stages. The superior performance of T-Copilot highlights the advantage of our joint training paradigm, where the Mistake Log is continuously updated throughout the Pilots training trajectory and enables the Copilot to effectively leverage intermediate-stage information. Table 16: Ablation study on model design. We denote Latest as the variant where the 1B Copilot is trained using only the latest Pilot checkpoint. Table 17: Ablation study on λ. We use the T-Copilot-1B on LLaMA-3 series models. Pilot Copilot AQuA GSM8K MAWPS SVAMP Avg. LLaMA-3.2-1B LLaMA-3.2-3B LLaMA-3.1-8B Latest T-Copilot Latest T-Copilot Latest T-Copilot 27.5 28.3 34.6 36.6 37.6 38.9 30.1 32.2 57.1 58. 64.6 66.1 79.4 81.5 87.5 89.1 90.0 90.8 49.6 51.6 65.2 68. 73.9 75.4 46.7 48.4 61.1 63.2 66.5 67.8 HellaSwag GSM8K λ 1B 3B 8B 1B 3B 8B 0.3 62.0 90.6 90.9 29.8 56.8 64.4 0.5 62.8 90.9 91.5 30.4 57.6 65.9 0.8 63.1 91.2 92.4 31.8 58.1 65.7 1.0 63.3 91.1 93.5 32.2 58.2 66.1 Design Variants of the Decoder-Only Copilot. To validate the efficacy of our proposed decoderonly Copilot design, we explore several alternative architectural variants and empirically compare their impact on the models final performance. Specifically, we examine different insertion patterns for the Copilots new attention mechanism, i.e., the input and hidden states representations from the Pilot model recorded in the Mistake Log. We experiment with various design patterns and modify the Decoder-only Copilot model accordingly. The design options are listed below: Pattern 1 (Ours): Collect the hidden states across all Pilot model layers LP and insert them as key-value (KV) inputs for the even-numbered layers of the Copilot model. Pattern 2: Collect the hidden states across all Pilot model layers LP and insert them as KV inputs for each layer of the Copilot model. This setup examines whether integrating hidden states into all 31 layers of the Copilot model improves performance by leveraging more entry points for processing the pilot models hidden states information. Pattern 3: Collect only the first half (LP /2) layers of the Pilot models hidden states and insert them as key-value (KV) inputs for the Copilot model. Combined with Pattern 4, this setup investigates where the Pilot model makes more mistakes during the learning trajectory. Pattern 4: In contrast to Pattern 3, Pattern 4 collects only the second half (L1 layers) of the Pilot models hidden states and inserts them as KV inputs for the Copilot model. Table 18: Empirical comparison of different design patterns of the Decoder-only Copilot model. We evaluate the LLaMA-3.2-1B Pilot model and T-Copilot-1B. We report the average accuracy on three independent runs. The highest accuracy for each dataset is highlighted in bold. Input Patterns PIQA HellaSwag BoolQ AQuA GSM8K SWAMP Pattern 1 Pattern 2 Pattern 3 Pattern 4 80.2 78.4 75.7 79.3 63.3 61.2 60.7 63.1 65.5 62.8 63.6 63. 28.3 27.1 28.1 27.2 32.2 27.9 30.4 31.8 51.6 49.3 49.8 50.4 We follow the same experiment setups as stated in Section 5. Table 18 compares all 4 patterns on three commonsense reasoning and three arithmetic reasoning tasks. The results of Pattern 2 indicate that without the self-attention mechanism to capture dependencies in the Copilot models generated outputs, the Copilot model struggles to effectively leverage the additional hidden state information during fine-tuning and inference. Additionally, the results comparing Pattern 3 and Pattern 4 do not reveal clear performance trend. This suggests that the Pilot model makes mistakes at different layers depending on the assigned task. Therefore, the Mistake Log MT should capture all hidden states from the Pilot model to ensure that no relevant error-related information is omitted during the Copilot models learning process. Based on this empirical analysis, we demonstrate the effectiveness of Pattern 1 for our Copilot model design. Choice of λ. In theorem 4.1, we theoretically provide bound on the range of λ with 0 < λ < λ0. Here, we empirically study the effect of different λ configurations. The results in Table 17 show that performance generally improves with larger λ values in the range [0, 1]. The optimal value is observed around λ = 1.0. The results demonstrate that higher λ amplifies the effect of T-Copilot, which aligns with our Copilot model design."
        },
        {
            "title": "H Additional Related Works",
            "content": "Transformers for Language Modeling The Transformer is sequence-to-sequence model architecture that employs attention-based mechanisms, making it highly effective for autoregressive language modeling [2, 13, 65]. The vanilla Transformer [78] follows an encoder-decoder structure, comprising stack of identical layers in both the encoder and decoder components. Each layer consists of multi-head self-attention mechanism [15], layer normalization [4], position-wise feedforward network, and residual connection [29]. The encoder-decoder structure serves as the foundation for many early-stage influential LLMs, such as T5 [66], and BART [45]. These models have demonstrated strong capabilities on certain generation tasks [39, 67]. On the other hand, (causal) decoder-only Transformer models [63, 64], trained with the autoregressive language modeling objective [70, 79], have demonstrated exceptional performance in open-ended generation and reasoning tasks [13, 83, 82]. The superior generalization capabilities have established decoder-only Transformers as the backbone of recent state-of-the-art LLMs such as PaLM [16], Falcon [3], LLaMA [77, 21], and ChatGPT [1, 51, 36]. In this work, we develop the Transformer Copilot framework to support both encoder-decoder and decoder-only Transformer architectures. Our intuition is to provide flexibility across broad range of model configurations and downstream task scenarios. LLMs Adaptation with Fine-tuning. Large language models perform well across many NLP tasks, yet adapting them to specialized tasks remains challenging [81]. The standard solution, full-parameter 32 fine-tuning [52], retrains all model parameters on task-specific data. Applying full fine-tuning has proven effective at improving performance, but can sometimes be computationally costly [74]. Recent work on parameter-efficient fine-tuning approaches [40, 32, 86, 98, 28], such as prefix-tuning [47] and LoRA [32], aims to reduce the computational overhead by tuning only small subset of model parameters while still leveraging the expressive power of pre-trained models. Our learning framework builds upon the aforementioned methods fine-tuning paradigm and aims to refine the fine-tuning and inference by utilizing mistake information during the models learning trajectory. Additionally, since the Copilot model retains the decoder module structure, our framework can seamlessly integrate with various adaptation techniques such as DoRA [50] and ReFT [85]. Differences from Boosting and neural exploration. The core idea of Boosting [9, 24] is to train series of models, where each subsequent model focuses on correcting the errors made by the previous ones. However, the proposed Copilot framework is distinct from boosting in several key ways. First, in boosting, the subsequent model is trained to correct the errors of fixed, post-trained weak model, whereas the Copilot learns from the mistakes (errors) made by strong, pre-trained pilot model during its fine-tuning trajectory. Second, the labels (errors) that the subsequent model in boosting attempts to predict are derived from the fixed parameters of the preceding weak model, whereas the labels that the Copilot learns are based on the fine-tuning dynamics of the Pilots parameters. Third, while all models in boosting only take data features as inputs, the Copilot also takes the internal state of the Pilot model as part of its input. Fourth, boosting does not require modifications to the base models, whereas the Copilot framework involves modifying the model structure, specifically the Transformer architecture. Another related work is neural exploration methods [37, 87, 62, 5, 7]. For example, one recent work called EE-Net [6] introduces an exploration neural network to manage trade-offs between exploitation and exploration in the contextual bandits setting. In contrast, the Copilot focuses on learning from the mistakes of the Pilot model in an offline, supervised learning regime, specifically tailored for Transformer-based sequence-to-sequence generation tasks."
        }
    ],
    "affiliations": [
        "Princeton University",
        "University of Illinois Urbana-Champaign"
    ]
}