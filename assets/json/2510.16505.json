{
    "paper_title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies",
    "authors": [
        "Lukas Selch",
        "Yufang Hou",
        "M. Jehanzeb Mirza",
        "Sivan Doveh",
        "James Glass",
        "Rogerio Feris",
        "Wei Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants."
        },
        {
            "title": "Start",
            "content": "PRISMM-BENCH: BENCHMARK OF PEER-REVIEW GROUNDED MULTIMODAL INCONSISTENCIES Lukas Selch1 Yufang Hou2 M. Jehanzeb Mirza3 Sivan Doveh4 James Glass3 Rogerio Feris5 Wei Lin1 1Johannes Kepler University Linz 3MIT CSAIL 4Stanford University 2Interdisciplinary Transformation University Austria 5 MIT-IBM Watson AI Lab https://github.com/da-luggas/prismm-bench https://huggingface.co/datasets/wlin21at/PRISMM-Bench"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess models capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.154.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants. 5 2 0 2 1 2 ] . [ 2 5 0 5 6 1 . 0 1 5 2 : r Figure 1: We collect reviewer-flagged inconsistencies in scientific papers and transform them into QA tasks that probe detection, correction, and reasoning over multimodal inconsistencies. Correspondence: wlin2021at@gmail.com"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in Large Multimodal Models (LMMs) have sparked growing interest in their potential to serve as intelligent assistants for scientific research (Eger et al., 2025), supporting tasks such as figure and chart interpretation (Roberts et al., 2024; Tanasa & Oprea, 2025; Wu et al., 2024), paper summarization (Tan et al., 2025; Saxena et al., 2025; Yu et al., 2025), and error detection (Yang et al., 2025b; Miyai et al., 2024; Alsaif et al., 2024). Yet, central open question remains: can LMMs truly reason over the complex multimodal structure of scientific documents? central challenge in this setting is detecting and resolving inconsistencies between text, figures, tables, or equations in scientific papers. These issues are often subtle, arising from copy-paste mistakes, outdated results, or inconsistent notation, and require domain knowledge to detect. Fig. 1 illustrates such case, where the reward function is defined differently in the figure and the inline text. Our analysis of ICLR 2025 submissions reveals that 17.0% contained at least one such inconsistency flagged by reviewers. These discrepancies undermine clarity, reproducibility, and ultimately scientific trust. Existing benchmarks, however, fall short of exposing this. Document QA datasets (Mathew et al., 2021; 2022; Zhu et al., 2022) or standalone scientific visual element tasks focusing on diagrams, charts, or tables (Kafle et al., 2018; Masry et al., 2022; Cheng et al., 2022) miss the multimodal dependencies of scholarly works. Synthetic datasets (Yan et al., 2025) generate artificial errors, but these are often obvious and unrepresentative of real-world complexity. However, constructing such benchmark of authentic inconsistencies is challenging as these cases are rare, scattered, and labor-intensive to verify, often requiring domain expertise to identify and validate. To address this, we leverage valuable but underutilized resource - open peer reviews, focusing on instances where reviewers flag mismatches across different modalities, such as between text, figures, tables, and equations, thereby providing natural source of real-world, human-identified inconsistencies. In this paper, we introduce PRISMM-Bench, Peer-Review-sourced Inconsistency Set for Multimodal Models. Unlike synthetic datasets, PRISMM-Bench captures inconsistencies explicitly flagged by human reviewers in scientific papers on OpenReview. Through multi-stage pipeline combining large-scale review mining, LLM-assisted filtering, and rigorous human verification, we curate dataset of 262 inconsistencies across 242 papers submitted to ICLR 2025, spanning 13 categories of visual-textual and inter-visual mismatches. PRISMM-Bench provides principled resource for evaluating and improving LMMs, grounded in the real challenges of understanding and verifying scientific papers. Grounded on these recent reviews, we minimize data contamination risk and demonstrate pipeline with the potential to construct continuously updated live benchmark. Building on this inconsistency set, we design benchmark suite of three multiple-choice question (MCQ) tasks: 1) Inconsistency Identification - detect what the inconsistency is; 2) Inconsistency Remedy - determine how to fix the inconsistency; and 3) Inconsistency Pair Match - identify which two elements conflict. Together, these tasks form tiered framework that evaluates not only models ability to spot inconsistencies but also its capacity to propose remedy, and reason over relationships between different modality components. In MCQ evaluation, key challenge is models tendency to rely on linguistic biases in answer choices. Prior work has shown that LLMs often exploit choice-only shortcuts, achieving non-trivial accuracy without reading the question (Chandak et al., 2025; Turner & Kurzeja, 2025; Balepur et al., 2024; Chizhov et al., 2025), and similar effects appear in multimodal MCQs (Chandak et al., 2025). To address this, we propose novel structured JSON-based answer representation that deemphasizes stylistic cues and minimizes spurious correlations. Inspired by Das et al. (2014) and Banarescu et al. (2013), our design converts free-form natural language into uniform structured representations that reduce model sensitivity to surface-level patterns. Our user study confirms that this approach suppresses linguistic shortcuts and better aligns models with human reasoning. We benchmark 21 state-of-the-art LMMs, spanning large open-weight models such as GLM-4.5V 106B (Hong et al., 2025) and InternVL3 78B (Zhu et al., 2025), as well as leading proprietary models including Gemini 2.5 Pro (Comanici et al., 2025) and GPT-5 (OpenAI, 2025). Results show that while large open-weight models achieve around 40% accuracy, even the strongest proprietary models reach just 54.2%, underscoring difficulty of the benchmark and limitations of current LMMs."
        },
        {
            "title": "Preprint",
            "content": "Our contributions are fourfold: (1) We propose reviewer-sourced dataset of real multimodal inconsistencies in scientific papers, spanning diverse categories and grounded in peer review. (2) We construct benchmark suite of three tasks probing detection, correction, and relational reasoning over these inconsistencies. (3) We are the first to propose JSON-based debiasing method for MCQ, converting free-form responses into uniform structured representations. (4) We evaluate 21 state-of-the-art LMMs, exposing their current limitations in detecting, understanding and correcting inconsistencies in scientific papers. The dataset and code for creating the benchmark and evaluating LMMs are publicly available."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Large Multimodal Models (LMMs). Large Multimodal Models (LMMs) pair vision encoder with large language model, enabling open-ended reasoning across tasks such as image captioning, VQA, document understanding, and chart interpretation. Early approaches like BLIP-2 (Li et al., 2023) and InstructBLIP (Dai et al., 2023) introduced instruction tuning on pre-trained visionlanguage models, while the LLaVA series (Liu et al., 2023b;a; Li et al., 2024a) advanced perception and reasoning via large-scale visual instruction tuning. Several recent studies (?????Hansen et al., 2025; Mei et al., 2025) have advanced these models by introducing improved training and adaptation strategies. Recent models extend these capabilities: Qwen-2.5 VL (Bai et al., 2025) offers precise object localization, dynamic resolution, and agentic tool execution; InternVL3 (Zhu et al., 2025) improves perception and reasoning through domain-specific pretraining on 3D scenes, GUIs, and video; Gemma 3 (Team et al., 2025), Ovis 2 (Lu et al., 2025), and GLM 4.5V (Hong et al., 2025) demonstrate strong performance across diverse multimodal benchmarks. High-resolution variants such as InternLM XComposer 2.5 (Zhang et al., 2024a) and VILA HD 4K (Shi et al., 2025), enable detailed perception and document processing. Proprietary models like GPT-5 (OpenAI, 2025) and Gemini 2.5 Pro (Comanici et al., 2025) set the state-of-the-art on complex multimodal tasks through large-scale training and enhanced reasoning. These LMMs form the foundation for evaluating multimodal reasoning over scientific documents. In PRISMM-Bench, we benchmark 21 top-performing models to detect, understand, and correct real-world inconsistencies in peer-reviewed papers, exposing both their strengths and limitations. Multimodal Benchmark on Scientific Paper Understanding. Prior benchmarks often focus on isolated scientific elements such as diagrams (Kafle et al., 2018; Chaudhry et al., 2020; Kahou et al., 2018), charts (Masry et al., 2022; Methani et al., 2020), or tables (Cheng et al., 2022; Nan et al., 2022). Recent datasets like MathVista (Lu et al., 2024), MathVerse (Zhang et al., 2024b), and ArXivQA (Li et al., 2024b) integrate multiple modalities, but still treat figures and equations in isolation rather than in full-paper context. Whole-paper QA resources such as PubMedQA (Jin et al., 2019), BioASQ (Krithara et al., 2023) and QASPER (Dasigi et al., 2021) provide humanwritten questions, yet these are mostly abstract-based and limited to yes/no or short-span answers. Closer to our setting, QASA (Lee et al., 2023) provides 1.8K expert-written questions on ML papers, but remains text-only and does not require reasoning over figures or tables. SPIQA (Pramanick et al., 2024) introduces multimodal scientific QA, yet the questions are LLM-generated or humanannotated with an emphasis on information seeking, not grounded on expert reviews that often aim to critically evaluate scientific findings. SciDQA (Singh et al., 2024) is sourced from reviewerauthor QA pairs, but it remains text-only LLM benchmark without involving visual elements. In contrast, PRISMM-Bench is the first benchmark grounded in reviewer-flagged multimodal inconsistencies in scientific papers. Unlike prior work that isolates figures, tables, or text, our benchmark integrates visual and textual reasoning within the natural context of full research papers, while grounding tasks in authentic peer review feedback rather than synthetic or abstract-level annotations. Understanding of Inconsistencies. Research on inconsistencies in language models spans prediction variance across paraphrased queries (Ravichander et al., 2020; Elazar et al., 2021) to factual inconsistency in summarization and long-form QA. To address the latter, prior work has introduced QA-based benchmarks (e.g., WikiContradict (Hou et al., 2024)), evaluation metrics (e.g., QAFactEval (Fabbri et al., 2022)), and detection methods based on QA (Wang et al., 2020), natural language inference (Lattimer et al., 2023), or probabilistic reasoning (Marinescu et al., 2025)."
        },
        {
            "title": "Preprint",
            "content": "Closest to our setting, MMIR (Yan et al., 2025) evaluates multimodal models on artificially injected inconsistencies in materials such as slides and posters. In contrast, PRISMM-Bench introduces realworld reviewer-flagged inconsistencies in scientific papers. Rather than synthetic perturbations, our benchmark captures authentic challenges faced during scholarly review, spanning textual, visual, and cross-modal errors, and extends evaluation beyond detection to proposing remedies. Language Biases in Evaluation Benchmarks. Multiple-choice evaluation is prone to linguistic biases, where models exploit surface-level patterns in answer options rather than reasoning over content. Prior studies show LLMs can achieve high accuracy even without the question, such as in TruthfulQA (Turner & Kurzeja, 2025), HellaSwag (Zellers et al., 2019), and ARC (Balepur et al., 2024). For example, Chandak et al. (2025) report 83% accuracy on TruthfulQA v2 using answer choices alone, with shortcut rates above 70% on HellaSwag. The recent trend of generating distractors with LLMs (e.g., in MMLU-Pro; Wang et al. (2024a)) can even exacerbate these artifacts. To mitigate such biases, structured representations offer promising direction. Analogous to authorship obfuscation in stylometry (Chinchor, 1998), structured formats remove stylistic and surface cues while retaining semantics. Drawing inspiration from FrameNet-based semantic parsing (Das et al., 2014) and MUC slot filling (Uchendu et al., 2023), PRISMM-Bench introduces JSON-based answer representations that encode key elements for capturing inconsistencies in scientific papers. This design reduces artifacts in answer choices and compels models to engage with multimodal content rather than exploiting linguistic shortcuts."
        },
        {
            "title": "3 PRISMM-BENCH",
            "content": "PRISMM-Bench is built through six-stage pipeline  (Fig. 2)  : (1) review sourcing, (2) LLM-based review filtering, (3) manual annotation of reviewer-flagged inconsistencies (Sec. 3.1), (4) LMMbased task generation, (5) manual verification to finalize benchmark tasks (Sec. 3.2), and (6) LLMbased debiasing to reduce language biases (Sec. 3.3). The evaluation step is introduced in Sec. 3.4. 3.1 COLLECTION OF REVIEWER-FLAGGED INCONSISTENCIES To build benchmark of realistic and authentic inconsistencies, we sourced cases flagged by reviewers on OpenReview (ope, b), where comments often highlight discrepancies between textual content and visual or mathematical components, including figures, tables, and equations. Review Sourcing Strategy. We collected reviews from ICLR 2025 submissions via the OpenReview API v2 (ope, c). To maximize the likelihood that flagged inconsistencies persisted in the final public PDFs, we restricted to rejected or withdrawn submissions without rebuttals.1 This yielded 12,366 reviews (details in App. D.1). LLM Review Filtering. As manual screening for all reviews was infeasible, we employed an LLM for review filtering. Specifically, we used Mistral Nemo 2407 (Mistral, 2024) with low temperature settings to summarize reviews and identify potential inconsistency mentions, resulting in curated set of 5,258 potential inconsistencies spanning 3,206 reviews (prompt details in App. F.1). Manual Verification. We performed manual annotation pass using custom web-based annotation tool. The tool presented the annotator with one reviewer-flagged inconsistency at time, alongside the corresponding paper in an embedded PDF viewer. Annotators (1) verified whether reviewer comment described factual and identifiable inconsistency, and (2) annotated the relevant textual and/or visual parts of the paper. For visual elements, the annotator selected and cropped regions from the PDF. For textual elements, they specified the page, line, and content. In addition, each inconsistency was assigned category and brief description in the annotators own words. The tool logged annotations together with the original reviewers comment and automatically collected metadata such as the crop bounding boxes in structured format. Full details of the annotator background, annotation tool, captured metadata, annotation criteria and schema are provided in App. G. 1Our earlier exploration of review sourcing revealed that most reviewer-flagged inconsistencies were resolved during rebuttal and did not persist in the final versions, motivating the current refined sourcing strategy."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Pipeline of PRISMM-Bench. The top row illustrates the six main steps: (1) review sourcing, (2) LLM-based review filtering, and (3) manual annotation of metadata for reviewer-flagged inconsistencies (Sec. 3.1), (4) LMM-based task generation, (5) manual verification to construct benchmark tasks (Sec. 3.2), and (6) LLM-based debiasing to mitigate language biases (Sec. 3.3). The bottom row shows representative outputs at each stage: filtered reviews after step 2, inconsistency annotation after step 3, an example multiple-choice question in natural language after step 4, and its debiased JSON-format counterpart after step 6. This process produced 262 validated inconsistencies across 242 ICLR submissions. We identified 13 categories of inconsistencies based on the elements involved (distribution shown in Fig. 9). The most common cases were figuretext mismatches (24.4%) and intra-figure inconsistencies (24.0%). 3.2 GENERATION OF BENCHMARK TASKS From the verified inconsistencies, we constructed three multiple-choice tasks with four options (one correct, three distractors), following the evaluation choice of most recent frontier model releases (Yang et al., 2025a; Liu et al., 2025; Comanici et al., 2025; Team et al., 2024) and benchmarking efforts (Wang et al., 2024b; Zhang et al., 2025; Shabtay et al., 2025). We design the following three multiple-choice tasks. Inconsistency Identification (Ident). The first task evaluates models ability to recognize inconsistencies within the given paper context, framed by the question: What is the inconsistency in"
        },
        {
            "title": "Preprint",
            "content": "these parts of scientific paper? We adopt this generic question style because our preliminary study showed that sample-specific questions (e.g. What inconsistency is observed between Figure 2 and the accompanying text regarding the generated road network?) reveal the inconsistency content and oversimplify the task (see App. D.4.2 for details). Candidate answers were generated using Gemini 2.5 Flash based on inconsistency descriptions and corresponding multimodal context. The answers were manually refined to ensure (1) the correct choice captured the inconsistency precisely and (2) the distractors are contextually relevant and plausible, but incorrect. We show an example of the Ident task in Fig. 2. Inconsistency Remedy (Remedy). This task extends beyond simple detection by requiring models to how to fix the inconsistency by asking the question What action needs to be taken to resolve the inconsistency in these parts of scientific paper? Gemini 2.5 Flash was employed to reformulate the inconsistency statements from Ident into specific, actionable remedy formulations. This task evaluates whether models can propose plausible solutions, rather than merely spotting inconsistencies. Inconsistency Pair Match (Match). This task is built on subset of inconsistencies that involve two distinct visual elements within paper (135 samples). Given one element as context, the model must select its inconsistency counterpart from four options. By restricting the task to visual-visual mismatches, we specifically assess models ability to detect representation errors without relying on textual cues, simulating the common peer-review challenge of cross-checking figures and tables for consistency. More details about the task generation process are provided in App. D.4. We provide qualitative examples of the three tasks in App. and dataset viewing tool in the supplementary materials. 3.3 ALLEVIATION OF LINGUISTIC BIASES During pilot experiments, we observed that models achieved well above random accuracy even when the visual context was withheld. For example, Gemini 2.5 Flash reached 57.6% accuracy on the Ident task without context (vs. 25% random chance). This indicated that models exploit linguistic priors and surface patterns in the answer options rather than reasoning over the actual content. Further analysis showed that factors such as answer length, relative position, and phrasing contributed to this bias, echoing known challenges in multiple-choice design (Gierl et al., 2017). To combat this bias, we first tried refining the distractors with text manipulation, which proved insufficient. Therefore, we introduced structured representations that minimize natural language cues. We designed the EvidenceClaim JSON format for the Ident task and the TargetAction JSON format for the Remedy task. Converting answers into these structured formats using an LLM reduced Gemini 2.5 Flashs no-context accuracy on the Ident task to 34.0%. We manually verified 20% subset of the inconsistencies to ensure the semantic fidelity of the JSON-formatted answers. An example of the Evidence-Claim JSON format for the Ident task is provided in Fig. 2. Full details on our debiasing procedure and the structured formats are provided in App. D.4.2. This design choice is further supported by our user study (Sec. 4.3), which reveals that humans rely minimally on linguistic priors. In contrast, models evaluated on natural language options maintain high accuracy without context, exposing fundamental mismatch in evaluation fidelity. By adopting structured JSON representations, we align model evaluation conditions more closely with human cognitive constraints, suppressing surface-level shortcuts and enabling fairer assessment of true multimodal reasoning. 3.4 CONTEXTUAL GRANULARITY IN EVALUATION We evaluate model performance under three levels of contextual granularity, reflecting different real-world reading conditions and reasoning demands. Focused Context (Focused). The model is presented only with the minimal necessary components an extracted visual element (e.g., cropped figure or table) and/or the precise text passage (e.g., sentence or paragraph) involved in the inconsistency, as annotated. This setting isolates the key content, testing the models ability to detect inconsistencies with minimal noise."
        },
        {
            "title": "Preprint",
            "content": "Page Context (Page). The model receives 144 DPI rasterized image of the entire page(s) where the inconsistency occurs. Visual elements are not pre-cropped, requiring the model to locate and interpret relevant content within the full page layout. This simulates realistic reading conditions where inconsistencies must be identified without prior localization. Document Context (Document). The model receives the entire scientific paper as sequence of page images. To accommodate architectural constraints, we follow MMLongBench-Doc (Ma et al., 2024) and segment the document into collages: total of 5 images are fed to the model, each containing npages/5 pages arranged in 3-column grid. This setting evaluates the models capacity for long-range, cross-page reasoning and document-level grounding. For models with highresolution processing constraints, such as LLaVA Onevision (7B, 72B), we reduce input to 3 images with npages/3 pages each to avoid exceeding the context window."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Model Selection. We evaluate 21 LMMs spanning diverse range of architectures: 16 open-weight models of varying scales, two specialized high-resolution models, and three proprietary models. Selection was guided by performance on the Open VLM Leaderboard (ope, a) and the availability of the latest model versions. Inference Details. To ensure consistent scoring, we enforced strict answer format. Prompts at both the system and user level instructed models to output only the letter corresponding to their chosen option. For reasoning-enabled models, answers were extracted separately from reasoning traces, enclosed in <think></think> tags, before postprocessing against the ground truth. All open-weight models were grouped into three parameter count categories and evaluated with vLLM v0.10.1 (vll). Experiments were conducted on 4NVIDIA A100 64GB GPUs with greedy decoding, except for InternVL3.5 (8B, 38B) which required temperature of 0.6 for stable reasoning. Proprietary models were accessed via their official APIs with greedy decoding except GPT-5 (minimal, high) which has fixed temperature of 1.0. Each model was evaluated on all three tasks (Ident, Remedy, Match), and across three contextual granularity levels (Sec. 3.4). For Match, only Focused Context was used. This design yields seven evaluation configurations per model, enabling fine-grained analysis of how model architecture, scale and input context affect inconsistency detection performance. 4.2 MAIN RESULTS Table 1 summarizes the aggregate performance of all evaluated models. Our results reveal clear trends in how LMMs handle inconsistency detection and correction. Performance Landscape. Proprietary models substantially outperform their open-weight counterparts. Gemini 2.5 Pro and GPT-5 (high reasoning) reach the highest average performance of 54.2%. By contrast, the best open-weight model GLM 4.5V 106B achieves 41.9%, matching GPT-5 (minimal reasoning) but trailing its high-reasoning variant by 12.3 points. These results underscore the difficulty of the benchmark: even the best-performing models remain far from the reliability required of automated scientific assistants. Impact of Context and Task Formulation. Performance drops consistently as context scope expands. Models achieve their best accuracy in the Focused setting but often degrade toward random chance under Page and Document inputs, reflecting persistent challenges with distraction and longrange grounding in dense, multi-page inputs. To rule out input quality effects, we performed an ablation study on rasterization resolution, confirming our 144 DPI baseline (cf. App. C.1). Task formulation also plays critical role. Remedy scores are consistently lower than Ident, showing that proposing corrections requires deeper reasoning than only detection. Performance on Match varies widely across models: Gemma 3 12B achieves 64.4%, rivaling proprietary models, whereas"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Accuracy (%) of 21 diverse LMMs across three tasks (Ident, Remedy, Match) and three levels of contextual granularity (Sec. 3.4). For Match, results are reported only under the Focused setting. Best result per task bolded, second best underlined. denotes reasoning models. Focused Page Document Params [B] Ident Remedy Match (135) (262) (262) Ident Remedy (262) (262) Ident Remedy Average (262) (262) (659) Model Gemma 3 4B LLaVA OV 7B Ovis2 8B Qwen 2.5 VL 7B InternVL3 8B InternVL3.5 8BR Gemma 3 27B Gemma 3 12B Qwen 2.5 VL 32B Ovis2 34B InternVL3 38B InternVL3.5 38BR 4.0 7.0 8.0 7.0 8.0 8.0 27.0 12.0 32.0 34.0 38.0 38.0 LLaVA OV 72B Qwen 2.5 VL 72B InternVL3 78B GLM 4.5V 106BR 72.0 72.0 78.0 106. Small Open-Weight Models (<9B) 25.6 29.4 33.2 30.9 35.1 48.9 25.2 26.3 27.5 28.2 28.2 34.7 38.5 23.7 17.0 59.3 54.8 49.6 24.4 32.4 36.6 29.0 28.6 36.6 24.0 27.9 23.7 29.8 26.7 30. Medium Open-Weight Models (9B38B) 34.4 32.1 42.4 49.6 47.3 57.3 36.3 50.8 49.6 55.7 30.5 30.5 35.9 40.5 37.0 44.7 64.4 64.4 45.9 33.3 57.8 44.4 27.9 32.4 36.6 40.1 39.7 42. Large Open-Weight Models (>38B) 30.5 34.7 37.8 42.0 28.1 26.7 45.9 54.1 32.1 41.6 38.9 46.6 Specialized High-Resolution Models 25.6 29.0 32.8 26.7 32.4 36. 30.5 30.5 33.2 33.2 34.0 31.7 32.4 36.3 36.6 37.8 28.6 26.7 43.1 48.1 36.6 25.6 27.1 26.3 29.0 33.6 32.4 26.7 26.7 27.9 32.1 29.4 30. 27.9 23.3 32.1 30.5 28.2 28.2 32.4 36.6 34.4 26.1 28.3 29.0 31.3 32.7 37.6 32.1 32.5 35.9 38.0 38.3 40.1 30.9 35.2 38.3 41. 26.7 27.8 43.9 54.2 54.2 26.0 26.0 34.4 35.1 32.4 31.7 27.9 29.0 33.6 32.8 26.0 27.9 36.3 50.0 54. InternLM XC 2.5 7B VILA HD 4K 8B GPT-5 (minimal)R GPT-5 (high)R Gemini 2.5 ProR 7.0 8.0 28.2 30.9 54.6 68.3 69. 24.0 28.2 23.7 23.7 26.7 26.7 Proprietary Models 40.5 55.0 66.0 61.5 71.1 66. 47.7 58.8 58.0 much larger models such as InternVL3 78B lag behind at 45.9%. These results suggest that architectural design, not just scale, is critical for relational reasoning. Model Characteristics. Reasoning-enabled models show benefit. For example, InternVL3.5 8B outperforms its non-reasoning predecessor InternVL3 8B by over 5 percentage points, achieving accuracy comparable to models with nearly nine times more parameters. Turning off chain-ofthought reduces accuracy by up to 19 points (cf. App. C.2), directly confirming the contribution of reasoning. In contrast, high-resolution specialists (VILA HD 4K 8B, InternLM XC 2.5 7B) show little advantage in extended-context settings. More broadly, our results challenge the bigger is better paradigm: scaling up parameter counts alone does not guarantee higher performance, with diminishing returns observed from mediumto large-scale models. Overall, these findings highlight the current limitations of LMMs for scientific document analysis. Future progress will require advances in reasoning architectures to move beyond error detection toward correction, as well as more robust mechanisms for grounding over long, distractive contexts. 4.3 USER STUDY AND LINGUISTIC BIAS ANALYSIS To complement our benchmark, we conducted user study to establish human baseline and quantify visual reliance the extent to which answers depend on genuine multimodal reasoning rather than linguistic shortcuts. While our benchmark uses structured JSON outputs for models, our participants are evaluated on natural language (NL) questions as structured formats are less practical without prior training. To enable direct comparison, we re-evaluated representative LMMs on the same Ident task subset using natural language answer options. Setup. Eight non-author participants with at least PhD-level computer science research experience each answered ten randomly sampled Ident task questions: five in Focused context and five in Doc-"
        },
        {
            "title": "Preprint",
            "content": "ument context. For each question, participants first answered without context (question + answer options only), then with context. Focused context consisted of cropped images and/or text excerpts; Document context contains links to original PDFs. The survey was implemented via custom web interface (cf. App. E). Table 2: User Study Results. For each context scope, we report Accuracy (%) for both Natural Language (NL) and JSON answer options. Human performance with NL is shown for reference. Model Human InternVL3.5 8BR InternVL3.5 38BR Qwen 2.5 VL 72B Gemini 2.5 ProR Without Context Focused Context Whole Document Context NL 27.5 49.3 53.7 47.8 70.1 JSON 28.4 25.3 38.8 37.3 NL 77.5 76.3 76.3 65.8 81.6 JSON 47.4 71.1 65.8 65.8 NL 65.0 56.8 70.3 43.2 83.8 JSON 35.1 40.5 48.6 37.8 Analysis. As shown in Table 2, top models like Gemini 2.5 Pro exceed humans under Focused Context (81.6% vs. 77.5%) and Whole Document Context (83.8% vs. 65.0%). However, crucial difference emerges in the Without Context condition: LMMs maintain high accuracy (up to 70.1%), whereas human performance drops near chance (27.5%). This indicates that LMMs rely heavily on linguistic regularities that humans cannot exploit. Switching to JSON formatting neutralizes this advantage. Without context, model performance collapses toward human levels (e.g., InternVL3.5 38B drops from 53.7% to 25.3%). With context and JSON-structured answer, LMMs no longer match human NL performance, confirming that linguistic shortcuts inflate perceived model capability. To quantify how much models and human rely on visual evidence versus linguistic priors, we compute the Visual Reliance Ratio R, adapted from the normalized Perceptual Score (Gat et al., 2021): = Accwith context Accwithout context 1 Accwithout context (1) Higher indicates stronger dependence on visual context. Human participants achieve = 69.0%, while the top model (InternVL3.5 8B) achieves = 53.5%, confirming that humans rely more on visual grounding than current LMMs. Probing Linguistic Bias. To confirm this effect generalizes beyond the user study subset, we reevaluated the same four representative LMMs on the full Ident dataset under both Natural Language and JSON formats  (Table 3)  . The same pattern holds: under natural language, models achieve inflated accuracies without context (e.g. 60.3% for Gemini 2.5 Pro) but performance drops toward chance under JSON. Correspondingly, increases under JSON for all models, showing that structured outputs suppress linguistic shortcuts and force models to rely more on visual evidence. Table 3: Impact of answer representation on without-context performance and visual reliance. Accuracy is reported for the Ident task with Focused Context. is computed according to Eq. 1. Model Natural Language JSON Accuracy Accuracy InternVL3.5 8B InternVL3.5 38B Qwen 2.5 VL 72B Gemini 2.5 Pro 43.5 50.4 47.7 60. 27.5 33.1 19.7 51.1 27.1 24.4 34.7 36.6 30.0 41.1 24.7 51.9 Insights. Two key conclusions emerge: (1) MCQs with long-form answers in natural language overstate LMM performance, as models can exploit linguistic regularities imperceptible or irrelevant to humans. (2) Structured JSON representations mitigate this bias, revealing that even the strongest LMMs still fall short of human-level visual grounding and rely on surface cues when available."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We introduce PRISMM-Bench, multimodal benchmark for evaluating LMMs on real-world scientific inconsistencies. We show that even top-performing models struggle with cross-modal reasoning and long-context grounding, while structured answer formats mitigate linguistic shortcuts. This"
        },
        {
            "title": "Preprint",
            "content": "work highlights limitations of LMMS as scientific assistants and motivates future improvements in filtering pipelines, cross-domain datasets, and debiasing strategies for long-form MCQs evaluation. Limitations. Our benchmark is limited in scope: it currently focuses on AI-domain papers from ICLR 2025 and emphasizes rejected submissions to capture unresolved errors. As result, both the scale and domain coverage are restricted. Future work should expand to other fields and venues, and explore inconsistencies that may persist in accepted papers, offering broader and more representative testbed."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work introduces PRISMM-Bench, benchmark for evaluating multimodal large language models (MLLMs) on scientific document understanding. In developing the benchmark, we exclusively use publicly available research papers from ICLR 2025, which are distributed under the Creative Commons Attribution 4.0 (CC-BY 4.0) license. This license explicitly permits redistribution, remixing, and adaptation of the material with proper attribution, and we ensure that all source materials are used in full compliance with these terms. Our study also includes small-scale user study to establish human baseline. All participants were experienced researchers, voluntarily consented to take part, and no personally identifying information was collected or reported. The study design posed no foreseeable risks to participants and did not involve vulnerable populations. We recognize that benchmarks can influence the direction of future model development. While PRISMM-Bench may highlight weaknesses in existing systems, it is not intended to facilitate misuse, such as adversarial attacks on models, but rather to promote more robust and trustworthy scientific document analysis. We release the benchmark with the goal of supporting transparent, reproducible, and ethical research, in line with the ICLR Code of Ethics. No conflicts of interest, sensitive data, or privacy concerns arise in this work."
        },
        {
            "title": "REFERENCES",
            "content": "Open vlm leaderboard. vlm_leaderboard, a. https://huggingface.co/spaces/opencompass/open_ Openreview. https://openreview.net, b. Openreview api v2. https://docs.openreview.net/reference/api-v2/ openapi-definition, c. vllm. https://docs.vllm.ai/en/v0.10.1/. Khalid Alsaif, Aiiad Albeshri, Maher Khemakhem, and Fathy Eassa. Multimodal large language model-based fault detection and diagnosis in context of industry 4.0. Electronics, 13 (24):4912, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. 2025. URL https://arxiv. org/abs/2502.13923. Nishant Balepur, Abhilasha Ravichander, and Rachel Rudinger. Artifacts or abduction: How do llms answer multiple-choice questions without the question?, 2024. URL https://arxiv. org/abs/2402.12483. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract Meaning Representation for sembanking. In Antonio Pareja-Lora, Maria Liakata, and Stefanie Dipper (eds.), Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pp. 178186, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-2322/. Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, and Jonas Geiping. Answer matching outperforms multiple choice for language model evaluation, 2025. URL https: //arxiv.org/abs/2507.02856. Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal, and Ajay Joshi. Leaf-qa: Locate, encode & attend for figure question answering. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 35123521, 2020. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. Hitab: hierarchical table dataset for question answering and natural language generation, 2022. URL https://arxiv.org/abs/2108.06712."
        },
        {
            "title": "Preprint",
            "content": "Nancy A. Chinchor. Overview of MUC-7. In Seventh Message Understanding Conference (MUC7): Proceedings of Conference Held in Fairfax, Virginia, April 29 - May 1, 1998, 1998. URL https://aclanthology.org/M98-1001/. Pavel Chizhov, Mattia Nee, Pierre-Carl Langlais, and Ivan P. Yamshchikov. What the hellaswag? on the validity of common-sense reasoning benchmarks, 2025. URL https://arxiv.org/ abs/2504.07825. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. 2025. URL https://arxiv.org/abs/2507.06261. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. 2023. Dipanjan Das, Desai Chen, Andre F. T. Martins, Nathan Schneider, and Noah A. Smith. Framesemantic parsing. Computational Linguistics, 40(1):956, March 2014. doi: 10.1162/COLI 00163. URL https://aclanthology.org/J14-1002/. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers, 2021. URL https: //arxiv.org/abs/2105.03011. Steffen Eger, Yong Cao, Jennifer DSouza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, and Tristan Miller. Transforming science with large language models: survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation, 2025. URL https://arxiv.org/abs/2502.05151. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:10121031, 2021. Alexander R. Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. Qafacteval: Improved qabased factual consistency evaluation for summarization, 2022. URL https://arxiv.org/ abs/2112.08542. Itai Gat, Idan Schwartz, and Alexander Schwing. Perceptual score: What data modalities does your model perceive?, 2021. URL https://arxiv.org/abs/2110.14375. Mark Gierl, Okan Bulut, Qi Guo, and Xinxin Zhang. Developing, analyzing, and using distractors for multiple-choice tests in education: comprehensive review. Review of Educational Research, 87:0034654317726529, 08 2017. doi: 10.3102/0034654317726529. Jacob Hansen, Wei Lin, Junmo Kang, Muhammad Jehanzeb Mirza, Hongyin Luo, Rogerio Feris, Alan Ritter, James Glass, and Leonid Karlinsky. Instructify: Demystifying metadata to visual instruction tuning data conversion. arXiv preprint arXiv:2505.18115, 2025. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025. Yufang Hou, Alessandra Pascale, Javier Carnerero-Cano, Tigran Tchrakian, Radu Marinescu, Elizabeth Daly, Inkit Padhi, and Prasanna Sattigeri. Wikicontradict: benchmark for evaluating llms on real-world knowledge conflicts from wikipedia. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 109701109747. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/paper_files/paper/2024/file/ URL c63819755591ea972f8570beffca6b1b-Paper-Datasets_and_Benchmarks_ Track.pdf."
        },
        {
            "title": "Preprint",
            "content": "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering, 2019. URL https://arxiv.org/abs/ 1909.06146. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 56485656, 2018. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning, 2018. URL https: //arxiv.org/abs/1710.07300. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. Bioasq-qa: manually curated corpus for biomedical question answering. Scientific Data, 10 (1):170, 2023. Barrett Lattimer, Patrick CHen, Xinyuan Zhang, and Yi Yang. Fast and accurate factual inconsistency detection over long documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 16911703. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.emnlp-main.105. URL http://dx.doi.org/10. 18653/v1/2023.emnlp-main.105. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. Qasa: advanced question answering on scientific articles. In International Conference on Machine Learning, pp. 1903619052. PMLR, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. URL https://arxiv.org/abs/2408.03326. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models, 2023. URL https://arxiv. org/abs/2301.12597. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models, 2024b. URL https://arxiv.org/abs/2403.00231. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. 2025. URL https://arxiv.org/abs/2412.19437. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. 2023b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310. 02255. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2.5 technical report. 2025. URL https://arxiv. org/abs/2508.11737. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:95963 96010, 2024."
        },
        {
            "title": "Preprint",
            "content": "Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, and Alessandra Pascale. Factreasoner: probabilistic approach to long-form factuality assessment for large language models, 2025. URL https://arxiv. org/abs/2502.18573. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. URL https://arxiv.org/abs/2203.10244. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, and Yiming Wang. Perla: Perceptive 3d language assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1436914379, 2025. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over In Proceedings of the ieee/cvf winter conference on applications of computer scientific plots. vision, pp. 15271536, 2020. Mistral. Nemo. https://mistral.ai/news/mistral-nemo/, 2024. Large language model. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, and Kiyoharu Aizawa. Unsolvable problem detection: Evaluating trustworthiness of large multimodal models. 2024. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, et al. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:3549, 2022. OpenAI. Gpt-5. https://chat.openai.com/, 2025. Large language model. Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. Spiqa: dataset for multimodal question answering on scientific papers. Advances in Neural Information Processing Systems, 37:118807118833, 2024. Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. On the systematicity of probing contextualized word representations: The case of hypernymy in bert. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pp. 88102, 2020. Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. Scifibench: Benchmarking large multimodal models for scientific figure interpretation. Advances in Neural Information Processing Systems, 37:1869518728, 2024. Rohit Saxena, Pasquale Minervini, and Frank Keller. Postersum: multimodal benchmark for scientific poster summarization, 2025. URL https://arxiv.org/abs/2502.17540. Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, and Raja Giryes. Livexiv multi-modal live benchmark based on arxiv papers content, 2025. URL https://arxiv. org/abs/2410.10783. Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, and Hongxu Yin. Scaling vision pre-training to 4k resolution, 2025. URL https://arxiv.org/abs/2503.19903."
        },
        {
            "title": "Preprint",
            "content": "Shruti Singh, Nandan Sarkar, and Arman Cohan. Scidqa: deep reading comprehension dataset over scientific papers, 2024. URL https://arxiv.org/abs/2411.05338. Zusheng Tan, Xinyi Zhong, Jing-Yu Ji, Wei Jiang, and Billy Chiu. Enhancing large language models In Proceedings of the 31st for scientific multimodal summarization with multimodal output. International Conference on Computational Linguistics: Industry Track, pp. 263275, 2025. Andreea-Maria Tanasa and Simona-Vasilica Oprea. Rethinking chart understanding using multimodal large language models. Computers, Materials & Continua, 84(2), 2025. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. 2024. URL https://arxiv.org/ abs/2408.00118. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. 2025. URL https://arxiv.org/abs/2503.19786. Alex Turner and Mark Kurzeja. Gaming truthfulqa: Simple heuristics exposed dataset weaknesses, 2025. Adaku Uchendu, Thai Le, and Dongwon Lee. Attribution and obfuscation of neural text authorship: data mining perspective. SIGKDD Explor. Newsl., 25(1):118, July 2023. ISSN 19310145. doi: 10.1145/3606274.3606276. URL https://doi.org/10.1145/3606274. 3606276. Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries, 2020. URL https://arxiv.org/abs/2004.04228. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024a. URL https://arxiv.org/abs/2406.01574. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024b. Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, and Yuyu Luo. Chartinsights: Evaluating multimodal large language models for low-level chart question answering, 2024. URL https://arxiv.org/abs/2405.07001. Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, and Xin Eric Wang. Multimodal inconsistency reasoning (mmir): new benchmark for multimodal reasoning models, 2025. URL https://arxiv.org/abs/2502.16033. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. 2025a. URL https:// arxiv.org/abs/2505.09388. Haiqi Yang, Jinzhe Li, Gengxu Li, Yi Chang, and Yuan Wu. Can large multimodal models actively recognize faulty inputs? systematic evaluation framework of their input scrutiny ability, 2025b. URL https://arxiv.org/abs/2508.04017. Wenhui Yu, Gengshen Wu, and Jungong Han. Deep multimodal-interactive document summarization network and its cross-modal textimage retrieval application for future smart city information management systems. Smart Cities, 8(3):96, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? 2019. URL https://arxiv.org/abs/1905.07830."
        },
        {
            "title": "Preprint",
            "content": "Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output, 2024a. URL https://arxiv. org/abs/2407.03320. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024b. Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, et al. Automated generation of challenging multiple-choice questions for vision language model evaluation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2958029590, 2025. Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 48574866, 2022. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. 2025. URL https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "APPENDIX",
            "content": "In the appendix, we illustrate qualitative examples (App. A), including dataset viewer and examples of text-table and figure-equation inconsistencies. We then provide comprehensive list of assets (App. B), detailing the data sources, licenses, and models used, including both open-source and proprietary ones. The ablations section (App. C) explores the impact of rasterization resolution on model performance and analyzes the effectiveness of Chain-of-Thought (CoT) reasoning with detailed case study. The dataset construction section (App. D.1) explains our refined methodology for sourcing, filtering, and annotating inconsistencies from scientific papers, including discussion of the annotation criteria, custom-built annotation tool, and dataset statistics. Next, we detail the process of generating LLM-based questions (App. D.4) for our benchmark tasks (Inconsistency Identification, Remedy, and Pair-Match), including our debiasing strategies using structured JSON representations. Finally, we provide details on the user study implementation (App. E), the full LLM prompts (App. F) used for various tasks, and screenshots (App. and H) of our annotation and survey applications to provide clear understanding of our methodology."
        },
        {
            "title": "A QUALITATIVE EXAMPLES AND DATASET VIEWER",
            "content": "Alongside the source code, the supplementary materials contain an annotation viewer, which can be used to visually explore out dataset in the web browser. The viewer can be launched by opening the index.html file inside the annotation viewer folder (we recommend using Google Chrome). We furthermore show qualitative examples of texttable inconsistency  (Fig. 3)  and figureequation inconsistency  (Fig. 4)  , together with their corresponding evaluation tasks."
        },
        {
            "title": "B LIST OF ASSETS",
            "content": "Our images and annotations are sourced from publicly available datasets, and we distribute our data in compliance with the licensing terms of the original sources. The document and review data source can be found here: ICLR 2025 on OpenReview (https://openreview.net/group?id=ICLR.cc/2025/Conference): All papers were released under the CC BY 4.0 license. The list of source code and model weights can be found here: Qwen2.5-VL (https://github.com/QwenLM/Qwen2.5-VL): Released under the Apache-2. license. LLaVA-NeXT (https://github.com/LLaVA-VL/LLaVA-NeXT): Released under the Apache-2.0 license. Gemma 3 (https://github.com/google-deepmind/gemma): Released under the Apache-2. license. Ovis 2 (https://github.com/AIDC-AI/Ovis): Released under the Apache-2.0 license. InternVL (https://github.com/OpenGVLab/InternVL): Released under the MIT license. GLM-V (https://github.com/zai-org/GLM-V): Released under the Apache-2.0 license. Mistral NeMo (https://github.com/mistralai/mistral-inference: Released under Apache-2.0 license vLLM (https://github.com/vllm-project/vllm): Released under the Apache-2.0 license. MinerU (https://github.com/opendatalab/MinerU): Released under the AGPL-3.0 license. The list of proprietary models used can be found here: Google Gemini (https://deepmind.google/models/gemini/flash/): Used in version Gemini Flash 2.5 and Gemini Pro 2.5, released on June 17, 2025."
        },
        {
            "title": "Preprint",
            "content": "OpenAI GPT (https://github.com/LLaVA-VL/LLaVA-NeXT): Used in version GPT-5, released on August 7, 2025."
        },
        {
            "title": "C ABLATIONS",
            "content": "C."
        },
        {
            "title": "IMPACT OF RASTERIZATION RESOLUTION",
            "content": "Scientific papers contain dense text and fine-grained visual elements such as axis labels, annotations, and subscripts, which are often crucial for detecting subtle inconsistencies. To test whether rasterization resolution impacts detection performance, we varied the DPI used to extract images from the PDF and evaluated representative set of strongest proprietary and open-weight models of different sizes on the Inconsistency Ident task with Focused Context, keeping all other settings fixed. Table 4: Accuracy of LMMs under different rasterization resolutions. Results are reported for the Ident task with Focused Context. Percentage change is calculated relative to the 144 DPI baseline. DPI 72 144 300 VILA HD 4K 8B InternVL3.5 8B InternVL3.5 38B Ovis2 34B Gemini 2.5 Pro Default % Change Default % Change Default % Change Default % Change Default % Change 31.3 30.9 29.4 +1.3 -4. 42.7 48.9 45.4 -12.7 -7.2 45.8 57.3 51.9 -20.1 -9.4 43.1 49.6 50.4 -13.1 +1. 67.2 69.5 70.6 -3.3 +1.6 Low resolutions harm performance. Most models showed significant drops at 72 DPI, up to - 20.1% for InternVL3.5 38B. Open-weight models were generally more vulnerable, though even Gemini 2.5 Pro declined by -3.3%. Suprisingly, VILA H, despite being trained for high-resolution inputs, showed slight accuracy gain at this lower setting. Higher resolutions do not always improve accuracy. Increasing from 144 to 300 DPI yielded mixed outcomes. While Gemini 2.5 Pro and Ovis2 34B benefited slightly, InternVL models performed worse, and VILA HD again failed to leverage the higher fidelity despite its specialized training. This suggests that additional detail can sometimes overwhelm global reasoning or misalign with training distributions. Resolution sensitivity is architecture-specific. Overall, the assumption that higher resolution improves inconsistency detection does not hold universally. Performance varies with model design and pretraining data, and high-resolution training does not guarantee an edge in handling scientific inconsistencies. Careful DPI control is crtical for fair evaluation. In our benchmark, 144 DPI provides practical balance of visual clarity, computational cost, and cross-model comparability. C.2 CHAIN-OF-THOUGHT REASONING Reasoning variants perform better than non-reasoning counterparts and reach results comparable to much larger models. For instance, InternVL3.5 8B achieves an average of 37.6%, rivaling large open-weight models and surpassing several 72B non-reasoning models. We therefore study how reasoning-enabled models leverage chain-of-thought (CoT) to improve performance on PRISMMBench. To do so, we re-evaluated selection of reasoning models on the Ident task with Focused Context with reasoning turned off and compared the performance. Disabling reasoning reveals the critical role of CoT in detecting subtle inconsistencies. For example, GLM 4.5V drops from 55.7% to 46.6% (-16.3%), InternVL3.5 8B from 48.9% to 40.0% (-18.2%), and InternVL3.5 38B suffers the largest decline, from 57.4% to 37.8% (-34.0%). Why Reasoning Helps. To understand these gains, we focused on inconsistencies where InternVL3.5 38B succeeded with reasoning but failed without. We identified three consistent patterns: (1) Structured input handling. Reasoning-enabled models interpret the JSON-formatted options in natural language, clarifying subtle distinctions without exploring linguistic biases (cf. low without-context performance for reasoning models in Sec. 4.3). (2) Cross-modal grounding. CoT traces show models explicitly reasoning over both text and visuals, breaking complex information into smaller units and reusing them later in the reasoning chain. (3) Concept linking. Reason-"
        },
        {
            "title": "Preprint",
            "content": "ing enables models to connect fine-grained context with domain knowledge and abstract concepts, allowing stronger logical inference beyond surface pattern recognition. Case Study. Fig. 5 illustrates this effect with the Unique Successful Jailbreaks metric, which must be non-negative. The figure, however, contains error bars extending below zeros, resulting in an inconsistency. Figure 5: Inconsistency example for case study. Right: Visual context. Left: Question and answer options for Ident task. Natural language options used for ease-of-comprehension, LMM was tasked using JSON. Without reasoning, InternVL3.5 38B selected the distractor x-axis labels overlap, (option A) justifying it with generic but factually incorrect critique, as the labels were perfectly legible. The model defaulted to template-like response rather than verifying claims against visual evidence. In contrast, the reasoning-enabled model produced systematic chain-of-thought: (1) ruling out label overlap (by observing the labels were spaced out and readable), (2) confirming the y-axis range was sufficient (noting all data was within the 0-20 range), (3) dismissing legend critiques (since legend isnt necessary when bars are directly labeled), and (4) crucially, identifying the logical error of error bars that shouldnt go below zero if the metric [...] cant be negative). This stepwise elimination and domain-aware inference led to the correct answer. The full reasoning chain of InternVL3.5 38B is available in Fig. 6. This case highlights two key strengths of reasoning: (1) systematic elimination of distractors, and (2) integration of domain knowledge (e.g. non-negativity of counts) with visual grounding. Although reasoning increase output length (average of 473 tokens per query in our case), it substantially improves multimodal consistency and robustness, making CoT key mechanism for handling subtle scientific document inconsistencies."
        },
        {
            "title": "D DATASET CONSTRUCTION",
            "content": "D.1 REVIEW SOURCING Initial Exploration with Regex Matching. Before finalizing the review sourcing strategy described in the main paper, we conducted an exploratory study to detect potential inconsistencies mentioned in reviews for ICLR 2024 using simple regular expression (regex) approach. Reviews were accessed via the OpenReview API2, focusing on the Weaknesses and Questions sections, which were most likely to contain critical feedback. Each sentence was parsed for co-occurrence of terms related to inconsistencies (e.g., mismatch, conflict) and references to visual elements (e.g., figure, table, equation). The pseudocode for this procedure is shown below: Manual inspection confirmed that reviews indeed contained valuable references to visual-textual mismatches. However, two limitations emerged: (1) regex captured only strict keyword formula2https://docs.openreview.net/reference/api-v"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Pseudocode for regex matching 1: DEFINEPATTERN(inconsistency pattern, r(inconsisten mismatch doesn[]t match not match conflict discrepanc)) 2: DEFINEPATTERN(visual pattern, r(figure fig.? table graph plot image diagram equation)) sections EXTRACTSECTIONS(review) for each section in sections do 3: results [ ] 4: for each review in reviews do 5: 6: 7: 8: 9: sentences SPLITINTOSENTENCES(section) for each sentence in sentences do if MATCHES(sentence, inconsistency pattern) and MATCHES(sentence, visual pattern) then APPEND(results, sentence) end if 10: 11: 12: 13: 14: end for 15: return results end for end for tions, missing paraphrased or indirect mentions of inconsistencies, and (2) many inconsistencies referenced papers that had been updated after rebuttal, making it impossible to locate the original errors in the PDF versions available through OpenReview. Refined Collection Strategy. To address these issues, we refined our strategy in two ways: 1. Conference selection: We shifted to the more recent ICLR 2025, focusing on papers without author rebuttals. This ensured that flagged inconsistencies were more likely to remain in the available PDFs. 2. LLM filtering: Instead of regex, we employed Mistral Nemo 2407 at low temperature to summarize reviews and extract candidate inconsistency statements. This approach captured non-strict formulations (e.g., does not align with instead of mismatch) and produced structured outputs, making them easier to present to annotators in the verification interface. This refinement reduced our initial pool of 75,550 ICLR 2025 reviews to 12,366 reviews. The LLM outputs were stored in structured JSON format, with each paper ID associated with list of flagged inconsistencies. Example Output. Throughout the appendix, we are going to illustrate our data preparation pipeline use the paper ID vXSCD3ToCS3 as an example. We illustrate the example out after the LLMassisted filtering in Fig. 7. This structured representation provided natural starting point for the subsequent manual verification stage described in App. D.2. D.2 ANNOTATION PROCESS Annotator Background. The annotation was conducted by the first author, who has an advanced background in Computer Science and Machine Learning. consistent annotation standard was maintained throughout the project; any ambiguous or borderline cases were discussed with senior researchers until consensus was reached. Annotation Criteria. During manual verification, the annotator judged each reviewer comment against the following criteria: 1. The comment reflects objective feedback rather than subjective suggestion. 3https://openreview.net/forum?id=vXSCD3ToCS"
        },
        {
            "title": "Preprint",
            "content": "2. The comment describes an inconsistency involving two contradicting facts. 3. Both conflicting parts can be located in the PDF. 4. The inconsistency can be identified without deep domain-specific expertise (focus on visual/document-level inconsistencies). 5. The inconsistency is significant and factual, not minor typo or stylistic choice. Annotation Interface. We implemented custom web-based tool in Next.js. The interface displayed the reviewers comment (extracted by the LLM) alongside the corresponding paper embedded as PDF viewer (compare Fig. 18, Fig. 19 for screenshots of the apps interface). The annotator could: Read the reviewers comment and decide whether it fulfilled criteria (1) and (2). If not, the instance was skipped. Search and inspect the relevant region of the embedded PDF. Toggle between one-part and two-part annotation modes: One-part: single element (e.g., figure-caption inconsistency). Two-part: Two separate elements (e.g., figure vs. text, or figure vs. figure). Specify for each part whether it was textual or visual: Visual: Select the page, then draw bounding box on rendered thumbnail version the PDF page. Textual: Enter the page and line number, and copy the relevant text snippet from the PDF. Assign an inconsistency category via drop-down menu. Provide short free-text description of the inconsistency in their own words. Recorded Metadata. Each annotation combined automatically and manually collected fields: Automatically recorded: element type (text or image), bounding box (relative coordinates) for visual selections, internal image identifier, reviewers original comment. Manually entered: page and line numbers, copied textual content, inconsistency category, and short description by the annotator. Example Output. Annotations were stored in JSON format, combining visual/textual parts, reviewer comment, category, and description. We illustrate the example annotation output in JSON format in Fig. 8. D.3 STATISTICS OF INCONSISTENCY COLLECTION. The annotation resulted in 262 inconsistencies from 242 ICLR papers. The average page count of each PDF was 18 pages. total of 19 papers (7.9%) had more than one inconsistency. The paper subjects were equally distributed across the range of topics for ICLR 20254, with (1) representation learning (14.5%), (2) transfer learning (11.2%), (3) datasets and benchmarks (10.8%) and generative models (9.5%) being the most frequent topics. We identified 13 categories of inconsistencies based on the elements involved, with the distribution shown in Fig. 9. The most common cases were figuretext mismatches and intra-figure inconsistencies. D.4 LLM-BASED QUESTION GENERATION D.4.1 INCONSISTENCY IDENTIFICATION (IDENT) The Inconsistency Identification (Ident) task was the first benchmark task we designed. For each annotated inconsistency, we instructed Gemini Flash to generate multiple-choice question (MCQ) with four options, one of which correctly describes the inconsistency. 4https://iclr.cc/Conferences/2025/CallForPapers"
        },
        {
            "title": "Preprint",
            "content": "Inputs. As input to the model, we provided: The annotated context (visual and/or textual parts). The annotators free-text description of the inconsistency. Prompt. After experimenting with several formulations, we found that minimalist prompt yielded the most creative and plausible distractors. We provide the final version of the prompt in Fig. 10. Output Format. The model produced structured output containing the question, the correct answer, and three distractor answers. Manual Verification. Each generated question underwent manual verification: Correct answer: must (1) faithfully reflect the annotators description and (2) directly connect to the annotated context. Distractors: must (1) be grounded in the annotated inconsistency parts, (2) avoid obvious contradictions within the answer itself, (3) only mention elements present in the provided context, and (4) describe an inconsistency rather than confirming correct fact from the paper We also post-processed the text to remove stylistic artifacts often appended by the LLM, e.g. the parenthetical , indicating an inconsistency. Example Output. We illustrate an example of the generated multiple-choice question for inconsistency identification task in Fig. 11. D.4.2 DEBIASING THE INCONSISTENCY IDENTIFICATION TASK Initial Observations. When first evaluating the Ident task with Gemini 2.5 Flash, we observed unexpectedly high accuracy: 84.4% with the original LLM-generated questions. (e.g.: What inconsistency is observed between Figure 2 and the accompanying text regarding the generated road network?) 79.4% after replacing the LLM-generated question with the generic formulation: What is the inconsistency in these parts of scientific paper? Even in sanity check where the model was shown only the question and answer options (without the annotated context), performance remained at 57.6% accuracy, far above the random baseline of 25%. This indicated strong reliance on linguistic cues in the answer phrasing. Mitigation Strategies. Moving forward, we solely employed the generic question formulation throughout all inconsistencies. For reducing the without context accuracy Accnc, we systematically explored ways of reducing linguistic priors by rewriting the answer options: Normalizing answer length: Accnc = 48.1%. Filtering for MCQs where the correct answer is shortest: Accnc = 46.2%. Rephrasing distractors according to best practices in MCQ test design (Gierl et al., 2017): Accnc = 41.6%. Shortening all answer options into nominal style: Accnc = 38.2%. While these interventions reduced bias, they did not remove it completely. Structured Representation: EvidenceClaim JSON. As more robust solution, we abandoned free-form natural language and introduced structured, human-readable JSON representation that removes stylistic cues while preserving the semantic contradiction. The schema is: { \"letter\": \"A\" \"B\" \"C\" \"D\", \"attribute\": str,"
        },
        {
            "title": "Preprint",
            "content": "\"claim\": { \"source\": \"expectation\" str, \"statement\": str }, \"evidence\": { \"source\": str, \"statement\": str } } Patterns. Two patterns of contradiction are covered: Claim vs. Evidence: claim from one paper element is contradicted by evidence from another. Expectation vs. Evidence: claim contradicts common expectations of scientific correctness. In this case, the claims source is always expectation We prompted Gemini 2.5 Flash to convert the natural language MCQs into this structured format. The full prompt can be inspected in App. F.2. 20% subset was manually validated for consistency. Effect on Model Behavior. This representation further reduced the no-context accuracy to 34.0%. Given the full context, accuracy on the new JSON format decreased from 79.4% to 69.5%. However, the fraction of performance attributable to visual grounding (Eq. 1) increased from 51.4% to 53.8%. Thus, the structured format acts as regularizer, forcing models to rely more strongly on the provided paper context. Example Output. For the running example, we illustrate the example of debiased output in the evidence-claim JSON format for the inconsistency identification task in Fig. 12. D.4.3 INCONSISTENCY REMEDY TASK Task Design. The Inconsistency Remedy (Remedy) task extends beyond identifying an inconsistency to determining how it can be resolved. To avoid linguistic artifacts, we directly employed structured representation in JSON format. This representation adapts the EvidenceClaim schema to more action-oriented form, the TargetAction JSON: { } \"letter\": \"A\" \"B\" \"C\" \"D\", \"attribute\": str, \"target\": str, \"other_involved\": str, \"action\": \"modify\" \"remove\" \"add\" \"reposition\" \"replace\", \"edit_statement\": str, \"reason\": str Here, attribute captures the element at target specifies where the change is applied, other involved records additional parts if necessary, and the fields action, edit statement, and reason summarize the correction. issue, LLM Conversion Process. We found that prompting an LLM to directly convert the natural language MCQs from the Ident task into TargetAction JSON yielded the most reliable results in terms of readability and correctness. The prompt is depicted in Sec. F.3. Example Output. Four our example used throughout this appendix, the task looks as follows: We illustrate the example output in Target-Action JSON format for the inconsistency remedy task in Fig. 13."
        },
        {
            "title": "Preprint",
            "content": "D.4.4 INCONSISTENCY PAIR-MATCH TASK Task Design. The Inconsistency Pair-Match (Match) task focuses on the subset of inconsistencies that involve two distinct visual parts. The model is presented with one element (text or visual) as the question context and must identify the corresponding inconsistent visual element among four options. Filtering of Eligible Cases. Not all inconsistency categories are suitable for pair matching. Categories where the contradiction is contained entirely within single element (i.e., figurecaption, figure-only, table-only, tablecaption, algorithm-only) were excluded. This filtering left 135 out of the 262 inconsistencies in the dataset. Distractor Construction. To ensure challenging and fair distractors, we extracted all figures, tables, and equations from the 242 papers in our dataset using MinerU5, which produced image crops with unique IDs, modality labels, and page numbers. We then implemented python script to sample distractors as follows: Distractors were restricted to the same modality as the correct answer. Preference was given to elements appearing on the same page or on adjacent pages to the correct element, so that distractors were topically similar. Sampling was done within the same paper. Each paper contained enough visual elements of the same modality so we didnt have to fallback to using elements from other papers. This procedure reduced trivial elimination strategies (e.g., selecting the only figure among tables) and forced models to consider fine-grained inconsistencies. Example Output. In the running example, the annotated inconsistency links an in-line text with figure. The text is fixed as the question context, and the answer options are image IDs referring to extracted figures, with one image ID being the correct image cropped in the annotations. We show the example of output used in the inconsistency pair-match task in Fig. 14. USER STUDY IMPLEMENTATION & STATISTICS Setup. We conducted the user study in online form, using custom web app. The participants were greeted with an onboarding screen, where they entered the following information to assess their eligibility to be included in the user study: (1) email address, (2) academic field, (3) academic level and(4) AI exposure. Afterwards, they were shown instructions for the survey and an introduction into the question formats and different context modalities. For each participants, ten tasks were randomly sampled from our dataset. For the first five tasks, the participants were shown the Focused Context with the exact cropped images and/or text passages from the paper. For the last five tasks, the participants were instructed to open link to the original PDF and use the whole document to answer the question. In this case, they were provided with the visual element they should focus on in the paper. Screenshots of the user interface are provided in appendix H. Upon submission, the following datapoints were saved automatically: (1) The task ID, (2) The chosen answer by the participants, (3) whether the task was correctly answered with/without context, (4) whether the question was accompanied by Focused Context or Full Document Context and (5) the time it took the participants to answer each question. Statistics. Our eight participants all have background in either artificial intelligence, computer science or mathematics at an academic level of PhD or higher (7 PhD, 1 postdoc). All stated to exhibit advanced exposure levels to AI, which we defined as being comfortable with reading, interpreting and critically evaluating AI scientific literature. The median answer time for questions without provided visual context was 45s, with Focused Context 145s and with Whole Document Context 169s. In 65% of the cases, participants changed their answers once provided with the context. In total, participants processed 80 inconsistencies. 5https://github.com/opendatalab/MinerU"
        },
        {
            "title": "F LLM PROMPTS",
            "content": "Here we provide the full prompts used to instruct the LLMs. F.1 LLM PROMPT FOR REVIEW FILTERING We provide the prompt for LLM-based review filtering in Fig. 15. Given the reviewers comment, the model is instructed in chain-of-thought prompting manner to systematically analyze each desired characteristic of visual inconsistency. Few-shot examples help clarify the output format. F.2 LLM PROMPT FOR CONVERTING INTO EVIDENCE-CLAIM FORMAT We provide the prompt for LLM-assisted conversion of natural language answers (for the inconsistency identification task) into evidence-claim JSON format in Fig. 16. The evidence-claim JSON format is used as answer options in the inconsistency identification task. The structured JSON-based answer representation is for mitigating the language biases in multiple-choice evaluation. F.3 LLM PROMPT FOR CONVERTING INTO TARGET-ACTION FORMAT We provide the prompt for LLM-assisted conversion of natural language answers (for the inconsistency identification task) into target-action JSON format in Fig. 17. Based on the question-answer pairs in inconsistency identification task, we generate question with answers for the inconsistency remedy task. The target-action JSON format is used as answer options in the inconsistency remedy task."
        },
        {
            "title": "G SCREENSHOTS OF THE ANNOTATION APP",
            "content": "We show some examples of the interface of the annotation tool in Fig. 18 and Fig. 19."
        },
        {
            "title": "H SCREENSHOTS OF THE SURVEY APP",
            "content": "We show some examples of the interface of the survey web interface in Fig. 20, Fig. 21 and Fig. 22."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: qualitative example of texttable inconsistency and its corresponding evaluation tasks of Ident, Remedy and Match."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: qualitative example of figure-equation inconsistency and its corresponding evaluation tasks of Ident, Remedy and Match."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Raw reasoning output by InternVL3.5 38B on 3MDmM0rMPQ. Figure 7: Example output after the LLM-assisted filtering."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Example annotation output in JSON format. Figure 9: Distribution of inconsistency types. We identified 13 categories of inconsistencies based on the elements involved. The most common cases are figure-text mismatches and intra-figure (figure-only) inconsistencies Figure 10: Prompt for preparing multiple-choice questions in the inconsistency identification task."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Example of generated multiple-choice question for the inconsistency identification task. Figure 12: Example of debiased output in evidence-claim JSON format for the inconsistency identification task."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Example of output in target-action JSON format for the inconsistency remedy task, directly converted from the natural language MCQs from the inconsistency identification task. Figure 14: Example output for the inconsistency pair-match task."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Prompt for LLM-based review filtering."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Prompt for LLM-assisted conversion of natural language answers of the inconsistency identification task into evidence-claim JSON format. The evidence-claim JSON format is used as answer options in the inconsistency identification task."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Prompt for LLM-assisted conversion of natural language answers of the inconsistency identification task into target-action JSON format. The target-action JSON is used as answer options in the inconsistency remedy task."
        },
        {
            "title": "Preprint",
            "content": "Figure 18: First part of annotation app showing an overview over the annotation progress and embedded original PDF file."
        },
        {
            "title": "Preprint",
            "content": "Figure 19: Second part of annotation app for drawing bounding boxes, entering text and further details about the inconsistency."
        },
        {
            "title": "Preprint",
            "content": "Figure 20: First part of survey interface showing question with no context provided."
        },
        {
            "title": "Preprint",
            "content": "Figure 21: Second part of survey interface showing question with Focused Context."
        },
        {
            "title": "Preprint",
            "content": "Figure 22: Third part of survey interface showing question with Full Document Context."
        }
    ],
    "affiliations": [
        "Interdisciplinary Transformation University Austria",
        "Johannes Kepler University Linz",
        "MIT CSAIL",
        "MIT-IBM Watson AI Lab",
        "Stanford University"
    ]
}