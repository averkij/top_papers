{
    "paper_title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction",
    "authors": [
        "Zilin Xiao",
        "Qi Ma",
        "Mengting Gu",
        "Chun-cheng Jason Chen",
        "Xintao Chen",
        "Vicente Ordonez",
        "Vijai Mohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 9 0 8 1 . 9 0 5 2 : r MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction Zilin Xiao1,2,, Qi Ma1, Mengting Gu1, Jason Chen1, Xintao Chen1, Vicente Ordonez2, Vijai Mohan1 1Meta Superintelligence Labs, 2Rice University Work done at Meta Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka MultiVector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As result, we enable test-time scaling in multimodal retrieval where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters. Date: September 23, 2025 Correspondence: Zilin at zilin@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal embedding models play an essential role in image search (Gordo et al., 2016), visual question answering (Hu et al., 2018; Zheng et al., 2021) and visual document retrieval (Faysse et al., 2025), where models project heterogeneous inputs, such as images and texts, into unified representation space. While existing methods, including CLIP (Radford et al., 2021), BLIP (Li et al., 2022) and SigLIP (Zhai et al., 2023) have demonstrated superior performance in cross-modal retrieval, their performance remains limited in scenarios where the inputs involve complex and diverse instructions. Thanks to recent advances in building embeddings through foundation vision-language models (VLMs), one could apply contrastive learning on the extracted embedding from the hidden states of the last layer of VLM to learn meaningful multimodal representations while retaining pre-trained knowledge. Despite growing progress in multimodal embedding VLMs, the common practice of condensing the entire query and candidate into single vector is not an optimal choice, as fine-grained details are lost between modalities (Yao et al., 2022; Thrush et al., 2022) and this process has theoretical limitations (Weller et al., 2025). In text retrieval, ColBERT (Khattab and Zaharia, 2020) introduced multi-vector late interaction mechanism that retains multiple token-level embeddings and uses lightweight scoring between query and document token representations. This approach preserves significantly more contextual information than single-vector methods while still allowing independent encoding of queries and documents, and has motivated recent trend of devising multi-vector embeddings for multimodal retrieval (Faysse et al., 2025; Xu et al., 2025; Günther et al., 2025). Despite their effectiveness, multi-vector methods incur substantial efficiency costs in terms of index size, retrieval latency and feasibility. In these methods, each image is encoded into hundreds of patch embeddings, and each query text into several token embeddings, all of which must be stored and compared during retrieval. This results in large index sizes and slower retrieval processing. Moreover, multimodal-to-multimodal retrieval becomes impractical where both query and candidate sides have images, 1 Figure 1 Different multimodal retrieval frameworks at training-time. Upper Left: Single vector retrieval method computes score for each pair of query and candidate and uses contrastive objective to maximize the score for corresponding pairs. Upper Right: In multi-vector retrieval, scores are computed via maximum similarity across vector pairs and aggregated before applying the contrastive objective. Lower: MetaEmbed structures query and candidate vectors into hierarchical nested groups and trains coarse-to-fine multi-vector embeddings that enable scalable retrieval with flexible test-time control over embedding granularity. as similarity computation for each pair of query and candidate involves interactions between thousands of query tokens and thousands of candidate tokens, making both training and inference prohibitive due to computational demands. In this work, we propose MetaEmbed as scalable late-interaction training recipe that advances multimodal retrieval with flexible multi-vector method, illustrated in Figure 1. Instead of encoding the query and candidate into one vector, we introduce small number of learnable Meta Tokens appended to the input sequence of the query and candidate. Their last-layer hidden states serve as set of contextualized representations for late interaction, namely Meta Embeddings. To enable flexible late interaction, where users can trade off retrieval accuracy against computational budget and retrieval latency, we draw inspiration from Matryoshka Representation Learning (Kusupati et al., 2022) and design the Matryoshka Multi-Vector Retrieval (MMR) module in MetaEmbed. By performing contrastive learning across parallel nested groups of representations at training-time, the model learns coarse-to-fine multi-vector embeddings that can be selectively utilized for late interactions depending on the computation budget at test-time. Increasing the number of Meta Embeddings used at indexing improves the retrieval quality at the cost of index storage budget and retrieval latency, thus enabling test-time scaling in multimodal retrieval. We first validate MetaEmbed on the Massive Multimodal Embedding Benchmark (MMEB) (Jiang et al., 2024) and Visual Document Retrieval Benchmarks (ViDoRe) v2 (Faysse et al., 2025; Macé et al., 2025), which represent comprehensive suite of retrieval tests covering images, text and visual documents. Our experiments show that MetaEmbed achieves state-of-the-art retrieval performance across diverse scenarios. To further examine its generality and training scalability, we evaluate MetaEmbed with different VLM architectures and model sizes, including decoder-only models such as Qwen2.5-VL (Bai et al., 2025) and PaliGemma (Beyer et al., 2024), as well as cross-attention-based models such as Llama-3.2-Vision (Grattafiori et al., 2024). Notably, test-time scalability remains effective even at the 32B scale, showing minimal diminished returns with larger models. We hope MetaEmbed charts path toward multimodal retrieval systems that are both accurate and deployable at scale, advancing the pursuit of generality, efficiency, and flexibility."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Embedding. These methods aim to project heterogeneous inputs, such as text and images, into shared representation space for cross-modal understanding and retrieval (e.g Frome et al. (2013); Kiros et al. (2014); Faghri et al. (2018)). More recent large scale models such as CLIP (Radford et al., 2021), MetaCLIP (Xu et al., 2023; Chuang et al., 2025), BLIP (Li et al., 2022) and SigLIP (Zhai et al., 2023) encode each modality independently and apply contrastive training to enforce cross-modal alignment. More recent methods are built upon stronger VLMs (Xiao et al., 2024; Kong et al., 2025; Qin et al., 2025; Ju and Lee, 2025; Lin et al., 2025). For instance, VLM2Vec (Jiang et al., 2025) adapts Phi-3.5-V (Abdin et al., 2024), VLM2Vec-V2 (Meng et al., 2025) and GME (Zhang et al., 2024b) builds on Qwen2 (Wang et al., 2024) and LLaVE (Lan et al., 2025) finetunes on LLaVA (Li et al., 2024). Beyond architectural choices, the community has also explored innovative strategies in data construction and training. MegaPairs (Zhou et al., 2024) and mmE5 (Chen et al., 2025b) curate large-scale synthetic data with sophisticated pipelines to support contrastive training. UniME (Gu et al., 2025) achieves strong results through diverse data combined with teacher model distillation. B3 (Thirukovalluru et al., 2025b) incorporates novel insights into batch mining techniques. MoCa (Chen et al., 2025a) used continual pre-training to produce bidirectional embeddings. Nevertheless, many existing multimodal retrieval methods predominantly rely on single-vector retrieval, which hinders further scaling as embedding size becomes bottleneck. Multi-Vector Retrieval. Multi-vector retrieval refers to family of dense retrieval methods that represent queries and documents with multiple embeddings rather than single vector (Tolias et al., 2016; Tan et al., 2019; Ren et al., 2017), with ColBERT (Khattab and Zaharia, 2020) being successful recent example of this paradigm by introducing late interaction framework. While many variants have been proposed to improve multi-vector retrieval efficiency through approximation (Lee et al., 2023; Engels et al., 2023; Jayaram et al., 2024) and compression (Santhanam et al., 2022b,a; Li et al., 2023), naive ColBERT-style methods such as ColPali, ColQwen (Faysse et al., 2025) and others (Günther et al., 2025; Xu et al., 2025) still remain dominant in the context of text-image retrieval. However, these approaches do not support multimodal-to-multimodal retrieval, since introducing hundreds of image tokens on the query side renders both training and inference computationally prohibitive, highlighting the need for our proposed MetaEmbed framework. Matryoshka Representation Learning. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) was introduced to encode features at multiple granularities within single vector in nested structure. Popular text-only single-vector retrieval models (Zhang et al., 2025; Günther et al., 2025) natively support MRL, enabling retrieval to dynamically select the first few dimensions according to the available computational budget. Although prior work (Cai et al., 2025) has applied Matryoshka methods for token budgeting in VLM generation, to the best of our knowledge, MetaEmbed presents the first work that leverages such framework for multi-vector retrieval and achieves successful test-time scaling."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first revisit the definition of multimodal retrieval and how late interaction works to utilize multiple vectors for retrieval. Then we introduce the MetaEmbed recipe, its model architecture, and how it enables test-time scaling in multimodal retrieval."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Problem Definition. Multimodal retrieval consists of retrieving relevant content across different modalities, where the query can be text qt, an image qi or combination of both (qt, qi). And the retrieval candidates can likewise be of any modality or multimodal combination. Given query and set of candidates {c1, c2, . . . , cN }, multimodal retrieval model typically defines similarity function s(q, c) to measure the relevance between and candidate c. The retrieved top-1 prediction is then determined by: = arg max s(q, c). c{c1,...,cN } (1) Late Interaction. For query and candidate c, let their multi-vector representations be denoted as Eq RNqD and Ed RNdD, where is the embedding dimension, and Nq, Nd are the number of tokenlevel vectors for the query and the candidate, respectively. The late interaction operator LI(q, d) captures the most informative alignment by selecting, for each query vector E(i) , its maximum similarity (dot product) with the document vectors E(j) , and summing across all query vectors: LI(q, d) = Nq (cid:88) i=1 max j[1,Nd] (cid:68) E(i) , E(j) (cid:69) . (2)"
        },
        {
            "title": "3.2 Our Design",
            "content": "MetaEmbed Recipe. MetaEmbed is designed as scalable late-interaction retrieval model that introduces small number of learnable Meta Tokens appended to the input sequence of both queries and candidates. These Meta Tokens are processed jointly with the original input by an underlying Vision-Language Model (VLM), and their final hidden states serve as Meta Embeddings. Unlike patchor token-level embeddings, Meta Embeddings provide set of compressed yet expressive vectors that capture fine-grained semantics through contextualization. This design drastically reduces the number of vectors required for retrieval while maintaining retrieval quality. Formally, let VLM with parameters θ define conditional probability distribution pθ (y x, I) where = [x1, . . . , xn] is the query or document text prompt and are associated input images. MetaEmbed augments the input with learnable, input-specific Meta Tokens: queries use Mq RRqD, and candidates use Mc RRcD. For an input (x, I), the transformer consumes z(0) = (cid:2) ; ; Mq ; Mc (cid:3) R(P +n+Rq+Rc)D, where and are tokenized visual and text inputs. The model produces last-layer hidden states = Fθ(z(0)) R(P +n+Rq+Rc)D, where Fθ denotes the transformer network parameterized by θ. We extract the final hidden states at the Meta Tokens positions to obtain query-side embeddings E(q) meta RRqD or candidate-side embeddings E(c) meta and E(c) meta constitutes compact, contextualized multi-vector representation produced in two separate forward passes of the VLM. meta RRcD, followed by L2 normalization. Each E(q) Matryoshka Multi-Vector Retrieval (MMR). With E(q) meta RRqD and E(c) late-interaction score between query and candidate as follows: meta RRcD available, we can compute s(q, c) = Rq (cid:88) i=1 max j[1,Rc] (cid:68) E(i) , E(j) (cid:69) . (3) While effective, using all vectors for every instance is not flexible: the index size scales as O(N Rc D) for candidates, and the scoring cost scales as O(Rq Rc D) per pair. We therefore seek mechanism that maintains strong retrieval quality under tight resources and scales to higher accuracy as more compute is allocated. Inspired by Kusupati et al. (2022) , we impose prefix-nested structure on Meta Embeddings so that the first few vectors form coarse summary, and additional vectors refine the representation. Concretely, fix group sizes for queries so that 1 r(1) < r(2) < < r(G) = Rq, and for candidates so that 1 r(1) < r(2) < < r(G) = Rc. For any input, define the g-th group of query embeddings as E(q,g) = E(q) candidates E(c,g) = E(c) , :]. We then compute group-specific late-interaction scores meta[1:r(g) meta[1:r(g) , :], and similarly for s(g)(q, c) = r(g) (cid:88) i=1 max j[1,r(g) ] (cid:68) E(g,i) , E(g,j) (cid:69) . (4) During training, we optimize contrastive objectives across all groups in parallel, encouraging each prefix to be discriminative on its own while remaining consistent with larger prefixes. 4 Figure 2 Illustration of test-time scaling with varying retrieval budgets. Left: MetaEmbed constructs nested multi-vector index that can be retrieved flexibly given different budgets. Middle: How the scoring latency grows with respect to the index size. Scoring latency is reported with 100,000 candidates per query on an A100 GPU. See 5 for full efficiency analysis. Right: MetaEmbed-7B performance curve with different retrieval budgets. See Figure 3 (b) for full metrics. Training Objective. Let = (q(b), c(b), c(b,)) b=1 be minibatch where each query has corresponding positive candidate c(b) and one additional hard negative c(b,). For each group g, we define the similarity scores between query and candidate as follows: S(g) u,v = 1 τ s(g)(cid:0)q(u), c(v)(cid:1), (5) with τ > 0 as temperature hyper-parameter. For query u, the denominator of the softmax spans (i) all in-batch candidates {c(1), . . . , c(B)} and (ii) its explicit hard negative c(u,). The InfoNCE loss (Oord et al., 2018) for group is: L(g) NCE ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) u=1 log exp(S(g) u,u) + (cid:80) v=u exp(S(g) τ s(g)(q(u), c(u,))) exp(S(g) u,u) u,v) + exp( 1 . (6) The final loss combines all groups with group-specific hyper-parameters wg as importance scales: Lfinal = (cid:88) g=1 wg L(g) NCE. (7) , r(g) Test-time Scaling. The nested design yields simple accuracy-efficiency knob, as illustrated in Figure 2. At indexing time, one may store only the first r(g) vectors for each candidate. At query time, the system selects (r(g) ) based on latency constraints and computes s(g)(q, c) for scoring. Coarse prefixes (g small) are ideal for fast retrieval scoring, while larger prefixes (g large) improve precision at the expense of additional compute. Because those groups are optimized in parallel, we can seamlessly adjust the retrieval granularity and budget without retraining the system by selecting different group size at test-time. In later sections, we refer to the selected combination of group sizes (r(g) ) as the retrieval budget. , r(g) c"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we first introduce experimental settings, including models, training data and benchmarks. We then report comprehensive results to showcase the effectiveness and robustness of MetaEmbed."
        },
        {
            "title": "4.1 Settings",
            "content": "Models. To evaluate the effectiveness of MetaEmbed as training recipe, we conduct experiments on various VLMs of different sizes, including Qwen2.5-VL (Bai et al., 2025), PaliGemma (Beyer et al., 2024) and Llama-3.2-Vision (Grattafiori et al., 2024). Qwen2.5-VL and PaliGemma represent unified multimodal architectures that process text and vision inputs, while Llama-3.2-Vision represents cross-attention-based 5 Table 1 Precision@1 (%) results on MMEB, which includes 36 tasks across four categories: Classification, Visual Question Answering (VQA), Retrieval, and Visual Grounding. IND and OOD represent the in-domain average and out-of-domain average metrics, respectively. Bold denotes the best scores in the subset and the second-best scores are highlighted with underline. Models CLIP MagicLens UniIR MM-EMBED GME VLM2Vec VLM2Vec-V2 MMRet mmE5 MoCa-3B MoCa-7B B3-7B Size 428M 613M 428M 7B 7B 7B 2B 7B 11B 3B 7B 7B MetaEmbed-3BGemma 3B MetaEmbed-11B MetaEmbed-3B MetaEmbed-7B MetaEmbed-32B 11B 3B 7B 32B Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall Baseline Models 19.7 8.3 15.0 32.2 41.2 49.9 56.3 57.4 62.7 62.9 64.7 66. 53.2 35.4 60.1 63.8 67.8 67.4 69.5 69.9 71.0 70.6 75.0 74.1 55.2 38.8 42.1 48.1 56.9 61.2 62.9 56.0 67.6 59.8 65.8 70.0 MetaEmbed PaliGemma Initialized 53.5 70.9 64.9 62.2 26.0 62.2 57.8 53.4 86.1 77.3 83.6 89.7 88. 92.4 84.6 47.6 67.5 68.0 72.4 72.3 74.7 75.9 42.8 57.1 59.1 66.6 61.5 67.6 67.1 45.4 27.8 42.8 50.0 55.8 62.9 64.9 64.1 69.8 67.5 71.5 72.0 79.5 68. 61.3 65.4 MetaEmbed Llama-3.2-Vision Initialized 42.1 74.3 66.4 91. 65.7 64.3 65.1 MetaEmbed Qwen2.5-VL Initialized 68.1 74.2 71.9 78.7 62.7 71. 73.7 78.6 78.9 79.6 85.4 88.1 73.5 81.8 82. 63.8 70.0 73.7 69.1 76.6 78.7 designs where visual information is integrated into the language model through cross-attention layers. In this section, MetaEmbed-3B, -7B and -32B refer to models finetuned on Qwen2.5-VL backbones and MetaEmbed-11B is finetuned on the Llama-3.2-Vision model. Training. We train MetaEmbed-7B on 32 NVIDIA H100 SXM5 96GB GPUs for 3,500 steps with global batch size of 2, 048, which leads to 30 hours for training. Appendix has training details for other variants. We use LoRA (Hu et al., 2022) with rank of 32 and scaling factor α = 32 in all training. For models with Matryoshka Multi-Vector Retrieval, we empirically choose = 5 group sizes of (rq, rc) as {(1, 1), (2, 4), (4, 8), (8, 16), (16, 64)} and discuss other group size options in 4.3. Group-specific hyperparameter wg in Equation 7 is set to 1 following Kusupati et al. (2022). Contrastive training temperature τ is set to 0.03. We only incorporate MMEB-train (Jiang et al., 2025) and ViDoRe-train (Faysse et al., 2025) with one explicit hard negative from Chen et al. (2025a) for training all variants of MetaEmbed. Evaluation. We assess the general multimodal embedding ability of MetaEmbed on the Massive Multimodal Embedding Benchmark (MMEB) (Jiang et al., 2024) and use Precision@1 as the evaluation metric. MMEB is an established benchmark covering 36 tasks across four types, including classification, visual question answering (VQA) e.g. ScienceQA (Lu et al., 2022), VizWiz (Gurari et al., 2018), ChartQA (Masry et al., 2022), retrieval across variety of domains e.g. Visual News (Liu et al., 2021), FashionIQ (Wu et al., 2021), OvenWiki (Hu et al., 2023), and visual grounding e.g. COCO (Lin et al., 2014), RefCOCO (Kazemzadeh et al., 2014; Yu et al., 2016). In addition, to compare with existing multi-vector solutions on text-image retrieval, we evaluate MetaEmbed on Visual Document Retrieval Benchmarks (ViDoRe) v2 (Macé et al., 2025) and use average NDCG@5 as the metric. ViDoRe (Faysse et al., 2025) was first introduced to benchmark visual document retrieval capabilities in different domains, and its v2 version mitigates performance saturation by including more generalized settings and incorporating multilingual subsets. We refer to Appendix for an introduction to selected baseline methods. 6 Table 2 NDCG@5 (%) results on the ViDoRe v2 benchmark, which covers 7 tasks on visual document retrieval. Syn denotes synthetic data, Mul indicates multilingual tasks, and Bio refers to biomedical domains. Bold denotes the best scores in the subset and the second-best scores are highlighted with underline. Models Size ESG_Human Eco_Mul Bio ESG_Syn ESG_Syn_Mul Bio_Mul Eco Avg. SigLIP VLM2Vec VisRAG-Ret GME mmE5 MoCa-3B MoCa-7B ColPali ColQwen2 MetaEmbed-3B MetaEmbed-7B 652M 7B 3B 7B 11B 3B 7B 3B 3B 3B 7B 28.8 33.9 53. 65.8 52.8 63.3 58.8 51.1 62.2 63.7 62.9 Single-Vector Retrieval 19.8 33.8 14.0 36.7 38.8 42.0 45.9 54.8 48.7 64.0 56.2 54.3 55.1 51.3 44.3 57.3 58.3 62.5 55.3 63.2 57.6 Multi-Vector Retrieval 57.0 59.7 49.9 53.4 61.8 53. MetaEmbed 61.7 65.0 62.6 62.9 55.5 54.2 21.9 38.4 46.4 56.7 54.7 54.8 51. 55.7 54.2 57.4 61.1 18.2 29.7 47.7 55.1 46.8 59.8 61.3 56.5 56.5 58. 61.9 29.8 23.8 51.4 38.7 59.6 51.0 62.9 59.3 48.6 50.5 62.8 59.8 63.8 58.8 51.6 54.5 61.5 57.5 62.3 60.3 60.9 61."
        },
        {
            "title": "4.2 Main Results\nWe report the overall multimodal embedding performance of different MetaEmbed variants and baseline\nmethods on MMEB in Table 1. Similarly, we present the visual document retrieval performance of MetaEmbed\nand baselines on ViDoRe v2 in Table 2. All MetaEmbed results are reported with 16 query-side vectors\nand 64 candidate-side vectors, i.e. (rq, rc) = (16, 64), and we will discuss the impact of the number of Meta\nEmbeddings used in §4.3. We conclude key observations from those metrics as follows.",
            "content": "MetaEmbed delivers substantial improvements over the best existing single-vector baselines at comparable model sizes. At the 3B scale, MetaEmbed achieves 69.1 overall on MMEB, already surpassing MoCa-3B (67.5) with +1.6% relative improvement. At 7B, the margin widens: MetaEmbed reaches 76.6, outperforming MoCa-7B (71.5) and mmE5 (69.8) by over 5-7 points. Scaling further to 32B yields 78.7 overall, clear improvement over both the strongest baselines and our smaller variants. Importantly, the relative gains of MetaEmbed increase with model size while the 3B variant offers competitive results, the 7B and 32B models establish new state-of-the-art performance with the gap over baselines widening as scale increases. This trend suggests that MetaEmbed scales more favorably than prior approaches, with benefits compounding in larger regimes. The choice of VLM backbone has pronounced effect on MetaEmbed performance across tasks. MetaEmbed11B with Llama-3.2-Vision backbone shows strong grounding and solid classification abilities, but its VQA score drops sharply to 42.1 more than 32 points lower than the Qwen2.5-VL-initialized 7B model (74.2). This limitation caps its overall score at 65.1 despite excelling in other subtasks. In contrast, Qwen2.5-VL initialization consistently delivers balanced improvements across all metrics: MetaEmbed-7B achieves 76.6, and scaling further to 32B pushes the state of the art to 78.7, with especially strong VQA and retrieval capabilities. We notice that if the underlying base model itself struggles on some domains when used as generative model, such weakness directly propagates into MetaEmbed as an embedding model. For example, Huang et al. (2025) suggests that Llama-3.2-Vision-11B is less competitive in most zero-shot VQA benchmarks, and such weakness is inherited in MetaEmbed-11B. MetaEmbed demonstrates strong retrieval performance on ViDoRe v2, particularly in multilingual and biomedical domains, despite not being trained on multilingual data. Even at the 3B scale, MetaEmbed matches or surpasses much larger baselines, showing robustness across all seven evaluation tracks. When scaled to 7B, the model yields further gains, with the largest improvements appearing in multilingual and biomedical domains. This is especially noteworthy given that no explicit multilingual data was included during training, suggesting that MetaEmbed effectively retains and leverages cross-lingual capabilities from its backbone. 7 Figure 3 Impact of retrieval budget on MMEB across MetaEmbed of varying model sizes. Retrieval budget is denoted as (rq, rc), i.e. tuple of the number of Meta Embeddings used on query and candidate side. Increasing the retrieval budget from (1,1) to (16,64) consistently improves performance for all model sizes, with larger gains observed in higher-capacity models. The dashed green lines indicate the best single-vector retrieval performance and red arrows indicate the absolute gain (in percentage points) between MetaEmbed and single-vector retrieval."
        },
        {
            "title": "4.3 Ablation Studies\nTo better understand MetaEmbed, we design comprehensive ablation studies to investigate its test-time\nscaling capabilities, the effectiveness of MMR and its robustness across different models.",
            "content": "(a) MetaEmbed with (16, 64) retrieval budget shows less diminishing returns as model size scales. Green numbers denote the gain compared to the preceding model size. How does the performance scales with the retrieval cost? We present the performance plots of different MetaEmbed models with respect to retrieval cost in Fig. 3. Data points in each plot correspond to MetaEmbed performance with specific model size with varying test-time retrieval budgets. The dashed green line marks the best-performance single-vector retrieval model with identical training settings. The plots show that across model sizes, the curves rise steadily as more retrieval budget is allocated. While the improvement is modest for smaller models as MetaEmbed3B shows +3.3 points relative gain against the single-vector method, we observe that it becomes more noticeable as the base model size grows and MetaEmbed brings the most pronounced improvements on the 32B model with +6.6 points gain. Does MetaEmbed apply to pre-trained VLMs of different sizes and architectures? Fig. 3 (d) already demonstrates that MetaEmbed-11B shows advantages when finetuned on different VLM, Llama-3.2-Vision-11B, showing the robustness of our method across architectures. Another key observation is that MetaEmbed makes more effective use of larger model capacity compared to single-vector methods. Fig. 4a presents the performance of the two approaches on MMEB under identical training settings as model size increases. We find that MetaEmbed achieves more substantial gains than single-vector retrieval. Notably, the improvement of the single-vector baseline from 7B to 32B is no longer statistically significant while MetaEmbed still holds noticeable gain. Figure 4 Ablation studies. (b) Average NDCG@5 (%) on ViDoRe v1 benchmark with varying retrieval budgets on MetaEmbed-3B with and without MMR design. How effective is the MMR design? To investigate how MMR functions, i.e. how it organizes query and candidate information in nested order of importance, we use average NDCG@5 on ViDoRe-v1 and report testtime scaling curves on two variants of MetaEmbed-3B: with and without MMR in Fig. 4b. If MMR is not enabled during training, the flexibility of MetaEmbed will be severely constrained, as evidenced by the substantial performance drop under low retrieval budgets. For example, the performance drop hits 9.0 points when using MetaEmbed without MMR as single-vector retrieval model, i.e. (rq, rc) = (1, 1). Although the gap narrows as retrieval budget increases, the model with MMR consistently outperforms the non-MMR model as shown in the figure. Surprisingly, we find that even at the full budget (rq, rc) = (16, 64), the MMR model still performs slightly better, demonstrating that MMR does not sacrifice the original multivector retrieval ability at full scale. We additionally report the test-time scaling results on MMEB in Appendix where MMR shows negligible performance degradation. 8 Table 3 Efficiency analysis of MetaEmbed-7B with different retrieval budgets on an A100 GPU with 100,000 candidates per query with scoring batch size of 1,000. Query encoding and index generation latency are omitted because they remain the same for all variants. Latency refers specifically to scoring latency and mean and standard deviation of latency are reported with 10 runs. Index is stored and compared with bfloat16 precision (Wang and Kanwar, 2019). Retrieval Budget Scoring FLOPs (G) Latency (ms) Index Memory (GiB) MMEB Acc (%) (1, 1) (2, 4) (4, 8) (8, 16) (16, 64) 0.71 5.73 22.94 91.75 733.89 1.670.13 1.660.12 1.670.12 1.920.12 6.250.07 0.68 2.67 5.34 10.68 42.72 71.3 72.0 72.9 74.3 76."
        },
        {
            "title": "5 Discussion",
            "content": "In this section, we mainly discuss the efficiency of MetaEmbed as flexible multi-vector retrieval method, with focus on index memory consumption and latency under varying retrieval budgets. typical online retrieval process consists of three stages: (a) Query encoding, where the query is processed by the encoder to obtain contextualized embeddings. (b) Scoring, where query embeddings are compared with candidate document embeddings in the index. For single-vector dense retrieval, this is dot-product operation between pairs of vectors, while in multi-vector retrieval such as MetaEmbed it requires late interaction (e.g., MaxSim in Eq. 2) between multiple embeddings. (c) Ranking, lightweight operation where candidate documents are sorted based on the scores. We report the efficiency analysis of MetaEmbed-7B in Table 3 with the following observations: 1. Although the number of scoring FLOPs grows substantially with larger retrieval budgets, the scoring stage itself is not compute-bounded until the extreme case of (16, 64). The measured latencies remain nearly flat across moderate budgets, demonstrating that GPU throughput can accommodate the additional FLOPs without becoming bottleneck. 2. The relative contribution of scoring cost to the overall retrieval pipeline is negligible compared to query encoding. For instance, encoding an image query of 1024 tokens requires 42.72 TFLOPs and 788ms. These figures are orders of magnitude larger than the scoring costs reported in Table 3, indicating that efficiency improvements should primarily target encoding rather than scoring with small number of candidates. 3. As flexible multi-vector retrieval method, index memory consumption can grow proportionally with the retrieval budget. While this can present challenges for large deployments, the issue can be mitigated by using balanced retrieval budget or more frequent offloading of index data to CPU memory. Overall, these findings suggest that MetaEmbed is efficient in practice. Query encoding dominates latency, scoring is lightweight under most realistic budgets with small number of candidates, and memory scaling can be controlled by either selecting balanced retrieval budgets or system-level strategies such as CPU swapping."
        },
        {
            "title": "6 Conclusion",
            "content": "We present MetaEmbed, new paradigm for multimodal retrieval that rethinks the construction and interaction of embeddings at scale. By leveraging small set of learnable Meta Tokens and training them through our proposed Matryoshka Multi-Vector Retrieval (MMR) framework, MetaEmbed organizes information into coarse-to-fine levels of granularity. This design enables flexible late interaction that balances retrieval accuracy, index size, and latency unlocking test-time scalability for multimodal retrieval. We believe MetaEmbed opens path toward more general, efficient, and controllable multimodal retrieval, bridging the gap between fine-grained expressiveness and large-scale deployability. Acknowledgements. We thank Anshumali Shrivastava, Xueyuan Su, Xu Han and Norman Huang for insightful discussions and support."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, et al. Phi-3 technical report: highly capable language model locally on your phone, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models. In The Thirteenth International Conference on Learning Representations, 2025. Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, and Zhicheng Dou. Moca: Modality-aware continual pre-training makes better bidirectional multimodal embeddings. arXiv preprint arXiv:2506.23115, 2025a. Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. CoRR, abs/2502.08468, 2025b. doi: 10.48550/ ARXIV.2502.08468. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, et al. Meta clip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Joshua Engels, Benjamin Coleman, Vihan Lakshman, and Anshumali Shrivastava. DESSERT: An efficient algorithm for vector set search with vector set queries. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Fartash Faghri, David Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. 2018. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: deep visual-semantic embedding model. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. Albert Gordo, Jon Almazán, Jerome Revaud, and Diane Larlus. Deep image retrieval: Learning global representations for image search. In European conference on computer vision, pages 241257. Springer, 2016. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432, 2025. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Michael Günther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Bo Wang, Sedigheh Eslami, Scott Martens, Maximilian Werk, Nan Wang, and Han Xiao. jina-embeddings-v4: Universal embeddings for multimodal multilingual retrieval, 2025. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Hexiang Hu, Wei-Lun Chao, and Fei Sha. Learning answer embeddings for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 54285436, 2018. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1206512075, October 2023. Chengyue Huang, Yuchen Zhu, Sichen Zhu, Jingyun Xiao, Moises Andrade, Shivang Chopra, and Zsolt Kira. Mimicking or reasoning: Rethinking multi-modal in-context learning in vision-language models. arXiv preprint arXiv:2506.07936, 2025. Rajesh Jayaram, Laxman Dhulipala, Majid Hadian, Jason Lee, and Vahab Mirrokni. Muvera: Multi-vector retrieval via fixed dimensional encoding. Advances in Neural Information Processing Systems, 37:101042101073, 2024. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. VLM2vec: Training visionlanguage models for massive multimodal embedding tasks. In The Thirteenth International Conference on Learning Representations, 2025. Yeong-Joon Ju and Seong-Whan Lee. From generator to embedder: Harnessing innate abilities of multimodal llms via building zero-shot discriminative embedding model. arXiv preprint arXiv:2508.00955, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. \"ReferItGame: Referring to Objects in Photographs of Natural Scenes\". In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798, Doha, Qatar, October 2014. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948, 2020. Jamie Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models, 2014. Fanheng Kong, Jingyuan Zhang, Yahui Liu, Hongzhi Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yu Tian, Fuzheng Zhang, Guorui Zhou, et al. Modality curation: Building universal embeddings for advanced multimodal information retrieval. arXiv preprint arXiv:2505.19650, 2025. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812, 2025. Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Zhao. Rethinking the role of token retrieval in multi-vector retrieval. Advances in Neural Information Processing Systems, 36:1538415405, 2023. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1288812900. PMLR, 2022. 11 Minghan Li, Sheng-Chieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. CITADEL: Conditional token interaction via dynamic lexical routing for efficient and effective multi-vector retrieval. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1189111907, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.663. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: experiences on accelerating data parallel training. Proceedings of the VLDB Endowment, 13(12):30053018, 2020. Sheng-chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal multimodal retrieval with multimodal llms, 2024. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. MM-EMBED: In The Thirteenth International UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS. Conference on Learning Representations, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision ECCV 2014, pages 740755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 67616771, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 25072521. Curran Associates, Inc., 2022. Quentin Macé, António Loison, and Manuel Faysse. Vidore benchmark v2: Raising the bar for visual retrieval. arXiv preprint arXiv:2505.17166, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. \"ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning\". In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, et al. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. arXiv preprint arXiv:2507.04590, 2025. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Pan, and Bei Yu. Unimoco: Unified modality completion for robust multi-modal embeddings. arXiv preprint arXiv:2505.11815, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, and Alan Yuille. Multiple instance visual-semantic embedding. In Gabriel Brostow Tae-Kyun Kim, Stefanos Zafeiriou and Krystian Mikolajczyk, editors, Proceedings of the British Machine Vision Conference (BMVC), pages 89.189.12. BMVA Press, September 2017. Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. Plaid: an efficient engine for late interaction retrieval. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 17471756, 2022a. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 37153734, 2022b. Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, and Vicente Ordonez. Drill-down: Interactive retrieval of complex scenes using natural language queries. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293, 2025a. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293, 2025b. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. In Proceedings of the Winoground: Probing vision and language models for visio-linguistic compositionality. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. Giorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling of cnn activations. In ICLR 2016-International Conference on Learning Representations, pages 112, 2016. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. doi: 10.48550/ARXIV.2409.12191. Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus. Google Cloud Blog, 8 2019. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer, 2024. doi: 10.1007/978-3-031-73021-4_23. Orion Weller, Michael Boratko, Iftekhar Naim, and Jinhyuk Lee. On the theoretical limitations of embedding-based retrieval. arXiv preprint arXiv:2508.21038, 2025. Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1130711317, June 2021. Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, and Vicente Ordonez. Grounding language models for visual entity recognition. In European Conference on Computer Vision, pages 393411. Springer, 2024. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. Mengyao Xu, Gabriel Moreira, Ronay Ak, Radek Osmulski, Yauhen Babakhin, Zhiding Yu, Benedikt Schifferer, and Even Oldridge. Llama nemoretriever colembed: Top-performing text-image retrieval model. arXiv preprint arXiv:2507.05513, 2025. Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. FILIP: Fine-grained interactive language-image pre-training. In International Conference on Learning Representations, 2022. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision ECCV 2016, pages 6985, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46475-6. 13 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1194111952. IEEE, 2023. doi: 10.1109/ICCV51070.2023.01100. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms, 2024b. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12):38483860, 2023. Wenfeng Zheng, Lirong Yin, Xiaobing Chen, Zhiyang Ma, Shan Liu, and Bo Yang. Knowledge base graph embedding module design for visual question answering model. Pattern recognition, 120:108153, 2021. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "Table 4 Training details of MetaEmbed variants. Batch Size Learning Rate Training Cost Embedding Dim MetaEmbed-3BGemma MetaEmbed-3B MetaEmbed-7B MetaEmbed-11B MetaEmbed-32B 2,048 2,048 1,536 1,024 1, 1 104 1 104 1 104 1 104 1"
        },
        {
            "title": "32 H100s for 14h\n32 H100s for 23h\n32 H100s for 30h\n32 H100s for 10h\n64 H100s for 25h",
            "content": "2,048 2,048 3,584 4,096 5,120 We list the details of each MetaEmbed variant in Table 4. We reserve 1% training data from each subset of MMEB-train as evaluation split and training was early stopped when evaluation loss stops dropping. All models are trained with gradient checkpointing (Chen et al., 2016) to reduce memory usage. Training was conducted using PyTorch (Paszke et al., 2019) 2.6.0+cu124 and FlashAttention 2.0 (Dao, 2024). For the 3B configuration, we adopt Distributed Data Parallel (Li et al., 2020) while for all other model sizes we use Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) v2. To prevent distributed hanging when training samples contain no images under FSDP, we pad those samples with placeholder image to ensure the visual encoder is activated on each GPU."
        },
        {
            "title": "B Detailed MMEB Ablation Results",
            "content": "Table 5 Comparison between MetaEmbed and single-vector & multi-vector retrieval models trained with identical settings. NoMMR indicates Matryoshka Multi-Vector Retrieval (MMR) is disabled. denotes the difference to the best single-vector retrieval method, i.e. single-last. 3B 7B 32B 11B"
        },
        {
            "title": "Type",
            "content": "MMEB MMEB MMEB MMEB single-last single-mean split-(16, 64) (1, 1) (2, 4) (4, 8) (8, 16) (16, 64) NoMMR-(16, 64) 65.6 65.2 64.2 65.4 65.8 66.7 67.5 69.1 69.3 0 -0.4 -1.4 -0.2 +0.2 +1.1 +1.9 +3. +3.7 71.6 71.2 70.1 0 -0.4 -1.5 MetaEmbed 71.3 72.0 72.9 74.3 76.6 -0.3 +0.4 +1.3 +2.7 +5.0 77. +5.4 72.1 71.1 70.5 72.4 73.2 74.2 75.8 78.7 79.1 0 -1.0 -1.6 +0.3 +1.1 +2.1 +3.7 +6. +7.0 60.1 58.1 56.0 60.0 60.6 62.2 63.3 65.1 66.2 0 -2.0 -4.1 -0.1 +0.5 +2.1 +3.2 +5. +6.1 To further compare the superiority of MetaEmbed against both single-vector and multi-vector retrieval methods, we design the following baselines based on the same pre-trained models for fair comparison: 1. single-last: single-vector retrieval method that uses the last-token hidden state from the last layer as the retrieval representation. 2. single-mean: similar to the above, but applies average pooling over all last-layer hidden states to obtain the retrieval representation. 3. split-(16, 64): simple multi-vector retrieval baseline where the query-side last-layer hidden states are evenly partitioned into 16 segments, with the mean of each segment taken as 16 query vectors; the same 15 process produces 64 candidate-side vectors. This method does not introduce additional parameters, making it suitable fixed-length multi-vector retrieval baseline. Our findings indicate that MetaEmbed goes beyond test-time scaling advantages mentioned in the main text: it consistently outperforms the top single-vector method as well as naive multi-vector baseline. Moreover, the MMR design brings no statistically significant loss, demonstrating that it adds flexibility while maintaining retrieval quality."
        },
        {
            "title": "C Baseline Method Introduction",
            "content": "For clarity and completeness, we provide short introductions to the baseline methods considered in Table 1. All performance metrics reported on baseline methods are directly taken from the corresponding original papers or Chen et al. (2025a). CLIP (Radford et al., 2021). CLIP is dual-encoder trained with contrastive learning on 400M imagetext pairs. It learns aligned representations for both modalities, enabling strong zero-shot classification and retrieval capabilities. MagicLens (Zhang et al., 2024a). MagicLens is lightweight dual-encoder for instruction-guided image retrieval. It is trained in self-supervised manner on roughly 36.7M (query-image, text instruction, targetimage) triplets mined from co-occurring web images. UniIR (Wei et al., 2024). UniIR is unified, instruction-guided multimodal retriever that handles eight retrieval task formats spanning text, image, and mixed-modality queries/candidates. It is jointly trained on ten heterogeneous datasets, showing robust in-distribution performance and zero-shot generalization across tasks. MM-Embed (Lin et al., 2024). MM-Embed converts multimodal large language model into universal bi-encoder for retrieval. It is fine-tuned with modality-aware hard negatives across diverse retrieval datasets to improve cross-modal alignment. GME (Zhang et al., 2024b). The General Multimodal Embedder is trained on large synthetic dataset containing diverse multimodal queries and documents. It introduces fused-modal training examples (mixed textimage inputs) to enable universal any-to-any modality retrieval. VLM2Vec (Jiang et al., 2025). VLM2Vec transforms pretrained vision-language model into universal embedding model through instruction-tuned contrastive learning. It is trained on the Massive Multimodal Embedding Benchmark (MMEB), covering 36 tasks across classification, VQA, retrieval, and grounding. MMRet (Zhou et al., 2024). MMRet builds on the MM-Embed framework but introduces further refinements in negative sampling and large-scale fine-tuning on massive synthetic dataset, making it one of the strongest retrieval-centric embedding models in prior work. mmE5 (Chen et al., 2025b). mmE5 extends the multilingual text embedding model E5 into the multimodal setting. It is trained with multilingual and multimodal signals, leveraging synthetic image-text pairs and hard negatives, and achieves strong state-of-the-art results on MMEB prior to more recent models. MoCa (Chen et al., 2025a). MoCa (Modality-aware Causal Pre-training) introduces two-stage process: modality-aware continual pre-training to adapt causal visionlanguage models for bidirectional encoding, followed by heterogeneous contrastive fine-tuning across text, image, and mixed modality pairs. We evaluate both MoCa-3B and MoCa-7B, which show competitive overall performance among baselines on MMEB. 16 B3 (Thirukovalluru et al., 2025a) B3-7B is 7B-parameter instruction-tuned multimodal retriever with It achieves strong results on MMEB and serves as an additional specialized batch mining techniques. competitive baseline."
        }
    ],
    "affiliations": [
        "Meta Superintelligence Labs",
        "Rice University"
    ]
}