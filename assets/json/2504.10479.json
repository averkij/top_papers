{
    "paper_title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
    "authors": [
        "Jinguo Zhu",
        "Weiyun Wang",
        "Zhe Chen",
        "Zhaoyang Liu",
        "Shenglong Ye",
        "Lixin Gu",
        "Yuchen Duan",
        "Hao Tian",
        "Weijie Su",
        "Jie Shao",
        "Zhangwei Gao",
        "Erfei Cui",
        "Yue Cao",
        "Yangzhou Liu",
        "Weiye Xu",
        "Hao Li",
        "Jiahao Wang",
        "Han Lv",
        "Dengnian Chen",
        "Songze Li",
        "Yinan He",
        "Tan Jiang",
        "Jiapeng Luo",
        "Yi Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Wenqi Shao",
        "Junjun He",
        "Yingtong Xiong",
        "Wenwen Qu",
        "Peng Sun",
        "Penglong Jiao",
        "Lijun Wu",
        "Kaipeng Zhang",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Limin Wang",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 7 4 0 1 . 4 0 5 2 : r InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models Jinguo Zhu1, Weiyun Wang5,1, Zhe Chen4,1, Zhaoyang Liu1, Shenglong Ye1, Lixin Gu1, Yuchen Duan6,1, Hao Tian2, Weijie Su1, Jie Shao4,1, Zhangwei Gao7,1, Erfei Cui7,1, Yue Cao4,1, Yangzhou Liu4,1, Weiye Xu1, Hao Li1, Jiahao Wang1, Han Lv1, Dengnian Chen1, Songze Li1, Yinan He1, Tan Jiang2, Jiapeng Luo2, Yi Wang1, Conghui He1, Botian Shi1, Xingcheng Zhang1, Wenqi Shao1, Junjun He1, Yingtong Xiong1, Wenwen Qu1, Peng Sun1, Penglong Jiao1, Lijun Wu1, Kaipeng Zhang1, Huipeng Deng1, Jiaye Ge1, Kai Chen1, Limin Wang4,1, Min Dou1, Lewei Lu2, Xizhou Zhu3,1, Tong Lu4, Dahua Lin6,1, Yu Qiao1, Jifeng Dai3,1(cid:66), Wenhai Wang6,1(cid:66) 1Shanghai AI Laboratory 2SenseTime Research 3Tsinghua University 4Nanjing University 5Fudan University 6The Chinese University of Hong Kong 7Shanghai Jiao Tong University Code: https://github.com/OpenGVLab/InternVL Model: https://huggingface.co/OpenGVLab/InternVL3-78B Data: https://huggingface.co/datasets/OpenGVLab/InternVL-Data"
        },
        {
            "title": "Abstract",
            "content": "We introduce InternVL3, significant advancement in the InternVL series featuring native multimodal pre-training paradigm. Rather than adapting text-only large language model (LLM) into multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across wide range of multi-modal tasks. In particular, InternVL3-78B achieves score of 72.2 on the MMMU benchmark, setting new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) [32, 65, 120, 21, 19, 122, 67, 113, 96, 135, 70, 31, 84, 116, 18, 88, 104, 68] have recently achieved or even surpassed human-level performance in broad spectrum of tasks, underscoring their potential as significant stride toward artificial general intelligence (AGI). Yet, the majority of leading MLLMsboth open-source and proprietaryare adapted from text-only large language models through sophisticated multi-stage pipelines [21, 19, 18, 5, 120, 7]. These post-hoc approaches are built upon the * equal contribution; interns at OpenGVLab, Shanghai AI Laboratory; (cid:66) corresponding authors (daijifeng@tsinghua.edu.cn, wangwenhai@pjlab.org.cn). Figure 1: Multimodal performance of the InternVL series and other advanced MLLMs. The InternVL series has consistently exhibited progressive enhancements in multimodal capabilities. The newly released InternVL3 significantly outperforms existing open-source MLLMs. Moreover, even in comparison with state-ofthe-art closed-source commercial models, InternVL3 continues to demonstrate highly competitive performance. original text-based pre-training processes, thereby introducing alignment challenges when integrating additional modalities such as vision. In practice, bridging modality gaps often necessitates incorporating auxiliary data from specialized domains (e.g., optical character recognition scenarios) and intricate parameter-freezing or multi-stage fine-tuning schedules to ensure that core linguistic capacities remain uncompromised [72, 7, 5, 18]. Such resource-intensive strategies highlight the need for more efficient multimodal training paradigms. In this report, we introduce InternVL3, the latest milestone in the InternVL series [21, 20, 18], which is distinguished by its native multimodal pre-training strategy. Rather than first pre-training text-only large language model and subsequently retrofitting it via multimodal alignment to support visual processing, InternVL3 learns multimodal capabilities from the pre-training stage by jointly exposed to both text-only corpora and diverse multimodal datasets. This unified approach enables the model to simultaneously acquire linguistic and multimodal competencies in more efficient and integrated manner. InternVL3 further excels through multiple innovations that reinforce both performance and scalability. We employ variable visual position encoding (V2PE) mechanism [41] to accommodate longer multimodal contexts. Furthermore, advanced post-training strategiescomprising supervised fine-tuning (SFT) and mixed preference optimization (MPO) [123]together with test-time scaling strategies [124] and an optimized training infrastructure [15], significantly enhance InternVL3s efficiency and performance. Comprehensive empirical evaluations demonstrate that InternVL3 surpasses its predecessors (e.g., InternVL2.5 [18]) across wide range of tasks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, and multilingual capabilities. Notably, by incorporating expanded domain-specific datasets, InternVL3 also exhibits marked improvements in tool usage, GUI agents, industrial image analysis, and spatial reasoning, thus substantially extending the multimodal scenarios addressed by the InternVL series. It proves highly competitive with other open-source MLLMs such as Qwen2.5-VL [7] and remains on par with closed-source models (e.g., ChatGPT-4o [97], Claude-3.5 Sonnet [3], Gemini-2.5 Pro [116]). This versatility is evidenced by its 72.2-point performance on the MMMU benchmark [140], setting new standard among open-source MLLMs. Additionally, InternVL3 demonstrates language capabilities comparable to other advanced LLMs of similar scale. 2 Figure 2: Performance of various MLLMs on the OpenCompass multimodal academic leaderboard. The enhanced InternVL seriesInternVL3demonstrates outstanding multimodal capabilities, significantly outperforming both the Qwen2.5-VL series and closed-source models such as Step-1o, GLM-4v-Plus, and GPT-4o. Remarkably, InternVL3-78B also remains highly competitive with the state-of-the-art Gemini-2.5-Pro. To foster further advancements within the open-source community, we will release the training data1 and model weights alongside this work, thereby ensuring transparency and reproducibility for the continued development of next-generation MLLMs. 2 InternVL Building upon the prior InternVL series [21, 19, 18], we propose InternVL3, new generation within the InternVL model family. InternVL3 is specifically designed to streamline the training pipeline while significantly enhancing multimodal capabilities. In this section, we first delineate the core components of InternVL3, including its model architecture, training procedures, test-time scaling strategies, and infrastructure-level optimizations."
        },
        {
            "title": "2.1 Model Architecture",
            "content": "The architecture of InternVL3 follows the same general framework as its predecessors, adhering to the ViTMLP-LLM paradigm [65, 18, 40, 20]. Detailed architectural specifications are summarized in Table 1. Although the native pre-training paradigm discussed later could enable training MLLMs from scratch, we choose to initialize the ViT and LLM components with pre-trained model weights to reduce computational costs. The vision encoder is available in two configurations: InternViT-300M and InternViT-6B. For the language model, we leverage pre-trained large language models (LLMs), specifically the Qwen2.5 series and InternLM3-8B. Importantly, our LLM components are initialized solely from pre-trained base models, without employing instruction-tuned variants. The multilayer perceptron (MLP) utilized in the model is two-layer network with random initialization. In line with the approach taken in InternVL2.5, InternVL3 incorporates pixel unshuffle operation to enhance scalability for processing high-resolution images. This operation reduces the visual token count to one-quarter of its original value, representing each 448448 image tile with 256 visual tokens. InternVL3 also integrates the Variable Visual Position Encoding Variable Visual Position Encoding. (V2PE) [41], which utilizes smaller, more flexible position increments for visual tokens. This modifica1The open-source data are being organized, and comprehensive list will be included in future revision of this report. 3 Model Name InternVL3-1B InternVL3-2B InternVL3-8B InternVL3-9B InternVL3-14B InternVL3-38B InternVL3-78B #Param 0.9B 1.9B 8.1B 9.2B 15.1B 38.4B 78.4B Vision Encoder InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-6B-448px-V2.5 InternViT-6B-448px-V2.5 Language Model Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-7B InternLM3-8B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B OpenCompass Academic 57.4 63.9 73.3 72.4 75.5 77.3 79.5 Table 1: Pre-trained models used in the InternVL3 series. The OpenCompass scores for the InternVL3 series were obtained through our local testing. tion facilitates the handling of longer multimodal contexts without excessively extending the position window. Specifically, each training sample for the MLLM is represented as: = (cid:0)x1, x2, . . . , xL (cid:1), where each token xi can be textual token embedding, visual embedding, or another modality-specific representation (e.g., video patch embeddings). The position index pi for any token xi can be computed sequentially as follows: (1) (cid:26)0, pi = fpos(pi1, xi), if = 1, for = 2, 3, . . . , N. (2) In contrast to traditional MLLMs, where position indices increment uniformly by 1 for each token, irrespective of modality, V2PE employs modality-specific recursive function for position index computation. This results in distinct position index assignments for textual and visual tokens: pi = pi1 + (cid:26)1, δ, if xi is textual token, if xi is visual token, (3) where δ is smaller increment (δ < 1), reducing the rate at which position indices increase for visual tokens. The standard increment of 1 is retained for textual tokens to preserve their positional distinctions. In line with the original V2PE design, we maintain that δ remains constant within single image to preserve the relative positional relationships. During training, δ is randomly chosen for each image from predefined set of fractional values: 1 16 During inference, δ can be flexibly selected based on the input sequence length, enabling balance between task performance and ensuring that position indices remain within the models valid context range. Notably, when δ = 1, V2PE reverts to the conventional positional encoding used in InternVL2.5. δ = 1 128 1 256 1 32 1 1 8 1 2 1 4 (4) 1, (cid:27) (cid:26) , , , , , , , ."
        },
        {
            "title": "2.2 Native Multimodal Pre-Training",
            "content": "We propose native multimodal pre-training approach that consolidates language pre-training and multi-modal alignment training into single pre-training stage. Unlike conventional paradigmswhere language-only large model is first trained (typically with language pre-training followed by language post-training) and subsequently adapted to accommodate additional modalitiesour method performs integrated optimization by interleaving multimodal data (e.g., imagetext, videotext, or interleaved imagetext sequences) with large-scale textual corpora during the pre-training process. This unified training scheme enables the pre-trained model to learn both linguistic and multimodal capabilities simultaneously, ultimately enhancing its capability to handle vision-language tasks without introducing additional bridging modules or subsequent inter-model alignment procedures. Multimodal Autoregressive Formulation. Let denote Transformer-based model parameterized by θ that can process text, image, and video simultaneously. Specifically, for an arbitrary training sample = (cid:0)x1, x2, . . . , xL (cid:1) with the token length of L, we adopt the standard left-to-right autoregressive objective: Lfull(θ) = (cid:88) i= wi log pθ (cid:0)xi (cid:12) (cid:12) x1, . . . , xi1 (cid:1), (5) where wi denotes the loss weight of token i. Although this formulation naturally propagates gradients through tokens of all modalities, we restrict the loss computation exclusively to text tokens, resulting in: Ltext-only(θ) = (cid:88) i=2 xi Text wi log pθ (cid:0)xi (cid:12) (cid:12) x1, . . . , xi1 (cid:1). (6) Under this selective objective, visual tokens serve as conditioning context for text prediction and are not directly predicted. Consequently, the model learns to embed multimodal information in manner that is beneficial for downstream language decoding tasks. Notably, regarding the design choice of the token weight wi, as discussed in InternVL2.5 [18], the widely used token averaging and sample averaging strategies can lead to gradients biased toward longer and shorter responses, respectively. To mitigate this issue, we adopt square averaging, which is defined as: wi = 1 l0 , 1 l0.5 , 1 l1 , for token averaging for square averaging for sample averaging, (7) where denotes the number of tokens in the training sample on which the loss needs to be calculated. Joint Parameter Optimization. Unlike the conventional language-only training followed by multimodal adaptation paradigm, our method updates all model parameters jointly during multimodal pre-training. Specifically, let θ = arg min Ex Dmulti (cid:2)Ltext-only(θ)(cid:3), θ (8) where Dmulti is the union of large-scale text-only and multimodal corpora (e.g., imagetext or videotext pairs). We thus optimize single model to handle these combined data sources. This multi-task joint optimization ensures that text representations and visual features are learned in concert, reinforcing alignment across modalities. Moreover, this integrated optimization departs from conventional language-only training followed by multimodal adaptation pipelines, which often freeze or partially fine-tune certain layers in the LLM component or even in the ViT encoder when adapting to MLLM. In contrast, our method trains every layer jointly, allowing all parameters to be jointly optimized on large-scale multimodal corpora and ensuring that both linguistic and visual features evolve synchronously. As result, the final parameters are primed for high performance on both pure language and multimodal tasks, without additional tuning steps. Data. The pre-training data utilized in InternVL3 is broadly classified into two categories: multimodal data and pure language data. The multimodal dataset comprises synthesis of pre-existing datasets alongside newly acquired real-world data. Specifically, we leverage the pre-training corpus from InternVL2.5, which covers diverse range of domains such as image captioning, general question answering, mathematics, charts, optical character recognition (OCR), knowledge grounding, document understanding, multi-turn dialogue, and medical data. Although the overall data scale was not increased, the utility of this dataset was significantly improved by updating not only to the MLP module weights but also to those associated with the ViT and LLM components. In addition, to enhance the models ability to generalize in real-world applications, additional data is incorporated from tasks related to graphical user interfaces (GUI), tool usage, 3D scene understanding, and video comprehension. To compensate for the relatively short and less diverse textual content typically found in multimodal datasets, we integrate pure language data into the pre-training process. This helps preserve and amplify the models capabilities in language understanding and generation. The language corpus is primarily constructed on the pre-training data from InternLM2.5 and is further augmented with various open-source text datasets [8, 76, 78]. This enhancement aims to improve the models performance on knowledge-intensive tasks, as well as its proficiency in mathematical and reasoning tasks. Given the complexity of balancing these heterogeneous data sources, determining an appropriate sampling strategy is non-trivial. In InternVL3, we adopt two-stage strategy to establish the optimal sampling ratio between multimodal and language data. Initially, we train separate models on the multimodal and language datasets and evaluate their performance on corresponding benchmarks, allowing us to identify optimal sampling ratios within each modality. Then, under fixed total training budget, we combine the two modalities and determine their relative sampling ratio. Empirical studies show that 1:3 ratio of language to multimodal data 5 yields the best overall performance across both unimodal and multimodal benchmarks. Under this configuration, the total number of training tokens is approximately 200 billion, comprising 50 billion from language data and 150 billion from multimodal data."
        },
        {
            "title": "2.3 Post-Training",
            "content": "After the Native Multimodal Pre-Training, we apply two-stage post-training strategy to further enhance the multimodal conversation and reasoning abilities of our models. This strategy consists of Supervised Fine-Tuning (SFT) and Mixed Preference Optimization (MPO). In the SFT phase, the model is trained to imitate the highquality responses under positive supervision signals. In the subsequent MPO phase, we introduce additional supervision from both positive and negative samples, thereby further improving its overall abilities. Supervised Fine-Tuning. In this phase, the techniques of random JPEG compression, square loss re-weighting, and multimodal data packing proposed in InternVL2.5 [18] are also employed in the InternVL3 series. The main advancement of the SFT phase in InternVL3 compared to InternVL2.5 lies in the use of higher-quality and more diverse training data. Specifically, we further extend training samples for tool usage, 3D scene understanding, GUI operations, long context tasks, video understanding, scientific diagrams, creative writing, and multimodal reasoning. Mixed Preference Optimization. During Pre-training and SFT, the model is trained to predict the next token conditioned on previous ground-truth tokens. However, during inference, the model predicts each token based on its own prior outputs. This discrepancy between ground-truth tokens and model-predicted tokens introduces distribution shift, which can impair the models Chain-of-Thought (CoT) reasoning capabilities. To mitigate this issue, we employ Mixed Preference Optimization (MPO) [123], which introduces additional supervision from both positive and negative samples to align the model response distribution with the ground-truth distribution, thereby improving reasoning performance. Specifically, the training objective of MPO is combination of preference loss Lp, quality loss Lq, and generation loss Lg, which can be formulated as follows: = wpLp + wqLq + wgLg, (9) where represents the weight assigned to each loss component. Specifically, the DPO loss [100] serves as the preference loss to enable the model to learn the relative preference between chosen and rejected responses: (cid:18) Lp = log σ β log πθ (yc x) π0 (yc x) β log πθ (yr x) π0 (yr x) (cid:19) , (10) where β is the KL penalty coefficient, and x, yc, and yr are user query, chosen response, and rejected response, respectively. The policy model πθ is initialized from model π0. After that, the BCO loss [52] is employed as the quality loss, which helps the model to understand the absolute quality of individual responses: Lq = L+ + , (11) and where L+ represent the loss for chosen and rejected responses, respectively. They are calculated independently, requiring the model to differentiate the absolute quality of individual responses. The loss terms are given by: L+ = log σ (cid:18) β log πθ (yc x) π0 (yc x) (cid:19) δ , q = log σ (cid:18) (cid:18) β log πθ (yr x) π0 (yr x) (cid:19)(cid:19) δ , (12) (13) where δ represents the reward shift, calculated as the moving average of previous rewards to stabilize training. Finally, the LM loss is used as the generation loss to help the model learn the generation process of preferred responses. The loss function is defined in Equation 6. Data. For SFT data, we construct the training corpora based on those used in InternVL2.5 [18] while introducing additional tool usage, 3D scene understanding, GUI operations, scientific diagrams, creative writing, and multimodal reasoning samples. As result, the number of training samples grows from 16.3M in InternVL2.5 to 21.7M in InternVL3. For MPO data, we construct preference pairs based on the data pipeline and samples proposed in MMPR v1.2 [123], which cover wide range of domains, including general visual question answering (VQA) [42, 49, 89, 82, 126, 125], science [56, 16, 81], chart [90, 53, 11], mathematics [71, 103, 10, 80, 54, 39, 146, 105], OCR [91, 106, 9, 48, 95], and document [24]. We use the SFT versions of InternVL3-8B, 38B, and 78B to generate rollouts. During the MPO phase, all models are trained on the same dataset, which comprises about 300K samples."
        },
        {
            "title": "2.4 Test-Time Scaling",
            "content": "Test-Time Scaling has been shown to be an effective method to enhance the reasoning abilities of LLMs and MLLMs [107, 93, 86, 69, 119, 35, 151, 124]. In this work, we use the Best-of-N evaluation strategy and employ VisualPRM-8B [124] as the critic model to select the best response for reasoning and mathematics evaluation. Visual Process Reward Model. VisualPRM first assigns quality score to each step of the given solution and then averages these scores to obtain the overall score for this solution. This process is formulated as multi-turn chat task so that we can effectively leverage the generation ability of MLLMs. The image I, question q, and the first step s0 of the step-by-step solution = {s0, s1, , sn} to this question are included in the first turn and new step is presented in each subsequent turn. During the training stage, the model is required to predict the correctness of the given step in each turn as follows: ci (yi I, q, si), (14) where ci {+, } denotes the correctness of i-th step. During the inference stage, the score for each step is defined as the probability of generating +. Data. VisualPRM400K [124] is used to train VisualPRM, which is constructed based on multimodal questions collected from MMPR v1.2 [123]. Following the data pipeline in VisualPRM400K, we further expand VisualPRM400K by sampling rollouts from the 8B and 38B variants of InternVL3."
        },
        {
            "title": "2.5 Infrastructure",
            "content": "To facilitate model training, we extend the InternEVO framework [15]originally designed to optimize the Zero Redundancy Optimizer (ZeRO) for large-scale LLM trainingto support the training of our InternVL models. This extension enables efficient scaling to hundreds of billions of parameters across thousands of GPUs. The enhanced framework introduces flexible and decoupled sharding strategies for the ViT, MLP, and LLM components, significantly improving training efficiency by overlapping communication and computation. It further supports comprehensive range of parallelism strategiesincluding data, tensor, sequence, and pipeline parallelismas well as their arbitrary combinations. key challenge in MLLM training is the imbalance in computational load caused by the varying proportions of visual and textual tokens. Such imbalances can lead to inefficiencies by overburdening either the ViT or LLM modules. To address this, we introduce suite of techniques that dynamically balance computational workloads across modules, ensuring efficient and equitable resource utilization. For InternVL models of varying scales, the extended InternEVO framework formulates an optimization objective that identifies the optimal configuration to minimize both memory consumption and communication overhead across different module dimensions. To support sequences of up to 32K tokens, our approach incorporates both head-parallel and sequence-parallel techniques, effectively overcoming scalability bottlenecks while preserving computational efficiency. Compared to the training of InternVL2.5, the application of InternEVO in InternVL3 results in training speedup of 50% to 200% for models of comparable size, given the same computational budget."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we first compare the overall multimodal capabilities of InternVL3 with those of current advanced MLLMs using widely adopted multimodal benchmarks. Subsequently, we evaluate the performance of InternVL3 in various domains, including multimodal reasoning, mathematics, optical character recognition (OCR), chart and document understanding, multi-image understanding, real-world comprehension, comprehensive multimodal evaluation, multimodal hallucination evaluation, visual grounding, multimodal multilingual understanding, video understanding, and other multimodal tasks. Additionally, we provide detailed evaluation of the language capabilities of InternVL3. Finally, we analyze the advantages of several key modifications in InternVL3 compared to its predecessor, InternVL2.5, including the naive multimodal pre-training, the V2PE positional encoding, and the improvements brought by the post-training technique."
        },
        {
            "title": "3.1 Overall Comparison to Other Advanced MLLMs",
            "content": "Figure 1 provides detailed assessment of InternVL3s performance across diverse set of benchmarks, including MMMU [140], MathVista [79], AI2D [56], ChartQA [90], DocVQA [92], InfographicVQA [91], 7 HallusionBench [44], OCRBench [75], and LongVideoBench [128]. Compared with previous models, InternVL3 demonstrates substantial improvements across wide range of task categories. These advancements can be primarily attributed to enhanced training strategies, refined testing methodologies, and the expanded training corpus. More specifically, InternVL3 achieves an impressive score of 72.2 on the MMMU benchmark, underscoring its superior capacity to manage complex multimodal challenges. Beyond its performance on MMMU, InternVL3 consistently outperforms earlier versions of the InternVL series on variety of tasks, thereby emphasizing its broad applicability to real-world scenarios that require sophisticated multimodal comprehension and reasoning. In addition to surpassing its open-source counterparts, InternVL3 exhibits competitive performance relative to leading closed-source commercial models, such as ChatGPT-4o-latest [97] and Claude-3.5 Sonnet [3]. In many cases, the performance gap between InternVL3 and these proprietary models is notably narrowedand in certain benchmarks, such as AI2D and ChartQA, InternVL3 even surpasses them. Nonetheless, our results further reveal that Gemini2.5 Pro [116] maintains performance edge on select tasks (e.g., on HallusionBench), indicating that despite the notable progress in InternVL3, there remains room for further refinement of our InternVL series."
        },
        {
            "title": "3.2 Multimodal Reasoning and Mathematics",
            "content": "To comprehensively evaluate the multimodal reasoning and mathematical capabilities of InternVL3, we conduct experiments on series of benchmarks, including MMMU [140] for multidisciplinary reasoning, MathVista [79], MathVision [118], MathVerse [145] for mathematical reasoning, as well as DynaMath [154], WeMath [98] and LogicVista [130] for complementary evaluation on logical reasoning. As shown in Table 2, InternVL3 exhibits strong performance across all tested benchmarks. Specifically, on the MMMU benchmark, InternVL3-based models consistently outperform smaller-scale competitors. For instance, with increasing model size, InternVL3-78B reaches score over 72 on MMMU, indicating robust understanding and reasoning capability in handling abstract multidisciplinary concepts. In the mathematical domain, InternVL3 demonstrates significant gains across various benchmarks. On MathVista, InternVL3-78B records performance close to 79.0, while on MathVision and MathVerse, the results are also competitive, evidencing the models enhanced ability to tackle challenging mathematical problems. Furthermore, performance on DynaMath, WeMath, and LogicVista consistently improves with scaling. The overall scorea mean calculated across all benchmarksshows that InternVL3 models achieve balanced enhancement across different aspects, surpassing many of the preceding open-source methods. notable characteristic of InternVL3 is the efficiency of the best-of-N evaluation strategy [124]. When applying this method, even models with relatively smaller parameter sizes (e.g., InternVL3-1B and InternVL3-2B) exhibit substantial improvements in reasoning performance. Specifically, in the Vision-Only split of MathVerse, the best-of-8 strategy leads to increases of approximately 6.0 and 3.2 percentage points for InternVL3-38B and InternVL3-78B, respectively. This improvement underscores the effectiveness of test-time scaling."
        },
        {
            "title": "3.3 OCR, Chart, and Document Understanding",
            "content": "To assess the models integrated visionlanguage understanding in tasks involving text, document, and chart comprehension, we perform comprehensive evaluation over nine benchmarks, including AI2D [56], ChartQA [90], TextVQA [106], DocVQA [92], InfoVQA [91], OCRBench [75], SEED-2-Plus [60], CharXiv [127], and VCR [147]. As illustrated in Table 3, the InternVL3 series not only maintains robust performance across these benchmarks but also demonstrates competitive or superior results when compared to other open-source and closed-source counterparts. At the 1B scale, InternVL3-1B achieves performance that is roughly on par with previous lower-scale models. At the 2B scale, InternVL3-2B not only improves its absolute scoresfor instance, reaching 78.7/87.4 on AI2D and 88.3 on DocVQAbut also exhibits performance edge over similarly parameterized models such as Qwen2-VL-2B [120]. Although its TextVQA performance (77.0) remains comparable to that of Qwen2-VL-2B, the enhancements in document and chart understanding suggest that the proposed native multimodal pre-training are particularly effective in tasks requiring precise visualtextual integration. The benefits of the new pre-training protocol become even more pronounced at larger scales. Mid-scale models like InternVL3-8B and InternVL3-9B deliver substantial gains, with InternVL3-8B achieving 85.2/92.6 on AI2D, 92.7 on DocVQA, and VCR scores of 94.5/98.1. Moreover, when compared with heavyweight systems such as Qwen2-VL-72B [120] or even closed-source models like GPT-4o-20240513 [96], the high-scale variants 8 Model LLaVA-OV-0.5B [59] InternVL2.5-1B [18] InternVL3-1B w/ VisualPRM-Bo8 [124] Aquila-VL-2B [43] Qwen2.5-VL-3B [7] Ovis-2B [83] Ovis-4B [83] InternVL2.5-2B [18] InternVL2.5-4B [18] InternVL3-2B w/ VisualPRM-Bo8 [124] LLaVA-OV-7B [59] MiniCPM-V2.6 [134] MiniCPM-o2.6 [134] Ovis-8B [83] Qwen2.5-VL-8B [7] InternVL2.5-8B [18] InternVL3-8B w/ VisualPRM-Bo8 [124] InternVL3-9B w/ VisualPRM-Bo8 [124] Ovis2-16B [83] InternVL2.5-26B [18] InternVL3-14B w/ VisualPRM-Bo8 [124] Cambrian-34B [115] VILA-1.5-40B [70] Ovis2-34B [83] InternVL2.5-38B [18] InternVL3-38B w/ VisualPRM-Bo8 [124] GPT-4o-20241120 [96] Claude-3.7-Sonnet [3] Gemini-2.0-Flash [30] Gemini-2.0-Pro [29] LLaVA-OV-72B [59] QvQ-72B-Preview [114] Qwen2.5-VL-72B [7] InternVL2.5-78B [18] InternVL3-78B w/ VisualPRM-Bo8 [124] MMMU MathVista MathVision MathVerse DynaMath WeMath LogicVista Overall 31.4 41.2 43.4 55.4 46.9 51.2 45.6 49.0 43.2 51.8 48.6 57.8 47.9 49.8 50.9 57.4 55.0 56.2 62.7 66.0 57.7 63.7 60.7 60.7 67.1 69.3 49.7 55.1 66.7 63.9 70.1 71.0 70.7 75.0 72.6 69.9 55.7 70.3 68.2 70.0 72.2 72.2 34.8 47.1 45.8 62.1 59.1 61.2 64.1 69.6 51.1 64.1 57.0 70.5 58.6 60.8 73.3 71.8 67.8 64.5 71.6 75.2 71.5 76.2 73.7 68.2 75.1 77.9 53.2 49.5 76.1 71.9 75.1 79.4 60.0 66.8 70.4 71.3 67.1 70.3 74.2 72.3 79.0 80.5 21.1 18.8 21.7 17.9 21.9 17.7 21.5 14.0 18.4 21.7 26.6 18.3 23.4 21.7 25.9 25.4 17.0 29.3 37.5 27.6 33.9 30.1 23.4 37.2 40.1 31.9 32.2 34.2 41.8 31.2 41.9 43.6 48.1 25.3 34.9 39.3 32.2 43.1 40.8 16.4 18.7 28.9 17.4 31.2 29.4 38.5 22.3 27.7 25.3 36.7 19.3 18.9 35.0 42.3 41.1 22.8 39.8 46.3 35.3 45.8 45.8 24.0 44.4 47.7 50.1 36.9 48.2 54.2 40.6 46.7 47.8 67.3 27.2 48.2 47.3 39.2 51.0 54.2 5.6 5.8 13.4 5.0 13.2 10.0 18.0 4.4 15.2 14.6 21.4 9.0 9.8 10.4 20.4 21.0 9.4 25.5 28.5 26.7 29.1 26.3 11.4 31.3 33.1 27.5 20.0 35.3 36.1 34.5 39.7 42.1 43.3 15.6 30.7 35.9 19.2 35.1 37.3 11.1 13.4 28.5 15.9 22.9 9.9 16.9 8.0 21.2 22.4 38.5 20.9 16.4 25.2 27.2 35.2 23.5 37.1 48.1 33.8 46.6 45.0 30.9 43.0 52.0 51.9 38.3 48.6 55.2 45.8 49.3 47.4 56.5 32.0 39.0 49.1 39.8 46.1 52. 26.0 29.8 34.9 30.6 40.3 34.7 35.3 27.3 34.2 36.9 40.5 33.3 27.5 36.0 39.4 44.1 36.0 44.1 49.7 49.2 50.6 47.4 39.6 51.2 56.2 49.9 47.9 58.4 58.4 52.8 58.2 52.3 53.2 40.9 58.2 55.7 49.0 55.9 57.9 24.1 25.1 35.0 27.5 34.6 30.2 35.5 24.3 33.2 32.4 41.7 29.6 29.5 36.1 40.6 41.4 32.8 44.3 50.2 43.1 49.4 47.0 36.9 49.9 53.8 50.6 44.4 52.8 56.6 47.9 53.9 53.7 58.5 37.7 50.2 52.8 46.0 54.6 56.5 Table 2: Comparison of multimodal reasoning and mathematical performance. MMMU [140] is multidisciplinary reasoning benchmark. MathVista [79], MathVision [118], MathVerse [145], DynaMath [154], and WeMath [98] are mathematics benchmarks. For MathVerse, we report the performance on Vision-Only split. LogicVista [130] is logical reasoning benchmark. Part of the results are collected from the OpenCompass leaderboard [26]. The overall score is the average score of the above benchmarks. w/ VisualPRM-Bo8 denotes that the model is evaluated with Best-of-8 settings, where VisualPRM [124] serves as the critic model. of InternVL3particularly InternVL3-38B and InternVL3-78Bpush the envelope further. For instance, InternVL3-78B attains remarkable OCRBench score of 906 and VCR scores of 96.0/98.6, clearly surpassing the corresponding metrics of comparable models."
        },
        {
            "title": "3.4 Multi-Image Understanding",
            "content": "we evaluate the multi-image relation perception and understanding capabilities of InternVL3 across suite of widely recognized benchmarks, including BLINK [38], Mantis-Eval [50], MMIU [94], MuirBench [117], MMT-Bench [136], and MIRB [152], as presented in Table 4. These benchmarks comprehensively assess skills such as cross-image reasoning and context integration, all of which are crucial for effective multimodal interaction. InternVL3 consistently outperforms its earlier counterparts across different parameter scales. For instance, at the 1B scale, InternVL3-1B exhibits modest yet consistent improvement over preceding models, achieving BLINK score of 42.9 and an MMT-Bench score of 52.9. The performance gains become even more pronounced 9 Model Name AI2D (w / wo M) 64.1 / 70.5 69.3 / 77.8 69.4 / 78.3 74.7 / 84.6 81.6 / 75.0 / 74.1 / 82.3 74.9 / 83.5 78.7 / 87.4 84.4 / 82.1 / / 93.2 83.0 / 92.1 83.9 / 83.8 / 91.7 84.5 / 92.8 85.2 / 92.6 84.6 / 92.9 86.0 / 93.7 80.7 / 89.8 84.5 / 92.5 86.4 / 94.4 LLaVA-OneVision-0.5B [59] 57.1 / InternVL2-1B [19] InternVL2.5-1B [18] InternVL3-1B Qwen2-VL-2B [120] Qwen2.5-VL-3B [7] Aquila-VL-2B [43] InternVL2-2B [19] InternVL2.5-2B [18] InternVL3-2B Ovis1.6-Gemma2-9B [83] MiniCPM-V2.6 [134] Molmo-7B-D [31] Qwen2-VL-7B [120] Qwen2.5-VL-7B [7] InternVL2-8B [19] InternVL2.5-8B [18] InternVL3-8B InternVL3-9B InternVL3-14B InternVL-Chat-V1.5 [19] InternVL2-26B [19] InternVL2.5-26B [18] Qwen2.5-VL-32B [7] Cambrian-34B [115] VILA-1.5-40B [70] InternVL2-40B [19] InternVL2.5-38B [18] InternVL3-38B GPT-4V [96] GPT-4o-20240513 [96] Claude-3-Opus [3] Claude-3.5-Sonnet [3] Gemini-1.5-Pro [101] LLaVA-OneVision-72B [59] 85.6 / NVLM-D-72B [28] Molmo-72B [31] Qwen2-VL-72B [120] Qwen2.5-VL-72B [7] InternVL2-Llama3-76B [19] 87.6 / 94.8 InternVL2.5-78B [18] 89.1 / 95.7 89.7 / 96.0 InternVL3-78B 79.5 / 69.9 / 86.6 / 94.5 87.6 / 95.1 88.9 / 95.5 78.2 / 89.4 84.6 / 94.2 70.6 / 88.1 81.2 / 94.7 79.1 / 94.4 85.2 / 94.2 / 96.3 88.1 / 88.7 / ChartQA (test avg) 61.4 72.9 75.9 75.3 73.5 84.0 76.5 76.2 79.2 80.2 82.4 84.1 83.0 87.3 83.3 84.8 86.6 86.2 87.3 83.8 84.9 87.2 75.6 67.2 86.2 88.2 89.2 78.5 85.7 80.8 90.8 87.2 83.7 86.0 87.3 88.3 89.5 88.4 88.3 89. TextVQA (val) 70.5 72.0 74.1 79.7 79.3 76.4 73.4 74.3 77.0 80.1 81.7 84.3 84.9 77.4 79.1 80.2 79.4 80.5 80.6 82.3 82.4 76.7 73.6 83.0 82.7 83.9 78.0 77.4 67.5 74.1 78.8 80.5 82.1 83.1 85.5 83.5 84.4 83.4 84.3 DocVQA (test) 70.0 81.7 84.8 81.9 90.1 93.9 85.0 86.9 88.7 88.3 90.8 92.2 94.5 95.7 91.6 93.0 92.7 93.6 94.1 90.9 92.9 94.0 94.8 75.5 93.9 95.3 95.4 88.4 92.8 89.3 95.2 93.1 91.3 92.6 93.5 96.5 96.4 94.1 95.1 95.4 InfoVQA (test) 41.8 50.9 56.0 53.7 65.5 77.1 58.3 58.9 60.9 66.1 72.6 76.5 82.6 74.8 77.6 76.8 79.6 83.6 72.5 75.9 79.8 83.4 46.0 78.7 83.6 85.0 75.1 79.2 55.6 74.3 81.0 74.9 81.9 84.5 87.3 82.0 84.1 86.5 OCR Bench 565 754 785 790 809 797 772 784 804 835 830 852 694 866 864 794 822 880 877 875 724 825 852 600 460 837 842 886 645 736 694 788 754 741 853 877 885 839 854 906 SEED-2 Plus 54.3 59.0 58.2 62.4 67.6 63.0 60.0 60.9 64.6 65.7 69.0 70.4 67.5 69.7 69.7 68.8 70.3 66.3 67.6 70.8 69.2 71.2 71.6 53.8 72.0 44.2 71.7 73.0 69.7 71.3 71.9 CharXiv (RQ / DQ) 18.1 / 30.7 19.0 / 38.4 21.0 / 47.1 31.3 / 58.6 21.0 / 40.6 21.3 / 49.7 28.3 / 54.7 31.0 / 57.1 42.5/73.9 31.2 / 56.1 32.9 / 68.6 37.6 / 73.6 38.0 / 72.5 43.1 / 82.2 29.2 / 58.5 33.4 / 62.4 35.9 / 73.5 27.3 / 59.7 24.0 / 38.7 32.3 / 66.0 42.4 / 79.6 46.4 / 87.2 37.1 / 79.9 47.1 / 84.5 30.2 / 71.6 60.2 / 84.3 43.3 / 72.0 49.7 / 87.4 38.9 / 75.2 42.4 / 82.3 46.0 / 85. VCR-EN-Easy (EM / Jaccard) 21.5 / 48.4 91.5 / 97.0 89.3 / 96.2 81.5 / 70.0 / 32.9 / 59.2 93.2 / 97.6 91.2 / 96.9 73.9 / 85.7 89.7 / 93.8 37.9 / 61.5 92.6 / 97.4 94.5 / 98.1 94.2 / 97.9 94.8 / 98.2 14.7 / 51.4 74.5 / 86.7 94.4 / 98.0 79.7 / 89.3 84.7 / 92.6 94.7 / 98.2 96.1 / 98.7 52.0 / 65.4 91.6 / 96.4 62.0 / 77.7 63.9 / 74.7 62.7 / 77.7 91.3 / 94.6 83.2 / 91.3 95.7 / 94.5 96.0 / 98.6 Overall 54.9 68.3 68.6 62.0 72.1 74.7 69.7 79.6 81.3 81.3 83.4 65.9 76.7 81.8 79.3 83.6 85.5 70.0 81.6 67.3 78.7 81.1 83.9 85.8 Table 3: Comparison of OCR, chart, and document understanding performance. We evaluate OCRrelated capabilities across 9 benchmarks, including AI2D [56], ChartQA [90], TextVQA [106], DocVQA [92], InfoVQA [91], OCRBench [75], SEED-2-Plus [60], CharXiv [127], and VCR [147]. Part of results are collected from [33, 31, 3, 127, 147] and the OpenCompass leaderboard [26]. at the 2B scale; InternVL3-2B attains remarkable 65.9 on Mantis-Eval, representing an improvement of over 11 points relative to InternVL2.5-2B, and also boosts its MMT-Bench performance to 59.5. Such enhancements indicate that the advanced pre-training strategies and enhanced training datasets in InternVL3 significantly elevate its capability to capture and reason over inter-image relationships. At higher scales, the trend continues. InternVL3-8B and its subsequent larger variants not only secure steady improvements on BLINK and MMT-Bench but also demonstrate substantial gains on the MIRB and MuirBench benchmarks. In particular, InternVL3-78B reaches BLINK score of 66.3 and an MMT-Bench score of 73.2, positioning it as competitive alternative to leading closed-source models like GPT-4o. These results suggest that the learning multimodal capabilities via native multimodal pre-training and the scaling of model parameters are key contributors to the elevated performance observed across diverse evaluation settings. Despite these encouraging outcomes, noticeable performance gap between our InternVL3 and other MLLMs like Qwen2.5VL still exists on certain benchmarks, such as MuirBench, implying that future work may benefit from further enhancements in training data curation and additional model refinements. 10 Model Name LLaVA-OneVision-0.5B [59] InternVL2-1B [19] InternVL2.5-1B [18] InternVL3-1B Qwen2-VL-2B [120] Qwen2.5-VL-3B [6] InternVL2-2B [19] InternVL2.5-2B [18] InternVL3-2B Qwen2-VL-7B [120] Qwen2.5-VL-7B [6] MiniCPM-V2.6 [134] InternVL2-8B [19] InternVL2.5-8B [18] InternVL3-8B InternVL3-9B InternVL3-14B InternVL-Chat-V1.5 [19] InternVL2-26B [19] InternVL2.5-26B [18] Cambrian-34B [115] InternVL2-40B [19] InternVL2.5-38B [18] InternVL3-38B GPT-4V [96] GPT-4o-20240513 [96] Claude-3.5-Sonnet [3] Gemini-1.5-Pro [101] LLaVA-OneVision-72B [59] Qwen2-VL-72B [120] Qwen2.5-VL-72B [6] InternVL2-Llama3-76B [19] InternVL2.5-78B [18] InternVL3-78B BLINK (val) 52.1 38.6 42.0 42.9 44.4 47.6 43.8 44.0 50.3 53.2 56.4 53.0 50.9 54.8 55.5 58.6 60.3 46.6 56.2 61.8 57.2 63.2 64.0 54.6 68.0 55.4 64.4 56.8 63.8 66. Mantis Eval 39.6 46.1 51.2 50.2 48.4 54.8 65.9 69.0 65.4 67.7 70.1 70.1 76.0 66.8 69.6 75.6 71.4 78.3 77.9 62.7 77.6 73.7 77.0 79.3 MMIU 37.3 38.5 39.3 39.8 43.5 43.0 42.0 46.7 46.8 50.4 50.9 37.4 42.6 49.4 47.9 55.3 57.4 55.7 53.4 53.4 44.2 55.8 60.4 Muir Bench 25.5 29.3 29.9 31.2 47.7 32.5 40.6 38.8 59.6 48.7 51.1 55.0 51.4 56.2 38.5 50.6 61.1 54.4 62.7 63.8 62.3 68.0 54.8 70.7 51.2 63.5 64.5 MMT (val) 49.5 50.3 52.9 55.1 50.4 54.5 59.5 64.0 60.8 60.0 62.3 65.0 65.4 70.3 58.0 60.6 66.9 66.2 70.0 71.8 64.3 65.4 64.5 71.8 67.4 70.8 73.2 MIRB (avg) 31.5 35.6 36.1 32.1 36.4 42.9 50.0 52.5 56.8 58.6 59.3 50.3 53.7 55.7 55.2 61.2 62.3 53.1 58.2 61.1 64. Overall 38.7 41.3 42.1 41.2 45.6 50.1 52.8 55.9 58.2 59.1 62.2 49.6 55.6 61.8 58.7 65.1 66.2 58.6 65.3 68.0 RealWorld QA 55.6 50.3 57.5 58.2 62.6 65.4 57.3 60.1 64.3 70.1 68.5 65.0 64.4 70.1 70.8 70.5 70.7 66.0 68.3 74.5 67.8 71.8 73.5 75.6 61.4 75.4 60.1 67.5 71.9 77.8 75.7 72.2 78.7 78.0 MME-RW (EN) 40.2 44.2 46.0 53.1 47.3 48.8 53.8 56.5 57.4 53.5 59.1 62.0 61.3 64.0 49.4 58.7 61.8 44.1 61.8 64.0 67.3 45.2 51.6 38.2 63.2 63.0 62.9 65.4 WildVision (win rate) 17.8 43.4 43.8 31.8 44.2 48.8 54.4 62.0 69.8 63.8 69.8 56.6 62.2 65.2 63.2 66.4 71.6 71.8 80.6 65.8 71.4 73.6 R-Bench (dis) 55.6 59.0 60.4 56.8 62.2 67.5 64.0 67.9 70.1 74.1 70.3 69.3 67.9 70.1 72.9 73.3 72.1 73.3 65.6 77.7 74.1 77.2 77. Overall 41.0 51.0 52.1 48.3 53.8 58.6 60.1 65.3 69.2 66.5 68.5 60.0 64.8 68.6 67.5 69.0 72.0 69.7 68.8 72.6 73.6 Table 4: Comparison of multi-image and real-world understanding performance. Multi-image benchmarks include BLINK [38], Mantis-Eval [50], MMIU [94], MuirBench [117], MMT-Bench [136], and MIRB [152]. Real-world benchmarks encompass RealWorldQA [27], MME-RealWorld [150], WildVision [85], and RBench [61]. Part of the results are sourced from the benchmark papers and the OpenCompass leaderboard [26]."
        },
        {
            "title": "3.5 Real-World Comprehension",
            "content": "We evaluate the InternVL3 series on four real-world comprehension benchmarksRealWorldQA [27], MMERealWorld [150], WildVision [85], and R-Bench [61]to assess its ability to tackle realistic and complex tasks. As shown in Table 4, even the smallest variant in the InternVL3 family (InternVL3-1B) demonstrates promising performance with RealWorldQA score of 58.2, an MME-RealWorld score of 46.0, WildVision win rate of 43.8, and an R-Bench score of 60.4. Scaling up the model yields further enhancements across all metrics. Mid-sized variants such as InternVL3-8B and InternVL3-14B continue this positive trend, with InternVL3-8B reporting RealWorldQA score of 70.8 and an R-Bench score of 74.1. These improvements highlight the effectiveness of scaling, as larger models provide more robust representations and enhanced comprehension capabilities in real-world scenarios. At the higher end of the scale, the InternVL3-38B and InternVL3-78B models achieve top-tier results among the InternVL3 series. Notably, InternVL3-78B records RealWorldQA score of 78.0, an MME-RealWorld score of 65.4, WildVision win rate of 73.6, and an R-Bench score of 77.4. When compared with competitive models, such as GPT-4o [96]which scores 75.4 on RealWorldQA and 80.6 on WildVisionthe InternVL3 series exhibits competitive strengths. InternVL3-78B not only surpasses GPT-4o on RealWorldQA and closely matches its R-Bench performance but also considerably outperforms it on MME-RealWorld, indicating an overall robust performance on tasks demanding both perceptual precision and comprehensive understanding. 11 Model Name MME (sum) MMB (EN / CN) LLaVA-OneVision-0.5B [59] 1438.0 61.6 / 55.5 1794.4 65.4 / 60.7 InternVL2-1B [19] 1950.5 70.7 / 66.3 InternVL2.5-1B [18] 1934.4 72.6 / 67.9 InternVL3-1B 1872.0 74.9 / 73.5 Qwen2-VL-2B [120] 2157 79.1 / 78.1 Qwen2.5-VL-3B [6] 1876.8 73.2 / 70.9 InternVL2-2B [19] 2138.2 74.7 / 71.9 InternVL2.5-2B [18] 2221.2 81.1 / 78.4 InternVL3-2B 2326.8 83.0 / 80.5 Qwen2-VL-7B [120] 2347 83.5 / 83.4 Qwen2.5-VL-7B [6] 2348.4 81.5 / 79.3 MiniCPM-V2.6 [134] 2210.3 81.7 / 81.2 InternVL2-8B [19] 2344.1 84.6 / 82.6 InternVL2.5-8B [18] 2415.4 83.4 / 82.2 InternVL3-8B 2372.8 83.4 / 82.2 InternVL3-9B 2478.3 85.6 / 84.1 InternVL3-14B 2194.2 82.2 / 82.0 InternVL-Chat-V1.5 [19] 2260.7 83.4 / 82.0 InternVL2-26B [19] 2373.3 85.4 / 85.5 InternVL2.5-26B [18] 80.4 / 79.2 Cambrian-34B [115] 2307.5 86.8 / 86.5 InternVL2-40B [19] 2455.8 86.5 / 86.3 InternVL2.5-38B [18] 2523.6 87.6 / 86.8 InternVL3-38B 1926.6 81.0 / 80.2 GPT-4V [96] 83.4 / 82.1 GPT-4o-20240513 [96] Claude-3-Opus [3] 1586.8 63.3 / 59.2 82.6 / 83.5 Claude-3.5-Sonnet [3] Gemini-1.5-Pro [101] 73.9 / 73.8 LLaVA-OneVision-72B [59] 2261.0 85.8 / 85.3 2482.7 86.5 / 86.6 Qwen2-VL-72B [120] Qwen2.5-VL-72B [6] 2448.0 88.6 / 87.9 InternVL2-Llama3-76B [19] 2414.7 86.5 / 86.3 2494.5 88.3 / 88.5 InternVL2.5-78B [18] 2549.8 89.0 / 88.7 InternVL3-78B MMBv1.1 (EN) 59.6 61.6 68.4 69.9 72.2 77.4 70.2 72.2 78.6 80.7 82.6 78.0 79.5 83.2 81.7 81.7 83.5 80.3 81.5 84.2 78.3 85.1 85.5 86.9 80.0 83.1 60.1 80.9 74.6 85.0 85.9 88.4 85.5 87.4 87.7 MMVet (turbo) 32.2 32.7 48.8 59.5 49.5 61.8 39.5 60.8 62.2 62.0 67.1 60.0 54.2 62.8 81.3 76.2 80.2 61.5 62.1 65.0 53.2 65.5 68.8 83.9 67.5 69.1 51.7 70.1 64.0 60.6 74.0 76.2 65.7 72.3 81.3 MMVetv2 (0613) 36.1 43.2 47.5 39.6 52.3 53.9 52.3 58.1 66.3 65.4 68.4 51.5 57.2 60.8 63.8 62.1 69.6 66.3 71.0 55.8 71.8 66.9 66.9 68.4 65.5 70. MMStar Overall 37.7 45.7 50.1 51.5 48.0 55.9 50.1 53.7 60.7 60.7 63.9 57.5 62.0 62.8 68.2 66.3 68.8 57.3 61.2 66.5 54.2 65.4 67.9 71.5 56.0 64.7 45.7 65.1 59.1 65.8 68.3 70.8 67.4 69.5 72.5 51.7 58.9 61.9 58.0 65.3 69.8 69.2 73.2 77.7 76.3 79.0 69.7 71.8 75.2 75.7 77.0 81.5 70.7 55.5 78.7 77.2 79.2 82.0 HallBench (avg) 27.9 34.0 39.0 41.4 41.7 46.3 37.9 42.6 42.5 50.6 52.9 48.1 45.2 50.1 49.9 51.2 55.1 50.3 50.7 55.0 41.6 56.9 56.8 57.1 46.5 55.0 37.8 55.5 45.6 49.0 58.1 55.2 55.2 57.4 59.1 MMHal (score) 2.25 2.49 2.59 2.52 2.94 3.26 3.40 3.60 3.33 3.65 3.61 3.47 3.49 3.11 3.55 3.70 3.75 3.71 3.77 4.00 3.83 3.89 3.85 CRPE (relation) 57.5 60.9 64.0 73.6 66.3 70.2 71.5 74.4 76.4 75.2 75.8 78.4 76.3 75.0 77.3 75.4 75.6 79.1 77.6 78.3 77.1 76.6 79.2 77.6 78.8 79. POPE (avg) 87.3 89.9 90.7 88.3 90.6 89.6 88.1 87.3 86.9 90.6 91.1 90.4 90.2 88.4 88.0 90.6 88.4 90.7 90.6 86.9 89.0 90.8 90.3 Overall 45.3 48.1 49.7 48.8 51.6 51.7 54.1 53.6 52.8 55.7 55.2 55.0 56.5 54.3 54.5 57.1 56.7 57.4 57.1 55.6 56.4 57.7 58.1 Table 5: Comparison of comprehensive multimodal understanding and hallucination performance. Comprehensive multimodal benchmarks include MME [36], MMBench series [74], MMVet series [137, 138], and MMStar [13]. Hallucination benchmarks encompass HallusionBench [44], MMHal [110], CRPE [125], and POPE [66]. Part of the results are sourced from the benchmark papers and the OpenCompass leaderboard [26]."
        },
        {
            "title": "3.6 Comprehensive Multimodal Evaluation",
            "content": "The comprehensive multimodal evaluation is based on established benchmarks including MME [36], MMBench (evaluating both English and Chinese tasks) [74], MMBench v1.1 (English) [74], MMVet [137], MMVet v2 [138], and MMStar [13], as summarized in Table 5. Specifically, InternVL3-1B achieves an MMBench score of 72.6/67.9 (English/Chinese) and improves the MMBench v1.1 score to 69.9, compared to the InternVL2.5-1B baseline (70.7/66.3 and 68.4, respectively). The improvements become more pronounced at the 2B scale, where InternVL3-2B records an MME of 2221.2 and reaches an MMBench performance of 81.1/78.4, along with an MMBench v1.1 score of 78.6. At larger scales, InternVL3 models consistently demonstrate superior performance. For example, the InternVL38B model achieves an MME of 2415.4, while the InternVL3-38B and InternVL3-78B models record MME scores of 2523.6 and 2549.8, respectively. The corresponding MMBench and MMBench v1.1 scores also show steady improvements, with InternVL3-78B attaining 89.0/88.7 for English/Chinese and 87.7 for English-only tasks. When compared with other competitive models, such as Qwen2-VL-72B and Qwen2.5-VL-72B, the InternVL3 seriesespecially the 78B variantoffers consistent performance advantage on the multimodal understanding benchmarks. 12 Model Name Grounding-DINO-L [73] UNINEXT-H [132] ONE-PEACE [121] Qwen2.5-VL-3B [6] InternVL3-1B InternVL3-2B Shikra-7B [12] Ferret-v2-13B [143] CogVLM-Grounding [122] MM1.5 [142] Qwen2-VL-7B [120] Qwen2.5-VL-7B [6] TextHawk2 [139] InternVL2-8B [19] InternVL2.5-8B [18] InternVL3-8B InternVL3-9B InternVL3-14B Qwen2-VL-72B [120] Qwen2.5-VL-72B [6] InternVL2-Llama3-76B [19] InternVL2.5-78B [18] InternVL3-38B InternVL3-78B RefCOCO test-A 93.2 94.3 94.2 91.7 90.1 92.6 90.6 95.0 94.8 92.5 93.6 92.5 93.0 91.1 94.5 94.6 93.2 94.4 95.3 94.6 94.8 95.6 95.1 95. test-B 88.2 91.5 89.3 84.0 81.7 86.4 80.2 88.9 89.0 86.7 87.3 85.4 87.6 80.7 85.9 88.0 86.6 87.8 90.7 89.7 88.4 92.5 90.2 90.3 val 90.6 92.6 92.6 89.1 85.8 89.8 87.0 92.6 92.8 91.7 90.0 91.9 87.1 90.3 92.5 91.8 92.0 93.2 92.7 92.2 93.7 93.2 93.4 RefCOCO+ test-A 89.0 89.6 92.2 88.0 84.1 89.2 87.4 92.1 92.9 88.7 90.5 89.1 90.0 87.9 91.5 92.5 91.0 92.1 93.8 92.2 93.1 94.7 93.2 93.8 test-B 75.9 79.8 83.2 74.1 69.2 76.5 72.1 81.4 83.4 77.8 79.5 76.9 80.4 71.4 78.8 81.8 79.9 81.5 85.6 83.7 82.8 86.9 85.2 85.3 val 82.8 85.2 88.8 82.4 76.6 84.0 81.6 87.4 88.7 85.8 84.2 86.2 79.8 85.2 88.2 86.4 87.4 90.1 88.9 88.8 90.4 89.8 90.1 RefCOCOg test val 87.0 86.1 89.4 88.7 89.3 89.2 85.7 85.2 82.6 82.8 87.2 87.6 82.2 82.3 90.0 89.4 90.8 89.8 87.1 87.8 87.3 87.2 87.2 88.1 88.2 82.7 82.7 87.6 86.7 90.0 89.6 88.5 88.0 89.3 88.6 90.4 89.9 90.3 89.9 90.3 89.5 92.2 92.7 91.5 91.4 91.5 91. Overall 86.6 88.9 89.8 85.0 81.6 86.7 82.9 89.6 90.3 87.9 86.6 88.2 82.9 87.6 89.6 88.2 89.1 91.1 90.3 90.0 92.3 91.2 91.4 Table 6: Comparison of visual grounding performance. We evaluate InternVLs visual grounding capability on RefCOCO, RefCOCO+, and RefCOCOg datasets [55, 87]. Parts of the results are collected from [120]."
        },
        {
            "title": "3.7 Multimodal Hallucination Evaluation",
            "content": "We evaluate InternVLs propensity for hallucinations on four established benchmarksHallusionBench [44], MMHal-Bench [110], CRPE [125], and POPE [66]as detailed in Table 5. In comparison with previous InternVL series, the new InternVL3 models demonstrate overall competitive performance across varying scales, while providing consistent improvements in handling multimodal hallucination challenges. In the small-parameter regime, InternVL3-1B attains HallusionBench score of 41.4, representing an appreciable gain over the InternVL2.5-1B baseline, which scored 39.0. Similarly, the 2B variant of InternVL3 shows comparable HallusionBench performance (42.5) to its InternVL2.5 counterpart (42.6), while registering modest improvement in CRPE performance (71.5 vs. 70.2). In the large-scale setting, InternVL3-38B and InternVL3-78B are particularly noteworthy. InternVL3-38B obtains HallusionBench score of 57.1, while InternVL3-78B reaches 59.1, accompanied by CRPE improvement to 79.2. These figures position the InternVL3 series as competitive with leading closedand open-source models such as GPT-4o and the Qwen2.5-VL series. Despite these advancements, minor declines on certain benchmarks, such as MMHal, indicate that although the InternVL3 series has made overall progress, optimizing data and training strategies to achieve more consistent improvements remains an important direction for future work."
        },
        {
            "title": "3.8 Visual Grounding",
            "content": "We evaluate InternVLs visual grounding capability on the RefCOCO [55], RefCOCO+[55], and RefCOCOg[87] datasets, where the model is tasked with accurately localizing target objects in images from given textual descriptions. Table 6 shows comprehensive comparison across various models, including several specialized grounding models as well as multiple MLLLMs. Among the smaller-scale models, we observe that while Qwen2.5-VL-3B achieves an average score of 85.0, the InternVL3-1B and InternVL3-2B models yield average scores of 81.6 and 86.7, respectively. Notably, when scaling up, the InternVL3 series exhibits promising improvements. InternVL3-8B, InternVL3-9B, and InternVL3-14B yield average scores around 88.289.6, reflecting consistent trend of performance gains as the model size increases. However, when reaching larger scales, the performance gains appear to plateau. For instance, InternVL2.5-78B reaches an average score of 92.3, and InternVL3-78B only shows score of 91.4. We speculate that this is because InternVL3s training data expansion does not include additional grounding-specific data and the relative reduction in grounding-targeted data could have restricted the localization capabilities. 13 Model Name InternVL2-1B [19] InternVL2.5-1B [18] InternVL3-1B Qwen2-VL-2B [120] Qwen2.5-VL-3B [6] InternVL2-2B [19] InternVL2.5-2B [18] InternVL3-2B mPLUG-Owl2 [135] Qwen2-VL-7B [120] Qwen2.5-VL-7B [6] InternVL2-8B [19] InternVL2.5-8B [18] InternVL3-8B InternVL3-9B InternVL3-14B InternVL-Chat-V1.5 [19] InternVL2-26B [19] InternVL2.5-26B [18] InternVL2-40B [19] InternVL2.5-38B [18] InternVL3-38B GPT-4V [96] GPT-4o [96] Gemini-1.0-Pro [113] Qwen2-VL-72B [120] Qwen2.5-VL-72B [6] InternVL2-Llama3-76B [19] InternVL2.5-78B [18] InternVL3-78B en 73.2 78.8 79.4 78.3 79.4 81.4 81.9 67.3 83.9 83.4 84.3 85.1 84.8 85.7 82.6 83.8 86.2 85.3 86.4 86.7 75.0 75.0 86.8 85.3 86.3 87. zh 67.4 70.2 70.1 74.2 71.6 74.4 78.3 61.0 82.4 81.5 83.1 83.1 83.7 84.7 80.8 81.7 83.8 84.1 85.1 85.6 74.2 71.9 85.3 85.1 85.6 86.6 MMMB ar pt 53.5 55.5 55.0 61.5 58.0 62.3 68.3 72.6 43.5 54.0 48.3 58.2 68.6 75.4 45.8 59.7 79.0 81.2 66.3 76.1 69.3 78.6 81.6 82.5 69.9 80.6 83.7 83.1 65.2 76.3 68.8 78.0 73.3 81.6 70.3 81.1 84.3 84.1 84.8 84.5 73.5 71.5 69.9 70.6 84.8 85.2 82.8 82.8 84.8 85.1 86.5 85.5 tr 43.8 45.3 47.6 61.8 46.4 46.4 62.9 45.4 74.7 69.2 71.5 76.2 68.5 79.3 68.6 69.3 73.7 74.2 82.8 82.6 69.0 69.6 84.2 83.0 83.1 84.6 ru 55.2 61.1 61.9 72.8 48.1 53.2 74.6 62.6 82.4 75.7 79.5 83.4 80.8 83.6 74.0 76.3 82.8 81.4 84.9 85.1 73.1 72.7 85.3 83.7 85.4 86.1 en 67.9 72.5 72.6 72.1 73.8 76.5 81.3 66.2 81.8 82.9 83.8 85.5 86.5 86.7 81.1 82.7 86.1 86.2 87.5 89.0 77.6 73.6 86.9 87.8 90.0 89.4 Multilingual MMBench tr zh 31.8 61.2 37.8 64.7 39.5 66.2 54.4 71.1 31.3 69.6 33.9 71.6 59.5 77.8 47.7 59.4 74.5 81.6 66.0 81.8 67.8 83.2 75.9 85.6 68.3 85.2 80.7 85.8 66.7 80.2 69.6 81.8 75.0 85.5 74.2 85.8 84.0 88.6 84.3 89.3 70.5 74.4 69.8 72.1 84.4 87.2 85.0 87.3 84.9 89.7 86.6 90. pt 50.8 57.0 62.3 69.9 51.4 55.9 75.9 58.2 79.1 76.0 79.4 83.2 79.1 83.2 76.9 77.8 80.7 82.8 85.3 87.1 72.5 70.3 85.8 85.9 87.4 88.7 ar 43.3 43.0 48.0 61.1 29.8 37.3 66.4 37.9 75.6 60.5 64.3 79.2 64.3 81.1 56.2 61.9 67.5 64.0 84.5 84.6 72.3 61.1 83.5 83.1 83.3 86.1 MTVQA (avg) 12.6 21.4 22.2 20.0 24.8 10.9 21.8 26.7 25.6 29.2 20.9 27.6 30.2 27.1 31.6 20.5 17.7 28.5 20.6 31.7 32.4 22.0 27.8 30.9 31.7 22.0 31.9 32.5 ru 52.7 53.2 60.3 69.3 42.3 44.8 70.7 60.4 79.3 74.4 77.3 82.6 79.1 83.8 71.0 74.4 79.6 81.8 85.9 87.4 74.8 70.5 85.3 85.7 86.3 88.1 Overall 40.7 46.0 47.9 52.6 39.3 45.2 57.4 61.6 56.6 60.4 64.7 60.7 66.2 55.7 56.2 62.6 59.7 67.4 68.1 56.1 67.2 63.9 68.0 68. Table 7: Comparison of multimodal multilingual performance. We evaluate multilingual capabilities across 3 benchmarks, including MMMB [108], Multilingual MMBench [108] and MTVQA [112]. The languages evaluated are English (en), Chinese (zh), Portuguese (pt), Arabic (ar), Turkish (tr), and Russian (ru)."
        },
        {
            "title": "3.9 Multimodal Multilingual Understanding",
            "content": "We assess InternVLs multimodal multilingual understanding capabilities using benchmarksMMMB, Multilingual MMBench [108], and MTVQA [112]as shown in Table 7. The InternVL3 series demonstrates consistent improvements in multilingual performance compared to previous predecessors. For example, the lightweight InternVL3-1B already shows modest improvement over InternVL2.5-1B, while the larger-scale variants, such as InternVL3-38B and InternVL3-78B, achieve significantly higher average scores across all three benchmarks. Comparisons with other leading models further highlight the effectiveness of the InternVL3 series. Notably, the InternVL3 variants achieve performance that is competitive with or superior to models such as Qwen2VL-72B [120] and Qwen2.5-VL-72B [6]. Overall, the enhanced performance of the InternVL3 series across MMMB, Multilingual MMBench, and MTVQA underscores the promise of our approach in advancing global multimodal applications."
        },
        {
            "title": "3.10 Video Understanding",
            "content": "Video understanding is essential for evaluating how well MLLMs capture temporal and multimodal cues in complex video content. In this work, we assess the InternVL3 series on six established benchmarksVideoMME [37], MVBench [64], MMBench-Video [34], MLVU [153], LongVideoBench [128], and CG-Bench [2], as detailed in Table 8. Overall, the InternVL3 models demonstrate clear performance improvements and strong scalability trend over their predecessors. As the model capacity increases, the performance gains become more pronounced. For instance, InternVL3-2B records higher Video-MME scores (58.9/61.4) and improved MVBench and MLVU performance compared to the earlier 2B variants. 14 Model Name InternVL2-1B [19] InternVL2.5-1B [18] InternVL3-1B Qwen2-VL-2B [120] Qwen2.5-VL-3B [7] InternVL2-2B [19] InternVL2.5-2B [18] InternVL3-2B VideoChat2-HD [63] MiniCPM-V-2.6 [134] LLaVA-OneVision-7B [59] Qwen2-VL-7B [120] Qwen2.5-VL-7B [7] InternVL2-8B [19] InternVL2.5-8B [18] InternVL3-8B InternVL3-9B InternVL3-14B InternVL2-26B [19] InternVL2.5-26B Oryx-1.5-32B [77] Qwen2.5-VL-32B [7] VILA-1.5-40B [70] InternVL2-40B [19] InternVL2.5-38B [18] InternVL3-38B GPT-4V/4T [1] GPT-4o-20240513 [96] GPT-4o-20240806 [96] Gemini-1.5-Pro [101] VideoLLaMA2-72B [23] LLaVA-OneVision-72B [59] Qwen2-VL-72B [120] Qwen2.5-VL-72B [7] InternVL2-Llama3-76B [19] InternVL2.5-78B [18] InternVL3-78B Video-MME (wo / sub) 42.9 / 45.4 50.3 / 52.3 51.0 / 53.0 55.6 / 60.4 61.5 / 67.6 46.2 / 49.1 51.9 / 54.1 58.9 / 61.4 45.3 / 55.7 60.9 / 63.6 58.2 / 63.3 / 69.0 65.1 / 71.6 56.3 / 59.3 64.2 / 66.9 66.3 / 68.9 66.7 / 68.9 70.4 / 73.0 57.0 / 60.2 66.9 / 69.2 67.3 / 74.9 70.5 / 77.9 60.1 / 61.1 66.1 / 68.6 70.7 / 73.1 72.7 / 75.0 59.9 / 63.3 71.9 / 77.2 75.0 / 81.3 61.4 / 63.1 66.2 / 69.5 71.2 / 77.8 73.3 / 79.1 64.7 / 67.8 72.1 / 74.0 72.7 / 75. MVBench 57.5 64.3 63.1 63.2 67.0 60.2 68.8 70.4 62.3 56.7 67.0 69.6 65.8 72.0 75.4 74.3 76.6 67.5 75.2 70.1 72.0 74.4 76.9 43.7 62.0 59.4 73.6 70.4 69.6 76.4 78.7 MMBench-Video (val) 1.14 1.36 1.3 1.63 1.30 1.44 1.42 1.22 1.70 1.44 1.79 1.57 1.68 1.69 1.69 1.73 1.67 1.86 1.52 1.93 1.61 1.78 1.82 1.81 1.53 1.63 1.87 1.30 1.70 2.02 1.71 1.97 1.81 MLVU (M-Avg) 51.6 57.3 53.0 68.2 54.3 61.4 64.2 47.9 70.2 64.0 68.9 71.4 70.8 73.3 64.2 72.3 72.3 56.7 71.0 75.3 77.8 49.2 64.6 66.4 74.6 69.9 75.7 79.5 LongVideoBench (val total) 43.3 47.9 48.1 43.3 46.0 52.0 55.4 54.9 55.6 45.3 54.6 60.0 58.8 62.5 63.9 56.1 59.9 60.6 63.3 67.3 59.1 66.7 64.0 61.3 60.7 61.1 63.6 65.7 CG-Bench (long / clue acc.) 24.8 / 39.1 30.8 / 50.7 38.6 / 55.2 41.1 / 58.0 44.1 / 60.6 46.9 / 62.8 41.8 / 58.3 40.1 / 56.4 41.3 / 56.2 42.2 / 58.5 48.4 / 65. Overall 46.9 54.9 61.4 62.3 64.9 67.5 66.0 68.3 Table 8: Comparison of video understanding performance. We evaluate InternVLs video understanding capabilities across 6 benchmarks. For Video-MME [37], MMBench-Video [34], MLVU [153], and LongVideoBench [128], we test with four different settings: 16, 32, 48, and 64 frames, and report the maximum results. For MVBench [64], we conduct testing using 16 frames. For CG-Bench [2], we use 32 frames. The scaling behavior of the InternVL3 series is further evident in the larger models. InternVL3-14B attains Video-MME score of 70.4/73.0, while InternVL3-38B and InternVL3-78B push these metrics even higher, reaching scores of 72.7/75.0 and 72.7/75.7, respectively. Additionally, the inclusion of CG-Bench evaluations for the InternVL3 series provides further insight into long-range video reasoning, with performance steadily improving as model size increasesfor example, InternVL3-78B attains 48.4/65.3 on CG-Bench. When compared with other open-source models, the InternVL3 series demonstrates competitive advantages. For instance, while Qwen2.5-VL models achieve impressive results (with Qwen2.5-VL-72B scoring 73.3/79.1 on Video-MME), the InternVL3 series tends to outperform them in other metrics, such as MVBench and MLVU. Similarly, while closed-source systems like Gemini-1.5-Pro sometimes yield superior results on select benchmarks (e.g., Video-MME), the overall performance of InternVL3, especially at larger scales, is highly competitive."
        },
        {
            "title": "3.11 GUI Grounding",
            "content": "GUI grounding requires precise localization and understanding of interface elements, which is critical for applications like automated UI testing and assistive technologies. In Table 9, we report the performance on GUI grounding benchmarks, comparing InternVL3 with state-of-the-art multimodal and GUI-specific models. The results demonstrate that InternVL3 achieves competitive performance across different scales. On 15 Method GPT-4o Gemini 2.0 Claude Aguvis-72B Qwen2.5-VL-72B UI-TARS-72B InternVL3-8B -38B -72B ScreenSpot ScreenSpot-V2 18.1 84.0 83.0 89.2 87.1 88.4 90.3 79.5 81. 85.6 88.3 88.7 90.9 Table 9: Performance of InternVL3 and other models on GUI grounding benchmarks. Model Name GPT-4o [96] Gemini-1.5 Pro [101] VILA-1.5-8B [70] LongVA-7B [144] LLaVA-NeXT-Video-7B [149] LLaVA-OneVision-7B [59] InternVL3-8B InternVL3-38B LLaVA-NeXT-Video-72B [149] LLaVA-OneVision-72B [59] InternVL3-78B Obj.count Abs.Dist. Obj.size Room Size Rel.Dist. Rel.Dir. Route Plan Appr.Order Overall 37.0 51.3 32.1 33.1 43.5 42.5 48.3 53.5 42.4 42.5 55.9 43.8 64.1 50.3 38.9 47.8 47.4 48.4 46.1 57.4 57.6 44. 38.2 43.6 18.8 22.2 24.2 12.3 33.6 41.7 35.3 37.5 39.5 28.5 34.6 24.8 15.7 30.6 24.4 35.4 60.7 48.6 44.6 54.5 41.3 46.3 34.8 43.3 42.4 35.2 36.4 38.6 36.7 39.9 39.5 31.5 36.0 31.0 25.4 34.0 29.4 27.3 28.9 35.0 32.5 28.9 5.3 30.9 21.8 16.6 14.0 20.2 39.0 50.2 22.8 23.9 53.7 34.0 45.4 28.9 29.2 35.6 32.4 42.1 48.9 40.9 40.2 48. 46.2 56.2 17.4 38.0 48.5 47.7 68.1 71.7 48.9 43.5 71.2 Table 10: Performance of InternVL3 and other models on VSI-Bench. ScreenSpot [22], InternVL3-72B achieves 88.7% accuracy, slightly outperforming UI-TARS-72B [99] (88.4%) and Qwen2.5-VL-72B (87.1%), while Aguvis-72B [131] leads with 89.2%. Notably, InternVL3-38B (85.6%) surpasses GPT-4o (18.1%) and Gemini 2.0 (84.0%) by significant margin. For the more challenging ScreenSpot-V2 [129] benchmark, InternVL3 exhibits strong scaling behavior: InternVL3-72B achieves 90.9%, outperforming UI-TARS-72B (90.3%). The 8B variant (81.4%) already surpasses UI-TARS-72B, while the 38B model (88.3%) further closes the gap to the 72B version. These results highlight InternVL3s robustness in GUI understanding tasks, particularly in handling complex screen layouts and dynamic interfaces. The performance improvements with model scale suggest that larger architectures better capture the fine-grained visual-textual alignments required for precise GUI grounding. The superior performance of the InternVL3 models highlights their robustness in interpreting complex visual layouts. Future work will explore extending these capabilities to more dynamic and interactive GUI environments."
        },
        {
            "title": "3.12 Spatial Reasoning",
            "content": "Spatial reasoning involves constructing mental representation of three-dimensional environment from visual inputsa capability that is vital for applications such as autonomous driving. Table 10 reports the performance results on the Visual-Spatial Intelligence Benchmark (VSI-Bench) [133], where InternVL3 is compared against other state-of-the-art MLLMs. The results clearly indicate that InternVL3 outperforms its competitors in spatial reasoning tasks. In particular, the InternVL3-8B variant achieves score of 42.1, leading all open-source MLLMs in the benchmark. Moreover, the InternVL3-38B and InternVL3-78B variants score 48.9 and 48.4, respectivelyboth superior to proprietary models such as GPT-4o, Gemini-1.5 Flash, and Gemini-1.5 Pro. Furthermore, InternVL3 exhibits exceptional performance in several sub-category tasks within the benchmark. It attains score of 71.2 in object counting, 53.7 in absolute distance estimation, 55.9 in relative distance estimation, and 54.5 in appearance order prediction, demonstrating its robust spatial reasoning capabilities. These promising results underscore the potential of InternVL3 for advancing 3D scene understanding, and future work will explore its integration into various downstream applications."
        },
        {
            "title": "3.13 Evaluation on Language Capability",
            "content": "Table 11 presents the performance evaluation of language capabilities across diverse array of benchmarks. These benchmarks cover comprehensive assessments in general knowledge, linguistic understanding, reasoning, mathematics, and coding tasks, such as MMLU [45], CMMLU [62], C-Eval [47], GAOKAO-Bench [148], TriviaQA [51], NaturalQuestions [57, 109], RACE [58], WinoGrande [102], HellaSwag [141], BigBench Hard [111], GSM8K-Test [25], MATH [46], TheoremQA [17], HumanEval [14], MBPP [4], and MBPP-CN [4]. In particular, the experiments conducted compare the performance of Qwen2.5 chat models against corresponding InternVL3 variants. Both model series share the same pre-trained Qwen2.5 base model as their initialization. After undergoing native multimodal pre-training followed by additional post-training, the In16 C 5 . 0 - 5 . 2 Version 4d595a c13365 2daf24 4c31db 2121ce 3dcea1 8c358f 69ee4f b36770 e42710 5b92b0 1d7fe4 393424 6f0af8 8e312c a447ff 9114d5 46.4 47.2 53.5 30.9 24.2 8.2 35.2 51.5 47.2 39.3 21. 39.0 27.8 12.3 27.4 38.5 19.6 1 - 3 r I 49.8 56.7 59.0 46.6 21.5 8.5 66.3 68.8 52.9 47.0 34. 47.2 32.7 12.9 39.0 47.5 30.6 C 5 . 1 - 5 . 2 Q 61.8 62.9 66.2 53.7 39.8 15.2 81.2 76.0 56.5 62.0 39. 61.6 49.3 14.4 51.8 51.4 34.4 C 7 - 5 . 2 Q 74.2 78.8 77.8 81.3 55.8 17.9 90.8 86.8 71.5 85.4 65. 80.1 72.6 20.1 82.3 74.3 64.4 8 - 3 r I 77.3 84.4 84.5 89.5 51.5 28.2 95.1 90.8 78.1 90.2 77. 83.1 72.2 25.5 78.1 69.3 64.4 2 - 3 r I 64.8 72.2 73.3 67.7 41.2 15.9 84.7 84.6 61.9 73.8 52. 72.5 57.3 15.6 62.8 60.7 45.8 C 4 1 - 5 . 2 Q 79.5 82.6 81.4 86.9 65.1 19.7 92.1 89.6 79.1 90.5 73. 82.4 73.7 18.5 81.1 76.7 75.4 4 1 - 3 r I 82.1 85.8 85.6 91.2 67.4 31.4 96.3 93.0 84.3 93.0 82. 88.4 76.3 24.1 78.1 75.1 67.2 C 2 3 - 5 . 2 Q 83.3 85.8 86.5 90.8 65.8 19.7 92.3 91.5 83.8 92.1 85. 84.7 81.1 21.9 89.0 83.7 77.8 8 3 - 3 r I 85.4 88.7 89.2 93.5 70.1 31.0 97.4 94.2 86.7 95.5 87. 89.7 72.2 18.9 87.8 77.4 75.4 C 2 7 - 5 . 2 Q 84.4 87.4 88.1 91.0 74.0 23.8 96.1 91.7 83.9 92.7 85. 88.2 81.4 22.9 87.2 86.8 76.0 8 7 - 3 r I 86.9 89.9 89.5 93.1 74.7 39.0 97.6 94.2 87.8 95.6 85. 90.5 78.9 30.4 82.3 76.7 76.0 Dataset MMLU CMMLU C-Eval GAOKAO TriviaQA NaturalQuestions C3 RACE-High WinoGrande HellaSwag BBH GSM8K MATH TheoremQA HumanEval MBPP MBPP-CN Overall - 33.5 42. 51.6 59.2 69.4 72.9 73.4 76. 77.4 78.9 78.9 80.5 Table 11: Comparison of language model performance across multiple benchmarks. These results were obtained using the OpenCompass toolkit. We compare InternVL3 with Qwen2.5 Chat models, whose corresponding pre-trained base models are employed as the initialization of the language component in InternVL3. Please note that the evaluation scores of the Qwen2.5 series may differ from those officially reported, as we have adopted the prompt versions provided in the table across all datasets for OpenCompass evaluation. TextVQA VizWiz ChartQA DocVQA AI2D InfoVQA GQA SQA-I test test POPE Tiny MMMU SEED v1 LVLM val image Overall V2PE δ 1/256 1/64 1/16 1/4 1/1 val 78.4 78.0 78.3 78.7 79.0 78.7 val test avg 61.7 61.7 62.0 62.1 62.2 61. 81.4 81.2 81.7 81.7 82.4 82.2 val 89.4 88.5 89.4 90.4 91.0 90.2 test 81.1 81.0 81.3 81.6 81.8 81.7 val 69.4 67.7 69.6 70.4 71.7 71.4 60. 94.4 87.9 348.5 61.0 60.9 61.1 61.2 61.2 94.4 94.7 95.0 94.9 94.6 88.3 88.3 88.2 88.1 88. 345.3 345.7 345.0 345.8 347.2 52.6 52.9 52.3 53.3 52.6 52.4 75.6 75.9 76.1 76.1 76.2 76.1 75. 75.0 75.3 75.6 75.9 75.7 Table 12: Performance of the pre-trained InternVL3-8B model on multimodal benchmarks with different positional encoding strategies. When employing V2PE, the impact of different positional increment values δ is systematically evaluated. ternVL3 series consistently demonstrates superior performance over the Qwen2.5 chat models across most evaluation benchmarks. This observed enhancement in language capabilities primarily arises from several factors, including the integration of approximately 25% pure-language data, joint parameter optimization during native multimodal pre-training, and the extensive use of high-quality textual corpora during the subsequent post-training stage. Such an approach not only strengthens multimodal comprehension but also significantly enhances language proficiency. Consequently, even when derived from identical pre-trained base models, the integrated multimodal and pure-text training strategy employed by InternVL3 results in substantially improved performance in language capabilities compared to the specialized training pipeline designed for pure-text tasks used by the Qwen2.5 chat models. 17 Figure 3: Performance comparison on multimodal benchmarks under different training strategies. Native multimodal pre-training endows MLLMs with strong multimodal capabilities, even without further post-training."
        },
        {
            "title": "Model",
            "content": "InternVL3-1B InternVL3-2B InternVL3-8B InternVL3-9B InternVL3-14B InternVL3-38B InternVL3-78B MPO MMMU MathVista MathVision MathVerse DynaMath WeMath LogicVista Overall 18.1 18.7 23.2 25.3 36.9 39.8 32.2 35.3 38.8 44.4 45.1 48.2 44.2 51.0 24.6 25.1 (+0.5) 30.7 32.4 (+1.7) 41.4 44.3 (+2.9) 41.6 43.1 (+1.5) 46.2 49.9 (+3.7) 48.3 52.8 (+4.5) 50.5 54.6 (+4.1) 14.7 13.4 18.1 22.4 32.7 37.1 32.5 33.8 38.1 43.0 41.7 48.6 42.5 46.1 13.8 18.8 22.0 21.7 24.7 29.3 28.9 27.6 31.2 37.2 34.2 34.2 35.2 43.1 47.2 45.8 59.0 57.0 67.4 71.6 68.8 71.5 70.5 75.1 71.2 75.1 74.0 79. 43.4 43.4 49.1 48.6 61.9 62.7 59.0 57.7 67.1 67.1 69.3 70.1 72.2 72.2 31.1 29.8 30.0 36.9 43.2 44.1 46.5 49.2 49.9 51.2 54.4 58.4 53.5 55.9 4.2 5.8 13.4 14.6 22.8 25.5 23.0 26.7 27.9 31.3 22.2 35.3 31.7 35.1 Table 13: Comparison of reasoning abilities before and after Mixed Preference Optimization (MPO)."
        },
        {
            "title": "3.14 Ablation Study",
            "content": "The Effectiveness of Native Multimodal Pre-Training. To assess the effectiveness of native multimodal pre-training, we conduct experiments on the InternVL2-8B model while keeping its architecture, initialization parameters, and training data entirely unchanged. Traditionally, InternVL2-8B employs training pipeline that begins with an MLP warmup phase for multimodal alignment, followed by an instruction-tuning stage. In our experiments, we substitute the conventional MLP warmup phase with our native multimodal pre-training process. This modification isolates the contribution of native multimodal pre-training to the overall multimodal capability of the model. The evaluation results in Figure 3 show that the model with native multimodal pre-training exhibits performance on most benchmarks that is comparable to the fully multi-stage-trained InternVL2-8B baseline. Furthermore, when followed by instruction tuning on higher-quality data, the model demonstrates further performance gains across evaluated multimodal tasks. These findings underscore the efficiency of native multimodal pre-training in imparting powerful multimodal capabilities to MLLMs. The Evaluation of Variable Visual Position Encoding. To promote the multimodal capabilities in long-context scenarios, InternVL3 employs Variable Visual Position Encoding (V2PE) in its visual embedding. However, in the original V2PE [41], this specialized positional encoding for visual tokens did not yield benefits on 18 multimodal tasks with moderate context lengths. To further explore the efficacy of V2PE in broader setting, we incorporated it during the native multimodal pre-training stage and evaluated the InternVL3-8B pre-trained model on standard multimodal benchmarks. As reported in Table 12, the introduction of V2PE leads to significant performance gains across most evaluation metrics. In addition, our ablation studiesby varying the positional increment δreveal that even for tasks primarily involving short contexts, relatively small δ values can achieve optimal performance. These findings provide important insights for future efforts aimed at refining position encoding strategies for visual tokens in MLLMs. It is important to note that, to ensure fair comparisons, all results elsewhere in this report maintain fixed δ = 1, except for the experimental results presented in Table 12. Mixed Preference Optimization. Here, we demonstrate the effectiveness of MPO. As shown in Table 13, models fine-tuned with MPO demonstrate superior reasoning performance across seven multimodal reasoning benchmarks compared to their counterparts without MPO. Specifically, InternVL3-78B and InternVL3-38B outperform their counterparts by 4.1 and 4.5 points, respectively. Notably, the training data used for MPO is subset of that used for SFT, indicating that the performance improvements primarily stem from the training algorithm rather than the training data."
        },
        {
            "title": "4 Conclusion",
            "content": "We have introduced InternVL3, significant advancement in the InternVL series that implements native multimodal pre-training paradigm. By jointly learning linguistic and multimodal capabilities during the pretraining phase, InternVL3 avoids the training complexities and optimization challenges typically associated with post-hoc MLLM training pipelines. Through the incorporation of variable visual position encoding (V2PE) for extended multimodal contexts, advanced post-training strategiessuch as supervised fine-tuning and mixed preference optimizationand test-time scaling, InternVL3 establishes new open-source benchmark across wide range of multimodal tasks, while simultaneously preserving robust linguistic competencies. Notably, InternVL3-78B attains 72.2-point score on the MMMU benchmark, exceeding previous open-source MLLMs and reducing the performance gap relative to leading proprietary counterparts (e.g., Gemini-2.5 Pro). In line with our commitment to fostering community-driven innovation in multimodal large language models, we will publicly release InternVL3s training data and model weights, thereby encouraging further research and development in this rapidly evolving field."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 15 [2] Anonymous. CG-bench: Clue-grounded question answering benchmark for long video understanding. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review. 14, 15 [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024. 2, 8, 9, 10, 11, 12 [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 2 [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 11, 12, 13, 14 [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 9, 10, 15 [8] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 2024. 5 19 [9] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and In Proceedings of the IEEE/CVF International Dimosthenis Karatzas. Scene text visual question answering. Conference on Computer Vision, pages 42914301, 2019. 6 [10] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 15111520, 2022. 6 [11] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. 6 [12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 13 [13] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, arXiv preprint Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. 12 [14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [15] Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang, Qinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, et al. Internevo: Efficient long-sequence large language model training via hybrid parallelism and redundant sharding. arXiv preprint arXiv:2401.09149, 2024. 2, 7 [16] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024. 6 [17] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 78897901. Association for Computational Linguistics, 2023. 16 [18] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 1, 2, 3, 5, 6, 9, 10, 11, 12, 13, 14, 15 [19] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 3, 10, 11, 12, 13, 14, [20] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2, 3 [21] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 2, 3 [22] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. 16 [23] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 15 [24] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 845855, 2018. 6 [25] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 16 [26] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: //github.com/open-compass/opencompass, 2023. 9, 10, 11, 12 20 [27] X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model. https://x.ai/blog/grok-1.5v, 2024. 11 [28] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. 10 [29] Google Deepmind. Gemini 2.0 is now available to everyone. https://blog.google/technology/ google-deepmind/gemini-model-updates-february-2025/, 202. 9 [30] Google Deepmind. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/ technology/google-deepmind/google-gemini-ai-update-december-2024/, 2024. [31] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 1, 10 [32] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024. 1 [33] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 [34] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 14, 15 [35] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In Conference on Computer Vision and Pattern Recognition Workshop, pages 178178, 2004. [36] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 12 [37] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 14, 15 [38] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 9, 11 [39] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 6 [40] Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, et al. Mini-internvl: flexible-transfer pocket multimodal model with 5% parameters and 90% performance. arXiv preprint arXiv:2410.16261, 2024. 3 [41] Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. arXiv preprint arXiv:2412.09616, 2024. 2, 3, [42] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69046913, 2017. 6 [43] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 9, 10 [44] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566, 2023. 8, 12, 13 21 [45] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In The International Conference on Learning Representations, 2020. 16 [46] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. [47] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024. 16 [48] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. 6 [49] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. 6 [50] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 9, 11 [51] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. 16 [52] Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization for large language model alignment. arXiv preprint arXiv:2404.04656, 2024. 6 [53] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56485656, 2018. 6 [54] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. [55] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 787798, 2014. 13 [56] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision, pages 235251, 2016. 6, 7, 8, 10 [57] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. 16 [58] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [59] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 9, 10, 11, 12, 15, 16 [60] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 8, 10 [61] Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, et al. R-bench: Are your large multimodal model robust to real-world corruptions? arXiv preprint arXiv:2410.05474, 2024. 11 [62] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. 16 [63] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 22 [64] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 14, 15 [65] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48044814, 2022. 1, 3 [66] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 12, 13 [67] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023. 1 [68] Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier visionlanguage models. arXiv preprint arXiv:2501.14818, 2025. [69] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 7 [70] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 1, 9, 10, 15, 16 [71] Adam Dahlgren Lindström and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. 6 [72] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2023. [73] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2025. 13 [74] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 12 [75] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 8, 10 [76] Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint, 2024. 5 [77] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialtemporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 15 [78] Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. Scp-116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain, 2025. 5 [79] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 7, 8, 9 [80] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 6 [81] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 6 [82] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. 6 [83] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 9, 10 [84] Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, et al. Bluelm-v-3b: Algorithm and system co-design for multimodal large language models on mobile devices. arXiv preprint arXiv:2411.10640, 2024. 1 [85] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. [86] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2, 2024. 7 [87] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1120, 2016. 13 [88] Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. 1 [89] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31953204, 2019. 6 [90] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 22632279, 2022. 6, 7, 8, 10 [91] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 6, 7, 8, [92] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 22002209, 2021. 7, 8, 10 [93] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. 7 [94] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024. 9, 11 [95] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947952, 2019. 6 [96] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023. 1, 8, 9, 10, 11, 12, 14, 15, 16 [97] OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2025. 2, 8 [98] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 8, 9 [99] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 16 [100] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 6 [101] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 10, 11, 12, 15, 16 [102] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 87328740, 2020. 16 [103] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. 6 [104] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [105] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. 6 [106] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 6, 8, [107] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 7 [108] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. 14 [109] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. 16 [110] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 12, [111] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 16 [112] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv preprint arXiv:2405.11985, 2024. 14 [113] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 14 [114] Qwen Team. Qvq: To see the world with wisdom, December 2024. 9 [115] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 9, 10, 11, 12 [116] DeepMind. Gemini 2.5 pro. https://deepmind.google/technologies/gemini/pro/, 2025. 1, 2, [117] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 9, 11 [118] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 8, 9 [119] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 7 [120] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 8, 10, 11, 12, 13, 14, 15 [121] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv:2305.11172, 2023. 13 [122] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 1, 13 [123] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 2, 6, 7 [124] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. 2, 7, 8, 9 [125] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024. 6, 12, [126] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In The International Conference on Learning Representations, 2024. 6 [127] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024. 8, 10 [128] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. 8, 14, 15 [129] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. 16 [130] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. 8, 9 [131] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. 2024. 16 [132] B. Yan, Yi Jiang, Jiannan Wu, D. Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 13 [133] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. 16 [134] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 9, 10, 11, 12, [135] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023. 1, 14 [136] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 9, 11 [137] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 12 [138] Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. 12 [139] Ya-Qi Yu, Minghui Liao, Jiwen Zhang, and Jihao Wu. Texthawk2: large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens. arXiv preprint arXiv:2410.05261, 2024. 26 [140] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 2, 7, 8, 9 [141] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. 16 [142] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 13 [143] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 13 [144] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [145] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 8, 9 [146] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 6 [147] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio. Vcr: Visual caption restoration. arXiv preprint arXiv:2406.06462, 2024. 8, 10 [148] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. [149] Zhang, Li, Liu, Lee, Gui, Fu, Feng, Liu, and Li. Llava-next: strong zero-shot video understanding model. 2024. 16 [150] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. 11 [151] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. 7 [152] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742, 2024. 9, 11 [153] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 14, [154] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 8,"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "SenseTime Research",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}