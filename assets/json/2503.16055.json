{
    "paper_title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
    "authors": [
        "Abdelrahman Elsayed",
        "Sarim Hashmi",
        "Mohammed Elseiagy",
        "Hu Wang",
        "Mohammad Yaqub",
        "Ibrahim Almakky"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: https://github.com/BioMedIA-MBZUAI/SALT"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . e [ 1 5 5 0 6 1 . 3 0 5 2 : r SALT: Parameter-Efficient Fine-Tuning via Singular Value Adaptation with Low-Rank Transformation Abdelrahman Elsayed*, Sarim Hashmi*, Mohammed Elseiagy, Hu Wang, Mohammad Yaqub, Ibrahim Almakky Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE {abdelrahman.elsayed, sarim.hashmi, mohammed.abdelaziz, hu.wang, mohammad.yaqub, ibrahim.almakky}@mbzuai.ac.ae Abstract. The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains significant barrier. ParameterEfficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: https://github.com/BioMedIA-MBZUAI/SALT. Keywords: Singular Value Decomposition, Parameter-Efficient FineTuning, LoRA, Foundation Models, Domain Adaptation."
        },
        {
            "title": "Introduction",
            "content": "Medical image segmentation plays pivotal role in modern healthcare by enabling precise delineation of anatomical structures, pathological regions, and functional tissues across diverse imaging modalities such as CT, MRI, X-ray, Equal Contribution 2 A. Elsayed et al. ultrasound, and endoscopy [11,20]. While Deep Learning (DL) has revolutionized this field, with architectures like U-Net [20] and nnU-Net [8] achieving state-of-the-art (SOTA) performance through sophisticated designs and extensive data augmentation, these models often rely on large parameter counts, limiting their adaptability to new datasets or modalities. For instance, nnU-Nets success hinges on dynamic architecture adjustments and aggressive augmentation pipelines, while DeepLabv3+ [3] employs computationally intensive components like spatial pyramid pooling. Recent advances in foundation models have introduced specialized tools for diverse computer vision tasks. CLIP [18] and ALIGN [9], trained on large-scale image-text datasets, excel at cross-modal understanding for zero-shot image classification and open-set retrieval. In contrast, the Segment Anything Model (SAM) [10,19], trained on billions of natural images, exhibits robust zero-shot generalization capabilities and initially raised hopes for universal segmentation. However, SAMs performance on medical data has proven suboptimal, with studies [26,6] demonstrating its inferiority to specialized models like U-Net++ [27], particularly in Dice scores across 12 medical datasets. Even with prompt engineering, SAMs zero-shot accuracy remains below fully supervised methods [4], underscoring the domain gap between natural and medical images. Recent efforts to adapt SAM to medical imaging, such as MedSAM [13], SAM Adapter [23], and S-SAM [16], have made strides by incorporating medical training data, low-rank adapters, or SVD-based tuning. Yet challenges persist: PEFT methods must balance minimal parameter overhead, computational feasibility, and preservation of pre-trained knowledge while addressing unique medical imaging challenges like speckle noise, low contrast, and modality-specific artifacts. Promising approaches that arose to adapt foundational models, especially language models, like LoRA [7] and AdaLoRA [25] approximate weight updates via low-rank matrices but struggle to capture both dominant and nuanced features, while SVD-based methods like PiSSA [15] focus on principal singular vectors to capture the most significant data patterns, but this rigid prioritization can limit their adaptability to dynamic or fine-grained data characteristics. To address these limitations, we propose Singular Value Adaptation with Low-Rank Transformation (SALT), novel PEFT framework that synergizes the strengths of SVD and low-rank adaptation. SALT selectively scales critical singular values to preserve foundational knowledge while applying trainable low-rank transformations to residual components, enabling efficient adaptation to medical domains with minimal parameter overhead. Evaluated on five challenging medical datasets, with sample sizes ranging from 20 to 1000 and demanding high level of detailed segmentation, SALT consistently outperforms LoRA and SVD, achieving 2 to 5% improvement in Dice score while utilizing only 3.9% trainable parameters. Our contributions are threefold: We introduce SALT, hybrid PEFT method utilizing SVD-based dominant singular value adaptation with low-rank residual updates, achieving parameter-efficient domain adaptation. SALT: Singular Value Adaptation with Low-Rank Transformation 3 Fig. 1: Overview of the SALT architecture. Starting from the segmentation model and providing detailed view of the SALT mechanism within the MultiHead Attention layers of the image encoder. We demonstrate SALTs utility through extensive experiments on diverse medical datasets from various modalities, showing consistent improvements over state-of-the-art PEFT methods. We present comprehensive study exploring how singular value distributions and trainable parameters allocation impact domain-specific fine-tuning for different challenging medical datasets."
        },
        {
            "title": "2 Methodology",
            "content": "Problem Formulation. Let denote SAM model with pre-trained parameters Θ, which consists of an image encoder (Ei), prompt encoder (Ep), mask decoder (Dm). Ei contains blocks, each comprising multi-head attention module and Multi-Layer Perceptron (MLP). For given input image and prompt embedding p, the encoder Ei produces intermediate feature maps {f (l)}L . Concretely, in the l-th block, the main operations can be written as: l=1 q, k, = qkv + bl qkv, on = M + bl , (1) qkv, where RD are the attention/MLP weights and biases for block {1, ..., L}. While effective for natural images, the pretrained weights RDK may not capture all domain-specific nuances. RDK and bl qkv, bl SALT. Through SALT, we refine these weights using the following procedure. First, consider weight matrix W(l). SALT decomposes W(l) using Singular Value Decomposition (SVD). W(l) = U(l) Σ(l) (cid:0)V(l)(cid:1) , (2) A. Elsayed et al. Fig. 2: Comparison of PEFT methods: (A) Full fine-tuning updates all parameters. (B) LoRA updates low-rank matrices. (C) SVD fine-tunes scale and shift components of . (D) SALT (Ours) integrates scale, shift, and low-rank updates for singular values. where U(l) RDD (left singular vectors), (V(l)) RKK (right singular vectors), and Σ(l) RDK is diagonal matrix of the singular values corresponding to each singular vector. We denote min(D, K) as rmax and split this matrix into two matrices (i) Σ(l) which contains diagonal vector of top singular values where rmax (ii) Σ(l) containing the remaining singular values where = rmax r. Then SALT applies (i) trainable scale and shift transformations to the top singular values. Concretely, define parameters α(l) and β(l) as diagonal matrices Rrr to transform Σ(l) : Σ (l) = α(l) Σ(l) + β(l), (3) where denotes Hadamard product, and (ii) LoRA-based low-rank update X(l)Y(l) where X(l) Rrdlora , Y(l) Rdlorar to the remaining singular values Σ(l) (l) is then activated elementwise by ReLU to preserve semi-definiteness, yielding the updated weight matrix: . The updated modified singular matrix Σ (cid:102)W(l) = U(l) (cid:2)ReLU(cid:0)Σ (l)(cid:1)(cid:3) (cid:0)V(l)(cid:1) . (4) During fine-tuning, only the newly introduced SALT parameters {α(l), β(l), X(l), Y(l)} are optimized along with Normalization Layers and Text Affine Layers, while all original SAM weights remain frozen. The prompt encoder Ep, receiving text prompts p, yields additional embeddings merged with the image-encoder Ei output in the mask decoder Dm. We define SALT, denoted S, as the combination of two components: The original SAM models frozen output S(i; Θfixed), which remains static during fine-tuning, and lightweight, learnable adjustment term S(i; θ), introduced by SALT layers,capturing task-specific refinements through the trainable parameters θ. This formulation ensures that the core SAM backbone remains unchanged, while task-specific adaptations are layered on top via the SALT modules. Putting this together, our objective is to minimize the loss function L(S(i), y) where is an input sample and is the corresponding ground SALT: Singular Value Adaptation with Low-Rank Transformation 5 truth label. To train SALT, we use combination of segmentation losses, including Focal loss, Dice loss, and regularization term expressed as: Lreg = α + β sF (cid:125) (cid:124) (cid:123)(cid:122) Scale-Shift , + XY (cid:124) (cid:123)(cid:122) (cid:125) LoRA (5) where denotes Frobenius norm. This dual constraint balances two goals: retaining the useful, pre-trained features and making only the minimal modifications needed for effective domain adaptation. Rationale for the SALT Design As shown in Fig. 2, LoRA [7] modifies weight matrix RDK by adding XY , where RDr and RrK have rank min(D, K). This reduces trainable parameters from K, down to r(D+K), but can underfit if is too small. By contrast, SVD-based fine tuning in S-SAM [16], shown in Fig. 2, learns only the scale and shift of each singular value in . Since rank(W ) = min(D, K), this corresponds to 2 min(D, K) parameters, fewer than LoRA, yet fully updating every singular value may lack flexibility for subtle domain shifts. SALT in Fig. 2 merges the strengths of both. It trains the top singular values via scale and shift and uses LoRA blocks for the lower singular subspace. This dual strategy retains key features while capturing domain-specific nuances, balancing parameter efficiency with adaptation capability."
        },
        {
            "title": "3 Experiments",
            "content": "Datasets. In this work, we use datasets spanning three anatomical domains: neurovascular analysis with DIAS [12], dynamic 2D+time DSA dataset for intracranial artery segmentation in cerebrovascular disease (20 train/10 test). Retinal assessment via ROSE [14] for OCT-Angiography-based microvasculature analysis (22 train/8 test). DRIVE [22] for diabetic retinopathy screening in RGB fundus images (14 train/6 test). Cardiovascular studies using ARCADE [17] (700 train/300 test) and XRay-Angio [2] (93 train/41 test), both leveraging Xray angiography, where the former is for region-based coronary artery disease diagnostics and the latter is for multiscale segmentation of occluded vessels. The datasets were randomly partitioned into training and testing subsets to ensure unbiased evaluation. Implementation Details. We apply SALT to the image encoder layers of the prompt-adapted SAM with 512 input size and text-based prompts. Following the pipeline in S-SAM [16], training uses AdamW (LR = 1 104, weight decay = 1 102), batch size 5, and 200 epochs, with step decay scheduler for learning rate adjustment. Data augmentation includes resizing, random flips, 10 rotations, and brightness adjustments. The total loss combines focal loss, Dice loss, and regularization loss (scaled by 0.01). Evaluation metrics include the Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff Distance (HD95). Experiments were conducted on an NVIDIA RTX 4090 (24GB). 6 A. Elsayed et al. Table 1: Performance comparison of traditional DL and SAM-based PEFT methods using Dice and HD95 metrics on variety of medical segmentation datasets. Model Rank %Trainable ARCADE DIAS ROSE XRay-Angio Drive Average Dice HD95 Dice HD95 Dice HD95 Dice HD95 Dice HD95 Dice HD95 Traditional DL Methods U-Net [20] UNETR [5] RegUnet [20] SegNet [1] DeepLabV3+ [3] Segformer [24] 100% / 6.5M 0.36 100% / 91M 0.48 100% / 7.8M 0.11 100% / 29M 0.14 100% / 45M 0.55 100% / 43M 0.53 157.14 162.44 247.79 231.11 103.22 113. 0.13 0.31 0.35 0.13 0.55 0.57 262.66 183.97 172.09 295.06 77.66 73.64 0.63 0.63 0.22 0.38 0.52 0.56 8.07 8.96 58.08 9.09 12.90 11.21 SAM w/ text prompts S-SAM [16] LoRA [7] LoRA [7] SALT (ours) SALT (ours) 0.00 0.76 0.40% / 1M 0.64% / 1.5M 0.78 256 14.08% / 33.9M 0.81 0.46% / 1.1M 0.78 3.9% / 9.4M 0.81 4 256 SAM-Based PEFT Methods (241M) - 52.44 39.58 36.89 51.74 42.50 0.00 0.67 0.61 0.63 0.69 0.71 - 42.36 27.81 34.36 39.58 27. 0.00 0.65 0.59 0.62 0.66 0.67 - 12.41 16.26 14.62 12.19 12.64 0.71 0.74 0.42 0.09 0.74 0.75 0.04 0.75 0.73 0.76 0.75 0.77 30.40 20.62 79.51 110.94 14.03 16.55 - 27.56 26.78 24.84 29.55 23. 0.18 0.46 0.43 0.14 0.46 0.48 0.00 0.70 0.62 0.68 0.72 0.75 84.43 33.58 72.97 79.03 27.79 36.98 - 15.81 21.04 19.00 16.04 13.20 0.40 0.52 0.31 0.18 0.56 0.58 0.01 0.71 0.66 0.70 0.72 0. 108.54 81.91 126.08 145.05 47.12 50.47 - 30.12 26.29 25.94 29.82 23."
        },
        {
            "title": "4 Results and Discussion",
            "content": "We evaluate the performance of SALT against SOTA PEFT techniques, specifically LoRA [7] and S-SAM [16]. As shown in Table 1, our experimental evaluation demonstrates the effectiveness of SAM-based PEFT methods compared to traditional deep learning approaches across five medical imaging datasets. While traditional methods like DeepLabV3+ [3] and U-Net [20] showed competitive performance on datasets such as XRay-Angio [2], they require training of all parameters. On the other hand, SALT achieves superior performance with significantly fewer trainable parameters. With rank-256 configuration, attaining an average Dice score of 0.74 while requiring only 3.9% of parameters to be trained. This represents substantial improvement over both traditional methods and other parameter-efficient approaches like LoRA [7] and S-SAM [16]. Notably, SALT consistently achieves higher Dice scores over other methods across all datasets, achieving the highest scores on DIAS (0.71), ROSE (0.67), XRayAngio (0.77), and Drive (0.75). The rank configuration comparison reveals that while rank-4 SALT delivers competitive results with minimal parameter overhead (0.46% trainable), the rank-256 version provides optimal performance with modest increase in trainable parameters. Our analysis of boundary accuracy via HD95 further underscores SALTs performance. While traditional methods exhibited high HD95 errors, such as SegNet [1] scoring 145.05 and RegUnet [20] scoring 126.08, SAM-based PEFT approaches showed improvements, with SALT achieving the lowest average HD95 (23.87) against LoRA[7] (25.94) and S-SAM [16] (30.12). Notably, SALT demonstrated Strong boundary adherence in challenging cases. SALTs 13.20 HD95 on DRIVE [22] against S-SAM [16] achieving 15.81 reflects precise vessel delineation in low-contrast retinal images. This dual advantage in both Dice and HD95 confirms SALTs ability to maintain structural accuracy and boundary precision while operating in parameter-efficient regimes. Qualitative Analysis. As illustrated in Fig 3, the qualitative results highlight the SOTA performance of our proposed SALT method across various medical imaging datasets. Compared to traditional approaches like UNETR [5] and USALT: Singular Value Adaptation with Low-Rank Transformation 7 Fig. 3: Qualitative Vessel Segmentation. The figure displays segmentation results for five datasets (Arcade [17], Rose [14], Dias [12], X-ray Angio [2], Drive [22]). Columns show the ground truth, SALT, SAM-based PEFT methods, and traditional DL models. Predicted masks are white, and false negatives are red. Net [20], as well as other SAM-based PEFT methods such as S-SAM [16] and LoRA [7], SALT consistently delivers more precise vessel segmentation. In the ROSE [14] dataset, SALT excels at capturing the intricate, web-like patterns of retinal vessels while maintaining structural continuity with fewer false positives. For the DIAS [12] dataset, SALT effectively segments complex branching patterns, preserving both main vessel trunks and subtle secondary branches. In X-ray angiography [2] images, SALT achieves clear vessel boundaries with fewer false positives compared to traditional methods. Additionally, in the Drive [22] dataset, it successfully segments retinal vessels despite varying widths and complex backgrounds. Extension to SAM2 [19]. We extend SALT to SAM2 and perform ablation studies comparing LoRA [7], SVD-based fine tuning, and SALT on the Hiera [21] image encoder. The SAM2 [19] image encoder utilizes hierarchical architecture, capturing multi-scale features from input images with variable embedding dimensions ranging from low-dimensional, early-stage representations to highdimensional, later-stage ones. This design enables LoRA to adapt effectively. In contrast, the original SAM [10] image encoder employs backbone with constant, high-dimensional embeddings, where LoRA exhibited instability and was A. Elsayed et al. (a) (b) Fig. 4: Ablation study on SALT configurations And adaptability to SAM2. a) Evaluate model performance and efficiency across LoRA ranks, with the x-axis for parameter combinations, the y-axis for Dice scores, and circle size for trainable parameter percentage. The numbers on the x-axis represent the rank of the top singular values, which are modified using scale and shift in each mentioned layer Conv2d, MLP, and MHA. b) Compares SALT, LoRA [7], and SVD PEFTs integrated with SAM2 on two datasets of different sizes. sometimes outperformed by S-SAMs SVD-based fine tuning method [16]. In SAM2, however, LoRA consistently outperforms relative to SVD, while SALT achieves competitive performance to LoRA with 2.4 times fewer parameters at rank 256 and 1.8 fewer at rank 4. Ablation Study. We conducted comprehensive ablation experiments on the Drive dataset [22] to evaluate the impact of different architectural components and rank configurations in SALT. Fig 4a presents the results across various combinations of LoRA ranks (4, 64, 256) and SALT ranks for Multi-Head Attention (MHA), neck convolutional layers (NECK-Conv2d), and Multi-Layer Perceptron (MLP) modules. The SALT rank for given layer refers to the number of dominant singular values in the layers weight matrix that are adaptively scaled and shifted during fine-tuning. Key findings are: 1) increasing the LoRA rank from 4 to 256 consistently improved Dice scores (0.72 0.76) while maintaining modest parameter increase from 0.46% to 5.86% and 2) reducing MHA and MLP ranks from 700 to 200 further enhanced performance, increasing Dice scores from 0.75 to 0.76 with LoRA rank 256. These results suggest that higher LoRA ranks improve expressiveness, while component-specific rank selection is crucial for optimal performance. The optimal configuration (LoRA rank 256 and components rank 200) achieved Dice score of 0.76 with only 5.84% of trainable parameters, demonstrating SALTs effective balance of performance and efficiency. SALT: Singular Value Adaptation with Low-Rank Transformation"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce SALT, novel PEFT framework that adapts the Segment Anything Model (SAM) to domain-specific tasks, particularly medical image segmentation. By utilizing SVD-based scaling and shifting of the top singular values with LoRA-based low-rank updates, SALT achieves efficient and flexible adaptation, enabling precise segmentation with minimal computational overhead. Experiments show that SALT outperforms state-of-the-art SAM PEFT adaptations while preserving parameter efficiency and adaptability. Future work will extend SALT to other foundation models and explore dynamic rank selection to enable optimal parameter allocation."
        },
        {
            "title": "6 Acknowledgments",
            "content": "The authors gratefully acknowledges Juan Lugo for his contributions through out the project."
        },
        {
            "title": "References",
            "content": "1. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence 39(12), 24812495 (2017) 2. Cervantes-Sanchez, F., Cruz-Aceves, I., Hernandez-Aguirre, A., HernandezGonzalez, M.A., Solorio-Meza, S.E.: Automatic segmentation of coronary arteries in x-ray angiograms using multiscale analysis and artificial neural networks. Applied Sciences 9(24), 5507 (2019) 3. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 801818 (2018) 4. Cheng, D., Qin, Z., Jiang, Z., Zhang, S., Lao, Q., Li, K.: Sam on medical images: comprehensive study on three prompt modes. arXiv preprint arXiv:2305.00035 (2023) 5. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 574584 (2022) 6. He, S., Bao, R., Li, J., Grant, P., Ou, Y.: Accuracy of segment-anything model image segmentation tasks. arXiv preprint arXiv:2304.09324 (sam) in medical (2023) 7. Hu, E.J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: International Conference on Learning Representations (2022) 8. Isensee, F., Petersen, J., Klein, A., Zimmerer, D., Jaeger, P.F., Kohl, S., Wasserthal, J., Koehler, G., Norajitra, T., Wirkert, S., et al.: nnu-net: Self-adapting framework for u-net-based medical image segmentation (2018). arXiv preprint arXiv:1809.10486 (1809) 10 A. Elsayed et al. 9. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International conference on machine learning. pp. 49044916. PMLR (2021) 10. Kirillov, A., Mintun, E., Ravi, N., et al.: Segment anything. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 40154026 (2023) 11. Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., van der Laak, J.A., van Ginneken, B., Sánchez, C.I.: survey on deep learning in medical image analysis. Medical Image Analysis 42, 6088 (Dec 2017) 12. Liu, W., Tian, T., Wang, L., Xu, W., Li, L., Li, H., Zhao, W., Tian, S., Pan, X., Deng, Y., et al.: Dias: dataset and benchmark for intracranial artery segmentation in dsa sequences. Medical Image Analysis 97, 103247 (2024) 13. Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical images. Nature Communications 15(1), 654 (2024) 14. Ma, Y., Hao, H., Xie, J., Fu, H., Zhang, J., Yang, J., Wang, Z., Liu, J., Zheng, Y., Zhao, Y.: Rose: retinal oct-angiography vessel segmentation dataset and new model. IEEE transactions on medical imaging 40(3), 928939 (2020) 15. Meng, F., Wang, Z., Zhang, M.: Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems 37, 121038121072 (2025) 16. Paranjape, J.N., Sikder, S., Vedula, S.S., Patel, V.M.: S-sam: Svd-based fine-tuning of segment anything model for medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 720730. Springer (2024) 17. Popov, M., Amanturdieva, A., Zhaksylyk, N., Alkanov, A., Saniyazbekov, A., Aimyshev, T., Ismailov, E., Bulegenov, A., Kuzhukeyev, A., Kulanbayeva, A., Kalzhanov, A., Temenov, N., Kolesnikov, A., Sakhov, O., Fazli, S.: Dataset for automatic region-based coronary artery disease diagnostics using x-ray angiography images. Scientific Data 11, Article 20 (2024) 18. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 87488763. PmLR (2021) 19. Ravi, N., Gabeur, V., Hu, Y.T., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) 20. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) Medical Image Computing and Computer-Assisted Intervention MICCAI 2015. pp. 234241. Springer International Publishing, Cham (2015) 21. Ryali, C., Hu, Y.T., Bolya, D., Wei, C., Fan, H., Huang, P.Y., Aggarwal, V., Chowdhury, A., Poursaeed, O., Hoffman, J., Malik, J., Li, Y., Feichtenhofer, C.: Hiera: hierarchical vision transformer without the bells-and-whistles. In: Proceedings of the 40th International Conference on Machine Learning. ICML23, JMLR.org (2023) 22. Staal, J., Abramoff, M., Niemeijer, M., Viergever, M., van Ginneken, B.: Ridgebased vessel segmentation in color images of the retina. IEEE Transactions on Medical Imaging 23(4), 501509 (2004) 23. Wu, J., Fu, R., Fang, H., Liu, Y., Wang, Z., Xu, Y., Jin, Y., Arbel, T.: Medical sam adapter: Adapting segment anything model for medical image segmentation. arxiv 2023. arXiv preprint arXiv:2304.12620 (2023) SALT: Singular Value Adaptation with Low-Rank Transformation 24. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems 34, 1207712090 (2021) 25. Zhang, Q., Chen, M., Bukharin, A., Karampatziakis, N., He, P., Cheng, Y., Chen, W., Zhao, T.: Adalora: Adaptive budget allocation for parameter-efficient finetuning. arXiv preprint arXiv:2303.10512 (2023) 26. Zhang, Y., Shen, Z., Jiao, R.: Segment anything model for medical image segmentation: Current applications and future directions. Computers in Biology and Medicine p. 108238 (2024) 27. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: nested u-net architecture for medical image segmentation. In: Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, proceedings 4. pp. 311. Springer (2018)"
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE"
    ]
}