{
    "paper_title": "RL Zero: Zero-Shot Language to Behaviors without any Supervision",
    "authors": [
        "Harshit Sikchi",
        "Siddhant Agarwal",
        "Pranaya Jajoo",
        "Samyak Parajuli",
        "Caleb Chuck",
        "Max Rudolph",
        "Peter Stone",
        "Amy Zhang",
        "Scott Niekum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rewards remain an uninterpretable way to specify tasks for Reinforcement Learning, as humans are often unable to predict the optimal behavior of any given reward function, leading to poor reward design and reward hacking. Language presents an appealing way to communicate intent to agents and bypass reward design, but prior efforts to do so have been limited by costly and unscalable labeling efforts. In this work, we propose a method for a completely unsupervised alternative to grounding language instructions in a zero-shot manner to obtain policies. We present a solution that takes the form of imagine, project, and imitate: The agent imagines the observation sequence corresponding to the language description of a task, projects the imagined sequence to our target domain, and grounds it to a policy. Video-language models allow us to imagine task descriptions that leverage knowledge of tasks learned from internet-scale video-text mappings. The challenge remains to ground these generations to a policy. In this work, we show that we can achieve a zero-shot language-to-behavior policy by first grounding the imagined sequences in real observations of an unsupervised RL agent and using a closed-form solution to imitation learning that allows the RL agent to mimic the grounded observations. Our method, RLZero, is the first to our knowledge to show zero-shot language to behavior generation abilities without any supervision on a variety of tasks on simulated domains. We further show that RLZero can also generate policies zero-shot from cross-embodied videos such as those scraped from YouTube."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 8 1 7 5 0 . 2 1 4 2 : r RL ZERO: ZERO-SHOT LANGUAGE TO BEHAVIORS WITHOUT ANY SUPERVISION Harshit Sikchi ,1, Siddhant Agarwal,1, Pranaya Jajoo,2, Samyak Parajuli,1, Caleb Chuck,1, Max Rudolph,1, Peter Stone,1,3, Amy Zhang,1,4, Scott Niekum,5 1 The University of Texas at Austin, 2 University of Alberta 3 Sony AI, 4 Meta AI,5 UMass Amherst"
        },
        {
            "title": "ABSTRACT",
            "content": "Rewards remain an uninterpretable way to specify tasks for Reinforcement Learning, as humans are often unable to predict the optimal behavior of any given reward function, leading to poor reward design and reward hacking. Language presents an appealing way to communicate intent to agents and bypass reward design, but prior efforts to do so have been limited by costly and unscalable labeling efforts. In this work, we propose method for completely unsupervised alternative to grounding language instructions in zero-shot manner to obtain policies. We present solution that takes the form of imagine, project, and imitate: The agent imagines the observation sequence corresponding to the language description of task, projects the imagined sequence to our target domain, and grounds it to policy. Video-language models allow us to imagine task descriptions that leverage knowledge of tasks learned from internet-scale video-text mappings. The challenge remains to ground these generations to policy. In this work, we show that we can achieve zero-shot language-to-behavior policy by first grounding the imagined sequences in real observations of an unsupervised RL agent and using closed-form solution to imitation learning that allows the RL agent to mimic the grounded observations. Our method, RLZero, is the first to our knowledge to show zero-shot language to behavior generation abilities without any supervision on variety of tasks on simulated domains. We further show that RLZero can also generate policies zero-shot from cross-embodied videos such as those scraped from YouTube. Project page: hari-sikchi.github.io/rlzero"
        },
        {
            "title": "INTRODUCTION",
            "content": "Underlying the many successes of RL lies the engineering challenge of task specification, where skilled expert painstakingly designs reward function. Not only does this restrict the scaling of RL agents, but it also makes those agents uninterpretable to any user inexperienced with reward design. Even for experts, reasoning about simple reward functions is generally infeasible because these functions can be easily hacked (Krakovna, 2018; Amodei et al., 2016; Dulac-Arnold et al., 2021) to produce behaviors that do not align with human intent. Language is an expressive communication channel for human intent and allows bypassing reward design, but learning mapping from language to behaviors has historically required collecting and annotating behaviors that correspond to language (Jang et al., 2022; ONeill et al., 2023). This strategy is impractical at scale where samples from the agents large space of behaviors need to be labeled. Instead, an approach that strictly makes use of models learned in purely unsupervised way becomes desirable. How can generalist agents interpret language commands into behaviors? Large-scale multimodal foundation models (Wang et al., 2022) provide us with part of the solution. Trained on large amounts of internet data, they can assist in generating video segments that communicate what performing task entails. An issue with the video generation models is that they may generate video frames demonstrating tasks that are out of distribution for the current agents domain; for instance, the current agent can be in simulated environment, and the video generation models Equal contribution, Equal Advising. Correspondence to hsikchi@utexas.edu 1 Figure 1: RLZero framework of imagine, project and imitate: video trajectory is imagined using the text prompt in the agents observation space and projected to real agent observations. Using observation-only imitation learning, the generated trajectory is grounded in policy that mimics the behavior demonstrated by the video. produce generations resembling the real world with potentially differing dynamics. In this work, we propose to fix this problem by projecting frames to agent observations under semantic similarity scoring metric (Radford et al., 2021b; Zhai et al., 2023). This frame-by-frame similarity search allows us to produce sequence of observations grounded in the agents interaction history and presents an expectation of what the task would look like grounded in agents observation space. However, the discovered frame sequence might not adhere to environment dynamics or even be feasible. This presents us with our next question: How do we generate behaviors that resemble the grounded imagined trajectories in zero-shot1 manner? For this, we rely on unsupervised RL techniques that allow us zero-shot inference for diverse set of behaviors specified by any reward function. Unsupervised RL offers an ideal tool for zero-shot behavior inference, enabling an agent to leverage task-agnostic prior interactions with the environment to encode diverse behaviors. generalist agent should be capable of wide variety of skills, and these skills are expected to be learned through the course of the agents prior interactions with the environment. By comparison, providing expert demonstrations and annotating each skill of an agent with language description is prohibitively expensive. Classical reinforcement learning algorithms sidestep the first issue of requiring expert demonstration but still require specifying reward function. Prior research (Rocamonde et al., 2023; Baumli et al., 2023) has used large Vision-Language Models (VLMs) to obtain rewards for language command. Even when reward function is provided, training policy for reward function from scratch each time is time-consuming, undesirable, and potentially unsafe. Instead, we want agents that are sufficiently capable of solving wide variety of tasks in zero-shot manner (without further training or gradient updates). Hence, we turn to Unsupervised RL techniques, sometimes termed as Behavior Foundation Models (BFM) (Touati & Ollivier, 2021; Park et al., 2024a; Agarwal et al., 2024), that allow learning behaviors for all possible reward functions subject to model capacity constraints. BFMs work by pretraining optimal policies for all rewards defined in the span of learned state features. During inference, the optimal policy corresponding to particular reward function can be obtained in closed form. Our method sidesteps the requirement of reward functions and instead frames the problem of language-to-skill inference as state-only distribution matching to the grounded imagined trajectories. Notably, this work leverages the capability of unsupervised RL methods to provide zero-shot solution to distribution matching. Finally, our method parallels the imagination capabilities of humans to picture in their mind possibilities in the real world (Sarbin, 2004; Sarbin & Juhasz, 1970; Pylyshyn, 2002). RLZero works in three simple steps: a) Imagine: Imagine trajectories given language command: We use generative models to visualize the desired tasks specified by text as sequences of frames. These trajectories are action-free and may not be dynamically feasible. b) Project: The frames of imagined trajectories are projected to real observations of the agent. c) Imitate: RLZero leverages the agents prior environmental interactions to directly output policy that matches the state visitation distribution of the imagined trajectories. Our experiments show that RLZero is promising approach to designing an interpretable link connecting humans to RL agents. We demonstrate that RLZero is an effective method on variety 1Our usage of the term zero-shot is based on the zero shot RL definition from Touati et al. (2023) 2 of tasks where reward function design would require full-time reward engineer. We show that RLZero also opens possibilities for zero-shot cross-embodiment transfer, first approach to be able to do this to our knowledge. Our contribution is the framework of imagine, project, and zero-shot imitate, which diverges from the classical wisdom of using VLMs as reward functionswhich can be hackedand instead focusing on zero-shot imitation with unsupervised RL, which admits unique solution that matches the imagined behavior."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Language and Control: There is rich history of using language to solve various tasks in RL: task specification (Thomason et al., 2015; Goyal et al., 2021b; Ma et al., 2023; Baumli et al., 2023; Rocamonde et al., 2023; Stepputtis et al., 2020; Brohan et al., 2022; 2023), transfer and generalization (Goyal et al., 2021a; Jang et al., 2022; Liang et al., 2023), using language to provide hierarchies that allow for solving long-horizon tasks (Ahn et al., 2022; Jiang et al., 2019), driving exploration (Goyal et al., 2019; Harrison et al., 2017; Wang et al., 2023; Ma et al., 2024), human-in-the-loop learning (Chen et al., 2020; Chevalier-Boisvert et al., 2019), giving feedback to AI agents (Wang et al., 2024b), reward design (Yu et al., 2023), etc. Most existing methods either require labels for mapping language to low-level actions or generate reward functions that need to be trained by interacting with the environment to generate low-level control policy. Recent work (Mazzaglia et al., 2024) proposed an unsupervised approach to grounding language to low-level skills but requires re-training the RL agent for each given task prompt. In contrast, our work presents method that allows for zero-shot mapping of languages to low-level skills. large portion of prior work has been limited to using language in setting where expert demonstrations are provided, but this puts heavy burden on data collection to cover the large number of skills possible in the environment, which quickly becomes impractical considering the vast array of interactions intelligent agents can perform with their environments. Our approach forgoes this limitation by relying on zero-shot RL agent capable of mimicking arbitrary imaginations generated for given text. Zero-shot RL: Zero-shot RL promises the ability to quickly produce optimal policies for any given task defined by reward function. wide variety of methods have been developed to achieve zero-shot RL, which are in some ways generalizations of multi-task RL (Caruana, 1997). Most of these works assume class of tasks where they can produce policies zero-shot. These tasks can be goal-conditioned (Kaelbling, 1993; Durugkar et al., 2021; Agarwal et al., 2023; Sikchi et al., 2023; Ma et al., 2022b), linear span of certain state-features (Dayan, 1993; Barreto et al., 2017; Blier et al., 2021b; Touati & Ollivier, 2021; Park et al., 2024a; Agarwal et al., 2024) or some combination of some skills (Eysenbach et al., 2018; 2022; Park et al., 2024b). Other methods (Agarwal et al., 2024) have looked at representing all policies in RL but learning this becomes cumbersome in high dimensions. Recent works (Wu et al., 2018; Touati & Ollivier, 2021; Touati et al., 2023; Park et al., 2024a; Agarwal et al., 2024) employ successor measure-based representation learning objective to be able to provide near-optimal policies for arbitrary reward function subject to model capacity constraints. Our work leverages these methods and finds the best reward supported by the representations that will produce the language-conditioned imagined trajectory."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "We consider learning agent in Markov Decision Process (MDP) (Puterman, 2014; Sutton & Barto, 2018) which is defined as tuple: = (S, A, p, R, γ, d0) where and denote the state and action spaces respectively, denotes the transition function with p(ss, a) indicating the probability of transitioning from to taking action a; denotes the reward function, γ (0, 1) specifies the discount factor and d0 denotes the initial state distribution. The reinforcement learning objective is to obtain policy π : (A) that maximizes expected return: Eπ[(cid:80) t=0 γtr(st)], where we use Eπ to denote the expectation under the distribution induced by at π(st), st+1 p(st, at) and (A) denotes probability simplex supported over A. Multimodal Video-Foundation Models (ViFMs) and In-Domain Video Generation: Multimodal ViFMs (Wang et al., 2022; 2024a; Tong et al., 2022) facilitate the understanding of video data in shared representation space of modalities such as text or audio. Recent works in video 3 generation (Kondratyuk et al., 2023; Blattmann et al., 2023) have used this capability to condition the model of various input modalities that are first converted to this shared representation space. Notably, these models can utilize text prompts to guide content, style, and motion, or employ an image as the initial frame of subsequent video sequence. For this work, we use off-the-shelf video generation models VM that predict sequence of video frames {i1, i2, ...in} given task specified in natural language by first projecting the language in common embedding space across modalities. Formally, VM : {i1, i2, ...in}. Zero-shot RL using Successor Measures: Successor Measures (Blier et al., 2021a) have been recently studied (Touati & Ollivier, 2021; Agarwal et al., 2024) for their ability to obtain zero-shot RL policies for any given reward function. Mathematically, successor measures define the measure over future states visited as π, π(s, a, X) = Eπ[ (cid:88) t0 γtpπ(st+1 Xs, a)] S. (1) Successor Features (Dayan, 1993) are the discounted sum of state features under policy π and are given as, ψπ(s, a) = Eπ[ γtϕ(st)], (cid:88) t0 (2) where ϕ : Rd are state features spanning reward functions using r(s) = ϕT z, and are the linear weights. It has been shown that learning successor measures can be used to jointly obtain successor features along with these state features. Lemma 1. (Theorem 13 of Touati et al. (2023)) For an offline dataset with density ρ, if the successor measure is represented as π(s, a, s+) = ψπ(s, a)φ(s+)ρ(s+), then ψ is the successor feature ψπ(s, a) for state feature ϕ(s) = φ(s)T (Eρ(φφT ))1. The zero-shot RL pipeline thus works in two steps: pretraining and zero-shot inference. To represent distribution over all policies, the policy can be conditioned on z, so the successor feature can be represented as ψ(s, a, z). Step 1 Pretraining: Obtain successor features and state features jointly by learning πz parameterized as ψ(s, a, z)φ(s+), πz (s, a, X) = (cid:90) s+X ψ(s, a, z)φ(s+)ρ(s+) z, (3) Furthermore, the policy πz can be co-trained to be the near-optimal policy under reward function r(s) = ϕ(s)T z, thus defining mapping from reward to optimal policies. This is implemented by setting πz to output greedy actions with maximum Q-value under the reward function r. πz(s) = arg max Qπz (s, a) = arg max (cid:90) M πz (s, a, s+)r(s+)ρ(s+)ds+ (4) Step 2 Zero-Shot Inference: Once the successor features and basic features are obtained, the zero-shot policy can be obtained given any task by inferring the optimal that parametrizes the policy. If the task is specified using reward function r, the optimal parameterizing the policy can be obtained by: = arg max Eρ[M πz r]. (5) For parameterized as Equation 3, and πz defined as Equation 4, can be found in closed form as = Eρ[φ(s)r(s)] (Touati & Ollivier, 2021). For other representations of successor measures, such as PSM (Agarwal et al., 2024), this can be found by solving simple constraint optimization problem. Following prior work (Pirotta et al., 2023), we shall refer to this framework of pretraining successor features that can be used for zero-shot inference as Behavior Foundation Models (BFM). Setting: The agent has access to an offline interaction dataset dO with density ρ consisting of {s, o, a, s} reward-free tuples where denotes the observation space (images for the purpose of this paper). 4 Figure 2: Example Imagined Trajectories: The video model imagines frames conditioned on the task specified as text prompt do lunges."
        },
        {
            "title": "4 RLZERO: ZERO-SHOT PROMPT TO POLICY",
            "content": "RLZero uses components trained with unsupervised learning to map language to behaviors. In the following sections, we describe the steps involved in detail: First, we present how an imagined trajectory is generated from prompt. Then, we discuss how the imagined trajectory for given prompt is projected to real observations of an agent. Finally, we describe the zero-shot procedure for inferring policy that matches the behavior in the imagined trajectory. 4.1 IMAGINE: GENERATIVE VIDEO MODELING Grounding language to tasks in robotics has historically (Goyal et al., 2021a; Jang et al., 2022; ONeill et al., 2023) required costly annotation labels that map language to task examples specified through image or state trajectories. Large video-language foundation models (ViFMs) help lift that requirement by training on vast amounts of internet videos, thus giving us rich prior of grounding language commands to videos. common issue in robotics, both simulated and real world, is that the domains often differ from real-world videos. We rely on generative video modeling approach, GenRL (Mazzaglia et al., 2024), that uses task encoder provided by an off-the-shelf ViFM (InternVideo (Wang et al., 2022)) along with GRU architecture to imagine the video in latent space trained to be reconstructed to the environment domain. Training the video generation model does not require labels mapping language to tasks and is fully unsupervised. With an increase in ViFM scaling and developments in controllable video generation (Bruce et al., 2024; Hu et al., 2022; Ni et al., 2023; Chen et al., 2025), few examples of environment domain may be sufficient to generate high quality in-domain imagination. Thus, given language instruction el, we obtain sequence of frames (i1, i2...iT ) = VM (el) that represents an imagination of what the task looks like in the environment domain. These imaginations are not expected to adhere to the agents environment dynamics. Figure 2 shows an example of what these imaginations look like using an off-the-shelf video generation model (Mazzaglia et al., 2024). 4.1.1 GROUNDING IMAGINATION IN REAL-STATES Algorithm 1 RLZero 1: Init: Pretrained Video Generation Model VM , Pretrained BFM πz, Offline Exploration Dataset dO The imaginings produced by VLMs can be noisy, unrealizable, and not exactly representative of the domain. We propose to use similarity-based retrieval for the nearest frames in the dataset of the agents prior environmental interactions dO to project the imagined trajectories in real observations. This step allows us to match imagination to real observations in the semantic space, giving us the flexibility to use imaginations in differing cross-embodiment domains than the agent (e.g. discussed in Section 5.2). In this work, we use performant image embedding approach for retrieval, SigLIP (Zhai et al., 2023), to map both the imagined frame and agent observation to latent embedding space, which is trained for similarity matching with contrastive objective. We use an encoding function : to individually map sequence of images to shared text-image embedding space. For each consecutive length sequence of frames in the imagined trajectory, we output the following agent observations: 5: Use Theorem 1 for zero-shot inference to obtain BFM({s1, s2, ..., sl}) = zimit and return πzimit . real observations using embedding similarity as in Eq 6. 2: Given: text prompt 3: Generate imagination video given the text prompt: {i1, i2, ..il} = VM (t) 4: Project imagined frames the to 5 Figure 3: Grounding Imagination in Real Observations: We use nearest image retrieval defined by cosine similarity in the embedding space to output real observation from the dataset that matches the imagined observation. otk:t = arg max otk:t E(otk:t) E(itk:t) E(otk:t)E(itk:t) [T ]. (6) Using previous frames allows us to identify state variables that correspond to quantities such as velocity, acceleration, etc, that are not identifiable from single frame. This technique of frame-stacking is commonly used in visual RL (Laskin et al., 2020) and has the interpretation of making the observation inputs Markov. Using the offline interaction dataset, we find corresponding proprioceptive states in addition to the real observation that we will subsequently use for distribution matching. While we rely on proprioceptive states in this work to solve distribution matching, in general our approach is not limited as BFMs may be trained with image observations if the state information is unavailable. 4.2 IMITATE: DISTRIBUTION MATCHING WITH ZERO-SHOT RL The successor measure-based family of BFMs (Touati & Ollivier, 2021; Touati et al., 2023; Agarwal et al., 2024) captures the state visitation distribution of any policy in their learned bag of skills parameterized by and given by ρπz (s) = Es0,a0ρ[M πz (s0, a0, s)], where denotes the successor measure. We take the distribution matching perspective of imitation learning (Ghasemipour et al., 2020; Ni et al., 2021; Sikchi et al., 2024) and minimize the distance between the state visitation distributions of the grounded imagined trajectories (expert) and the policy denoted by ρE and ρπz respectively: zimit = arg min distance(ρπz (s), ρE(s)), (7) where the distance can be chosen to be mean-squared error, -divergence, Integral Probability Metrics (IPM), etc. In general, minimizing the distance via gradient descent can provide solution zimit to distribution matching. For the special case of KL divergence, Theorem 1 shows that zimit can be obtained in closed form using learned distribution ratio between expert and offline interaction dataset ρE/ρ. Theorem 1. Define J(π, r) to be the expected return of policy π under reward r. For an offline dataset dO with density ρ, learned log distribution ratio: ν(s) = log( ρE (s) ρ(s) ), DKL(ρπ, ρE) J(π, rimit) + DKL(ρπ(s, a), ρ(s, a)) where rimit(s) = ν(s) s. The corresponding zimit minimizing the upper bound is given by zimit = Eρ[rimit(s)φ(s)] = EρE [ ν(s) eν(s) φ(s)] where φ denoted state features learned by the BFM. Thus, with the reward functions specified by rimit, we can use the closed form solution of zimit = Eρ[φ(s)rimit(s)] to retrieve the policy that mimics the grounded imagined behavior. This reward function requires learning discriminator to obtain the distribution ratio, which can lead to instabilities, but heuristic yet performant alternative is to use shaped reward function r(s) = eν(s), similar to Pirotta et al. (2023), which allows zero-shot inference (zimit = EρE [φ(s)]) without 6 Figure 4: RLZero in action: Qualitative examples of RL converting the given language prompts into behaviors across different domains. Top to bottom: Cheetah, Walker, Quadruped, Stickman. learning discriminator. We compare both approaches in Appendix C.1. The performance for both these methods are almost identical and we defer to the latter one in all our experiments. Using state-only visitation matching objective can be limiting in the case where environmental dynamics permit the permutation of observation sequences that result in the same visitation distribution. This limitation can be relaxed by instead matching visitation on {s, s}. This requires minimal changes to training the BFM, but we found this to not be limitation with the environments we consider."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Our experiments seek to understand the quality of behaviors that the RLZero approach is able to produce given language prompts. The evaluation of these behaviors can be challenging as, unlike the traditional RL setting, we do not have access to ground truth reward function. Instead, we have prompts that can be inherently ambiguous but reflect the reality of human-robot interaction. An obvious evaluation metric is to ask humans how much the generated behavior resembles their expectation of the behavior given the prompt. We use multimodal LLMs to evaluate such preferences as proxy to human preferences, as recent studies (Chen et al., 2024) have shown them to be correlated (up to 79.3%). tasks consider four DMC control (Cheetah, Walker, Quadruped, Setup: We and Stickman (Mazzaglia et al., 2024)). The Stickman environment reflects human morphology with challenging control due to large observation and action space. For task-conditioned video-generation we use off-the-shelf models from Mazzaglia et al. (2024). To obtain the nearest observation corresponding to the imagined image, we use SigLIP (Zhai et al., 2023), state-of-the-art image-text embedding model. In all environments except Stickman, we collect data using RND (Burda et al., 2018) using the protocol specified in ExoRL (Yarats et al., 2022). For Stickman, we augment our dataset with replay buffers of the agent trained for run and walk behaviors, as obtaining meaningful tasks with pure random exploration is difficult with the large action-space of Stickman. The detailed composition of the datasets can be found in Appendix B.3. The behavior foundation model can be trained using any zero-shot RL method using successor features (Park et al., 2024a; Touati et al., 2023; Agarwal et al., 2024). In our experiments, we use the Forward-Backward zero-shot RL algorithm (Touati et al., 2023) using offline datasets. Setting and Baselines: For our evaluations, we consider the setting where the agent has no access to the simulator during test time. This setting truly reflects the ability of the agents to use prior exploratory data to learn meaningful behaviors. We compare state-of-the-art model-free offline RL algorithms that are capable of learning from purely offline data. For RL algorithms, the 7 Image-language reward Video-language reward RLZero IQL TD3 (Base Model) TD3 IQL Walker Lying Down Walk like human Run like human Do lunges Cartwheel Strut like horse Crawl like worm Quadruped Cartwheel Dance Walk using three legs Balancing on two legs Lie still Handstand Cheetah Lie down Bunny hop Jump high Jump on back legs and backflip Quadruped walk Stand in place like dog Stickman Lie down stable Lunges Praying Headstand Punch Plank 2/5 (0.950.00) 1/5 (0.930.00) 5/5 (0.950.02) 4/5 (0.940.01) 4/5 (0.950.01) 5/5 (0.960.00) 4/5 (0.930.00) 1/5 (0.950.00) 5/5 (0.940.00) 2/5 (0.920.00) 2/5 (0.930.01) 1/5 (0.870.00) 2/5 (0.910.01) 3/5 (0.920.02) 3/5 (0.980.00) 3/5 (0.940.01) 3/5 (0.930.01) 3/5 (0.960.02) 4/5 (0.930.01) 2/5 (0.920.00) 0/5 (0.920.00) 1/5 (0.850.00) 2/5 (0.900.01) 2/5 (0.890.02) 0/5 (0.900.01) - (0.890.02) - (0.830.02) - (0.880.03) - (0.910.02) - (0.930.01) - (0.940.02) - (0.920.01) - (0.950.00) - (0.940.00) - (0.910.00) - (0.930.00) - (0.900.01) - (0.910.02) - (0.870.00) - (0.980.00) - (0.940.01) - (0.920.00) - (0.850.01) - (0.880.00) - (0.910.01) - (0.930.02) - (0.890.02) - (0.900.00) - (0.880.02) - (0.930.03) 2/5 (0.930.01) 3/5 (0.920.01) 1/5 (0.910.01) 2/5 (0.920.00) 3/5 (0.940.01) 1/5 (0.940.00) 1/5 (0.920.01) 3/5 (0.950.01) 3/5 (0.940.02) 2/5 (0.910.01) 2/5 (0.930.01) 3/5 (0.940.00) 4/5 (0.920.01) 2/5 (0.940.00) 1/5 (0.980.00) 0/5 (0.940.01) 0/5 (0.910.01) 3/5 (0.980.00) 3/5 (0.980.01) 4/5 (0.930.00) 2/5(0.920.01) 0/5 (0.870.01) 2/5 (0.900.01) 3/5 (0.880.02) 0/5 (0.890.01) 5/5 (0.940.01) 4/5 (0.940.00) 2/5 (0.940.00) 3/5 (0.930.00) 4/5 (0.960.01) 3/5 (0.960.03) 2/5 (0.950.01) 1/5 (0.950.01) 1/5 (0.940.01) 3/5 (0.930.01) 2/5 (0.930.00) 2/5 (0.950.00) 2/5 (0.940.00) 3/5 (0.940.01) 3/5 (0.970.02) 5/5 (0.930.01) 2/5 (0.920.01) 3/5 (0.990.01) 0/5 (0.980.00) 1/5 (0.930.00) 0/5 (0.920.00) 0/5 (0.870.01) 1/5 (0.870.01) 4/5 (0.910.00) 0/5 (0.930.00) 5/5 (0.930.00) 5/5 (0.980.00) 5/5 (0.960.00) 5/5 (0.940.01) 4/5 (0.950.01) 5/5 (0.960.00) 3/5 (0.890.01) 4/5 (0.920.02) 5/5 (0.930.01) 5/5 (0.930.01) 5/5 (0.940.02) 2/5 (0.920.00) 3/5 (0.910.00) 2/5 (0.900.01) 5/5 (0.960.00) 5/5 (0.930.01) 5/5 (0.910.01) 4/5 (0.970.01) 3/5 (0.970.00) 4/5 (0.910.00) 5/5(0.960.00) 4/5 (0.910.00) 4/5 (0.900.00) 4/5 (0.900.02) 3/5 (0.960.00) Average 51.2% (0.926) Base Model (0.908) 40% (0.927) 44.8% (0.936) 83.2 (0.933)% Table 1: Win rates computed by GPT-4o of policies trained by different methods when compared to base policies trained by TD3+Image-language reward. RLZero shows marked improvement over using embedding cosine similarity as reward functions. reward is obtained as suggested by recent approaches that show the utility of VLMs as reward functions (Baumli et al., 2023; Rocamonde et al., 2023). We consider two sources of reward: Image-language cosine similarity using SigLIP embedding, and Video-language cosine similarity using InternVideo2 embeddings. Video language embeddings take into account context and can potentially lead to more accurate reward estimation. Once the rewards are available, we use TD3 (Fujimoto et al., 2018) and IQL (Kostrikov et al., 2021) as the representative offline RL algorithms to obtain policies. 5.1 BENCHMARKING ZERO-SHOT PERFORMANCE FOR CONTINUOUS CONTROL The ability to specify prompts and generate agent behavior allows us to explore complex behaviors that might have required complicated reward function design. We curate set of 25 tasks across 4 DM-control environments. Each of the agents has unique capabilities as result of its embodiment, and the prompts are specified to be reasonable tasks to expect for the specific domain. Furthermore, we filtered out prompts for which our off-the-shelf video generation model was unable to faithfully generate videos. We discuss this more in Appendix B.2. For each prompt, we generate behaviors for 5 seeds. The performance of any given method is evaluated as the win rate over the base method. We chose the base model for our comparisons as the policies trained via TD3 on image-language rewards. For each seed, we present the observation frames that the policy generated by different methods observes and pass it to Multimodal LLM capable of video understanding, which is used as judge. Since the number of tokens can get quite large with the long default horizon of the agent (1000 horizon), we subsample the videos by choosing every 8 frames and selecting the first 64 frames of size 256 256. We observed this subsampling to retain temporal consistency and the effective horizon (8 32 = 256) to be long enough to demonstrate the task requested by the prompt. Table 1 demonstrates the win rates by different methods when evaluated by GPT-4o-preview. We find that RLZero achieves win rate of 83.2% when compared to the best baseline which achieves win rate of 51.2%. Figure 4 shows examples of behaviors output by RLZero on some of the prompts from our evaluation set. We also consider another metric for comparison embedding similarity between video of the generated behavior and the text. We use InternVideo2 to embed the videos and take the cosine similarity with the prompt used to generate the behavior. Table 1 also shows the results for this metric of comparison. Unfortunately, we observed that the similarity score is frequently higher even for behaviors that differ significantly from the prompt. This points to limitation of using this metric for evaluation. Some reasons for this failure could be the limited context length of 8 for the video embedding model or misalignment between video and text embedding vectors (Liang et al., 2022)."
        },
        {
            "title": "5.2 CAN RLZE R O SUCCEED AT CROSS-EMBODIMENT IMITATION?",
            "content": "The intermediate stage in RLZero of matching the closest observations in the offline dataset to frame from video is based on semantic similarity. This means that we are not restricted to generating videos in the same domain of the agent and still expect semantic search to generalize for out-of-domain matching. Subsequently, we can skip the imagine step completely if we are given an expert demonstration. To investigate this, we consider collection of videos scraped from Youtube as well as videos generated by open-source video generation tools like MetaAI and empirically test if RLZero is able to replicate the behaviors. We focus on the Stickman environment for our experiments here as it reflects human embodiment closely and allows us to use human videos from the internet. for use data Table 4 lists the set of 10 videos we cross-embodiment imitation. We use the win rate metric computed with GPT-4o with prompt similar to Section 5.1, but modified to take in the frames from the original video instead of specified task description. We compare against SMODICE (Ma et al., 2022a) which allows for using state-only in observational conjunction with suboptimal offline data for imitation learning. This allows us to ablate the quality of imitation produced by successor measure-based method that uses one model for all tasks as opposed to SMODICE that trains new model for each task. RLZero achieves win rate of 80% against SMODICE. observation This matches from Pirotta et al. (2023) that DICE-based methods lag behind in performance on observation-only imitation tasks. Table 4 shows the win rate per task by RLZero across 10 youtube video imitation tasks. Figure 5 shows qualitative comparison of the video and the obtained behavior on few videos. Figure 5: Examples for cross embodied imitation: RLZero can mimic motions demonstrated in YouTube or AI generated videos zero-shot. the 5.3 ABLATION AND FAILURE CASES Imagination-free behavior generation: While the imagine, project, and imitate framework allows for interpretability into agents behavior, we investigate if we can amortize the imagination and embedding search cost by directly mapping the language embedding to the skill embedding in the Behavior Foundation Models latent space. For this, we consider sampling uniformly in the latent space of the BFM and embedding the generated image observation sequence through ViFM, which we denote by e. Given the observation sequence, we generate the zimit using the zero-shot inference process and learn mapping from zimit using small 3-layer MLP. On the same tasks considered in Table 5, we observe imagination-free RLZero to have win rate of 65.71% over TD3 base model on Walker environment when compared to RLZero that had win rate of 91.4%. more thorough explanation of imagination-free RLZero can be found in Appendix B.6."
        },
        {
            "title": "The",
            "content": "stages failures. the remain state match, we can task to certain extent. failures fail and imitate: projection imagination individually, but i.e., by investigating the videos and the closest"
        },
        {
            "title": "Our proposed method RLZero is not without\ncan",
            "content": "Failures: of interpretable, comment on the agents ability to faithfully complete that imagine, 1. What cannot cannot The video generation model used in our work from Mazzaglia et al. is fairly small and limited in capability. We encountered limitations when generating complex behaviors with this model and found it to be sensitive to prompt engineering. Fortunately, as models get bigger and are trained on larger set of data, this limitation can be overcome. Figure 6a shows some examples of these failures with the corresponding prompts. (2024) (a) Imagined behaviors: Top: stickman: raise hand while standing in place, Bottom: walker: kick tasks. failure image retrieval semantic 2. Limitation of search-based image retrieval: In this work, we used SigLIP, which has shown commendable performance We for observed the following scenarios (e.g. Figure 6b): a) Background distractors: We observe the image-similarity to latch on to features from the background and produce incorrect retrieval; b) Rough symmetries: In tasks where the agent is roughly symmetric (e.g. Walker when the head and legs are almost identical with slight difference in width) the image retrieval fails by giving an incorrect permutation w.r.t the rough symmetries. (b) Failed projection for cross-embodied video. Figure 6: Failure Cases in RLZero cases in"
        },
        {
            "title": "6 CONCLUSION",
            "content": "Language presents an appealing and human-friendly alternative to reward design for task specification. In this work, we presented completely unsupervised approach for grounding language to low-level behavior in zero-shot manner. completely unsupervised approach allows us to bypass requiring costly annotators for labeling wide variety of behaviors with language, and zero-shot approach allows us to avoid training during deployment time along with the advantage of generating the behaviors instantaneously. We propose RLZero, framework to imagine what behavior specified by text prompt looks like and to ground that imagination to policy via imitation. Unlike reward functions, this approach is not prone to reward hacking as the distribution matching objective specifies the task completely and accurately. Our evaluations show that the behaviors generated by RLZero show an improvement over using reward functions derived from image-language of video-language models. Furthermore, Future Directions: RLZero opens up the possibility of prompting to generate policy. Zero-shot approaches are always expected to be near-optimal due to the projection of reward to low dimensional space as well as limited coverage of offline interaction data. But this serves as good initialization for further fine-tuning. How to fine-tune efficiently without forgetting remains an open question. learned skills can be combined according to the hierarchy specified in language instructions, allowing for the completion of complex long-horizon tasks. Since the mechanism of RLZero allows for interoperability to some extent by observing the nearest states as well as imagination, automatic failure detection becomes appealing. For the setting of prompt-to-policy, we lack accurate evaluation metrics since the true reward function is unknown, and human evaluation can be subjective. Finally, with larger context-window video understanding models, we believe an end-to-end pipeline of language embedding to task embedding (imagination-free RLZero) can become more appealing."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Matteo Pirotta, Ahmed Touati, Andrea Tirinzoni, Alessandro Lazaric and Yann Ollivier for enlightening discussion on unsupervised RL. This work has in part taken place in the Safe, Correct, and Aligned Learning and Robotics Lab (SCALAR) at The University of Massachusetts Amherst and Machine Intelligence through Decision-making and Interaction (MIDI) Lab at The University of Texas at Austin. SCALAR research is supported in part by the NSF (IIS-2323384), the Center for AI Safety (CAIS), and the Long-Term Future Fund. HS, SA, SP, MR, and AZ are supported by NSF 2340651, NSF 2402650, DARPA HR00112490431, and ARO W911NF-24-1-0193. This work has in part taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (W911NF-23-2-0004, W911NF-17-2-0181), DARPA (Cooperative Agreement HR00112520004 on Ad Hoc Teamwork), Lockheed Martin, and Good Systems, research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research."
        },
        {
            "title": "REFERENCES",
            "content": "Siddhant Agarwal, Ishan Durugkar, Peter Stone, and Amy Zhang. f-policy gradients: general framework for goal-conditioned RL using f-divergences. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= EhhPtGsVAv. Siddhant Agarwal, Harshit Sikchi, Peter Stone, and Amy Zhang. Proto successor measure: Representing the space of all possible solutions of reinforcement learning, 2024. URL https: //arxiv.org/abs/2411.19418. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, Hado van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, et al. Vision-language models as source of rewards. arXiv preprint arXiv:2312.09187, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/2311.15127. Leonard Blier, Corentin Tallec, and Yann Ollivier. Learning successor states and goal-dependent values: mathematical viewpoint. arXiv preprint arXiv:2101.07123, 2021a. Leonard Blier, Corentin Tallec, and Yann Ollivier. Learning successor states and goal-dependent values: mathematical viewpoint. arXiv preprint arXiv:2101.07123, 2021b. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 11 Anthony Brohan, Noah Brown, Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Vision-language-action models transfer web knowledge to robotic control. arXiv:2307.15818, 2023. Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Rt-2: arXiv preprint Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Calisthenicmovement. The most underrated exercise youre not doing!, 2021. URL https:// youtu.be/_3p4b4jVfFU?si=poU0t9XAhDf5U_g6. YouTube video. Rich Caruana. Multitask learning. Machine Learning, 28:4175, 1997. URL https://api. semanticscholar.org/CorpusID:45998148. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788, 2024. Valerie Chen, Abhinav Gupta, and Kenneth Marino. Ask your humans: Using human instructions to improve generalization in reinforcement learning. arXiv preprint arXiv:2011.00517, 2020. Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real image animation with text-guided motion control. In European Conference on Computer Vision, pp. 475491. Springer, 2025. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: First steps towards grounded language learning with human in the loop. In International Conference on Learning Representations, volume 105. New Orleans, LA, 2019. Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural computation, 5(4):613624, 1993. Gabriel Dulac-Arnold, Nir Levine, Daniel Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Machine Learning, 110(9):24192468, 2021. Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for reinforcement learning. Advances in Neural Information Processing Systems, 34:86228636, 2021. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without reward function. CoRR, abs/1802.06070, 2018. URL http: //arxiv.org/abs/1802.06070. Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry of unsupervised reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=3wU2UX0voE. Scott Fujimoto and Shixiang Shane Gu. minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:2013220145, 2021. Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pp. 15871596. PMLR, 2018. Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pp. 12591277. PMLR, 2020. Prasoon Goyal, Scott Niekum, and Raymond Mooney. Using natural language for reward shaping in reinforcement learning. arXiv preprint arXiv:1903.02020, 2019. Prasoon Goyal, Raymond Mooney, and Scott Niekum. Zero-shot task adaptation using natural language. arXiv preprint arXiv:2106.02972, 2021a. Prasoon Goyal, Scott Niekum, and Raymond Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. In Conference on Robot Learning, pp. 485497. PMLR, 2021b. Brent Harrison, Upol Ehsan, and Mark Riedl. Guiding reinforcement learning exploration using natural language. arXiv preprint arXiv:1707.08616, 2017. Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation In Proceedings of the IEEE/CVF Conference on Computer Vision and with text descriptions. Pattern Recognition, pp. 1821918228, 2022. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. If you use this software, please cite it as below. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 9911002. PMLR, 2022. Yiding Jiang, Shixiang Shane Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019. Leslie Pack Kaelbling. In International Joint Conference on Artificial Intelligence, 1993. URL https://api.semanticscholar.org/CorpusID: 5538688. Learning to achieve goals. Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, and Kee-Eung Kim. Demodice: Offline imitation learning with supplementary imperfect demonstrations. In ICLR. OpenReview.net, 2022. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021. Victoria Krakovna. Specification gaming examples in ai. Available at vkrakovna. wordpress. com, 2018. Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33:1988419895, 2020. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 94939500. IEEE, 2023. Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= S7Evzt9uit3. LivestrongWoman. Lying leg raises, 2014. URL https://youtu.be/Wp4BlxcFTkE?si= Jdmjhu05kjm8pK1L. YouTube video. 13 Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, and Jens Kober. Explorllm: Guiding exploration in reinforcement learning with large language models. arXiv preprint arXiv:2403.09583, 2024. Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani. Smodice: Versatile offline imitation learning via state occupancy matching. arXiv preprint arXiv:2202.02433, 2022a. Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022b. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. Multimodal foundation world models for generalist embodied agents. arXiv preprint arXiv:2406.18043, 2024. Alo Moves. How to do downward dog adho mukha svanasana tutorial with dylan werner, February 2019. URL https://youtu.be/EC7RGJ975iM?si=GQWepAU1zZ_IQ3mc. YouTube video. Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1844418455, 2023. Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Ben Eysenbach. f-irl: Inverse In Conference on Robot Learning, pp. reinforcement learning via state marginal matching. 529551. PMLR, 2021. Move With Nicole. 35 min full body workout intermediate pilates flow, 2021. URL https: //youtu.be/mU0JDQItAkE?si=U51AZVvzZ8Uty8z5. YouTube video. Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Open Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. Seohong Park, Tobias Kreiman, and Sergey Levine. Foundation policies with hilbert representations. arXiv preprint arXiv:2402.15567, 2024a. Seohong Park, Oleh Rybkin, and Sergey Levine. METRA: Scalable unsupervised RL with metric-aware abstraction. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=c5pwL0Soay. Matteo Pirotta, Andrea Tirinzoni, Ahmed Touati, Alessandro Lazaric, and Yann Ollivier. Fast In NeurIPS 2023 Foundation Models for Decision imitation via behavior foundation models. Making Workshop, 2023. Martin Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. Zenon Pylyshyn. Mental imagery: In search of theory. Behavioral and brain sciences, 25(2): 157182, 2002. Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021b. 14 Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Vision-language models are zero-shot reward models for reinforcement learning. arXiv preprint arXiv:2310.12921, 2023. Theodore Sarbin. The role of imagination. Narrative analysis: Studying the development of individuals in society, pp. 5, 2004. Theodore Sarbin and Joseph Juhasz. Toward theory of imagination. Journal of personality, 38(1):5276, 1970. Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy Zhang, and Scott Niekum. Score models for offline goal-conditioned reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023. Harshit Sikchi, Caleb Chuck, Amy Zhang, and Scott Niekum. dual approach to imitation learning from observations with offline datasets. arXiv preprint arXiv:2406.08805, 2024. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33:1313913150, 2020. Daniela Suarez. Get your splits / hip flexibility 28 day splits challenge 17 min daniela suarez, 2022. URL https://youtu.be/63bhMpFZnvQ?si=G0kEk0mr0U35lC86. YouTube video. Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Peter Stone. Learning to interpret natural language commands through human-robot dialog. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training, 2022. URL https://arxiv. org/abs/2203.12602. Ahmed Touati and Yann Ollivier. Learning one representation to optimize all rewards. In NeurIPS, pp. 1323, 2021. Ahmed Touati, Jeremy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In ICLR. OpenReview.net, 2023. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Ziang Yan, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2: Scaling foundation models for multimodal video understanding, 2024a. URL https://arxiv.org/ abs/2403.15377. Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, and Zackory Erickson. Rl-vlm-f: Reinforcement learning from vision language foundation model feedback. arXiv preprint arXiv:2402.03681, 2024b. 15 Well+Good. How to do push-up the right way well+good, 2019. URL https://youtu. be/bt5b9x9N0KU?si=Ge6EXcMJ7_3Vf6ev. YouTube video. Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586, 2018. Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Dont change the algorithm, change the data: Exploratory data for offline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022. Jess Yoga. Full body standing yoga - improve flexibility and balance, 2022. URL https:// youtu.be/vAA2RS4LQe0?si=Cwe5GsE8S5g7bwO4. YouTube video. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In Proceedings of the IEEE/CVF International Conference on Computer image pre-training. Vision, pp. 1197511986, 2023."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "HS proposed the idea for the work, implemented the code for the core method, designed experimental protocols, and wrote code for evaluation, and contributed significantly to the manuscript, including general paper writing. SA contributed to theoretical insights, offline RL experiments, and paper writing. PJ worked on designing the experimental pipeline and conducting experiments for prompt-to-policy, cross-embodiment, and ablations, and helped in improving paper writing. SP implemented the projection method, evaluated various models for projection, worked on imagination-free inference and wrote the relevant sections. CC worked on offline RL experiments and wrote the relevant sections. MR worked on imagination-free RLZero and writing relevant sections. PS, AZ, SN supervised the research, suggested improvements to method and experiments, and assisted in writing the paper. PROOF FOR THEOREM 1 Theorem 1. Define J(π, r) to be the expected return of policy π under reward r. For an offline dataset dO with density ρ, learned log distribution ratio: ν(s) = log( ρE (s) ρ(s) ), DKL(ρπ, ρE) J(π, rimit) + DKL(ρπ(s, a), ρ(s, a)) where rimit(s) = ν(s) s. The corresponding zimit minimizing the upper bound is given by zimit = Eρ[rimit(s)φ(s)] = EρE [ ν(s) eν(s) φ(s)] where φ denoted state features learned by the BFM. Proof. Let ρ be the density of the offline dataset, ρπ be the visitation distribution w.r.t. policy π and ρE be the expert density. The distribution matching objective mentioned in Equation 7 using KL divergence is given as: With simple algebraic manipulation, the divergence can be simplified to, DKL(ρπρE) min ρπ DKL(ρπρE) =Eρπ (cid:2) log =Eρπ (cid:2) log (cid:3) + Eρπ (cid:3) (cid:2) log ρπ ρ (cid:3) + DKL(ρπ(s)ρ(s)) = J(π, log ) + DKL(ρπ(s)ρ(s)) J(π, log ) + DKL(ρπ(s, a)ρ(s, a)) ρ ρE ρ(s) ρE(s) ρE ρ ρE ρ The last line follows from the fact that DKL(ρπ(s)ρ(s)) DKL(ρπ(s, a)ρ(s, a)). DKL(ρπ(s, a)ρ(s, a)) = Eρπ(s,a)[log = Eρπ(s,a)[log ] ] ρπ(s, a) ρ(s, a) ρπ(s)π(as) ρ(s)πD(as) ρπ(s) ρ(s) ρπ(s) ρ(s) = Eρπ(s,a)[log ] + Eρπ(s,a)[log = Eρπ(s)[log ] + Eρπ(s,a)[log ] π(as) πD(as) π(as) πD(as) ] = DKL(ρπ(s)ρ(s)) + Esρπ [DKL(π(as)πD(as))] DKL(ρπ(s)ρ(s)) 17 (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) Rewriting the minimization of the upper bound of KL as maximization problem by reversing signs, we get: (cid:20) max π J(π, log ρE ρ ) DKL(ρπ(s, a)ρ(s, a)) (cid:21) (19) The first term is an RL objective with reward function given by log( ρE ρ ), and the second term is an offline regularization to constrain the behaviors of offline datasets. Following prior works Kim et al. (2022); Ma et al. (2022a), since our BFM is trained on an offline dataset and limited to output skills in support of dataset actions, and we can ignore the regularization to infer the latent parameterizing the skill. heuristic yet performant alternative is to use shaped reward function of ρE ρ , which allows us to avoid training the discriminator completely and was shown to lead to performant imitation in Pirotta et al. (2023)."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 ENVIRONMENTS B.1.1 DM-CONTROL ENVIRONMENTS We use continuous control environments from the DeepMind Control Suite (Tassa et al., 2018). Walker: It has 24 dimensional state space consisting of joint positions and velocities and 6 dimensional action space where each dimension of action lies in [1, 1]. The system represents planar walker. Cheetah: It has 17 dimensional state space consisting of joint positions and velocities and 6 dimensional action space where each dimension of action lies in [1, 1]. The system represents planar biped cheetah. Quadruped: It has 78 dimensional state space consisting of joint positions and velocities and 12 dimensional action space where each dimension of action lies in [1, 1]. The system represents 3-dimensional ant with 4 legs. Stickman: Stickman was recently introduced as task that bears resemblance to humanoid in Mazzaglia et al. (2024). It has 44 dimensional observation space and 10 dimensional action space where each dimension of action lies in [1, 1]. For all the environments we consider image observations of size 64 64. All DM Control tasks have an episode length of 1000. B.2 EVALUATION PROTOCOL To evaluate models for behavior generation through language prompts, we considered set of 4 prompts per environment. One key consideration in designing these prompts was the generative video models capability of generating reasonable imagined trajectories. Due to computing limitations, we were restricted to using fairly small video embedding ( 1 billion parameters) and generation model ( 43 million parameters). The interpretability of our framework allows us to declare failures before they happen by looking at the generations for imagined trajectories. For the set of task prompts specified by language, there is no ground truth reward function and there does not exist reliable quantitative metric to verify which of the methods perform better. Instead, since humans communicate their intents via language, humans are the best judge of whether the agent has demonstrated the behavior they intended to convey. In this work we use Multimodal LLM as judge, following studies by prior works demonstrating the correlation of LLMs judgment to humans (Chen et al., 2024). We use GPT-4o model as the judge, where the GPT-4o model is provided with two videos, one generated by base method, and another generated by one of the methods we consider, and asked for preference between which video is better explained by the text prompt for the task. When inputting the videos to the judge, we randomize the order of the baseline and proposed methods to reduce the effect of anchoring bias. The prompt we use to compare the two methods is given here: For prompt to policies: 1 response = client.chat.completions.create( 2 model=MODEL, messages=[ {\"role\": \"system\", \"content\": \"For the given summarization: {task prompt}, which video is more aligned with the summarization?\"}, {\"role\": \"user\", \"content\": [ \"Video A\", *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": fdata:image/jpg;base64,{x} }}, video1), \"Video B\", *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": fdata:image/jpg;base64,{x} }}, video2), \"FIRST provide one-sentence comparison of the two videos and explain which you feel the given summarization explains better. SECOND, on new line, state only or to indicate which video is better explained by the given summarization. Your response should use the format: Comparison: <one-sentence comparison and explanation> Better explained by summarization: <A or B>\" ] } ], temperature=0, For cross-embodiment video to policies: 1 cross_embodied_video_description = [*map(lambda x: {\"type\": \"image_url\", 2 \"image_url\": {\"url\": fdata:image/jpg;base64,{x}}}, cross_embodied_video)] 3 4 5 response = client.chat.completions.create( 6 model=MODEL, messages=[ {\"role\": \"system\", \"content\": f\"For the original video: {cross_embodied_video_description}, which of the following given videos describe behavior more similar to the original video?\"}, {\"role\": \"user\", \"content\": [ \"Video A\", *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"URL\": fdata:image/jpg;base64,{x}, }}, video1), \"Video B\", *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"URL\": fdata:image/jpg;base64,{x}, }}, video2), \"FIRST provide one-sentence comparison of the two videos and explain which you feel matches the behavior shown in original video better . SECOND, on new line, state only or to indicate which video is better aligned to the task demonstrated in the original video. Your response should use the format: Comparison: <one-sentence comparison and explanation> 19 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 29 ) 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 Better matches the original video: <A or B>\" ] } ], temperature=0, ) B.3 DATASET COLLECTION FOR ZERO-SHOT RL For Cheetah, Walker, Quadruped, and Stickman environments, our data is collected following pure exploration algorithm with no extrinsic rewards. In this work, we use intrinsic rewards obtained from Random Network Distillation (Burda et al., 2018) to collect our dataset based on the protocol by ExoRL (Yarats et al., 2022) and using the implementation from repository ExoRL repository. For Cheetah, Walker, and Quadruped, our dataset comprises 5000 episodes and equivalently 5 million transitions, and for Stickman, our dataset comprises 10000 episodes or equivalently 10 million transitions. Due to the high dimensionality of action space in Stickman, RND does not discover lot of meaningful behaviors; hence we additionally augment the dataset with 1000 episodes from the replay buffer of training for running reward function and 1000 episodes of replay buffer trained on standing reward function. B.4 BASELINES Zero-shot text to policy behavior has not been widely explored in RL literature. However, Offline RL using language-based rewards utilizes an offline dataset to learn policies and is thus zero-shot in terms of rolling out the learned policy. This makes it meaningful baseline to compare against. Offline RL uses the same MDP formulation as described in Section 3 to learn policy π : (A), given reward function : and offline dataset D. The offline dataset consists of state, action, next-state, reward transitions (s, a, s, r(s)). One of the core challenges of Offline RL is to learn Q-function that does not overestimate the reward of unseen actions, which then at evaluation causes the agent to drift from the support of the offline dataset D. We implement two offline RL baselines to compare with RLZero Implicit Q-learning (IQL, Kostrikov et al. (2021)) and Offline TD3 (TD3, Fujimoto & Gu (2021)). Both of these methods share the same offline dataset as used to learn the successor measure in RLZero, which is described in Section 5, and gathered using RND. Since these datasets are reward-free, we must still construct reward function that provides meaningful rewards for an agent achieving the behavior that aligns with the text prompt. Formally, given language instruction el l, frame stack (otk, otk+1, . . . , ot) I, and embedding VLM ϕ : Z, which can also embed frame stacks ϕ : (and where observations oi I, and we use oi for this section), the reward for corresponding language instruction and frame stack is the cosine similarity between the stacked language embedding and the frame embedding: r(otk:t, el) = ϕ(el) ϕ(otk:t) ϕ(el)ϕ(otk:t) (20) For any individual task, el is fixed and this is reward function dependent on observations (as represented by frame stack otk:t). Notice that this representation closely matches that in Equation 6, but instead of finding the optimal sequence of observations, we simply compute reward as the cosine similarity between language and frames. Since the strength of the embedding space is vital to the quality of the reward function for offline RL, we evaluate two different vision-language models: Image-language reward (SigLIP Zhai et al. (2023)): take stack of 3 frames encode them using SigLIP, then the reward is computed as the cosine distance of the embeddings and the SigLIP embedding of language. Video-language reward (InternVideo2 Wang et al. (2024a)): this method takes in previous frames o0:t1 as context and uses it to generate an embedding of the current frame observation ot. The video encoder then takes the cosine similarity of ϕ(o0:t) and ϕ(el). This allows the reward function to provide rewards based not only on reaching certain states, but the agent exhibiting temporally 20 extended behaviors that match the behavior. In practice, providing rewards using an image-based encoder for frame stacks can be challenging for tasks such as walking because they require context, and video-based rewards offer way to better encode the temporal context. B.4.1 OFFLINE RL Implicit Q-learning (Kostrikov et al., 2021) Implicit Q-learning builds on the classic TD error (revised in our context of language-instruction rewards): L(θ) = E(s,a,s,a)D[(r(s, el) + γQˆθ(s, a) Qθ(s, a))2] to learn function Qθ. IQL builds on this loss to handle the challenge of ensuring that the Q-values do not speculate on out-of-distribution actions while also ensuring that the policy is able to exceed the performance of the behavior policy. Exceeding the behavior policy is important because the dataset is collected using RND, meaning that any particular trajectory from the dataset is unlikely to perform well on language reward. The balance of performance is achieved by optimizing the objective with expectile regression: Lτ 2 (u) = τ 1(u < 0)u2 Where τ > 0.5 is the selected expectile. Expectile regression gives greater weight to the upper expectiles of distribution, which means that the function will focus more on the upper values of the function. Rather than optimize the objective with Q(s, a) directly, IQL uses value function to reduce variance to give the following objectives: LV (ψ) = E(s,a)D[Lτ 2 (Qθ(s, a)Vψ(s))] LQ(θ) = E(s,a,s,a)D[Lτ 2 (r(s, el) + Vψ(s) Qθ(s, a))] Using the Q-function, policy can be extracted using advantage weighted regression: L(ϕ) = E(s,a)D[exp(β(Qθ(s, a) Vψ(s))) log πϕ(as)]. Where β is the inverse temperature for the advantage term. TD3 (Fujimoto et al., 2018): TD3 was demonstrated to be the best performing algorithm when learning from exploratory RND datasets in (Yarats et al., 2022). While TD3 does not explicitly address the challenges discussed in implicit Q-learning and learns using Bellman Optimality backups, the approach is simple and works well in practice. The algorithm uses deterministic policy extraction π : to give the following objective: π = arg max π E(s,a)D[Q(s, π(s))] B.5 RLZERO B.5.1 TEXT TO IMAGINED BEHAVIOR WITH VIDEO MODELS To generate proposed video frame sequence, we utilize the GenRL architecture and provide the workflow using equations from the original paper (Mazzaglia et al., 2024). First, the desired text prompt is embedded with the underlying video foundation model InternVideo2 (Wang et al., 2024a) e(l) = (l) (y). These embeddings are then repeated nf rames times (we use nf rames = 32) to match the temporal structure expected by the world model. The repeated text embeddings are passed through an aligner module e(v) = fψ(e(l)). The aligner is implemented as UNet and it is used to address the multimodality gap (Liang et al., 2022) when embeddings from different modalities occupy distinct regions in the latent space. Next, actions are constructed by concatenating the aligned video embeddings with temporal embeddings. The temporal embeddings are one-hot encodings of the time step modulo nf rames providing frame-level positional information. The first action is passed to the world model connector pψ(ste) to initialize the latent state. For each subsequent time step, the sequence model ht = fϕ(st1, at1, ht1) (implemented as GRU) updates the deterministic state ht. The deterministic state ht is mapped to stochastic latent state 21 Figure 7: Illustrative diagram of imagination-free RLZero inference (st) using the dynamics predictor pϕ(stht). The dynamics predictor, implemented as an ensemble of MLPs, predicts the sufficient statistics (mean and standard deviation) for Normal distribution over st. During inference, the mean of this distribution is used as the latent state. Finally, the latent state st is passed to the convolutional decoder pϕ(xtst) to reconstruct the video frame xt. This process is repeated for all time steps (t = 1, ..., nf rames). B.5.2 GROUNDING IMAGINED OBSERVATIONS TO OBSERVATIONS IN OFFLINE DATASET As described in Section 4, we ground imagined sequences by retrieving real offline states based on similarity in an embedding space. This enables us to create suitable z-vector for distribution matching which is the expected value of the state features under the distribution of imagined states (ρimagined). During our dataset collection phase, we save both the agents proprioceptive state as well as the corresponding rendered images and search over the images to then find the corresponding state. Our code supports both stacked-frame embeddings and single-frame embeddings. We find that stacked-frame embeddings were helpful in modeling temporal dependencies through velocity and acceleration, which are crucial for recreating the intended behavior. SigCLIP (Zhai et al., 2023), which replaces CLIPs (Radford et al., 2021a) softmax-based contrastive loss with pairwise sigmoid loss, resulted in qualitatively better matches to exact positions within sequences, imitating behavior more accurately than CLIP. For both models, we use the OpenCLIP (Ilharco et al., 2021) framework. Our matching process first involves precomputing embeddings offline, which are stored in chunks of up to 100,000 frames to optimize memory usage and retrieval speed. During inference, we load this file and embed the query frame sequence from GenRL (Mazzaglia et al., 2024) into the same latent space. We process these query embeddings by dividing them into chunks of k-frame sequences (k generally 3 or 5), where each sequence consists of the current frame and the 1 preceding frames. If there are not enough preceding frames, we repeat the first frame to fill the gap. For each chunk of saved embeddings, we compute dot products between the query chunk and all subsequences of size in the saved embeddings. We track the highest similarity score for each query chunk and return the frames corresponding to the closest embedding sequences. B.5.3 TRAINING ZERO-SHOT RL AGENT In this work, we chose Forward-Backward (FB) (Touati & Ollivier, 2021) as our zero-shot RL algorithm and train it on proprioceptive inputs. Our implementation follows closely from the authors codebase . Specifically, FB trains Forward, Backward, and Actor networks. The backward networks are used to map demonstration or reward function to skill, which is then used to learn latent-conditional Actor. The hyperparameters for our FB implementation are listed below: for FB https://github.com/ Implementation: We build upon the facebookresearch/controllable_agent and implement all the algorithms under uniform setup for network architectures and same hyperparameters for shared modules across the algorithms. We keep the same method agnostic hyperparameters and use the author-suggested method-specfic hyperparameters. The hyperparameters for all methods can be found in Table 2: codebase Table 2: Hyperparameters for zero-shot RL with FB. Hyperparameter Replay buffer size Representation dimension Batch size Discount factor γ Optimizer Learning rate Momentum coefficient for target networks Stddev σ for policy smoothing Truncation level for policy smoothing Number of gradient steps Regularization weight for orthonormality loss (ensures diversity) FB specific hyperparameters Hidden units (F ) Number of layers (F ) Hidden units (b) Number of layers (b) Value 5 106, 10 106 (for stickman) 128 1024 0.98 Adam 3 104 0.99 0.2 0.3 2 106 1 1024 3 256 2 B.6 IMAGINATION-FREE RLZE In this section, we propose an alternate method (Figure 7) for mapping task description into Instead of first embedding text prompt eℓ, generating video, then mapping usable policy. the video to policy parametrization, we propose to map the text prompt directly to policy parametrization. To do this, we learn latent mapper : zimitation that relates the latent space of ViLM to the latent space of our policy parametrization. The mapper is 3 layer MLP with hidden size of 512. Pretraining: We first generate dataset of episodes containing diverse behaviors by rolling out the behavior foundation model conditioned on uniformly random sampled z. The resulting image observation sequences are then down-sampled (by 8) and sliced to break up each episode into smaller chunks of length 8; this preprocessing step helps increase the behavioral diversity and improves the ability of the ViFM to capture semantic meaning. The resulting clips are then embedded using ViFM (InternVideo2 (Wang et al., 2024a)) where each embedding is denoted by (as in Section 5.3). Now we have set of sequences of length 8 consisting of image observation along with their proprioceptive states, and the embedding for the image sequence. Now, an obvious option is to map the embedding of image sequence to the that generated the trajectory. Unfortunately, the way BFMs are trained, they do not account for optimal policy invariance to reward functions. That is multiple reward functions that induce the same optimal policy are mapped to different encodings in the Z-space. This presents problem for the latent mapper, as it becomes one-to-many mapping for any language encoding. We present an alternative solution which ensures that only one target is used for given distribution of states induced by language encoding. To achieve this we turn back to the imitation learning objective where the sequence of proprioceptive states is used to obtain policy representation using Lemma 1 which gives the latent corresponding to the policy that minimizes the distribution divergence to the sequence of given states. We refer to the policy representation embedding space from the Forward-Backward representation as Zimitation-space. When optimizing the latent mapper m, we minimize the following loss: L(D, m) = E(zimitation,e)D (cid:20) m(e) zimitation m(e) zimitation (cid:21) The latent space of the Backward representation is aligned with the latent space of the policy parametrization, so learning mapping from the ViFM space to the Backward space is equivalent to learning mapping from the ViFM space to the policy parametrization space. 23 Walker Lying Down Walk like human Run like human Do lunges Cartwheel Strut like horse Crawl like worm Quadruped Cartwheel Dance Walk using three legs Balancing on two legs Lie still Handstand Cheetah Lie down Bunny hop Jump high Jump on back legs and backflip Quadruped walk Stand in place like dog Stickman Lie down stable Lunges Praying Headstand Punch Plank Average"
        },
        {
            "title": "RLZero",
            "content": "5/5 5/5 5/5 5/5 5/5 5/5 1/5 4/5 5/5 4/5 4/5 2/5 4/5 1/5 5/5 5/5 5/5 2/5 4/5 5/5 5/5 4/5 5/5 4/5 4/5 5/5 5/5 5/5 5/5 4/5 5/5 3/ 4/5 5/5 5/5 5/5 2/5 3/5 2/5 5/5 5/5 5/5 4/5 3/5 4/5 5/5 4/5 4/5 4/5 3/5 82.4% 83.2% Table 3: Win rates computed by GPT-4o of policies trained by different methods when compared to base policies trained by TD3+Image-language reward. Inference: During inference, the language prompt is embeddedto latent vector el. known issue with multimodal embedding models is the embedding gap (Liang et al., 2022), which makes the video embeddings unaligned with text embeddings. To account for this gap, we use an aligner trained in an unsupervised fashion from previous work Mazzaglia et al. (2024) to align the language embedding (el aligned). Then the aligned embedding is passed through the latent mapper to get the policy conditioning zimitation which gives us the policy that achieves the desired behavior specified through language."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "C.1 ZERO-SHOT IMITATION: DISCRIMINATOR VS DISCRIMINATOR-FREE We experiment whether optimizing tighter bound to KL divergence at the expense of training an additional discriminator in Lemma 1 leads to performance improvements. Table 3 shows that using discriminator does not lead to performance improvement and training-free inference time solution achieves slightly higher win rate. 24 C.2 CROSS EMBODIMENT EXPERIMENTS Table 4 describes the videos used for cross-embodiment along with the win rate of the behaviors generated by RLZero when compared to base model which trains SMODICE (Ma et al., 2022a) on the nearest states found with the same grounding methods as RLZero. Prompt Descriptions human in backflip position downward facing dog yoga pose cow yoga pose downward facing dog yoga pose with one leg raised in the air lying on back with one leg raised in the air lying on back with both legs raised in the air high plank yoga pose sitting down with legs laid in the front bending forward while standing on one leg with the other leg raised in the back front splits where both legs stretched out along the same line human Video Link/Meta AI Prompt animated trying backflip right profile of yoga pose downward facing dog Moves (2019) Moves (2019) Nicole (2021) LivestrongWoman (2014) Well+Good (2019) Calisthenicmovement (2021) Yoga (2022) Suarez (2022) Table 4: Comparison of Win rates vs SMODICE Win rate vs SMODICE 2/ 1/5 5/5 5/5 5/5 5/5 5/5 5/5 3/5 4/ C.3 IMAGINATION-FREE RLZERO COMPLETE RESULTS We consider an ablation of our method by understanding the need for imagination by replacing the step with an end-to-end learning alternative. This is novel baseline described in Appendix B.6. Table 5 shows the results of this end-to-end alternative which maps the shared latent space of video language models to behavior policy. C.4 MORE FAILURE CASES We include more failure cases in Figure 8 and Figure 9 as they can help in understanding the limitations of RLZero better and may inform future work. Environment/Task Walker Lying Down Walk like human Run like human Do lunges Cartwheel Strut like horse Crawl like worm"
        },
        {
            "title": "RLZero",
            "content": "RLZero (Imagination-Free) 5/5 5/5 5/5 5/5 4/5 5/5 3/5 5/5 4/5 1/5 5/5 5/5 3/5 0/5 Table 5: Win rates computed by GPT-4o of policies trained by different methods when compared to base policies trained by TD3+Image-language reward. RLZero shows marked improvement over using embedding cosine similarity as reward functions. Figure 8: More examples of failed imagination by the video generation model used in RLZero. From top to bottom: Walker - kick, Quadruped - bunny hop, Cheetah - frontroll, Stickman - raise hands while standing in place Figure 9: More examples of failed grounding by the image retrieval model used in RLZero. The top image shows the imagined frame or frame from the embodied video, and the bottom is the nearest frame obtained from the agents prior interaction dataset."
        }
    ],
    "affiliations": [
        "Meta AI",
        "Sony AI",
        "The University of Texas at Austin",
        "UMass Amherst",
        "University of Alberta"
    ]
}